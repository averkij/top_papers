
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 23 papers. October 17.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">17 октября</span> | <span id="title-articles-count">23 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-16.html">⬅️ <span id="prev-date">16.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-18.html">➡️ <span id="next-date">18.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'};
        let feedDateNext = {'ru': '18.10', 'en': '10/18', 'zh': '10月18日'};
        let feedDatePrev = {'ru': '16.10', 'en': '10/16', 'zh': '10月16日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.11623', 'title': 'VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI', 'url': 'https://huggingface.co/papers/2410.11623', 'abstract': 'Recent advancements in Multi-modal Large Language Models (MLLMs) have opened new avenues for applications in Embodied AI. Building on previous work, EgoThink, we introduce VidEgoThink, a comprehensive benchmark for evaluating egocentric video understanding capabilities. To bridge the gap between MLLMs and low-level control in Embodied AI, we design four key interrelated tasks: video question-answering, hierarchy planning, visual grounding and reward modeling. To minimize manual annotation costs, we develop an automatic data generation pipeline based on the Ego4D dataset, leveraging the prior knowledge and multimodal capabilities of GPT-4o. Three human annotators then filter the generated data to ensure diversity and quality, resulting in the VidEgoThink benchmark. We conduct extensive experiments with three types of models: API-based MLLMs, open-source image-based MLLMs, and open-source video-based MLLMs. Experimental results indicate that all MLLMs, including GPT-4o, perform poorly across all tasks related to egocentric video understanding. These findings suggest that foundation models still require significant advancements to be effectively applied to first-person scenarios in Embodied AI. In conclusion, VidEgoThink reflects a research trend towards employing MLLMs for egocentric vision, akin to human capabilities, enabling active observation and interaction in the complex real-world environments.', 'score': 46, 'issue_id': 137, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': '67310ef96a11f09c', 'authors': ['Sijie Cheng', 'Kechen Fang', 'Yangyang Yu', 'Sicheng Zhou', 'Bohao Li', 'Ye Tian', 'Tingguang Li', 'Lei Han', 'Yang Liu'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University', 'Department of Mechanical and Industrial Engineering, University of Toronto', 'Institute for AI Industry Research (AIR), Tsinghua University', 'School of Data Science, The Chinese University of HongKong', 'Tencent Robotics X', 'Zhili College, Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11623.jpg', 'data': {'categories': ['#science', '#synthetic', '#benchmark', '#graphs', '#video', '#multimodal', '#data', '#robotics', '#open_source', '#games'], 'emoji': '👁️', 'ru': {'title': 'VidEgoThink: новый рубеж в понимании эгоцентрического видео для воплощенного ИИ', 'desc': 'Статья представляет VidEgoThink - комплексный бенчмарк для оценки возможностей понимания эгоцентрического видео мультимодальными большими языковыми моделями (MLLM). Авторы разработали четыре ключевые взаимосвязанные задачи: ответы на вопросы по видео, иерархическое планирование, визуальная локализация и моделирование вознаграждений. Данные для бенчмарка были сгенерированы автоматически на основе датасета Ego4D с использованием GPT-4o и отфильтрованы аннотаторами. Эксперименты показали, что современные MLLM, включая GPT-4o, плохо справляются с задачами понимания эгоцентрического видео, что указывает на необходимость дальнейших исследований в этой области.'}, 'en': {'title': 'Pioneering Egocentric Video Understanding with VidEgoThink', 'desc': 'The paper introduces VidEgoThink, a benchmark designed to evaluate how well multi-modal large language models (MLLMs) understand egocentric videos, which are videos captured from a first-person perspective. It focuses on four tasks: video question-answering, hierarchy planning, visual grounding, and reward modeling, to connect MLLMs with low-level control in Embodied AI. An automatic data generation pipeline is used to create diverse and high-quality data, which is then filtered by human annotators. The study finds that current MLLMs, including GPT-4o, struggle with these tasks, indicating that more advancements are needed for these models to effectively handle first-person video understanding in real-world scenarios.'}, 'zh': {'title': 'VidEgoThink：迈向具身AI的多模态大语言模型', 'desc': '这篇论文介绍了VidEgoThink，这是一个用于评估自我中心视频理解能力的基准。研究设计了四个关键任务：视频问答、层次规划、视觉定位和奖励建模，以连接多模态大语言模型与具身人工智能的低级控制。通过自动数据生成管道和人工筛选，确保数据的多样性和质量。实验结果表明，现有的多模态大语言模型在自我中心视频理解任务上表现不佳，表明基础模型在具身人工智能中的应用仍需重大进展。'}}}, {'id': 'https://huggingface.co/papers/2410.12381', 'title': 'HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks', 'url': 'https://huggingface.co/papers/2410.12381', 'abstract': "Coding tasks have been valuable for evaluating Large Language Models (LLMs), as they demand the comprehension of high-level instructions, complex reasoning, and the implementation of functional programs -- core capabilities for advancing Artificial General Intelligence. Despite the progress in Large Multimodal Models (LMMs), which extend LLMs with visual perception and understanding capabilities, there remains a notable lack of coding benchmarks that rigorously assess these models, particularly in tasks that emphasize visual reasoning. To address this gap, we introduce HumanEval-V, a novel and lightweight benchmark specifically designed to evaluate LMMs' visual understanding and reasoning capabilities through code generation. HumanEval-V includes 108 carefully crafted, entry-level Python coding tasks derived from platforms like CodeForces and Stack Overflow. Each task is adapted by modifying the context and algorithmic patterns of the original problems, with visual elements redrawn to ensure distinction from the source, preventing potential data leakage. LMMs are required to complete the code solution based on the provided visual context and a predefined Python function signature outlining the task requirements. Every task is equipped with meticulously handcrafted test cases to ensure a thorough and reliable evaluation of model-generated solutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering significant challenges. Proprietary models like GPT-4o achieve only 13% pass@1 and 36.4% pass@10, while open-weight models with 70B parameters score below 4% pass@1. Ablation studies further reveal the limitations of current LMMs in vision reasoning and coding capabilities. These results underscore key areas for future research to enhance LMMs' capabilities. We have open-sourced our code and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark.", 'score': 42, 'issue_id': 135, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '7693dd92c7f5b4e2', 'authors': ['Fengji Zhang', 'Linquan Wu', 'Huiyu Bai', 'Guancheng Lin', 'Xiao Li', 'Xiao Yu', 'Yue Wang', 'Bei Chen', 'Jacky Keung'], 'affiliations': ['City University of Hong Kong', 'Rhymes AI', 'Tsinghua University', 'Wuhan University', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12381.jpg', 'data': {'categories': ['#reasoning', '#leakage', '#benchmark', '#cv', '#graphs', '#agi', '#plp', '#multimodal', '#open_source', '#games'], 'emoji': '👁️\u200d🗨️', 'ru': {'title': 'HumanEval-V: Новый рубеж в оценке визуально-кодовых способностей AI', 'desc': 'Статья представляет новый бенчмарк HumanEval-V для оценки способностей больших мультимодальных моделей (LMM) к визуальному пониманию и рассуждению через генерацию кода. Бенчмарк содержит 108 задач по программированию на Python, адаптированных с визуальным контекстом. Авторы провели оценку 19 современных LMM, выявив значительные трудности даже для лучших моделей. Результаты показывают ограничения текущих LMM в области визуального рассуждения и кодирования, обозначая направления для будущих исследований.'}, 'en': {'title': 'Enhancing LMMs: Bridging the Visual Reasoning Gap in Code Generation', 'desc': "The paper introduces HumanEval-V, a new benchmark designed to evaluate Large Multimodal Models (LMMs) on their ability to understand and reason with visual information through coding tasks. It highlights the gap in existing benchmarks that fail to rigorously test LMMs' visual reasoning capabilities, especially in the context of code generation. The benchmark consists of 108 Python coding tasks, each adapted with unique visual elements to prevent data leakage, and includes handcrafted test cases for thorough evaluation. Results from testing 19 state-of-the-art LMMs reveal significant challenges in their visual reasoning and coding abilities, pointing to areas for future research."}, 'zh': {'title': '提升LMMs的视觉推理与编程能力', 'desc': '这篇论文介绍了一种新的基准测试工具HumanEval-V，用于评估大型多模态模型（LMMs）的视觉理解和推理能力。HumanEval-V包含108个经过精心设计的Python编程任务，这些任务结合了视觉元素，以测试模型在视觉推理和代码生成方面的能力。研究发现，当前的LMMs在这些任务中表现不佳，揭示了它们在视觉推理和编程能力上的局限性。论文强调了未来研究的关键领域，以提高LMMs的能力。'}}}, {'id': 'https://huggingface.co/papers/2410.12787', 'title': 'The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio', 'url': 'https://huggingface.co/papers/2410.12787', 'abstract': 'Recent advancements in large multimodal models (LMMs) have significantly enhanced performance across diverse tasks, with ongoing efforts to further integrate additional modalities such as video and audio. However, most existing LMMs remain vulnerable to hallucinations, the discrepancy between the factual multimodal input and the generated textual output, which has limited their applicability in various real-world scenarios. This paper presents the first systematic investigation of hallucinations in LMMs involving the three most common modalities: language, visual, and audio. Our study reveals two key contributors to hallucinations: overreliance on unimodal priors and spurious inter-modality correlations. To address these challenges, we introduce the benchmark The Curse of Multi-Modalities (CMM), which comprehensively evaluates hallucinations in LMMs, providing a detailed analysis of their underlying issues. Our findings highlight key vulnerabilities, including imbalances in modality integration and biases from training data, underscoring the need for balanced cross-modal learning and enhanced hallucination mitigation strategies. Based on our observations and findings, we suggest potential research directions that could enhance the reliability of LMMs.', 'score': 30, 'issue_id': 135, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': 'e40b5fd56a795812', 'authors': ['Sicong Leng', 'Yun Xing', 'Zesen Cheng', 'Yang Zhou', 'Hang Zhang', 'Xin Li', 'Deli Zhao', 'Shijian Lu', 'Chunyan Miao', 'Lidong Bing'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab, Hangzhou, China', 'IHPC A*STAR, Singapore', 'Nanyang Technological University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12787.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#multimodal', '#interpretability', '#ethics', '#training'], 'emoji': '🌈', 'ru': {'title': 'Борьба с галлюцинациями в мультимодальных моделях', 'desc': 'Статья представляет первое систематическое исследование галлюцинаций в крупных мультимодальных моделях (LMM), охватывающее три основные модальности: язык, визуальную и аудио. Авторы выявили две ключевые причины галлюцинаций: чрезмерная опора на одномодальные приоры и ложные межмодальные корреляции. Для оценки галлюцинаций в LMM был создан бенчмарк The Curse of Multi-Modalities (CMM). Исследование подчеркивает необходимость сбалансированного кросс-модального обучения и улучшенных стратегий по снижению галлюцинаций.'}, 'en': {'title': 'Taming Hallucinations in Multimodal Models', 'desc': "This paper explores the issue of hallucinations in large multimodal models (LMMs), which occur when the model's output doesn't match the input from different modalities like language, visuals, and audio. The study identifies two main causes of these hallucinations: relying too much on single-modality information and incorrect connections between different modalities. To tackle these problems, the authors introduce a new benchmark called The Curse of Multi-Modalities (CMM) to evaluate and analyze these hallucinations. The research suggests that improving the balance in how different modalities are integrated and addressing biases in training data can help reduce hallucinations in LMMs."}, 'zh': {'title': '破解多模态模型的幻觉挑战', 'desc': '这篇论文研究了大型多模态模型（LMMs）在处理语言、视觉和音频三种常见模态时出现的幻觉问题。研究发现，幻觉主要由对单一模态的过度依赖和模态间的虚假相关性引起。为了解决这些问题，作者提出了一个名为“多模态诅咒”的基准，用于全面评估LMMs中的幻觉现象。研究结果强调了模态整合的不平衡和训练数据的偏差，建议未来的研究应关注跨模态学习的平衡和幻觉的减轻策略。'}}}, {'id': 'https://huggingface.co/papers/2410.12628', 'title': 'DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception', 'url': 'https://huggingface.co/papers/2410.12628', 'abstract': 'Document Layout Analysis is crucial for real-world document understanding systems, but it encounters a challenging trade-off between speed and accuracy: multimodal methods leveraging both text and visual features achieve higher accuracy but suffer from significant latency, whereas unimodal methods relying solely on visual features offer faster processing speeds at the expense of accuracy. To address this dilemma, we introduce DocLayout-YOLO, a novel approach that enhances accuracy while maintaining speed advantages through document-specific optimizations in both pre-training and model design. For robust document pre-training, we introduce the Mesh-candidate BestFit algorithm, which frames document synthesis as a two-dimensional bin packing problem, generating the large-scale, diverse DocSynth-300K dataset. Pre-training on the resulting DocSynth-300K dataset significantly improves fine-tuning performance across various document types. In terms of model optimization, we propose a Global-to-Local Controllable Receptive Module that is capable of better handling multi-scale variations of document elements. Furthermore, to validate performance across different document types, we introduce a complex and challenging benchmark named DocStructBench. Extensive experiments on downstream datasets demonstrate that DocLayout-YOLO excels in both speed and accuracy. Code, data, and models are available at https://github.com/opendatalab/DocLayout-YOLO.', 'score': 27, 'issue_id': 134, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '0c81afcfe57d6c2f', 'authors': ['Zhiyuan Zhao', 'Hengrui Kang', 'Bin Wang', 'Conghui He'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12628.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#cv', '#optimization', '#data', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '📄', 'ru': {'title': 'DocLayout-YOLO: Быстрый и точный анализ макета документов', 'desc': 'DocLayout-YOLO - это новый подход к анализу макета документов, оптимизирующий соотношение скорости и точности. Метод использует алгоритм Mesh-candidate BestFit для создания синтетического набора данных DocSynth-300K, что улучшает предобучение модели. В архитектуру внедрен модуль Global-to-Local Controllable Receptive для лучшей обработки элементов документа разного масштаба. Авторы также представляют новый бенчмарк DocStructBench для оценки производительности на различных типах документов.'}, 'en': {'title': 'DocLayout-YOLO: Fast and Accurate Document Layout Analysis', 'desc': 'The paper introduces DocLayout-YOLO, a new method for document layout analysis that balances speed and accuracy by using document-specific optimizations. It employs a unique pre-training strategy with the Mesh-candidate BestFit algorithm to create a diverse dataset called DocSynth-300K, enhancing model performance. The model also features a Global-to-Local Controllable Receptive Module to handle different document element scales effectively. Extensive testing shows that DocLayout-YOLO performs well in both speed and accuracy across various document types.'}, 'zh': {'title': 'DocLayout-YOLO：速度与准确性的完美结合', 'desc': '这篇论文介绍了一种新的文档布局分析方法，称为DocLayout-YOLO，它在提高准确性的同时保持了速度优势。通过引入Mesh-candidate BestFit算法，作者创建了一个名为DocSynth-300K的大规模数据集，用于增强文档的预训练效果。为了优化模型，论文提出了一种全局到局部的可控感受模块，以更好地处理文档元素的多尺度变化。实验结果表明，DocLayout-YOLO在速度和准确性方面都表现出色。'}}}, {'id': 'https://huggingface.co/papers/2410.12409', 'title': 'Revealing the Barriers of Language Agents in Planning', 'url': 'https://huggingface.co/papers/2410.12409', 'abstract': 'Autonomous planning has been an ongoing pursuit since the inception of artificial intelligence. Based on curated problem solvers, early planning agents could deliver precise solutions for specific tasks but lacked generalization. The emergence of large language models (LLMs) and their powerful reasoning capabilities has reignited interest in autonomous planning by automatically generating reasonable solutions for given tasks. However, prior research and our experiments show that current language agents still lack human-level planning abilities. Even the state-of-the-art reasoning model, OpenAI o1, achieves only 15.6% on one of the complex real-world planning benchmarks. This highlights a critical question: What hinders language agents from achieving human-level planning? Although existing studies have highlighted weak performance in agent planning, the deeper underlying issues and the mechanisms and limitations of the strategies proposed to address them remain insufficiently understood. In this work, we apply the feature attribution study and identify two key factors that hinder agent planning: the limited role of constraints and the diminishing influence of questions. We also find that although current strategies help mitigate these challenges, they do not fully resolve them, indicating that agents still have a long way to go before reaching human-level intelligence.', 'score': 23, 'issue_id': 134, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': 'faf0a6d62d983811', 'authors': ['Jian Xie', 'Kexun Zhang', 'Jiangjie Chen', 'Siyu Yuan', 'Kai Zhang', 'Yikai Zhang', 'Lei Li', 'Yanghua Xiao'], 'affiliations': ['ByteDance Inc.', 'Carnegie Mellon University', 'Fudan University', 'The Ohio State University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12409.jpg', 'data': {'categories': ['#reasoning', '#rl', '#benchmark', '#agi', '#interpretability', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Преодолевая барьеры: путь к человеческому уровню планирования для ИИ', 'desc': 'Статья исследует проблемы автономного планирования в контексте языковых моделей. Авторы выявляют два ключевых фактора, препятствующих эффективному планированию агентов: ограниченную роль ограничений и уменьшающееся влияние вопросов. Исследование показывает, что даже современные модели, такие как OpenAI o1, достигают лишь 15.6% успеха на сложных задачах планирования реального мира. Несмотря на существующие стратегии улучшения, авторы заключают, что языковые агенты все еще далеки от человеческого уровня планирования.'}, 'en': {'title': 'Bridging the Gap: Enhancing Language Models for Human-Level Planning', 'desc': 'This paper explores the limitations of current language models in achieving human-level planning abilities. It identifies two main factors hindering progress: the limited role of constraints and the diminishing influence of questions. Despite existing strategies to address these issues, they are not fully effective, suggesting that language agents need further development. The study uses feature attribution to analyze these challenges and highlights the gap between current capabilities and human-level intelligence.'}, 'zh': {'title': '揭示语言模型规划能力的瓶颈', 'desc': '这篇论文探讨了自主规划在人工智能中的发展历程，指出当前语言模型在规划能力上仍然不及人类水平。研究发现，限制条件的有限作用和问题影响力的减弱是阻碍语言代理规划能力的两个关键因素。尽管现有策略在一定程度上缓解了这些问题，但并未彻底解决，表明代理距离达到人类智能还有很长的路要走。通过特征归因研究，作者揭示了这些深层次问题的机制和局限性。'}}}, {'id': 'https://huggingface.co/papers/2410.12613', 'title': 'Exploring Model Kinship for Merging Large Language Models', 'url': 'https://huggingface.co/papers/2410.12613', 'abstract': 'Model merging has become one of the key technologies for enhancing the capabilities and efficiency of Large Language Models (LLMs). However, our understanding of the expected performance gains and principles when merging any two models remains limited. In this work, we introduce model kinship, the degree of similarity or relatedness between LLMs, analogous to biological evolution. With comprehensive empirical analysis, we find that there is a certain relationship between model kinship and the performance gains after model merging, which can help guide our selection of candidate models. Inspired by this, we propose a new model merging strategy: Top-k Greedy Merging with Model Kinship, which can yield better performance on benchmark datasets. Specifically, we discover that using model kinship as a criterion can assist us in continuously performing model merging, alleviating the degradation (local optima) in model evolution, whereas model kinship can serve as a guide to escape these traps. Code is available at https://github.com/zjunlp/ModelKinship.', 'score': 19, 'issue_id': 134, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '5154e5ff2c4f7709', 'authors': ['Yedi Hu', 'Yunzhi Yao', 'Ningyu Zhang', 'Shumin Deng', 'Huajun Chen'], 'affiliations': ['National University of Singapore, NUS-NCS Joint Lab, Singapore', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12613.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': '🧬', 'ru': {'title': 'Родство моделей: ключ к эффективному слиянию LLM', 'desc': "Статья посвящена новой стратегии объединения моделей для улучшения больших языковых моделей (LLM). Авторы вводят понятие 'родства моделей', аналогичное биологической эволюции, и исследуют его связь с производительностью после слияния. На основе этого предлагается стратегия 'Top-k Жадного Слияния с Учетом Родства Моделей', которая показывает лучшие результаты на контрольных наборах данных. Исследование демонстрирует, как учет родства моделей помогает избежать локальных оптимумов при эволюции моделей."}, 'en': {'title': '"Model Kinship: Guiding the Evolution of Language Models"', 'desc': "This paper explores the concept of model merging to improve the performance of Large Language Models (LLMs). It introduces 'model kinship,' a measure of similarity between models, which can predict the effectiveness of merging two models. The authors propose a new strategy called Top-k Greedy Merging with Model Kinship, which uses this similarity measure to enhance model performance on benchmark datasets. This approach helps avoid local optima, improving the overall efficiency and capability of LLMs."}, 'zh': {'title': '模型亲缘关系：提升大语言模型合并性能的新策略', 'desc': '这篇论文探讨了大语言模型（LLMs）合并的性能提升和原则。研究引入了模型亲缘关系的概念，类似于生物进化，来分析模型合并后的性能提升。通过实证分析，发现模型亲缘关系与合并后的性能提升有一定关系，这可以指导我们选择合适的候选模型。基于此，提出了一种新的模型合并策略：基于模型亲缘关系的Top-k贪心合并策略，能够在基准数据集上获得更好的性能。'}}}, {'id': 'https://huggingface.co/papers/2410.11081', 'title': 'Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models', 'url': 'https://huggingface.co/papers/2410.11081', 'abstract': 'Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512x512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64x64, and 1.88 on ImageNet 512x512, narrowing the gap in FID scores with the best existing diffusion models to within 10%.', 'score': 18, 'issue_id': 138, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '0c5455c7c793e07e', 'authors': ['Cheng Lu', 'Yang Song'], 'affiliations': ['OpenAI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11081.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Стабильное обучение непрерывных моделей консистентности для быстрой генерации изображений', 'desc': 'Статья представляет новый подход к обучению непрерывных по времени моделей консистентности (CMs) для генеративных моделей диффузии. Авторы предлагают унифицированную теоретическую основу, объясняющую причины нестабильности обучения предыдущих подходов. Они вводят улучшения в параметризацию процесса диффузии, архитектуру нейронной сети и целевые функции обучения. Эти изменения позволили обучить модель с 1.5 миллиардами параметров на ImageNet 512x512, достигнув показателей FID, близких к лучшим существующим моделям диффузии.'}, 'en': {'title': 'Revolutionizing Consistency Models: Faster, Better, and Scalable', 'desc': 'The paper introduces a new approach to improve consistency models (CMs), which are a type of generative model used for creating data like images. Traditional CMs often face issues due to discretized timesteps, leading to errors and extra parameters. The authors propose a unified framework to address these issues by enhancing the diffusion process, network design, and training methods. Their improved model can handle large-scale data and achieves competitive performance with fewer sampling steps, as shown by their impressive FID scores on various datasets.'}, 'zh': {'title': '突破一致性模型的训练瓶颈', 'desc': '这篇论文介绍了一种新的连续时间一致性模型（CMs），用于生成模型的快速采样。传统的CMs使用离散时间步长，容易引入误差和不稳定性。作者提出了一种简化的理论框架，统一了扩散模型和CMs的参数化，解决了训练不稳定的问题。通过改进扩散过程的参数化、网络架构和训练目标，成功在大规模数据集上训练了CMs，并取得了优异的生成效果。'}}}, {'id': 'https://huggingface.co/papers/2410.10672', 'title': 'Large Language Model Evaluation via Matrix Nuclear-Norm', 'url': 'https://huggingface.co/papers/2410.10672', 'abstract': "As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due to their \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD). To mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only serves as a metric to quantify the data compression proficiency of LLM but also provides a convex approximation of matrix rank to capture both predictive discriminability and diversity. By employing the \\( L_{1,2}-norm \\) to further approximate the nuclear norm, we can effectively assess the model's information compression capabilities. This approach reduces the time complexity to \\( O(n^2) \\) and eliminates the need for SVD computation. Consequently, the Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This performance gap becomes more pronounced with larger models, as validated in tests with other models like Pythia. Additionally, evaluations on benchmarks and model responses confirm that our proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLMs' performance, striking a balance between accuracy and computational efficiency. The code is available at https://github.com/MLGroupJLU/MatrixNuclearNorm.", 'score': 18, 'issue_id': 137, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'dbc484f05cdfefb9', 'authors': ['Yahan Li', 'Tingyu Xia', 'Yi Chang', 'Yuan Wu'], 'affiliations': ['International Center of Future Science, Jilin University', 'Key Laboratory of Symbolic Computation and Knowledge Engineering, Jilin University', 'School of Artificial Intelligence, Jilin University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.10672.jpg', 'data': {'categories': ['#small_models', '#benchmark', '#optimization', '#math', '#training', '#open_source'], 'emoji': '🧮', 'ru': {'title': 'Эффективная оценка LLM: быстрее, масштабируемее, точнее', 'desc': 'Статья представляет новую метрику оценки больших языковых моделей (LLM) - Matrix Nuclear-Norm. Эта метрика позволяет эффективно оценивать способность моделей сжимать информацию и уменьшать избыточность. По сравнению с традиционной метрикой Matrix Entropy, новый подход имеет меньшую вычислительную сложность O(n^2) вместо O(n^3). Тесты показали, что Matrix Nuclear-Norm работает в 8-24 раза быстрее для больших моделей, сохраняя при этом надежность оценки.'}, 'en': {'title': 'Speeding Up LLM Evaluation with Matrix Nuclear-Norm', 'desc': 'The paper introduces the Matrix Nuclear-Norm as a new metric for evaluating large language models (LLMs) in terms of their ability to compress information and reduce redundancy. This metric offers a more computationally efficient alternative to traditional methods like Matrix Entropy, reducing time complexity from O(n^3) to O(n^2) by eliminating the need for Singular Value Decomposition (SVD). By using the L_{1,2}-norm to approximate the nuclear norm, the approach maintains accuracy while significantly speeding up the evaluation process, especially for large models like CEREBRAS-GPT and Pythia. The Matrix Nuclear-Norm is validated as a reliable and scalable tool for assessing LLM performance, balancing accuracy with computational efficiency.'}, 'zh': {'title': '矩阵核范数：高效评估大型语言模型的新工具', 'desc': '随着大型语言模型的发展，评估其信息压缩能力和减少冗余的有效指标变得至关重要。传统的矩阵熵指标虽然有用，但由于奇异值分解的复杂性，对大规模模型的计算负担较重。我们提出了矩阵核范数作为新的评估指标，不仅能量化模型的数据压缩能力，还能通过凸近似矩阵秩来捕捉预测的可辨识性和多样性。通过使用 \\( L_{1,2}-norm \\) 进一步近似核范数，我们显著降低了计算复杂度，并在不需要奇异值分解的情况下提高了评估速度。'}}}, {'id': 'https://huggingface.co/papers/2410.11817', 'title': 'Improving Long-Text Alignment for Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2410.11817', 'abstract': 'The rapid advancement of text-to-image (T2I) diffusion models has enabled them to generate unprecedented results from given texts. However, as text inputs become longer, existing encoding methods like CLIP face limitations, and aligning the generated images with long texts becomes challenging. To tackle these issues, we propose LongAlign, which includes a segment-level encoding method for processing long texts and a decomposed preference optimization method for effective alignment training. For segment-level encoding, long texts are divided into multiple segments and processed separately. This method overcomes the maximum input length limits of pretrained encoding models. For preference optimization, we provide decomposed CLIP-based preference models to fine-tune diffusion models. Specifically, to utilize CLIP-based preference models for T2I alignment, we delve into their scoring mechanisms and find that the preference scores can be decomposed into two components: a text-relevant part that measures T2I alignment and a text-irrelevant part that assesses other visual aspects of human preference. Additionally, we find that the text-irrelevant part contributes to a common overfitting problem during fine-tuning. To address this, we propose a reweighting strategy that assigns different weights to these two components, thereby reducing overfitting and enhancing alignment. After fine-tuning 512 times 512 Stable Diffusion (SD) v1.5 for about 20 hours using our method, the fine-tuned SD outperforms stronger foundation models in T2I alignment, such as PixArt-alpha and Kandinsky v2.2. The code is available at https://github.com/luping-liu/LongAlign.', 'score': 14, 'issue_id': 134, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': 'c9ba30d5d0f611d5', 'authors': ['Luping Liu', 'Chao Du', 'Tianyu Pang', 'Zehan Wang', 'Chongxuan Li', 'Dong Xu'], 'affiliations': ['Renmin University of China', 'Sea AI Lab, Singapore', 'The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11817.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#open_source', '#architecture', '#long_context'], 'emoji': '🖼️', 'ru': {'title': 'LongAlign: Превосходя ограничения в генерации изображений по длинным текстам', 'desc': 'Статья представляет метод LongAlign для улучшения генерации изображений по длинным текстовым описаниям. Авторы предлагают сегментированное кодирование длинных текстов и декомпозицию оптимизации предпочтений для более эффективного обучения моделей. Метод преодолевает ограничения существующих подходов, таких как CLIP, и позволяет лучше согласовывать генерируемые изображения с длинными текстами. После дообучения модель Stable Diffusion с помощью LongAlign превзошла более сильные базовые модели в задаче генерации изображений по тексту.'}, 'en': {'title': '"Long Texts, Perfect Images: The LongAlign Revolution"', 'desc': 'The paper introduces LongAlign, a novel approach to improve text-to-image (T2I) diffusion models when dealing with long text inputs. It addresses the limitations of existing encoding methods like CLIP by segmenting long texts and optimizing alignment through decomposed preference models. By dividing texts into segments, LongAlign overcomes input length restrictions, and its reweighting strategy reduces overfitting during fine-tuning. The method significantly enhances T2I alignment, outperforming other models like PixArt-alpha and Kandinsky v2.2.'}, 'zh': {'title': '长文本对齐新突破：LongAlign方法', 'desc': '这篇论文介绍了一种名为LongAlign的新方法，用于解决文本到图像生成模型在处理长文本时的对齐问题。LongAlign通过将长文本分段编码，克服了预训练编码模型的输入长度限制。为了优化对齐效果，论文提出了一种分解的偏好优化方法，利用CLIP模型的偏好评分机制。通过重新加权策略，LongAlign有效减少了过拟合问题，并在对齐性能上超越了其他基础模型。'}}}, {'id': 'https://huggingface.co/papers/2410.12405', 'title': 'ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs', 'url': 'https://huggingface.co/papers/2410.12405', 'abstract': 'Large language models (LLMs) have demonstrated impressive capabilities across various tasks, but their performance is highly sensitive to the prompts utilized. This variability poses challenges for accurate assessment and user satisfaction. Current research frequently overlooks instance-level prompt variations and their implications on subjective evaluations. To address these shortcomings, we introduce ProSA, a framework designed to evaluate and comprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity metric, PromptSensiScore, and leverages decoding confidence to elucidate underlying mechanisms. Our extensive study, spanning multiple tasks, uncovers that prompt sensitivity fluctuates across datasets and models, with larger models exhibiting enhanced robustness. We observe that few-shot examples can alleviate this sensitivity issue, and subjective evaluations are also susceptible to prompt sensitivities, particularly in complex, reasoning-oriented tasks. Furthermore, our findings indicate that higher model confidence correlates with increased prompt robustness. We believe this work will serve as a helpful tool in studying prompt sensitivity of LLMs. The project is released at: https://github.com/open-compass/ProSA .', 'score': 13, 'issue_id': 134, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': 'd2f8e1c0dcdf19fa', 'authors': ['Jingming Zhuo', 'Songyang Zhang', 'Xinyu Fang', 'Haodong Duan', 'Dahua Lin', 'Kai Chen'], 'affiliations': ['Jilin University', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12405.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#interpretability', '#training', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'ProSA: Новый подход к оценке чувствительности языковых моделей к промптам', 'desc': 'Авторы представляют ProSA - фреймворк для оценки и понимания чувствительности больших языковых моделей (LLM) к промптам. Они вводят новую метрику PromptSensiScore и используют уверенность декодирования для объяснения механизмов работы LLM. Исследование показывает, что чувствительность к промптам варьируется между датасетами и моделями, причем более крупные модели демонстрируют большую устойчивость. Авторы обнаружили, что примеры few-shot могут снизить проблему чувствительности, а субъективные оценки также подвержены влиянию промптов.'}, 'en': {'title': 'Mastering the Art of Prompting: Enhancing LLM Robustness', 'desc': 'Large language models (LLMs) can perform well on many tasks, but their success depends a lot on how they are prompted. This paper introduces ProSA, a framework to study how sensitive LLMs are to different prompts using a new metric called PromptSensiScore. The study shows that bigger models handle prompt changes better, and using few-shot examples can help reduce sensitivity. It also finds that when models are more confident, they are less affected by changes in prompts.'}, 'zh': {'title': '理解和提升大型语言模型的提示词鲁棒性', 'desc': '大型语言模型在不同任务中表现出色，但对提示词的敏感性很高，这影响了评估的准确性和用户满意度。为了解决这个问题，研究人员提出了ProSA框架，用于评估和理解提示词敏感性。研究发现，提示词敏感性在不同数据集和模型中波动，较大的模型表现出更强的鲁棒性。通过使用少量示例可以减轻这种敏感性问题，并且模型的高置信度与提示词的鲁棒性增加有关。'}}}, {'id': 'https://huggingface.co/papers/2410.08968', 'title': 'Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements', 'url': 'https://huggingface.co/papers/2410.08968', 'abstract': 'The current paradigm for safety alignment of large language models (LLMs) follows a one-size-fits-all approach: the model refuses to interact with any content deemed unsafe by the model provider. This approach lacks flexibility in the face of varying social norms across cultures and regions. In addition, users may have diverse safety needs, making a model with static safety standards too restrictive to be useful, as well as too costly to be re-aligned.   We propose Controllable Safety Alignment (CoSA), a framework designed to adapt models to diverse safety requirements without re-training. Instead of aligning a fixed model, we align models to follow safety configs -- free-form natural language descriptions of the desired safety behaviors -- that are provided as part of the system prompt. To adjust model safety behavior, authorized users only need to modify such safety configs at inference time. To enable that, we propose CoSAlign, a data-centric method for aligning LLMs to easily adapt to diverse safety configs. Furthermore, we devise a novel controllability evaluation protocol that considers both helpfulness and configured safety, summarizing them into CoSA-Score, and construct CoSApien, a human-authored benchmark that consists of real-world LLM use cases with diverse safety requirements and corresponding evaluation prompts.   We show that CoSAlign leads to substantial gains of controllability over strong baselines including in-context alignment. Our framework encourages better representation and adaptation to pluralistic human values in LLMs, and thereby increasing their practicality.', 'score': 12, 'issue_id': 140, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': 'b81f79d5fa0e574a', 'authors': ['Jingyu Zhang', 'Ahmed Elgohary', 'Ahmed Magooda', 'Daniel Khashabi', 'Benjamin Van Durme'], 'affiliations': ['Johns Hopkins University', 'Microsoft Responsible AI Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.08968.jpg', 'data': {'categories': ['#benchmark', '#multilingual', '#inference', '#ethics', '#data', '#training', '#alignment'], 'emoji': '🛡️', 'ru': {'title': 'Гибкая настройка безопасности языковых моделей без переобучения', 'desc': "Статья представляет новый подход к обеспечению безопасности больших языковых моделей (LLM) под названием Controllable Safety Alignment (CoSA). В отличие от традиционного подхода 'один размер для всех', CoSA позволяет адаптировать модели к различным требованиям безопасности без переобучения. Авторы предлагают метод CoSAlign для настройки LLM на следование конфигурациям безопасности, задаваемым в виде текстовых описаний. Также разработан новый протокол оценки контролируемости и создан бенчмарк CoSApien с реальными сценариями использования LLM."}, 'en': {'title': 'Flexible Safety for Diverse Needs: CoSA Framework for LLMs', 'desc': "The paper introduces Controllable Safety Alignment (CoSA), a framework that allows large language models (LLMs) to adapt to diverse safety requirements without needing to be retrained. Instead of using a fixed safety standard, CoSA uses safety configurations, which are natural language descriptions of desired safety behaviors, that can be modified by authorized users at inference time. The authors propose CoSAlign, a data-centric method that enhances the model's ability to adjust to these safety configurations, and introduce a new evaluation protocol called CoSA-Score to measure both helpfulness and safety. The framework aims to better represent and adapt to varying human values, making LLMs more practical and flexible in real-world applications."}, 'zh': {'title': '灵活适应多元安全需求的语言模型', 'desc': '目前的大型语言模型安全对齐方法通常采用一刀切的方式，拒绝与任何被认为不安全的内容互动。这种方法缺乏灵活性，难以适应不同文化和地区的社会规范。我们提出了一种名为可控安全对齐（CoSA）的框架，通过在系统提示中提供安全配置来调整模型的安全行为，而无需重新训练。CoSAlign方法使得模型能够轻松适应多样化的安全需求，提高了模型的实用性。'}}}, {'id': 'https://huggingface.co/papers/2410.07722', 'title': 'DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities', 'url': 'https://huggingface.co/papers/2410.07722', 'abstract': "Learned Sparse Retrieval (LSR) models use vocabularies from pre-trained transformers, which often split entities into nonsensical fragments. Splitting entities can reduce retrieval accuracy and limits the model's ability to incorporate up-to-date world knowledge not included in the training data. In this work, we enhance the LSR vocabulary with Wikipedia concepts and entities, enabling the model to resolve ambiguities more effectively and stay current with evolving knowledge. Central to our approach is a Dynamic Vocabulary (DyVo) head, which leverages existing entity embeddings and an entity retrieval component that identifies entities relevant to a query or document. We use the DyVo head to generate entity weights, which are then merged with word piece weights to create joint representations for efficient indexing and retrieval using an inverted index. In experiments across three entity-rich document ranking datasets, the resulting DyVo model substantially outperforms state-of-the-art baselines.", 'score': 12, 'issue_id': 139, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'aff6e5f440cea35e', 'authors': ['Thong Nguyen', 'Shubham Chatterjee', 'Sean MacAvaney', 'Iain Mackie', 'Jeff Dalton', 'Andrew Yates'], 'affiliations': ['University of Amsterdam', 'University of Edinburgh', 'University of Glasgow'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.07722.jpg', 'data': {'categories': ['#rag', '#reasoning', '#graphs', '#dataset', '#transfer_learning', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Динамический словарь для точного поиска информации', 'desc': 'Статья представляет новый подход к улучшению моделей разреженного поиска (LSR) путем обогащения их словаря концепциями и сущностями из Википедии. Авторы предлагают использовать динамическую головную часть модели (DyVo), которая объединяет существующие вложения сущностей с компонентом поиска сущностей. Это позволяет модели более эффективно разрешать неоднозначности и оставаться в курсе меняющихся знаний. Эксперименты на трех наборах данных по ранжированию документов, богатых сущностями, показывают значительное превосходство предложенной модели DyVo над современными базовыми моделями.'}, 'en': {'title': 'Boosting Retrieval with Dynamic Vocabulary', 'desc': 'The paper introduces a method to improve Learned Sparse Retrieval (LSR) models by enhancing their vocabulary with Wikipedia concepts and entities. This approach helps the model better understand and retrieve information by resolving ambiguities and incorporating current world knowledge. A key innovation is the Dynamic Vocabulary (DyVo) head, which combines entity embeddings with word piece weights for more accurate indexing and retrieval. Experiments show that this method significantly outperforms existing models in entity-rich document ranking tasks.'}, 'zh': {'title': '动态词汇头：提升实体检索的准确性', 'desc': '这篇论文介绍了一种改进的稀疏检索模型，称为动态词汇头（DyVo），它通过结合维基百科的概念和实体来增强模型的词汇表。传统的模型在处理实体时常常将其分割成无意义的片段，影响了检索的准确性。DyVo头利用现有的实体嵌入和实体检索组件来识别与查询或文档相关的实体，并生成实体权重。实验结果表明，DyVo模型在多个实体丰富的文档排名数据集上显著优于现有的最先进基线。'}}}, {'id': 'https://huggingface.co/papers/2410.08584', 'title': 'ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression', 'url': 'https://huggingface.co/papers/2410.08584', 'abstract': 'The efficiency of large vision-language models (LVLMs) is constrained by the computational bottleneck of the attention mechanism during the prefill phase and the memory bottleneck of fetching the key-value (KV) cache in the decoding phase, particularly in scenarios involving high-resolution images or videos. Visual content often exhibits substantial redundancy, resulting in highly sparse attention maps within LVLMs. This sparsity can be leveraged to accelerate attention computation or compress the KV cache through various approaches. However, most studies focus on addressing only one of these bottlenecks and do not adequately support dynamic adjustment of sparsity concerning distinct layers or tasks. In this paper, we present ZipVL, an efficient inference framework designed for LVLMs that resolves both computation and memory bottlenecks through a dynamic ratio allocation strategy of important tokens. This ratio is adaptively determined based on the layer-specific distribution of attention scores, rather than fixed hyper-parameters, thereby improving efficiency for less complex tasks while maintaining high performance for more challenging ones. Then we select important tokens based on their normalized attention scores and perform attention mechanism solely on those important tokens to accelerate the prefill phase. To mitigate the memory bottleneck in the decoding phase, we employ mixed-precision quantization to the KV cache, where high-bit quantization is used for caches of important tokens, while low-bit quantization is applied to those of less importance. Our experiments demonstrate that ZipVL can accelerate the prefill phase by 2.6times and reduce GPU memory usage by 50.0%, with a minimal accuracy reduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively enhancing the generation efficiency of LVLMs.', 'score': 11, 'issue_id': 136, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': 'faf1b859b79c97a2', 'authors': ['Yefei He', 'Feng Chen', 'Jing Liu', 'Wenqi Shao', 'Hong Zhou', 'Kaipeng Zhang', 'Bohan Zhuang'], 'affiliations': ['Shanghai AI Laboratory, China', 'The University of Adelaide, Australia', 'ZIP Lab, Monash University, Australia', 'Zhejiang University, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.08584.jpg', 'data': {'categories': ['#benchmark', '#cv', '#inference', '#video', '#optimization', '#graphs', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'ZipVL: Ускорение LVLM без потери качества', 'desc': 'ZipVL - это эффективный фреймворк для вывода крупных моделей зрения и языка (LVLM). Он решает проблемы вычислительной и памятной эффективности с помощью динамической стратегии распределения важных токенов. Фреймворк ускоряет фазу предзаполнения, выполняя механизм внимания только на важных токенах. Для уменьшения использования памяти при декодировании применяется смешанная квантизация кэша ключей и значений.'}, 'en': {'title': 'ZipVL: Streamlining Vision-Language Models for Speed and Efficiency', 'desc': 'The paper introduces ZipVL, a framework that enhances the efficiency of large vision-language models by addressing both computational and memory bottlenecks. It dynamically allocates important tokens based on layer-specific attention scores, optimizing the attention mechanism and reducing redundancy. By using mixed-precision quantization for the key-value cache, it balances memory usage and performance. Experiments show that ZipVL significantly speeds up processing and reduces memory requirements with minimal impact on accuracy.'}, 'zh': {'title': 'ZipVL：提升视觉语言模型效率的新策略', 'desc': '这篇论文介绍了一种名为ZipVL的高效推理框架，旨在解决大规模视觉语言模型（LVLMs）中的计算和内存瓶颈问题。ZipVL通过动态分配重要token的比例，基于层特定的注意力分数分布来提高效率。为了加速预填充阶段，ZipVL仅对重要token执行注意力机制，并在解码阶段对KV缓存进行混合精度量化。实验表明，ZipVL在加速预填充阶段和减少GPU内存使用方面表现出色，同时保持了高性能。'}}}, {'id': 'https://huggingface.co/papers/2410.12490', 'title': 'Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective', 'url': 'https://huggingface.co/papers/2410.12490', 'abstract': 'Latent-based image generative models, such as Latent Diffusion Models (LDMs) and Mask Image Models (MIMs), have achieved notable success in image generation tasks. These models typically leverage reconstructive autoencoders like VQGAN or VAE to encode pixels into a more compact latent space and learn the data distribution in the latent space instead of directly from pixels. However, this practice raises a pertinent question: Is it truly the optimal choice? In response, we begin with an intriguing observation: despite sharing the same latent space, autoregressive models significantly lag behind LDMs and MIMs in image generation. This finding contrasts sharply with the field of NLP, where the autoregressive model GPT has established a commanding presence. To address this discrepancy, we introduce a unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling. Furthermore, we propose a simple but effective discrete image tokenizer to stabilize the latent space for image generative modeling. Experimental results show that image autoregressive modeling with our tokenizer (DiGIT) benefits both image understanding and image generation with the next token prediction principle, which is inherently straightforward for GPT models but challenging for other generative models. Remarkably, for the first time, a GPT-style autoregressive model for images outperforms LDMs, which also exhibits substantial improvement akin to GPT when scaling up model size. Our findings underscore the potential of an optimized latent space and the integration of discrete tokenization in advancing the capabilities of image generative models. The code is available at https://github.com/DAMO-NLP-SG/DiGIT.', 'score': 8, 'issue_id': 141, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '959f8e0892b466b9', 'authors': ['Yongxin Zhu', 'Bocheng Li', 'Hang Zhang', 'Xin Li', 'Linli Xu', 'Lidong Bing'], 'affiliations': ['School of Computer Science and Technology, University of Science and Technology of China', 'School of Data Science, University of Science and Technology of China', 'State Key Laboratory of Cognitive Intelligence', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12490.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'DiGIT: Революция в генеративном моделировании изображений', 'desc': 'Статья представляет новый подход к генеративному моделированию изображений. Авторы предлагают дискретный токенизатор изображений для стабилизации латентного пространства. Эксперименты показывают, что авторегрессионная модель с этим токенизатором (DiGIT) превосходит модели латентной диффузии. Результаты демонстрируют потенциал оптимизированного латентного пространства и дискретной токенизации для улучшения генеративных моделей изображений.'}, 'en': {'title': 'Revolutionizing Image Generation: Bridging the Gap with Discrete Tokenization', 'desc': 'This paper explores the effectiveness of latent-based image generative models, particularly focusing on the performance gap between autoregressive models and other models like Latent Diffusion Models (LDMs) and Mask Image Models (MIMs). The authors propose a new discrete image tokenizer, DiGIT, to stabilize the latent space, which enhances the performance of autoregressive models in image generation. Their experiments demonstrate that, with this tokenizer, autoregressive models can outperform LDMs, showing significant improvements similar to those seen in NLP with GPT models. This work highlights the importance of optimizing latent space and using discrete tokenization to improve image generative modeling.'}, 'zh': {'title': '优化潜在空间，提升图像生成', 'desc': '这篇论文探讨了图像生成模型中潜在空间的优化问题，特别是自回归模型在图像生成中的表现。研究发现，尽管自回归模型在自然语言处理中表现优异，但在图像生成中却不如潜在扩散模型和掩码图像模型。为了解决这个问题，作者提出了一种新的离散图像标记器（DiGIT），以稳定图像生成的潜在空间。实验结果表明，这种方法不仅提升了图像理解和生成的效果，还使得自回归模型在图像生成中首次超越了潜在扩散模型。'}}}, {'id': 'https://huggingface.co/papers/2410.11878', 'title': 'Neural Metamorphosis', 'url': 'https://huggingface.co/papers/2410.11878', 'abstract': 'This paper introduces a new learning paradigm termed Neural Metamorphosis (NeuMeta), which aims to build self-morphable neural networks. Contrary to crafting separate models for different architectures or sizes, NeuMeta directly learns the continuous weight manifold of neural networks. Once trained, we can sample weights for any-sized network directly from the manifold, even for previously unseen configurations, without retraining. To achieve this ambitious goal, NeuMeta trains neural implicit functions as hypernetworks. They accept coordinates within the model space as input, and generate corresponding weight values on the manifold. In other words, the implicit function is learned in a way, that the predicted weights is well-performed across various models sizes. In training those models, we notice that, the final performance closely relates on smoothness of the learned manifold. In pursuit of enhancing this smoothness, we employ two strategies. First, we permute weight matrices to achieve intra-model smoothness, by solving the Shortest Hamiltonian Path problem. Besides, we add a noise on the input coordinates when training the implicit function, ensuring models with various sizes shows consistent outputs. As such, NeuMeta shows promising results in synthesizing parameters for various network configurations. Our extensive tests in image classification, semantic segmentation, and image generation reveal that NeuMeta sustains full-size performance even at a 75% compression rate.', 'score': 8, 'issue_id': 138, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'b168c08bbe553e03', 'authors': ['Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11878.jpg', 'data': {'categories': ['#optimization', '#training', '#transfer_learning', '#architecture'], 'emoji': '🦋', 'ru': {'title': 'Самоизменяющиеся нейросети: один манифолд для всех архитектур', 'desc': 'Статья представляет новую парадигму обучения под названием Neural Metamorphosis (NeuMeta), которая позволяет создавать самоизменяющиеся нейронные сети. NeuMeta обучает нейронные неявные функции в качестве гипер-сетей, которые генерируют веса для сетей любого размера, даже для ранее невиданных конфигураций. Для улучшения производительности авторы применяют методы перестановки весовых матриц и добавления шума к входным координатам. Эксперименты показывают, что NeuMeta сохраняет полную производительность даже при 75% сжатии сети.'}, 'en': {'title': '"Shape-Shifting Networks: One Model, Infinite Possibilities"', 'desc': 'The paper introduces Neural Metamorphosis (NeuMeta), a new approach that allows neural networks to adapt their size and architecture without needing separate models. NeuMeta learns a continuous weight manifold, enabling the generation of weights for any network size directly from this manifold. This is achieved by training neural implicit functions as hypernetworks, which take model space coordinates as input to produce corresponding weights. The approach ensures smoothness in the learned manifold, using techniques like weight matrix permutation and input noise, resulting in high performance across various network configurations.'}, 'zh': {'title': '神经变形：自我变形的神经网络新范式', 'desc': '这篇论文介绍了一种新的学习范式，称为神经变形（NeuMeta），旨在构建自我变形的神经网络。与为不同架构或大小单独设计模型不同，NeuMeta直接学习神经网络的连续权重流形。通过训练神经隐函数作为超网络，NeuMeta可以在不重新训练的情况下，从流形中直接采样任何大小网络的权重。实验表明，NeuMeta在图像分类、语义分割和图像生成中表现出色，即使在75%的压缩率下也能保持全尺寸性能。'}}}, {'id': 'https://huggingface.co/papers/2410.09870', 'title': 'ChroKnowledge: Unveiling Chronological Knowledge of Language Models in Multiple Domains', 'url': 'https://huggingface.co/papers/2410.09870', 'abstract': "Large language models (LLMs) have significantly impacted many aspects of our lives. However, assessing and ensuring their chronological knowledge remains challenging. Existing approaches fall short in addressing the accumulative nature of knowledge, often relying on a single time stamp. To overcome this, we introduce ChroKnowBench, a benchmark dataset designed to evaluate chronologically accumulated knowledge across three key aspects: multiple domains, time dependency, temporal state. Our benchmark distinguishes between knowledge that evolves (e.g., scientific discoveries, amended laws) and knowledge that remain constant (e.g., mathematical truths, commonsense facts). Building on this benchmark, we present ChroKnowledge (Chronological Categorization of Knowledge), a novel sampling-based framework for evaluating and updating LLMs' non-parametric chronological knowledge. Our evaluation shows: (1) The ability of eliciting temporal knowledge varies depending on the data format that model was trained on. (2) LLMs partially recall knowledge or show a cut-off at temporal boundaries rather than recalling all aspects of knowledge correctly. Thus, we apply our ChroKnowPrompt, an in-depth prompting to elicit chronological knowledge by traversing step-by-step through the surrounding time spans. We observe that our framework successfully updates the overall knowledge across the entire timeline in both the biomedical domain (+11.9%) and the general domain (+2.8%), demonstrating its effectiveness in refining temporal knowledge. This non-parametric approach also enables knowledge updates not only in open-source models but also in proprietary LLMs, ensuring comprehensive applicability across model types. We perform a comprehensive analysis based on temporal characteristics of ChroKnowPrompt and validate the potential of various models to elicit intrinsic temporal knowledge through our method.", 'score': 7, 'issue_id': 140, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': 'c37b3e5475dd218a', 'authors': ['Yein Park', 'Chanwoong Yoon', 'Jungwoo Park', 'Donghyeon Lee', 'Minbyul Jeong', 'Jaewoo Kang'], 'affiliations': ['AIGEN Sciences', 'Korea University', 'Upstage AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.09870.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#agi', '#interpretability', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '⏳', 'ru': {'title': 'Хронологическая оценка знаний языковых моделей', 'desc': 'Статья представляет ChroKnowBench - новый набор данных для оценки хронологических знаний языковых моделей. Авторы предлагают framework ChroKnowledge для оценки и обновления непараметрических хронологических знаний LLM. Исследование показывает, что способность извлекать временные знания зависит от формата данных, на которых обучалась модель. Применение ChroKnowPrompt позволяет успешно обновлять знания моделей по всей временной шкале.'}, 'en': {'title': '"Mastering Time: Enhancing LLMs with Chronological Knowledge"', 'desc': "This paper introduces ChroKnowBench, a benchmark dataset designed to evaluate how well large language models (LLMs) understand and update chronological knowledge. It distinguishes between knowledge that changes over time, like scientific discoveries, and knowledge that remains constant, like mathematical truths. The authors also present ChroKnowledge, a framework that uses a novel prompting method to improve LLMs' ability to recall and update temporal knowledge. Their approach shows significant improvements in knowledge accuracy across different domains, demonstrating its effectiveness in refining the temporal understanding of LLMs."}, 'zh': {'title': '提升大语言模型的时间知识：ChroKnowBench的创新应用', 'desc': '这篇论文介绍了一种名为ChroKnowBench的基准数据集，用于评估大语言模型在时间上累积的知识。研究发现，现有方法在处理知识的时间依赖性方面存在不足，尤其是在知识演变和不变性之间的区分上。为了解决这个问题，作者提出了一种新的采样框架ChroKnowledge，通过逐步提示来更新和评估模型的时间知识。实验结果表明，这种方法在生物医学和一般领域中有效提高了模型的时间知识更新能力。'}}}, {'id': 'https://huggingface.co/papers/2410.12391', 'title': 'Tracking Universal Features Through Fine-Tuning and Model Merging', 'url': 'https://huggingface.co/papers/2410.12391', 'abstract': 'We study how features emerge, disappear, and persist across models fine-tuned on different domains of text. More specifically, we start from a base one-layer Transformer language model that is trained on a combination of the BabyLM corpus, and a collection of Python code from The Stack. This base model is adapted to two new domains of text: TinyStories, and the Lua programming language, respectively; and then these two models are merged using these two models using spherical linear interpolation. Our exploration aims to provide deeper insights into the stability and transformation of features across typical transfer-learning scenarios using small-scale models and sparse auto-encoders.', 'score': 5, 'issue_id': 139, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '6c1b6c634a13e9b7', 'authors': ['Niels Horn', 'Desmond Elliott'], 'affiliations': ['Department of Computer Science, University of Copenhagen'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12391.jpg', 'data': {'categories': ['#small_models', '#synthetic', '#plp', '#training', '#transfer_learning'], 'emoji': '🔄', 'ru': {'title': 'Эволюция признаков в моделях при переносе обучения', 'desc': 'Исследование посвящено изучению появления, исчезновения и сохранения признаков в моделях, дообученных на разных текстовых доменах. Авторы начинают с базовой однослойной модели Transformer, обученной на корпусе BabyLM и коллекции Python-кода из The Stack. Затем эта модель адаптируется к двум новым доменам: TinyStories и язык программирования Lua. Результаты объединяются с использованием сферической линейной интерполяции для получения более глубокого понимания стабильности и трансформации признаков в сценариях трансферного обучения.'}, 'en': {'title': 'Unveiling Feature Dynamics in Transfer Learning', 'desc': "The paper investigates how features in machine learning models change when they are fine-tuned on different types of text data. It uses a one-layer Transformer model initially trained on a mix of simple English text and Python code. This model is then adapted to new domains, specifically children's stories and Lua programming language, and the resulting models are combined using a technique called spherical linear interpolation. The study aims to understand how features remain stable or transform during transfer learning, using small models and sparse auto-encoders."}, 'zh': {'title': '探索特征在迁移学习中的稳定性与转变', 'desc': '这篇论文研究了在不同文本领域微调的模型中，特征是如何出现、消失和持续存在的。研究从一个基础的单层Transformer语言模型开始，该模型在BabyLM语料库和Python代码集合上进行训练。然后，这个基础模型被适配到两个新的文本领域：TinyStories和Lua编程语言，并通过球面线性插值合并这两个模型。我们的探索旨在通过小规模模型和稀疏自编码器，深入了解特征在典型迁移学习场景中的稳定性和转变。'}}}, {'id': 'https://huggingface.co/papers/2410.12722', 'title': 'WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation', 'url': 'https://huggingface.co/papers/2410.12722', 'abstract': 'Multimodal/vision language models (VLMs) are increasingly being deployed in healthcare settings worldwide, necessitating robust benchmarks to ensure their safety, efficacy, and fairness. Multiple-choice question and answer (QA) datasets derived from national medical examinations have long served as valuable evaluation tools, but existing datasets are largely text-only and available in a limited subset of languages and countries. To address these challenges, we present WorldMedQA-V, an updated multilingual, multimodal benchmarking dataset designed to evaluate VLMs in healthcare. WorldMedQA-V includes 568 labeled multiple-choice QAs paired with 568 medical images from four countries (Brazil, Israel, Japan, and Spain), covering original languages and validated English translations by native clinicians, respectively. Baseline performance for common open- and closed-source models are provided in the local language and English translations, and with and without images provided to the model. The WorldMedQA-V benchmark aims to better match AI systems to the diverse healthcare environments in which they are deployed, fostering more equitable, effective, and representative applications.', 'score': 5, 'issue_id': 138, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '6a533574af0f5068', 'authors': ['João Matos', 'Shan Chen', 'Siena Placino', 'Yingya Li', 'Juan Carlos Climent Pardo', 'Daphna Idan', 'Takeshi Tohyama', 'David Restrepo', 'Luis F. Nakayama', 'Jose M. M. Pascual-Leone', 'Guergana Savova', 'Hugo Aerts', 'Leo A. Celi', 'A. Ian Wong', 'Danielle S. Bitterman', 'Jack Gallifant'], 'affiliations': ['Alcalá University', 'BIDMC', 'Ben-Gurion University of the Negev', 'Boston Childrens Hospital', 'Duke', 'Harvard', 'International University of Health and Welfare', 'MIT', 'Maastricht University', 'Mass General Brigham', 'Oxford', 'St. Lukes Medical Center'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12722.jpg', 'data': {'categories': ['#science', '#benchmark', '#cv', '#multilingual', '#healthcare', '#multimodal', '#ethics', '#dataset', '#open_source', '#machine_translation'], 'emoji': '🏥', 'ru': {'title': 'Мультимодальный многоязычный эталон для оценки ИИ в медицине', 'desc': 'WorldMedQA-V - это новый многоязычный мультимодальный набор данных для оценки визуально-языковых моделей в здравоохранении. Он содержит 568 вопросов с вариантами ответов и медицинскими изображениями из 4 стран на оригинальных языках и в английском переводе. Датасет позволяет оценивать производительность моделей с изображениями и без них, на разных языках. Цель WorldMedQA-V - обеспечить более справедливое и эффективное применение ИИ-систем в различных условиях здравоохранения.'}, 'en': {'title': 'Enhancing Healthcare AI: A Global, Multimodal Approach', 'desc': 'The paper introduces WorldMedQA-V, a new benchmarking dataset for evaluating vision language models (VLMs) in healthcare. This dataset includes 568 multiple-choice questions paired with medical images from four countries, offering both original and English translations. It aims to address the limitations of existing text-only datasets by providing a multilingual and multimodal approach. The goal is to ensure that AI systems are more effective and fair in diverse healthcare settings.'}, 'zh': {'title': 'WorldMedQA-V：多语言多模态医疗评估新基准', 'desc': '这篇论文介绍了一种新的多语言、多模态基准数据集，名为WorldMedQA-V，用于评估在医疗领域的视觉语言模型（VLMs）。该数据集包含来自四个国家的568个多项选择题和相应的医学图像，并提供原始语言和经过验证的英语翻译。研究提供了常见开源和闭源模型在本地语言和英语翻译下的基线性能，并比较了有无图像输入的效果。WorldMedQA-V旨在更好地匹配AI系统与多样化的医疗环境，促进更公平和有效的应用。'}}}, {'id': 'https://huggingface.co/papers/2410.12491', 'title': 'Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse RL', 'url': 'https://huggingface.co/papers/2410.12491', 'abstract': 'Large language models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF) have demonstrated remarkable capabilities, but their underlying reward functions and decision-making processes remain opaque. This paper introduces a novel approach to interpreting LLMs by applying inverse reinforcement learning (IRL) to recover their implicit reward functions. We conduct experiments on toxicity-aligned LLMs of varying sizes, extracting reward models that achieve up to 80.40% accuracy in predicting human preferences. Our analysis reveals key insights into the non-identifiability of reward functions, the relationship between model size and interpretability, and potential pitfalls in the RLHF process. We demonstrate that IRL-derived reward models can be used to fine-tune new LLMs, resulting in comparable or improved performance on toxicity benchmarks. This work provides a new lens for understanding and improving LLM alignment, with implications for the responsible development and deployment of these powerful systems.', 'score': 4, 'issue_id': 138, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '38785426efa90355', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#small_models', '#rl', '#rlhf', '#interpretability', '#ethics', '#training', '#alignment'], 'emoji': '🔍', 'ru': {'title': "Расшифровка 'черного ящика' языковых моделей", 'desc': 'Статья представляет новый подход к интерпретации больших языковых моделей (LLM) с использованием обратного обучения с подкреплением (IRL) для восстановления их неявных функций вознаграждения. Эксперименты проводились на LLM различных размеров, настроенных на снижение токсичности, с извлечением моделей вознаграждения, достигающих точности до 80,40% в предсказании предпочтений человека. Анализ выявил ключевые аспекты неидентифицируемости функций вознаграждения, связи между размером модели и интерпретируемостью, а также потенциальных проблем в процессе RLHF. Исследование показывает, что модели вознаграждения, полученные с помощью IRL, могут использоваться для дополнительного обучения новых LLM, что приводит к сопоставимой или улучшенной производительности в тестах на токсичность.'}, 'en': {'title': 'Decoding the Secrets of Language Models with Inverse Reinforcement Learning', 'desc': 'This paper explores how large language models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF) can be better understood by using inverse reinforcement learning (IRL) to uncover their hidden reward functions. By applying IRL to LLMs of different sizes, the study achieves up to 80.40% accuracy in predicting human preferences, offering insights into the challenges of identifying reward functions and the link between model size and interpretability. The research highlights potential issues in the RLHF process and shows that IRL-derived reward models can enhance the performance of new LLMs on toxicity benchmarks. This approach provides a fresh perspective on aligning LLMs responsibly, aiding in their development and deployment.'}, 'zh': {'title': '逆向强化学习：揭示大型语言模型的隐秘奖励', 'desc': '这篇论文介绍了一种新方法，通过逆向强化学习来解释大型语言模型的隐含奖励函数。研究表明，使用这种方法可以提取出高达80.40%准确率的人类偏好预测模型。分析揭示了奖励函数的不可识别性、模型大小与可解释性之间的关系，以及RLHF过程中的潜在问题。通过这种方法微调的新模型在毒性基准测试中表现出色，提供了理解和改进LLM对齐的新视角。'}}}, {'id': 'https://huggingface.co/papers/2410.12109', 'title': 'OMCAT: Omni Context Aware Transformer', 'url': 'https://huggingface.co/papers/2410.12109', 'abstract': 'Large Language Models (LLMs) have made significant strides in text generation and comprehension, with recent advancements extending into multimodal LLMs that integrate visual and audio inputs. However, these models continue to struggle with fine-grained, cross-modal temporal understanding, particularly when correlating events across audio and video streams. We address these challenges with two key contributions: a new dataset and model, called OCTAV and OMCAT respectively. OCTAV (Omni Context and Temporal Audio Video) is a novel dataset designed to capture event transitions across audio and video. Second, OMCAT (Omni Context Aware Transformer) is a powerful model that leverages RoTE (Rotary Time Embeddings), an innovative extension of RoPE, to enhance temporal grounding and computational efficiency in time-anchored tasks. Through a robust three-stage training pipeline-feature alignment, instruction tuning, and OCTAV-specific training-OMCAT excels in cross-modal temporal understanding. Our model demonstrates state-of-the-art performance on Audio-Visual Question Answering (AVQA) tasks and the OCTAV benchmark, showcasing significant gains in temporal reasoning and cross-modal alignment, as validated through comprehensive experiments and ablation studies. Our dataset and code will be made publicly available. The link to our demo page is https://om-cat.github.io.', 'score': 4, 'issue_id': 135, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': 'c2efc033900c5b2f', 'authors': ['Arushi Goel', 'Karan Sapra', 'Matthieu Le', 'Rafael Valle', 'Andrew Tao', 'Bryan Catanzaro'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12109.jpg', 'data': {'categories': ['#science', '#benchmark', '#cv', '#graphs', '#video', '#optimization', '#multimodal', '#training', '#dataset', '#open_source', '#audio', '#architecture', '#alignment'], 'emoji': '🎬', 'ru': {'title': 'OMCAT: Новый подход к мультимодальному временному пониманию', 'desc': 'Статья представляет новый датасет OCTAV и модель OMCAT для улучшения временного понимания в мультимодальных задачах. OCTAV создан для захвата переходов событий между аудио и видео потоками. OMCAT использует инновационный метод RoTE для улучшения временной привязки и вычислительной эффективности. Модель демонстрирует передовые результаты в задачах аудио-визуального ответа на вопросы и на бенчмарке OCTAV.'}, 'en': {'title': "Mastering Time: OMCAT's Leap in Cross-Modal Understanding", 'desc': 'This paper introduces a new dataset called OCTAV and a model named OMCAT to improve cross-modal temporal understanding in multimodal large language models. OCTAV is designed to capture event transitions across audio and video, while OMCAT uses Rotary Time Embeddings to enhance temporal grounding. The model undergoes a three-stage training process, resulting in state-of-the-art performance on Audio-Visual Question Answering tasks. The research demonstrates significant improvements in temporal reasoning and cross-modal alignment, with the dataset and code being made publicly available.'}, 'zh': {'title': '跨模态时间理解的新突破', 'desc': '大型语言模型（LLMs）在文本生成和理解方面取得了显著进展，最近的进展扩展到整合视觉和音频输入的多模态LLMs。然而，这些模型在细粒度的跨模态时间理解上仍然存在困难，特别是在音频和视频流事件的关联上。我们通过两个关键贡献来解决这些挑战：一个新的数据集OCTAV和一个新的模型OMCAT。OMCAT模型通过使用RoTE（旋转时间嵌入）来增强时间锚定任务的时间基础和计算效率，在跨模态时间理解上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2410.11900', 'title': 'FLARE: Faithful Logic-Aided Reasoning and Exploration', 'url': 'https://huggingface.co/papers/2410.11900', 'abstract': 'Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce Faithful Logic-Aided Reasoning and Exploration (\\ours), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on 7 out of 9 diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that {\\ours} allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.', 'score': 3, 'issue_id': 161, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '9dc3af0706a41348', 'authors': ['Erik Arakelyan', 'Pasquale Minervini', 'Pat Verga', 'Patrick Lewis', 'Isabelle Augenstein'], 'affiliations': ['Cohere', 'Miniml.AI', 'University of Copenhagen', 'University of Edinburgh'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11900.jpg', 'data': {'categories': ['#rag', '#reasoning', '#benchmark', '#plp', '#interpretability', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Логика и LLM объединяются для надежного рассуждения', 'desc': 'Эта статья представляет новый подход к рассуждению и исследованию пространства задач с использованием больших языковых моделей (LLM). Метод называется Faithful Logic-Aided Reasoning and Exploration (FLARE) и сочетает планирование решения с помощью LLM и формализацию запроса в логическом программировании. FLARE позволяет вычислять достоверность процесса рассуждений и анализировать шаги многоэтапного поиска без использования внешних решателей. Авторы сообщают о достижении наилучших результатов на 7 из 9 разнообразных тестов на рассуждение.'}, 'en': {'title': 'Enhancing Reasoning Faithfulness with Logic-Aided Exploration', 'desc': 'This paper presents a new method called Faithful Logic-Aided Reasoning and Exploration (FLARE) that enhances question answering and reasoning using Large Language Models (LLMs). FLARE combines the strengths of LLMs with logic programming to create a more interpretable approach for solving complex problems. It allows for a detailed exploration of the reasoning process by simulating code execution and analyzing each step in a multi-hop search. The results show that FLARE achieves state-of-the-art performance on various reasoning tasks while maintaining high faithfulness in its reasoning outputs.'}, 'zh': {'title': '忠实推理与探索的新方法', 'desc': '现代问答（QA）和推理方法通常依赖大型语言模型（LLM）和提示技术，如思维链（CoT），以期在问题空间中进行更细致的探索和推理。然而，这些方法在生成与模型中间推理链一致的输出时存在困难。我们提出了一种新的可解释方法——忠实逻辑辅助推理与探索（\textit{ours}），通过任务分解来遍历问题空间。该方法利用LLM规划解决方案，并通过逻辑编程代码将查询软形式化为事实和谓词，进行多跳搜索，从而计算推理过程的忠实度。'}}}, {'id': 'https://huggingface.co/papers/2410.09724', 'title': 'Taming Overconfidence in LLMs: Reward Calibration in RLHF', 'url': 'https://huggingface.co/papers/2410.09724', 'abstract': 'Language model calibration refers to the alignment between the confidence of the model and the actual performance of its responses. While previous studies point out the overconfidence phenomenon in Large Language Models (LLMs) and show that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) are overconfident with a more sharpened output probability, in this study, we reveal that RLHF tends to lead models to express verbalized overconfidence in their own responses. We investigate the underlying cause of this overconfidence and demonstrate that reward models used for Proximal Policy Optimization (PPO) exhibit inherent biases towards high-confidence scores regardless of the actual quality of responses. Building upon this insight, we propose two PPO variants: PPO-M: PPO with Calibrated Reward Modeling and PPO-C: PPO with Calibrated Reward Calculation. PPO-M integrates explicit confidence scores in reward model training, which calibrates reward models to better capture the alignment between response quality and verbalized confidence. PPO-C adjusts the reward score during PPO based on the difference between the current reward and the moving average of past rewards. Both PPO-M and PPO-C can be seamlessly integrated into the current PPO pipeline and do not require additional golden labels. We evaluate our methods on both Llama3-8B and Mistral-7B across six diverse datasets including multiple-choice and open-ended generation. Experiment results demonstrate that both of our methods can reduce calibration error and maintain performance comparable to standard PPO. We further show that they do not compromise model capabilities in open-ended conversation settings.', 'score': 2, 'issue_id': 141, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': 'a649dfe2bf936909', 'authors': ['Jixuan Leng', 'Chengsong Huang', 'Banghua Zhu', 'Jiaxin Huang'], 'affiliations': ['Carnegie Mellon University', 'UC Berkeley', 'Washington University in St. Louis'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.09724.jpg', 'data': {'categories': ['#small_models', '#rl', '#rlhf', '#benchmark', '#optimization', '#interpretability', '#training', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'Калибровка уверенности: новый подход к RLHF', 'desc': 'Статья исследует проблему переоценки уверенности в больших языковых моделях (LLM), обученных с помощью обучения с подкреплением на основе обратной связи от человека (RLHF). Авторы выявляют, что модели вознаграждения, используемые в процессе Проксимальной оптимизации политики (PPO), имеют склонность к высоким оценкам уверенности независимо от качества ответов. Предлагаются два варианта PPO: PPO-M с калиброванным моделированием вознаграждения и PPO-C с калиброванным расчетом вознаграждения. Эксперименты показывают, что оба метода снижают ошибку калибровки, сохраняя производительность на уровне стандартного PPO.'}, 'en': {'title': 'Balancing Confidence: Calibrating Language Models for Better Alignment', 'desc': 'This paper explores the issue of overconfidence in Large Language Models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF). It identifies that reward models used in Proximal Policy Optimization (PPO) are biased towards high-confidence scores, leading to verbalized overconfidence. To address this, the authors propose two new PPO variants: PPO-M, which incorporates calibrated reward modeling, and PPO-C, which adjusts reward scores based on past performance. These methods effectively reduce calibration error without sacrificing model performance, as demonstrated in experiments across various datasets.'}, 'zh': {'title': '校准语言模型：减少过度自信，提升模型表现', 'desc': '这篇论文研究了大型语言模型在使用人类反馈强化学习训练后表现出的过度自信现象。研究发现，奖励模型在近端策略优化中对高置信度评分存在固有偏见。为此，作者提出了两种新的近端策略优化变体：PPO-M和PPO-C，分别通过校准奖励模型和校准奖励计算来解决这个问题。实验结果表明，这两种方法能够减少校准误差，同时保持模型性能。'}}}, {'id': 'https://huggingface.co/papers/2410.11843', 'title': 'From Commands to Prompts: LLM-based Semantic File System for AIOS', 'url': 'https://huggingface.co/papers/2410.11843', 'abstract': 'Large language models (LLMs) have demonstrated significant potential in the development of intelligent applications and systems such as LLM-based agents and agent operating systems (AIOS). However, when these applications and systems interact with the underlying file system, the file system still remains the traditional paradigm: reliant on manual navigation through precise commands. This paradigm poses a bottleneck to the usability of these systems as users are required to navigate complex folder hierarchies and remember cryptic file names. To address this limitation, we propose an LLM-based semantic file system ( LSFS ) for prompt-driven file management. Unlike conventional approaches, LSFS incorporates LLMs to enable users or agents to interact with files through natural language prompts, facilitating semantic file management. At the macro-level, we develop a comprehensive API set to achieve semantic file management functionalities, such as semantic file retrieval, file update monitoring and summarization, and semantic file rollback). At the micro-level, we store files by constructing semantic indexes for them, design and implement syscalls of different semantic operations (e.g., CRUD, group by, join) powered by vector database. Our experiments show that LSFS offers significant improvements over traditional file systems in terms of user convenience, the diversity of supported functions, and the accuracy and efficiency of file operations. Additionally, with the integration of LLM, our system enables more intelligent file management tasks, such as content summarization and version comparison, further enhancing its capabilities.', 'score': 1, 'issue_id': 161, 'pub_date': '2024-09-23', 'pub_date_card': {'ru': '23 сентября', 'en': 'September 23', 'zh': '9月23日'}, 'hash': '68eb78dc881f88ae', 'authors': ['Zeru Shi', 'Kai Mei', 'Mingyu Jin', 'Yongye Su', 'Chaoji Zuo', 'Wenyue Hua', 'Wujiang Xu', 'Yujie Ren', 'Zirui Liu', 'Mengnan Du', 'Dong Deng', 'Yongfeng Zhang'], 'affiliations': ['Dalian University of Technology', 'EPFL', 'New Jersey Institute of Technology', 'Purdue University', 'Rutgers University', 'University of Minnesota'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11843.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#data', '#agents', '#architecture', '#alignment'], 'emoji': '🗂️', 'ru': {'title': 'Революция в управлении файлами: семантическая файловая система на основе ИИ', 'desc': 'Статья представляет семантическую файловую систему на основе больших языковых моделей (LSFS). Эта система позволяет пользователям и агентам взаимодействовать с файлами с помощью естественного языка, облегчая семантическое управление файлами. LSFS включает комплексный набор API для семантических операций с файлами и использует векторные базы данных для хранения и индексации. Эксперименты показывают значительные улучшения в удобстве использования, разнообразии функций и эффективности файловых операций по сравнению с традиционными файловыми системами.'}, 'en': {'title': 'Revolutionizing File Management with Natural Language', 'desc': 'This paper introduces a new system called the LLM-based Semantic File System (LSFS) that enhances file management using large language models (LLMs). Unlike traditional file systems that require users to remember complex commands and navigate through folders, LSFS allows users to interact with files using natural language prompts. The system includes a comprehensive API for various semantic file management tasks, such as retrieval and monitoring, and utilizes semantic indexing and vector databases for efficient operations. Experiments show that LSFS significantly improves user convenience and the accuracy of file operations compared to conventional systems.'}, 'zh': {'title': '语义文件管理，智能化未来', 'desc': '大型语言模型（LLM）在智能应用和系统的开发中展现了巨大的潜力，但传统的文件系统仍依赖于手动导航和精确命令，这限制了用户的使用体验。为了解决这个问题，我们提出了一种基于LLM的语义文件系统（LSFS），允许用户通过自然语言提示与文件进行交互。LSFS通过构建语义索引和实现不同的语义操作系统调用，提供了语义文件检索、更新监控和摘要等功能。实验表明，LSFS在用户便利性、功能多样性和文件操作的准确性与效率方面显著优于传统文件系统。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (2)', '#agi (3)', '#alignment (5)', '#architecture (13)', '#audio (1)', '#benchmark (15)', '#cv (8)', '#data (4)', '#dataset (5)', '#diffusion (3)', '#ethics (4)', '#games (2)', '#graphs (5)', '#hallucinations (1)', '#healthcare (1)', '#inference (2)', '#interpretability (8)', '#leakage (1)', '#long_context (1)', '#low_resource', '#machine_translation (1)', '#math (1)', '#multilingual (2)', '#multimodal (5)', '#open_source (11)', '#optimization (11)', '#plp (3)', '#rag (2)', '#reasoning (6)', '#rl (3)', '#rlhf (2)', '#robotics (1)', '#science (4)', '#security', '#small_models (4)', '#story_generation', '#survey', '#synthetic (3)', '#training (16)', '#transfer_learning (3)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-10-17 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-17 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-17 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    