
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 23 papers. October 17.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">23 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-16.html">â¬…ï¸ <span id="prev-date">16.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-18.html">â¡ï¸ <span id="next-date">18.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 17', 'zh': '10æœˆ17æ—¥'};
        let feedDateNext = {'ru': '18.10', 'en': '10/18', 'zh': '10æœˆ18æ—¥'};
        let feedDatePrev = {'ru': '16.10', 'en': '10/16', 'zh': '10æœˆ16æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.11623', 'title': 'VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI', 'url': 'https://huggingface.co/papers/2410.11623', 'abstract': 'Recent advancements in Multi-modal Large Language Models (MLLMs) have opened new avenues for applications in Embodied AI. Building on previous work, EgoThink, we introduce VidEgoThink, a comprehensive benchmark for evaluating egocentric video understanding capabilities. To bridge the gap between MLLMs and low-level control in Embodied AI, we design four key interrelated tasks: video question-answering, hierarchy planning, visual grounding and reward modeling. To minimize manual annotation costs, we develop an automatic data generation pipeline based on the Ego4D dataset, leveraging the prior knowledge and multimodal capabilities of GPT-4o. Three human annotators then filter the generated data to ensure diversity and quality, resulting in the VidEgoThink benchmark. We conduct extensive experiments with three types of models: API-based MLLMs, open-source image-based MLLMs, and open-source video-based MLLMs. Experimental results indicate that all MLLMs, including GPT-4o, perform poorly across all tasks related to egocentric video understanding. These findings suggest that foundation models still require significant advancements to be effectively applied to first-person scenarios in Embodied AI. In conclusion, VidEgoThink reflects a research trend towards employing MLLMs for egocentric vision, akin to human capabilities, enabling active observation and interaction in the complex real-world environments.', 'score': 46, 'issue_id': 137, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': '67310ef96a11f09c', 'authors': ['Sijie Cheng', 'Kechen Fang', 'Yangyang Yu', 'Sicheng Zhou', 'Bohao Li', 'Ye Tian', 'Tingguang Li', 'Lei Han', 'Yang Liu'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University', 'Department of Mechanical and Industrial Engineering, University of Toronto', 'Institute for AI Industry Research (AIR), Tsinghua University', 'School of Data Science, The Chinese University of HongKong', 'Tencent Robotics X', 'Zhili College, Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11623.jpg', 'data': {'categories': ['#science', '#synthetic', '#benchmark', '#graphs', '#video', '#multimodal', '#data', '#robotics', '#open_source', '#games'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'VidEgoThink: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VidEgoThink - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Ğ±Ñ‹Ğ»Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ego4D Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GPT-4o Ğ¸ Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ MLLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4o, Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Pioneering Egocentric Video Understanding with VidEgoThink', 'desc': 'The paper introduces VidEgoThink, a benchmark designed to evaluate how well multi-modal large language models (MLLMs) understand egocentric videos, which are videos captured from a first-person perspective. It focuses on four tasks: video question-answering, hierarchy planning, visual grounding, and reward modeling, to connect MLLMs with low-level control in Embodied AI. An automatic data generation pipeline is used to create diverse and high-quality data, which is then filtered by human annotators. The study finds that current MLLMs, including GPT-4o, struggle with these tasks, indicating that more advancements are needed for these models to effectively handle first-person video understanding in real-world scenarios.'}, 'zh': {'title': 'VidEgoThinkï¼šè¿ˆå‘å…·èº«AIçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†VidEgoThinkï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç†è§£èƒ½åŠ›çš„åŸºå‡†ã€‚ç ”ç©¶è®¾è®¡äº†å››ä¸ªå…³é”®ä»»åŠ¡ï¼šè§†é¢‘é—®ç­”ã€å±‚æ¬¡è§„åˆ’ã€è§†è§‰å®šä½å’Œå¥–åŠ±å»ºæ¨¡ï¼Œä»¥è¿æ¥å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸å…·èº«äººå·¥æ™ºèƒ½çš„ä½çº§æ§åˆ¶ã€‚é€šè¿‡è‡ªåŠ¨æ•°æ®ç”Ÿæˆç®¡é“å’Œäººå·¥ç­›é€‰ï¼Œç¡®ä¿æ•°æ®çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œè¡¨æ˜åŸºç¡€æ¨¡å‹åœ¨å…·èº«äººå·¥æ™ºèƒ½ä¸­çš„åº”ç”¨ä»éœ€é‡å¤§è¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.12381', 'title': 'HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks', 'url': 'https://huggingface.co/papers/2410.12381', 'abstract': "Coding tasks have been valuable for evaluating Large Language Models (LLMs), as they demand the comprehension of high-level instructions, complex reasoning, and the implementation of functional programs -- core capabilities for advancing Artificial General Intelligence. Despite the progress in Large Multimodal Models (LMMs), which extend LLMs with visual perception and understanding capabilities, there remains a notable lack of coding benchmarks that rigorously assess these models, particularly in tasks that emphasize visual reasoning. To address this gap, we introduce HumanEval-V, a novel and lightweight benchmark specifically designed to evaluate LMMs' visual understanding and reasoning capabilities through code generation. HumanEval-V includes 108 carefully crafted, entry-level Python coding tasks derived from platforms like CodeForces and Stack Overflow. Each task is adapted by modifying the context and algorithmic patterns of the original problems, with visual elements redrawn to ensure distinction from the source, preventing potential data leakage. LMMs are required to complete the code solution based on the provided visual context and a predefined Python function signature outlining the task requirements. Every task is equipped with meticulously handcrafted test cases to ensure a thorough and reliable evaluation of model-generated solutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering significant challenges. Proprietary models like GPT-4o achieve only 13% pass@1 and 36.4% pass@10, while open-weight models with 70B parameters score below 4% pass@1. Ablation studies further reveal the limitations of current LMMs in vision reasoning and coding capabilities. These results underscore key areas for future research to enhance LMMs' capabilities. We have open-sourced our code and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark.", 'score': 42, 'issue_id': 135, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 16', 'zh': '10æœˆ16æ—¥'}, 'hash': '7693dd92c7f5b4e2', 'authors': ['Fengji Zhang', 'Linquan Wu', 'Huiyu Bai', 'Guancheng Lin', 'Xiao Li', 'Xiao Yu', 'Yue Wang', 'Bei Chen', 'Jacky Keung'], 'affiliations': ['City University of Hong Kong', 'Rhymes AI', 'Tsinghua University', 'Wuhan University', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12381.jpg', 'data': {'categories': ['#reasoning', '#leakage', '#benchmark', '#cv', '#graphs', '#agi', '#plp', '#multimodal', '#open_source', '#games'], 'emoji': 'ğŸ‘ï¸\u200dğŸ—¨ï¸', 'ru': {'title': 'HumanEval-V: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ AI', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº HumanEval-V Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 108 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Python, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ 19 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LMM, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… LMM Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ±Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing LMMs: Bridging the Visual Reasoning Gap in Code Generation', 'desc': "The paper introduces HumanEval-V, a new benchmark designed to evaluate Large Multimodal Models (LMMs) on their ability to understand and reason with visual information through coding tasks. It highlights the gap in existing benchmarks that fail to rigorously test LMMs' visual reasoning capabilities, especially in the context of code generation. The benchmark consists of 108 Python coding tasks, each adapted with unique visual elements to prevent data leakage, and includes handcrafted test cases for thorough evaluation. Results from testing 19 state-of-the-art LMMs reveal significant challenges in their visual reasoning and coding abilities, pointing to areas for future research."}, 'zh': {'title': 'æå‡LMMsçš„è§†è§‰æ¨ç†ä¸ç¼–ç¨‹èƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•å·¥å…·HumanEval-Vï¼Œç”¨äºè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚HumanEval-VåŒ…å«108ä¸ªç»è¿‡ç²¾å¿ƒè®¾è®¡çš„Pythonç¼–ç¨‹ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡ç»“åˆäº†è§†è§‰å…ƒç´ ï¼Œä»¥æµ‹è¯•æ¨¡å‹åœ¨è§†è§‰æ¨ç†å’Œä»£ç ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„LMMsåœ¨è¿™äº›ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨è§†è§‰æ¨ç†å’Œç¼–ç¨‹èƒ½åŠ›ä¸Šçš„å±€é™æ€§ã€‚è®ºæ–‡å¼ºè°ƒäº†æœªæ¥ç ”ç©¶çš„å…³é”®é¢†åŸŸï¼Œä»¥æé«˜LMMsçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.12787', 'title': 'The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio', 'url': 'https://huggingface.co/papers/2410.12787', 'abstract': 'Recent advancements in large multimodal models (LMMs) have significantly enhanced performance across diverse tasks, with ongoing efforts to further integrate additional modalities such as video and audio. However, most existing LMMs remain vulnerable to hallucinations, the discrepancy between the factual multimodal input and the generated textual output, which has limited their applicability in various real-world scenarios. This paper presents the first systematic investigation of hallucinations in LMMs involving the three most common modalities: language, visual, and audio. Our study reveals two key contributors to hallucinations: overreliance on unimodal priors and spurious inter-modality correlations. To address these challenges, we introduce the benchmark The Curse of Multi-Modalities (CMM), which comprehensively evaluates hallucinations in LMMs, providing a detailed analysis of their underlying issues. Our findings highlight key vulnerabilities, including imbalances in modality integration and biases from training data, underscoring the need for balanced cross-modal learning and enhanced hallucination mitigation strategies. Based on our observations and findings, we suggest potential research directions that could enhance the reliability of LMMs.', 'score': 30, 'issue_id': 135, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 16', 'zh': '10æœˆ16æ—¥'}, 'hash': 'e40b5fd56a795812', 'authors': ['Sicong Leng', 'Yun Xing', 'Zesen Cheng', 'Yang Zhou', 'Hang Zhang', 'Xin Li', 'Deli Zhao', 'Shijian Lu', 'Chunyan Miao', 'Lidong Bing'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab, Hangzhou, China', 'IHPC A*STAR, Singapore', 'Nanyang Technological University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12787.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#multimodal', '#interpretability', '#ethics', '#training'], 'emoji': 'ğŸŒˆ', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LMM), Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞµ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: ÑĞ·Ñ‹Ğº, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹: Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¾Ğ¿Ğ¾Ñ€Ğ° Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ¸ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼ĞµĞ¶Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² LMM Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº The Curse of Multi-Modalities (CMM). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¿Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Taming Hallucinations in Multimodal Models', 'desc': "This paper explores the issue of hallucinations in large multimodal models (LMMs), which occur when the model's output doesn't match the input from different modalities like language, visuals, and audio. The study identifies two main causes of these hallucinations: relying too much on single-modality information and incorrect connections between different modalities. To tackle these problems, the authors introduce a new benchmark called The Curse of Multi-Modalities (CMM) to evaluate and analyze these hallucinations. The research suggests that improving the balance in how different modalities are integrated and addressing biases in training data can help reduce hallucinations in LMMs."}, 'zh': {'title': 'ç ´è§£å¤šæ¨¡æ€æ¨¡å‹çš„å¹»è§‰æŒ‘æˆ˜', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å¤„ç†è¯­è¨€ã€è§†è§‰å’ŒéŸ³é¢‘ä¸‰ç§å¸¸è§æ¨¡æ€æ—¶å‡ºç°çš„å¹»è§‰é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œå¹»è§‰ä¸»è¦ç”±å¯¹å•ä¸€æ¨¡æ€çš„è¿‡åº¦ä¾èµ–å’Œæ¨¡æ€é—´çš„è™šå‡ç›¸å…³æ€§å¼•èµ·ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œå¤šæ¨¡æ€è¯…å’’â€çš„åŸºå‡†ï¼Œç”¨äºå…¨é¢è¯„ä¼°LMMsä¸­çš„å¹»è§‰ç°è±¡ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†æ¨¡æ€æ•´åˆçš„ä¸å¹³è¡¡å’Œè®­ç»ƒæ•°æ®çš„åå·®ï¼Œå»ºè®®æœªæ¥çš„ç ”ç©¶åº”å…³æ³¨è·¨æ¨¡æ€å­¦ä¹ çš„å¹³è¡¡å’Œå¹»è§‰çš„å‡è½»ç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.12628', 'title': 'DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception', 'url': 'https://huggingface.co/papers/2410.12628', 'abstract': 'Document Layout Analysis is crucial for real-world document understanding systems, but it encounters a challenging trade-off between speed and accuracy: multimodal methods leveraging both text and visual features achieve higher accuracy but suffer from significant latency, whereas unimodal methods relying solely on visual features offer faster processing speeds at the expense of accuracy. To address this dilemma, we introduce DocLayout-YOLO, a novel approach that enhances accuracy while maintaining speed advantages through document-specific optimizations in both pre-training and model design. For robust document pre-training, we introduce the Mesh-candidate BestFit algorithm, which frames document synthesis as a two-dimensional bin packing problem, generating the large-scale, diverse DocSynth-300K dataset. Pre-training on the resulting DocSynth-300K dataset significantly improves fine-tuning performance across various document types. In terms of model optimization, we propose a Global-to-Local Controllable Receptive Module that is capable of better handling multi-scale variations of document elements. Furthermore, to validate performance across different document types, we introduce a complex and challenging benchmark named DocStructBench. Extensive experiments on downstream datasets demonstrate that DocLayout-YOLO excels in both speed and accuracy. Code, data, and models are available at https://github.com/opendatalab/DocLayout-YOLO.', 'score': 27, 'issue_id': 134, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 16', 'zh': '10æœˆ16æ—¥'}, 'hash': '0c81afcfe57d6c2f', 'authors': ['Zhiyuan Zhao', 'Hengrui Kang', 'Bin Wang', 'Conghui He'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12628.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#cv', '#optimization', '#data', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': 'ğŸ“„', 'ru': {'title': 'DocLayout-YOLO: Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ°ĞºĞµÑ‚Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'DocLayout-YOLO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¼Ğ°ĞºĞµÑ‚Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Mesh-candidate BestFit Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DocSynth-300K, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Global-to-Local Controllable Receptive Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº DocStructBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'DocLayout-YOLO: Fast and Accurate Document Layout Analysis', 'desc': 'The paper introduces DocLayout-YOLO, a new method for document layout analysis that balances speed and accuracy by using document-specific optimizations. It employs a unique pre-training strategy with the Mesh-candidate BestFit algorithm to create a diverse dataset called DocSynth-300K, enhancing model performance. The model also features a Global-to-Local Controllable Receptive Module to handle different document element scales effectively. Extensive testing shows that DocLayout-YOLO performs well in both speed and accuracy across various document types.'}, 'zh': {'title': 'DocLayout-YOLOï¼šé€Ÿåº¦ä¸å‡†ç¡®æ€§çš„å®Œç¾ç»“åˆ', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–‡æ¡£å¸ƒå±€åˆ†ææ–¹æ³•ï¼Œç§°ä¸ºDocLayout-YOLOï¼Œå®ƒåœ¨æé«˜å‡†ç¡®æ€§çš„åŒæ—¶ä¿æŒäº†é€Ÿåº¦ä¼˜åŠ¿ã€‚é€šè¿‡å¼•å…¥Mesh-candidate BestFitç®—æ³•ï¼Œä½œè€…åˆ›å»ºäº†ä¸€ä¸ªåä¸ºDocSynth-300Kçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œç”¨äºå¢å¼ºæ–‡æ¡£çš„é¢„è®­ç»ƒæ•ˆæœã€‚ä¸ºäº†ä¼˜åŒ–æ¨¡å‹ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§å…¨å±€åˆ°å±€éƒ¨çš„å¯æ§æ„Ÿå—æ¨¡å—ï¼Œä»¥æ›´å¥½åœ°å¤„ç†æ–‡æ¡£å…ƒç´ çš„å¤šå°ºåº¦å˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDocLayout-YOLOåœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.12409', 'title': 'Revealing the Barriers of Language Agents in Planning', 'url': 'https://huggingface.co/papers/2410.12409', 'abstract': 'Autonomous planning has been an ongoing pursuit since the inception of artificial intelligence. Based on curated problem solvers, early planning agents could deliver precise solutions for specific tasks but lacked generalization. The emergence of large language models (LLMs) and their powerful reasoning capabilities has reignited interest in autonomous planning by automatically generating reasonable solutions for given tasks. However, prior research and our experiments show that current language agents still lack human-level planning abilities. Even the state-of-the-art reasoning model, OpenAI o1, achieves only 15.6% on one of the complex real-world planning benchmarks. This highlights a critical question: What hinders language agents from achieving human-level planning? Although existing studies have highlighted weak performance in agent planning, the deeper underlying issues and the mechanisms and limitations of the strategies proposed to address them remain insufficiently understood. In this work, we apply the feature attribution study and identify two key factors that hinder agent planning: the limited role of constraints and the diminishing influence of questions. We also find that although current strategies help mitigate these challenges, they do not fully resolve them, indicating that agents still have a long way to go before reaching human-level intelligence.', 'score': 23, 'issue_id': 134, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 16', 'zh': '10æœˆ16æ—¥'}, 'hash': 'faf0a6d62d983811', 'authors': ['Jian Xie', 'Kexun Zhang', 'Jiangjie Chen', 'Siyu Yuan', 'Kai Zhang', 'Yikai Zhang', 'Lei Li', 'Yanghua Xiao'], 'affiliations': ['ByteDance Inc.', 'Carnegie Mellon University', 'Fudan University', 'The Ohio State University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12409.jpg', 'data': {'categories': ['#reasoning', '#rl', '#benchmark', '#agi', '#interpretability', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ñ‹: Ğ¿ÑƒÑ‚ÑŒ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°, Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‰ĞµĞµÑÑ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº OpenAI o1, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 15.6% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ´Ğ°Ğ»ĞµĞºĞ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Bridging the Gap: Enhancing Language Models for Human-Level Planning', 'desc': 'This paper explores the limitations of current language models in achieving human-level planning abilities. It identifies two main factors hindering progress: the limited role of constraints and the diminishing influence of questions. Despite existing strategies to address these issues, they are not fully effective, suggesting that language agents need further development. The study uses feature attribution to analyze these challenges and highlights the gap between current capabilities and human-level intelligence.'}, 'zh': {'title': 'æ­ç¤ºè¯­è¨€æ¨¡å‹è§„åˆ’èƒ½åŠ›çš„ç“¶é¢ˆ', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è‡ªä¸»è§„åˆ’åœ¨äººå·¥æ™ºèƒ½ä¸­çš„å‘å±•å†ç¨‹ï¼ŒæŒ‡å‡ºå½“å‰è¯­è¨€æ¨¡å‹åœ¨è§„åˆ’èƒ½åŠ›ä¸Šä»ç„¶ä¸åŠäººç±»æ°´å¹³ã€‚ç ”ç©¶å‘ç°ï¼Œé™åˆ¶æ¡ä»¶çš„æœ‰é™ä½œç”¨å’Œé—®é¢˜å½±å“åŠ›çš„å‡å¼±æ˜¯é˜»ç¢è¯­è¨€ä»£ç†è§„åˆ’èƒ½åŠ›çš„ä¸¤ä¸ªå…³é”®å› ç´ ã€‚å°½ç®¡ç°æœ‰ç­–ç•¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£äº†è¿™äº›é—®é¢˜ï¼Œä½†å¹¶æœªå½»åº•è§£å†³ï¼Œè¡¨æ˜ä»£ç†è·ç¦»è¾¾åˆ°äººç±»æ™ºèƒ½è¿˜æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ã€‚é€šè¿‡ç‰¹å¾å½’å› ç ”ç©¶ï¼Œä½œè€…æ­ç¤ºäº†è¿™äº›æ·±å±‚æ¬¡é—®é¢˜çš„æœºåˆ¶å’Œå±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.12613', 'title': 'Exploring Model Kinship for Merging Large Language Models', 'url': 'https://huggingface.co/papers/2410.12613', 'abstract': 'Model merging has become one of the key technologies for enhancing the capabilities and efficiency of Large Language Models (LLMs). However, our understanding of the expected performance gains and principles when merging any two models remains limited. In this work, we introduce model kinship, the degree of similarity or relatedness between LLMs, analogous to biological evolution. With comprehensive empirical analysis, we find that there is a certain relationship between model kinship and the performance gains after model merging, which can help guide our selection of candidate models. Inspired by this, we propose a new model merging strategy: Top-k Greedy Merging with Model Kinship, which can yield better performance on benchmark datasets. Specifically, we discover that using model kinship as a criterion can assist us in continuously performing model merging, alleviating the degradation (local optima) in model evolution, whereas model kinship can serve as a guide to escape these traps. Code is available at https://github.com/zjunlp/ModelKinship.', 'score': 19, 'issue_id': 134, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 16', 'zh': '10æœˆ16æ—¥'}, 'hash': '5154e5ff2c4f7709', 'authors': ['Yedi Hu', 'Yunzhi Yao', 'Ningyu Zhang', 'Shumin Deng', 'Huajun Chen'], 'affiliations': ['National University of Singapore, NUS-NCS Joint Lab, Singapore', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12613.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ LLM', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ 'Ñ€Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸, Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ ĞµĞ³Ğ¾ ÑĞ²ÑĞ·ÑŒ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾ÑĞ»Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ 'Top-k Ğ–Ğ°Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¡Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ñ Ğ£Ñ‡ĞµÑ‚Ğ¾Ğ¼ Ğ Ğ¾Ğ´ÑÑ‚Ğ²Ğ° ĞœĞ¾Ğ´ĞµĞ»ĞµĞ¹', ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, ĞºĞ°Ğº ÑƒÑ‡ĞµÑ‚ Ñ€Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼ÑƒĞ¼Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."}, 'en': {'title': '"Model Kinship: Guiding the Evolution of Language Models"', 'desc': "This paper explores the concept of model merging to improve the performance of Large Language Models (LLMs). It introduces 'model kinship,' a measure of similarity between models, which can predict the effectiveness of merging two models. The authors propose a new strategy called Top-k Greedy Merging with Model Kinship, which uses this similarity measure to enhance model performance on benchmark datasets. This approach helps avoid local optima, improving the overall efficiency and capability of LLMs."}, 'zh': {'title': 'æ¨¡å‹äº²ç¼˜å…³ç³»ï¼šæå‡å¤§è¯­è¨€æ¨¡å‹åˆå¹¶æ€§èƒ½çš„æ–°ç­–ç•¥', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åˆå¹¶çš„æ€§èƒ½æå‡å’ŒåŸåˆ™ã€‚ç ”ç©¶å¼•å…¥äº†æ¨¡å‹äº²ç¼˜å…³ç³»çš„æ¦‚å¿µï¼Œç±»ä¼¼äºç”Ÿç‰©è¿›åŒ–ï¼Œæ¥åˆ†ææ¨¡å‹åˆå¹¶åçš„æ€§èƒ½æå‡ã€‚é€šè¿‡å®è¯åˆ†æï¼Œå‘ç°æ¨¡å‹äº²ç¼˜å…³ç³»ä¸åˆå¹¶åçš„æ€§èƒ½æå‡æœ‰ä¸€å®šå…³ç³»ï¼Œè¿™å¯ä»¥æŒ‡å¯¼æˆ‘ä»¬é€‰æ‹©åˆé€‚çš„å€™é€‰æ¨¡å‹ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹åˆå¹¶ç­–ç•¥ï¼šåŸºäºæ¨¡å‹äº²ç¼˜å…³ç³»çš„Top-kè´ªå¿ƒåˆå¹¶ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.11081', 'title': 'Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models', 'url': 'https://huggingface.co/papers/2410.11081', 'abstract': 'Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512x512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64x64, and 1.88 on ImageNet 512x512, narrowing the gap in FID scores with the best existing diffusion models to within 10%.', 'score': 18, 'issue_id': 138, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': '0c5455c7c793e07e', 'authors': ['Cheng Lu', 'Yang Song'], 'affiliations': ['OpenAI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11081.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#architecture'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ (CMs) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ, Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 1.5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° ImageNet 512x512, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ FID, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ñ… Ğº Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Consistency Models: Faster, Better, and Scalable', 'desc': 'The paper introduces a new approach to improve consistency models (CMs), which are a type of generative model used for creating data like images. Traditional CMs often face issues due to discretized timesteps, leading to errors and extra parameters. The authors propose a unified framework to address these issues by enhancing the diffusion process, network design, and training methods. Their improved model can handle large-scale data and achieves competitive performance with fewer sampling steps, as shown by their impressive FID scores on various datasets.'}, 'zh': {'title': 'çªç ´ä¸€è‡´æ€§æ¨¡å‹çš„è®­ç»ƒç“¶é¢ˆ', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è¿ç»­æ—¶é—´ä¸€è‡´æ€§æ¨¡å‹ï¼ˆCMsï¼‰ï¼Œç”¨äºç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿé‡‡æ ·ã€‚ä¼ ç»Ÿçš„CMsä½¿ç”¨ç¦»æ•£æ—¶é—´æ­¥é•¿ï¼Œå®¹æ˜“å¼•å…¥è¯¯å·®å’Œä¸ç¨³å®šæ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§ç®€åŒ–çš„ç†è®ºæ¡†æ¶ï¼Œç»Ÿä¸€äº†æ‰©æ•£æ¨¡å‹å’ŒCMsçš„å‚æ•°åŒ–ï¼Œè§£å†³äº†è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚é€šè¿‡æ”¹è¿›æ‰©æ•£è¿‡ç¨‹çš„å‚æ•°åŒ–ã€ç½‘ç»œæ¶æ„å’Œè®­ç»ƒç›®æ ‡ï¼ŒæˆåŠŸåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè®­ç»ƒäº†CMsï¼Œå¹¶å–å¾—äº†ä¼˜å¼‚çš„ç”Ÿæˆæ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.10672', 'title': 'Large Language Model Evaluation via Matrix Nuclear-Norm', 'url': 'https://huggingface.co/papers/2410.10672', 'abstract': "As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due to their \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD). To mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only serves as a metric to quantify the data compression proficiency of LLM but also provides a convex approximation of matrix rank to capture both predictive discriminability and diversity. By employing the \\( L_{1,2}-norm \\) to further approximate the nuclear norm, we can effectively assess the model's information compression capabilities. This approach reduces the time complexity to \\( O(n^2) \\) and eliminates the need for SVD computation. Consequently, the Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This performance gap becomes more pronounced with larger models, as validated in tests with other models like Pythia. Additionally, evaluations on benchmarks and model responses confirm that our proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLMs' performance, striking a balance between accuracy and computational efficiency. The code is available at https://github.com/MLGroupJLU/MatrixNuclearNorm.", 'score': 18, 'issue_id': 137, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': 'dbc484f05cdfefb9', 'authors': ['Yahan Li', 'Tingyu Xia', 'Yi Chang', 'Yuan Wu'], 'affiliations': ['International Center of Future Science, Jilin University', 'Key Laboratory of Symbolic Computation and Knowledge Engineering, Jilin University', 'School of Artificial Intelligence, Jilin University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.10672.jpg', 'data': {'categories': ['#small_models', '#benchmark', '#optimization', '#math', '#training', '#open_source'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° LLM: Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ĞµĞµ, Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) - Matrix Nuclear-Norm. Ğ­Ñ‚Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ‚ÑŒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¾Ğ¹ Matrix Entropy, Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ O(n^2) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ O(n^3). Ğ¢ĞµÑÑ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Matrix Nuclear-Norm Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² 8-24 Ñ€Ğ°Ğ·Ğ° Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸.'}, 'en': {'title': 'Speeding Up LLM Evaluation with Matrix Nuclear-Norm', 'desc': 'The paper introduces the Matrix Nuclear-Norm as a new metric for evaluating large language models (LLMs) in terms of their ability to compress information and reduce redundancy. This metric offers a more computationally efficient alternative to traditional methods like Matrix Entropy, reducing time complexity from O(n^3) to O(n^2) by eliminating the need for Singular Value Decomposition (SVD). By using the L_{1,2}-norm to approximate the nuclear norm, the approach maintains accuracy while significantly speeding up the evaluation process, especially for large models like CEREBRAS-GPT and Pythia. The Matrix Nuclear-Norm is validated as a reliable and scalable tool for assessing LLM performance, balancing accuracy with computational efficiency.'}, 'zh': {'title': 'çŸ©é˜µæ ¸èŒƒæ•°ï¼šé«˜æ•ˆè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°å·¥å…·', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œè¯„ä¼°å…¶ä¿¡æ¯å‹ç¼©èƒ½åŠ›å’Œå‡å°‘å†—ä½™çš„æœ‰æ•ˆæŒ‡æ ‡å˜å¾—è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„çŸ©é˜µç†µæŒ‡æ ‡è™½ç„¶æœ‰ç”¨ï¼Œä½†ç”±äºå¥‡å¼‚å€¼åˆ†è§£çš„å¤æ‚æ€§ï¼Œå¯¹å¤§è§„æ¨¡æ¨¡å‹çš„è®¡ç®—è´Ÿæ‹…è¾ƒé‡ã€‚æˆ‘ä»¬æå‡ºäº†çŸ©é˜µæ ¸èŒƒæ•°ä½œä¸ºæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¸ä»…èƒ½é‡åŒ–æ¨¡å‹çš„æ•°æ®å‹ç¼©èƒ½åŠ›ï¼Œè¿˜èƒ½é€šè¿‡å‡¸è¿‘ä¼¼çŸ©é˜µç§©æ¥æ•æ‰é¢„æµ‹çš„å¯è¾¨è¯†æ€§å’Œå¤šæ ·æ€§ã€‚é€šè¿‡ä½¿ç”¨ \\( L_{1,2}-norm \\) è¿›ä¸€æ­¥è¿‘ä¼¼æ ¸èŒƒæ•°ï¼Œæˆ‘ä»¬æ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶åœ¨ä¸éœ€è¦å¥‡å¼‚å€¼åˆ†è§£çš„æƒ…å†µä¸‹æé«˜äº†è¯„ä¼°é€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.11817', 'title': 'Improving Long-Text Alignment for Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2410.11817', 'abstract': 'The rapid advancement of text-to-image (T2I) diffusion models has enabled them to generate unprecedented results from given texts. However, as text inputs become longer, existing encoding methods like CLIP face limitations, and aligning the generated images with long texts becomes challenging. To tackle these issues, we propose LongAlign, which includes a segment-level encoding method for processing long texts and a decomposed preference optimization method for effective alignment training. For segment-level encoding, long texts are divided into multiple segments and processed separately. This method overcomes the maximum input length limits of pretrained encoding models. For preference optimization, we provide decomposed CLIP-based preference models to fine-tune diffusion models. Specifically, to utilize CLIP-based preference models for T2I alignment, we delve into their scoring mechanisms and find that the preference scores can be decomposed into two components: a text-relevant part that measures T2I alignment and a text-irrelevant part that assesses other visual aspects of human preference. Additionally, we find that the text-irrelevant part contributes to a common overfitting problem during fine-tuning. To address this, we propose a reweighting strategy that assigns different weights to these two components, thereby reducing overfitting and enhancing alignment. After fine-tuning 512 times 512 Stable Diffusion (SD) v1.5 for about 20 hours using our method, the fine-tuned SD outperforms stronger foundation models in T2I alignment, such as PixArt-alpha and Kandinsky v2.2. The code is available at https://github.com/luping-liu/LongAlign.', 'score': 14, 'issue_id': 134, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': 'c9ba30d5d0f611d5', 'authors': ['Luping Liu', 'Chao Du', 'Tianyu Pang', 'Zehan Wang', 'Chongxuan Li', 'Dong Xu'], 'affiliations': ['Renmin University of China', 'Sea AI Lab, Singapore', 'The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11817.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#open_source', '#architecture', '#long_context'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'LongAlign: ĞŸÑ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ LongAlign Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº CLIP, Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸. ĞŸĞ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Stable Diffusion Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LongAlign Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ.'}, 'en': {'title': '"Long Texts, Perfect Images: The LongAlign Revolution"', 'desc': 'The paper introduces LongAlign, a novel approach to improve text-to-image (T2I) diffusion models when dealing with long text inputs. It addresses the limitations of existing encoding methods like CLIP by segmenting long texts and optimizing alignment through decomposed preference models. By dividing texts into segments, LongAlign overcomes input length restrictions, and its reweighting strategy reduces overfitting during fine-tuning. The method significantly enhances T2I alignment, outperforming other models like PixArt-alpha and Kandinsky v2.2.'}, 'zh': {'title': 'é•¿æ–‡æœ¬å¯¹é½æ–°çªç ´ï¼šLongAlignæ–¹æ³•', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLongAlignçš„æ–°æ–¹æ³•ï¼Œç”¨äºè§£å†³æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶çš„å¯¹é½é—®é¢˜ã€‚LongAligné€šè¿‡å°†é•¿æ–‡æœ¬åˆ†æ®µç¼–ç ï¼Œå…‹æœäº†é¢„è®­ç»ƒç¼–ç æ¨¡å‹çš„è¾“å…¥é•¿åº¦é™åˆ¶ã€‚ä¸ºäº†ä¼˜åŒ–å¯¹é½æ•ˆæœï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åˆ†è§£çš„åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨CLIPæ¨¡å‹çš„åå¥½è¯„åˆ†æœºåˆ¶ã€‚é€šè¿‡é‡æ–°åŠ æƒç­–ç•¥ï¼ŒLongAlignæœ‰æ•ˆå‡å°‘äº†è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¹¶åœ¨å¯¹é½æ€§èƒ½ä¸Šè¶…è¶Šäº†å…¶ä»–åŸºç¡€æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.12405', 'title': 'ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs', 'url': 'https://huggingface.co/papers/2410.12405', 'abstract': 'Large language models (LLMs) have demonstrated impressive capabilities across various tasks, but their performance is highly sensitive to the prompts utilized. This variability poses challenges for accurate assessment and user satisfaction. Current research frequently overlooks instance-level prompt variations and their implications on subjective evaluations. To address these shortcomings, we introduce ProSA, a framework designed to evaluate and comprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity metric, PromptSensiScore, and leverages decoding confidence to elucidate underlying mechanisms. Our extensive study, spanning multiple tasks, uncovers that prompt sensitivity fluctuates across datasets and models, with larger models exhibiting enhanced robustness. We observe that few-shot examples can alleviate this sensitivity issue, and subjective evaluations are also susceptible to prompt sensitivities, particularly in complex, reasoning-oriented tasks. Furthermore, our findings indicate that higher model confidence correlates with increased prompt robustness. We believe this work will serve as a helpful tool in studying prompt sensitivity of LLMs. The project is released at: https://github.com/open-compass/ProSA .', 'score': 13, 'issue_id': 134, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 16', 'zh': '10æœˆ16æ—¥'}, 'hash': 'd2f8e1c0dcdf19fa', 'authors': ['Jingming Zhuo', 'Songyang Zhang', 'Xinyu Fang', 'Haodong Duan', 'Dahua Lin', 'Kai Chen'], 'affiliations': ['Jilin University', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12405.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#interpretability', '#training', '#open_source', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ProSA: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ProSA - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ PromptSensiScore Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ LLM. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ few-shot Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ° ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ¶ĞµĞ½Ñ‹ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Mastering the Art of Prompting: Enhancing LLM Robustness', 'desc': 'Large language models (LLMs) can perform well on many tasks, but their success depends a lot on how they are prompted. This paper introduces ProSA, a framework to study how sensitive LLMs are to different prompts using a new metric called PromptSensiScore. The study shows that bigger models handle prompt changes better, and using few-shot examples can help reduce sensitivity. It also finds that when models are more confident, they are less affected by changes in prompts.'}, 'zh': {'title': 'ç†è§£å’Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æç¤ºè¯é²æ£’æ€§', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¯¹æç¤ºè¯çš„æ•æ„Ÿæ€§å¾ˆé«˜ï¼Œè¿™å½±å“äº†è¯„ä¼°çš„å‡†ç¡®æ€§å’Œç”¨æˆ·æ»¡æ„åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†ProSAæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å’Œç†è§£æç¤ºè¯æ•æ„Ÿæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œæç¤ºè¯æ•æ„Ÿæ€§åœ¨ä¸åŒæ•°æ®é›†å’Œæ¨¡å‹ä¸­æ³¢åŠ¨ï¼Œè¾ƒå¤§çš„æ¨¡å‹è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚é€šè¿‡ä½¿ç”¨å°‘é‡ç¤ºä¾‹å¯ä»¥å‡è½»è¿™ç§æ•æ„Ÿæ€§é—®é¢˜ï¼Œå¹¶ä¸”æ¨¡å‹çš„é«˜ç½®ä¿¡åº¦ä¸æç¤ºè¯çš„é²æ£’æ€§å¢åŠ æœ‰å…³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.08968', 'title': 'Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements', 'url': 'https://huggingface.co/papers/2410.08968', 'abstract': 'The current paradigm for safety alignment of large language models (LLMs) follows a one-size-fits-all approach: the model refuses to interact with any content deemed unsafe by the model provider. This approach lacks flexibility in the face of varying social norms across cultures and regions. In addition, users may have diverse safety needs, making a model with static safety standards too restrictive to be useful, as well as too costly to be re-aligned.   We propose Controllable Safety Alignment (CoSA), a framework designed to adapt models to diverse safety requirements without re-training. Instead of aligning a fixed model, we align models to follow safety configs -- free-form natural language descriptions of the desired safety behaviors -- that are provided as part of the system prompt. To adjust model safety behavior, authorized users only need to modify such safety configs at inference time. To enable that, we propose CoSAlign, a data-centric method for aligning LLMs to easily adapt to diverse safety configs. Furthermore, we devise a novel controllability evaluation protocol that considers both helpfulness and configured safety, summarizing them into CoSA-Score, and construct CoSApien, a human-authored benchmark that consists of real-world LLM use cases with diverse safety requirements and corresponding evaluation prompts.   We show that CoSAlign leads to substantial gains of controllability over strong baselines including in-context alignment. Our framework encourages better representation and adaptation to pluralistic human values in LLMs, and thereby increasing their practicality.', 'score': 12, 'issue_id': 140, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 11', 'zh': '10æœˆ11æ—¥'}, 'hash': 'b81f79d5fa0e574a', 'authors': ['Jingyu Zhang', 'Ahmed Elgohary', 'Ahmed Magooda', 'Daniel Khashabi', 'Benjamin Van Durme'], 'affiliations': ['Johns Hopkins University', 'Microsoft Responsible AI Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.08968.jpg', 'data': {'categories': ['#benchmark', '#multilingual', '#inference', '#ethics', '#data', '#training', '#alignment'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Controllable Safety Alignment (CoSA). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° 'Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ²ÑĞµÑ…', CoSA Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ CoSAlign Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ LLM Ğ½Ğ° ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ·Ğ°Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¼ Ğ² Ğ²Ğ¸Ğ´Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CoSApien Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM."}, 'en': {'title': 'Flexible Safety for Diverse Needs: CoSA Framework for LLMs', 'desc': "The paper introduces Controllable Safety Alignment (CoSA), a framework that allows large language models (LLMs) to adapt to diverse safety requirements without needing to be retrained. Instead of using a fixed safety standard, CoSA uses safety configurations, which are natural language descriptions of desired safety behaviors, that can be modified by authorized users at inference time. The authors propose CoSAlign, a data-centric method that enhances the model's ability to adjust to these safety configurations, and introduce a new evaluation protocol called CoSA-Score to measure both helpfulness and safety. The framework aims to better represent and adapt to varying human values, making LLMs more practical and flexible in real-world applications."}, 'zh': {'title': 'çµæ´»é€‚åº”å¤šå…ƒå®‰å…¨éœ€æ±‚çš„è¯­è¨€æ¨¡å‹', 'desc': 'ç›®å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨å¯¹é½æ–¹æ³•é€šå¸¸é‡‡ç”¨ä¸€åˆ€åˆ‡çš„æ–¹å¼ï¼Œæ‹’ç»ä¸ä»»ä½•è¢«è®¤ä¸ºä¸å®‰å…¨çš„å†…å®¹äº’åŠ¨ã€‚è¿™ç§æ–¹æ³•ç¼ºä¹çµæ´»æ€§ï¼Œéš¾ä»¥é€‚åº”ä¸åŒæ–‡åŒ–å’Œåœ°åŒºçš„ç¤¾ä¼šè§„èŒƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå¯æ§å®‰å…¨å¯¹é½ï¼ˆCoSAï¼‰çš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨ç³»ç»Ÿæç¤ºä¸­æä¾›å®‰å…¨é…ç½®æ¥è°ƒæ•´æ¨¡å‹çš„å®‰å…¨è¡Œä¸ºï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚CoSAlignæ–¹æ³•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè½»æ¾é€‚åº”å¤šæ ·åŒ–çš„å®‰å…¨éœ€æ±‚ï¼Œæé«˜äº†æ¨¡å‹çš„å®ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.07722', 'title': 'DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities', 'url': 'https://huggingface.co/papers/2410.07722', 'abstract': "Learned Sparse Retrieval (LSR) models use vocabularies from pre-trained transformers, which often split entities into nonsensical fragments. Splitting entities can reduce retrieval accuracy and limits the model's ability to incorporate up-to-date world knowledge not included in the training data. In this work, we enhance the LSR vocabulary with Wikipedia concepts and entities, enabling the model to resolve ambiguities more effectively and stay current with evolving knowledge. Central to our approach is a Dynamic Vocabulary (DyVo) head, which leverages existing entity embeddings and an entity retrieval component that identifies entities relevant to a query or document. We use the DyVo head to generate entity weights, which are then merged with word piece weights to create joint representations for efficient indexing and retrieval using an inverted index. In experiments across three entity-rich document ranking datasets, the resulting DyVo model substantially outperforms state-of-the-art baselines.", 'score': 12, 'issue_id': 139, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': 'aff6e5f440cea35e', 'authors': ['Thong Nguyen', 'Shubham Chatterjee', 'Sean MacAvaney', 'Iain Mackie', 'Jeff Dalton', 'Andrew Yates'], 'affiliations': ['University of Amsterdam', 'University of Edinburgh', 'University of Glasgow'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.07722.jpg', 'data': {'categories': ['#rag', '#reasoning', '#graphs', '#dataset', '#transfer_learning', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° (LSR) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸Ğ· Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ½ÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (DyVo), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ² ĞºÑƒÑ€ÑĞµ Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ñ…ÑÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DyVo Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Boosting Retrieval with Dynamic Vocabulary', 'desc': 'The paper introduces a method to improve Learned Sparse Retrieval (LSR) models by enhancing their vocabulary with Wikipedia concepts and entities. This approach helps the model better understand and retrieve information by resolving ambiguities and incorporating current world knowledge. A key innovation is the Dynamic Vocabulary (DyVo) head, which combines entity embeddings with word piece weights for more accurate indexing and retrieval. Experiments show that this method significantly outperforms existing models in entity-rich document ranking tasks.'}, 'zh': {'title': 'åŠ¨æ€è¯æ±‡å¤´ï¼šæå‡å®ä½“æ£€ç´¢çš„å‡†ç¡®æ€§', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ”¹è¿›çš„ç¨€ç–æ£€ç´¢æ¨¡å‹ï¼Œç§°ä¸ºåŠ¨æ€è¯æ±‡å¤´ï¼ˆDyVoï¼‰ï¼Œå®ƒé€šè¿‡ç»“åˆç»´åŸºç™¾ç§‘çš„æ¦‚å¿µå’Œå®ä½“æ¥å¢å¼ºæ¨¡å‹çš„è¯æ±‡è¡¨ã€‚ä¼ ç»Ÿçš„æ¨¡å‹åœ¨å¤„ç†å®ä½“æ—¶å¸¸å¸¸å°†å…¶åˆ†å‰²æˆæ— æ„ä¹‰çš„ç‰‡æ®µï¼Œå½±å“äº†æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚DyVoå¤´åˆ©ç”¨ç°æœ‰çš„å®ä½“åµŒå…¥å’Œå®ä½“æ£€ç´¢ç»„ä»¶æ¥è¯†åˆ«ä¸æŸ¥è¯¢æˆ–æ–‡æ¡£ç›¸å…³çš„å®ä½“ï¼Œå¹¶ç”Ÿæˆå®ä½“æƒé‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDyVoæ¨¡å‹åœ¨å¤šä¸ªå®ä½“ä¸°å¯Œçš„æ–‡æ¡£æ’åæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.08584', 'title': 'ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression', 'url': 'https://huggingface.co/papers/2410.08584', 'abstract': 'The efficiency of large vision-language models (LVLMs) is constrained by the computational bottleneck of the attention mechanism during the prefill phase and the memory bottleneck of fetching the key-value (KV) cache in the decoding phase, particularly in scenarios involving high-resolution images or videos. Visual content often exhibits substantial redundancy, resulting in highly sparse attention maps within LVLMs. This sparsity can be leveraged to accelerate attention computation or compress the KV cache through various approaches. However, most studies focus on addressing only one of these bottlenecks and do not adequately support dynamic adjustment of sparsity concerning distinct layers or tasks. In this paper, we present ZipVL, an efficient inference framework designed for LVLMs that resolves both computation and memory bottlenecks through a dynamic ratio allocation strategy of important tokens. This ratio is adaptively determined based on the layer-specific distribution of attention scores, rather than fixed hyper-parameters, thereby improving efficiency for less complex tasks while maintaining high performance for more challenging ones. Then we select important tokens based on their normalized attention scores and perform attention mechanism solely on those important tokens to accelerate the prefill phase. To mitigate the memory bottleneck in the decoding phase, we employ mixed-precision quantization to the KV cache, where high-bit quantization is used for caches of important tokens, while low-bit quantization is applied to those of less importance. Our experiments demonstrate that ZipVL can accelerate the prefill phase by 2.6times and reduce GPU memory usage by 50.0%, with a minimal accuracy reduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively enhancing the generation efficiency of LVLMs.', 'score': 11, 'issue_id': 136, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 11', 'zh': '10æœˆ11æ—¥'}, 'hash': 'faf1b859b79c97a2', 'authors': ['Yefei He', 'Feng Chen', 'Jing Liu', 'Wenqi Shao', 'Hong Zhou', 'Kaipeng Zhang', 'Bohan Zhuang'], 'affiliations': ['Shanghai AI Laboratory, China', 'The University of Adelaide, Australia', 'ZIP Lab, Monash University, Australia', 'Zhejiang University, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.08584.jpg', 'data': {'categories': ['#benchmark', '#cv', '#inference', '#video', '#optimization', '#graphs', '#architecture'], 'emoji': 'ğŸš€', 'ru': {'title': 'ZipVL: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ LVLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'ZipVL - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (LVLM). ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ñ„Ğ°Ğ·Ñƒ Ğ¿Ñ€ĞµĞ´Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ…. Ğ”Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'ZipVL: Streamlining Vision-Language Models for Speed and Efficiency', 'desc': 'The paper introduces ZipVL, a framework that enhances the efficiency of large vision-language models by addressing both computational and memory bottlenecks. It dynamically allocates important tokens based on layer-specific attention scores, optimizing the attention mechanism and reducing redundancy. By using mixed-precision quantization for the key-value cache, it balances memory usage and performance. Experiments show that ZipVL significantly speeds up processing and reduces memory requirements with minimal impact on accuracy.'}, 'zh': {'title': 'ZipVLï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹æ•ˆç‡çš„æ–°ç­–ç•¥', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºZipVLçš„é«˜æ•ˆæ¨ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸­çš„è®¡ç®—å’Œå†…å­˜ç“¶é¢ˆé—®é¢˜ã€‚ZipVLé€šè¿‡åŠ¨æ€åˆ†é…é‡è¦tokençš„æ¯”ä¾‹ï¼ŒåŸºäºå±‚ç‰¹å®šçš„æ³¨æ„åŠ›åˆ†æ•°åˆ†å¸ƒæ¥æé«˜æ•ˆç‡ã€‚ä¸ºäº†åŠ é€Ÿé¢„å¡«å……é˜¶æ®µï¼ŒZipVLä»…å¯¹é‡è¦tokenæ‰§è¡Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶åœ¨è§£ç é˜¶æ®µå¯¹KVç¼“å­˜è¿›è¡Œæ··åˆç²¾åº¦é‡åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒZipVLåœ¨åŠ é€Ÿé¢„å¡«å……é˜¶æ®µå’Œå‡å°‘GPUå†…å­˜ä½¿ç”¨æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.12490', 'title': 'Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective', 'url': 'https://huggingface.co/papers/2410.12490', 'abstract': 'Latent-based image generative models, such as Latent Diffusion Models (LDMs) and Mask Image Models (MIMs), have achieved notable success in image generation tasks. These models typically leverage reconstructive autoencoders like VQGAN or VAE to encode pixels into a more compact latent space and learn the data distribution in the latent space instead of directly from pixels. However, this practice raises a pertinent question: Is it truly the optimal choice? In response, we begin with an intriguing observation: despite sharing the same latent space, autoregressive models significantly lag behind LDMs and MIMs in image generation. This finding contrasts sharply with the field of NLP, where the autoregressive model GPT has established a commanding presence. To address this discrepancy, we introduce a unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling. Furthermore, we propose a simple but effective discrete image tokenizer to stabilize the latent space for image generative modeling. Experimental results show that image autoregressive modeling with our tokenizer (DiGIT) benefits both image understanding and image generation with the next token prediction principle, which is inherently straightforward for GPT models but challenging for other generative models. Remarkably, for the first time, a GPT-style autoregressive model for images outperforms LDMs, which also exhibits substantial improvement akin to GPT when scaling up model size. Our findings underscore the potential of an optimized latent space and the integration of discrete tokenization in advancing the capabilities of image generative models. The code is available at https://github.com/DAMO-NLP-SG/DiGIT.', 'score': 8, 'issue_id': 141, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 16', 'zh': '10æœˆ16æ—¥'}, 'hash': '959f8e0892b466b9', 'authors': ['Yongxin Zhu', 'Bocheng Li', 'Hang Zhang', 'Xin Li', 'Linli Xu', 'Lidong Bing'], 'affiliations': ['School of Computer Science and Technology, University of Science and Technology of China', 'School of Data Science, University of Science and Technology of China', 'State Key Laboratory of Cognitive Intelligence', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12490.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'DiGIT: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑÑ‚Ğ¸Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ (DiGIT) Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Image Generation: Bridging the Gap with Discrete Tokenization', 'desc': 'This paper explores the effectiveness of latent-based image generative models, particularly focusing on the performance gap between autoregressive models and other models like Latent Diffusion Models (LDMs) and Mask Image Models (MIMs). The authors propose a new discrete image tokenizer, DiGIT, to stabilize the latent space, which enhances the performance of autoregressive models in image generation. Their experiments demonstrate that, with this tokenizer, autoregressive models can outperform LDMs, showing significant improvements similar to those seen in NLP with GPT models. This work highlights the importance of optimizing latent space and using discrete tokenization to improve image generative modeling.'}, 'zh': {'title': 'ä¼˜åŒ–æ½œåœ¨ç©ºé—´ï¼Œæå‡å›¾åƒç”Ÿæˆ', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­æ½œåœ¨ç©ºé—´çš„ä¼˜åŒ–é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯è‡ªå›å½’æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡è‡ªå›å½’æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å›¾åƒç”Ÿæˆä¸­å´ä¸å¦‚æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œæ©ç å›¾åƒæ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„ç¦»æ•£å›¾åƒæ ‡è®°å™¨ï¼ˆDiGITï¼‰ï¼Œä»¥ç¨³å®šå›¾åƒç”Ÿæˆçš„æ½œåœ¨ç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•ä¸ä»…æå‡äº†å›¾åƒç†è§£å’Œç”Ÿæˆçš„æ•ˆæœï¼Œè¿˜ä½¿å¾—è‡ªå›å½’æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­é¦–æ¬¡è¶…è¶Šäº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.11878', 'title': 'Neural Metamorphosis', 'url': 'https://huggingface.co/papers/2410.11878', 'abstract': 'This paper introduces a new learning paradigm termed Neural Metamorphosis (NeuMeta), which aims to build self-morphable neural networks. Contrary to crafting separate models for different architectures or sizes, NeuMeta directly learns the continuous weight manifold of neural networks. Once trained, we can sample weights for any-sized network directly from the manifold, even for previously unseen configurations, without retraining. To achieve this ambitious goal, NeuMeta trains neural implicit functions as hypernetworks. They accept coordinates within the model space as input, and generate corresponding weight values on the manifold. In other words, the implicit function is learned in a way, that the predicted weights is well-performed across various models sizes. In training those models, we notice that, the final performance closely relates on smoothness of the learned manifold. In pursuit of enhancing this smoothness, we employ two strategies. First, we permute weight matrices to achieve intra-model smoothness, by solving the Shortest Hamiltonian Path problem. Besides, we add a noise on the input coordinates when training the implicit function, ensuring models with various sizes shows consistent outputs. As such, NeuMeta shows promising results in synthesizing parameters for various network configurations. Our extensive tests in image classification, semantic segmentation, and image generation reveal that NeuMeta sustains full-size performance even at a 75% compression rate.', 'score': 8, 'issue_id': 138, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 10', 'zh': '10æœˆ10æ—¥'}, 'hash': 'b168c08bbe553e03', 'authors': ['Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11878.jpg', 'data': {'categories': ['#optimization', '#training', '#transfer_learning', '#architecture'], 'emoji': 'ğŸ¦‹', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸ĞµÑÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸: Ğ¾Ğ´Ğ¸Ğ½ Ğ¼Ğ°Ğ½Ğ¸Ñ„Ğ¾Ğ»Ğ´ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Neural Metamorphosis (NeuMeta), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ°Ğ¼Ğ¾Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸ĞµÑÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸. NeuMeta Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³Ğ¸Ğ¿ĞµÑ€-ÑĞµÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²ĞµÑĞ° Ğ´Ğ»Ñ ÑĞµÑ‚ĞµĞ¹ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ½ĞµĞµ Ğ½ĞµĞ²Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ²ĞµÑĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NeuMeta ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ 75% ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ ÑĞµÑ‚Ğ¸.'}, 'en': {'title': '"Shape-Shifting Networks: One Model, Infinite Possibilities"', 'desc': 'The paper introduces Neural Metamorphosis (NeuMeta), a new approach that allows neural networks to adapt their size and architecture without needing separate models. NeuMeta learns a continuous weight manifold, enabling the generation of weights for any network size directly from this manifold. This is achieved by training neural implicit functions as hypernetworks, which take model space coordinates as input to produce corresponding weights. The approach ensures smoothness in the learned manifold, using techniques like weight matrix permutation and input noise, resulting in high performance across various network configurations.'}, 'zh': {'title': 'ç¥ç»å˜å½¢ï¼šè‡ªæˆ‘å˜å½¢çš„ç¥ç»ç½‘ç»œæ–°èŒƒå¼', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å­¦ä¹ èŒƒå¼ï¼Œç§°ä¸ºç¥ç»å˜å½¢ï¼ˆNeuMetaï¼‰ï¼Œæ—¨åœ¨æ„å»ºè‡ªæˆ‘å˜å½¢çš„ç¥ç»ç½‘ç»œã€‚ä¸ä¸ºä¸åŒæ¶æ„æˆ–å¤§å°å•ç‹¬è®¾è®¡æ¨¡å‹ä¸åŒï¼ŒNeuMetaç›´æ¥å­¦ä¹ ç¥ç»ç½‘ç»œçš„è¿ç»­æƒé‡æµå½¢ã€‚é€šè¿‡è®­ç»ƒç¥ç»éšå‡½æ•°ä½œä¸ºè¶…ç½‘ç»œï¼ŒNeuMetaå¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä»æµå½¢ä¸­ç›´æ¥é‡‡æ ·ä»»ä½•å¤§å°ç½‘ç»œçš„æƒé‡ã€‚å®éªŒè¡¨æ˜ï¼ŒNeuMetaåœ¨å›¾åƒåˆ†ç±»ã€è¯­ä¹‰åˆ†å‰²å’Œå›¾åƒç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œå³ä½¿åœ¨75%çš„å‹ç¼©ç‡ä¸‹ä¹Ÿèƒ½ä¿æŒå…¨å°ºå¯¸æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.09870', 'title': 'ChroKnowledge: Unveiling Chronological Knowledge of Language Models in Multiple Domains', 'url': 'https://huggingface.co/papers/2410.09870', 'abstract': "Large language models (LLMs) have significantly impacted many aspects of our lives. However, assessing and ensuring their chronological knowledge remains challenging. Existing approaches fall short in addressing the accumulative nature of knowledge, often relying on a single time stamp. To overcome this, we introduce ChroKnowBench, a benchmark dataset designed to evaluate chronologically accumulated knowledge across three key aspects: multiple domains, time dependency, temporal state. Our benchmark distinguishes between knowledge that evolves (e.g., scientific discoveries, amended laws) and knowledge that remain constant (e.g., mathematical truths, commonsense facts). Building on this benchmark, we present ChroKnowledge (Chronological Categorization of Knowledge), a novel sampling-based framework for evaluating and updating LLMs' non-parametric chronological knowledge. Our evaluation shows: (1) The ability of eliciting temporal knowledge varies depending on the data format that model was trained on. (2) LLMs partially recall knowledge or show a cut-off at temporal boundaries rather than recalling all aspects of knowledge correctly. Thus, we apply our ChroKnowPrompt, an in-depth prompting to elicit chronological knowledge by traversing step-by-step through the surrounding time spans. We observe that our framework successfully updates the overall knowledge across the entire timeline in both the biomedical domain (+11.9%) and the general domain (+2.8%), demonstrating its effectiveness in refining temporal knowledge. This non-parametric approach also enables knowledge updates not only in open-source models but also in proprietary LLMs, ensuring comprehensive applicability across model types. We perform a comprehensive analysis based on temporal characteristics of ChroKnowPrompt and validate the potential of various models to elicit intrinsic temporal knowledge through our method.", 'score': 7, 'issue_id': 140, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': 'c37b3e5475dd218a', 'authors': ['Yein Park', 'Chanwoong Yoon', 'Jungwoo Park', 'Donghyeon Lee', 'Minbyul Jeong', 'Jaewoo Kang'], 'affiliations': ['AIGEN Sciences', 'Korea University', 'Upstage AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.09870.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#agi', '#interpretability', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': 'â³', 'ru': {'title': 'Ğ¥Ñ€Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ChroKnowBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ…Ñ€Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ framework ChroKnowledge Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ…Ñ€Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ LLM. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ChroKnowPrompt Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ğ²ÑĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑˆĞºĞ°Ğ»Ğµ.'}, 'en': {'title': '"Mastering Time: Enhancing LLMs with Chronological Knowledge"', 'desc': "This paper introduces ChroKnowBench, a benchmark dataset designed to evaluate how well large language models (LLMs) understand and update chronological knowledge. It distinguishes between knowledge that changes over time, like scientific discoveries, and knowledge that remains constant, like mathematical truths. The authors also present ChroKnowledge, a framework that uses a novel prompting method to improve LLMs' ability to recall and update temporal knowledge. Their approach shows significant improvements in knowledge accuracy across different domains, demonstrating its effectiveness in refining the temporal understanding of LLMs."}, 'zh': {'title': 'æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ—¶é—´çŸ¥è¯†ï¼šChroKnowBenchçš„åˆ›æ–°åº”ç”¨', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºChroKnowBenchçš„åŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´ä¸Šç´¯ç§¯çš„çŸ¥è¯†ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†çŸ¥è¯†çš„æ—¶é—´ä¾èµ–æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨çŸ¥è¯†æ¼”å˜å’Œä¸å˜æ€§ä¹‹é—´çš„åŒºåˆ†ä¸Šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„é‡‡æ ·æ¡†æ¶ChroKnowledgeï¼Œé€šè¿‡é€æ­¥æç¤ºæ¥æ›´æ–°å’Œè¯„ä¼°æ¨¡å‹çš„æ—¶é—´çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨ç”Ÿç‰©åŒ»å­¦å’Œä¸€èˆ¬é¢†åŸŸä¸­æœ‰æ•ˆæé«˜äº†æ¨¡å‹çš„æ—¶é—´çŸ¥è¯†æ›´æ–°èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.12391', 'title': 'Tracking Universal Features Through Fine-Tuning and Model Merging', 'url': 'https://huggingface.co/papers/2410.12391', 'abstract': 'We study how features emerge, disappear, and persist across models fine-tuned on different domains of text. More specifically, we start from a base one-layer Transformer language model that is trained on a combination of the BabyLM corpus, and a collection of Python code from The Stack. This base model is adapted to two new domains of text: TinyStories, and the Lua programming language, respectively; and then these two models are merged using these two models using spherical linear interpolation. Our exploration aims to provide deeper insights into the stability and transformation of features across typical transfer-learning scenarios using small-scale models and sparse auto-encoders.', 'score': 5, 'issue_id': 139, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 16', 'zh': '10æœˆ16æ—¥'}, 'hash': '6c1b6c634a13e9b7', 'authors': ['Niels Horn', 'Desmond Elliott'], 'affiliations': ['Department of Computer Science, University of Copenhagen'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12391.jpg', 'data': {'categories': ['#small_models', '#synthetic', '#plp', '#training', '#transfer_learning'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑÑ‡ĞµĞ·Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ÑÑ‚ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ´Ğ½Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Transformer, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ BabyLM Ğ¸ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¸ Python-ĞºĞ¾Ğ´Ğ° Ğ¸Ğ· The Stack. Ğ—Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğº Ğ´Ğ²ÑƒĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼: TinyStories Ğ¸ ÑĞ·Ñ‹Ğº Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Lua. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ÑÑ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unveiling Feature Dynamics in Transfer Learning', 'desc': "The paper investigates how features in machine learning models change when they are fine-tuned on different types of text data. It uses a one-layer Transformer model initially trained on a mix of simple English text and Python code. This model is then adapted to new domains, specifically children's stories and Lua programming language, and the resulting models are combined using a technique called spherical linear interpolation. The study aims to understand how features remain stable or transform during transfer learning, using small models and sparse auto-encoders."}, 'zh': {'title': 'æ¢ç´¢ç‰¹å¾åœ¨è¿ç§»å­¦ä¹ ä¸­çš„ç¨³å®šæ€§ä¸è½¬å˜', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åœ¨ä¸åŒæ–‡æœ¬é¢†åŸŸå¾®è°ƒçš„æ¨¡å‹ä¸­ï¼Œç‰¹å¾æ˜¯å¦‚ä½•å‡ºç°ã€æ¶ˆå¤±å’ŒæŒç»­å­˜åœ¨çš„ã€‚ç ”ç©¶ä»ä¸€ä¸ªåŸºç¡€çš„å•å±‚Transformerè¯­è¨€æ¨¡å‹å¼€å§‹ï¼Œè¯¥æ¨¡å‹åœ¨BabyLMè¯­æ–™åº“å’ŒPythonä»£ç é›†åˆä¸Šè¿›è¡Œè®­ç»ƒã€‚ç„¶åï¼Œè¿™ä¸ªåŸºç¡€æ¨¡å‹è¢«é€‚é…åˆ°ä¸¤ä¸ªæ–°çš„æ–‡æœ¬é¢†åŸŸï¼šTinyStorieså’ŒLuaç¼–ç¨‹è¯­è¨€ï¼Œå¹¶é€šè¿‡çƒé¢çº¿æ€§æ’å€¼åˆå¹¶è¿™ä¸¤ä¸ªæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¢ç´¢æ—¨åœ¨é€šè¿‡å°è§„æ¨¡æ¨¡å‹å’Œç¨€ç–è‡ªç¼–ç å™¨ï¼Œæ·±å…¥äº†è§£ç‰¹å¾åœ¨å…¸å‹è¿ç§»å­¦ä¹ åœºæ™¯ä¸­çš„ç¨³å®šæ€§å’Œè½¬å˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.12722', 'title': 'WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation', 'url': 'https://huggingface.co/papers/2410.12722', 'abstract': 'Multimodal/vision language models (VLMs) are increasingly being deployed in healthcare settings worldwide, necessitating robust benchmarks to ensure their safety, efficacy, and fairness. Multiple-choice question and answer (QA) datasets derived from national medical examinations have long served as valuable evaluation tools, but existing datasets are largely text-only and available in a limited subset of languages and countries. To address these challenges, we present WorldMedQA-V, an updated multilingual, multimodal benchmarking dataset designed to evaluate VLMs in healthcare. WorldMedQA-V includes 568 labeled multiple-choice QAs paired with 568 medical images from four countries (Brazil, Israel, Japan, and Spain), covering original languages and validated English translations by native clinicians, respectively. Baseline performance for common open- and closed-source models are provided in the local language and English translations, and with and without images provided to the model. The WorldMedQA-V benchmark aims to better match AI systems to the diverse healthcare environments in which they are deployed, fostering more equitable, effective, and representative applications.', 'score': 5, 'issue_id': 138, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 16', 'zh': '10æœˆ16æ—¥'}, 'hash': '6a533574af0f5068', 'authors': ['JoÃ£o Matos', 'Shan Chen', 'Siena Placino', 'Yingya Li', 'Juan Carlos Climent Pardo', 'Daphna Idan', 'Takeshi Tohyama', 'David Restrepo', 'Luis F. Nakayama', 'Jose M. M. Pascual-Leone', 'Guergana Savova', 'Hugo Aerts', 'Leo A. Celi', 'A. Ian Wong', 'Danielle S. Bitterman', 'Jack Gallifant'], 'affiliations': ['AlcalÃ¡ University', 'BIDMC', 'Ben-Gurion University of the Negev', 'Boston Childrens Hospital', 'Duke', 'Harvard', 'International University of Health and Welfare', 'MIT', 'Maastricht University', 'Mass General Brigham', 'Oxford', 'St. Lukes Medical Center'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12722.jpg', 'data': {'categories': ['#science', '#benchmark', '#cv', '#multilingual', '#healthcare', '#multimodal', '#ethics', '#dataset', '#open_source', '#machine_translation'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ', 'desc': 'WorldMedQA-V - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 568 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ· 4 ÑÑ‚Ñ€Ğ°Ğ½ Ğ½Ğ° Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ Ğ² Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ±ĞµĞ· Ğ½Ğ¸Ñ…, Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ¦ĞµĞ»ÑŒ WorldMedQA-V - Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Healthcare AI: A Global, Multimodal Approach', 'desc': 'The paper introduces WorldMedQA-V, a new benchmarking dataset for evaluating vision language models (VLMs) in healthcare. This dataset includes 568 multiple-choice questions paired with medical images from four countries, offering both original and English translations. It aims to address the limitations of existing text-only datasets by providing a multilingual and multimodal approach. The goal is to ensure that AI systems are more effective and fair in diverse healthcare settings.'}, 'zh': {'title': 'WorldMedQA-Vï¼šå¤šè¯­è¨€å¤šæ¨¡æ€åŒ»ç–—è¯„ä¼°æ–°åŸºå‡†', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šè¯­è¨€ã€å¤šæ¨¡æ€åŸºå‡†æ•°æ®é›†ï¼Œåä¸ºWorldMedQA-Vï¼Œç”¨äºè¯„ä¼°åœ¨åŒ»ç–—é¢†åŸŸçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªå››ä¸ªå›½å®¶çš„568ä¸ªå¤šé¡¹é€‰æ‹©é¢˜å’Œç›¸åº”çš„åŒ»å­¦å›¾åƒï¼Œå¹¶æä¾›åŸå§‹è¯­è¨€å’Œç»è¿‡éªŒè¯çš„è‹±è¯­ç¿»è¯‘ã€‚ç ”ç©¶æä¾›äº†å¸¸è§å¼€æºå’Œé—­æºæ¨¡å‹åœ¨æœ¬åœ°è¯­è¨€å’Œè‹±è¯­ç¿»è¯‘ä¸‹çš„åŸºçº¿æ€§èƒ½ï¼Œå¹¶æ¯”è¾ƒäº†æœ‰æ— å›¾åƒè¾“å…¥çš„æ•ˆæœã€‚WorldMedQA-Væ—¨åœ¨æ›´å¥½åœ°åŒ¹é…AIç³»ç»Ÿä¸å¤šæ ·åŒ–çš„åŒ»ç–—ç¯å¢ƒï¼Œä¿ƒè¿›æ›´å…¬å¹³å’Œæœ‰æ•ˆçš„åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.12491', 'title': 'Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse RL', 'url': 'https://huggingface.co/papers/2410.12491', 'abstract': 'Large language models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF) have demonstrated remarkable capabilities, but their underlying reward functions and decision-making processes remain opaque. This paper introduces a novel approach to interpreting LLMs by applying inverse reinforcement learning (IRL) to recover their implicit reward functions. We conduct experiments on toxicity-aligned LLMs of varying sizes, extracting reward models that achieve up to 80.40% accuracy in predicting human preferences. Our analysis reveals key insights into the non-identifiability of reward functions, the relationship between model size and interpretability, and potential pitfalls in the RLHF process. We demonstrate that IRL-derived reward models can be used to fine-tune new LLMs, resulting in comparable or improved performance on toxicity benchmarks. This work provides a new lens for understanding and improving LLM alignment, with implications for the responsible development and deployment of these powerful systems.', 'score': 4, 'issue_id': 138, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 16', 'zh': '10æœˆ16æ—¥'}, 'hash': '38785426efa90355', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#small_models', '#rl', '#rlhf', '#interpretability', '#ethics', '#training', '#alignment'], 'emoji': 'ğŸ”', 'ru': {'title': "Ğ Ğ°ÑÑˆĞ¸Ñ„Ñ€Ğ¾Ğ²ĞºĞ° 'Ñ‡ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‰Ğ¸ĞºĞ°' ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹", 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (IRL) Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° LLM Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ², Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 80,40% Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ» ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ½ĞµĞ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ RLHF. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ IRL, Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… LLM, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ½Ğ° Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Decoding the Secrets of Language Models with Inverse Reinforcement Learning', 'desc': 'This paper explores how large language models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF) can be better understood by using inverse reinforcement learning (IRL) to uncover their hidden reward functions. By applying IRL to LLMs of different sizes, the study achieves up to 80.40% accuracy in predicting human preferences, offering insights into the challenges of identifying reward functions and the link between model size and interpretability. The research highlights potential issues in the RLHF process and shows that IRL-derived reward models can enhance the performance of new LLMs on toxicity benchmarks. This approach provides a fresh perspective on aligning LLMs responsibly, aiding in their development and deployment.'}, 'zh': {'title': 'é€†å‘å¼ºåŒ–å­¦ä¹ ï¼šæ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„éšç§˜å¥–åŠ±', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡é€†å‘å¼ºåŒ–å­¦ä¹ æ¥è§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹çš„éšå«å¥–åŠ±å‡½æ•°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•å¯ä»¥æå–å‡ºé«˜è¾¾80.40%å‡†ç¡®ç‡çš„äººç±»åå¥½é¢„æµ‹æ¨¡å‹ã€‚åˆ†ææ­ç¤ºäº†å¥–åŠ±å‡½æ•°çš„ä¸å¯è¯†åˆ«æ€§ã€æ¨¡å‹å¤§å°ä¸å¯è§£é‡Šæ€§ä¹‹é—´çš„å…³ç³»ï¼Œä»¥åŠRLHFè¿‡ç¨‹ä¸­çš„æ½œåœ¨é—®é¢˜ã€‚é€šè¿‡è¿™ç§æ–¹æ³•å¾®è°ƒçš„æ–°æ¨¡å‹åœ¨æ¯’æ€§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†ç†è§£å’Œæ”¹è¿›LLMå¯¹é½çš„æ–°è§†è§’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.12109', 'title': 'OMCAT: Omni Context Aware Transformer', 'url': 'https://huggingface.co/papers/2410.12109', 'abstract': 'Large Language Models (LLMs) have made significant strides in text generation and comprehension, with recent advancements extending into multimodal LLMs that integrate visual and audio inputs. However, these models continue to struggle with fine-grained, cross-modal temporal understanding, particularly when correlating events across audio and video streams. We address these challenges with two key contributions: a new dataset and model, called OCTAV and OMCAT respectively. OCTAV (Omni Context and Temporal Audio Video) is a novel dataset designed to capture event transitions across audio and video. Second, OMCAT (Omni Context Aware Transformer) is a powerful model that leverages RoTE (Rotary Time Embeddings), an innovative extension of RoPE, to enhance temporal grounding and computational efficiency in time-anchored tasks. Through a robust three-stage training pipeline-feature alignment, instruction tuning, and OCTAV-specific training-OMCAT excels in cross-modal temporal understanding. Our model demonstrates state-of-the-art performance on Audio-Visual Question Answering (AVQA) tasks and the OCTAV benchmark, showcasing significant gains in temporal reasoning and cross-modal alignment, as validated through comprehensive experiments and ablation studies. Our dataset and code will be made publicly available. The link to our demo page is https://om-cat.github.io.', 'score': 4, 'issue_id': 135, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 15', 'zh': '10æœˆ15æ—¥'}, 'hash': 'c2efc033900c5b2f', 'authors': ['Arushi Goel', 'Karan Sapra', 'Matthieu Le', 'Rafael Valle', 'Andrew Tao', 'Bryan Catanzaro'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.12109.jpg', 'data': {'categories': ['#science', '#benchmark', '#cv', '#graphs', '#video', '#optimization', '#multimodal', '#training', '#dataset', '#open_source', '#audio', '#architecture', '#alignment'], 'emoji': 'ğŸ¬', 'ru': {'title': 'OMCAT: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ OCTAV Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ OMCAT Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. OCTAV ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°Ğ¼Ğ¸. OMCAT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ RoTE Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OCTAV.'}, 'en': {'title': "Mastering Time: OMCAT's Leap in Cross-Modal Understanding", 'desc': 'This paper introduces a new dataset called OCTAV and a model named OMCAT to improve cross-modal temporal understanding in multimodal large language models. OCTAV is designed to capture event transitions across audio and video, while OMCAT uses Rotary Time Embeddings to enhance temporal grounding. The model undergoes a three-stage training process, resulting in state-of-the-art performance on Audio-Visual Question Answering tasks. The research demonstrates significant improvements in temporal reasoning and cross-modal alignment, with the dataset and code being made publicly available.'}, 'zh': {'title': 'è·¨æ¨¡æ€æ—¶é—´ç†è§£çš„æ–°çªç ´', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆå’Œç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæœ€è¿‘çš„è¿›å±•æ‰©å±•åˆ°æ•´åˆè§†è§‰å’ŒéŸ³é¢‘è¾“å…¥çš„å¤šæ¨¡æ€LLMsã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç»†ç²’åº¦çš„è·¨æ¨¡æ€æ—¶é—´ç†è§£ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨éŸ³é¢‘å’Œè§†é¢‘æµäº‹ä»¶çš„å…³è”ä¸Šã€‚æˆ‘ä»¬é€šè¿‡ä¸¤ä¸ªå…³é”®è´¡çŒ®æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼šä¸€ä¸ªæ–°çš„æ•°æ®é›†OCTAVå’Œä¸€ä¸ªæ–°çš„æ¨¡å‹OMCATã€‚OMCATæ¨¡å‹é€šè¿‡ä½¿ç”¨RoTEï¼ˆæ—‹è½¬æ—¶é—´åµŒå…¥ï¼‰æ¥å¢å¼ºæ—¶é—´é”šå®šä»»åŠ¡çš„æ—¶é—´åŸºç¡€å’Œè®¡ç®—æ•ˆç‡ï¼Œåœ¨è·¨æ¨¡æ€æ—¶é—´ç†è§£ä¸Šè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.11900', 'title': 'FLARE: Faithful Logic-Aided Reasoning and Exploration', 'url': 'https://huggingface.co/papers/2410.11900', 'abstract': 'Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce Faithful Logic-Aided Reasoning and Exploration (\\ours), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on 7 out of 9 diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that {\\ours} allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.', 'score': 3, 'issue_id': 161, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 14', 'zh': '10æœˆ14æ—¥'}, 'hash': '9dc3af0706a41348', 'authors': ['Erik Arakelyan', 'Pasquale Minervini', 'Pat Verga', 'Patrick Lewis', 'Isabelle Augenstein'], 'affiliations': ['Cohere', 'Miniml.AI', 'University of Copenhagen', 'University of Edinburgh'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11900.jpg', 'data': {'categories': ['#rag', '#reasoning', '#benchmark', '#plp', '#interpretability', '#training', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ›Ğ¾Ğ³Ğ¸ĞºĞ° Ğ¸ LLM Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Faithful Logic-Aided Reasoning and Exploration (FLARE) Ğ¸ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ² Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. FLARE Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑˆĞ°Ğ³Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 7 Ğ¸Ğ· 9 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Enhancing Reasoning Faithfulness with Logic-Aided Exploration', 'desc': 'This paper presents a new method called Faithful Logic-Aided Reasoning and Exploration (FLARE) that enhances question answering and reasoning using Large Language Models (LLMs). FLARE combines the strengths of LLMs with logic programming to create a more interpretable approach for solving complex problems. It allows for a detailed exploration of the reasoning process by simulating code execution and analyzing each step in a multi-hop search. The results show that FLARE achieves state-of-the-art performance on various reasoning tasks while maintaining high faithfulness in its reasoning outputs.'}, 'zh': {'title': 'å¿ å®æ¨ç†ä¸æ¢ç´¢çš„æ–°æ–¹æ³•', 'desc': 'ç°ä»£é—®ç­”ï¼ˆQAï¼‰å’Œæ¨ç†æ–¹æ³•é€šå¸¸ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæç¤ºæŠ€æœ¯ï¼Œå¦‚æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œä»¥æœŸåœ¨é—®é¢˜ç©ºé—´ä¸­è¿›è¡Œæ›´ç»†è‡´çš„æ¢ç´¢å’Œæ¨ç†ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨ç”Ÿæˆä¸æ¨¡å‹ä¸­é—´æ¨ç†é“¾ä¸€è‡´çš„è¾“å‡ºæ—¶å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¯è§£é‡Šæ–¹æ³•â€”â€”å¿ å®é€»è¾‘è¾…åŠ©æ¨ç†ä¸æ¢ç´¢ï¼ˆ\textit{ours}ï¼‰ï¼Œé€šè¿‡ä»»åŠ¡åˆ†è§£æ¥éå†é—®é¢˜ç©ºé—´ã€‚è¯¥æ–¹æ³•åˆ©ç”¨LLMè§„åˆ’è§£å†³æ–¹æ¡ˆï¼Œå¹¶é€šè¿‡é€»è¾‘ç¼–ç¨‹ä»£ç å°†æŸ¥è¯¢è½¯å½¢å¼åŒ–ä¸ºäº‹å®å’Œè°“è¯ï¼Œè¿›è¡Œå¤šè·³æœç´¢ï¼Œä»è€Œè®¡ç®—æ¨ç†è¿‡ç¨‹çš„å¿ å®åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.09724', 'title': 'Taming Overconfidence in LLMs: Reward Calibration in RLHF', 'url': 'https://huggingface.co/papers/2410.09724', 'abstract': 'Language model calibration refers to the alignment between the confidence of the model and the actual performance of its responses. While previous studies point out the overconfidence phenomenon in Large Language Models (LLMs) and show that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) are overconfident with a more sharpened output probability, in this study, we reveal that RLHF tends to lead models to express verbalized overconfidence in their own responses. We investigate the underlying cause of this overconfidence and demonstrate that reward models used for Proximal Policy Optimization (PPO) exhibit inherent biases towards high-confidence scores regardless of the actual quality of responses. Building upon this insight, we propose two PPO variants: PPO-M: PPO with Calibrated Reward Modeling and PPO-C: PPO with Calibrated Reward Calculation. PPO-M integrates explicit confidence scores in reward model training, which calibrates reward models to better capture the alignment between response quality and verbalized confidence. PPO-C adjusts the reward score during PPO based on the difference between the current reward and the moving average of past rewards. Both PPO-M and PPO-C can be seamlessly integrated into the current PPO pipeline and do not require additional golden labels. We evaluate our methods on both Llama3-8B and Mistral-7B across six diverse datasets including multiple-choice and open-ended generation. Experiment results demonstrate that both of our methods can reduce calibration error and maintain performance comparable to standard PPO. We further show that they do not compromise model capabilities in open-ended conversation settings.', 'score': 2, 'issue_id': 141, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 13', 'zh': '10æœˆ13æ—¥'}, 'hash': 'a649dfe2bf936909', 'authors': ['Jixuan Leng', 'Chengsong Huang', 'Banghua Zhu', 'Jiaxin Huang'], 'affiliations': ['Carnegie Mellon University', 'UC Berkeley', 'Washington University in St. Louis'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.09724.jpg', 'data': {'categories': ['#small_models', '#rl', '#rlhf', '#benchmark', '#optimization', '#interpretability', '#training', '#alignment'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞšĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº RLHF', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM), Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (RLHF). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ ĞŸÑ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (PPO), Ğ¸Ğ¼ĞµÑÑ‚ ÑĞºĞ»Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ´Ğ²Ğ° Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ° PPO: PPO-M Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ PPO-C Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ PPO.'}, 'en': {'title': 'Balancing Confidence: Calibrating Language Models for Better Alignment', 'desc': 'This paper explores the issue of overconfidence in Large Language Models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF). It identifies that reward models used in Proximal Policy Optimization (PPO) are biased towards high-confidence scores, leading to verbalized overconfidence. To address this, the authors propose two new PPO variants: PPO-M, which incorporates calibrated reward modeling, and PPO-C, which adjusts reward scores based on past performance. These methods effectively reduce calibration error without sacrificing model performance, as demonstrated in experiments across various datasets.'}, 'zh': {'title': 'æ ¡å‡†è¯­è¨€æ¨¡å‹ï¼šå‡å°‘è¿‡åº¦è‡ªä¿¡ï¼Œæå‡æ¨¡å‹è¡¨ç°', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½¿ç”¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒåè¡¨ç°å‡ºçš„è¿‡åº¦è‡ªä¿¡ç°è±¡ã€‚ç ”ç©¶å‘ç°ï¼Œå¥–åŠ±æ¨¡å‹åœ¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ä¸­å¯¹é«˜ç½®ä¿¡åº¦è¯„åˆ†å­˜åœ¨å›ºæœ‰åè§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§æ–°çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–å˜ä½“ï¼šPPO-Må’ŒPPO-Cï¼Œåˆ†åˆ«é€šè¿‡æ ¡å‡†å¥–åŠ±æ¨¡å‹å’Œæ ¡å‡†å¥–åŠ±è®¡ç®—æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ–¹æ³•èƒ½å¤Ÿå‡å°‘æ ¡å‡†è¯¯å·®ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.11843', 'title': 'From Commands to Prompts: LLM-based Semantic File System for AIOS', 'url': 'https://huggingface.co/papers/2410.11843', 'abstract': 'Large language models (LLMs) have demonstrated significant potential in the development of intelligent applications and systems such as LLM-based agents and agent operating systems (AIOS). However, when these applications and systems interact with the underlying file system, the file system still remains the traditional paradigm: reliant on manual navigation through precise commands. This paradigm poses a bottleneck to the usability of these systems as users are required to navigate complex folder hierarchies and remember cryptic file names. To address this limitation, we propose an LLM-based semantic file system ( LSFS ) for prompt-driven file management. Unlike conventional approaches, LSFS incorporates LLMs to enable users or agents to interact with files through natural language prompts, facilitating semantic file management. At the macro-level, we develop a comprehensive API set to achieve semantic file management functionalities, such as semantic file retrieval, file update monitoring and summarization, and semantic file rollback). At the micro-level, we store files by constructing semantic indexes for them, design and implement syscalls of different semantic operations (e.g., CRUD, group by, join) powered by vector database. Our experiments show that LSFS offers significant improvements over traditional file systems in terms of user convenience, the diversity of supported functions, and the accuracy and efficiency of file operations. Additionally, with the integration of LLM, our system enables more intelligent file management tasks, such as content summarization and version comparison, further enhancing its capabilities.', 'score': 1, 'issue_id': 161, 'pub_date': '2024-09-23', 'pub_date_card': {'ru': '23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 23', 'zh': '9æœˆ23æ—¥'}, 'hash': '68eb78dc881f88ae', 'authors': ['Zeru Shi', 'Kai Mei', 'Mingyu Jin', 'Yongye Su', 'Chaoji Zuo', 'Wenyue Hua', 'Wujiang Xu', 'Yujie Ren', 'Zirui Liu', 'Mengnan Du', 'Dong Deng', 'Yongfeng Zhang'], 'affiliations': ['Dalian University of Technology', 'EPFL', 'New Jersey Institute of Technology', 'Purdue University', 'Rutgers University', 'University of Minnesota'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.11843.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#data', '#agents', '#architecture', '#alignment'], 'emoji': 'ğŸ—‚ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ°Ğ¼Ğ¸: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LSFS). Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ°Ğ¼Ğ¸. LSFS Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ API Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑƒĞ´Ğ¾Ğ±ÑÑ‚Ğ²Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing File Management with Natural Language', 'desc': 'This paper introduces a new system called the LLM-based Semantic File System (LSFS) that enhances file management using large language models (LLMs). Unlike traditional file systems that require users to remember complex commands and navigate through folders, LSFS allows users to interact with files using natural language prompts. The system includes a comprehensive API for various semantic file management tasks, such as retrieval and monitoring, and utilizes semantic indexing and vector databases for efficient operations. Experiments show that LSFS significantly improves user convenience and the accuracy of file operations compared to conventional systems.'}, 'zh': {'title': 'è¯­ä¹‰æ–‡ä»¶ç®¡ç†ï¼Œæ™ºèƒ½åŒ–æœªæ¥', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ™ºèƒ½åº”ç”¨å’Œç³»ç»Ÿçš„å¼€å‘ä¸­å±•ç°äº†å·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä¼ ç»Ÿçš„æ–‡ä»¶ç³»ç»Ÿä»ä¾èµ–äºæ‰‹åŠ¨å¯¼èˆªå’Œç²¾ç¡®å‘½ä»¤ï¼Œè¿™é™åˆ¶äº†ç”¨æˆ·çš„ä½¿ç”¨ä½“éªŒã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºLLMçš„è¯­ä¹‰æ–‡ä»¶ç³»ç»Ÿï¼ˆLSFSï¼‰ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºä¸æ–‡ä»¶è¿›è¡Œäº¤äº’ã€‚LSFSé€šè¿‡æ„å»ºè¯­ä¹‰ç´¢å¼•å’Œå®ç°ä¸åŒçš„è¯­ä¹‰æ“ä½œç³»ç»Ÿè°ƒç”¨ï¼Œæä¾›äº†è¯­ä¹‰æ–‡ä»¶æ£€ç´¢ã€æ›´æ–°ç›‘æ§å’Œæ‘˜è¦ç­‰åŠŸèƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒLSFSåœ¨ç”¨æˆ·ä¾¿åˆ©æ€§ã€åŠŸèƒ½å¤šæ ·æ€§å’Œæ–‡ä»¶æ“ä½œçš„å‡†ç¡®æ€§ä¸æ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–‡ä»¶ç³»ç»Ÿã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (2)', '#agi (3)', '#alignment (5)', '#architecture (13)', '#audio (1)', '#benchmark (15)', '#cv (8)', '#data (4)', '#dataset (5)', '#diffusion (3)', '#ethics (4)', '#games (2)', '#graphs (5)', '#hallucinations (1)', '#healthcare (1)', '#inference (2)', '#interpretability (8)', '#leakage (1)', '#long_context (1)', '#low_resource', '#machine_translation (1)', '#math (1)', '#multilingual (2)', '#multimodal (5)', '#open_source (11)', '#optimization (11)', '#plp (3)', '#rag (2)', '#reasoning (6)', '#rl (3)', '#rlhf (2)', '#robotics (1)', '#science (4)', '#security', '#small_models (4)', '#story_generation', '#survey', '#synthetic (3)', '#training (16)', '#transfer_learning (3)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2024-10-17 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-17 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-17 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    