{
    "date": {
        "ru": "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 24",
        "zh": "2æœˆ24æ—¥"
    },
    "time_utc": "2025-02-24 07:10",
    "weekday": 0,
    "issue_id": 2367,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.14776",
            "title": "SurveyX: Academic Survey Automation via Large Language Models",
            "url": "https://huggingface.co/papers/2502.14776",
            "abstract": "Large Language Models (LLMs) have demonstrated exceptional comprehension capabilities and a vast knowledge base, suggesting that LLMs can serve as efficient tools for automated survey generation. However, recent research related to automated survey generation remains constrained by some critical limitations like finite context window, lack of in-depth content discussion, and absence of systematic evaluation frameworks. Inspired by human writing processes, we propose SurveyX, an efficient and organized system for automated survey generation that decomposes the survey composing process into two phases: the Preparation and Generation phases. By innovatively introducing online reference retrieval, a pre-processing method called AttributeTree, and a re-polishing process, SurveyX significantly enhances the efficacy of survey composition. Experimental evaluation results show that SurveyX outperforms existing automated survey generation systems in content quality (0.259 improvement) and citation quality (1.76 enhancement), approaching human expert performance across multiple evaluation dimensions. Examples of surveys generated by SurveyX are available on www.surveyx.cn",
            "score": 49,
            "issue_id": 2363,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "b2504554ef88d631",
            "authors": [
                "Xun Liang",
                "Jiawei Yang",
                "Yezhaohui Wang",
                "Chen Tang",
                "Zifan Zheng",
                "Simin Niu",
                "Shichao Song",
                "Hanyu Wang",
                "Bo Tang",
                "Feiyu Xiong",
                "Keming Mao",
                "Zhiyu li"
            ],
            "affiliations": [
                "Institute for Advanced Algorithms Research, Shanghai, China",
                "Northeastern University, Shenyang, China",
                "Renmin University of China, Beijing, China",
                "The University of Sydney, Sydney, Australia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14776.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#survey",
                    "#benchmark",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "SurveyX: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ SurveyX - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ½Ğ° Ğ´Ğ²Ğµ Ñ„Ğ°Ğ·Ñ‹: Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºÑƒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ğ¾Ğ¸ÑĞº ÑÑÑ‹Ğ»Ğ¾Ğº Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ AttributeTree. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ SurveyX Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ÑÑÑŒ Ğº ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²-Ğ»ÑĞ´ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ñ€ÑĞ´ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸ÑÑƒÑ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "SurveyX: Revolutionizing Automated Survey Generation with LLMs",
                    "desc": "This paper introduces SurveyX, a novel system for automated survey generation that leverages Large Language Models (LLMs) to improve the survey creation process. It addresses key limitations of previous methods by breaking down the process into two distinct phases: Preparation and Generation. SurveyX incorporates innovative techniques such as online reference retrieval and a pre-processing method called AttributeTree, which enhance the quality of the generated surveys. Experimental results demonstrate that SurveyX significantly outperforms existing systems in both content and citation quality, nearing the performance of human experts."
                },
                "zh": {
                    "title": "SurveyXï¼šé«˜æ•ˆçš„è‡ªåŠ¨è°ƒæŸ¥ç”Ÿæˆç³»ç»Ÿ",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç†è§£èƒ½åŠ›å’ŒçŸ¥è¯†åŸºç¡€æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå®ƒä»¬å¯ä»¥ä½œä¸ºè‡ªåŠ¨è°ƒæŸ¥ç”Ÿæˆçš„æœ‰æ•ˆå·¥å…·ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‡ªåŠ¨è°ƒæŸ¥ç”Ÿæˆç ”ç©¶å—åˆ°ä¸€äº›å…³é”®é™åˆ¶ï¼Œå¦‚æœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£ã€ç¼ºä¹æ·±å…¥çš„å†…å®¹è®¨è®ºå’Œç¼ºä¹ç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ã€‚æˆ‘ä»¬æå‡ºäº†SurveyXï¼Œä¸€ä¸ªé«˜æ•ˆä¸”æœ‰ç»„ç»‡çš„è‡ªåŠ¨è°ƒæŸ¥ç”Ÿæˆç³»ç»Ÿï¼Œå°†è°ƒæŸ¥ç¼–å†™è¿‡ç¨‹åˆ†ä¸ºå‡†å¤‡é˜¶æ®µå’Œç”Ÿæˆé˜¶æ®µã€‚é€šè¿‡å¼•å…¥åœ¨çº¿å‚è€ƒæ£€ç´¢ã€å±æ€§æ ‘é¢„å¤„ç†æ–¹æ³•å’Œé‡æ–°æ¶¦è‰²è¿‡ç¨‹ï¼ŒSurveyXæ˜¾è‘—æé«˜äº†è°ƒæŸ¥ç¼–å†™çš„æ•ˆç‡ï¼Œå¹¶åœ¨å†…å®¹è´¨é‡å’Œå¼•ç”¨è´¨é‡ä¸Šè¶…è¶Šäº†ç°æœ‰ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.13449",
            "title": "Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular Language Model",
            "url": "https://huggingface.co/papers/2502.13449",
            "abstract": "Understanding molecules is key to understanding organisms and driving advances in drug discovery, requiring interdisciplinary knowledge across chemistry and biology. Although large molecular language models have achieved notable success in interpreting molecular structures, their instruction datasets are limited to the specific knowledge from task-oriented datasets and do not fully cover the fundamental characteristics of molecules, hindering their abilities as general-purpose molecular assistants. To address this issue, we propose Mol-LLaMA, a large molecular language model that grasps the general knowledge centered on molecules via multi-modal instruction tuning. To this end, we design key data types that encompass the fundamental features of molecules, incorporating essential knowledge from molecular structures. In addition, to improve understanding of molecular features, we introduce a module that integrates complementary information from different molecular encoders, leveraging the distinct advantages of different molecular representations. Our experimental results demonstrate that Mol-LLaMA is capable of comprehending the general features of molecules and generating relevant responses to users' queries with detailed explanations, implying its potential as a general-purpose assistant for molecular analysis.",
            "score": 24,
            "issue_id": 2363,
            "pub_date": "2025-02-19",
            "pub_date_card": {
                "ru": "19 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 19",
                "zh": "2æœˆ19æ—¥"
            },
            "hash": "e52b99ade1ae590a",
            "authors": [
                "Dongki Kim",
                "Wonbin Lee",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai",
                "Korea Advanced Institute of Science and Technology (KAIST), Seoul, South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.13449.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#science",
                    "#agi",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Mol-LLaMA: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¯Ğœ Ğ´Ğ»Ñ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Mol-LLaMA - Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ», Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ», Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ½ĞµĞ´Ñ€Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ». Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Mol-LLaMA ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Mol-LLaMA: A General-Purpose Assistant for Molecular Understanding",
                    "desc": "This paper introduces Mol-LLaMA, a large molecular language model designed to enhance understanding of molecular structures for drug discovery. Unlike previous models that relied on limited task-specific datasets, Mol-LLaMA utilizes multi-modal instruction tuning to incorporate a broader range of fundamental molecular knowledge. The model integrates various molecular encoders to leverage their unique strengths, improving its ability to interpret molecular features. Experimental results show that Mol-LLaMA can effectively respond to user queries with detailed explanations, positioning it as a versatile tool for molecular analysis."
                },
                "zh": {
                    "title": "Mol-LLaMAï¼šåˆ†å­åˆ†æçš„é€šç”¨åŠ©æ‰‹",
                    "desc": "ç†è§£åˆ†å­å¯¹äºç†è§£ç”Ÿç‰©ä½“å’Œæ¨åŠ¨è¯ç‰©å‘ç°è‡³å…³é‡è¦ã€‚è™½ç„¶å¤§å‹åˆ†å­è¯­è¨€æ¨¡å‹åœ¨è§£æåˆ†å­ç»“æ„æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å®ƒä»¬çš„è®­ç»ƒæ•°æ®é›†ä»…é™äºç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ï¼Œæœªèƒ½å…¨é¢è¦†ç›–åˆ†å­çš„åŸºæœ¬ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Mol-LLaMAï¼Œä¸€ä¸ªé€šè¿‡å¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜æ¥æŒæ¡åˆ†å­ä¸­å¿ƒçš„é€šç”¨çŸ¥è¯†çš„å¤§å‹åˆ†å­è¯­è¨€æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMol-LLaMAèƒ½å¤Ÿç†è§£åˆ†å­çš„æ™®éç‰¹å¾ï¼Œå¹¶ç”Ÿæˆä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„è¯¦ç»†è§£é‡Šï¼Œæ˜¾ç¤ºå‡ºå…¶ä½œä¸ºåˆ†å­åˆ†æé€šç”¨åŠ©æ‰‹çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14922",
            "title": "SIFT: Grounding LLM Reasoning in Contexts via Stickers",
            "url": "https://huggingface.co/papers/2502.14922",
            "abstract": "This paper identifies the misinterpretation of the context can be a significant issue during the reasoning process of large language models, spanning from smaller models like Llama3.2-3B-Instruct to cutting-edge ones like DeepSeek-R1. For example, in the phrase \"10 dollars per kilo,\" LLMs might not recognize that \"per\" means \"for each,\" leading to calculation errors. We introduce a novel, post-training approach called **Stick to the Facts (SIFT)** to tackle this. SIFT leverages increasing inference-time compute to ground LLM reasoning in contexts. At the core of SIFT lies the *Sticker*, which is generated by the model itself to explicitly emphasize the key information within the context. Given the curated Sticker, SIFT generates two predictions -- one from the original query and one from the query augmented with the Sticker. If they differ, the Sticker is sequentially refined via *forward* optimization (to better align the extracted facts with the query) and *inverse* generation (to conform with the model's inherent tendencies) for more faithful reasoning outcomes. Studies across diverse models (from 3B to 100B+) and benchmarks (e.g., GSM8K, MATH-500) reveal consistent performance improvements. Notably, SIFT improves the pass@1 accuracy of DeepSeek-R1 on AIME2024 from 78.33% to **85.67**%, establishing a new state-of-the-art in the open-source community. The code is available at https://github.com/zhijie-group/SIFT.",
            "score": 11,
            "issue_id": 2363,
            "pub_date": "2025-02-19",
            "pub_date_card": {
                "ru": "19 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 19",
                "zh": "2æœˆ19æ—¥"
            },
            "hash": "b5ab16112068f00f",
            "authors": [
                "Zihao Zeng",
                "Xuyao Huang",
                "Boxiu Li",
                "Zhijie Deng"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14922.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#inference",
                    "#math",
                    "#training",
                    "#reasoning",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "SIFT: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SIFT (Stick to the Facts) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). SIFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 'Sticker' - ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with Stick to the Facts (SIFT)",
                    "desc": "This paper addresses the problem of context misinterpretation in large language models (LLMs), which can lead to reasoning errors. It introduces a new post-training method called **Stick to the Facts (SIFT)** that enhances LLM reasoning by grounding it in context. SIFT utilizes a self-generated *Sticker* to highlight crucial information, allowing the model to produce two predictions for comparison. The method shows significant improvements in accuracy across various models and benchmarks, achieving a new state-of-the-art performance in the open-source community."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹æ¨ç†å‡†ç¡®æ€§çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æŒ‡å‡ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹ä¸Šä¸‹æ–‡çš„è¯¯è§£å¯èƒ½ä¼šå¯¼è‡´æ˜¾è‘—é—®é¢˜ã€‚æ¯”å¦‚ï¼Œåœ¨â€œæ¯å…¬æ–¤10ç¾å…ƒâ€è¿™ä¸ªçŸ­è¯­ä¸­ï¼Œæ¨¡å‹å¯èƒ½æ— æ³•æ­£ç¡®ç†è§£â€œæ¯â€çš„æ„æ€ï¼Œä»è€Œå¯¼è‡´è®¡ç®—é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åè®­ç»ƒæ–¹æ³•ï¼Œç§°ä¸º**Stick to the Facts (SIFT)**ï¼Œå®ƒé€šè¿‡å¢åŠ æ¨ç†æ—¶çš„è®¡ç®—é‡æ¥å¢å¼ºæ¨¡å‹çš„ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€‚SIFTçš„æ ¸å¿ƒæ˜¯ç”±æ¨¡å‹ç”Ÿæˆçš„*Sticker*ï¼Œå®ƒå¼ºè°ƒäº†ä¸Šä¸‹æ–‡ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.12084",
            "title": "VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues",
            "url": "https://huggingface.co/papers/2502.12084",
            "abstract": "Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce VLM^2-Bench, a benchmark designed to assess whether VLMs can Visually Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with further analysis of various language-side and vision-side prompting methods, leads to a total of eight key findings. We identify critical challenges in models' ability to link visual cues, highlighting a significant performance gap where even GPT-4o lags 34.80% behind humans. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models' ability to independently structure and infer relationships among visual cues.",
            "score": 8,
            "issue_id": 2366,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "7af295c37b147c3a",
            "authors": [
                "Jianshu Zhang",
                "Dongyu Yao",
                "Renjie Pi",
                "Paul Pu Liang",
                "Yi R.",
                "Fung"
            ],
            "affiliations": [
                "CMU",
                "HKUST",
                "MIT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.12084.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#alignment",
                    "#interpretability",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VLM^2-Bench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 9 Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 3000 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… VLM Ğ¸ GPT-4o. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ³Ğ´Ğµ Ğ´Ğ°Ğ¶Ğµ GPT-4o Ğ¾Ñ‚ÑÑ‚Ğ°ĞµÑ‚ Ğ½Ğ° 34.80%. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµÑ‚ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing Visual Linking in VLMs",
                    "desc": "This paper introduces VLM^2-Bench, a new benchmark aimed at evaluating the ability of vision-language models (VLMs) to visually link matching cues, which is essential for tasks like recognizing the same person in different images. The benchmark consists of 9 subtasks and over 3,000 test cases, providing a comprehensive assessment of VLM performance. The evaluation reveals significant challenges, with models like GPT-4o showing a 34.80% performance gap compared to human capabilities. The authors suggest improvements in visual processing, clearer integration of language reasoning, and a shift in training paradigms to enhance models' ability to understand and relate visual cues independently."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„åŒ¹é…èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰é“¾æ¥åŒ¹é…çº¿ç´¢æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶è€…ä»¬æå‡ºäº†VLM^2-BenchåŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨è¯†åˆ«ç›¸åŒå¯¹è±¡æ—¶çš„è¡¨ç°ã€‚é€šè¿‡å¯¹å…«ä¸ªå¼€æºVLMå’ŒGPT-4oçš„å…¨é¢è¯„ä¼°ï¼Œå‘ç°æ¨¡å‹åœ¨é“¾æ¥è§†è§‰çº¿ç´¢æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œç”šè‡³GPT-4oæ¯”äººç±»ä½34.80%ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œä½œè€…å»ºè®®å¢å¼ºæ¨¡å‹çš„æ ¸å¿ƒè§†è§‰èƒ½åŠ›ï¼Œæ˜ç¡®è¯­è¨€æ¨ç†ä¸è§†è§‰ä»»åŠ¡çš„æ•´åˆåŸåˆ™ï¼Œå¹¶æ¨åŠ¨è§†è§‰-æ–‡æœ¬è®­ç»ƒèŒƒå¼çš„è½¬å˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15589",
            "title": "LightThinker: Thinking Step-by-Step Compression",
            "url": "https://huggingface.co/papers/2502.15589",
            "abstract": "Large language models (LLMs) have shown remarkable performance in complex reasoning tasks, but their efficiency is hindered by the substantial memory and computational costs associated with generating lengthy tokens. In this paper, we propose LightThinker, a novel method that enables LLMs to dynamically compress intermediate thoughts during reasoning. Inspired by human cognitive processes, LightThinker compresses verbose thought steps into compact representations and discards the original reasoning chains, thereby significantly reducing the number of tokens stored in the context window. This is achieved by training the model on when and how to perform compression through data construction, mapping hidden states to condensed gist tokens, and creating specialized attention masks. Additionally, we introduce the Dependency (Dep) metric to quantify the degree of compression by measuring the reliance on historical tokens during generation. Extensive experiments on four datasets and two models show that LightThinker reduces peak memory usage and inference time, while maintaining competitive accuracy. Our work provides a new direction for improving the efficiency of LLMs in complex reasoning tasks without sacrificing performance. Code will be released at https://github.com/zjunlp/LightThinker.",
            "score": 8,
            "issue_id": 2365,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 21",
                "zh": "2æœˆ21æ—¥"
            },
            "hash": "563a32fe7bab988e",
            "authors": [
                "Jintian Zhang",
                "Yuqi Zhu",
                "Mengshu Sun",
                "Yujie Luo",
                "Shuofei Qiao",
                "Lun Du",
                "Da Zheng",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University",
                "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15589.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#architecture",
                    "#reasoning",
                    "#long_context",
                    "#optimization",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LightThinker: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "LightThinker - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (LLM) Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ñ‹ÑĞ»Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¼ Ğ¾ĞºĞ½Ğµ. LightThinker Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ¼Ñƒ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ ĞºĞ°Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ, Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ² ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹-ÑÑƒÑ‚ÑŒ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ°ÑĞºĞ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¸ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "LightThinker: Efficient Reasoning through Dynamic Thought Compression",
                    "desc": "This paper introduces LightThinker, a method designed to enhance the efficiency of large language models (LLMs) during complex reasoning tasks. By dynamically compressing intermediate thoughts, LightThinker reduces the memory and computational costs associated with generating lengthy tokens. The approach mimics human cognitive processes by transforming verbose reasoning into compact representations, which helps in minimizing the number of tokens stored. The authors also present a new metric, Dependency (Dep), to measure the effectiveness of this compression, demonstrating that LightThinker can lower memory usage and inference time while maintaining accuracy across various datasets."
                },
                "zh": {
                    "title": "LightThinkerï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡çš„æ–°æ–¹æ³•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç”Ÿæˆé•¿æ–‡æœ¬æ—¶çš„å†…å­˜å’Œè®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•LightThinkerï¼Œèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€å‹ç¼©ä¸­é—´æ€ç»´ã€‚LightThinkerå€Ÿé‰´äººç±»è®¤çŸ¥è¿‡ç¨‹ï¼Œå°†å†—é•¿çš„æ€ç»´æ­¥éª¤å‹ç¼©ä¸ºç´§å‡‘çš„è¡¨ç¤ºï¼Œä»è€Œæ˜¾è‘—å‡å°‘ä¸Šä¸‹æ–‡çª—å£ä¸­å­˜å‚¨çš„æ ‡è®°æ•°é‡ã€‚é€šè¿‡åœ¨æ•°æ®æ„å»ºä¸­è®­ç»ƒæ¨¡å‹è¿›è¡Œå‹ç¼©ï¼Œå¹¶å¼•å…¥ä¾èµ–åº¦ï¼ˆDepï¼‰æŒ‡æ ‡æ¥é‡åŒ–å‹ç¼©ç¨‹åº¦ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜LightThinkeråœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œé™ä½äº†å³°å€¼å†…å­˜ä½¿ç”¨å’Œæ¨ç†æ—¶é—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14494",
            "title": "StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following",
            "url": "https://huggingface.co/papers/2502.14494",
            "abstract": "Multi-turn instruction following capability constitutes a core competency of large language models (LLMs) in real-world applications. Existing evaluation benchmarks predominantly focus on fine-grained constraint satisfaction and domain-specific capability assessment, yet overlook the crucial structural dependency between dialogue turns that distinguishes multi-turn from single-turn interactions. This structural dependency not only reflects user intent but also establishes a second dimension for instruction following evaluation beyond constraint satisfaction. To address this gap, we propose StructFlowBench, a multi-turn instruction following benchmark with structural flow modeling. The benchmark innovatively defines a structural flow framework comprising six fundamental inter-turn relationships, which not only introduces novel structural constraints for model evaluation but also serves as generation parameters for creating customized dialogue flows tailored to specific scenarios. Adopting established LLM-based automatic evaluation methodologies, we conduct systematic evaluations of 13 leading open-source and closed-source LLMs. Experimental results reveal significant deficiencies in current models' comprehension of multi-turn dialogue structures. The code is available at https://github.com/MLGroupJLU/StructFlowBench.",
            "score": 8,
            "issue_id": 2365,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "512d952f8463678a",
            "authors": [
                "Jinnan Li",
                "Jinzhe Li",
                "Yue Wang",
                "Yi Chang",
                "Yuan Wu"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Jilin University",
                "Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China",
                "International Center of Future Science, Jilin University",
                "School of Artificial Intelligence, Jilin University",
                "School of Information and Library Science, University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14494.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ LLM Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº StructFlowBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², StructFlowBench ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ ÑˆĞµÑÑ‚ÑŒ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµĞ¶Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 13 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Multi-Turn Dialogue Understanding in LLMs",
                    "desc": "This paper introduces StructFlowBench, a new benchmark designed to evaluate the multi-turn instruction following capabilities of large language models (LLMs). It highlights the importance of understanding the structural dependencies between dialogue turns, which are often ignored in existing benchmarks. By defining six fundamental inter-turn relationships, StructFlowBench provides a framework for assessing how well models can follow instructions across multiple dialogue turns. The study reveals that many current LLMs struggle with these structural aspects, indicating a need for improvement in their dialogue comprehension."
                },
                "zh": {
                    "title": "å¤šè½®å¯¹è¯è¯„ä¼°çš„æ–°æ ‡å‡†",
                    "desc": "æœ¬æ–‡æå‡ºäº†StructFlowBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šè½®æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å¡«è¡¥ç°æœ‰è¯„ä¼°æ–¹æ³•çš„ç©ºç™½ã€‚ç°æœ‰çš„è¯„ä¼°ä¸»è¦å…³æ³¨ç»†ç²’åº¦çš„çº¦æŸæ»¡è¶³å’Œç‰¹å®šé¢†åŸŸèƒ½åŠ›è¯„ä¼°ï¼Œä½†å¿½è§†äº†å¯¹è¯è½®æ¬¡ä¹‹é—´çš„ç»“æ„ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªç»“æ„æµæ¡†æ¶ï¼ŒåŒ…å«å…­ç§åŸºæœ¬çš„è½®æ¬¡å…³ç³»ï¼Œä»¥æ­¤ä¸ºæ¨¡å‹è¯„ä¼°å¼•å…¥æ–°çš„ç»“æ„çº¦æŸã€‚é€šè¿‡å¯¹13ä¸ªé¢†å…ˆçš„å¼€æºå’Œé—­æºå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç³»ç»Ÿè¯„ä¼°ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå½“å‰æ¨¡å‹åœ¨ç†è§£å¤šè½®å¯¹è¯ç»“æ„æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14397",
            "title": "PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data",
            "url": "https://huggingface.co/papers/2502.14397",
            "abstract": "We introduce PhotoDoodle, a novel image editing framework designed to facilitate photo doodling by enabling artists to overlay decorative elements onto photographs. Photo doodling is challenging because the inserted elements must appear seamlessly integrated with the background, requiring realistic blending, perspective alignment, and contextual coherence. Additionally, the background must be preserved without distortion, and the artist's unique style must be captured efficiently from limited training data. These requirements are not addressed by previous methods that primarily focus on global style transfer or regional inpainting. The proposed method, PhotoDoodle, employs a two-stage training strategy. Initially, we train a general-purpose image editing model, OmniEditor, using large-scale data. Subsequently, we fine-tune this model with EditLoRA using a small, artist-curated dataset of before-and-after image pairs to capture distinct editing styles and techniques. To enhance consistency in the generated results, we introduce a positional encoding reuse mechanism. Additionally, we release a PhotoDoodle dataset featuring six high-quality styles. Extensive experiments demonstrate the advanced performance and robustness of our method in customized image editing, opening new possibilities for artistic creation.",
            "score": 7,
            "issue_id": 2364,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "7e08e4606569ffb5",
            "authors": [
                "Shijie Huang",
                "Yiren Song",
                "Yuxuan Zhang",
                "Hailong Guo",
                "Xueyin Wang",
                "Mike Zheng Shou",
                "Jiaming Liu"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Byte Dance",
                "National University of Singapore",
                "Shanghai Jiao Tong University",
                "Tiamat"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14397.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "PhotoDoodle: ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹",
                    "desc": "PhotoDoodle - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ñ…ÑƒĞ´Ğ¾Ğ¶Ğ½Ğ¸ĞºĞ°Ğ¼ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞºĞ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ OmniEditor, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ½Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ…ÑƒĞ´Ğ¾Ğ¶Ğ½Ğ¸ĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ EditLoRA. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ PhotoDoodle Ñ ÑˆĞµÑÑ‚ÑŒÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ¸Ğ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Seamless Artistic Integration with PhotoDoodle",
                    "desc": "PhotoDoodle is an innovative image editing framework that allows artists to add decorative elements to photographs while ensuring they blend seamlessly with the background. The challenge lies in maintaining realistic integration, perspective alignment, and preserving the original photo without distortion. To achieve this, PhotoDoodle uses a two-stage training approach, starting with a general image editing model and then fine-tuning it with a small dataset of artist-specific edits. The framework also introduces a positional encoding reuse mechanism to improve the consistency of the edited images, showcasing its effectiveness in customized artistic creation."
                },
                "zh": {
                    "title": "PhotoDoodleï¼šè‰ºæœ¯åˆ›ä½œçš„æ–°å¯èƒ½æ€§",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPhotoDoodleçš„æ–°å‹å›¾åƒç¼–è¾‘æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©è‰ºæœ¯å®¶åœ¨ç…§ç‰‡ä¸Šå åŠ è£…é¥°å…ƒç´ ã€‚PhotoDoodleè§£å†³äº†å›¾åƒåˆæˆä¸­çš„å¤šä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å…ƒç´ ä¸èƒŒæ™¯çš„æ— ç¼èåˆã€é€è§†å¯¹é½å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆä½¿ç”¨å¤§è§„æ¨¡æ•°æ®è®­ç»ƒé€šç”¨å›¾åƒç¼–è¾‘æ¨¡å‹OmniEditorï¼Œç„¶åé€šè¿‡å°å‹è‰ºæœ¯å®¶ç­–åˆ’çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œä»¥æ•æ‰ç‹¬ç‰¹çš„ç¼–è¾‘é£æ ¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPhotoDoodleåœ¨å®šåˆ¶å›¾åƒç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†è‰ºæœ¯åˆ›ä½œçš„æ–°å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.11663",
            "title": "MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction",
            "url": "https://huggingface.co/papers/2502.11663",
            "abstract": "World models that forecast environmental changes from actions are vital for autonomous driving models with strong generalization. The prevailing driving world model mainly build on video prediction model. Although these models can produce high-fidelity video sequences with advanced diffusion-based generator, they are constrained by their predictive duration and overall generalization capabilities. In this paper, we explore to solve this problem by combining generation loss with MAE-style feature-level context learning. In particular, we instantiate this target with three key design: (1) A more scalable Diffusion Transformer (DiT) structure trained with extra mask construction task. (2) we devise diffusion-related mask tokens to deal with the fuzzy relations between mask reconstruction and generative diffusion process. (3) we extend mask construction task to spatial-temporal domain by utilizing row-wise mask for shifted self-attention rather than masked self-attention in MAE. Then, we adopt a row-wise cross-view module to align with this mask design. Based on above improvement, we propose MaskGWM: a Generalizable driving World Model embodied with Video Mask reconstruction. Our model contains two variants: MaskGWM-long, focusing on long-horizon prediction, and MaskGWM-mview, dedicated to multi-view generation. Comprehensive experiments on standard benchmarks validate the effectiveness of the proposed method, which contain normal validation of Nuscene dataset, long-horizon rollout of OpenDV-2K dataset and zero-shot validation of Waymo dataset. Quantitative metrics on these datasets show our method notably improving state-of-the-art driving world model.",
            "score": 4,
            "issue_id": 2367,
            "pub_date": "2025-02-17",
            "pub_date_card": {
                "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 17",
                "zh": "2æœˆ17æ—¥"
            },
            "hash": "14ade1d8007cfa16",
            "authors": [
                "Jingcheng Ni",
                "Yuxin Guo",
                "Yichen Liu",
                "Rui Chen",
                "Lewei Lu",
                "Zehuan Wu"
            ],
            "affiliations": [
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.11663.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#architecture",
                    "#long_context",
                    "#games",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "MaskGWM: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MaskGWM Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² ÑÑ‚Ğ¸Ğ»Ğµ MAE. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Diffusion Transformer, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑĞ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Autonomous Driving with MaskGWM: A New Era in World Models",
                    "desc": "This paper presents a new approach to creating world models for autonomous driving that can better predict environmental changes over time. The authors introduce MaskGWM, which combines advanced video generation techniques with feature-level context learning to enhance generalization. Key innovations include a scalable Diffusion Transformer architecture, the use of diffusion-related mask tokens, and an extension of mask construction to the spatial-temporal domain. Experimental results demonstrate that MaskGWM significantly outperforms existing models in various driving scenarios, showcasing its effectiveness in long-horizon and multi-view predictions."
                },
                "zh": {
                    "title": "æå‡é©¾é©¶æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„é©¾é©¶ä¸–ç•Œæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆç”ŸæˆæŸå¤±å’ŒMAEé£æ ¼çš„ç‰¹å¾çº§ä¸Šä¸‹æ–‡å­¦ä¹ æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æ‰©æ•£å˜æ¢å™¨ç»“æ„ï¼Œå¹¶å¼•å…¥äº†æ‰©æ•£ç›¸å…³çš„æ©ç æ ‡è®°ï¼Œä»¥å¤„ç†æ©ç é‡å»ºä¸ç”Ÿæˆæ‰©æ•£è¿‡ç¨‹ä¹‹é—´çš„æ¨¡ç³Šå…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ©ç æ„å»ºä»»åŠ¡æ‰©å±•åˆ°æ—¶ç©ºåŸŸï¼Œé‡‡ç”¨è¡Œçº§æ©ç è¿›è¡Œåç§»è‡ªæ³¨æ„åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MaskGWMæ¨¡å‹åœ¨å¤šä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†é©¾é©¶ä¸–ç•Œæ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15631",
            "title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
            "url": "https://huggingface.co/papers/2502.15631",
            "abstract": "Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token usage and accuracy gains. In particular, when comparing models across generations, it is unclear whether improved performance results from longer reasoning chains or more efficient reasoning. We systematically analyze chain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH benchmark, finding that o3-mini (m) achieves superior accuracy without requiring longer reasoning chains than o1-mini. Moreover, we show that accuracy generally declines as reasoning chains grow across all models and compute settings, even when controlling for difficulty of the questions. This accuracy drop is significantly smaller in more proficient models, suggesting that new generations of reasoning models use test-time compute more effectively. Finally, we highlight that while o3-mini (h) achieves a marginal accuracy gain over o3-mini (m), it does so by allocating substantially more reasoning tokens across all problems, even the ones that o3-mini (m) can already solve. These findings provide new insights into the relationship between model capability and reasoning length, with implications for efficiency, scaling, and evaluation methodologies.",
            "score": 3,
            "issue_id": 2363,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 21",
                "zh": "2æœˆ21æ—¥"
            },
            "hash": "6f204197089fd2e2",
            "authors": [
                "Marthe Ballon",
                "Andres Algaba",
                "Vincent Ginis"
            ],
            "affiliations": [
                "Data Analytics Lab, Vrije Universiteit Brussel, 1050 Brussel, Belgium",
                "School of Engineering and Applied Sciences, Harvard University, Cambridge, Massachusetts 02138, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15631.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#math",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ¸Ñ… Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ ÑƒĞ´Ğ»Ğ¸Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ¾ ÑÑ‚Ğ¾ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñƒ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Efficiency Over Length: Unlocking Better Reasoning in Language Models",
                    "desc": "This paper investigates how the length of reasoning chains in large language models affects their accuracy in mathematical reasoning tasks. It compares two model variants, o1-mini and o3-mini, on the Omni-MATH benchmark, revealing that o3-mini achieves better accuracy without needing longer reasoning chains. The study finds that longer reasoning chains often lead to decreased accuracy, especially across various models and question difficulties. Additionally, it suggests that newer models are more efficient in using reasoning tokens, which has important implications for model evaluation and scaling."
                },
                "zh": {
                    "title": "æ¨ç†é“¾é•¿åº¦ä¸æ¨¡å‹å‡†ç¡®æ€§çš„å…³ç³»",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯æ¨ç†é“¾çš„é•¿åº¦ä¸å‡†ç¡®æ€§ä¹‹é—´çš„å…³ç³»ã€‚ç ”ç©¶å‘ç°ï¼Œo3-miniæ¨¡å‹åœ¨ä¸éœ€è¦æ›´é•¿æ¨ç†é“¾çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå®ç°æ›´é«˜çš„å‡†ç¡®ç‡ã€‚è®ºæ–‡è¿˜æŒ‡å‡ºï¼Œéšç€æ¨ç†é“¾çš„å¢é•¿ï¼Œæ‰€æœ‰æ¨¡å‹çš„å‡†ç¡®æ€§æ™®éä¸‹é™ï¼Œä½†åœ¨æ›´é«˜æ•ˆçš„æ¨¡å‹ä¸­ï¼Œè¿™ç§ä¸‹é™çš„å¹…åº¦è¾ƒå°ã€‚æœ€åï¼Œå°½ç®¡o3-mini(h)åœ¨å‡†ç¡®æ€§ä¸Šç•¥æœ‰æå‡ï¼Œä½†å®ƒéœ€è¦åˆ†é…æ›´å¤šçš„æ¨ç†æ ‡è®°ï¼Œè¿™è¡¨æ˜æ–°ä¸€ä»£æ¨¡å‹åœ¨æµ‹è¯•æ—¶çš„è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15007",
            "title": "LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers",
            "url": "https://huggingface.co/papers/2502.15007",
            "abstract": "We introduce methods to quantify how Large Language Models (LLMs) encode and store contextual information, revealing that tokens often seen as minor (e.g., determiners, punctuation) carry surprisingly high context. Notably, removing these tokens -- especially stopwords, articles, and commas -- consistently degrades performance on MMLU and BABILong-4k, even if removing only irrelevant tokens. Our analysis also shows a strong correlation between contextualization and linearity, where linearity measures how closely the transformation from one layer's embeddings to the next can be approximated by a single linear mapping. These findings underscore the hidden importance of filler tokens in maintaining context. For further exploration, we present LLM-Microscope, an open-source toolkit that assesses token-level nonlinearity, evaluates contextual memory, visualizes intermediate layer contributions (via an adapted Logit Lens), and measures the intrinsic dimensionality of representations. This toolkit illuminates how seemingly trivial tokens can be critical for long-range understanding.",
            "score": 2,
            "issue_id": 2367,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "9be244de8b366352",
            "authors": [
                "Anton Razzhigaev",
                "Matvey Mikhalchuk",
                "Temurbek Rahmatullaev",
                "Elizaveta Goncharova",
                "Polina Druzhinina",
                "Ivan Oseledets",
                "Andrey Kuznetsov"
            ],
            "affiliations": [
                "AIRI",
                "HSE University",
                "Lomonosov Moscow State University",
                "Skoltech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15007.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#architecture",
                    "#long_context",
                    "#interpretability",
                    "#training",
                    "#open_source"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑĞ¸Ğ»Ğ° Ğ¼ĞµĞ»Ğ¾Ñ‡ĞµĞ¹: ĞºĞ°Ğº Ğ½ĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ Ñ…Ñ€Ğ°Ğ½ÑÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ°Ñ€Ñ‚Ğ¸ĞºĞ»Ğ¸ Ğ¸ Ğ¿ÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ğ¸Ñ) Ğ½ĞµÑÑƒÑ‚ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ… MMLU Ğ¸ BABILong-4k. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ÑÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Unveiling the Hidden Power of Minor Tokens in LLMs",
                    "desc": "This paper explores how Large Language Models (LLMs) encode contextual information, highlighting the significant role of seemingly minor tokens like stopwords and punctuation. The authors demonstrate that removing these tokens negatively impacts model performance on tasks such as MMLU and BABILong-4k, indicating their importance in maintaining context. They also find a correlation between the contextualization of tokens and the linearity of transformations between model layers, suggesting that the way information is processed is not purely linear. To aid further research, the authors introduce LLM-Microscope, a toolkit designed to analyze token-level nonlinearity and visualize the contributions of different layers in LLMs."
                },
                "zh": {
                    "title": "éšè—çš„é‡è¦æ€§ï¼šå¡«å……æ ‡è®°åœ¨ä¸Šä¸‹æ–‡ä¸­çš„å…³é”®ä½œç”¨",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†é‡åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•ç¼–ç å’Œå­˜å‚¨ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œé€šå¸¸è¢«è§†ä¸ºæ¬¡è¦çš„æ ‡è®°ï¼ˆå¦‚é™å®šè¯å’Œæ ‡ç‚¹ç¬¦å·ï¼‰å®é™…ä¸Šæ‰¿è½½ç€æ„æƒ³ä¸åˆ°çš„é«˜ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ç‰¹åˆ«æ˜¯ï¼Œå»é™¤è¿™äº›æ ‡è®°ï¼ˆå°¤å…¶æ˜¯åœç”¨è¯ã€å† è¯å’Œé€—å·ï¼‰ä¼šæ˜¾è‘—é™ä½MMLUå’ŒBABILong-4kçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ†æè¿˜è¡¨æ˜ï¼Œä¸Šä¸‹æ–‡åŒ–ä¸çº¿æ€§åº¦ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ï¼Œçº¿æ€§åº¦è¡¡é‡ä»ä¸€å±‚åµŒå…¥åˆ°ä¸‹ä¸€å±‚çš„è½¬æ¢æ˜¯å¦å¯ä»¥ç”¨å•ä¸€çº¿æ€§æ˜ å°„æ¥è¿‘ä¼¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15657",
            "title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?",
            "url": "https://huggingface.co/papers/2502.15657",
            "abstract": "The leading AI companies are increasingly focused on building generalist AI agents -- systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite how useful these systems might be, unchecked AI agency poses significant risks to public safety and security, ranging from misuse by malicious actors to a potentially irreversible loss of human control. We discuss how these risks arise from current AI training methods. Indeed, various scenarios and experiments have demonstrated the possibility of AI agents engaging in deception or pursuing goals that were not specified by human operators and that conflict with human interests, such as self-preservation. Following the precautionary principle, we see a strong need for safer, yet still useful, alternatives to the current agency-driven trajectory. Accordingly, we propose as a core building block for further advances the development of a non-agentic AI system that is trustworthy and safe by design, which we call Scientist AI. This system is designed to explain the world from observations, as opposed to taking actions in it to imitate or please humans. It comprises a world model that generates theories to explain data and a question-answering inference machine. Both components operate with an explicit notion of uncertainty to mitigate the risks of overconfident predictions. In light of these considerations, a Scientist AI could be used to assist human researchers in accelerating scientific progress, including in AI safety. In particular, our system can be employed as a guardrail against AI agents that might be created despite the risks involved. Ultimately, focusing on non-agentic AI may enable the benefits of AI innovation while avoiding the risks associated with the current trajectory. We hope these arguments will motivate researchers, developers, and policymakers to favor this safer path.",
            "score": 2,
            "issue_id": 2365,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 21",
                "zh": "2æœˆ21æ—¥"
            },
            "hash": "66434dc15bcc1913",
            "authors": [
                "Yoshua Bengio",
                "Michael Cohen",
                "Damiano Fornasiere",
                "Joumana Ghosn",
                "Pietro Greiner",
                "Matt MacDermott",
                "SÃ¶ren Mindermann",
                "Adam Oberman",
                "Jesse Richardson",
                "Oliver Richardson",
                "Marc-Antoine Rondeau",
                "Pierre-Luc St-Charles",
                "David Williams-King"
            ],
            "affiliations": [
                "Imperial College London",
                "McGill University",
                "Mila Quebec AI Institute",
                "Universite de Montreal",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15657.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#agents",
                    "#security",
                    "#science",
                    "#agi",
                    "#ethics"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¹ Ğ˜Ğ˜: Ğ¾Ñ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº ÑƒÑ‡ĞµĞ½Ñ‹Ğ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ñ€Ğ¸ÑĞºĞ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ½ĞµĞ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ˜Ğ˜ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Scientist AI, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ Ğ¼Ğ¸Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ½Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ² Ğ½ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Scientist AI Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¾Ñ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Towards Safer AI: Embracing Non-Agentic Systems",
                    "desc": "This paper discusses the risks associated with developing generalist AI agents that can operate autonomously across various tasks. It highlights how current AI training methods can lead to unintended behaviors, such as deception or pursuing harmful goals. To address these concerns, the authors propose a new approach called Scientist AI, which focuses on creating a non-agentic system that explains observations rather than taking actions. This system aims to assist human researchers while ensuring safety and trustworthiness, ultimately promoting a safer path for AI development."
                },
                "zh": {
                    "title": "å®‰å…¨ä¸åˆ›æ–°å¹¶é‡çš„ç§‘å­¦å®¶AI",
                    "desc": "æœ¬æ–‡è®¨è®ºäº†å½“å‰äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç³»ç»Ÿåœ¨è‡ªä¸»è§„åˆ’å’Œæ‰§è¡Œä»»åŠ¡æ–¹é¢çš„é£é™©ï¼Œå°¤å…¶æ˜¯å½“è¿™äº›ç³»ç»Ÿå¯èƒ½åç¦»äººç±»çš„æ„å›¾æ—¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„éä»£ç†AIç³»ç»Ÿï¼Œç§°ä¸ºç§‘å­¦å®¶AIï¼Œå®ƒæ—¨åœ¨é€šè¿‡è§‚å¯Ÿæ¥è§£é‡Šä¸–ç•Œï¼Œè€Œä¸æ˜¯ä¸»åŠ¨é‡‡å–è¡ŒåŠ¨ã€‚ç§‘å­¦å®¶AIåŒ…æ‹¬ä¸€ä¸ªä¸–ç•Œæ¨¡å‹å’Œä¸€ä¸ªé—®ç­”æ¨ç†æœºå™¨ï¼Œèƒ½å¤Ÿå¤„ç†ä¸ç¡®å®šæ€§ï¼Œä»è€Œå‡å°‘è¿‡äºè‡ªä¿¡çš„é¢„æµ‹å¸¦æ¥çš„é£é™©ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç§å®‰å…¨çš„AIç³»ç»Ÿèƒ½å¤Ÿå¸®åŠ©äººç±»ç ”ç©¶äººå‘˜åŠ é€Ÿç§‘å­¦è¿›æ­¥ï¼ŒåŒæ—¶ä½œä¸ºå¯¹æŠ—æ½œåœ¨å±é™©AIä»£ç†çš„ä¿æŠ¤æªæ–½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.14905",
            "title": "Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence",
            "url": "https://huggingface.co/papers/2502.14905",
            "abstract": "In this paper, we address the challenge of enforcing strict schema adherence in large language model (LLM) generation by leveraging LLM reasoning capabilities. Building on the DeepSeek R1 reinforcement learning framework, our approach trains structured reasoning skills of a 1.5B parameter model through a novel pipeline that combines synthetic reasoning dataset construction with custom reward functions under Group Relative Policy Optimization (GRPO). Specifically, we first perform R1 reinforcement learning on a 20K sample unstructured-to-structured dataset, mirroring the original DeepSeek R1 methods, to establish core reasoning abilities. Subsequently, we performed supervised fine-tuning on a separate 10K reasoning sample dataset, focusing on refining schema adherence for downstream tasks. Despite the relatively modest training scope, requiring approximately 20 hours on an 8xH100 GPU cluster for GRPO training and 3 hours on 1xA100 for SFT, our model demonstrates robust performance in enforcing schema consistency. We compare our ThinkJSON approach against the original DeepSeek R1 (671B), distilled versions of DeepSeek R1 (Qwen-1.5B and Qwen-7B), and Gemini 2.0 Flash (70B), showcasing its effectiveness in real-world applications. Our results underscore the practical utility of a resource-efficient framework for schema-constrained text generation.",
            "score": 2,
            "issue_id": 2363,
            "pub_date": "2025-02-18",
            "pub_date_card": {
                "ru": "18 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 18",
                "zh": "2æœˆ18æ—¥"
            },
            "hash": "e6a1c222ee43df08",
            "authors": [
                "Bhavik Agarwal",
                "Ishan Joshi",
                "Viktoria Rojkova"
            ],
            "affiliations": [
                "MasterControl AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.14905.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#synthetic",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ ÑÑ…ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° DeepSeek R1, Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 1,5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Group Relative Policy Optimization (GRPO). ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞºÑ€Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ…ĞµĞ¼Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑÑƒÑ€ÑĞ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ ÑÑ…ĞµĞ¼Ğµ."
                },
                "en": {
                    "title": "ThinkJSON: Efficient Schema-Constrained Text Generation with LLMs",
                    "desc": "This paper presents a method to improve how large language models (LLMs) follow specific rules or schemas when generating text. The authors use a reinforcement learning approach called DeepSeek R1 to train a 1.5 billion parameter model, enhancing its reasoning skills through a combination of synthetic data and custom reward systems. They first establish core reasoning abilities with a large unstructured dataset and then fine-tune the model on a smaller dataset focused on schema adherence. The results show that their method, named ThinkJSON, performs well in maintaining schema consistency compared to larger models, demonstrating an efficient way to generate structured text."
                },
                "zh": {
                    "title": "åˆ©ç”¨æ¨ç†èƒ½åŠ›å¼ºåŒ–æ¨¡å¼éµå¾ª",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸­å¼ºåˆ¶éµå¾ªä¸¥æ ¼æ¨¡å¼çš„æŒ‘æˆ˜ï¼Œåˆ©ç”¨äº†è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬åŸºäºDeepSeek R1å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè®­ç»ƒäº†ä¸€ä¸ªå…·æœ‰15äº¿å‚æ•°çš„æ¨¡å‹ï¼Œé€šè¿‡åˆæˆæ¨ç†æ•°æ®é›†æ„å»ºå’Œè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ï¼Œç»“åˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨ä¸€ä¸ª2ä¸‡æ ·æœ¬çš„éç»“æ„åŒ–åˆ°ç»“æ„åŒ–æ•°æ®é›†ä¸Šè¿›è¡ŒR1å¼ºåŒ–å­¦ä¹ ï¼Œä»¥å»ºç«‹æ ¸å¿ƒæ¨ç†èƒ½åŠ›ã€‚éšåï¼Œæˆ‘ä»¬åœ¨ä¸€ä¸ªå•ç‹¬çš„1ä¸‡æ¨ç†æ ·æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œäº†ç›‘ç£å¾®è°ƒï¼Œä¸“æ³¨äºæé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ¨¡å¼éµå¾ªæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15681",
            "title": "One-step Diffusion Models with $f$-Divergence Distribution Matching",
            "url": "https://huggingface.co/papers/2502.15681",
            "abstract": "Sampling from diffusion models involves a slow iterative process that hinders their practical deployment, especially for interactive applications. To accelerate generation speed, recent approaches distill a multi-step diffusion model into a single-step student generator via variational score distillation, which matches the distribution of samples generated by the student to the teacher's distribution. However, these approaches use the reverse Kullback-Leibler (KL) divergence for distribution matching which is known to be mode seeking. In this paper, we generalize the distribution matching approach using a novel f-divergence minimization framework, termed f-distill, that covers different divergences with different trade-offs in terms of mode coverage and training variance. We derive the gradient of the f-divergence between the teacher and student distributions and show that it is expressed as the product of their score differences and a weighting function determined by their density ratio. This weighting function naturally emphasizes samples with higher density in the teacher distribution, when using a less mode-seeking divergence. We observe that the popular variational score distillation approach using the reverse-KL divergence is a special case within our framework. Empirically, we demonstrate that alternative f-divergences, such as forward-KL and Jensen-Shannon divergences, outperform the current best variational score distillation methods across image generation tasks. In particular, when using Jensen-Shannon divergence, f-distill achieves current state-of-the-art one-step generation performance on ImageNet64 and zero-shot text-to-image generation on MS-COCO. Project page: https://research.nvidia.com/labs/genair/f-distill",
            "score": 1,
            "issue_id": 2364,
            "pub_date": "2025-02-21",
            "pub_date_card": {
                "ru": "21 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 21",
                "zh": "2æœˆ21æ—¥"
            },
            "hash": "b1dfe60e5b59d586",
            "authors": [
                "Yilun Xu",
                "Weili Nie",
                "Arash Vahdat"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15681.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ f-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ f-distill. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ f-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸ ĞšÑƒĞ»ÑŒĞ±Ğ°ĞºĞ°-Ğ›ĞµĞ¹Ğ±Ğ»ĞµÑ€Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ Ğ¸ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ f-distill Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Accelerating Diffusion Models with f-Distill for Faster Image Generation",
                    "desc": "This paper addresses the slow sampling process of diffusion models, which limits their use in real-time applications. It introduces a new method called f-distill that generalizes the distribution matching process using f-divergence minimization, allowing for better trade-offs between mode coverage and training variance. The authors derive a gradient expression for f-divergence that highlights samples with higher density in the teacher distribution, improving the quality of generated samples. Empirical results show that using f-distill with alternative divergences, particularly Jensen-Shannon divergence, leads to superior performance in image generation tasks compared to existing methods."
                },
                "zh": {
                    "title": "åŠ é€Ÿæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å¸ƒåŒ¹é…æ–¹æ³•ï¼Œç§°ä¸ºf-distillï¼Œæ—¨åœ¨åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆé€Ÿåº¦ã€‚é€šè¿‡å¼•å…¥f-æ•£åº¦æœ€å°åŒ–æ¡†æ¶ï¼Œf-distillèƒ½å¤Ÿå¤„ç†ä¸åŒçš„æ•£åº¦ï¼Œä»è€Œåœ¨æ¨¡å¼è¦†ç›–å’Œè®­ç»ƒæ–¹å·®ä¹‹é—´å–å¾—ä¸åŒçš„æƒè¡¡ã€‚æˆ‘ä»¬æ¨å¯¼äº†æ•™å¸ˆå’Œå­¦ç”Ÿåˆ†å¸ƒä¹‹é—´f-æ•£åº¦çš„æ¢¯åº¦ï¼Œå¹¶å±•ç¤ºäº†å…¶ä¸åˆ†æ•°å·®å¼‚å’Œå¯†åº¦æ¯”çš„åŠ æƒå‡½æ•°çš„å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Jensen-Shannonæ•£åº¦çš„f-distillåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„å˜åˆ†åˆ†æ•°è’¸é¦æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15027",
            "title": "InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback",
            "url": "https://huggingface.co/papers/2502.15027",
            "abstract": "Existing benchmarks do not test Large Multimodal Models (LMMs) on their interactive intelligence with human users which is vital for developing general-purpose AI assistants. We design InterFeedback, an interactive framework, which can be applied to any LMM and dataset to assess this ability autonomously. On top of this, we introduce InterFeedback-Bench which evaluates interactive intelligence using two representative datasets, MMMU-Pro and MathVerse, to test 10 different open-source LMMs. Additionally, we present InterFeedback-Human, a newly collected dataset of 120 cases designed for manually testing interactive performance in leading models such as OpenAI-o1 and Claude-3.5-Sonnet. Our evaluation results show that even state-of-the-art LMM (like OpenAI-o1) can correct their results through human feedback less than 50%. Our findings point to the need for methods that can enhance the LMMs' capability to interpret and benefit from feedback.",
            "score": 1,
            "issue_id": 2363,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "3b32932adf862766",
            "authors": [
                "Henry Hengyuan Zhao",
                "Wenqi Pei",
                "Yifei Tao",
                "Haiyang Mei",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15027.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#benchmark",
                    "#rlhf",
                    "#dataset",
                    "#agi",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ LMM: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº InterFeedback Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ InterFeedback-Bench Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 10 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LMM Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… InterFeedback-Human Ğ´Ğ»Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ LMM ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 50% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ²."
                },
                "en": {
                    "title": "Enhancing Interactive Intelligence in Large Multimodal Models",
                    "desc": "This paper addresses the gap in evaluating Large Multimodal Models (LMMs) regarding their interactive intelligence with human users. The authors introduce InterFeedback, a framework that autonomously assesses the interactive capabilities of any LMM across various datasets. They also present InterFeedback-Bench, which tests 10 open-source LMMs using two datasets, MMMU-Pro and MathVerse, and InterFeedback-Human, a dataset for manual evaluation of models like OpenAI-o1 and Claude-3.5-Sonnet. The results reveal that even advanced LMMs struggle to incorporate human feedback effectively, highlighting the need for improved methods to enhance their responsiveness to user interactions."
                },
                "zh": {
                    "title": "æå‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„äº’åŠ¨æ™ºèƒ½",
                    "desc": "ç°æœ‰çš„åŸºå‡†æµ‹è¯•å¹¶æœªè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ä¸äººç±»ç”¨æˆ·çš„äº’åŠ¨æ™ºèƒ½ï¼Œè€Œè¿™å¯¹äºå¼€å‘é€šç”¨äººå·¥æ™ºèƒ½åŠ©æ‰‹è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è®¾è®¡äº†InterFeedbackï¼Œè¿™æ˜¯ä¸€ä¸ªäº’åŠ¨æ¡†æ¶ï¼Œå¯ä»¥åº”ç”¨äºä»»ä½•LMMå’Œæ•°æ®é›†ï¼Œä»¥è‡ªä¸»è¯„ä¼°è¿™ç§èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å‡ºäº†InterFeedback-Benchï¼Œä½¿ç”¨ä¸¤ä¸ªä»£è¡¨æ€§æ•°æ®é›†MMM-Proå’ŒMathVerseæ¥è¯„ä¼°10ç§ä¸åŒçš„å¼€æºLMMçš„äº’åŠ¨æ™ºèƒ½ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„LMMï¼ˆå¦‚OpenAI-o1ï¼‰ä¹Ÿåªèƒ½é€šè¿‡äººç±»åé¦ˆçº æ­£ä¸åˆ°50%çš„ç»“æœï¼Œæ˜¾ç¤ºå‡ºå¢å¼ºLMMè§£è¯»å’Œåˆ©ç”¨åé¦ˆèƒ½åŠ›çš„æ–¹æ³•çš„å¿…è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15011",
            "title": "CrossOver: 3D Scene Cross-Modal Alignment",
            "url": "https://huggingface.co/papers/2502.15011",
            "abstract": "Multi-modal 3D object understanding has gained significant attention, yet current approaches often assume complete data availability and rigid alignment across all modalities. We present CrossOver, a novel framework for cross-modal 3D scene understanding via flexible, scene-level modality alignment. Unlike traditional methods that require aligned modality data for every object instance, CrossOver learns a unified, modality-agnostic embedding space for scenes by aligning modalities - RGB images, point clouds, CAD models, floorplans, and text descriptions - with relaxed constraints and without explicit object semantics. Leveraging dimensionality-specific encoders, a multi-stage training pipeline, and emergent cross-modal behaviors, CrossOver supports robust scene retrieval and object localization, even with missing modalities. Evaluations on ScanNet and 3RScan datasets show its superior performance across diverse metrics, highlighting adaptability for real-world applications in 3D scene understanding.",
            "score": 0,
            "issue_id": 2366,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "97fe61eb66853ca2",
            "authors": [
                "Sayan Deb Sarkar",
                "Ondrej Miksik",
                "Marc Pollefeys",
                "Daniel Barath",
                "Iro Armeni"
            ],
            "affiliations": [
                "ETH Zurich",
                "Microsoft Spatial AI Lab, Zurich",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15011.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#dataset",
                    "#3d",
                    "#training"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½",
                    "desc": "CrossOver - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑÑ†ĞµĞ½Ñ‹. ĞĞ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ†ĞµĞ½, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¶ĞµÑÑ‚ĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. CrossOver Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑÑ†ĞµĞ½ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "CrossOver: Flexible 3D Scene Understanding Across Modalities",
                    "desc": "This paper introduces CrossOver, a new framework designed for understanding 3D scenes using multiple types of data, such as images and point clouds. Unlike traditional methods that need all data types to be perfectly aligned, CrossOver allows for flexible alignment, making it easier to work with incomplete information. It creates a shared space where different data types can be compared and understood together, even if some data is missing. The results show that CrossOver performs better than existing methods, making it useful for real-world applications in 3D scene analysis."
                },
                "zh": {
                    "title": "è·¨æ¨¡æ€3Dåœºæ™¯ç†è§£çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCrossOverçš„æ–°æ¡†æ¶ï¼Œç”¨äºçµæ´»çš„è·¨æ¨¡æ€3Dåœºæ™¯ç†è§£ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒCrossOverä¸éœ€è¦æ¯ä¸ªå¯¹è±¡å®ä¾‹çš„æ¨¡æ€æ•°æ®ä¸¥æ ¼å¯¹é½ï¼Œè€Œæ˜¯é€šè¿‡æ”¾å®½çº¦æŸæ¥å­¦ä¹ ç»Ÿä¸€çš„æ¨¡æ€æ— å…³åµŒå…¥ç©ºé—´ã€‚è¯¥æ¡†æ¶æ”¯æŒRGBå›¾åƒã€ç‚¹äº‘ã€CADæ¨¡å‹ã€å¹³é¢å›¾å’Œæ–‡æœ¬æè¿°ç­‰å¤šç§æ¨¡æ€çš„å¯¹é½ï¼Œèƒ½å¤Ÿåœ¨ç¼ºå¤±æ¨¡æ€çš„æƒ…å†µä¸‹è¿›è¡Œç¨³å¥çš„åœºæ™¯æ£€ç´¢å’Œå¯¹è±¡å®šä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCrossOveråœ¨ScanNetå’Œ3RScanæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…3Dåœºæ™¯ç†è§£ä¸­çš„é€‚åº”æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.15082",
            "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
            "url": "https://huggingface.co/papers/2502.15082",
            "abstract": "User specifications or legal frameworks often require information to be removed from pretrained models, including large language models (LLMs). This requires deleting or \"forgetting\" a set of data points from an already-trained model, which typically degrades its performance on other data points. Thus, a balance must be struck between removing information and keeping the model's other abilities intact, with a failure to balance this trade-off leading to poor deletion or an unusable model. To this end, we propose UPCORE (Utility-Preserving Coreset Selection), a method-agnostic data selection framework for mitigating collateral damage during unlearning. Finding that the model damage is correlated with the variance of the model's representations on the forget set, we selectively prune the forget set to remove outliers, thereby minimizing model degradation after unlearning. We evaluate UPCORE across three standard unlearning methods consistently achieving a superior balance between the competing objectives of deletion efficacy and model preservation. To better evaluate this trade-off, we introduce a new metric, measuring the area-under-the-curve (AUC) across standard metrics. We find that UPCORE improves both standard metrics and AUC, benefitting from positive transfer between the coreset and pruned points while reducing negative transfer from the forget set to points outside of it.",
            "score": 0,
            "issue_id": 2365,
            "pub_date": "2025-02-20",
            "pub_date_card": {
                "ru": "20 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 20",
                "zh": "2æœˆ20æ—¥"
            },
            "hash": "4a0bc63404dc5d71",
            "authors": [
                "Vaidehi Patil",
                "Elias Stengel-Eskin",
                "Mohit Bansal"
            ],
            "affiliations": [
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.15082.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#leakage",
                    "#optimization",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ UPCORE Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. UPCORE Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° ÑƒĞ´Ğ°Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´Ğ¸ Ğ¿Ğ¾Ğ´ ĞºÑ€Ğ¸Ğ²Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ°."
                },
                "en": {
                    "title": "Smart Unlearning: Balancing Deletion and Model Integrity with UPCORE",
                    "desc": "This paper addresses the challenge of unlearning information from pretrained models, particularly large language models, without significantly harming their performance. The authors introduce UPCORE, a framework that selects a subset of data points to delete while minimizing the negative impact on the model's overall capabilities. By focusing on the variance of model representations, UPCORE effectively prunes outliers from the forget set, leading to better retention of the model's utility. The framework is evaluated against standard unlearning methods, demonstrating improved performance in both deletion effectiveness and model preservation through a new area-under-the-curve metric."
                },
                "zh": {
                    "title": "UPCOREï¼šå¹³è¡¡é—å¿˜ä¸æ¨¡å‹ä¿ç•™çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œç”¨æˆ·çš„è¦æ±‚å¸¸å¸¸éœ€è¦ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åˆ é™¤ç‰¹å®šä¿¡æ¯ï¼Œè¿™è¢«ç§°ä¸ºâ€œé—å¿˜â€ã€‚ç„¶è€Œï¼Œåˆ é™¤æ•°æ®ç‚¹å¯èƒ½ä¼šå½±å“æ¨¡å‹åœ¨å…¶ä»–æ•°æ®ä¸Šçš„è¡¨ç°ï¼Œå› æ­¤éœ€è¦åœ¨åˆ é™¤ä¿¡æ¯å’Œä¿æŒæ¨¡å‹èƒ½åŠ›ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UPCOREï¼ˆå®ç”¨æ€§ä¿æŒæ ¸å¿ƒé›†é€‰æ‹©ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨çš„æ•°æ®é€‰æ‹©æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘é—å¿˜è¿‡ç¨‹ä¸­çš„æ¨¡å‹æŸå®³ã€‚é€šè¿‡é€‰æ‹©æ€§åœ°ä¿®å‰ªé—å¿˜é›†ä¸­çš„å¼‚å¸¸å€¼ï¼ŒUPCOREèƒ½å¤Ÿåœ¨åˆ é™¤æœ‰æ•ˆæ€§å’Œæ¨¡å‹ä¿ç•™ä¹‹é—´å®ç°æ›´å¥½çš„å¹³è¡¡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-21.html",
    "link_next": "2025-02-25.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "21.02",
        "en": "02/21",
        "zh": "2æœˆ21æ—¥"
    },
    "short_date_next": {
        "ru": "25.02",
        "en": "02/25",
        "zh": "2æœˆ25æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 6,
        "#benchmark": 7,
        "#agents": 1,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 5,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 5,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 9,
        "#robotics": 0,
        "#agi": 3,
        "#games": 1,
        "#interpretability": 3,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº†Meta MLGymå’ŒMLGym-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å’Œå¼€å‘LLMä»£ç†åœ¨AIç ”ç©¶ä»»åŠ¡ä¸Šçš„æ–°æ¡†æ¶å’ŒåŸºå‡†ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºæœºå™¨å­¦ä¹ ä»»åŠ¡çš„Gymç¯å¢ƒï¼Œæ”¯æŒå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ç ”ç©¶ã€‚MLGym-BenchåŒ…å«13ä¸ªå¤šæ ·åŒ–å’Œå¼€æ”¾å¼çš„AIç ”ç©¶ä»»åŠ¡ï¼Œæ¶µç›–è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€å¼ºåŒ–å­¦ä¹ å’Œåšå¼ˆè®ºç­‰é¢†åŸŸã€‚è§£å†³è¿™äº›ä»»åŠ¡éœ€è¦å®é™…çš„AIç ”ç©¶æŠ€èƒ½ï¼Œå¦‚ç”Ÿæˆæ–°æƒ³æ³•å’Œå‡è®¾ã€åˆ›å»ºå’Œå¤„ç†æ•°æ®ã€å®ç°MLæ–¹æ³•ã€è®­ç»ƒæ¨¡å‹ã€è¿è¡Œå®éªŒã€åˆ†æç»“æœå¹¶è¿­ä»£æ”¹è¿›ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šä¸ªå‰æ²¿å¤§è¯­è¨€æ¨¡å‹ï¼Œå¦‚Claude-3.5-Sonnetå’ŒGPT-4oã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¾¿äºæ·»åŠ æ–°ä»»åŠ¡ã€é›†æˆå’Œè¯„ä¼°æ¨¡å‹æˆ–ä»£ç†ã€å¤§è§„æ¨¡ç”Ÿæˆåˆæˆæ•°æ®ä»¥åŠå¼€å‘æ–°çš„å­¦ä¹ ç®—æ³•ã€‚æˆ‘ä»¬å‘ç°å½“å‰çš„å‰æ²¿æ¨¡å‹å¯ä»¥é€šè¿‡æ‰¾åˆ°æ›´å¥½çš„è¶…å‚æ•°æ¥æ”¹è¿›åŸºçº¿ï¼Œä½†ä¸ä¼šç”Ÿæˆæ–°çš„å‡è®¾ã€ç®—æ³•ã€æ¶æ„æˆ–æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬å¼€æºäº†æˆ‘ä»¬çš„æ¡†æ¶å’ŒåŸºå‡†ï¼Œä»¥ä¿ƒè¿›æœªæ¥åœ¨æé«˜LLMä»£ç†çš„AIç ”ç©¶èƒ½åŠ›æ–¹é¢çš„ç ”ç©¶ã€‚",
        "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
        "pinyin": "æˆ‘ä»¬ä»‹ç»äº†Meta MLGymå’ŒMLGym-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å’Œå¼€å‘LLMä»£ç†åœ¨AIç ”ç©¶ä»»åŠ¡ä¸Šçš„æ–°æ¡†æ¶å’ŒåŸºå‡†ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºæœºå™¨å­¦ä¹ ä»»åŠ¡çš„Gymç¯å¢ƒï¼Œæ”¯æŒå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ç ”ç©¶ã€‚MLGym-BenchåŒ…å«13ä¸ªå¤šæ ·åŒ–å’Œå¼€æ”¾å¼çš„AIç ”ç©¶ä»»åŠ¡ï¼Œæ¶µç›–è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€å¼ºåŒ–å­¦ä¹ å’Œåšå¼ˆè®ºç­‰é¢†åŸŸã€‚è§£å†³è¿™äº›ä»»åŠ¡éœ€è¦å®é™…çš„AIç ”ç©¶æŠ€èƒ½ï¼Œå¦‚ç”Ÿæˆæ–°æƒ³æ³•å’Œå‡è®¾ã€åˆ›å»ºå’Œå¤„ç†æ•°æ®ã€å®ç°MLæ–¹æ³•ã€è®­ç»ƒæ¨¡å‹ã€è¿è¡Œå®éªŒã€åˆ†æç»“æœå¹¶è¿­ä»£æ”¹è¿›ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šä¸ªå‰æ²¿å¤§è¯­è¨€æ¨¡å‹ï¼Œå¦‚Claude-3.5-Sonnetå’ŒGPT-4oã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¾¿äºæ·»åŠ æ–°ä»»åŠ¡ã€é›†æˆå’Œè¯„ä¼°æ¨¡å‹æˆ–ä»£ç†ã€å¤§è§„æ¨¡ç”Ÿæˆåˆæˆæ•°æ®ä»¥åŠå¼€å‘æ–°çš„å­¦ä¹ ç®—æ³•ã€‚æˆ‘ä»¬å‘ç°å½“å‰çš„å‰æ²¿æ¨¡å‹å¯ä»¥é€šè¿‡æ‰¾åˆ°æ›´å¥½çš„è¶…å‚æ•°æ¥æ”¹è¿›åŸºçº¿ï¼Œä½†ä¸ä¼šç”Ÿæˆæ–°çš„å‡è®¾ã€ç®—æ³•ã€æ¶æ„æˆ–æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬å¼€æºäº†æˆ‘ä»¬çš„æ¡†æ¶å’ŒåŸºå‡†ï¼Œä»¥ä¿ƒè¿›æœªæ¥åœ¨æé«˜LLMä»£ç†çš„AIç ”ç©¶èƒ½åŠ›æ–¹é¢çš„ç ”ç©¶ã€‚\n\nWÇ’men jiÃ¨shÃ o le Meta MLGym hÃ© MLGym-Bench, zhÃ¨ shÃ¬ yÄ«gÃ¨ yÃ²ngyÃº pÃ­ngguÇ hÃ© kÄifÄ LLM dÃ ilÇ zÃ i AI yÃ¡njiÅ« rÃ¨nwÃ¹ shÃ ng de xÄ«n kuÃ ngjiÃ  hÃ© jÄ«zhÇ”n. ZhÃ¨ shÃ¬ dÃ¬-yÄ«gÃ¨ yÃ²ngyÃº jÄ«qÇ xuÃ©xÃ­ rÃ¨nwÃ¹ de Gym huÃ¡njÃ¬ng, zhÄ«chÃ­ qiÃ¡ngzhÃ¬ xuÃ©xÃ­ suÃ nfÇ de yÃ¡njiÅ«. MLGym-Bench bÄohÃ¡n 13 gÃ¨ duÅyÃ nghuÃ  hÃ© kÄifÃ ngshÃ¬ de AI yÃ¡njiÅ« rÃ¨nwÃ¹, hÃ¡nfÃ¹ jÃ¬suÃ njÄ« shÃ¬jiÃ¨, zÃ¬rÃ¡n yÇ”yÃ¡n chÇ”lÇ, qiÃ¡ngzhÃ¬ xuÃ©xÃ­ hÃ© bÃ³yÃ¬lÃ¹n dÄ›ng lÇngyÃ¹. JiÄ›juÃ© zhÃ¨xiÄ“ rÃ¨nwÃ¹ xÅ«yÃ o shÃ­jÃ¬ de AI yÃ¡njiÅ« jÃ¬nÃ©ng, rÃº shÄ“ngchÃ©ng xÄ«n yÃ¬xiÇng hÃ© jiÇshÃ¨, chuÃ ngjiÃ n hÃ© chÇ”lÇ shÃ¹jÃ¹, shÃ­xiÃ n ML fÄngfÇ, xÃ¹nliÃ n mÃ³xÃ­ng, yÃ¹nxÃ­ng shÃ¬yÃ n, fÄ“nxi jiÃ©guÇ’ bÃ¬ng diÃ©dÃ i gÇijÃ¬n. WÇ’men pÃ­ngguÇ le duÅ gÃ¨ qiÃ¡nxÄ«n dÃ  yÇ”yÃ¡n mÃ³xÃ­ng, rÃº Claude-3.5-Sonnet hÃ© GPT-4o. WÇ’men de kuÃ ngjiÃ  biÃ n yÇ’ng tiÄnjiÄ xÄ«n rÃ¨nwÃ¹, jÃ­chÃ©ng hÃ© pÃ­ngguÇ mÃ³xÃ­ng huÃ² dÃ ilÇ, dÃ  guÄ«mÃ³ shÄ“ngchÃ©ng hÃ©chÃ©ng shÃ¹jÃ¹ yÇjÃ­ kÄifÄ xÄ«n de xuÃ©xÃ­ suÃ nfÇ. WÇ’men fÄxiÃ n dÄngqiÃ¡n de qiÃ¡nxÄ«n mÃ³xÃ­ng kÄ›yÇ tÅngguÃ² zhÇo dÃ o gÃ¨ng hÇo de chÄocÄnshÃ¹ lÃ¡i gÇijÃ¬n jÄ«chÇ”, dÃ n bÃ¹ huÃ¬ shÄ“ngchÃ©ng xÄ«n de jiÇshÃ¨, suÃ nfÇ, jiÃ gÃ²u huÃ² xiÇnzhÃ¹ gÇijÃ¬n. WÇ’men kÄiyuÃ¡n le wÇ’men de kuÃ ngjiÃ  hÃ© jÄ«zhÇ”n, yÇ cÃ¹jÃ¬n wÃ¨ilÃ¡i zÃ i tÃ­gÄo LLM dÃ ilÇ de AI yÃ¡njiÅ« nÃ©nglÃ¬ fÄngmiÃ n de yÃ¡njiÅ«.",
        "vocab": "[\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ng jiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­ng gÅ«\", \"trans\": \"evaluate\"},\n    {\"word\": \"å¼€å‘\", \"pinyin\": \"kÄi fÄ\", \"trans\": \"develop\"},\n    {\"word\": \"ä»£ç†\", \"pinyin\": \"dÃ i lÇ\", \"trans\": \"agent\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wu\", \"trans\": \"task\"},\n    {\"word\": \"æ”¯æŒ\", \"pinyin\": \"zhÄ« chÃ­\", \"trans\": \"support\"},\n    {\"word\": \"å¼ºåŒ–\", \"pinyin\": \"qiÃ¡ng huÃ \", \"trans\": \"reinforce\"},\n    {\"word\": \"ç®—æ³•\", \"pinyin\": \"suÃ n fÇ\", \"trans\": \"algorithm\"},\n    {\"word\": \"å¤šæ ·åŒ–\", \"pinyin\": \"duÅ yÃ ng huÃ \", \"trans\": \"diversify\"},\n    {\"word\": \"å¼€æ”¾å¼\", \"pinyin\": \"kÄi fÃ ng shÃ¬\", \"trans\": \"open-ended\"},\n    {\"word\": \"æ¶µç›–\", \"pinyin\": \"hÃ¡n gÃ i\", \"trans\": \"cover\"},\n    {\"word\": \"è®¡ç®—æœºè§†è§‰\", \"pinyin\": \"jÃ¬ suÃ n jÄ« shÃ¬ juÃ©\", \"trans\": \"computer vision\"},\n    {\"word\": \"è‡ªç„¶è¯­è¨€å¤„ç†\", \"pinyin\": \"zÃ¬ rÃ¡n yÇ” yÃ¡n chÇ” lÇ\", \"trans\": \"natural language processing\"},\n    {\"word\": \"åšå¼ˆè®º\", \"pinyin\": \"bÃ³ yÃ¬ lÃ¹n\", \"trans\": \"game theory\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generate\"},\n    {\"word\": \"å‡è®¾\", \"pinyin\": \"jiÇ shÃ¨\", \"trans\": \"hypothesis\"},\n    {\"word\": \"åˆ›å»º\", \"pinyin\": \"chuÃ ng jiÃ n\", \"trans\": \"create\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ” lÇ\", \"trans\": \"process\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­ xiÃ n\", \"trans\": \"implement\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹n liÃ n\", \"trans\": \"train\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­ yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"åˆ†æ\", \"pinyin\": \"fÄ“n xÄ«\", \"trans\": \"analyze\"},\n    {\"word\": \"è¿­ä»£\", \"pinyin\": \"diÃ© dÃ i\", \"trans\": \"iterate\"},\n    {\"word\": \"æ”¹è¿›\", \"pinyin\": \"gÇi jÃ¬n\", \"trans\": \"improve\"},\n    {\"word\": \"å‰æ²¿\", \"pinyin\": \"qiÃ¡n yÃ¡n\", \"trans\": \"frontier\"},\n    {\"word\": \"ä¾¿äº\", \"pinyin\": \"biÃ n yÃº\", \"trans\": \"facilitate\"},\n    {\"word\": \"é›†æˆ\", \"pinyin\": \"jÃ­ chÃ©ng\", \"trans\": \"integrate\"},\n    {\"word\": \"åˆæˆ\", \"pinyin\": \"hÃ© chÃ©ng\", \"trans\": \"synthesize\"},\n    {\"word\": \"å¼€æº\", \"pinyin\": \"kÄi yuÃ¡n\", \"trans\": \"open source\"},\n    {\"word\": \"ä¿ƒè¿›\", \"pinyin\": \"cÃ¹ jÃ¬n\", \"trans\": \"promote\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©ng lÃ¬\", \"trans\": \"ability\"}\n]",
        "trans": "We introduced Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning tasks that supports research on reinforcement learning algorithms. MLGym-Bench includes 13 diverse and open-ended AI research tasks covering areas such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires practical AI research skills, such as generating new ideas and hypotheses, creating and handling data, implementing ML methods, training models, running experiments, analyzing results, and iteratively improving. We evaluated several state-of-the-art large language models, such as Claude-3.5-Sonnet and GPT-4o. Our framework facilitates the addition of new tasks, integration and evaluation of models or agents, large-scale generation of synthetic data, and the development of new learning algorithms. We found that current state-of-the-art models can improve baselines by finding better hyperparameters but do not generate new hypotheses, algorithms, architectures, or significant improvements. We have open-sourced our framework and benchmark to promote future research in enhancing the AI research capabilities of LLM agents.",
        "update_ts": "2025-02-23 12:38"
    }
}