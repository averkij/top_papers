{
    "date": {
        "ru": "4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 4",
        "zh": "9æœˆ4æ—¥"
    },
    "time_utc": "2025-09-04 17:10",
    "weekday": 3,
    "issue_id": 5721,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.01106",
            "title": "Robix: A Unified Model for Robot Interaction, Reasoning and Planning",
            "url": "https://huggingface.co/papers/2509.01106",
            "abstract": "Robix, a unified vision-language model, integrates robot reasoning, task planning, and natural language interaction, demonstrating superior performance in interactive task execution through chain-of-thought reasoning and a three-stage training strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Robix, a unified model that integrates robot reasoning, task planning, and natural language interaction within a single vision-language architecture. Acting as the high-level cognitive layer in a hierarchical robot system, Robix dynamically generates atomic commands for the low-level controller and verbal responses for human interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally with human within an end-to-end framework. Robix further introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix leverages chain-of-thought reasoning and adopts a three-stage training strategy: (1) continued pretraining to enhance foundational embodied reasoning abilities including 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering.",
            "score": 31,
            "issue_id": 5707,
            "pub_date": "2025-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "d0766d32afe23fec",
            "authors": [
                "Huang Fang",
                "Mengxi Zhang",
                "Heng Dong",
                "Wei Li",
                "Zixuan Wang",
                "Qifeng Zhang",
                "Xueyun Tian",
                "Yucheng Hu",
                "Hang Li"
            ],
            "affiliations": [
                "bytedance.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01106.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#alignment",
                    "#robotics",
                    "#multimodal",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Robix: Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Robix - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Robix Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ°Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ°."
                },
                "en": {
                    "title": "Robix: Revolutionizing Robot Interaction and Task Execution",
                    "desc": "Robix is a unified vision-language model designed to enhance robot reasoning, task planning, and natural language interaction. It operates as a cognitive layer in robotic systems, generating commands and responses that allow robots to execute complex tasks and interact with humans effectively. The model employs chain-of-thought reasoning and a three-stage training strategy, which includes pretraining, supervised finetuning, and reinforcement learning to improve its performance. Experiments show that Robix significantly outperforms existing models in executing diverse interactive tasks, showcasing its ability to generalize across various instruction types."
                },
                "zh": {
                    "title": "Robixï¼šæ™ºèƒ½æœºå™¨äººäº¤äº’çš„æ–°çºªå…ƒ",
                    "desc": "Robixæ˜¯ä¸€ç§ç»Ÿä¸€çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†æœºå™¨äººæ¨ç†ã€ä»»åŠ¡è§„åˆ’å’Œè‡ªç„¶è¯­è¨€äº¤äº’ã€‚å®ƒé€šè¿‡é“¾å¼æ€ç»´æ¨ç†å’Œä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå±•ç¤ºäº†åœ¨äº¤äº’ä»»åŠ¡æ‰§è¡Œä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚Robixèƒ½å¤ŸåŠ¨æ€ç”ŸæˆåŸå­å‘½ä»¤å’Œäººæœºäº¤äº’çš„è¯­è¨€å“åº”ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ‰§è¡Œå¤æ‚æŒ‡ä»¤å¹¶è¿›è¡Œè‡ªç„¶äº’åŠ¨ã€‚å®éªŒè¡¨æ˜ï¼ŒRobixåœ¨å¤šç§æŒ‡ä»¤ç±»å‹å’Œç”¨æˆ·å‚ä¸çš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„å¼€æºå’Œå•†ä¸šæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.00375",
            "title": "Open Data Synthesis For Deep Research",
            "url": "https://huggingface.co/papers/2509.00375",
            "abstract": "InfoSeek is a scalable framework for generating complex Deep Research tasks by synthesizing hierarchical constraint satisfaction problems, enabling models to outperform larger baselines on challenging benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly expected to go beyond simple factual queries toward Deep Research-tasks that require decomposing questions into sub-problems, coordinating multi-step reasoning, and synthesizing evidence from diverse sources. We formalize Deep Research tasks with verifiable answers as Hierarchical Constraint Satisfaction Problems (HCSPs), which are fundamentally different from single-constraint, multi-hop, or flat CSP formulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA) fail to capture this complexity, while recent synthetic datasets often introduce shortcut reasoning, knowledge leakage, or lack sufficient structural depth. To address this gap, we introduce InfoSeek, a scalable framework for synthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to recursively build a Research Tree from large-scale webpages, blurring intermediate nodes into valid sub-problems, and converting these trees into natural language questions that require traversing the full hierarchy. It also enables rapid scaling, yielding over 50K training examples, a curated test set, and reasoning trajectories generated via reject sampling. Experiments show that models trained on InfoSeek consistently outperform strong baselines. On a challenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass much larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash), while achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro). By preserving meta-information such as intermediate steps and retrieval labels, InfoSeek further supports advanced optimization strategies, including compound reward design and trajectory-level exploration. We provide our codes and datasets in https://github.com/VectorSpaceLab/InfoSeek{this repository}.",
            "score": 28,
            "issue_id": 5707,
            "pub_date": "2025-08-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 30",
                "zh": "8æœˆ30æ—¥"
            },
            "hash": "d7d79c964b418fac",
            "authors": [
                "Ziyi Xia",
                "Kun Luo",
                "Hongjin Qian",
                "Zheng Liu"
            ],
            "affiliations": [
                "BAAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00375.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#synthetic",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "InfoSeek: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "InfoSeek - ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ€ĞµĞ²Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ ĞµĞ³Ğ¾ Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° InfoSeek, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. InfoSeek Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Unlocking Deep Research with Hierarchical Constraints",
                    "desc": "InfoSeek is a novel framework designed to tackle complex Deep Research tasks by structuring them as Hierarchical Constraint Satisfaction Problems (HCSPs). This approach allows models to break down intricate questions into manageable sub-problems, facilitating multi-step reasoning and evidence synthesis from various sources. Unlike traditional benchmarks, InfoSeek generates a large dataset of over 50,000 training examples that reflect the complexity of real-world queries without shortcut reasoning. Experiments demonstrate that models trained with InfoSeek significantly outperform larger models on challenging benchmarks, showcasing its effectiveness in enhancing the capabilities of large language models."
                },
                "zh": {
                    "title": "InfoSeekï¼šæ·±åº¦ç ”ç©¶ä»»åŠ¡çš„æ–°æ¡†æ¶",
                    "desc": "InfoSeekæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå¤æ‚çš„æ·±åº¦ç ”ç©¶ä»»åŠ¡ï¼Œé€šè¿‡åˆæˆå±‚æ¬¡çº¦æŸæ»¡è¶³é—®é¢˜ï¼Œä½¿æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šæ›´å¤§çš„åŸºçº¿ã€‚è¯¥æ¡†æ¶ä½¿ç”¨åŒä»£ç†ç³»ç»Ÿï¼Œä»å¤§è§„æ¨¡ç½‘é¡µé€’å½’æ„å»ºç ”ç©¶æ ‘ï¼Œå°†ä¸­é—´èŠ‚ç‚¹æ¨¡ç³ŠåŒ–ä¸ºæœ‰æ•ˆçš„å­é—®é¢˜ï¼Œå¹¶å°†è¿™äº›æ ‘è½¬æ¢ä¸ºéœ€è¦éå†å®Œæ•´å±‚æ¬¡çš„è‡ªç„¶è¯­è¨€é—®é¢˜ã€‚InfoSeekèƒ½å¤Ÿå¿«é€Ÿæ‰©å±•ï¼Œç”Ÿæˆè¶…è¿‡50,000ä¸ªè®­ç»ƒç¤ºä¾‹ï¼Œå¹¶æä¾›ç»è¿‡ç­–åˆ’çš„æµ‹è¯•é›†å’Œé€šè¿‡æ‹’ç»é‡‡æ ·ç”Ÿæˆçš„æ¨ç†è½¨è¿¹ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºInfoSeekè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šå¤§å‹æ¨¡å‹å’Œå•†ä¸šAPIã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.03405",
            "title": "LMEnt: A Suite for Analyzing Knowledge in Language Models from\n  Pretraining Data to Representations",
            "url": "https://huggingface.co/papers/2509.03405",
            "abstract": "LMEnt is a suite for analyzing knowledge acquisition in language models during pretraining, providing annotated corpora, retrieval methods, and pretrained models to study knowledge representations and learning dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models (LMs) increasingly drive real-world applications that require world knowledge. However, the internal processes through which models turn data into representations of knowledge and beliefs about the world, are poorly understood. Insights into these processes could pave the way for developing LMs with knowledge representations that are more consistent, robust, and complete. To facilitate studying these questions, we present LMEnt, a suite for analyzing knowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a knowledge-rich pretraining corpus, fully annotated with entity mentions, based on Wikipedia, (2) an entity-based retrieval method over pretraining data that outperforms previous approaches by as much as 80.4%, and (3) 12 pretrained models with up to 1B parameters and 4K intermediate checkpoints, with comparable performance to popular open-sourced models on knowledge benchmarks. Together, these resources provide a controlled environment for analyzing connections between entity mentions in pretraining and downstream performance, and the effects of causal interventions in pretraining data. We show the utility of LMEnt by studying knowledge acquisition across checkpoints, finding that fact frequency is key, but does not fully explain learning trends. We release LMEnt to support studies of knowledge in LMs, including knowledge representations, plasticity, editing, attribution, and learning dynamics.",
            "score": 14,
            "issue_id": 5713,
            "pub_date": "2025-09-03",
            "pub_date_card": {
                "ru": "3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 3",
                "zh": "9æœˆ3æ—¥"
            },
            "hash": "c66ad503192a18e1",
            "authors": [
                "Daniela Gottesman",
                "Alon Gilae-Dotan",
                "Ido Cohen",
                "Yoav Gur-Arieh",
                "Marius Mosbach",
                "Ori Yoran",
                "Mor Geva"
            ],
            "affiliations": [
                "McGill University",
                "Mila Quebec AI Institute",
                "Tel Aviv University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.03405.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#interpretability",
                    "#dataset",
                    "#training",
                    "#benchmark",
                    "#science",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LMEnt: Ğ—Ğ°Ğ³Ğ»ÑĞ´Ñ‹Ğ²Ğ°Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "LMEnt - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑƒÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. LMEnt Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ¸ 12 Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking Knowledge Acquisition in Language Models with LMEnt",
                    "desc": "LMEnt is a comprehensive toolkit designed to analyze how language models acquire knowledge during their pretraining phase. It includes a specially annotated corpus based on Wikipedia, an advanced retrieval method that significantly improves performance, and a set of pretrained models with substantial parameters. This suite allows researchers to explore the relationship between entity mentions in the training data and the models' performance on knowledge tasks. By providing insights into knowledge representations and learning dynamics, LMEnt aims to enhance the development of more reliable and complete language models."
                },
                "zh": {
                    "title": "LMEntï¼šæ­ç¤ºè¯­è¨€æ¨¡å‹çŸ¥è¯†è·å–çš„ç§˜å¯†",
                    "desc": "LMEntæ˜¯ä¸€ä¸ªç”¨äºåˆ†æè¯­è¨€æ¨¡å‹åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çŸ¥è¯†è·å–çš„å·¥å…·å¥—ä»¶ã€‚å®ƒæä¾›äº†å¸¦æ³¨é‡Šçš„è¯­æ–™åº“ã€æ£€ç´¢æ–¹æ³•å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥ç ”ç©¶çŸ¥è¯†è¡¨ç¤ºå’Œå­¦ä¹ åŠ¨æ€ã€‚é€šè¿‡LMEntï¼Œç ”ç©¶äººå‘˜å¯ä»¥æ›´å¥½åœ°ç†è§£è¯­è¨€æ¨¡å‹å¦‚ä½•å°†æ•°æ®è½¬åŒ–ä¸ºå¯¹ä¸–ç•Œçš„çŸ¥è¯†å’Œä¿¡å¿µã€‚è¯¥å·¥å…·çš„æ¨å‡ºæœ‰åŠ©äºå¼€å‘æ›´ä¸€è‡´ã€ç¨³å¥å’Œå®Œæ•´çš„çŸ¥è¯†è¡¨ç¤ºçš„è¯­è¨€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.01977",
            "title": "MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware\n  Alignment and Disentanglement",
            "url": "https://huggingface.co/papers/2509.01977",
            "abstract": "MOSAIC framework enhances multi-subject image generation by ensuring precise semantic alignment and orthogonal feature disentanglement, achieving high fidelity even with multiple references.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-subject personalized generation presents unique challenges in maintaining identity fidelity and semantic coherence when synthesizing images conditioned on multiple reference subjects. Existing methods often suffer from identity blending and attribute leakage due to inadequate modeling of how different subjects should interact within shared representation spaces. We present MOSAIC, a representation-centric framework that rethinks multi-subject generation through explicit semantic correspondence and orthogonal feature disentanglement. Our key insight is that multi-subject generation requires precise semantic alignment at the representation level - knowing exactly which regions in the generated image should attend to which parts of each reference. To enable this, we introduce SemAlign-MS, a meticulously annotated dataset providing fine-grained semantic correspondences between multiple reference subjects and target images, previously unavailable in this domain. Building on this foundation, we propose the semantic correspondence attention loss to enforce precise point-to-point semantic alignment, ensuring high consistency from each reference to its designated regions. Furthermore, we develop the multi-reference disentanglement loss to push different subjects into orthogonal attention subspaces, preventing feature interference while preserving individual identity characteristics. Extensive experiments demonstrate that MOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably, while existing methods typically degrade beyond 3 subjects, MOSAIC maintains high fidelity with 4+ reference subjects, opening new possibilities for complex multi-subject synthesis applications.",
            "score": 7,
            "issue_id": 5711,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "5a1bf5cc33f3e20f",
            "authors": [
                "Dong She",
                "Siming Fu",
                "Mushui Liu",
                "Qiaoqiao Jin",
                "Hualiang Wang",
                "Mu Liu",
                "Jidong Jiang"
            ],
            "affiliations": [
                "ByteDance",
                "The Hong Kong University of Science and Technology",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.01977.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#synthetic",
                    "#benchmark"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "MOSAIC - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SemAlign-MS Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. MOSAIC Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ loss-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ 4 Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "MOSAIC: Mastering Multi-Subject Image Generation with Precision",
                    "desc": "The MOSAIC framework improves the generation of images featuring multiple subjects by focusing on precise semantic alignment and separating features effectively. It addresses common issues like identity blending and attribute leakage that arise when synthesizing images from multiple references. By introducing a new dataset, SemAlign-MS, it provides detailed semantic correspondences, which helps in maintaining clarity in the generated images. The framework's innovative loss functions ensure that different subjects are represented distinctly, allowing for high-quality image generation even with more than three subjects."
                },
                "zh": {
                    "title": "MOSAICï¼šå¤šä¸»ä½“å›¾åƒç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "MOSAICæ¡†æ¶é€šè¿‡ç¡®ä¿ç²¾ç¡®çš„è¯­ä¹‰å¯¹é½å’Œæ­£äº¤ç‰¹å¾è§£è€¦ï¼Œå¢å¼ºäº†å¤šä¸»ä½“å›¾åƒç”Ÿæˆçš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨å¤šä¸ªå‚è€ƒä¸»ä½“æ¡ä»¶ä¸‹åˆæˆå›¾åƒæ—¶èº«ä»½ä¿çœŸåº¦å’Œè¯­ä¹‰ä¸€è‡´æ€§çš„é—®é¢˜ã€‚MOSAICå¼•å…¥äº†SemAlign-MSæ•°æ®é›†ï¼Œæä¾›äº†å¤šå‚è€ƒä¸»ä½“ä¸ç›®æ ‡å›¾åƒä¹‹é—´çš„ç»†ç²’åº¦è¯­ä¹‰å¯¹åº”å…³ç³»ã€‚é€šè¿‡è¯­ä¹‰å¯¹åº”æ³¨æ„åŠ›æŸå¤±å’Œå¤šå‚è€ƒè§£è€¦æŸå¤±ï¼ŒMOSAICåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿåœ¨4ä¸ªä»¥ä¸Šçš„å‚è€ƒä¸»ä½“ä¸‹ä¿æŒé«˜ä¿çœŸåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.00428",
            "title": "Mixture of Global and Local Experts with Diffusion Transformer for\n  Controllable Face Generation",
            "url": "https://huggingface.co/papers/2509.00428",
            "abstract": "Face-MoGLE, a novel framework using Diffusion Transformers, achieves high-quality, controllable face generation through semantic-decoupled latent modeling, expert specialization, and dynamic gating.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable face generation poses critical challenges in generative modeling due to the intricate balance required between semantic controllability and photorealism. While existing approaches struggle with disentangling semantic controls from generation pipelines, we revisit the architectural potential of Diffusion Transformers (DiTs) through the lens of expert specialization. This paper introduces Face-MoGLE, a novel framework featuring: (1) Semantic-decoupled latent modeling through mask-conditioned space factorization, enabling precise attribute manipulation; (2) A mixture of global and local experts that captures holistic structure and region-level semantics for fine-grained controllability; (3) A dynamic gating network producing time-dependent coefficients that evolve with diffusion steps and spatial locations. Face-MoGLE provides a powerful and flexible solution for high-quality, controllable face generation, with strong potential in generative modeling and security applications. Extensive experiments demonstrate its effectiveness in multimodal and monomodal face generation settings and its robust zero-shot generalization capability. Project page is available at https://github.com/XavierJiezou/Face-MoGLE.",
            "score": 7,
            "issue_id": 5711,
            "pub_date": "2025-08-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 30",
                "zh": "8æœˆ30æ—¥"
            },
            "hash": "5f17d4af79b3fc6f",
            "authors": [
                "Xuechao Zou",
                "Shun Zhang",
                "Xing Fu",
                "Yue Li",
                "Kai Li",
                "Yushe Cao",
                "Congyan Lang",
                "Pin Tao",
                "Junliang Xing"
            ],
            "affiliations": [
                "Ant Group",
                "Beijing Jiaotong University",
                "Qinghai University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.00428.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#multimodal",
                    "#architecture",
                    "#security"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ»Ğ¸Ñ† Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²",
                    "desc": "Face-MoGLE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹. ĞĞ½Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ° ÑÑ‡ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¼ĞµÑÑŒ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ»Ğ¸Ñ†Ğ°. Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞµÑ‚ÑŒ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ñ‹, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Face-MoGLE: Mastering Controllable Face Generation with Diffusion Transformers",
                    "desc": "Face-MoGLE is a new framework that uses Diffusion Transformers to generate high-quality and controllable faces. It addresses the challenge of balancing semantic control with photorealism by employing semantic-decoupled latent modeling, which allows for precise manipulation of facial attributes. The framework incorporates a mixture of global and local experts to enhance both overall structure and detailed features, ensuring fine-grained control over the generated images. Additionally, a dynamic gating network adapts coefficients during the generation process, improving the model's flexibility and effectiveness in various face generation tasks."
                },
                "zh": {
                    "title": "Face-MoGLEï¼šé«˜è´¨é‡å¯æ§é¢éƒ¨ç”Ÿæˆçš„æ–°æ¡†æ¶",
                    "desc": "Face-MoGLEæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£å˜æ¢å™¨å®ç°é«˜è´¨é‡ã€å¯æ§çš„é¢éƒ¨ç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡è¯­ä¹‰è§£è€¦çš„æ½œåœ¨å»ºæ¨¡ã€ä¸“å®¶ä¸“é—¨åŒ–å’ŒåŠ¨æ€é—¨æ§æ¥è§£å†³ç”Ÿæˆå»ºæ¨¡ä¸­çš„æŒ‘æˆ˜ã€‚å®ƒå…è®¸ç²¾ç¡®çš„å±æ€§æ“æ§ï¼Œå¹¶ç»“åˆå…¨å±€å’Œå±€éƒ¨ä¸“å®¶ä»¥æ•æ‰æ•´ä½“ç»“æ„å’ŒåŒºåŸŸè¯­ä¹‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFace-MoGLEåœ¨å¤šæ¨¡æ€å’Œå•æ¨¡æ€é¢éƒ¨ç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.02722",
            "title": "Planning with Reasoning using Vision Language World Model",
            "url": "https://huggingface.co/papers/2509.02722",
            "abstract": "The Vision Language World Model (VLWM) achieves state-of-the-art performance in visual planning by integrating language-based world modeling, action policy learning, and dynamics modeling with semantic and temporal abstraction.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective planning requires strong world models, but high-level world models that can understand and reason about actions with semantic and temporal abstraction remain largely underdeveloped. We introduce the Vision Language World Model (VLWM), a foundation model trained for language-based world modeling on natural videos. Given visual observations, the VLWM first infers the overall goal achievements then predicts a trajectory composed of interleaved actions and world state changes. Those targets are extracted by iterative LLM Self-Refine conditioned on compressed future observations represented by Tree of Captions. The VLWM learns both an action policy and a dynamics model, which respectively facilitates reactive system-1 plan decoding and reflective system-2 planning via cost minimization. The cost evaluates the semantic distance between the hypothetical future states given by VLWM roll-outs and the expected goal state, and is measured by a critic model that we trained in a self-supervised manner. The VLWM achieves state-of-the-art Visual Planning for Assistance (VPA) performance on both benchmark evaluations and our proposed PlannerArena human evaluations, where system-2 improves the Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM baselines on RoboVQA and WorldPrediction benchmark.",
            "score": 5,
            "issue_id": 5718,
            "pub_date": "2025-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "9d66073795d0c729",
            "authors": [
                "Delong Chen",
                "Theo Moutakanni",
                "Willy Chung",
                "Yejin Bang",
                "Ziwei Ji",
                "Allen Bolourchi",
                "Pascale Fung"
            ],
            "affiliations": [
                "ISIR Sorbonne UniversitÃ©",
                "Meta FAIR",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.02722.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#benchmark",
                    "#cv",
                    "#optimization",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "VLWM: Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "VLWM (Vision Language World Model) - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ñ‹Ğ¼Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ´ĞµÑ€ĞµĞ²Ğ° Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹. VLWM Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ (VPA) ĞºĞ°Ğº Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ… PlannerArena."
                },
                "en": {
                    "title": "Revolutionizing Visual Planning with Language Understanding",
                    "desc": "The Vision Language World Model (VLWM) is a cutting-edge model that enhances visual planning by combining language understanding with world modeling. It learns to predict actions and changes in the world by analyzing natural videos, allowing it to infer goals and plan trajectories effectively. The model employs a two-system approach, where system-1 focuses on quick, reactive planning and system-2 engages in deeper, reflective planning to minimize costs based on expected outcomes. VLWM demonstrates superior performance in visual planning tasks, outperforming existing models in various benchmarks and evaluations."
                },
                "zh": {
                    "title": "è§†è§‰è¯­è¨€ä¸–ç•Œæ¨¡å‹ï¼šæ™ºèƒ½è§„åˆ’çš„æ–°çªç ´",
                    "desc": "è§†è§‰è¯­è¨€ä¸–ç•Œæ¨¡å‹ï¼ˆVLWMï¼‰é€šè¿‡ç»“åˆåŸºäºè¯­è¨€çš„ä¸–ç•Œå»ºæ¨¡ã€è¡ŒåŠ¨ç­–ç•¥å­¦ä¹ å’ŒåŠ¨æ€å»ºæ¨¡ï¼Œè¾¾åˆ°äº†è§†è§‰è§„åˆ’çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿç†è§£å’Œæ¨ç†å…·æœ‰è¯­ä¹‰å’Œæ—¶é—´æŠ½è±¡çš„é«˜å±‚æ¬¡ä¸–ç•Œæ¨¡å‹ï¼Œå¡«è¡¥äº†è¿™ä¸€é¢†åŸŸçš„ç©ºç™½ã€‚VLWMé¦–å…ˆæ ¹æ®è§†è§‰è§‚å¯Ÿæ¨æ–­æ•´ä½“ç›®æ ‡ï¼Œç„¶åé¢„æµ‹ç”±äº¤é”™çš„è¡ŒåŠ¨å’Œä¸–ç•ŒçŠ¶æ€å˜åŒ–ç»„æˆçš„è½¨è¿¹ã€‚é€šè¿‡è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼è®­ç»ƒçš„è¯„è®ºæ¨¡å‹è¯„ä¼°å‡è®¾æœªæ¥çŠ¶æ€ä¸æœŸæœ›ç›®æ ‡çŠ¶æ€ä¹‹é—´çš„è¯­ä¹‰è·ç¦»ï¼Œä»è€Œå®ç°äº†é«˜æ•ˆçš„è§„åˆ’ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-03.html",
    "link_next": "2025-09-05.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "03.09",
        "en": "09/03",
        "zh": "9æœˆ3æ—¥"
    },
    "short_date_next": {
        "ru": "05.09",
        "en": "09/05",
        "zh": "9æœˆ5æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 2,
        "#cv": 3,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}