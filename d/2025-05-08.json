{
    "date": {
        "ru": "8 мая",
        "en": "May 8",
        "zh": "5月8日"
    },
    "time_utc": "2025-05-08 12:21",
    "weekday": 3,
    "issue_id": 3657,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.02567",
            "title": "Unified Multimodal Understanding and Generation Models: Advances,\n  Challenges, and Opportunities",
            "url": "https://huggingface.co/papers/2505.02567",
            "abstract": "Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
            "score": 40,
            "issue_id": 3655,
            "pub_date": "2025-05-05",
            "pub_date_card": {
                "ru": "5 мая",
                "en": "May 5",
                "zh": "5月5日"
            },
            "hash": "0d49b4c41b7654a0",
            "authors": [
                "Xinjie Zhang",
                "Jintao Guo",
                "Shanshan Zhao",
                "Minghao Fu",
                "Lunhao Duan",
                "Guo-Hua Wang",
                "Qing-Guo Chen",
                "Zhao Xu",
                "Weihua Luo",
                "Kaifu Zhang"
            ],
            "affiliations": [
                "Alibaba Group",
                "Hong Kong University of Science and Technology",
                "Nanjing University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02567.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#survey",
                    "#multimodal",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Объединение мультимодального понимания и генерации изображений: путь к универсальным моделям ИИ",
                    "desc": "Эта статья представляет собой обзор современных подходов к объединению моделей мультимодального понимания и генерации изображений. Авторы анализируют три основных архитектурных парадигмы: основанные на диффузии, авторегрессивные и гибридные подходы. В работе также рассматриваются наборы данных и бенчмарки для унифицированных моделей. Обсуждаются ключевые проблемы в этой области, включая стратегию токенизации, кросс-модальное внимание и данные."
                },
                "en": {
                    "title": "Bridging the Gap: Unifying Multimodal Understanding and Image Generation",
                    "desc": "This paper surveys the integration of multimodal understanding and image generation models, which have traditionally developed separately. It highlights the architectural differences between autoregressive and diffusion-based models, emphasizing the challenges in unifying these approaches. The authors categorize existing unified models into three paradigms: diffusion-based, autoregressive-based, and hybrid methods. They also provide resources such as datasets and benchmarks to support future research in this emerging field."
                },
                "zh": {
                    "title": "统一多模态模型的未来探索",
                    "desc": "近年来，多模态理解模型和图像生成模型取得了显著进展，但这两个领域的发展相对独立，导致了不同的架构范式。自回归架构在多模态理解中占主导地位，而扩散模型则成为图像生成的基石。本文综述了当前统一框架的努力，介绍了多模态理解和文本到图像生成模型的基础概念及最新进展，并分析了三种主要的统一模型架构。我们还讨论了这一新兴领域面临的关键挑战，并提供了未来研究的参考资源。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04588",
            "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
            "url": "https://huggingface.co/papers/2505.04588",
            "abstract": "Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.",
            "score": 25,
            "issue_id": 3647,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "24edc7c3c5e5e23d",
            "authors": [
                "Hao Sun",
                "Zile Qiao",
                "Jiayan Guo",
                "Xuanbo Fan",
                "Yingyan Hou",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Yan Zhang"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04588.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "ZeroSearch: обучение LLM эффективному поиску без реальных поисковых систем",
                    "desc": "Статья представляет ZeroSearch - новую систему обучения с подкреплением для улучшения поисковых возможностей больших языковых моделей (LLM). В отличие от предыдущих подходов, ZeroSearch не требует взаимодействия с реальными поисковыми системами, что решает проблемы неконтролируемого качества документов и высоких затрат на API. Метод использует легковесную предобученную модель в качестве модуля поиска и стратегию постепенного ухудшения качества генерируемых документов во время обучения. Эксперименты показывают, что ZeroSearch эффективно улучшает поисковые способности LLM, причем модели с 14 миллиардами параметров даже превосходят реальные поисковые системы."
                },
                "en": {
                    "title": "ZeroSearch: Enhancing LLM Search Without Real Engines",
                    "desc": "This paper presents ZeroSearch, a novel reinforcement learning framework designed to enhance the search capabilities of large language models (LLMs) without relying on real search engines. It addresses two significant challenges: the unpredictable quality of documents from search engines and the high costs associated with frequent API calls during RL training. ZeroSearch utilizes a supervised fine-tuning approach to create a retrieval module that can generate both relevant and noisy documents, followed by a curriculum-based strategy that gradually increases the difficulty of retrieval tasks. Experimental results show that ZeroSearch can effectively improve LLM search performance, with larger models outperforming traditional search engines."
                },
                "zh": {
                    "title": "提升LLMs搜索能力的创新框架",
                    "desc": "有效的信息搜索对于提升大型语言模型（LLMs）的推理和生成能力至关重要。本文提出了一种名为ZeroSearch的强化学习框架，旨在提高LLMs的搜索能力，而无需与真实搜索引擎互动。该方法通过轻量级的监督微调，将LLM转变为一个检索模块，并在训练过程中逐步降低生成文档的质量，以激发模型的推理能力。实验结果表明，ZeroSearch能够有效提升LLMs的搜索能力，并在不同参数规模的模型中表现出良好的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04622",
            "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with\n  Auto-Regressive Transformer",
            "url": "https://huggingface.co/papers/2505.04622",
            "abstract": "Shape primitive abstraction, which decomposes complex 3D shapes into simple geometric elements, plays a crucial role in human visual cognition and has broad applications in computer vision and graphics. While recent advances in 3D content generation have shown remarkable progress, existing primitive abstraction methods either rely on geometric optimization with limited semantic understanding or learn from small-scale, category-specific datasets, struggling to generalize across diverse shape categories. We present PrimitiveAnything, a novel framework that reformulates shape primitive abstraction as a primitive assembly generation task. PrimitiveAnything includes a shape-conditioned primitive transformer for auto-regressive generation and an ambiguity-free parameterization scheme to represent multiple types of primitives in a unified manner. The proposed framework directly learns the process of primitive assembly from large-scale human-crafted abstractions, enabling it to capture how humans decompose complex shapes into primitive elements. Through extensive experiments, we demonstrate that PrimitiveAnything can generate high-quality primitive assemblies that better align with human perception while maintaining geometric fidelity across diverse shape categories. It benefits various 3D applications and shows potential for enabling primitive-based user-generated content (UGC) in games. Project page: https://primitiveanything.github.io",
            "score": 8,
            "issue_id": 3652,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "8205883cc18835a6",
            "authors": [
                "Jingwen Ye",
                "Yuze He",
                "Yanning Zhou",
                "Yiqin Zhu",
                "Kaiwen Xiao",
                "Yong-Jin Liu",
                "Wei Yang",
                "Xiao Han"
            ],
            "affiliations": [
                "Tencent AIPD, China",
                "Tsinghua University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04622.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#3d",
                    "#cv",
                    "#games"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "Универсальная абстракция 3D-форм с помощью ИИ",
                    "desc": "Статья представляет PrimitiveAnything - новый фреймворк для абстракции 3D-форм с помощью примитивов. Он использует трансформер, обученный на масштабных данных человеческих абстракций, для автоматической генерации сборок примитивов. PrimitiveAnything применяет унифицированную параметризацию для разных типов примитивов и генерирует высококачественные абстракции, соответствующие человеческому восприятию. Фреймворк демонстрирует хорошую обобщающую способность на разнообразных категориях форм и имеет потенциал для применения в играх и других 3D-приложениях."
                },
                "en": {
                    "title": "Revolutionizing 3D Shape Understanding with PrimitiveAnything",
                    "desc": "This paper introduces PrimitiveAnything, a new framework for breaking down complex 3D shapes into simpler geometric parts, which is important for both human understanding and computer applications. Unlike previous methods that either optimize geometry without understanding or rely on small datasets, PrimitiveAnything learns from large-scale human-created examples to improve its generalization across different shape types. The framework uses a shape-conditioned primitive transformer for generating these parts in a structured way, ensuring clarity in how different primitives are represented. The results show that PrimitiveAnything produces high-quality assemblies that align well with human perception, making it useful for various 3D applications, including user-generated content in games."
                },
                "zh": {
                    "title": "形状抽象的新突破：PrimitiveAnything",
                    "desc": "形状原始抽象是将复杂的3D形状分解为简单几何元素的过程，这对人类视觉认知至关重要，并在计算机视觉和图形学中有广泛应用。现有的原始抽象方法通常依赖于几何优化，缺乏语义理解，或者仅从小规模、特定类别的数据集中学习，难以在多样的形状类别中进行泛化。我们提出了PrimitiveAnything，一个将形状原始抽象重新定义为原始组装生成任务的新框架。该框架通过大规模人类创作的抽象学习原始组装过程，从而能够更好地捕捉人类如何将复杂形状分解为原始元素。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04512",
            "title": "HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation",
            "url": "https://huggingface.co/papers/2505.04512",
            "abstract": "Customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. In this paper, we propose HunyuanCustom, a multi-modal customized video generation framework that emphasizes subject consistency while supporting image, audio, video, and text conditions. Built upon HunyuanVideo, our model first addresses the image-text conditioned generation task by introducing a text-image fusion module based on LLaVA for enhanced multi-modal understanding, along with an image ID enhancement module that leverages temporal concatenation to reinforce identity features across frames. To enable audio- and video-conditioned generation, we further propose modality-specific condition injection mechanisms: an AudioNet module that achieves hierarchical alignment via spatial cross-attention, and a video-driven injection module that integrates latent-compressed conditional video through a patchify-based feature-alignment network. Extensive experiments on single- and multi-subject scenarios demonstrate that HunyuanCustom significantly outperforms state-of-the-art open- and closed-source methods in terms of ID consistency, realism, and text-video alignment. Moreover, we validate its robustness across downstream tasks, including audio and video-driven customized video generation. Our results highlight the effectiveness of multi-modal conditioning and identity-preserving strategies in advancing controllable video generation. All the code and models are available at https://hunyuancustom.github.io.",
            "score": 8,
            "issue_id": 3652,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "82e5839ef846d9d8",
            "authors": [
                "Teng Hu",
                "Zhentao Yu",
                "Zhengguang Zhou",
                "Sen Liang",
                "Yuan Zhou",
                "Qin Lin",
                "Qinglin Lu"
            ],
            "affiliations": [
                "Tencent Hunyuan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04512.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Мультимодальная генерация персонализированных видео с сохранением идентичности",
                    "desc": "HunyuanCustom - это мультимодальная система для генерации персонализированных видео, поддерживающая условия в виде изображений, аудио, видео и текста. Она использует модуль слияния текста и изображений на основе LLaVA для улучшенного мультимодального понимания, а также модуль усиления идентификации изображений для сохранения согласованности личности в кадрах. Система включает специальные механизмы для внедрения аудио- и видеоусловий, такие как AudioNet и сеть выравнивания признаков на основе патчей. Эксперименты показывают, что HunyuanCustom превосходит современные методы по согласованности идентичности, реалистичности и соответствию текста видео."
                },
                "en": {
                    "title": "HunyuanCustom: Consistent and Multi-Modal Video Generation",
                    "desc": "This paper introduces HunyuanCustom, a framework for generating customized videos that maintain subject consistency while accommodating various input types like images, audio, and text. It enhances multi-modal understanding through a text-image fusion module and reinforces identity features across video frames with an image ID enhancement module. Additionally, it incorporates specialized mechanisms for audio and video conditioning, ensuring effective alignment and integration of different modalities. The results show that HunyuanCustom outperforms existing methods in terms of identity consistency, realism, and alignment with text, proving its effectiveness in controllable video generation."
                },
                "zh": {
                    "title": "多模态定制视频生成的创新之路",
                    "desc": "定制视频生成旨在根据用户定义的条件生成特定主题的视频，但现有方法在身份一致性和输入模态方面常常面临挑战。本文提出了HunyuanCustom，一个多模态定制视频生成框架，强调主题一致性，并支持图像、音频、视频和文本条件。我们的模型通过引入基于LLaVA的文本-图像融合模块和图像ID增强模块，解决了图像-文本条件生成任务，从而增强多模态理解。实验结果表明，HunyuanCustom在身份一致性、真实感和文本-视频对齐方面显著优于现有的最先进方法，验证了多模态条件和身份保持策略在可控视频生成中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03821",
            "title": "Beyond Recognition: Evaluating Visual Perspective Taking in Vision\n  Language Models",
            "url": "https://huggingface.co/papers/2505.03821",
            "abstract": "We investigate the ability of Vision Language Models (VLMs) to perform visual perspective taking using a novel set of visual tasks inspired by established human tests. Our approach leverages carefully controlled scenes, in which a single humanoid minifigure is paired with a single object. By systematically varying spatial configurations - such as object position relative to the humanoid minifigure and the humanoid minifigure's orientation - and using both bird's-eye and surface-level views, we created 144 unique visual tasks. Each visual task is paired with a series of 7 diagnostic questions designed to assess three levels of visual cognition: scene understanding, spatial reasoning, and visual perspective taking. Our evaluation of several state-of-the-art models, including GPT-4-Turbo, GPT-4o, Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that while they excel in scene understanding, the performance declines significantly on spatial reasoning and further deteriorates on perspective-taking. Our analysis suggests a gap between surface-level object recognition and the deeper spatial and perspective reasoning required for complex visual tasks, pointing to the need for integrating explicit geometric representations and tailored training protocols in future VLM development.",
            "score": 8,
            "issue_id": 3655,
            "pub_date": "2025-05-03",
            "pub_date_card": {
                "ru": "3 мая",
                "en": "May 3",
                "zh": "5月3日"
            },
            "hash": "abede452b390c7de",
            "authors": [
                "Gracjan Góral",
                "Alicja Ziarko",
                "Piotr Miłoś",
                "Michał Nauman",
                "Maciej Wołczyk",
                "Michał Kosiński"
            ],
            "affiliations": [
                "Faculty of Mathematics, Informatics and Mechanics, University of Warsaw, S. Banacha 2, 02-097 Warsaw, PL",
                "Graduate School of Business, Stanford University, Stanford, CA 94305, USA",
                "IDEAS NCBR, Chmielna 69, 00-801 Warsaw, PL",
                "Institute of Mathematics, Polish Academy of Sciences, J. & J. Sniadeckich 8, 00-656 Warsaw, PL",
                "Robot Learning Lab, University of California, Berkeley, CA 94720, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03821.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#reasoning",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "VLM модели: от распознавания объектов к пониманию перспективы",
                    "desc": "Исследователи изучают способность моделей компьютерного зрения и обработки естественного языка (VLM) к визуальному восприятию перспективы. Они разработали набор из 144 визуальных задач, используя сцены с миниатюрной фигуркой человека и объектом в различных пространственных конфигурациях. Эксперименты показали, что современные модели, такие как GPT-4-Turbo и Claude Sonnet, хорошо справляются с пониманием сцен, но значительно хуже выполняют задачи пространственного мышления и восприятия перспективы. Результаты указывают на необходимость интеграции явных геометрических представлений и специализированных протоколов обучения в будущих разработках VLM."
                },
                "en": {
                    "title": "Bridging the Gap: Enhancing VLMs for Spatial Reasoning and Perspective Taking",
                    "desc": "This paper explores how well Vision Language Models (VLMs) can understand visual perspectives through a series of unique tasks. The tasks involve a humanoid figure and an object in various spatial arrangements, designed to test scene understanding, spatial reasoning, and visual perspective taking. The study evaluates several advanced models, finding that while they perform well in recognizing scenes, they struggle with more complex reasoning tasks. The results highlight a significant gap in the models' abilities, suggesting that future developments should focus on incorporating geometric representations and specialized training methods."
                },
                "zh": {
                    "title": "提升视觉语言模型的空间推理能力",
                    "desc": "本文研究了视觉语言模型（VLMs）在视觉视角理解方面的能力，使用了一组新颖的视觉任务，这些任务灵感来源于人类的经典测试。我们设计了144个独特的视觉任务，通过系统地改变空间配置，如物体相对于人形小人偶的位置和方向，来评估模型的表现。每个视觉任务配有7个诊断问题，旨在评估场景理解、空间推理和视觉视角理解三个层次的视觉认知。评估结果显示，尽管这些先进模型在场景理解方面表现出色，但在空间推理和视角理解方面的表现显著下降，表明在复杂视觉任务中，表面物体识别与更深层次的空间和视角推理之间存在差距。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04364",
            "title": "Benchmarking LLMs' Swarm intelligence",
            "url": "https://huggingface.co/papers/2505.04364",
            "abstract": "Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, particularly concerning the nuances of swarm intelligence. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination that arise when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks within a configurable 2D grid environment, forcing agents to rely primarily on local sensory input (k x k view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Evaluating several leading LLMs in a zero-shot setting, we find significant performance variations across tasks, highlighting the difficulties posed by local information constraints. While some coordination emerges, results indicate limitations in robust planning and strategy formation under uncertainty in these decentralized scenarios. Assessing LLMs under swarm-like conditions is crucial for realizing their potential in future decentralized systems. We release SwarmBench as an open, extensible toolkit-built upon a customizable and scalable physical system with defined mechanical properties. It provides environments, prompts, evaluation scripts, and the comprehensive experimental datasets generated, aiming to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. Our code repository is available at https://github.com/x66ccff/swarmbench.",
            "score": 7,
            "issue_id": 3648,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "4b0575d2194aee20",
            "authors": [
                "Kai Ruan",
                "Mowen Huang",
                "Ji-Rong Wen",
                "Hao Sun"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04364.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#benchmark",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "🐝",
                "ru": {
                    "title": "SwarmBench: Тестирование роевого интеллекта языковых моделей",
                    "desc": "Статья представляет SwarmBench - новый бенчмарк для оценки способностей больших языковых моделей (LLM) к роевому интеллекту в многоагентных системах. SwarmBench включает пять задач координации в 2D-сетке, где агенты ограничены локальным восприятием и коммуникацией. Результаты экспериментов показывают значительные различия в производительности LLM между задачами, выявляя сложности планирования в условиях неопределенности. Авторы предоставляют открытый инструментарий для воспроизводимых исследований координации на основе LLM в многоагентных системах."
                },
                "en": {
                    "title": "Unlocking Swarm Intelligence in Language Models",
                    "desc": "This paper explores how Large Language Models (LLMs) can coordinate in Multi-Agent Systems (MAS) under strict constraints, similar to natural swarms. It introduces SwarmBench, a new benchmark that evaluates the swarm intelligence of LLMs by simulating decentralized coordination tasks in a 2D grid environment. The study highlights the challenges of local perception and communication, revealing significant performance variations among LLMs when faced with limited information. The findings emphasize the need for further research into LLMs' capabilities in decentralized scenarios to unlock their potential in future systems."
                },
                "zh": {
                    "title": "探索大型语言模型的群体智能潜力",
                    "desc": "大型语言模型（LLMs）在复杂推理方面显示出潜力，但它们在多智能体系统（MAS）中在严格约束下的协调能力仍然未被充分探索，尤其是在群体智能的细微差别方面。现有基准测试往往无法完全捕捉到在不完整时空信息下，智能体进行去中心化协调所面临的独特挑战。为此，我们引入了SwarmBench，这是一个新颖的基准，旨在系统评估LLMs作为去中心化智能体的群体智能能力。通过评估多个领先的LLMs，我们发现它们在任务中的表现差异显著，突显了在局部信息限制下的协调困难。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04528",
            "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving",
            "url": "https://huggingface.co/papers/2505.04528",
            "abstract": "As a seemingly self-explanatory task, problem-solving has been a significant component of science and engineering. However, a general yet concrete formulation of problem-solving itself is missing. With the recent development of AI-based problem-solving agents, the demand for process-level verifiability is rapidly increasing yet underexplored. To fill these gaps, we present a principled formulation of problem-solving as a deterministic Markov decision process; a novel framework, FPS (Formal Problem-Solving), which utilizes existing FTP (formal theorem proving) environments to perform process-verified problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer verification for better human-alignment. The expressiveness, soundness and completeness of the frameworks are proven. We construct three benchmarks on problem-solving: FormalMath500, a formalization of a subset of the MATH500 benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and human-aligned evaluation, we propose RPE (Restricted Propositional Equivalence), a symbolic approach to determine the correctness of answers by formal verification. We evaluate four prevalent FTP models and two prompting methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving.",
            "score": 5,
            "issue_id": 3652,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "0e9e0d509e4b4624",
            "authors": [
                "Qi Liu",
                "Xinhao Zheng",
                "Renqiu Xia",
                "Xingzhi Qi",
                "Qinxiang Cao",
                "Junchi Yan"
            ],
            "affiliations": [
                "Sch. of Computer Science & Sch. of Artificial Intelligence, Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04528.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#benchmark",
                    "#alignment",
                    "#interpretability",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Формальная верификация процесса решения задач искусственным интеллектом",
                    "desc": "Статья представляет новый подход к формализации решения задач как марковского процесса принятия решений. Авторы предлагают фреймворк FPS (Formal Problem-Solving), использующий среды формального доказательства теорем для верификации процесса решения задач. Также представлен D-FPS (Deductive FPS), разделяющий решение и проверку ответа для лучшего соответствия человеческому подходу. Созданы три новых набора данных для оценки систем решения задач, а также предложен метод RPE для формальной верификации корректности ответов."
                },
                "en": {
                    "title": "Revolutionizing Problem-Solving with Formal Frameworks",
                    "desc": "This paper addresses the challenge of formalizing problem-solving in science and engineering by proposing a new framework called FPS (Formal Problem-Solving). It treats problem-solving as a deterministic Markov decision process, allowing for process-level verifiability in AI-based agents. The authors introduce D-FPS (Deductive FPS) to separate the solving process from answer verification, enhancing alignment with human reasoning. They also present benchmarks for evaluating problem-solving capabilities and a novel method, RPE (Restricted Propositional Equivalence), for verifying the correctness of solutions through formal methods."
                },
                "zh": {
                    "title": "形式化问题解决的新框架",
                    "desc": "这篇论文探讨了问题解决的形式化，提出了一种将问题解决视为确定性马尔可夫决策过程的框架。作者介绍了FPS（正式问题解决）框架，利用现有的正式定理证明环境进行过程验证的问题解决。为了提高人类对齐，论文还提出了D-FPS（演绎FPS），将求解与答案验证解耦。最后，作者构建了三个基准测试，并提出了一种符号方法RPE来评估答案的正确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04606",
            "title": "OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue\n  Resolution",
            "url": "https://huggingface.co/papers/2505.04606",
            "abstract": "The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on a single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover a narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only 10.5% of the issues with image information. Finally, we analyze the reasons behind current LLMs' failure on OmniGIRL, providing insights for future improvements.",
            "score": 3,
            "issue_id": 3657,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "25e97f182730fc25",
            "authors": [
                "Lianghong Guo",
                "Wei Tao",
                "Runhan Jiang",
                "Yanlin Wang",
                "Jiachi Chen",
                "Xilin Liu",
                "Yuchi Ma",
                "Mingzhi Mao",
                "Hongyu Zhang",
                "Zibin Zheng"
            ],
            "affiliations": [
                "Chongqing University, China",
                "Huawei Cloud Computing Technologies Co., Ltd., China",
                "Independent Researcher, China",
                "Sun Yat-sen University, Zhuhai Key Laboratory of Trusted Large Language Models, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04606.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multilingual",
                    "#dataset",
                    "#long_context",
                    "#multimodal"
                ],
                "emoji": "🐙",
                "ru": {
                    "title": "OmniGIRL: Вызов для языковых моделей в решении задач GitHub",
                    "desc": "Статья представляет OmniGIRL - новый многоязычный, мультимодальный и мультидоменный эталонный набор данных для автоматического разрешения проблем на GitHub. OmniGIRL включает 959 задач из репозиториев на четырех языках программирования и восьми различных доменах. Оценка показала, что современные языковые модели (LLM) демонстрируют ограниченную эффективность на OmniGIRL, особенно при работе с изображениями. Анализ причин неудач LLM на OmniGIRL предоставляет insights для будущих улучшений."
                },
                "en": {
                    "title": "OmniGIRL: A Comprehensive Benchmark for GitHub Issue Resolution",
                    "desc": "This paper introduces OmniGIRL, a new benchmark for automatically resolving GitHub issues using large language models (LLMs). Unlike existing benchmarks, OmniGIRL is designed to be multilingual, multimodal, and multi-domain, addressing the limitations of focusing on a single programming language and a narrow range of issues. The benchmark includes 959 instances from four programming languages and eight domains, highlighting the diversity of real-world problems. Evaluation results show that current LLMs perform poorly on this benchmark, particularly in resolving issues that require understanding images, indicating a need for further advancements in model capabilities."
                },
                "zh": {
                    "title": "OmniGIRL：多语言多模态的GitHub问题解决基准",
                    "desc": "本文提出了OmniGIRL，一个多语言、多模态和多领域的GitHub问题解决基准。现有的基准存在三个主要限制：只关注单一编程语言、覆盖领域狭窄以及仅依赖文本信息。OmniGIRL包含来自四种编程语言和八个不同领域的959个任务实例，旨在更全面地评估大型语言模型的能力。我们的评估显示，当前的语言模型在OmniGIRL上的表现有限，尤其在处理需要理解图像的问题时表现更差。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03912",
            "title": "OpenHelix: A Short Survey, Empirical Analysis, and Open-Source\n  Dual-System VLA Model for Robotic Manipulation",
            "url": "https://huggingface.co/papers/2505.03912",
            "abstract": "Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. Project page: https://openhelix-robot.github.io/.",
            "score": 3,
            "issue_id": 3652,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 мая",
                "en": "May 6",
                "zh": "5月6日"
            },
            "hash": "f7347c1b093f9488",
            "authors": [
                "Can Cui",
                "Pengxiang Ding",
                "Wenxuan Song",
                "Shuanghao Bai",
                "Xinyang Tong",
                "Zirui Ge",
                "Runze Suo",
                "Wanqi Zhou",
                "Yang Liu",
                "Bofang Jia",
                "Han Zhao",
                "Siteng Huang",
                "Donglin Wang"
            ],
            "affiliations": [
                "HKUST(GZ)",
                "Westlake University",
                "Xian Jiaotong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03912.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#agents",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Открытая платформа для исследования двухсистемных VLA архитектур",
                    "desc": "Статья посвящена двухсистемным архитектурам VLA (Vision-Language-Action) в области воплощенного интеллекта. Авторы анализируют и сравнивают существующие архитектуры, проводя систематическую эмпирическую оценку их ключевых элементов. Целью работы является создание открытой модели с низкими вычислительными затратами для дальнейших исследований. Проект планирует регулярно обновляться новыми экспериментальными выводами и улучшенными открытыми моделями."
                },
                "en": {
                    "title": "Empowering Embodied Intelligence with Open-Source VLA Models",
                    "desc": "This paper focuses on dual-system Vision-Language-Action (VLA) architectures, which are important for developing embodied intelligence. It highlights the current lack of open-source resources that allow for thorough performance analysis and optimization of these architectures. The authors summarize and compare existing designs and conduct empirical evaluations on their core elements. The goal is to provide a low-cost open-source model that can be continuously updated with new findings and improved performance options for researchers."
                },
                "zh": {
                    "title": "推动双系统VLA架构的开源探索",
                    "desc": "本文探讨了双系统视觉-语言-行动（VLA）架构在具身智能研究中的重要性，并指出目前缺乏足够的开源工作来进行性能分析和优化。作者总结并比较了现有双系统架构的结构设计，并对其核心设计元素进行了系统的实证评估。最终，本文将提供一个低成本的开源模型，以便进一步探索和研究。该项目将持续更新，提供更多实验结论和性能改进的开源模型供大家选择。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03570",
            "title": "OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents",
            "url": "https://huggingface.co/papers/2505.03570",
            "abstract": "In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse.",
            "score": 2,
            "issue_id": 3654,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 мая",
                "en": "May 6",
                "zh": "5月6日"
            },
            "hash": "e87199c8805bce4f",
            "authors": [
                "Mariya Davydova",
                "Daniel Jeffries",
                "Patrick Barker",
                "Arturo Márquez Flores",
                "Sinéad Ryan"
            ],
            "affiliations": [
                "Kentauros AI Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03570.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#agents",
                    "#open_source",
                    "#multimodal",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "OSUniverse: новый стандарт оценки ИИ-агентов в графическом интерфейсе",
                    "desc": "В статье представлен OSUniverse - комплексный многомодальный бенчмарк для ИИ-агентов, навигирующих в графическом интерфейсе. Бенчмарк включает задачи разной сложности, от простых кликов до многошаговых тестов в нескольких приложениях. Современные агенты достигают не более 50% успеха, в то время как обычные офисные работники справляются со всеми задачами. Бенчмарк имеет автоматизированную систему валидации с погрешностью менее 2%."
                },
                "en": {
                    "title": "OSUniverse: Benchmarking AI Navigation in Complex Desktop Tasks",
                    "desc": "This paper presents OSUniverse, a benchmark designed for evaluating advanced AI agents in navigating complex desktop tasks. The tasks are categorized by increasing difficulty, challenging agents with skills like precision and multi-step reasoning. The benchmark is calibrated so that current state-of-the-art agents score below 50%, while average human workers can achieve perfect scores. Additionally, it features an automated validation system with a low error rate, enabling reliable assessment of AI progress in GUI navigation."
                },
                "zh": {
                    "title": "OSUniverse：GUI导航AI的全新基准",
                    "desc": "本文介绍了OSUniverse，这是一个针对高级GUI导航AI代理的复杂多模态桌面任务基准，旨在易用性、可扩展性、全面覆盖测试案例和自动验证方面表现出色。我们将任务分为不同复杂度的级别，从基本的精确点击到需要灵活性、精确性和清晰思维的多步骤、多应用程序测试。在基准的第一版中，我们调整了测试案例的复杂性，以确保当时的最先进（SOTA）代理的结果不超过50%，而普通白领工人可以完美完成所有这些任务。该基准可以手动评分，同时我们还引入了一个平均错误率低于2%的自动验证机制，为全面自动化测量GUI导航AI代理的进展、能力和有效性提供了坚实基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03418",
            "title": "Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey",
            "url": "https://huggingface.co/papers/2505.03418",
            "abstract": "Problem-solving has been a fundamental driver of human progress in numerous domains. With advancements in artificial intelligence, Large Language Models (LLMs) have emerged as powerful tools capable of tackling complex problems across diverse domains. Unlike traditional computational systems, LLMs combine raw computational power with an approximation of human reasoning, allowing them to generate solutions, make inferences, and even leverage external computational tools. However, applying LLMs to real-world problem-solving presents significant challenges, including multi-step reasoning, domain knowledge integration, and result verification. This survey explores the capabilities and limitations of LLMs in complex problem-solving, examining techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation, and various LLM-based and tool-based verification techniques. Additionally, we highlight domain-specific challenges in various domains, such as software engineering, mathematical reasoning and proving, data analysis and modeling, and scientific research. The paper further discusses the fundamental limitations of the current LLM solutions and the future directions of LLM-based complex problems solving from the perspective of multi-step reasoning, domain knowledge integration and result verification.",
            "score": 2,
            "issue_id": 3652,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 мая",
                "en": "May 6",
                "zh": "5月6日"
            },
            "hash": "8417799a01a2ecc2",
            "authors": [
                "Da Zheng",
                "Lun Du",
                "Junwei Su",
                "Yuchen Tian",
                "Yuqi Zhu",
                "Jintian Zhang",
                "Lanning Wei",
                "Ningyu Zhang",
                "Huajun Chen"
            ],
            "affiliations": [
                "Ant Group, China",
                "The University of Hong Kong, China",
                "Zhejiang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03418.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#survey",
                    "#math",
                    "#training",
                    "#reasoning",
                    "#science",
                    "#data"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LLM: Новый рубеж в решении сложных задач",
                    "desc": "Эта статья исследует возможности и ограничения больших языковых моделей (LLM) в решении сложных задач. Авторы рассматривают такие техники, как рассуждения по цепочке мыслей (Chain-of-Thought), расширение знаний и различные методы верификации на основе LLM и инструментов. В статье обсуждаются проблемы применения LLM в различных областях, включая разработку программного обеспечения, математические рассуждения и доказательства, анализ данных и научные исследования. Также рассматриваются фундаментальные ограничения текущих решений на основе LLM и будущие направления развития в контексте многоступенчатых рассуждений, интеграции доменных знаний и верификации результатов."
                },
                "en": {
                    "title": "Unlocking Complex Problem-Solving with Large Language Models",
                    "desc": "This paper surveys the role of Large Language Models (LLMs) in solving complex problems across various fields. It highlights how LLMs combine computational power with human-like reasoning to generate solutions and make inferences. The paper addresses challenges such as multi-step reasoning, integrating domain knowledge, and verifying results when applying LLMs in real-world scenarios. It also discusses specific challenges in areas like software engineering and scientific research, while outlining future directions for improving LLM capabilities in complex problem-solving."
                },
                "zh": {
                    "title": "大型语言模型：复杂问题解决的新工具",
                    "desc": "本论文探讨了大型语言模型（LLMs）在复杂问题解决中的能力和局限性。与传统计算系统不同，LLMs结合了强大的计算能力和人类推理的近似，能够生成解决方案和进行推理。尽管LLMs在多步骤推理、领域知识整合和结果验证方面面临挑战，但它们在软件工程、数学推理、数据分析和科学研究等领域的应用潜力巨大。本文还讨论了当前LLM解决方案的基本局限性以及未来在复杂问题解决中的发展方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.00358",
            "title": "R&B: Domain Regrouping and Data Mixture Balancing for Efficient\n  Foundation Model Training",
            "url": "https://huggingface.co/papers/2505.00358",
            "abstract": "Data mixing strategies have successfully reduced the costs involved in training language models. While promising, such methods suffer from two flaws. First, they rely on predetermined data domains (e.g., data sources, task types), which may fail to capture critical semantic nuances, leaving performance on the table. Second, these methods scale with the number of domains in a computationally prohibitive way. We address these challenges via R&B, a framework that re-partitions training data based on semantic similarity (Regroup) to create finer-grained domains, and efficiently optimizes the data composition (Balance) by leveraging a Gram matrix induced by domain gradients obtained throughout training. Unlike prior works, it removes the need for additional compute to obtain evaluation information such as losses or gradients. We analyze this technique under standard regularity conditions and provide theoretical insights that justify R&B's effectiveness compared to non-adaptive mixing approaches. Empirically, we demonstrate the effectiveness of R&B on five diverse datasets ranging from natural language to reasoning and multimodal tasks. With as little as 0.01% additional compute overhead, R&B matches or exceeds the performance of state-of-the-art data mixing strategies.",
            "score": 2,
            "issue_id": 3652,
            "pub_date": "2025-05-01",
            "pub_date_card": {
                "ru": "1 мая",
                "en": "May 1",
                "zh": "5月1日"
            },
            "hash": "74b251baea8510bd",
            "authors": [
                "Albert Ge",
                "Tzu-Heng Huang",
                "John Cooper",
                "Avi Trost",
                "Ziyi Chu",
                "Satya Sai Srinath Namburi GNVV",
                "Ziyang Cai",
                "Kendall Park",
                "Nicholas Roberts",
                "Frederic Sala"
            ],
            "affiliations": [
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.00358.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#data",
                    "#multimodal"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "R&B: Умное смешивание данных для эффективного обучения языковых моделей",
                    "desc": "Статья представляет новый фреймворк R&B для оптимизации стратегий смешивания данных при обучении языковых моделей. R&B перегруппирует обучающие данные на основе семантического сходства и эффективно оптимизирует состав данных, используя матрицу Грама, полученную из градиентов доменов. Этот метод устраняет необходимость в дополнительных вычислениях для получения оценочной информации. Теоретический и эмпирический анализ показывает эффективность R&B по сравнению с неадаптивными подходами к смешиванию данных."
                },
                "en": {
                    "title": "R&B: Smarter Data Mixing for Language Models",
                    "desc": "This paper introduces R&B, a novel framework for improving data mixing strategies in training language models. R&B addresses two main issues: the reliance on fixed data domains and the high computational cost associated with scaling these domains. By regrouping training data based on semantic similarity and optimizing data composition using domain gradients, R&B creates more effective and efficient training domains. The authors provide theoretical insights and empirical evidence showing that R&B can achieve superior performance with minimal additional computational overhead compared to existing methods."
                },
                "zh": {
                    "title": "R&B：高效的数据混合新策略",
                    "desc": "本文提出了一种新的数据混合策略R&B，旨在解决现有方法的两个主要缺陷。首先，R&B通过语义相似性重新划分训练数据，创建更细粒度的数据域，从而捕捉到重要的语义细节。其次，该框架通过利用训练过程中获得的领域梯度的Gram矩阵，优化数据组合，避免了额外的计算开销。实验结果表明，R&B在多种数据集上表现优异，能够以极小的计算成本超越现有的最先进数据混合策略。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.02393",
            "title": "Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly\n  Detection",
            "url": "https://huggingface.co/papers/2505.02393",
            "abstract": "Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that synthesizes event representations directly from RGB videos and fuses them with image features through a principled, uncertainty-aware process. The system (i) models heavy-tailed sensor noise with a Student`s-t likelihood, deriving value-level inverse-variance weights via a Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at https://github.com/EavnJeong/IEF-VAD.",
            "score": 1,
            "issue_id": 3651,
            "pub_date": "2025-05-05",
            "pub_date_card": {
                "ru": "5 мая",
                "en": "May 5",
                "zh": "5月5日"
            },
            "hash": "b5c708abbb25e1ce",
            "authors": [
                "Sungheon Jeong",
                "Jihong Park",
                "Mohsen Imani"
            ],
            "affiliations": [
                "MOLOCO",
                "University of California, Irvine"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.02393.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#benchmark",
                    "#multimodal",
                    "#synthetic"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Синтез событий из RGB для точного обнаружения видеоаномалий",
                    "desc": "В этой статье представлен метод IEF-VAD для обнаружения аномалий в видео, который объединяет RGB-кадры с синтезированными событийными представлениями. Система моделирует шум датчика, применяет покадровые обновления в стиле фильтра Калмана и итеративно уточняет слитое латентное состояние. IEF-VAD достигает нового уровня производительности на нескольких реальных тестовых наборах данных для обнаружения аномалий. Метод подчеркивает важность синтетических событийных представлений для выделения ключевых признаков движения в задачах анализа видео."
                },
                "en": {
                    "title": "Enhancing Video Anomaly Detection with Image-Event Fusion",
                    "desc": "The paper introduces a new method called Image-Event Fusion for Video Anomaly Detection (IEF-VAD) that improves the detection of unusual events in videos. Traditional methods rely only on RGB frames, which can miss important motion details. IEF-VAD combines RGB video data with synthetic event representations to enhance the detection process, using advanced techniques to manage noise and improve accuracy. This approach achieves state-of-the-art results in various benchmarks without needing special sensors or labeled data."
                },
                "zh": {
                    "title": "图像与事件融合，提升视频异常检测的准确性",
                    "desc": "现有的视频异常检测器主要依赖RGB帧，但这些帧缺乏捕捉突发或瞬态运动线索的时间分辨率。为了解决这个问题，我们提出了一种图像-事件融合的视频异常检测框架（IEF-VAD），该框架直接从RGB视频合成事件表示，并通过一种基于不确定性的过程将其与图像特征融合。该系统通过拉普拉斯近似建模重尾传感器噪声，应用卡尔曼风格的逐帧更新来平衡时间上的模态，并迭代优化融合的潜在状态以消除残余的跨模态噪声。IEF-VAD在多个真实世界的异常检测基准上设定了新的最先进水平，展示了合成事件表示在强调RGB帧中常被低估的运动线索方面的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.03105",
            "title": "Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI\n  Knowledge Co-Creation",
            "url": "https://huggingface.co/papers/2505.03105",
            "abstract": "Scientific knowledge creation is fundamentally transforming as humans and AI systems evolve beyond tool-user relationships into co-evolutionary epistemic partnerships. When AlphaFold revolutionized protein structure prediction, researchers described engaging with an epistemic partner that reshaped how they conceptualized fundamental relationships. This article introduces Cognitio Emergens (CE), a framework addressing critical limitations in existing models that focus on static roles or narrow metrics while failing to capture how scientific understanding emerges through recursive human-AI interaction over time. CE integrates three components addressing these limitations: Agency Configurations describing how authority distributes between humans and AI (Directed, Contributory, Partnership), with partnerships dynamically oscillating between configurations rather than following linear progression; Epistemic Dimensions capturing six specific capabilities emerging through collaboration across Discovery, Integration, and Projection axes, creating distinctive \"capability signatures\" that guide development; and Partnership Dynamics identifying forces shaping how these relationships evolve, particularly the risk of epistemic alienation where researchers lose interpretive control over knowledge they formally endorse. Drawing from autopoiesis theory, social systems theory, and organizational modularity, CE reveals how knowledge co-creation emerges through continuous negotiation of roles, values, and organizational structures. By reconceptualizing human-AI scientific collaboration as fundamentally co-evolutionary, CE offers a balanced perspective that neither uncritically celebrates nor unnecessarily fears AI's evolving role, instead providing conceptual tools for cultivating partnerships that maintain meaningful human participation while enabling transformative scientific breakthroughs.",
            "score": 0,
            "issue_id": 3657,
            "pub_date": "2025-05-06",
            "pub_date_card": {
                "ru": "6 мая",
                "en": "May 6",
                "zh": "5月6日"
            },
            "hash": "24cdaf99b9b04dad",
            "authors": [
                "Xule Lin"
            ],
            "affiliations": [
                "Department of Management and Entrepreneurship, Imperial College London"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.03105.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#healthcare",
                    "#science",
                    "#ethics",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Cognitio Emergens: новая парадигма коэволюции человека и ИИ в науке",
                    "desc": "Статья представляет концепцию Cognitio Emergens (CE) - новую модель сотрудничества человека и искусственного интеллекта в научных исследованиях. CE описывает, как распределяются роли между людьми и ИИ, какие эпистемические способности возникают в процессе взаимодействия, и какие силы влияют на эволюцию этих отношений. Модель подчеркивает динамичный и рекурсивный характер создания научных знаний в партнерстве человека и ИИ. CE предлагает сбалансированный взгляд на роль ИИ в науке, избегая как чрезмерного оптимизма, так и необоснованных страхов."
                },
                "en": {
                    "title": "Transforming Scientific Collaboration: Humans and AI as Co-Evolutionary Partners",
                    "desc": "This paper discusses how the relationship between humans and AI in scientific research is changing from a simple tool-user dynamic to a more collaborative partnership. It introduces a new framework called Cognitio Emergens (CE) that addresses the limitations of existing models by focusing on the evolving roles and interactions between humans and AI over time. CE includes three main components: Agency Configurations that describe how authority is shared, Epistemic Dimensions that outline capabilities developed through collaboration, and Partnership Dynamics that explore how these relationships change. By viewing human-AI collaboration as a co-evolutionary process, the framework aims to enhance scientific understanding while ensuring that human input remains significant in the face of advancing AI capabilities."
                },
                "zh": {
                    "title": "人类与AI的共同进化：知识创造的新视角",
                    "desc": "这篇论文探讨了人类与人工智能（AI）之间的合作关系如何从简单的工具使用者转变为共同进化的知识伙伴。文章介绍了Cognitio Emergens（CE）框架，旨在解决现有模型的局限性，强调科学理解是如何通过人类与AI的互动逐步形成的。CE框架包括三个主要组成部分：代理配置、认知维度和伙伴动态，帮助描述人类与AI之间的权力分配、合作能力和关系演变。通过重新定义人类与AI的科学合作，CE提供了促进有意义的人类参与和科学突破的概念工具。"
                }
            }
        }
    ],
    "link_prev": "2025-05-07.html",
    "link_next": "2025-05-09.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "07.05",
        "en": "05/07",
        "zh": "5月7日"
    },
    "short_date_next": {
        "ru": "09.05",
        "en": "05/09",
        "zh": "5月9日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 6,
        "#agents": 5,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 10,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 1,
        "#reasoning": 6,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 5,
        "#survey": 2,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了提升大型语言模型（LLMs）搜索能力的重要性。最近的研究使用强化学习（RL）与实时搜索引擎互动来改进LLMs的搜索能力，但面临文档质量不可控和API费用高昂的挑战。为解决这些问题，作者提出了ZeroSearch，一种不需要与实际搜索引擎互动的RL框架。通过轻量级的监督微调和基于课程的滚动策略，ZeroSearch能够有效提升LLMs的搜索能力，并且在不同参数规模的模型上表现良好。",
        "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
        "pinyin": "Zhè piān wénzhāng tǎolùn le tíshēng dàxíng yǔyán móxíng (LLMs) sōusuǒ nénglì de zhòngyàoxìng. Zuìjìn de yánjiū shǐyòng qiángzhì xuéxí (RL) yǔ shíshí sōusuǒ yǐnqíng hùdòng lái gǎijìn LLMs de sōusuǒ nénglì, dàn miànlín wénjiàn zhìliàng bù kě kòng hé API fèiyòng gāo'áng de tiǎozhàn. Wèi jiějué zhèxiē wèntí, zuòzhě tíchū le ZeroSearch, yīzhǒng bù xūyào yǔ shíjì sōusuǒ yǐnqíng hùdòng de RL kuàngjià. Tōngguò qīngliàngjí de jiàndū wēitiáo hé jīyú kèchéng de gǔndòng cèlüè, ZeroSearch nénggòu yǒuxiào tíshēng LLMs de sōusuǒ nénglì, bìngqiě zài bùtóng cānshù guīmó de móxíng shàng biǎoxiàn liánghǎo.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'},\n{'word': '提升', 'pinyin': 'tí shēng', 'trans': 'improve'},\n{'word': '大型', 'pinyin': 'dà xíng', 'trans': 'large-scale'},\n{'word': '语言模型', 'pinyin': 'yǔ yán mó xíng', 'trans': 'language model'},\n{'word': '搜索', 'pinyin': 'sōu suǒ', 'trans': 'search'},\n{'word': '能力', 'pinyin': 'néng lì', 'trans': 'ability'},\n{'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'},\n{'word': '强化学习', 'pinyin': 'qiáng huà xué xí', 'trans': 'reinforcement learning'},\n{'word': '互动', 'pinyin': 'hù dòng', 'trans': 'interact'},\n{'word': '改进', 'pinyin': 'gǎi jìn', 'trans': 'improve'},\n{'word': '文档', 'pinyin': 'wén dàng', 'trans': 'document'},\n{'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'},\n{'word': '不可控', 'pinyin': 'bù kě kòng', 'trans': 'uncontrollable'},\n{'word': 'API', 'pinyin': 'API', 'trans': 'API'},\n{'word': '费用', 'pinyin': 'fèi yòng', 'trans': 'cost'},\n{'word': '高昂', 'pinyin': 'gāo áng', 'trans': 'high'},\n{'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'},\n{'word': '解决', 'pinyin': 'jiě jué', 'trans': 'solve'},\n{'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'},\n{'word': 'ZeroSearch', 'pinyin': 'ZeroSearch', 'trans': 'ZeroSearch'},\n{'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'},\n{'word': '轻量级', 'pinyin': 'qīng liàng jí', 'trans': 'lightweight'},\n{'word': '监督', 'pinyin': 'jiàn dū', 'trans': 'supervised'},\n{'word': '微调', 'pinyin': 'wēi tiáo', 'trans': 'fine-tune'},\n{'word': '基于', 'pinyin': 'jī yú', 'trans': 'based on'},\n{'word': '课程', 'pinyin': 'kè chéng', 'trans': 'course'},\n{'word': '滚动', 'pinyin': 'gǔn dòng', 'trans': 'rolling'},\n{'word': '策略', 'pinyin': 'cè lüè', 'trans': 'strategy'},\n{'word': '有效', 'pinyin': 'yǒu xiào', 'trans': 'effective'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'},\n{'word': '良好', 'pinyin': 'liáng hǎo', 'trans': 'good'},\n{'word': '参数', 'pinyin': 'cān shǔ', 'trans': 'parameter'},\n{'word': '规模', 'pinyin': 'guī mó', 'trans': 'scale'}]",
        "trans": "This article discusses the importance of enhancing the search capabilities of large language models (LLMs). Recent research has employed reinforcement learning (RL) to interact with real-time search engines to improve the search capabilities of LLMs, but this approach faces challenges such as uncontrollable document quality and high API costs. To address these issues, the authors propose ZeroSearch, an RL framework that does not require interaction with actual search engines. By utilizing lightweight supervised fine-tuning and a curriculum-based rolling strategy, ZeroSearch can effectively enhance the search capabilities of LLMs and performs well across models of different parameter sizes.",
        "update_ts": "2025-05-08 09:12"
    }
}