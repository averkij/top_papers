{
    "date": {
        "ru": "8 мая",
        "en": "May 8",
        "zh": "5月8日"
    },
    "time_utc": "2025-05-08 04:15",
    "weekday": 3,
    "issue_id": 3649,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.04588",
            "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
            "url": "https://huggingface.co/papers/2505.04588",
            "abstract": "Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.",
            "score": 7,
            "issue_id": 3647,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "24edc7c3c5e5e23d",
            "authors": [
                "Hao Sun",
                "Zile Qiao",
                "Jiayan Guo",
                "Xuanbo Fan",
                "Yingyan Hou",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Yan Zhang"
            ],
            "affiliations": [
                "Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04588.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "ZeroSearch: обучение LLM эффективному поиску без реальных поисковых систем",
                    "desc": "Статья представляет ZeroSearch - новую систему обучения с подкреплением для улучшения поисковых возможностей больших языковых моделей (LLM). В отличие от предыдущих подходов, ZeroSearch не требует взаимодействия с реальными поисковыми системами, что решает проблемы неконтролируемого качества документов и высоких затрат на API. Метод использует легковесную предобученную модель в качестве модуля поиска и стратегию постепенного ухудшения качества генерируемых документов во время обучения. Эксперименты показывают, что ZeroSearch эффективно улучшает поисковые способности LLM, причем модели с 14 миллиардами параметров даже превосходят реальные поисковые системы."
                },
                "en": {
                    "title": "ZeroSearch: Enhancing LLM Search Without Real Engines",
                    "desc": "This paper presents ZeroSearch, a novel reinforcement learning framework designed to enhance the search capabilities of large language models (LLMs) without relying on real search engines. It addresses two significant challenges: the unpredictable quality of documents from search engines and the high costs associated with frequent API calls during RL training. ZeroSearch utilizes a supervised fine-tuning approach to create a retrieval module that can generate both relevant and noisy documents, followed by a curriculum-based strategy that gradually increases the difficulty of retrieval tasks. Experimental results show that ZeroSearch can effectively improve LLM search performance, with larger models outperforming traditional search engines."
                },
                "zh": {
                    "title": "提升LLMs搜索能力的创新框架",
                    "desc": "有效的信息搜索对于提升大型语言模型（LLMs）的推理和生成能力至关重要。本文提出了一种名为ZeroSearch的强化学习框架，旨在提高LLMs的搜索能力，而无需与真实搜索引擎互动。该方法通过轻量级的监督微调，将LLM转变为一个检索模块，并在训练过程中逐步降低生成文档的质量，以激发模型的推理能力。实验结果表明，ZeroSearch能够有效提升LLMs的搜索能力，并在不同参数规模的模型中表现出良好的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.04364",
            "title": "Benchmarking LLMs' Swarm intelligence",
            "url": "https://huggingface.co/papers/2505.04364",
            "abstract": "Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, particularly concerning the nuances of swarm intelligence. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination that arise when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks within a configurable 2D grid environment, forcing agents to rely primarily on local sensory input (k x k view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Evaluating several leading LLMs in a zero-shot setting, we find significant performance variations across tasks, highlighting the difficulties posed by local information constraints. While some coordination emerges, results indicate limitations in robust planning and strategy formation under uncertainty in these decentralized scenarios. Assessing LLMs under swarm-like conditions is crucial for realizing their potential in future decentralized systems. We release SwarmBench as an open, extensible toolkit-built upon a customizable and scalable physical system with defined mechanical properties. It provides environments, prompts, evaluation scripts, and the comprehensive experimental datasets generated, aiming to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. Our code repository is available at https://github.com/x66ccff/swarmbench.",
            "score": 4,
            "issue_id": 3648,
            "pub_date": "2025-05-07",
            "pub_date_card": {
                "ru": "7 мая",
                "en": "May 7",
                "zh": "5月7日"
            },
            "hash": "4b0575d2194aee20",
            "authors": [
                "Kai Ruan",
                "Mowen Huang",
                "Ji-Rong Wen",
                "Hao Sun"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.04364.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#benchmark",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "🐝",
                "ru": {
                    "title": "SwarmBench: Тестирование роевого интеллекта языковых моделей",
                    "desc": "Статья представляет SwarmBench - новый бенчмарк для оценки способностей больших языковых моделей (LLM) к роевому интеллекту в многоагентных системах. SwarmBench включает пять задач координации в 2D-сетке, где агенты ограничены локальным восприятием и коммуникацией. Результаты экспериментов показывают значительные различия в производительности LLM между задачами, выявляя сложности планирования в условиях неопределенности. Авторы предоставляют открытый инструментарий для воспроизводимых исследований координации на основе LLM в многоагентных системах."
                },
                "en": {
                    "title": "Unlocking Swarm Intelligence in Language Models",
                    "desc": "This paper explores how Large Language Models (LLMs) can coordinate in Multi-Agent Systems (MAS) under strict constraints, similar to natural swarms. It introduces SwarmBench, a new benchmark that evaluates the swarm intelligence of LLMs by simulating decentralized coordination tasks in a 2D grid environment. The study highlights the challenges of local perception and communication, revealing significant performance variations among LLMs when faced with limited information. The findings emphasize the need for further research into LLMs' capabilities in decentralized scenarios to unlock their potential in future systems."
                },
                "zh": {
                    "title": "探索大型语言模型的群体智能潜力",
                    "desc": "大型语言模型（LLMs）在复杂推理方面显示出潜力，但它们在多智能体系统（MAS）中在严格约束下的协调能力仍然未被充分探索，尤其是在群体智能的细微差别方面。现有基准测试往往无法完全捕捉到在不完整时空信息下，智能体进行去中心化协调所面临的独特挑战。为此，我们引入了SwarmBench，这是一个新颖的基准，旨在系统评估LLMs作为去中心化智能体的群体智能能力。通过评估多个领先的LLMs，我们发现它们在任务中的表现差异显著，突显了在局部信息限制下的协调困难。"
                }
            }
        }
    ],
    "link_prev": "2025-05-07.html",
    "link_next": "2025-05-09.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "07.05",
        "en": "05/07",
        "zh": "5月7日"
    },
    "short_date_next": {
        "ru": "09.05",
        "en": "05/09",
        "zh": "5月9日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的多模态奖励模型，称为 UnifiedReward-Think。它能够进行长链条的推理，以提高视觉理解和生成任务的奖励信号准确性。模型通过三个步骤进行训练：首先，使用少量图像生成偏好数据蒸馏 GPT-4 的推理过程；然后，利用模型的先验知识和泛化能力，准备大规模的多模态偏好数据；最后，通过群体相对策略优化进行增强微调。实验结果证明了该模型的优越性。",
        "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement\n  Fine-Tuning",
        "pinyin": "Zhè piān wén zhāng jiè shào le yī zhǒng xīn de duō mó tài jiǎng lì mó xíng, chēng wéi UnifiedReward-Think. Tā néng gòu jìn xíng cháng liàn tiáo de tuí lǐ, yǐ tí gāo shì jué lǐ jiě hé shēng chéng rèn wù de jiǎng lì xìn hào zhùn què xìng. Mó xíng tōng guò sān gè bù zhòu jìn xíng xùn liàn: shǒu xiān, shǐ yòng shǎo liàng tú xiàng shēng chéng piàn hào shù jùn zhèng GPT-4 de tuí lǐ guò chéng; rán hòu, lì yòng mó xíng de xiān yán zhī shì hé fàn huà néng lì, zhǔn bèi dà guī mó de duō mó tài piàn hào shù jù; zùi hòu, tōng guò qún tǐ xiāng duì cè lüè yōu huà jìn xíng zēng qiáng wēi tiáo. Shí yàn jié guǒ zhèng míng le gāi mó xíng de yōu yuè xìng.",
        "vocab": "[\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"奖励\", \"pinyin\": \"jiǎng lì\", \"trans\": \"reward\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understanding\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēng chéng\", \"trans\": \"generation\"},\n    {\"word\": \"任务\", \"pinyin\": \"rèn wu\", \"trans\": \"task\"},\n    {\"word\": \"准确性\", \"pinyin\": \"zhǔn què xìng\", \"trans\": \"accuracy\"},\n    {\"word\": \"蒸馏\", \"pinyin\": \"zhēng liú\", \"trans\": \"distill\"},\n    {\"word\": \"先验\", \"pinyin\": \"xiān yàn\", \"trans\": \"prior\"},\n    {\"word\": \"知识\", \"pinyin\": \"zhī shi\", \"trans\": \"knowledge\"},\n    {\"word\": \"泛化\", \"pinyin\": \"fàn huà\", \"trans\": \"generalization\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"优化\", \"pinyin\": \"yōu huà\", \"trans\": \"optimization\"},\n    {\"word\": \"微调\", \"pinyin\": \"wēi tiáo\", \"trans\": \"fine-tuning\"},\n    {\"word\": \"证明\", \"pinyin\": \"zhèng míng\", \"trans\": \"prove\"},\n    {\"word\": \"优越性\", \"pinyin\": \"yōu yuè xìng\", \"trans\": \"superiority\"}\n]",
        "trans": "This article introduces a new multimodal reward model called UnifiedReward-Think. It is capable of performing long-chain reasoning to enhance the accuracy of reward signals for visual understanding and generation tasks. The model is trained through three steps: first, using a small amount of image generation preference data to distill the reasoning process of GPT-4; then, leveraging the model's prior knowledge and generalization capabilities to prepare large-scale multimodal preference data; finally, performing reinforcement fine-tuning through population-based relative strategy optimization. Experimental results demonstrate the superiority of this model.",
        "update_ts": "2025-05-07 09:12"
    }
}