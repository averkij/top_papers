{
    "date": {
        "ru": "26 декабря",
        "en": "December 26",
        "zh": "12月26日"
    },
    "time_utc": "2024-12-26 07:10",
    "weekday": 3,
    "issue_id": 1331,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.18547",
            "title": "Token-Budget-Aware LLM Reasoning",
            "url": "https://huggingface.co/papers/2412.18547",
            "abstract": "Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework, which dynamically estimates token budgets for different problems based on reasoning complexity and uses the estimated token budgets to guide the reasoning process. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE.",
            "score": 3,
            "issue_id": 1328,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 декабря",
                "en": "December 24",
                "zh": "12月24日"
            },
            "hash": "9a018cda2c47f064",
            "authors": [
                "Tingxu Han",
                "Chunrong Fang",
                "Shiyu Zhao",
                "Shiqing Ma",
                "Zhenyu Chen",
                "Zhenting Wang"
            ],
            "affiliations": [
                "Nanjing University",
                "Rutgers University",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18547.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "💡",
                "ru": {
                    "title": "Эффективные рассуждения ИИ: больше мыслей, меньше токенов",
                    "desc": "Статья описывает новый подход к рассуждениям в больших языковых моделях (LLM), направленный на оптимизацию использования токенов. Авторы предлагают метод, который динамически оценивает бюджет токенов для различных задач на основе сложности рассуждений. Этот подход позволяет значительно сократить расходы на токены при использовании метода цепочки мыслей (CoT), сохраняя при этом высокую точность. Эксперименты показывают эффективность предложенного метода в балансировании эффективности и точности рассуждений LLM."
                },
                "en": {
                    "title": "Optimizing Reasoning Efficiency in LLMs with Token Budgets",
                    "desc": "This paper addresses the reasoning efficiency of large language models (LLMs) by introducing a token-budget-aware framework. It highlights that while Chain-of-Thought (CoT) reasoning improves performance, it also increases token usage and costs. The authors propose a method to dynamically estimate token budgets based on the complexity of reasoning tasks, allowing for more efficient use of tokens. Experimental results demonstrate that this approach reduces token costs with minimal impact on performance, providing a balance between efficiency and accuracy in LLM reasoning."
                },
                "zh": {
                    "title": "优化推理，降低成本！",
                    "desc": "推理对于大型语言模型（LLMs）在多种任务中表现出色至关重要。虽然链式推理（CoT）方法通过将问题分解为中间步骤来提高LLM性能，但这也导致了显著的令牌使用开销，增加了成本。我们发现当前LLM的推理过程过于冗长，可以通过在提示中包含合理的令牌预算来压缩，但令牌预算的选择对压缩效果至关重要。我们提出了一种基于令牌预算的LLM推理框架，动态估计不同问题的令牌预算，从而在保持效率和准确性之间取得平衡。"
                }
            }
        }
    ],
    "link_prev": "2024-12-25.html",
    "link_next": "2024-12-27.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "25.12",
        "en": "12/25",
        "zh": "12月25日"
    },
    "short_date_next": {
        "ru": "27.12",
        "en": "12/27",
        "zh": "12月27日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了深度数据中缺失值的挑战。DepthLab 是一个基于图像扩散先验的深度修复模型，能够处理缺失区域。它具有两个优点：对连续区域和孤立点都能提供可靠的填补，并保持已知深度的尺度一致性。该方法在3D场景修复、文本到3D场景生成等任务中表现出色，性能和视觉质量都超过现有解决方案。项目页面和源代码可以在 https://johanan528.github.io/depthlab_web/ 找到。",
        "title": "DepthLab: From Partial to Complete",
        "pinyin": "这篇文章讨论了深度数据中缺失值的挑战。\nZhè piān wénzhāng tǎolùn le shēndù shùjù zhōng quēshīzhí de tiǎozhàn.\n\nDepthLab 是一个基于图像扩散先验的深度修复模型，能够处理缺失区域。\nDepthLab shì yīgè jīyú túxiàng kuòsàn xiānyán de shēndù xiūfù móxíng, nénggòu chǔlǐ quēshī qūyù.\n\n它具有两个优点：对连续区域和孤立点都能提供可靠的填补，并保持已知深度的尺度一致性。\nTā jùyǒu liǎng gè yōudiǎn: duì liánxù qūyù hé gūlìdiǎn dōu néng tígōng kěkào de tiánbǔ, bìng bǎochí yǐzhī shēndù de chǐdù yīzhìxìng.\n\n该方法在3D场景修复、文本到3D场景生成等任务中表现出色，性能和视觉质量都超过现有解决方案。\nGǎi fāngfǎ zài 3D chǎngjǐng xiūfù, wénběn dào 3D chǎngjǐng shēngchéng děng rènwù zhōng biǎoxiàn chūsè, xìngnéng hé shìjué zhìliàng dōu chāoguò xiànyǒu jiějué fāng'àn.\n\n项目页面和源代码可以在 https://johanan528.github.io/depthlab_web/ 找到。\nXiàngmù yèmiàn hé yuán dàimǎ kěyǐ zài https://johanan528.github.io/depthlab_web/ zhǎo dào.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '深度', 'pinyin': 'shēn dù', 'trans': 'depth'}, {'word': '缺失值', 'pinyin': 'quē shī zhí', 'trans': 'missing value'}, {'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'}, {'word': '基于', 'pinyin': 'jī yú', 'trans': 'based on'}, {'word': '图像', 'pinyin': 'tú xiàng', 'trans': 'image'}, {'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'}, {'word': '先验', 'pinyin': 'xiān yàn', 'trans': 'prior'}, {'word': '修复', 'pinyin': 'xiū fù', 'trans': 'repair'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '处理', 'pinyin': 'chǔ lǐ', 'trans': 'handle'}, {'word': '区域', 'pinyin': 'qū yù', 'trans': 'region'}, {'word': '连续', 'pinyin': 'lián xù', 'trans': 'continuous'}, {'word': '孤立点', 'pinyin': 'gū lì diǎn', 'trans': 'isolated point'}, {'word': '可靠', 'pinyin': 'kě kào', 'trans': 'reliable'}, {'word': '填补', 'pinyin': 'tián bǔ', 'trans': 'fill in'}, {'word': '保持', 'pinyin': 'bǎo chí', 'trans': 'maintain'}, {'word': '尺度', 'pinyin': 'chǐ dù', 'trans': 'scale'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '3D', 'pinyin': '3D', 'trans': '3D'}, {'word': '场景', 'pinyin': 'chǎng jǐng', 'trans': 'scene'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '任务', 'pinyin': 'rèn wù', 'trans': 'task'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'}, {'word': '解决方案', 'pinyin': 'jiě jué fāng àn', 'trans': 'solution'}, {'word': '项目', 'pinyin': 'xiàng mù', 'trans': 'project'}, {'word': '页面', 'pinyin': 'yè miàn', 'trans': 'page'}, {'word': '源代码', 'pinyin': 'yuán dài mǎ', 'trans': 'source code'}, {'word': '找到', 'pinyin': 'zhǎo dào', 'trans': 'find'}]",
        "trans": "This article discusses the challenges of missing values in depth data. DepthLab is a depth inpainting model based on image diffusion priors that can handle missing regions. It has two advantages: it provides reliable filling for both continuous regions and isolated points, and maintains the scale consistency of known depths. This method performs exceptionally well in tasks such as 3D scene repair and text-to-3D scene generation, outperforming existing solutions in both performance and visual quality. The project page and source code can be found at https://johanan528.github.io/depthlab_web/.",
        "update_ts": "2024-12-25 09:10"
    }
}