{
    "date": {
        "ru": "26 декабря",
        "en": "December 26",
        "zh": "12月26日"
    },
    "time_utc": "2024-12-26 11:08",
    "weekday": 3,
    "issue_id": 1335,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.18547",
            "title": "Token-Budget-Aware LLM Reasoning",
            "url": "https://huggingface.co/papers/2412.18547",
            "abstract": "Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework, which dynamically estimates token budgets for different problems based on reasoning complexity and uses the estimated token budgets to guide the reasoning process. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE.",
            "score": 6,
            "issue_id": 1328,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 декабря",
                "en": "December 24",
                "zh": "12月24日"
            },
            "hash": "9a018cda2c47f064",
            "authors": [
                "Tingxu Han",
                "Chunrong Fang",
                "Shiyu Zhao",
                "Shiqing Ma",
                "Zhenyu Chen",
                "Zhenting Wang"
            ],
            "affiliations": [
                "Nanjing University",
                "Rutgers University",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18547.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "💡",
                "ru": {
                    "title": "Эффективные рассуждения ИИ: больше мыслей, меньше токенов",
                    "desc": "Статья описывает новый подход к рассуждениям в больших языковых моделях (LLM), направленный на оптимизацию использования токенов. Авторы предлагают метод, который динамически оценивает бюджет токенов для различных задач на основе сложности рассуждений. Этот подход позволяет значительно сократить расходы на токены при использовании метода цепочки мыслей (CoT), сохраняя при этом высокую точность. Эксперименты показывают эффективность предложенного метода в балансировании эффективности и точности рассуждений LLM."
                },
                "en": {
                    "title": "Optimizing Reasoning Efficiency in LLMs with Token Budgets",
                    "desc": "This paper addresses the reasoning efficiency of large language models (LLMs) by introducing a token-budget-aware framework. It highlights that while Chain-of-Thought (CoT) reasoning improves performance, it also increases token usage and costs. The authors propose a method to dynamically estimate token budgets based on the complexity of reasoning tasks, allowing for more efficient use of tokens. Experimental results demonstrate that this approach reduces token costs with minimal impact on performance, providing a balance between efficiency and accuracy in LLM reasoning."
                },
                "zh": {
                    "title": "优化推理，降低成本！",
                    "desc": "推理对于大型语言模型（LLMs）在多种任务中表现出色至关重要。虽然链式推理（CoT）方法通过将问题分解为中间步骤来提高LLM性能，但这也导致了显著的令牌使用开销，增加了成本。我们发现当前LLM的推理过程过于冗长，可以通过在提示中包含合理的令牌预算来压缩，但令牌预算的选择对压缩效果至关重要。我们提出了一种基于令牌预算的LLM推理框架，动态估计不同问题的令牌预算，从而在保持效率和准确性之间取得平衡。"
                }
            }
        }
    ],
    "link_prev": "2024-12-25.html",
    "link_next": "2024-12-27.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "25.12",
        "en": "12/25",
        "zh": "12月25日"
    },
    "short_date_next": {
        "ru": "27.12",
        "en": "12/27",
        "zh": "12月27日"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章讨论了大语言模型（LLMs）中推理的重要性。虽然Chain-of-Thought（CoT）推理方法通过将问题分解为中间步骤来提高LLM性能，但也增加了令牌使用的开销，导致成本增加。研究发现，当前LLMs的推理过程过于冗长，可以通过在提示中包含合理的令牌预算来压缩，但令牌预算的选择对实际压缩效果至关重要。作者提出了一个令牌预算感知的LLM推理框架，根据推理复杂性动态估算不同问题的令牌预算，并使用估算的令牌预算指导推理过程。实验表明，这种方法在CoT推理中有效地减少了令牌成本，仅略微降低了性能，提供了一种在LLM推理中平衡效率和准确性的实用解决方案。代码：https://github.com/GeniusHTX/TALE。",
        "title": "Token-Budget-Aware LLM Reasoning",
        "pinyin": "这篇文章讨论了大语言模型（LLMs）中推理的重要性。虽然Chain-of-Thought（CoT）推理方法通过将问题分解为中间步骤来提高LLM性能，但也增加了令牌使用的开销，导致成本增加。研究发现，当前LLMs的推理过程过于冗长，可以通过在提示中包含合理的令牌预算来压缩，但令牌预算的选择对实际压缩效果至关重要。作者提出了一个令牌预算感知的LLM推理框架，根据推理复杂性动态估算不同问题的令牌预算，并使用估算的令牌预算指导推理过程。实验表明，这种方法在CoT推理中有效地减少了令牌成本，仅略微降低了性能，提供了一种在LLM推理中平衡效率和准确性的实用解决方案。代码：https://github.com/GeniusHTX/TALE。\n\nzhè piān wén zhāng tǎo lùn le dà yǔ yán mó xíng (LLMs) zhōng tuī lǐ de zhòng yào xìng. suī rán Chain-of-Thought (CoT) tuī lǐ fāng fǎ tōng guò jiāng wèn tí fēn jiě wéi zhōng jiān bù zhòu lái tí gāo LLM xìng néng, dàn yě zēng jiā le lìng pái shǐ yòng de kāi xiǎo, dǎo zhì chéng běn zēng jiā. yán jiū fā xiàn, dāng qián LLMs de tuī lǐ guò chéng guò yú rǒng cháng, kě yǐ tōng guò zài tí shì zhōng bāo hán hé lǐ de lìng pái yù suàn lái yā suō, dàn lìng pái yù suàn de xuǎn zé duì shí jì yā suō xiào guǒ zhì guān zhòng yào. zuò zhě tí chū le yī gè lìng pái yù suàn gǎn zhī de LLM tuī lǐ kuàng jià, gēn jù tuī lǐ fú zà xìng dòng tài gū sǔan bù tóng wèn tí de lìng pái yù suàn, bìng shǐ yòng gū sǔan de lìng pái yù suàn zhǐ dǎo tuī lǐ guò chéng. shí yàn biǎo míng, zhè zhǒng fāng fǎ zài CoT tuī lǐ zhōng yǒu xiào de jiǎn shǎo le lìng pái chéng běn, jǐn lüè wēi jīng le xìng néng, tí gōng le yī zhǒng zài LLM tuī lǐ zhōng píng héng xiào yì hé zhǔn què xìng de shí yòng jiě jué fāng àn. dài mǎ: https://github.com/GeniusHTX/TALE.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '大语言模型', 'pinyin': 'dà yǔ yán mó xíng', 'trans': 'large language model'}, {'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'reasoning'}, {'word': '重要性', 'pinyin': 'zhòng yào xìng', 'trans': 'importance'}, {'word': 'Chain-of-Thought', 'pinyin': 'Chain-of-Thought', 'trans': 'Chain-of-Thought'}, {'word': '提高', 'pinyin': 'tí gāo', 'trans': 'improve'}, {'word': '令牌', 'pinyin': 'lìng pái', 'trans': 'token'}, {'word': '使用', 'pinyin': 'shǐ yòng', 'trans': 'use'}, {'word': '开销', 'pinyin': 'kāi xiāo', 'trans': 'cost'}, {'word': '导致', 'pinyin': 'dǎo zhì', 'trans': 'lead to'}, {'word': '成本', 'pinyin': 'chéng běn', 'trans': 'cost'}, {'word': '增加', 'pinyin': 'zēng jiā', 'trans': 'increase'}, {'word': '研究', 'pinyin': 'yán jiū', 'trans': 'research'}, {'word': '发现', 'pinyin': 'fā xiàn', 'trans': 'discover'}, {'word': '当前', 'pinyin': 'dāng qián', 'trans': 'current'}, {'word': '过程', 'pinyin': 'guò chéng', 'trans': 'process'}, {'word': '冗长', 'pinyin': 'rǒng cháng', 'trans': 'tedious'}, {'word': '压缩', 'pinyin': 'yā suō', 'trans': 'compress'}, {'word': '提示', 'pinyin': 'tí shì', 'trans': 'prompt'}, {'word': '包含', 'pinyin': 'bāo hán', 'trans': 'include'}, {'word': '合理', 'pinyin': 'hé lǐ', 'trans': 'reasonable'}, {'word': '预算', 'pinyin': 'yù suàn', 'trans': 'budget'}, {'word': '至关重要', 'pinyin': 'zhì guān zhòng yào', 'trans': 'crucial'}, {'word': '提出', 'pinyin': 'tí chū', 'trans': 'propose'}, {'word': '感知', 'pinyin': 'gǎn zhī', 'trans': 'perceive'}, {'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'}, {'word': '根据', 'pinyin': 'gēn jù', 'trans': 'based on'}, {'word': '复杂性', 'pinyin': 'fù zá xìng', 'trans': 'complexity'}, {'word': '动态', 'pinyin': 'dòng tài', 'trans': 'dynamic'}, {'word': '估算', 'pinyin': 'gū suàn', 'trans': 'estimate'}, {'word': '指导', 'pinyin': 'zhǐ dǎo', 'trans': 'guide'}, {'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'}, {'word': '表明', 'pinyin': 'biǎo míng', 'trans': 'indicate'}, {'word': '方法', 'pinyin': 'fāng fǎ', 'trans': 'method'}, {'word': '有效', 'pinyin': 'yǒu xiào', 'trans': 'effective'}, {'word': '减少', 'pinyin': 'jiǎn shǎo', 'trans': 'reduce'}, {'word': '略微', 'pinyin': 'lüè wēi', 'trans': 'slightly'}, {'word': '降低', 'pinyin': 'jiàng dī', 'trans': 'lower'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '提供', 'pinyin': 'tí gōng', 'trans': 'provide'}, {'word': '平衡', 'pinyin': 'píng héng', 'trans': 'balance'}, {'word': '效率', 'pinyin': 'xiào lǜ', 'trans': 'efficiency'}, {'word': '准确性', 'pinyin': 'zhǔn què xìng', 'trans': 'accuracy'}, {'word': '解决方案', 'pinyin': 'jiě jué fāng àn', 'trans': 'solution'}, {'word': '实用', 'pinyin': 'shí yòng', 'trans': 'practical'}, {'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'}]",
        "trans": "This article discusses the importance of reasoning in large language models (LLMs). Although the Chain-of-Thought (CoT) reasoning method improves LLM performance by breaking down problems into intermediate steps, it also increases the overhead of token usage, leading to higher costs. Research has found that the current reasoning process in LLMs is overly lengthy and can be compressed by including a reasonable token budget in the prompts. However, the choice of token budget is crucial for the actual compression effect. The authors propose a token budget-aware LLM reasoning framework that dynamically estimates the token budget for different problems based on reasoning complexity and uses the estimated token budget to guide the reasoning process. Experiments show that this method effectively reduces token costs in CoT reasoning with only a slight decrease in performance, providing a practical solution for balancing efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE.",
        "update_ts": "2024-12-26 09:10"
    }
}