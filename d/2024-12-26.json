{
    "date": {
        "ru": "26 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 26",
        "zh": "12æœˆ26æ—¥"
    },
    "time_utc": "2024-12-26 11:08",
    "weekday": 3,
    "issue_id": 1335,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.18547",
            "title": "Token-Budget-Aware LLM Reasoning",
            "url": "https://huggingface.co/papers/2412.18547",
            "abstract": "Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework, which dynamically estimates token budgets for different problems based on reasoning complexity and uses the estimated token budgets to guide the reasoning process. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE.",
            "score": 6,
            "issue_id": 1328,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "9a018cda2c47f064",
            "authors": [
                "Tingxu Han",
                "Chunrong Fang",
                "Shiyu Zhao",
                "Shiqing Ma",
                "Zhenyu Chen",
                "Zhenting Wang"
            ],
            "affiliations": [
                "Nanjing University",
                "Rutgers University",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18547.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸ’¡",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜: Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹, Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM), Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (CoT), ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM."
                },
                "en": {
                    "title": "Optimizing Reasoning Efficiency in LLMs with Token Budgets",
                    "desc": "This paper addresses the reasoning efficiency of large language models (LLMs) by introducing a token-budget-aware framework. It highlights that while Chain-of-Thought (CoT) reasoning improves performance, it also increases token usage and costs. The authors propose a method to dynamically estimate token budgets based on the complexity of reasoning tasks, allowing for more efficient use of tokens. Experimental results demonstrate that this approach reduces token costs with minimal impact on performance, providing a balance between efficiency and accuracy in LLM reasoning."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ¨ç†ï¼Œé™ä½æˆæœ¬ï¼",
                    "desc": "æ¨ç†å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²è‡³å…³é‡è¦ã€‚è™½ç„¶é“¾å¼æ¨ç†ï¼ˆCoTï¼‰æ–¹æ³•é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºä¸­é—´æ­¥éª¤æ¥æé«˜LLMæ€§èƒ½ï¼Œä½†è¿™ä¹Ÿå¯¼è‡´äº†æ˜¾è‘—çš„ä»¤ç‰Œä½¿ç”¨å¼€é”€ï¼Œå¢åŠ äº†æˆæœ¬ã€‚æˆ‘ä»¬å‘ç°å½“å‰LLMçš„æ¨ç†è¿‡ç¨‹è¿‡äºå†—é•¿ï¼Œå¯ä»¥é€šè¿‡åœ¨æç¤ºä¸­åŒ…å«åˆç†çš„ä»¤ç‰Œé¢„ç®—æ¥å‹ç¼©ï¼Œä½†ä»¤ç‰Œé¢„ç®—çš„é€‰æ‹©å¯¹å‹ç¼©æ•ˆæœè‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä»¤ç‰Œé¢„ç®—çš„LLMæ¨ç†æ¡†æ¶ï¼ŒåŠ¨æ€ä¼°è®¡ä¸åŒé—®é¢˜çš„ä»¤ç‰Œé¢„ç®—ï¼Œä»è€Œåœ¨ä¿æŒæ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-25.html",
    "link_next": "2024-12-27.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "25.12",
        "en": "12/25",
        "zh": "12æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "27.12",
        "en": "12/27",
        "zh": "12æœˆ27æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ¨ç†çš„é‡è¦æ€§ã€‚è™½ç„¶Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºä¸­é—´æ­¥éª¤æ¥æé«˜LLMæ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†ä»¤ç‰Œä½¿ç”¨çš„å¼€é”€ï¼Œå¯¼è‡´æˆæœ¬å¢åŠ ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰LLMsçš„æ¨ç†è¿‡ç¨‹è¿‡äºå†—é•¿ï¼Œå¯ä»¥é€šè¿‡åœ¨æç¤ºä¸­åŒ…å«åˆç†çš„ä»¤ç‰Œé¢„ç®—æ¥å‹ç¼©ï¼Œä½†ä»¤ç‰Œé¢„ç®—çš„é€‰æ‹©å¯¹å®é™…å‹ç¼©æ•ˆæœè‡³å…³é‡è¦ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªä»¤ç‰Œé¢„ç®—æ„ŸçŸ¥çš„LLMæ¨ç†æ¡†æ¶ï¼Œæ ¹æ®æ¨ç†å¤æ‚æ€§åŠ¨æ€ä¼°ç®—ä¸åŒé—®é¢˜çš„ä»¤ç‰Œé¢„ç®—ï¼Œå¹¶ä½¿ç”¨ä¼°ç®—çš„ä»¤ç‰Œé¢„ç®—æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨CoTæ¨ç†ä¸­æœ‰æ•ˆåœ°å‡å°‘äº†ä»¤ç‰Œæˆæœ¬ï¼Œä»…ç•¥å¾®é™ä½äº†æ€§èƒ½ï¼Œæä¾›äº†ä¸€ç§åœ¨LLMæ¨ç†ä¸­å¹³è¡¡æ•ˆç‡å’Œå‡†ç¡®æ€§çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚ä»£ç ï¼šhttps://github.com/GeniusHTX/TALEã€‚",
        "title": "Token-Budget-Aware LLM Reasoning",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ¨ç†çš„é‡è¦æ€§ã€‚è™½ç„¶Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºä¸­é—´æ­¥éª¤æ¥æé«˜LLMæ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†ä»¤ç‰Œä½¿ç”¨çš„å¼€é”€ï¼Œå¯¼è‡´æˆæœ¬å¢åŠ ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰LLMsçš„æ¨ç†è¿‡ç¨‹è¿‡äºå†—é•¿ï¼Œå¯ä»¥é€šè¿‡åœ¨æç¤ºä¸­åŒ…å«åˆç†çš„ä»¤ç‰Œé¢„ç®—æ¥å‹ç¼©ï¼Œä½†ä»¤ç‰Œé¢„ç®—çš„é€‰æ‹©å¯¹å®é™…å‹ç¼©æ•ˆæœè‡³å…³é‡è¦ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªä»¤ç‰Œé¢„ç®—æ„ŸçŸ¥çš„LLMæ¨ç†æ¡†æ¶ï¼Œæ ¹æ®æ¨ç†å¤æ‚æ€§åŠ¨æ€ä¼°ç®—ä¸åŒé—®é¢˜çš„ä»¤ç‰Œé¢„ç®—ï¼Œå¹¶ä½¿ç”¨ä¼°ç®—çš„ä»¤ç‰Œé¢„ç®—æŒ‡å¯¼æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨CoTæ¨ç†ä¸­æœ‰æ•ˆåœ°å‡å°‘äº†ä»¤ç‰Œæˆæœ¬ï¼Œä»…ç•¥å¾®é™ä½äº†æ€§èƒ½ï¼Œæä¾›äº†ä¸€ç§åœ¨LLMæ¨ç†ä¸­å¹³è¡¡æ•ˆç‡å’Œå‡†ç¡®æ€§çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚ä»£ç ï¼šhttps://github.com/GeniusHTX/TALEã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng (LLMs) zhÅng tuÄ« lÇ de zhÃ²ng yÃ o xÃ¬ng. suÄ« rÃ¡n Chain-of-Thought (CoT) tuÄ« lÇ fÄng fÇ tÅng guÃ² jiÄng wÃ¨n tÃ­ fÄ“n jiÄ› wÃ©i zhÅng jiÄn bÃ¹ zhÃ²u lÃ¡i tÃ­ gÄo LLM xÃ¬ng nÃ©ng, dÃ n yÄ› zÄ“ng jiÄ le lÃ¬ng pÃ¡i shÇ yÃ²ng de kÄi xiÇo, dÇo zhÃ¬ chÃ©ng bÄ›n zÄ“ng jiÄ. yÃ¡n jiÅ« fÄ xiÃ n, dÄng qiÃ¡n LLMs de tuÄ« lÇ guÃ² chÃ©ng guÃ² yÃº rÇ’ng chÃ¡ng, kÄ› yÇ tÅng guÃ² zÃ i tÃ­ shÃ¬ zhÅng bÄo hÃ¡n hÃ© lÇ de lÃ¬ng pÃ¡i yÃ¹ suÃ n lÃ¡i yÄ suÅ, dÃ n lÃ¬ng pÃ¡i yÃ¹ suÃ n de xuÇn zÃ© duÃ¬ shÃ­ jÃ¬ yÄ suÅ xiÃ o guÇ’ zhÃ¬ guÄn zhÃ²ng yÃ o. zuÃ² zhÄ› tÃ­ chÅ« le yÄ« gÃ¨ lÃ¬ng pÃ¡i yÃ¹ suÃ n gÇn zhÄ« de LLM tuÄ« lÇ kuÃ ng jiÃ , gÄ“n jÃ¹ tuÄ« lÇ fÃº zÃ  xÃ¬ng dÃ²ng tÃ i gÅ« sÇ”an bÃ¹ tÃ³ng wÃ¨n tÃ­ de lÃ¬ng pÃ¡i yÃ¹ suÃ n, bÃ¬ng shÇ yÃ²ng gÅ« sÇ”an de lÃ¬ng pÃ¡i yÃ¹ suÃ n zhÇ dÇo tuÄ« lÇ guÃ² chÃ©ng. shÃ­ yÃ n biÇo mÃ­ng, zhÃ¨ zhÇ’ng fÄng fÇ zÃ i CoT tuÄ« lÇ zhÅng yÇ’u xiÃ o de jiÇn shÇo le lÃ¬ng pÃ¡i chÃ©ng bÄ›n, jÇn lÃ¼Ã¨ wÄ“i jÄ«ng le xÃ¬ng nÃ©ng, tÃ­ gÅng le yÄ« zhÇ’ng zÃ i LLM tuÄ« lÇ zhÅng pÃ­ng hÃ©ng xiÃ o yÃ¬ hÃ© zhÇ”n quÃ¨ xÃ¬ng de shÃ­ yÃ²ng jiÄ› juÃ© fÄng Ã n. dÃ i mÇ: https://github.com/GeniusHTX/TALE.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'å¤§è¯­è¨€æ¨¡å‹', 'pinyin': 'dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng', 'trans': 'large language model'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'reasoning'}, {'word': 'é‡è¦æ€§', 'pinyin': 'zhÃ²ng yÃ o xÃ¬ng', 'trans': 'importance'}, {'word': 'Chain-of-Thought', 'pinyin': 'Chain-of-Thought', 'trans': 'Chain-of-Thought'}, {'word': 'æé«˜', 'pinyin': 'tÃ­ gÄo', 'trans': 'improve'}, {'word': 'ä»¤ç‰Œ', 'pinyin': 'lÃ¬ng pÃ¡i', 'trans': 'token'}, {'word': 'ä½¿ç”¨', 'pinyin': 'shÇ yÃ²ng', 'trans': 'use'}, {'word': 'å¼€é”€', 'pinyin': 'kÄi xiÄo', 'trans': 'cost'}, {'word': 'å¯¼è‡´', 'pinyin': 'dÇo zhÃ¬', 'trans': 'lead to'}, {'word': 'æˆæœ¬', 'pinyin': 'chÃ©ng bÄ›n', 'trans': 'cost'}, {'word': 'å¢åŠ ', 'pinyin': 'zÄ“ng jiÄ', 'trans': 'increase'}, {'word': 'ç ”ç©¶', 'pinyin': 'yÃ¡n jiÅ«', 'trans': 'research'}, {'word': 'å‘ç°', 'pinyin': 'fÄ xiÃ n', 'trans': 'discover'}, {'word': 'å½“å‰', 'pinyin': 'dÄng qiÃ¡n', 'trans': 'current'}, {'word': 'è¿‡ç¨‹', 'pinyin': 'guÃ² chÃ©ng', 'trans': 'process'}, {'word': 'å†—é•¿', 'pinyin': 'rÇ’ng chÃ¡ng', 'trans': 'tedious'}, {'word': 'å‹ç¼©', 'pinyin': 'yÄ suÅ', 'trans': 'compress'}, {'word': 'æç¤º', 'pinyin': 'tÃ­ shÃ¬', 'trans': 'prompt'}, {'word': 'åŒ…å«', 'pinyin': 'bÄo hÃ¡n', 'trans': 'include'}, {'word': 'åˆç†', 'pinyin': 'hÃ© lÇ', 'trans': 'reasonable'}, {'word': 'é¢„ç®—', 'pinyin': 'yÃ¹ suÃ n', 'trans': 'budget'}, {'word': 'è‡³å…³é‡è¦', 'pinyin': 'zhÃ¬ guÄn zhÃ²ng yÃ o', 'trans': 'crucial'}, {'word': 'æå‡º', 'pinyin': 'tÃ­ chÅ«', 'trans': 'propose'}, {'word': 'æ„ŸçŸ¥', 'pinyin': 'gÇn zhÄ«', 'trans': 'perceive'}, {'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'}, {'word': 'æ ¹æ®', 'pinyin': 'gÄ“n jÃ¹', 'trans': 'based on'}, {'word': 'å¤æ‚æ€§', 'pinyin': 'fÃ¹ zÃ¡ xÃ¬ng', 'trans': 'complexity'}, {'word': 'åŠ¨æ€', 'pinyin': 'dÃ²ng tÃ i', 'trans': 'dynamic'}, {'word': 'ä¼°ç®—', 'pinyin': 'gÅ« suÃ n', 'trans': 'estimate'}, {'word': 'æŒ‡å¯¼', 'pinyin': 'zhÇ dÇo', 'trans': 'guide'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'}, {'word': 'è¡¨æ˜', 'pinyin': 'biÇo mÃ­ng', 'trans': 'indicate'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄng fÇ', 'trans': 'method'}, {'word': 'æœ‰æ•ˆ', 'pinyin': 'yÇ’u xiÃ o', 'trans': 'effective'}, {'word': 'å‡å°‘', 'pinyin': 'jiÇn shÇo', 'trans': 'reduce'}, {'word': 'ç•¥å¾®', 'pinyin': 'lÃ¼Ã¨ wÄ“i', 'trans': 'slightly'}, {'word': 'é™ä½', 'pinyin': 'jiÃ ng dÄ«', 'trans': 'lower'}, {'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'}, {'word': 'æä¾›', 'pinyin': 'tÃ­ gÅng', 'trans': 'provide'}, {'word': 'å¹³è¡¡', 'pinyin': 'pÃ­ng hÃ©ng', 'trans': 'balance'}, {'word': 'æ•ˆç‡', 'pinyin': 'xiÃ o lÇœ', 'trans': 'efficiency'}, {'word': 'å‡†ç¡®æ€§', 'pinyin': 'zhÇ”n quÃ¨ xÃ¬ng', 'trans': 'accuracy'}, {'word': 'è§£å†³æ–¹æ¡ˆ', 'pinyin': 'jiÄ› juÃ© fÄng Ã n', 'trans': 'solution'}, {'word': 'å®ç”¨', 'pinyin': 'shÃ­ yÃ²ng', 'trans': 'practical'}, {'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'}]",
        "trans": "This article discusses the importance of reasoning in large language models (LLMs). Although the Chain-of-Thought (CoT) reasoning method improves LLM performance by breaking down problems into intermediate steps, it also increases the overhead of token usage, leading to higher costs. Research has found that the current reasoning process in LLMs is overly lengthy and can be compressed by including a reasonable token budget in the prompts. However, the choice of token budget is crucial for the actual compression effect. The authors propose a token budget-aware LLM reasoning framework that dynamically estimates the token budget for different problems based on reasoning complexity and uses the estimated token budget to guide the reasoning process. Experiments show that this method effectively reduces token costs in CoT reasoning with only a slight decrease in performance, providing a practical solution for balancing efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE.",
        "update_ts": "2024-12-26 09:10"
    }
}