{
    "date": {
        "ru": "26 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 26",
        "zh": "12æœˆ26æ—¥"
    },
    "time_utc": "2024-12-26 00:45",
    "weekday": 3,
    "issue_id": 1325,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.18450",
            "title": "3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding",
            "url": "https://huggingface.co/papers/2412.18450",
            "abstract": "A 3D scene graph represents a compact scene model, storing information about the objects and the semantic relationships between them, making its use promising for robotic tasks. When interacting with a user, an embodied intelligent agent should be capable of responding to various queries about the scene formulated in natural language. Large Language Models (LLMs) are beneficial solutions for user-robot interaction due to their natural language understanding and reasoning abilities. Recent methods for creating learnable representations of 3D scenes have demonstrated the potential to improve the quality of LLMs responses by adapting to the 3D world. However, the existing methods do not explicitly utilize information about the semantic relationships between objects, limiting themselves to information about their coordinates. In this work, we propose a method 3DGraphLLM for constructing a learnable representation of a 3D scene graph. The learnable representation is used as input for LLMs to perform 3D vision-language tasks. In our experiments on popular ScanRefer, RIORefer, Multi3DRefer, ScanQA, Sqa3D, and Scan2cap datasets, we demonstrate the advantage of this approach over baseline methods that do not use information about the semantic relationships between objects. The code is publicly available at https://github.com/CognitiveAISystems/3DGraphLLM.",
            "score": 21,
            "issue_id": 1311,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "3d80e95d793a8b5e",
            "authors": [
                "Tatiana Zemskova",
                "Dmitry Yudin"
            ],
            "affiliations": [
                "Artificial Intelligence Research Institute",
                "Moscow Institute of Physics and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18450.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal",
                    "#reasoning",
                    "#open_source",
                    "#agents",
                    "#games",
                    "#graphs"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "3D-Ğ³Ñ€Ğ°Ñ„Ñ‹ ÑÑ†ĞµĞ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ 3DGraphLLM Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ° ÑÑ†ĞµĞ½Ñ‹. Ğ­Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ 3D-Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ScanRefer, RIORefer Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ."
                },
                "en": {
                    "title": "Enhancing Robot Understanding with 3D Scene Graphs and Language Models",
                    "desc": "This paper introduces 3DGraphLLM, a method for creating a learnable representation of 3D scene graphs that captures both object information and their semantic relationships. By integrating this representation with Large Language Models (LLMs), the approach enhances user-robot interactions, allowing robots to better understand and respond to natural language queries about 3D scenes. The authors demonstrate that their method outperforms existing techniques that only consider object coordinates, highlighting the importance of semantic relationships in improving LLM responses. Experiments conducted on various datasets show the effectiveness of 3DGraphLLM in performing 3D vision-language tasks."
                },
                "zh": {
                    "title": "æå‡æœºå™¨äººäº¤äº’çš„3Dåœºæ™¯ç†è§£",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º3DGraphLLMçš„æ–¹æ³•ï¼Œç”¨äºæ„å»º3Dåœºæ™¯å›¾çš„å¯å­¦ä¹ è¡¨ç¤ºã€‚è¯¥è¡¨ç¤ºèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰å¯¹è±¡ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œä»è€Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨3Dè§†è§‰-è¯­è¨€ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒï¼Œç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨è¯­ä¹‰å…³ç³»çš„ä¿¡æ¯å¯ä»¥æ˜¾è‘—æ”¹å–„æ¨¡å‹çš„å“åº”è´¨é‡ã€‚è¯¥æ–¹æ³•ä¸ºæœºå™¨äººä¸ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€äº¤äº’æä¾›äº†æ›´å¼ºå¤§çš„æ”¯æŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18153",
            "title": "DepthLab: From Partial to Complete",
            "url": "https://huggingface.co/papers/2412.18153",
            "abstract": "Missing values remain a common challenge for depth data across its wide range of applications, stemming from various causes like incomplete data acquisition and perspective alteration. This work bridges this gap with DepthLab, a foundation depth inpainting model powered by image diffusion priors. Our model features two notable strengths: (1) it demonstrates resilience to depth-deficient regions, providing reliable completion for both continuous areas and isolated points, and (2) it faithfully preserves scale consistency with the conditioned known depth when filling in missing values. Drawing on these advantages, our approach proves its worth in various downstream tasks, including 3D scene inpainting, text-to-3D scene generation, sparse-view reconstruction with DUST3R, and LiDAR depth completion, exceeding current solutions in both numerical performance and visual quality. Our project page with source code is available at https://johanan528.github.io/depthlab_web/.",
            "score": 21,
            "issue_id": 1305,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "c319c831137b3ce6",
            "authors": [
                "Zhiheng Liu",
                "Ka Leong Cheng",
                "Qiuyu Wang",
                "Shuzhe Wang",
                "Hao Ouyang",
                "Bin Tan",
                "Kai Zhu",
                "Yujun Shen",
                "Qifeng Chen",
                "Ping Luo"
            ],
            "affiliations": [
                "Aalto University",
                "Ant Group",
                "HKU",
                "HKUST",
                "Tongyi Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18153.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ•³ï¸",
                "ru": {
                    "title": "DepthLab: Ğ’Ğ¾ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "DepthLab - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ°Ñ…. ĞĞ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ñ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ñ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ². DepthLab Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³ 3D-ÑÑ†ĞµĞ½ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LiDAR."
                },
                "en": {
                    "title": "DepthLab: Bridging the Gap in Depth Data Completion",
                    "desc": "This paper presents DepthLab, a novel model designed to address the issue of missing values in depth data, which often occurs due to incomplete data collection or changes in perspective. DepthLab utilizes image diffusion priors to effectively inpaint depth information, ensuring that both continuous and isolated missing regions are filled accurately. The model maintains scale consistency with known depth values, which is crucial for realistic depth completion. DepthLab outperforms existing methods in various applications, such as 3D scene inpainting and LiDAR depth completion, demonstrating superior numerical and visual results."
                },
                "zh": {
                    "title": "æ·±åº¦ä¿®å¤æ–°çªç ´ï¼šDepthLabæ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDepthLabçš„æ·±åº¦å›¾åƒä¿®å¤æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦æ•°æ®ä¸­çš„ç¼ºå¤±å€¼é—®é¢˜ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å›¾åƒæ‰©æ•£å…ˆéªŒï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¡«è¡¥æ·±åº¦ä¸è¶³çš„åŒºåŸŸï¼Œç¡®ä¿è¿ç»­åŒºåŸŸå’Œå­¤ç«‹ç‚¹çš„å¯é ä¿®å¤ã€‚DepthLabåœ¨å¡«è¡¥ç¼ºå¤±å€¼æ—¶ï¼Œèƒ½å¤Ÿä¿æŒä¸å·²çŸ¥æ·±åº¦çš„ä¸€è‡´æ€§ï¼Œç¡®ä¿å°ºåº¦çš„å‡†ç¡®æ€§ã€‚é€šè¿‡è¿™äº›ä¼˜åŠ¿ï¼Œè¯¥æ¨¡å‹åœ¨3Dåœºæ™¯ä¿®å¤ã€æ–‡æœ¬åˆ°3Dåœºæ™¯ç”Ÿæˆã€ç¨€ç–è§†å›¾é‡å»ºå’ŒLiDARæ·±åº¦è¡¥å…¨ç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17739",
            "title": "Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization",
            "url": "https://huggingface.co/papers/2412.17739",
            "abstract": "Extending the context length of Language Models (LMs) by improving Rotary Position Embedding (RoPE) has become a trend. While existing works mainly address RoPE's limitations within attention mechanism, this paper provides an analysis across nearly all parts of LMs, uncovering their adverse effects on length generalization for RoPE-based attention. Using Discrete Signal Processing theory, we show that RoPE enables periodic attention by implicitly achieving Non-Uniform Discrete Fourier Transform. However, this periodicity is undermined by the spectral damage caused by: 1) linear layers and activation functions outside of attention; 2) insufficiently trained frequency components brought by time-domain truncation. Building on our observations, we propose Fourier Position Embedding (FoPE), which enhances attention's frequency-domain properties to improve both its periodic extension and length generalization. FoPE constructs Fourier Series and zero-outs the destructive frequency components, increasing model robustness against the spectrum damage. Experiments across various model scales show that, within varying context windows, FoPE can maintain a more stable perplexity and a more consistent accuracy in a needle-in-haystack task compared to RoPE and ALiBi. Several analyses and ablations bring further support to our method and theoretical modeling.",
            "score": 17,
            "issue_id": 1306,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "1ce9c827a32ec3c5",
            "authors": [
                "Ermo Hua",
                "Che Jiang",
                "Xingtai Lv",
                "Kaiyan Zhang",
                "Ning Ding",
                "Youbang Sun",
                "Biqing Qi",
                "Yuchen Fan",
                "Xue Kai Zhu",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Northeastern University",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17739.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#long_context",
                    "#architecture"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¤ÑƒÑ€ÑŒĞµ",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Fourier Position Embedding (FoPE). FoPE ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Rotary Position Embedding (RoPE) Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ FoPE ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ñ€ÑĞ´Ñ‹ Ğ¤ÑƒÑ€ÑŒĞµ Ğ¸ Ğ¾Ğ±Ğ½ÑƒĞ»ÑĞµÑ‚ Ğ´ĞµÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FoPE Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ°Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ RoPE Ğ¸ ALiBi."
                },
                "en": {
                    "title": "Enhancing Language Models with Fourier Position Embedding for Better Context Handling",
                    "desc": "This paper explores the limitations of Rotary Position Embedding (RoPE) in Language Models (LMs) and its impact on length generalization. It reveals that while RoPE allows for periodic attention through Non-Uniform Discrete Fourier Transform, this capability is compromised by linear layers and insufficient training of frequency components. The authors introduce Fourier Position Embedding (FoPE), which improves the frequency-domain characteristics of attention by eliminating harmful frequency components. Experimental results demonstrate that FoPE outperforms RoPE and ALiBi in maintaining stability in perplexity and accuracy across different context lengths."
                },
                "zh": {
                    "title": "æå‡è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸æ³›åŒ–èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†é€šè¿‡æ”¹è¿›æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰æ¥æ‰©å±•è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚æˆ‘ä»¬åˆ†æäº†RoPEåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¹‹å¤–çš„å„ä¸ªéƒ¨åˆ†çš„å½±å“ï¼Œå‘ç°å…¶å¯¹é•¿åº¦æ³›åŒ–çš„è´Ÿé¢æ•ˆåº”ã€‚åŸºäºç¦»æ•£ä¿¡å·å¤„ç†ç†è®ºï¼Œæˆ‘ä»¬æå‡ºäº†å‚…é‡Œå¶ä½ç½®åµŒå…¥ï¼ˆFoPEï¼‰ï¼Œå®ƒé€šè¿‡æ„å»ºå‚…é‡Œå¶çº§æ•°æ¥å¢å¼ºæ³¨æ„åŠ›çš„é¢‘åŸŸç‰¹æ€§ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFoPEåœ¨ä¸åŒä¸Šä¸‹æ–‡çª—å£ä¸‹èƒ½å¤Ÿä¿æŒæ›´ç¨³å®šçš„å›°æƒ‘åº¦å’Œä¸€è‡´çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18597",
            "title": "DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation",
            "url": "https://huggingface.co/papers/2412.18597",
            "abstract": "Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer MM-DiT architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiT's attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance without additional training.",
            "score": 10,
            "issue_id": 1307,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "210ce3ba0e7e45d2",
            "authors": [
                "Minghong Cai",
                "Xiaodong Cun",
                "Xiaoyu Li",
                "Wenze Liu",
                "Zhaoyang Zhang",
                "Yong Zhang",
                "Ying Shan",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "GVC Lab, Great Bay University",
                "MMLab, The Chinese University of Hong Kong",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18597.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#games",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸĞ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ DiTCtrl Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Multi-Modal Diffusion Transformer (MM-DiT) Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞµĞ³Ğ¾ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MPVBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Multi-Prompt Control",
                    "desc": "This paper introduces DiTCtrl, a novel method for generating videos using multiple prompts without the need for additional training. It leverages the Multi-Modal Diffusion Transformer (MM-DiT) architecture to facilitate smooth transitions and coherent object motion across sequential prompts. By analyzing the attention mechanism of MM-DiT, the authors enable precise semantic control, allowing for effective multi-prompt video generation. The proposed method outperforms existing techniques and is evaluated using a new benchmark called MPVBench, specifically designed for this purpose."
                },
                "zh": {
                    "title": "æ— è®­ç»ƒçš„å¤šæç¤ºè§†é¢‘ç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šæç¤ºè§†é¢‘ç”Ÿæˆæ–¹æ³•DiTCtrlï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤šä¸ªé¡ºåºæç¤ºæ—¶çš„å›°éš¾ã€‚æˆ‘ä»¬åˆ©ç”¨MM-DiTæ¶æ„ï¼Œé€šè¿‡åˆ†æå…¶æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†åœ¨å¤šæç¤ºè§†é¢‘ç”Ÿæˆä¸­å¹³æ»‘è¿‡æ¸¡å’Œä¸€è‡´çš„ç‰©ä½“è¿åŠ¨ã€‚DiTCtrlä¸éœ€è¦é¢å¤–çš„è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªæç¤ºä¸‹ç”Ÿæˆè‡ªç„¶æµç•…çš„è§†é¢‘ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†MPVBenchåŸºå‡†ï¼Œä»¥è¯„ä¼°å¤šæç¤ºç”Ÿæˆçš„æ€§èƒ½ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨æ— é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17758",
            "title": "In Case You Missed It: ARC 'Challenge' Is Not That Challenging",
            "url": "https://huggingface.co/papers/2412.17758",
            "abstract": "ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily due to an evaluation setup that prevents direct comparison of answer choices rather than inherent complexity. Although some researchers have quietly shifted to a more appropriate scheme over the last year, the implications of this change have yet to be widely acknowledged. We highlight this overlooked shift, show how similar evaluation practices falsely imply reasoning deficits in other benchmarks, and demonstrate that fairer methods dramatically reduce performance gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing so, we reveal how evaluation shapes perceived difficulty and offer guidelines to ensure that multiple-choice evaluations accurately reflect actual model capabilities.",
            "score": 7,
            "issue_id": 1311,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 23",
                "zh": "12æœˆ23æ—¥"
            },
            "hash": "38b823b470857f90",
            "authors": [
                "Åukasz Borchmann"
            ],
            "affiliations": [
                "Snowflake AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17758.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#interpretability",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ AI-Ñ‚ĞµÑÑ‚Ğ¾Ğ²: Ñ€Ğ¾Ğ»ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ ĞºĞ°Ğ¶ÑƒÑ‰ĞµĞ¹ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµÑÑ‚Ğ° ARC Challenge Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ° Ğ½Ğµ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ°Ğ¼Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Reevaluating Evaluation: Uncovering True Model Capabilities",
                    "desc": "The paper discusses the challenges faced by modern language models (LLMs) when evaluating their performance on the ARC Challenge compared to ARC Easy. It argues that the evaluation setup, which does not allow for direct comparison of answer choices, is the main reason for the perceived difficulty, rather than the tasks themselves being inherently harder. The authors highlight a recent shift in evaluation practices that has not been widely recognized, showing that adopting fairer methods can significantly improve performance metrics. They emphasize the importance of accurate evaluation methods to truly reflect the capabilities of models and avoid misleading conclusions about their reasoning abilities."
                },
                "zh": {
                    "title": "è¯„ä¼°æ–¹æ³•å½±å“æ¨¡å‹è¡¨ç°çš„è®¤çŸ¥",
                    "desc": "ARCæŒ‘æˆ˜å¯¹ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ä¼¼ä¹æ¯”ARCç®€å•æ›´å›°éš¾ï¼Œä¸»è¦æ˜¯å› ä¸ºè¯„ä¼°è®¾ç½®é˜»æ­¢äº†å¯¹ç­”æ¡ˆé€‰æ‹©çš„ç›´æ¥æ¯”è¾ƒï¼Œè€Œä¸æ˜¯å›ºæœ‰çš„å¤æ‚æ€§ã€‚å°½ç®¡ä¸€äº›ç ”ç©¶äººå‘˜åœ¨è¿‡å»ä¸€å¹´ä¸­æ‚„ç„¶è½¬å‘æ›´åˆé€‚çš„è¯„ä¼°æ–¹æ¡ˆï¼Œä½†è¿™ä¸€å˜åŒ–çš„å½±å“å°šæœªè¢«å¹¿æ³›è®¤å¯ã€‚æˆ‘ä»¬å¼ºè°ƒäº†è¿™ä¸€è¢«å¿½è§†çš„è½¬å˜ï¼Œå±•ç¤ºäº†ç±»ä¼¼çš„è¯„ä¼°å®è·µå¦‚ä½•é”™è¯¯åœ°æš—ç¤ºå…¶ä»–åŸºå‡†ä¸­çš„æ¨ç†ç¼ºé™·ï¼Œå¹¶è¯æ˜æ›´å…¬å¹³çš„æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ€§èƒ½å·®è·ï¼ˆä¾‹å¦‚åœ¨SIQAä¸Šï¼‰ï¼Œç”šè‡³äº§ç”Ÿè¶…äººç±»çš„ç»“æœï¼ˆOpenBookQAï¼‰ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬æ­ç¤ºäº†è¯„ä¼°å¦‚ä½•å½±å“æ„ŸçŸ¥çš„éš¾åº¦ï¼Œå¹¶æä¾›äº†ç¡®ä¿å¤šé¡¹é€‰æ‹©è¯„ä¼°å‡†ç¡®åæ˜ æ¨¡å‹å®é™…èƒ½åŠ›çš„æŒ‡å¯¼æ–¹é’ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14711",
            "title": "ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing",
            "url": "https://huggingface.co/papers/2412.14711",
            "abstract": "Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead. We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE's continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE.",
            "score": 7,
            "issue_id": 1305,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "0b43c3f140601a96",
            "authors": [
                "Ziteng Wang",
                "Jianfei Chen",
                "Jun Zhu"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14711.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ReMoE: Ğ”Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture-of-Experts",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ReMoE Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture-of-Experts (MoE). ReMoE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ReLU Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ TopK+Softmax. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ»Ğ¾ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ReMoE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ MoE Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "ReMoE: Revolutionizing Mixture-of-Experts with Differentiable Routing",
                    "desc": "This paper introduces ReMoE, a new architecture for Mixture-of-Experts (MoE) models that improves upon traditional TopK routers by making them fully differentiable. By using ReLU as the routing mechanism, ReMoE allows for continuous optimization, which enhances performance and scalability. The authors also present techniques to manage the sparsity of the router and ensure an even distribution of workload among experts. Experimental results show that ReMoE outperforms conventional MoE models in various scenarios, demonstrating better scalability with an increasing number of experts."
                },
                "zh": {
                    "title": "ReMoEï¼šæå‡æ··åˆä¸“å®¶æ¨¡å‹çš„æ€§èƒ½ä¸å¯æ‰©å±•æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¨€ç–æ¿€æ´»æ··åˆä¸“å®¶æ¨¡å‹ReMoEï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚ä¸ä¼ ç»Ÿçš„TopKè·¯ç”±å™¨ä¸åŒï¼ŒReMoEé‡‡ç”¨äº†å®Œå…¨å¯å¾®åˆ†çš„æ¶æ„ï¼Œä½¿ç”¨ReLUä½œä¸ºè·¯ç”±å™¨ï¼Œä»è€Œå…‹æœäº†éè¿ç»­æ€§å¸¦æ¥çš„é™åˆ¶ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†è°ƒèŠ‚è·¯ç”±å™¨ç¨€ç–æ€§çš„æ–¹æ³•ï¼Œä»¥å¹³è¡¡ä¸“å®¶ä¹‹é—´çš„è´Ÿè½½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReMoEåœ¨ä¸åŒæ¨¡å‹è§„æ¨¡å’Œä¸“å®¶æ•°é‡ä¸‹ï¼Œå‡ä¼˜äºä¼ ç»Ÿçš„TopKè·¯ç”±æ··åˆä¸“å®¶æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15443",
            "title": "SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic Retrieval",
            "url": "https://huggingface.co/papers/2412.15443",
            "abstract": "Retrieval-Augmented Generation (RAG) systems have become pivotal in leveraging vast corpora to generate informed and contextually relevant responses, notably reducing hallucinations in Large Language Models. Despite significant advancements, these systems struggle to efficiently process and retrieve information from large datasets while maintaining a comprehensive understanding of the context. This paper introduces SKETCH, a novel methodology that enhances the RAG retrieval process by integrating semantic text retrieval with knowledge graphs, thereby merging structured and unstructured data for a more holistic comprehension. SKETCH, demonstrates substantial improvements in retrieval performance and maintains superior context integrity compared to traditional methods. Evaluated across four diverse datasets: QuALITY, QASPER, NarrativeQA, and Italian Cuisine-SKETCH consistently outperforms baseline approaches on key RAGAS metrics such as answer_relevancy, faithfulness, context_precision and context_recall. Notably, on the Italian Cuisine dataset, SKETCH achieved an answer relevancy of 0.94 and a context precision of 0.99, representing the highest performance across all evaluated metrics. These results highlight SKETCH's capability in delivering more accurate and contextually relevant responses, setting new benchmarks for future retrieval systems.",
            "score": 6,
            "issue_id": 1305,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "2d16e57527037cb7",
            "authors": [
                "Aakash Mahalingam",
                "Vinesh Kumar Gande",
                "Aman Chadha",
                "Vinija Jain",
                "Divya Chaudhary"
            ],
            "affiliations": [
                "Amazon AI",
                "Meta",
                "Northeastern University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15443.jpg",
            "data": {
                "categories": [
                    "#graphs",
                    "#dataset",
                    "#hallucinations",
                    "#benchmark",
                    "#rag",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "SKETCH: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SKETCH, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ (RAG). SKETCH Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. SKETCH Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ RAGAS Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "SKETCH: Elevating RAG with Semantic and Structured Data Integration",
                    "desc": "This paper presents SKETCH, a new method that improves Retrieval-Augmented Generation (RAG) systems by combining semantic text retrieval with knowledge graphs. This integration allows for better processing of large datasets while ensuring a deeper understanding of context. SKETCH shows significant enhancements in retrieval performance and context integrity compared to traditional RAG methods. The results from various datasets demonstrate that SKETCH achieves high scores in answer relevancy and context precision, establishing new standards for retrieval systems."
                },
                "zh": {
                    "title": "SKETCHï¼šæå‡æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSKETCHçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æå‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚SKETCHé€šè¿‡å°†è¯­ä¹‰æ–‡æœ¬æ£€ç´¢ä¸çŸ¥è¯†å›¾è°±ç›¸ç»“åˆï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å¤„ç†å’Œæ£€ç´¢å¤§æ•°æ®é›†ä¸­çš„ä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒå¯¹ä¸Šä¸‹æ–‡çš„å…¨é¢ç†è§£ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒSKETCHåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨æ„å¤§åˆ©ç¾é£Ÿæ•°æ®é›†ä¸Šï¼Œè¾¾åˆ°äº†0.94çš„ç­”æ¡ˆç›¸å…³æ€§å’Œ0.99çš„ä¸Šä¸‹æ–‡ç²¾åº¦ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒSKETCHèƒ½å¤Ÿæä¾›æ›´å‡†ç¡®å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„å“åº”ï¼Œä¸ºæœªæ¥çš„æ£€ç´¢ç³»ç»Ÿè®¾å®šäº†æ–°çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18608",
            "title": "PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models",
            "url": "https://huggingface.co/papers/2412.18608",
            "abstract": "Text- or image-to-3D generators and 3D scanners can now produce 3D assets with high-quality shapes and textures. These assets typically consist of a single, fused representation, like an implicit neural field, a Gaussian mixture, or a mesh, without any useful structure. However, most applications and creative workflows require assets to be made of several meaningful parts that can be manipulated independently. To address this gap, we introduce PartGen, a novel approach that generates 3D objects composed of meaningful parts starting from text, an image, or an unstructured 3D object. First, given multiple views of a 3D object, generated or rendered, a multi-view diffusion model extracts a set of plausible and view-consistent part segmentations, dividing the object into parts. Then, a second multi-view diffusion model takes each part separately, fills in the occlusions, and uses those completed views for 3D reconstruction by feeding them to a 3D reconstruction network. This completion process considers the context of the entire object to ensure that the parts integrate cohesively. The generative completion model can make up for the information missing due to occlusions; in extreme cases, it can hallucinate entirely invisible parts based on the input 3D asset. We evaluate our method on generated and real 3D assets and show that it outperforms segmentation and part-extraction baselines by a large margin. We also showcase downstream applications such as 3D part editing.",
            "score": 5,
            "issue_id": 1308,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 24",
                "zh": "12æœˆ24æ—¥"
            },
            "hash": "7f6d99dea7ea25bc",
            "authors": [
                "Minghao Chen",
                "Roman Shapovalov",
                "Iro Laina",
                "Tom Monnier",
                "Jianyuan Wang",
                "David Novotny",
                "Andrea Vedaldi"
            ],
            "affiliations": [
                "Meta AI",
                "Visual Geometry Group, University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18608.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#hallucinations",
                    "#diffusion"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "PartGen: Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "PartGen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ¸Ğ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹, Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ²Ğ¸Ğ´Ğ¾Ğ²ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ²Ñ‚Ğ¾Ñ€ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-Ñ„Ğ¾Ñ€Ğ¼Ñ‹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¸. PartGen Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ°Ğ¶Ğµ Ğ²Ğ¾ÑÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ñ‡Ğ°ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "PartGen: Transforming 3D Generation with Meaningful Parts",
                    "desc": "This paper presents PartGen, a new method for generating 3D objects that are composed of meaningful, manipulable parts from various inputs like text, images, or unstructured 3D objects. It utilizes a multi-view diffusion model to segment the 3D object into plausible parts based on multiple views, ensuring consistency across different perspectives. A second diffusion model then reconstructs each part by filling in occlusions and integrating them into a cohesive whole, even generating parts that are not visible in the input. The results demonstrate that PartGen significantly outperforms existing methods for segmentation and part extraction, enabling advanced applications like 3D part editing."
                },
                "zh": {
                    "title": "PartGenï¼šç”Ÿæˆå¯æ“ä½œçš„3Dç‰©ä½“éƒ¨åˆ†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPartGençš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä»æ–‡æœ¬ã€å›¾åƒæˆ–éç»“æ„åŒ–3Då¯¹è±¡ç”Ÿæˆç”±æœ‰æ„ä¹‰éƒ¨åˆ†ç»„æˆçš„3Dç‰©ä½“ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹æå–3Då¯¹è±¡çš„éƒ¨åˆ†åˆ†å‰²ï¼Œå°†å¯¹è±¡åˆ’åˆ†ä¸ºå¤šä¸ªå¯ç‹¬ç«‹æ“ä½œçš„éƒ¨åˆ†ã€‚æ¥ç€ï¼Œç¬¬äºŒä¸ªå¤šè§†è§’æ‰©æ•£æ¨¡å‹å¯¹æ¯ä¸ªéƒ¨åˆ†è¿›è¡Œå¡«å……å’Œ3Dé‡å»ºï¼Œç¡®ä¿å„éƒ¨åˆ†åœ¨æ•´ä½“ä¸Šä¸‹æ–‡ä¸­å’Œè°èåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPartGenåœ¨ç”Ÿæˆå’ŒçœŸå®3Dèµ„äº§ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„åˆ†å‰²å’Œéƒ¨åˆ†æå–æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17847",
            "title": "Bridging the Data Provenance Gap Across Text, Speech and Video",
            "url": "https://huggingface.co/papers/2412.17847",
            "abstract": "Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalities--popular text, speech, and video datasets--from their detailed sourcing trends and use restrictions to their geographical and linguistic representation. Our manual analysis covers nearly 4000 public datasets between 1990-2024, spanning 608 languages, 798 sources, 659 organizations, and 67 countries. We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widely-used text, speech, and video datasets, carry non-commercial restrictions. Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire multimodal audit, allowing practitioners to trace data provenance across text, speech, and video.",
            "score": 2,
            "issue_id": 1322,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 19",
                "zh": "12æœˆ19æ—¥"
            },
            "hash": "a67cb86f2c58f7e0",
            "authors": [
                "Shayne Longpre",
                "Nikhil Singh",
                "Manuel Cherep",
                "Kushagra Tiwary",
                "Joanna Materzynska",
                "William Brannon",
                "Robert Mahari",
                "Manan Dey",
                "Mohammed Hamdy",
                "Nayan Saxena",
                "Ahmad Mustafa Anis",
                "Emad A. Alghamdi",
                "Vu Minh Chien",
                "Naana Obeng-Marnu",
                "Da Yin",
                "Kun Qian",
                "Yizhi Li",
                "Minnie Liang",
                "An Dinh",
                "Shrestha Mohanty",
                "Deividas Mataciunas",
                "Tobin South",
                "Jianguo Zhang",
                "Ariel N. Lee",
                "Campbell S. Lund",
                "Christopher Klamm",
                "Damien Sileo",
                "Diganta Misra",
                "Enrico Shippole",
                "Kevin Klyman",
                "Lester JV Miranda",
                "Niklas Muennighoff",
                "Seonghyeon Ye",
                "Seungone Kim",
                "Vipul Gupta",
                "Vivek Sharma",
                "Xuhui Zhou",
                "Caiming Xiong",
                "Luis Villa",
                "Stella Biderman",
                "Alex Pentland",
                "Sara Hooker",
                "Jad Kabbara"
            ],
            "affiliations": [
                "The Data Provenance Initiative"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17847.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#data",
                    "#ethics",
                    "#dataset",
                    "#synthetic",
                    "#low_resource"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ°ÑƒĞ´Ğ¸Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²: Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ñ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…: Ñ‚ĞµĞºÑÑ‚, Ñ€ĞµÑ‡ÑŒ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 4000 Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ 1990 Ğ¿Ğ¾ 2024 Ğ³Ğ¾Ğ´, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 608 ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ 67 ÑÑ‚Ñ€Ğ°Ğ½. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ²ĞµĞ±-ĞºÑ€Ğ°ÑƒĞ»Ğ¸Ğ½Ğ³Ğ°, ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ğ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ YouTube, Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ Ğ² Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ½Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ÑÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ 2013 Ğ³Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Unveiling the Landscape of AI Training Datasets",
                    "desc": "This paper presents a comprehensive analysis of nearly 4000 public datasets used in machine learning, focusing on text, speech, and video modalities. The study reveals that since 2019, most datasets have been sourced from web-crawled content, synthetic data, and social media, particularly YouTube. It also highlights that while the number of languages and regions represented in datasets has increased, the actual geographical and multilingual coverage has not improved significantly since 2013. The authors emphasize the importance of dataset transparency and responsible AI practices, providing their findings to help practitioners understand data sourcing and restrictions better."
                },
                "zh": {
                    "title": "æ•°æ®é€æ˜åº¦ä¸è´Ÿè´£ä»»çš„AIè¿›æ­¥",
                    "desc": "è¿™ç¯‡è®ºæ–‡åˆ†æäº†äººå·¥æ™ºèƒ½è®­ç»ƒæ•°æ®é›†çš„å±æ€§ï¼Œç‰¹åˆ«æ˜¯æ–‡æœ¬ã€è¯­éŸ³å’Œè§†é¢‘æ•°æ®é›†ã€‚ç ”ç©¶å‘ç°ï¼Œè‡ª2019å¹´ä»¥æ¥ï¼Œå¤šæ¨¡æ€æœºå™¨å­¦ä¹ åº”ç”¨ä¸»è¦ä¾èµ–äºç½‘ç»œçˆ¬è™«ã€åˆæˆæ•°æ®å’Œç¤¾äº¤åª’ä½“å¹³å°ï¼ˆå¦‚YouTubeï¼‰ä½œä¸ºè®­ç»ƒé›†æ¥æºã€‚å°½ç®¡å…¬å…±AIè®­ç»ƒæ•°æ®é›†ä¸­è¯­è¨€å’Œåœ°ç†è¦†ç›–é¢æœ‰æ‰€å¢åŠ ï¼Œä½†ç›¸å¯¹çš„åœ°ç†å’Œå¤šè¯­è¨€ä»£è¡¨æ€§å¹¶æ²¡æœ‰æ˜¾è‘—æ”¹å–„ã€‚è®ºæ–‡çš„å®¡è®¡ç»“æœä¸ºæ•°æ®æ¥æºã€ä½¿ç”¨é™åˆ¶å’Œè¥¿æ–¹ä¸­å¿ƒä¸»ä¹‰æä¾›äº†å®è¯åˆ†æï¼Œå¼ºè°ƒäº†æ•°æ®é€æ˜åº¦å’Œè´Ÿè´£ä»»ä½¿ç”¨çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16153",
            "title": "MotiF: Making Text Count in Image Animation with Motion Focal Loss",
            "url": "https://huggingface.co/papers/2412.16153",
            "abstract": "Text-Image-to-Video (TI2V) generation aims to generate a video from an image following a text description, which is also referred to as text-guided image animation. Most existing methods struggle to generate videos that align well with the text prompts, particularly when motion is specified. To overcome this limitation, we introduce MotiF, a simple yet effective approach that directs the model's learning to the regions with more motion, thereby improving the text alignment and motion generation. We use optical flow to generate a motion heatmap and weight the loss according to the intensity of the motion. This modified objective leads to noticeable improvements and complements existing methods that utilize motion priors as model inputs. Additionally, due to the lack of a diverse benchmark for evaluating TI2V generation, we propose TI2V Bench, a dataset consists of 320 image-text pairs for robust evaluation. We present a human evaluation protocol that asks the annotators to select an overall preference between two videos followed by their justifications. Through a comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced models, achieving an average preference of 72%. The TI2V Bench is released in https://wang-sj16.github.io/motif/.",
            "score": 2,
            "issue_id": 1319,
            "pub_date": "2024-12-20",
            "pub_date_card": {
                "ru": "20 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 20",
                "zh": "12æœˆ20æ—¥"
            },
            "hash": "48c7cc6d9e1fa27b",
            "authors": [
                "Shijie Wang",
                "Samaneh Azadi",
                "Rohit Girdhar",
                "Saketh Rambhatla",
                "Chen Sun",
                "Xi Yin"
            ],
            "affiliations": [
                "Brown University",
                "GenAI, Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16153.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#open_source",
                    "#optimization",
                    "#games",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "MotiF: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ MotiF Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. Ğ­Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… TI2V Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 320 Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚."
                },
                "en": {
                    "title": "MotiF: Enhancing Video Generation with Motion Awareness",
                    "desc": "The paper presents MotiF, a novel approach for generating videos from images based on text descriptions, addressing the challenge of aligning generated videos with specified motions. By utilizing optical flow to create a motion heatmap, MotiF enhances the model's focus on areas with significant motion, leading to better text alignment and motion generation. The authors also introduce TI2V Bench, a new dataset with 320 image-text pairs designed for evaluating text-image-to-video generation methods. Comprehensive evaluations show that MotiF significantly outperforms existing models, achieving a 72% preference rate in human assessments."
                },
                "zh": {
                    "title": "MotiFï¼šæå‡æ–‡æœ¬å¼•å¯¼è§†é¢‘ç”Ÿæˆçš„è¿åŠ¨å¯¹é½",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–‡æœ¬å¼•å¯¼å›¾åƒåŠ¨ç”»æ–¹æ³•ï¼Œç§°ä¸ºMotiFï¼Œæ—¨åœ¨ä»å›¾åƒç”Ÿæˆç¬¦åˆæ–‡æœ¬æè¿°çš„è§†é¢‘ã€‚ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆä¸æ–‡æœ¬æç¤ºä¸€è‡´çš„è§†é¢‘æ—¶ï¼Œå°¤å…¶æ˜¯åœ¨è¿åŠ¨æŒ‡å®šæ–¹é¢å­˜åœ¨å›°éš¾ã€‚MotiFé€šè¿‡å…³æ³¨è¿åŠ¨åŒºåŸŸæ¥æ”¹å–„æ–‡æœ¬å¯¹é½å’Œè¿åŠ¨ç”Ÿæˆï¼Œä½¿ç”¨å…‰æµç”Ÿæˆè¿åŠ¨çƒ­å›¾å¹¶æ ¹æ®è¿åŠ¨å¼ºåº¦åŠ æƒæŸå¤±ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†TI2V Benchæ•°æ®é›†ï¼Œä»¥ä¾¿å¯¹TI2Vç”Ÿæˆè¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†MotiFåœ¨å¤šä¸ªæ¨¡å‹ä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15797",
            "title": "Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning",
            "url": "https://huggingface.co/papers/2412.15797",
            "abstract": "Despite recent advances in large language models, open-source models often struggle to consistently perform well on complex reasoning tasks. Existing ensemble methods, whether applied at the token or output levels, fail to address these challenges. In response, we present Language model Ensemble with Monte Carlo Tree Search (LE-MCTS), a novel framework for process-level ensembling of language models. LE-MCTS formulates step-by-step reasoning with an ensemble of language models as a Markov decision process. In this framework, states represent intermediate reasoning paths, while actions consist of generating the next reasoning step using one of the language models selected from a predefined pool. Guided by a process-based reward model, LE-MCTS performs a tree search over the reasoning steps generated by different language models, identifying the most accurate reasoning chain. Experimental results on five mathematical reasoning benchmarks demonstrate that our approach outperforms both single language model decoding algorithms and language model ensemble methods. Notably, LE-MCTS improves performance by 3.6% and 4.3% on the MATH and MQA datasets, respectively, highlighting its effectiveness in solving complex reasoning problems.",
            "score": 1,
            "issue_id": 1315,
            "pub_date": "2024-12-20",
            "pub_date_card": {
                "ru": "20 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 20",
                "zh": "12æœˆ20æ—¥"
            },
            "hash": "ed8cd715177d35d0",
            "authors": [
                "Sungjin Park",
                "Xiao Liu",
                "Yeyun Gong",
                "Edward Choi"
            ],
            "affiliations": [
                "KAIST AI",
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15797.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#training",
                    "#math",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LE-MCTS: ĞĞ½ÑĞ°Ğ¼Ğ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LE-MCTS Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. LE-MCTS Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ°Ğº Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LE-MCTS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Reasoning in Language Models with LE-MCTS",
                    "desc": "This paper introduces a new method called Language model Ensemble with Monte Carlo Tree Search (LE-MCTS) to improve the reasoning abilities of open-source language models. LE-MCTS treats the reasoning process as a Markov decision process, where different states represent various reasoning paths and actions involve selecting a language model to generate the next step. By using a reward model to guide the search for the best reasoning chain, LE-MCTS effectively combines the strengths of multiple models. The results show that this approach significantly enhances performance on complex reasoning tasks compared to traditional methods."
                },
                "zh": {
                    "title": "é€šè¿‡LE-MCTSæå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
                    "desc": "å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹å–å¾—äº†è¿›å±•ï¼Œå¼€æºæ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šä»ç„¶è¡¨ç°ä¸ä½³ã€‚ç°æœ‰çš„é›†æˆæ–¹æ³•æ— æ³•æœ‰æ•ˆè§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºè¯­è¨€æ¨¡å‹é›†æˆä¸è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆLE-MCTSï¼‰ï¼Œç”¨äºè¯­è¨€æ¨¡å‹çš„è¿‡ç¨‹çº§é›†æˆã€‚LE-MCTSå°†é€æ­¥æ¨ç†å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œé€šè¿‡æ ‘æœç´¢è¯†åˆ«æœ€å‡†ç¡®çš„æ¨ç†é“¾ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†æ¨ç†æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-25.html",
    "link_next": "2024-12-27.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "25.12",
        "en": "12/25",
        "zh": "12æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "27.12",
        "en": "12/27",
        "zh": "12æœˆ27æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 3,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 2,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æ·±åº¦æ•°æ®ä¸­ç¼ºå¤±å€¼çš„æŒ‘æˆ˜ã€‚DepthLab æ˜¯ä¸€ä¸ªåŸºäºå›¾åƒæ‰©æ•£å…ˆéªŒçš„æ·±åº¦ä¿®å¤æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†ç¼ºå¤±åŒºåŸŸã€‚å®ƒå…·æœ‰ä¸¤ä¸ªä¼˜ç‚¹ï¼šå¯¹è¿ç»­åŒºåŸŸå’Œå­¤ç«‹ç‚¹éƒ½èƒ½æä¾›å¯é çš„å¡«è¡¥ï¼Œå¹¶ä¿æŒå·²çŸ¥æ·±åº¦çš„å°ºåº¦ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åœ¨3Dåœºæ™¯ä¿®å¤ã€æ–‡æœ¬åˆ°3Dåœºæ™¯ç”Ÿæˆç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ€§èƒ½å’Œè§†è§‰è´¨é‡éƒ½è¶…è¿‡ç°æœ‰è§£å†³æ–¹æ¡ˆã€‚é¡¹ç›®é¡µé¢å’Œæºä»£ç å¯ä»¥åœ¨ https://johanan528.github.io/depthlab_web/ æ‰¾åˆ°ã€‚",
        "title": "DepthLab: From Partial to Complete",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æ·±åº¦æ•°æ®ä¸­ç¼ºå¤±å€¼çš„æŒ‘æˆ˜ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le shÄ“ndÃ¹ shÃ¹jÃ¹ zhÅng quÄ“shÄ«zhÃ­ de tiÇozhÃ n.\n\nDepthLab æ˜¯ä¸€ä¸ªåŸºäºå›¾åƒæ‰©æ•£å…ˆéªŒçš„æ·±åº¦ä¿®å¤æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†ç¼ºå¤±åŒºåŸŸã€‚\nDepthLab shÃ¬ yÄ«gÃ¨ jÄ«yÃº tÃºxiÃ ng kuÃ²sÃ n xiÄnyÃ¡n de shÄ“ndÃ¹ xiÅ«fÃ¹ mÃ³xÃ­ng, nÃ©nggÃ²u chÇ”lÇ quÄ“shÄ« qÅ«yÃ¹.\n\nå®ƒå…·æœ‰ä¸¤ä¸ªä¼˜ç‚¹ï¼šå¯¹è¿ç»­åŒºåŸŸå’Œå­¤ç«‹ç‚¹éƒ½èƒ½æä¾›å¯é çš„å¡«è¡¥ï¼Œå¹¶ä¿æŒå·²çŸ¥æ·±åº¦çš„å°ºåº¦ä¸€è‡´æ€§ã€‚\nTÄ jÃ¹yÇ’u liÇng gÃ¨ yÅudiÇn: duÃ¬ liÃ¡nxÃ¹ qÅ«yÃ¹ hÃ© gÅ«lÃ¬diÇn dÅu nÃ©ng tÃ­gÅng kÄ›kÃ o de tiÃ¡nbÇ”, bÃ¬ng bÇochÃ­ yÇzhÄ« shÄ“ndÃ¹ de chÇdÃ¹ yÄ«zhÃ¬xÃ¬ng.\n\nè¯¥æ–¹æ³•åœ¨3Dåœºæ™¯ä¿®å¤ã€æ–‡æœ¬åˆ°3Dåœºæ™¯ç”Ÿæˆç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ€§èƒ½å’Œè§†è§‰è´¨é‡éƒ½è¶…è¿‡ç°æœ‰è§£å†³æ–¹æ¡ˆã€‚\nGÇi fÄngfÇ zÃ i 3D chÇngjÇng xiÅ«fÃ¹, wÃ©nbÄ›n dÃ o 3D chÇngjÇng shÄ“ngchÃ©ng dÄ›ng rÃ¨nwÃ¹ zhÅng biÇoxiÃ n chÅ«sÃ¨, xÃ¬ngnÃ©ng hÃ© shÃ¬juÃ© zhÃ¬liÃ ng dÅu chÄoguÃ² xiÃ nyÇ’u jiÄ›juÃ© fÄng'Ã n.\n\né¡¹ç›®é¡µé¢å’Œæºä»£ç å¯ä»¥åœ¨ https://johanan528.github.io/depthlab_web/ æ‰¾åˆ°ã€‚\nXiÃ ngmÃ¹ yÃ¨miÃ n hÃ© yuÃ¡n dÃ imÇ kÄ›yÇ zÃ i https://johanan528.github.io/depthlab_web/ zhÇo dÃ o.",
        "vocab": "[{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'}, {'word': 'æ·±åº¦', 'pinyin': 'shÄ“n dÃ¹', 'trans': 'depth'}, {'word': 'ç¼ºå¤±å€¼', 'pinyin': 'quÄ“ shÄ« zhÃ­', 'trans': 'missing value'}, {'word': 'æŒ‘æˆ˜', 'pinyin': 'tiÇo zhÃ n', 'trans': 'challenge'}, {'word': 'åŸºäº', 'pinyin': 'jÄ« yÃº', 'trans': 'based on'}, {'word': 'å›¾åƒ', 'pinyin': 'tÃº xiÃ ng', 'trans': 'image'}, {'word': 'æ‰©æ•£', 'pinyin': 'kuÃ² sÃ n', 'trans': 'diffusion'}, {'word': 'å…ˆéªŒ', 'pinyin': 'xiÄn yÃ n', 'trans': 'prior'}, {'word': 'ä¿®å¤', 'pinyin': 'xiÅ« fÃ¹', 'trans': 'repair'}, {'word': 'æ¨¡å‹', 'pinyin': 'mÃ³ xÃ­ng', 'trans': 'model'}, {'word': 'å¤„ç†', 'pinyin': 'chÇ” lÇ', 'trans': 'handle'}, {'word': 'åŒºåŸŸ', 'pinyin': 'qÅ« yÃ¹', 'trans': 'region'}, {'word': 'è¿ç»­', 'pinyin': 'liÃ¡n xÃ¹', 'trans': 'continuous'}, {'word': 'å­¤ç«‹ç‚¹', 'pinyin': 'gÅ« lÃ¬ diÇn', 'trans': 'isolated point'}, {'word': 'å¯é ', 'pinyin': 'kÄ› kÃ o', 'trans': 'reliable'}, {'word': 'å¡«è¡¥', 'pinyin': 'tiÃ¡n bÇ”', 'trans': 'fill in'}, {'word': 'ä¿æŒ', 'pinyin': 'bÇo chÃ­', 'trans': 'maintain'}, {'word': 'å°ºåº¦', 'pinyin': 'chÇ dÃ¹', 'trans': 'scale'}, {'word': 'ä¸€è‡´æ€§', 'pinyin': 'yÄ« zhÃ¬ xÃ¬ng', 'trans': 'consistency'}, {'word': '3D', 'pinyin': '3D', 'trans': '3D'}, {'word': 'åœºæ™¯', 'pinyin': 'chÇng jÇng', 'trans': 'scene'}, {'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generate'}, {'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨n wÃ¹', 'trans': 'task'}, {'word': 'è¡¨ç°', 'pinyin': 'biÇo xiÃ n', 'trans': 'performance'}, {'word': 'å‡ºè‰²', 'pinyin': 'chÅ« sÃ¨', 'trans': 'outstanding'}, {'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ng nÃ©ng', 'trans': 'performance'}, {'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'}, {'word': 'è´¨é‡', 'pinyin': 'zhÃ¬ liÃ ng', 'trans': 'quality'}, {'word': 'è§£å†³æ–¹æ¡ˆ', 'pinyin': 'jiÄ› juÃ© fÄng Ã n', 'trans': 'solution'}, {'word': 'é¡¹ç›®', 'pinyin': 'xiÃ ng mÃ¹', 'trans': 'project'}, {'word': 'é¡µé¢', 'pinyin': 'yÃ¨ miÃ n', 'trans': 'page'}, {'word': 'æºä»£ç ', 'pinyin': 'yuÃ¡n dÃ i mÇ', 'trans': 'source code'}, {'word': 'æ‰¾åˆ°', 'pinyin': 'zhÇo dÃ o', 'trans': 'find'}]",
        "trans": "This article discusses the challenges of missing values in depth data. DepthLab is a depth inpainting model based on image diffusion priors that can handle missing regions. It has two advantages: it provides reliable filling for both continuous regions and isolated points, and maintains the scale consistency of known depths. This method performs exceptionally well in tasks such as 3D scene repair and text-to-3D scene generation, outperforming existing solutions in both performance and visual quality. The project page and source code can be found at https://johanan528.github.io/depthlab_web/.",
        "update_ts": "2024-12-25 09:10"
    }
}