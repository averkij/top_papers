{
    "date": {
        "ru": "26 декабря",
        "en": "December 26",
        "zh": "12月26日"
    },
    "time_utc": "2024-12-26 00:45",
    "weekday": 3,
    "issue_id": 1325,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.18450",
            "title": "3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding",
            "url": "https://huggingface.co/papers/2412.18450",
            "abstract": "A 3D scene graph represents a compact scene model, storing information about the objects and the semantic relationships between them, making its use promising for robotic tasks. When interacting with a user, an embodied intelligent agent should be capable of responding to various queries about the scene formulated in natural language. Large Language Models (LLMs) are beneficial solutions for user-robot interaction due to their natural language understanding and reasoning abilities. Recent methods for creating learnable representations of 3D scenes have demonstrated the potential to improve the quality of LLMs responses by adapting to the 3D world. However, the existing methods do not explicitly utilize information about the semantic relationships between objects, limiting themselves to information about their coordinates. In this work, we propose a method 3DGraphLLM for constructing a learnable representation of a 3D scene graph. The learnable representation is used as input for LLMs to perform 3D vision-language tasks. In our experiments on popular ScanRefer, RIORefer, Multi3DRefer, ScanQA, Sqa3D, and Scan2cap datasets, we demonstrate the advantage of this approach over baseline methods that do not use information about the semantic relationships between objects. The code is publicly available at https://github.com/CognitiveAISystems/3DGraphLLM.",
            "score": 21,
            "issue_id": 1311,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 декабря",
                "en": "December 24",
                "zh": "12月24日"
            },
            "hash": "3d80e95d793a8b5e",
            "authors": [
                "Tatiana Zemskova",
                "Dmitry Yudin"
            ],
            "affiliations": [
                "Artificial Intelligence Research Institute",
                "Moscow Institute of Physics and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18450.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#multimodal",
                    "#reasoning",
                    "#open_source",
                    "#agents",
                    "#games",
                    "#graphs"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "3D-графы сцен улучшают понимание пространства языковыми моделями",
                    "desc": "Статья представляет метод 3DGraphLLM для создания обучаемого представления трехмерного графа сцены. Это представление используется в качестве входных данных для больших языковых моделей (LLM) для выполнения задач 3D-зрения и языка. Авторы демонстрируют преимущество этого подхода над базовыми методами, которые не используют информацию о семантических отношениях между объектами. Эксперименты проводились на популярных наборах данных, таких как ScanRefer, RIORefer и другие."
                },
                "en": {
                    "title": "Enhancing Robot Understanding with 3D Scene Graphs and Language Models",
                    "desc": "This paper introduces 3DGraphLLM, a method for creating a learnable representation of 3D scene graphs that captures both object information and their semantic relationships. By integrating this representation with Large Language Models (LLMs), the approach enhances user-robot interactions, allowing robots to better understand and respond to natural language queries about 3D scenes. The authors demonstrate that their method outperforms existing techniques that only consider object coordinates, highlighting the importance of semantic relationships in improving LLM responses. Experiments conducted on various datasets show the effectiveness of 3DGraphLLM in performing 3D vision-language tasks."
                },
                "zh": {
                    "title": "提升机器人交互的3D场景理解",
                    "desc": "这篇论文提出了一种名为3DGraphLLM的方法，用于构建3D场景图的可学习表示。该表示能够有效地捕捉对象之间的语义关系，从而提升大型语言模型（LLMs）在3D视觉-语言任务中的表现。通过在多个数据集上的实验，研究表明，利用语义关系的信息可以显著改善模型的响应质量。该方法为机器人与用户的自然语言交互提供了更强大的支持。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18153",
            "title": "DepthLab: From Partial to Complete",
            "url": "https://huggingface.co/papers/2412.18153",
            "abstract": "Missing values remain a common challenge for depth data across its wide range of applications, stemming from various causes like incomplete data acquisition and perspective alteration. This work bridges this gap with DepthLab, a foundation depth inpainting model powered by image diffusion priors. Our model features two notable strengths: (1) it demonstrates resilience to depth-deficient regions, providing reliable completion for both continuous areas and isolated points, and (2) it faithfully preserves scale consistency with the conditioned known depth when filling in missing values. Drawing on these advantages, our approach proves its worth in various downstream tasks, including 3D scene inpainting, text-to-3D scene generation, sparse-view reconstruction with DUST3R, and LiDAR depth completion, exceeding current solutions in both numerical performance and visual quality. Our project page with source code is available at https://johanan528.github.io/depthlab_web/.",
            "score": 21,
            "issue_id": 1305,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 декабря",
                "en": "December 24",
                "zh": "12月24日"
            },
            "hash": "c319c831137b3ce6",
            "authors": [
                "Zhiheng Liu",
                "Ka Leong Cheng",
                "Qiuyu Wang",
                "Shuzhe Wang",
                "Hao Ouyang",
                "Bin Tan",
                "Kai Zhu",
                "Yujun Shen",
                "Qifeng Chen",
                "Ping Luo"
            ],
            "affiliations": [
                "Aalto University",
                "Ant Group",
                "HKU",
                "HKUST",
                "Tongyi Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18153.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#3d",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🕳️",
                "ru": {
                    "title": "DepthLab: Восполнение пробелов в данных глубины с помощью ИИ",
                    "desc": "DepthLab - это модель для восстановления глубины изображения, основанная на диффузионных приорах. Она способна надежно заполнять как большие области, так и отдельные точки с отсутствующими данными глубины. Модель сохраняет согласованность масштаба с известной глубиной при заполнении пропусков. DepthLab превосходит существующие решения в различных задачах, включая инпейнтинг 3D-сцен и дополнение данных LiDAR."
                },
                "en": {
                    "title": "DepthLab: Bridging the Gap in Depth Data Completion",
                    "desc": "This paper presents DepthLab, a novel model designed to address the issue of missing values in depth data, which often occurs due to incomplete data collection or changes in perspective. DepthLab utilizes image diffusion priors to effectively inpaint depth information, ensuring that both continuous and isolated missing regions are filled accurately. The model maintains scale consistency with known depth values, which is crucial for realistic depth completion. DepthLab outperforms existing methods in various applications, such as 3D scene inpainting and LiDAR depth completion, demonstrating superior numerical and visual results."
                },
                "zh": {
                    "title": "深度修复新突破：DepthLab模型",
                    "desc": "本论文提出了一种名为DepthLab的深度图像修复模型，旨在解决深度数据中的缺失值问题。该模型利用图像扩散先验，能够有效填补深度不足的区域，确保连续区域和孤立点的可靠修复。DepthLab在填补缺失值时，能够保持与已知深度的一致性，确保尺度的准确性。通过这些优势，该模型在3D场景修复、文本到3D场景生成、稀疏视图重建和LiDAR深度补全等任务中表现优异，超越了现有的解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17739",
            "title": "Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization",
            "url": "https://huggingface.co/papers/2412.17739",
            "abstract": "Extending the context length of Language Models (LMs) by improving Rotary Position Embedding (RoPE) has become a trend. While existing works mainly address RoPE's limitations within attention mechanism, this paper provides an analysis across nearly all parts of LMs, uncovering their adverse effects on length generalization for RoPE-based attention. Using Discrete Signal Processing theory, we show that RoPE enables periodic attention by implicitly achieving Non-Uniform Discrete Fourier Transform. However, this periodicity is undermined by the spectral damage caused by: 1) linear layers and activation functions outside of attention; 2) insufficiently trained frequency components brought by time-domain truncation. Building on our observations, we propose Fourier Position Embedding (FoPE), which enhances attention's frequency-domain properties to improve both its periodic extension and length generalization. FoPE constructs Fourier Series and zero-outs the destructive frequency components, increasing model robustness against the spectrum damage. Experiments across various model scales show that, within varying context windows, FoPE can maintain a more stable perplexity and a more consistent accuracy in a needle-in-haystack task compared to RoPE and ALiBi. Several analyses and ablations bring further support to our method and theoretical modeling.",
            "score": 17,
            "issue_id": 1306,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 декабря",
                "en": "December 23",
                "zh": "12月23日"
            },
            "hash": "1ce9c827a32ec3c5",
            "authors": [
                "Ermo Hua",
                "Che Jiang",
                "Xingtai Lv",
                "Kaiyan Zhang",
                "Ning Ding",
                "Youbang Sun",
                "Biqing Qi",
                "Yuchen Fan",
                "Xue Kai Zhu",
                "Bowen Zhou"
            ],
            "affiliations": [
                "Northeastern University",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17739.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#long_context",
                    "#architecture"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Улучшение обработки длинных последовательностей с помощью преобразования Фурье",
                    "desc": "Эта статья представляет новый метод позиционного кодирования для языковых моделей - Fourier Position Embedding (FoPE). FoPE улучшает частотные свойства механизма внимания, что позволяет более эффективно обрабатывать длинные последовательности. Авторы анализируют ограничения существующего метода Rotary Position Embedding (RoPE) с точки зрения теории обработки дискретных сигналов. Предложенный метод FoPE конструирует ряды Фурье и обнуляет деструктивные частотные компоненты, повышая устойчивость модели к искажениям спектра. Эксперименты показывают, что FoPE обеспечивает более стабильную перплексию и точность на различных длинах контекста по сравнению с RoPE и ALiBi."
                },
                "en": {
                    "title": "Enhancing Language Models with Fourier Position Embedding for Better Context Handling",
                    "desc": "This paper explores the limitations of Rotary Position Embedding (RoPE) in Language Models (LMs) and its impact on length generalization. It reveals that while RoPE allows for periodic attention through Non-Uniform Discrete Fourier Transform, this capability is compromised by linear layers and insufficient training of frequency components. The authors introduce Fourier Position Embedding (FoPE), which improves the frequency-domain characteristics of attention by eliminating harmful frequency components. Experimental results demonstrate that FoPE outperforms RoPE and ALiBi in maintaining stability in perplexity and accuracy across different context lengths."
                },
                "zh": {
                    "title": "提升语言模型的上下文长度与泛化能力",
                    "desc": "本论文探讨了通过改进旋转位置嵌入（RoPE）来扩展语言模型（LM）的上下文长度。我们分析了RoPE在注意力机制之外的各个部分的影响，发现其对长度泛化的负面效应。基于离散信号处理理论，我们提出了傅里叶位置嵌入（FoPE），它通过构建傅里叶级数来增强注意力的频域特性，从而提高模型的鲁棒性。实验结果表明，FoPE在不同上下文窗口下能够保持更稳定的困惑度和一致的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18597",
            "title": "DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation",
            "url": "https://huggingface.co/papers/2412.18597",
            "abstract": "Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer MM-DiT architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiT's attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance without additional training.",
            "score": 10,
            "issue_id": 1307,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 декабря",
                "en": "December 24",
                "zh": "12月24日"
            },
            "hash": "210ce3ba0e7e45d2",
            "authors": [
                "Minghong Cai",
                "Xiaodong Cun",
                "Xiaoyu Li",
                "Wenze Liu",
                "Zhaoyang Zhang",
                "Yong Zhang",
                "Ying Shan",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "GVC Lab, Great Bay University",
                "MMLab, The Chinese University of Hong Kong",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18597.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#games",
                    "#diffusion",
                    "#benchmark"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Плавная генерация видео по нескольким запросам без переобучения",
                    "desc": "Статья представляет новый метод DiTCtrl для генерации видео по нескольким последовательным текстовым запросам без дополнительного обучения. Авторы анализируют механизм внимания в архитектуре Multi-Modal Diffusion Transformer (MM-DiT) и используют его для точного семантического контроля при переходе между разными запросами. Метод позволяет создавать видео с плавными переходами и согласованным движением объектов. Также предлагается новый бенчмарк MPVBench для оценки качества генерации видео по нескольким запросам."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Multi-Prompt Control",
                    "desc": "This paper introduces DiTCtrl, a novel method for generating videos using multiple prompts without the need for additional training. It leverages the Multi-Modal Diffusion Transformer (MM-DiT) architecture to facilitate smooth transitions and coherent object motion across sequential prompts. By analyzing the attention mechanism of MM-DiT, the authors enable precise semantic control, allowing for effective multi-prompt video generation. The proposed method outperforms existing techniques and is evaluated using a new benchmark called MPVBench, specifically designed for this purpose."
                },
                "zh": {
                    "title": "无训练的多提示视频生成新方法",
                    "desc": "本论文提出了一种新的多提示视频生成方法DiTCtrl，旨在解决现有模型在处理多个顺序提示时的困难。我们利用MM-DiT架构，通过分析其注意力机制，实现了在多提示视频生成中平滑过渡和一致的物体运动。DiTCtrl不需要额外的训练，能够在多个提示下生成自然流畅的视频。我们还引入了MPVBench基准，以评估多提示生成的性能，实验结果表明该方法在无额外训练的情况下达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17758",
            "title": "In Case You Missed It: ARC 'Challenge' Is Not That Challenging",
            "url": "https://huggingface.co/papers/2412.17758",
            "abstract": "ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily due to an evaluation setup that prevents direct comparison of answer choices rather than inherent complexity. Although some researchers have quietly shifted to a more appropriate scheme over the last year, the implications of this change have yet to be widely acknowledged. We highlight this overlooked shift, show how similar evaluation practices falsely imply reasoning deficits in other benchmarks, and demonstrate that fairer methods dramatically reduce performance gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing so, we reveal how evaluation shapes perceived difficulty and offer guidelines to ensure that multiple-choice evaluations accurately reflect actual model capabilities.",
            "score": 7,
            "issue_id": 1311,
            "pub_date": "2024-12-23",
            "pub_date_card": {
                "ru": "23 декабря",
                "en": "December 23",
                "zh": "12月23日"
            },
            "hash": "38b823b470857f90",
            "authors": [
                "Łukasz Borchmann"
            ],
            "affiliations": [
                "Snowflake AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17758.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#interpretability",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Переосмысление сложности AI-тестов: роль методики оценки",
                    "desc": "Статья анализирует причины кажущейся сложности теста ARC Challenge для современных моделей машинного обучения. Авторы утверждают, что основная проблема заключается в методике оценки, а не в сложности самих задач. Они демонстрируют, как более справедливые методы оценки значительно сокращают разрыв в производительности между моделями и людьми на различных бенчмарках. Исследование подчеркивает важность правильного выбора методов оценки для точного отражения реальных возможностей языковых моделей."
                },
                "en": {
                    "title": "Reevaluating Evaluation: Uncovering True Model Capabilities",
                    "desc": "The paper discusses the challenges faced by modern language models (LLMs) when evaluating their performance on the ARC Challenge compared to ARC Easy. It argues that the evaluation setup, which does not allow for direct comparison of answer choices, is the main reason for the perceived difficulty, rather than the tasks themselves being inherently harder. The authors highlight a recent shift in evaluation practices that has not been widely recognized, showing that adopting fairer methods can significantly improve performance metrics. They emphasize the importance of accurate evaluation methods to truly reflect the capabilities of models and avoid misleading conclusions about their reasoning abilities."
                },
                "zh": {
                    "title": "评估方法影响模型表现的认知",
                    "desc": "ARC挑战对现代大语言模型（LLM）来说似乎比ARC简单更困难，主要是因为评估设置阻止了对答案选择的直接比较，而不是固有的复杂性。尽管一些研究人员在过去一年中悄然转向更合适的评估方案，但这一变化的影响尚未被广泛认可。我们强调了这一被忽视的转变，展示了类似的评估实践如何错误地暗示其他基准中的推理缺陷，并证明更公平的方法显著减少了性能差距（例如在SIQA上），甚至产生超人类的结果（OpenBookQA）。通过这样做，我们揭示了评估如何影响感知的难度，并提供了确保多项选择评估准确反映模型实际能力的指导方针。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.14711",
            "title": "ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing",
            "url": "https://huggingface.co/papers/2412.14711",
            "abstract": "Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead. We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE's continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE.",
            "score": 7,
            "issue_id": 1305,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "0b43c3f140601a96",
            "authors": [
                "Ziteng Wang",
                "Jianfei Chen",
                "Jun Zhu"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.14711.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ReMoE: Дифференцируемая архитектура для эффективных моделей Mixture-of-Experts",
                    "desc": "Статья представляет новую архитектуру ReMoE для моделей Mixture-of-Experts (MoE). ReMoE использует полностью дифференцируемый маршрутизатор на основе ReLU вместо традиционного TopK+Softmax. Это позволяет эффективно распределять вычисления между токенами и слоями, а также обеспечивает специализацию по доменам. Эксперименты показывают, что ReMoE превосходит обычные MoE по производительности и масштабируемости при различных размерах моделей и количестве экспертов."
                },
                "en": {
                    "title": "ReMoE: Revolutionizing Mixture-of-Experts with Differentiable Routing",
                    "desc": "This paper introduces ReMoE, a new architecture for Mixture-of-Experts (MoE) models that improves upon traditional TopK routers by making them fully differentiable. By using ReLU as the routing mechanism, ReMoE allows for continuous optimization, which enhances performance and scalability. The authors also present techniques to manage the sparsity of the router and ensure an even distribution of workload among experts. Experimental results show that ReMoE outperforms conventional MoE models in various scenarios, demonstrating better scalability with an increasing number of experts."
                },
                "zh": {
                    "title": "ReMoE：提升混合专家模型的性能与可扩展性",
                    "desc": "本文提出了一种新的稀疏激活混合专家模型ReMoE，旨在提高模型的性能和可扩展性。与传统的TopK路由器不同，ReMoE采用了完全可微分的架构，使用ReLU作为路由器，从而克服了非连续性带来的限制。我们还提出了调节路由器稀疏性的方法，以平衡专家之间的负载。实验结果表明，ReMoE在不同模型规模和专家数量下，均优于传统的TopK路由混合专家模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15443",
            "title": "SKETCH: Structured Knowledge Enhanced Text Comprehension for Holistic Retrieval",
            "url": "https://huggingface.co/papers/2412.15443",
            "abstract": "Retrieval-Augmented Generation (RAG) systems have become pivotal in leveraging vast corpora to generate informed and contextually relevant responses, notably reducing hallucinations in Large Language Models. Despite significant advancements, these systems struggle to efficiently process and retrieve information from large datasets while maintaining a comprehensive understanding of the context. This paper introduces SKETCH, a novel methodology that enhances the RAG retrieval process by integrating semantic text retrieval with knowledge graphs, thereby merging structured and unstructured data for a more holistic comprehension. SKETCH, demonstrates substantial improvements in retrieval performance and maintains superior context integrity compared to traditional methods. Evaluated across four diverse datasets: QuALITY, QASPER, NarrativeQA, and Italian Cuisine-SKETCH consistently outperforms baseline approaches on key RAGAS metrics such as answer_relevancy, faithfulness, context_precision and context_recall. Notably, on the Italian Cuisine dataset, SKETCH achieved an answer relevancy of 0.94 and a context precision of 0.99, representing the highest performance across all evaluated metrics. These results highlight SKETCH's capability in delivering more accurate and contextually relevant responses, setting new benchmarks for future retrieval systems.",
            "score": 6,
            "issue_id": 1305,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "2d16e57527037cb7",
            "authors": [
                "Aakash Mahalingam",
                "Vinesh Kumar Gande",
                "Aman Chadha",
                "Vinija Jain",
                "Divya Chaudhary"
            ],
            "affiliations": [
                "Amazon AI",
                "Meta",
                "Northeastern University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15443.jpg",
            "data": {
                "categories": [
                    "#graphs",
                    "#dataset",
                    "#hallucinations",
                    "#benchmark",
                    "#rag",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "SKETCH: Революция в извлечении информации для генеративных моделей",
                    "desc": "Данная статья представляет новый метод SKETCH, улучшающий процесс извлечения информации в системах генерации с аугментацией извлечения (RAG). SKETCH объединяет семантический поиск текста с графами знаний, интегрируя структурированные и неструктурированные данные для более целостного понимания контекста. Метод показывает значительные улучшения в производительности извлечения и сохранении целостности контекста по сравнению с традиционными подходами. SKETCH превосходит базовые методы по ключевым метрикам RAGAS на четырех различных наборах данных, демонстрируя высокую точность и контекстуальную релевантность ответов."
                },
                "en": {
                    "title": "SKETCH: Elevating RAG with Semantic and Structured Data Integration",
                    "desc": "This paper presents SKETCH, a new method that improves Retrieval-Augmented Generation (RAG) systems by combining semantic text retrieval with knowledge graphs. This integration allows for better processing of large datasets while ensuring a deeper understanding of context. SKETCH shows significant enhancements in retrieval performance and context integrity compared to traditional RAG methods. The results from various datasets demonstrate that SKETCH achieves high scores in answer relevancy and context precision, establishing new standards for retrieval systems."
                },
                "zh": {
                    "title": "SKETCH：提升检索增强生成系统的新方法",
                    "desc": "本论文介绍了一种名为SKETCH的新方法，旨在提升检索增强生成（RAG）系统的性能。SKETCH通过将语义文本检索与知识图谱相结合，能够更有效地处理和检索大数据集中的信息，同时保持对上下文的全面理解。研究表明，SKETCH在多个数据集上表现优异，尤其是在意大利美食数据集上，达到了0.94的答案相关性和0.99的上下文精度。这些结果表明，SKETCH能够提供更准确和上下文相关的响应，为未来的检索系统设定了新的基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.18608",
            "title": "PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models",
            "url": "https://huggingface.co/papers/2412.18608",
            "abstract": "Text- or image-to-3D generators and 3D scanners can now produce 3D assets with high-quality shapes and textures. These assets typically consist of a single, fused representation, like an implicit neural field, a Gaussian mixture, or a mesh, without any useful structure. However, most applications and creative workflows require assets to be made of several meaningful parts that can be manipulated independently. To address this gap, we introduce PartGen, a novel approach that generates 3D objects composed of meaningful parts starting from text, an image, or an unstructured 3D object. First, given multiple views of a 3D object, generated or rendered, a multi-view diffusion model extracts a set of plausible and view-consistent part segmentations, dividing the object into parts. Then, a second multi-view diffusion model takes each part separately, fills in the occlusions, and uses those completed views for 3D reconstruction by feeding them to a 3D reconstruction network. This completion process considers the context of the entire object to ensure that the parts integrate cohesively. The generative completion model can make up for the information missing due to occlusions; in extreme cases, it can hallucinate entirely invisible parts based on the input 3D asset. We evaluate our method on generated and real 3D assets and show that it outperforms segmentation and part-extraction baselines by a large margin. We also showcase downstream applications such as 3D part editing.",
            "score": 5,
            "issue_id": 1308,
            "pub_date": "2024-12-24",
            "pub_date_card": {
                "ru": "24 декабря",
                "en": "December 24",
                "zh": "12月24日"
            },
            "hash": "7f6d99dea7ea25bc",
            "authors": [
                "Minghao Chen",
                "Roman Shapovalov",
                "Iro Laina",
                "Tom Monnier",
                "Jianyuan Wang",
                "David Novotny",
                "Andrea Vedaldi"
            ],
            "affiliations": [
                "Meta AI",
                "Visual Geometry Group, University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.18608.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#hallucinations",
                    "#diffusion"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "PartGen: Генерация структурированных 3D-объектов из неструктурированных данных",
                    "desc": "PartGen - это новый подход к генерации 3D-объектов, состоящих из значимых частей, на основе текста, изображения или неструктурированного 3D-объекта. Метод использует мультивидовую диффузионную модель для сегментации объекта на части, а затем применяет вторую модель для заполнения окклюзий и реконструкции 3D-формы каждой части. PartGen может даже воссоздавать полностью невидимые части объекта на основе контекста. Авторы демонстрируют превосходство метода над базовыми подходами к сегментации и извлечению частей, а также показывают его применимость для редактирования 3D-частей."
                },
                "en": {
                    "title": "PartGen: Transforming 3D Generation with Meaningful Parts",
                    "desc": "This paper presents PartGen, a new method for generating 3D objects that are composed of meaningful, manipulable parts from various inputs like text, images, or unstructured 3D objects. It utilizes a multi-view diffusion model to segment the 3D object into plausible parts based on multiple views, ensuring consistency across different perspectives. A second diffusion model then reconstructs each part by filling in occlusions and integrating them into a cohesive whole, even generating parts that are not visible in the input. The results demonstrate that PartGen significantly outperforms existing methods for segmentation and part extraction, enabling advanced applications like 3D part editing."
                },
                "zh": {
                    "title": "PartGen：生成可操作的3D物体部分",
                    "desc": "本文介绍了一种名为PartGen的新方法，旨在从文本、图像或非结构化3D对象生成由有意义部分组成的3D物体。该方法首先利用多视角扩散模型提取3D对象的部分分割，将对象划分为多个可独立操作的部分。接着，第二个多视角扩散模型对每个部分进行填充和3D重建，确保各部分在整体上下文中和谐融合。实验结果表明，PartGen在生成和真实3D资产上均显著优于现有的分割和部分提取方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.17847",
            "title": "Bridging the Data Provenance Gap Across Text, Speech and Video",
            "url": "https://huggingface.co/papers/2412.17847",
            "abstract": "Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalities--popular text, speech, and video datasets--from their detailed sourcing trends and use restrictions to their geographical and linguistic representation. Our manual analysis covers nearly 4000 public datasets between 1990-2024, spanning 608 languages, 798 sources, 659 organizations, and 67 countries. We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widely-used text, speech, and video datasets, carry non-commercial restrictions. Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire multimodal audit, allowing practitioners to trace data provenance across text, speech, and video.",
            "score": 2,
            "issue_id": 1322,
            "pub_date": "2024-12-19",
            "pub_date_card": {
                "ru": "19 декабря",
                "en": "December 19",
                "zh": "12月19日"
            },
            "hash": "a67cb86f2c58f7e0",
            "authors": [
                "Shayne Longpre",
                "Nikhil Singh",
                "Manuel Cherep",
                "Kushagra Tiwary",
                "Joanna Materzynska",
                "William Brannon",
                "Robert Mahari",
                "Manan Dey",
                "Mohammed Hamdy",
                "Nayan Saxena",
                "Ahmad Mustafa Anis",
                "Emad A. Alghamdi",
                "Vu Minh Chien",
                "Naana Obeng-Marnu",
                "Da Yin",
                "Kun Qian",
                "Yizhi Li",
                "Minnie Liang",
                "An Dinh",
                "Shrestha Mohanty",
                "Deividas Mataciunas",
                "Tobin South",
                "Jianguo Zhang",
                "Ariel N. Lee",
                "Campbell S. Lund",
                "Christopher Klamm",
                "Damien Sileo",
                "Diganta Misra",
                "Enrico Shippole",
                "Kevin Klyman",
                "Lester JV Miranda",
                "Niklas Muennighoff",
                "Seonghyeon Ye",
                "Seungone Kim",
                "Vipul Gupta",
                "Vivek Sharma",
                "Xuhui Zhou",
                "Caiming Xiong",
                "Luis Villa",
                "Stella Biderman",
                "Alex Pentland",
                "Sara Hooker",
                "Jad Kabbara"
            ],
            "affiliations": [
                "The Data Provenance Initiative"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.17847.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#data",
                    "#ethics",
                    "#dataset",
                    "#synthetic",
                    "#low_resource"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Большой аудит датасетов: тренды и проблемы в обучении мультимодальных ИИ-моделей",
                    "desc": "Статья представляет собой масштабный аудит наборов данных для машинного обучения в различных модальностях: текст, речь и видео. Авторы проанализировали около 4000 публичных датасетов с 1990 по 2024 год, охватывающих 608 языков и 67 стран. Исследование выявило тенденцию к использованию данных из веб-краулинга, синтетических данных и социальных медиа, особенно YouTube, для обучения мультимодальных моделей. Также отмечается, что несмотря на рост представленности различных языков и географий в датасетах, относительное разнообразие не улучшилось значительно с 2013 года."
                },
                "en": {
                    "title": "Unveiling the Landscape of AI Training Datasets",
                    "desc": "This paper presents a comprehensive analysis of nearly 4000 public datasets used in machine learning, focusing on text, speech, and video modalities. The study reveals that since 2019, most datasets have been sourced from web-crawled content, synthetic data, and social media, particularly YouTube. It also highlights that while the number of languages and regions represented in datasets has increased, the actual geographical and multilingual coverage has not improved significantly since 2013. The authors emphasize the importance of dataset transparency and responsible AI practices, providing their findings to help practitioners understand data sourcing and restrictions better."
                },
                "zh": {
                    "title": "数据透明度与负责任的AI进步",
                    "desc": "这篇论文分析了人工智能训练数据集的属性，特别是文本、语音和视频数据集。研究发现，自2019年以来，多模态机器学习应用主要依赖于网络爬虫、合成数据和社交媒体平台（如YouTube）作为训练集来源。尽管公共AI训练数据集中语言和地理覆盖面有所增加，但相对的地理和多语言代表性并没有显著改善。论文的审计结果为数据来源、使用限制和西方中心主义提供了实证分析，强调了数据透明度和负责任使用的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.16153",
            "title": "MotiF: Making Text Count in Image Animation with Motion Focal Loss",
            "url": "https://huggingface.co/papers/2412.16153",
            "abstract": "Text-Image-to-Video (TI2V) generation aims to generate a video from an image following a text description, which is also referred to as text-guided image animation. Most existing methods struggle to generate videos that align well with the text prompts, particularly when motion is specified. To overcome this limitation, we introduce MotiF, a simple yet effective approach that directs the model's learning to the regions with more motion, thereby improving the text alignment and motion generation. We use optical flow to generate a motion heatmap and weight the loss according to the intensity of the motion. This modified objective leads to noticeable improvements and complements existing methods that utilize motion priors as model inputs. Additionally, due to the lack of a diverse benchmark for evaluating TI2V generation, we propose TI2V Bench, a dataset consists of 320 image-text pairs for robust evaluation. We present a human evaluation protocol that asks the annotators to select an overall preference between two videos followed by their justifications. Through a comprehensive evaluation on TI2V Bench, MotiF outperforms nine open-sourced models, achieving an average preference of 72%. The TI2V Bench is released in https://wang-sj16.github.io/motif/.",
            "score": 2,
            "issue_id": 1319,
            "pub_date": "2024-12-20",
            "pub_date_card": {
                "ru": "20 декабря",
                "en": "December 20",
                "zh": "12月20日"
            },
            "hash": "48c7cc6d9e1fa27b",
            "authors": [
                "Shijie Wang",
                "Samaneh Azadi",
                "Rohit Girdhar",
                "Saketh Rambhatla",
                "Chen Sun",
                "Xi Yin"
            ],
            "affiliations": [
                "Brown University",
                "GenAI, Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.16153.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#dataset",
                    "#open_source",
                    "#optimization",
                    "#games",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "MotiF: улучшение генерации видео с помощью анализа движения",
                    "desc": "Статья представляет новый подход MotiF для генерации видео из изображения по текстовому описанию. Метод использует оптический поток для создания карты интенсивности движения и взвешивания функции потерь. Это улучшает соответствие генерируемого видео текстовому запросу и качество анимации. Авторы также предлагают новый набор данных TI2V Bench для оценки таких моделей, содержащий 320 пар изображение-текст."
                },
                "en": {
                    "title": "MotiF: Enhancing Video Generation with Motion Awareness",
                    "desc": "The paper presents MotiF, a novel approach for generating videos from images based on text descriptions, addressing the challenge of aligning generated videos with specified motions. By utilizing optical flow to create a motion heatmap, MotiF enhances the model's focus on areas with significant motion, leading to better text alignment and motion generation. The authors also introduce TI2V Bench, a new dataset with 320 image-text pairs designed for evaluating text-image-to-video generation methods. Comprehensive evaluations show that MotiF significantly outperforms existing models, achieving a 72% preference rate in human assessments."
                },
                "zh": {
                    "title": "MotiF：提升文本引导视频生成的运动对齐",
                    "desc": "本文介绍了一种新的文本引导图像动画方法，称为MotiF，旨在从图像生成符合文本描述的视频。现有方法在生成与文本提示一致的视频时，尤其是在运动指定方面存在困难。MotiF通过关注运动区域来改善文本对齐和运动生成，使用光流生成运动热图并根据运动强度加权损失。我们还提出了TI2V Bench数据集，以便对TI2V生成进行更全面的评估，并展示了MotiF在多个模型中的优越性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.15797",
            "title": "Ensembling Large Language Models with Process Reward-Guided Tree Search for Better Complex Reasoning",
            "url": "https://huggingface.co/papers/2412.15797",
            "abstract": "Despite recent advances in large language models, open-source models often struggle to consistently perform well on complex reasoning tasks. Existing ensemble methods, whether applied at the token or output levels, fail to address these challenges. In response, we present Language model Ensemble with Monte Carlo Tree Search (LE-MCTS), a novel framework for process-level ensembling of language models. LE-MCTS formulates step-by-step reasoning with an ensemble of language models as a Markov decision process. In this framework, states represent intermediate reasoning paths, while actions consist of generating the next reasoning step using one of the language models selected from a predefined pool. Guided by a process-based reward model, LE-MCTS performs a tree search over the reasoning steps generated by different language models, identifying the most accurate reasoning chain. Experimental results on five mathematical reasoning benchmarks demonstrate that our approach outperforms both single language model decoding algorithms and language model ensemble methods. Notably, LE-MCTS improves performance by 3.6% and 4.3% on the MATH and MQA datasets, respectively, highlighting its effectiveness in solving complex reasoning problems.",
            "score": 1,
            "issue_id": 1315,
            "pub_date": "2024-12-20",
            "pub_date_card": {
                "ru": "20 декабря",
                "en": "December 20",
                "zh": "12月20日"
            },
            "hash": "ed8cd715177d35d0",
            "authors": [
                "Sungjin Park",
                "Xiao Liu",
                "Yeyun Gong",
                "Edward Choi"
            ],
            "affiliations": [
                "KAIST AI",
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.15797.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#training",
                    "#math",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LE-MCTS: Ансамблирование языковых моделей на уровне процесса для улучшения сложных рассуждений",
                    "desc": "Статья представляет новый метод под названием LE-MCTS для улучшения производительности языковых моделей в задачах сложного рассуждения. LE-MCTS формулирует пошаговое рассуждение с ансамблем языковых моделей как марковский процесс принятия решений. Метод использует поиск по дереву Монте-Карло для выбора наиболее точной цепочки рассуждений. Эксперименты на пяти эталонных наборах данных по математическим рассуждениям показали, что LE-MCTS превосходит как отдельные языковые модели, так и другие методы ансамблирования."
                },
                "en": {
                    "title": "Enhancing Reasoning in Language Models with LE-MCTS",
                    "desc": "This paper introduces a new method called Language model Ensemble with Monte Carlo Tree Search (LE-MCTS) to improve the reasoning abilities of open-source language models. LE-MCTS treats the reasoning process as a Markov decision process, where different states represent various reasoning paths and actions involve selecting a language model to generate the next step. By using a reward model to guide the search for the best reasoning chain, LE-MCTS effectively combines the strengths of multiple models. The results show that this approach significantly enhances performance on complex reasoning tasks compared to traditional methods."
                },
                "zh": {
                    "title": "通过LE-MCTS提升语言模型推理能力",
                    "desc": "尽管大型语言模型取得了进展，开源模型在复杂推理任务上仍然表现不佳。现有的集成方法无法有效解决这些挑战。我们提出了一种新的框架，称为语言模型集成与蒙特卡洛树搜索（LE-MCTS），用于语言模型的过程级集成。LE-MCTS将逐步推理建模为马尔可夫决策过程，通过树搜索识别最准确的推理链，从而显著提高了推理性能。"
                }
            }
        }
    ],
    "link_prev": "2024-12-25.html",
    "link_next": "2024-12-27.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "25.12",
        "en": "12/25",
        "zh": "12月25日"
    },
    "short_date_next": {
        "ru": "27.12",
        "en": "12/27",
        "zh": "12月27日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 1,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 3,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 2,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章讨论了深度数据中缺失值的挑战。DepthLab 是一个基于图像扩散先验的深度修复模型，能够处理缺失区域。它具有两个优点：对连续区域和孤立点都能提供可靠的填补，并保持已知深度的尺度一致性。该方法在3D场景修复、文本到3D场景生成等任务中表现出色，性能和视觉质量都超过现有解决方案。项目页面和源代码可以在 https://johanan528.github.io/depthlab_web/ 找到。",
        "title": "DepthLab: From Partial to Complete",
        "pinyin": "这篇文章讨论了深度数据中缺失值的挑战。\nZhè piān wénzhāng tǎolùn le shēndù shùjù zhōng quēshīzhí de tiǎozhàn.\n\nDepthLab 是一个基于图像扩散先验的深度修复模型，能够处理缺失区域。\nDepthLab shì yīgè jīyú túxiàng kuòsàn xiānyán de shēndù xiūfù móxíng, nénggòu chǔlǐ quēshī qūyù.\n\n它具有两个优点：对连续区域和孤立点都能提供可靠的填补，并保持已知深度的尺度一致性。\nTā jùyǒu liǎng gè yōudiǎn: duì liánxù qūyù hé gūlìdiǎn dōu néng tígōng kěkào de tiánbǔ, bìng bǎochí yǐzhī shēndù de chǐdù yīzhìxìng.\n\n该方法在3D场景修复、文本到3D场景生成等任务中表现出色，性能和视觉质量都超过现有解决方案。\nGǎi fāngfǎ zài 3D chǎngjǐng xiūfù, wénběn dào 3D chǎngjǐng shēngchéng děng rènwù zhōng biǎoxiàn chūsè, xìngnéng hé shìjué zhìliàng dōu chāoguò xiànyǒu jiějué fāng'àn.\n\n项目页面和源代码可以在 https://johanan528.github.io/depthlab_web/ 找到。\nXiàngmù yèmiàn hé yuán dàimǎ kěyǐ zài https://johanan528.github.io/depthlab_web/ zhǎo dào.",
        "vocab": "[{'word': '讨论', 'pinyin': 'tǎo lùn', 'trans': 'discuss'}, {'word': '深度', 'pinyin': 'shēn dù', 'trans': 'depth'}, {'word': '缺失值', 'pinyin': 'quē shī zhí', 'trans': 'missing value'}, {'word': '挑战', 'pinyin': 'tiǎo zhàn', 'trans': 'challenge'}, {'word': '基于', 'pinyin': 'jī yú', 'trans': 'based on'}, {'word': '图像', 'pinyin': 'tú xiàng', 'trans': 'image'}, {'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'}, {'word': '先验', 'pinyin': 'xiān yàn', 'trans': 'prior'}, {'word': '修复', 'pinyin': 'xiū fù', 'trans': 'repair'}, {'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'}, {'word': '处理', 'pinyin': 'chǔ lǐ', 'trans': 'handle'}, {'word': '区域', 'pinyin': 'qū yù', 'trans': 'region'}, {'word': '连续', 'pinyin': 'lián xù', 'trans': 'continuous'}, {'word': '孤立点', 'pinyin': 'gū lì diǎn', 'trans': 'isolated point'}, {'word': '可靠', 'pinyin': 'kě kào', 'trans': 'reliable'}, {'word': '填补', 'pinyin': 'tián bǔ', 'trans': 'fill in'}, {'word': '保持', 'pinyin': 'bǎo chí', 'trans': 'maintain'}, {'word': '尺度', 'pinyin': 'chǐ dù', 'trans': 'scale'}, {'word': '一致性', 'pinyin': 'yī zhì xìng', 'trans': 'consistency'}, {'word': '3D', 'pinyin': '3D', 'trans': '3D'}, {'word': '场景', 'pinyin': 'chǎng jǐng', 'trans': 'scene'}, {'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generate'}, {'word': '任务', 'pinyin': 'rèn wù', 'trans': 'task'}, {'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'}, {'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'}, {'word': '性能', 'pinyin': 'xìng néng', 'trans': 'performance'}, {'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'}, {'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'}, {'word': '解决方案', 'pinyin': 'jiě jué fāng àn', 'trans': 'solution'}, {'word': '项目', 'pinyin': 'xiàng mù', 'trans': 'project'}, {'word': '页面', 'pinyin': 'yè miàn', 'trans': 'page'}, {'word': '源代码', 'pinyin': 'yuán dài mǎ', 'trans': 'source code'}, {'word': '找到', 'pinyin': 'zhǎo dào', 'trans': 'find'}]",
        "trans": "This article discusses the challenges of missing values in depth data. DepthLab is a depth inpainting model based on image diffusion priors that can handle missing regions. It has two advantages: it provides reliable filling for both continuous regions and isolated points, and maintains the scale consistency of known depths. This method performs exceptionally well in tasks such as 3D scene repair and text-to-3D scene generation, outperforming existing solutions in both performance and visual quality. The project page and source code can be found at https://johanan528.github.io/depthlab_web/.",
        "update_ts": "2024-12-25 09:10"
    }
}