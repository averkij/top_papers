
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 26 papers. October 27.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">27 октября</span> | <span id="title-articles-count">26 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-24.html">⬅️ <span id="prev-date">24.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-28.html">➡️ <span id="next-date">28.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'};
        let feedDateNext = {'ru': '28.10', 'en': '10/28', 'zh': '10月28日'};
        let feedDatePrev = {'ru': '24.10', 'en': '10/24', 'zh': '10月24日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.21618', 'title': 'DeepAgent: A General Reasoning Agent with Scalable Toolsets', 'url': 'https://huggingface.co/papers/2510.21618', 'abstract': 'DeepAgent, an end-to-end deep reasoning agent, autonomously performs thinking, tool discovery, and action execution using memory folding and reinforcement learning, outperforming baselines in various tool-use and application tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. This work takes a step toward more general and capable agents for real-world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.', 'score': 59, 'issue_id': 6621, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '58d13c7e0c138478', 'authors': ['Xiaoxi Li', 'Wenxiang Jiao', 'Jiarui Jin', 'Guanting Dong', 'Jiajie Jin', 'Yinuo Wang', 'Hao Wang', 'Yutao Zhu', 'Ji-Rong Wen', 'Yuan Lu', 'Zhicheng Dou'], 'affiliations': ['Renmin University of China', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2510.21618.jpg', 'data': {'categories': ['#optimization', '#rl', '#agents', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'DeepAgent: автономный AI-агент с глубоким reasoning и управлением памятью', 'desc': 'В статье представлен DeepAgent — end-to-end агент для решения сложных задач, который автономно выполняет рассуждения, поиск инструментов и действия в едином процессе. Для работы с длинными последовательностями взаимодействий используется механизм автономного сжатия памяти (memory folding), который структурирует историю в эпизодическую, рабочую и инструментальную память. Для обучения применяется reinforcement learning стратегия ToolPO, которая использует симулированные LLM API и детальное распределение credit assignment для токенов вызова инструментов. Эксперименты на восьми бенчмарках показывают превосходство DeepAgent над базовыми методами как в задачах с известными инструментами, так и в open-set сценариях.'}, 'en': {'title': 'DeepAgent: Autonomous Reasoning and Tool Discovery for Real-World Tasks', 'desc': 'DeepAgent is a novel deep reasoning agent designed to autonomously think, discover tools, and execute actions in a unified process. It addresses the challenges of long-horizon interactions by implementing a memory folding mechanism that compresses past interactions, thus minimizing errors while retaining essential information. The agent is trained using an end-to-end reinforcement learning approach called ToolPO, which effectively assigns credit to tool usage based on simulated API interactions. Extensive testing shows that DeepAgent outperforms existing frameworks in various tool-use and application tasks, paving the way for more capable AI agents in real-world scenarios.'}, 'zh': {'title': 'DeepAgent：自主思考与工具发现的智能代理', 'desc': 'DeepAgent是一种端到端的深度推理代理，能够自主进行思考、工具发现和行动执行。它通过记忆折叠和强化学习的方法，解决了长时间交互中的上下文长度爆炸问题。DeepAgent在多个工具使用和应用任务中表现优于基线模型，展示了其强大的问题解决能力。该研究为更通用和强大的现实世界代理迈出了重要一步。'}}}, {'id': 'https://huggingface.co/papers/2510.20888', 'title': 'Video-As-Prompt: Unified Semantic Control for Video Generation', 'url': 'https://huggingface.co/papers/2510.20888', 'abstract': "Video-As-Prompt (VAP) uses a reference video to guide a frozen Video Diffusion Transformer via a Mixture-of-Transformers expert, achieving state-of-the-art results in semantic-controlled video generation with strong zero-shot generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified, generalizable semantic control in video generation remains a critical open challenge. Existing methods either introduce artifacts by enforcing inappropriate pixel-wise priors from structure-based controls, or rely on non-generalizable, condition-specific finetuning or task-specific architectures. We introduce Video-As-Prompt (VAP), a new paradigm that reframes this problem as in-context generation. VAP leverages a reference video as a direct semantic prompt, guiding a frozen Video Diffusion Transformer (DiT) via a plug-and-play Mixture-of-Transformers (MoT) expert. This architecture prevents catastrophic forgetting and is guided by a temporally biased position embedding that eliminates spurious mapping priors for robust context retrieval. To power this approach and catalyze future research, we built VAP-Data, the largest dataset for semantic-controlled video generation with over 100K paired videos across 100 semantic conditions. As a single unified model, VAP sets a new state-of-the-art for open-source methods, achieving a 38.7% user preference rate that rivals leading condition-specific commercial models. VAP's strong zero-shot generalization and support for various downstream applications mark a significant advance toward general-purpose, controllable video generation.", 'score': 33, 'issue_id': 6621, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '519377609a2a0255', 'authors': ['Yuxuan Bian', 'Xin Chen', 'Zenan Li', 'Tiancheng Zhi', 'Shen Sang', 'Linjie Luo', 'Qiang Xu'], 'affiliations': ['Intelligent Creation Lab, ByteDance', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.20888.jpg', 'data': {'categories': ['#video', '#open_source', '#dataset', '#architecture', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Видео как промпт: универсальный контроль генерации через референс', 'desc': 'Статья представляет Video-As-Prompt (VAP) — новый подход к контролируемой генерации видео, где референсное видео используется как семантический промпт для замороженного Video Diffusion Transformer. Архитектура основана на Mixture-of-Transformers эксперте, который обеспечивает сильную zero-shot генерализацию без катастрофического забывания. Авторы создали VAP-Data — крупнейший датасет для семантически контролируемой генерации видео с более чем 100K парами видео по 100 условиям. Модель достигает state-of-the-art результатов среди open-source методов с 38.7% предпочтением пользователей, конкурируя с коммерческими специализированными моделями.'}, 'en': {'title': 'Revolutionizing Video Generation with Contextual Prompts', 'desc': "Video-As-Prompt (VAP) is a novel approach in video generation that utilizes a reference video to guide a pre-trained Video Diffusion Transformer, enhancing semantic control without the need for extensive fine-tuning. This method employs a Mixture-of-Transformers expert to ensure robust context retrieval while avoiding common pitfalls like catastrophic forgetting and inappropriate pixel-wise priors. VAP introduces a temporally biased position embedding to improve the model's ability to generate coherent and contextually relevant videos. With the creation of VAP-Data, a comprehensive dataset of over 100,000 paired videos, VAP achieves state-of-the-art results in user preference and demonstrates strong zero-shot generalization capabilities."}, 'zh': {'title': '视频生成的新范式：视频作为提示', 'desc': 'Video-As-Prompt (VAP) 是一种新方法，通过参考视频来引导一个冻结的Video Diffusion Transformer，从而实现语义控制的视频生成。该方法使用混合变换器专家，避免了传统方法中常见的伪影问题，并且不需要特定任务的微调。VAP 通过时间偏置位置嵌入来增强上下文检索的鲁棒性，防止灾难性遗忘。VAP-Data 数据集是目前最大的语义控制视频生成数据集，包含超过10万对视频，推动了这一领域的研究进展。'}}}, {'id': 'https://huggingface.co/papers/2510.21682', 'title': 'WorldGrow: Generating Infinite 3D World', 'url': 'https://huggingface.co/papers/2510.21682', 'abstract': 'WorldGrow, a hierarchical framework, generates large, continuous 3D environments with coherent geometry and realistic appearance using pre-trained 3D models and a coarse-to-fine generation strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We tackle the challenge of generating the infinitely extendable 3D world -- large, continuous environments with coherent geometry and realistic appearance. Existing methods face key challenges: 2D-lifting approaches suffer from geometric and appearance inconsistencies across views, 3D implicit representations are hard to scale up, and current 3D foundation models are mostly object-centric, limiting their applicability to scene-level generation. Our key insight is leveraging strong generation priors from pre-trained 3D models for structured scene block generation. To this end, we propose WorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our method features three core components: (1) a data curation pipeline that extracts high-quality scene blocks for training, making the 3D structured latent representations suitable for scene generation; (2) a 3D block inpainting mechanism that enables context-aware scene extension; and (3) a coarse-to-fine generation strategy that ensures both global layout plausibility and local geometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset, WorldGrow achieves SOTA performance in geometry reconstruction, while uniquely supporting infinite scene generation with photorealistic and structurally consistent outputs. These results highlight its capability for constructing large-scale virtual environments and potential for building future world models.', 'score': 26, 'issue_id': 6621, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'c8660a9a47c4dbe3', 'authors': ['Sikuang Li', 'Chen Yang', 'Jiemin Fang', 'Taoran Yi', 'Jia Lu', 'Jiazhong Cen', 'Lingxi Xie', 'Wei Shen', 'Qi Tian'], 'affiliations': ['Huawei Inc.', 'Huazhong University of Science and Technology', 'MoE Key Lab of Artificial Intelligence, School of Computer Science, SJTU'], 'pdf_title_img': 'assets/pdf/title_img/2510.21682.jpg', 'data': {'categories': ['#3d', '#dataset', '#games'], 'emoji': '🌍', 'ru': {'title': 'Бесконечная генерация 3D-миров блок за блоком', 'desc': 'WorldGrow - это иерархический фреймворк для генерации бесконечно расширяемых 3D-миров с реалистичной геометрией и текстурами. Метод использует предобученные 3D-модели и генерирует сцены блоками через механизм inpainting с учётом контекста. Ключевая особенность - стратегия генерации от грубого к детальному, обеспечивающая согласованность как глобальной структуры, так и локальных деталей. Система достигает state-of-the-art результатов на датасете 3D-FRONT и способна создавать фотореалистичные виртуальные окружения неограниченного масштаба.'}, 'en': {'title': 'Infinite 3D Worlds Made Real with WorldGrow', 'desc': 'WorldGrow is a hierarchical framework designed to create large, continuous 3D environments that maintain coherent geometry and realistic appearances. It addresses the limitations of existing methods, such as inconsistencies in 2D-lifting approaches and the challenges of scaling 3D implicit representations. By utilizing pre-trained 3D models, WorldGrow employs a coarse-to-fine generation strategy that enhances both the global layout and local details of the generated scenes. The framework demonstrates state-of-the-art performance in geometry reconstruction and supports the generation of infinite, photorealistic environments, making it a significant advancement in 3D scene synthesis.'}, 'zh': {'title': '无限扩展的3D世界生成框架', 'desc': 'WorldGrow是一个层次化框架，旨在生成大型、连续的3D环境，具有一致的几何形状和真实的外观。该方法利用预训练的3D模型和粗到细的生成策略，解决了现有方法在几何和外观一致性方面的挑战。WorldGrow的核心组件包括高质量场景块的数据整理管道、上下文感知的3D块修复机制，以及确保全局布局合理性和局部几何/纹理保真度的生成策略。经过在大规模3D-FRONT数据集上的评估，WorldGrow在几何重建方面达到了最先进的性能，并支持无限场景生成，展示了其构建大规模虚拟环境的能力。'}}}, {'id': 'https://huggingface.co/papers/2510.21583', 'title': 'Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image\n  Generation', 'url': 'https://huggingface.co/papers/2510.21583', 'abstract': "Chunk-GRPO, a chunk-level optimization approach for text-to-image generation, improves preference alignment and image quality by addressing inaccurate advantage attribution and neglecting temporal dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t Group Relative Policy Optimization (GRPO) has shown strong potential for flow-matching-based text-to-image (T2I) generation, but it faces two key limitations: inaccurate advantage attribution, and the neglect of temporal dynamics of generation. In this work, we argue that shifting the optimization paradigm from the step level to the chunk level can effectively alleviate these issues. Building on this idea, we propose Chunk-GRPO, the first chunk-level GRPO-based approach for T2I generation. The insight is to group consecutive steps into coherent 'chunk's that capture the intrinsic temporal dynamics of flow matching, and to optimize policies at the chunk level. In addition, we introduce an optional weighted sampling strategy to further enhance performance. Extensive experiments show that ChunkGRPO achieves superior results in both preference alignment and image quality, highlighting the promise of chunk-level optimization for GRPO-based methods.", 'score': 26, 'issue_id': 6625, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'de5b9cebb6c64fcc', 'authors': ['Yifu Luo', 'Penghui Du', 'Bo Li', 'Sinan Du', 'Tiantian Zhang', 'Yongzhe Chang', 'Kai Wu', 'Kun Gai', 'Xueqian Wang'], 'affiliations': ['Kolors Team, Kuaishou Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.21583.jpg', 'data': {'categories': ['#training', '#optimization', '#alignment', '#cv', '#rl'], 'emoji': '🧩', 'ru': {'title': 'Оптимизация по частям: улучшаем генерацию изображений через временные блоки', 'desc': 'Исследователи предложили Chunk-GRPO — новый подход к оптимизации генерации изображений из текста, который работает не с отдельными шагами, а с группами последовательных шагов («чанками»). Метод решает проблемы традиционного GRPO: неточное определение вклада каждого шага и игнорирование временной динамики процесса генерации. Группируя шаги в связные блоки, алгоритм лучше учитывает внутреннюю временную структуру flow matching моделей. Эксперименты показали превосходство Chunk-GRPO как в качестве изображений, так и в соответствии предпочтениям пользователей.'}, 'en': {'title': 'Enhancing Text-to-Image Generation with Chunk-Level Optimization', 'desc': "Chunk-GRPO is a novel approach for improving text-to-image generation by optimizing at the chunk level rather than the individual step level. This method addresses two main challenges: inaccurate advantage attribution and the neglect of temporal dynamics during the generation process. By grouping consecutive steps into coherent 'chunks', it captures the flow of generation more effectively and enhances policy optimization. The introduction of a weighted sampling strategy further boosts performance, leading to better preference alignment and higher image quality in the generated outputs."}, 'zh': {'title': '块级优化提升文本到图像生成质量', 'desc': 'Chunk-GRPO是一种针对文本到图像生成的块级优化方法，旨在提高偏好对齐和图像质量。该方法解决了优势归因不准确和忽视生成时间动态的问题。通过将优化范式从步骤级别转移到块级别，Chunk-GRPO能够有效捕捉流匹配的内在时间动态。实验结果表明，Chunk-GRPO在偏好对齐和图像质量方面均表现出色，展示了块级优化在GRPO方法中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2510.19871', 'title': 'From Denoising to Refining: A Corrective Framework for Vision-Language\n  Diffusion Model', 'url': 'https://huggingface.co/papers/2510.19871', 'abstract': "ReDiff, a refining-enhanced diffusion framework, addresses train-inference discrepancies in discrete diffusion models by enabling the model to identify and correct its own errors, improving coherence and factual accuracy in generated content.  \t\t\t\t\tAI-generated summary \t\t\t\t Discrete diffusion models have emerged as a promising direction for vision-language tasks, offering bidirectional context modeling and theoretical parallelization. However, their practical application is severely hindered by a train-inference discrepancy, which leads to catastrophic error cascades: initial token errors during parallel decoding pollute the generation context, triggering a chain reaction of compounding errors and leading to syntactic errors and semantic hallucinations. To address this fundamental challenge, we reframe the generation process from passive denoising to active refining. We introduce ReDiff, a refining-enhanced diffusion framework that teaches the model to identify and correct its own errors. Our approach features a two-stage training process: first, we instill a foundational revision capability by training the model to revise synthetic errors; second, we implement a novel online self-correction loop where the model is explicitly trained to revise its own flawed drafts by learning from an expert's corrections. This mistake-driven learning endows the model with the crucial ability to revisit and refine its already generated output, effectively breaking the error cascade. Extensive experiments demonstrate that ReDiff significantly improves the coherence and factual accuracy of generated content, enabling stable and efficient parallel generation far superior to traditional denoising methods. Our codes and models are available at https://rediff-hku.github.io/.", 'score': 24, 'issue_id': 6622, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '5e3206cbeb5b995b', 'authors': ['Yatai Ji', 'Teng Wang', 'Yuying Ge', 'Zhiheng Liu', 'Sidi Yang', 'Ying Shan', 'Ping Luo'], 'affiliations': ['ARC Lab, Tencent PCG', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.19871.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#diffusion', '#rl', '#training', '#cv'], 'emoji': '🔄', 'ru': {'title': 'Учим модель исправлять свои собственные ошибки', 'desc': 'Дискретные диффузионные модели сталкиваются с проблемой расхождения между обучением и инференсом, что приводит к каскаду ошибок при параллельной генерации токенов. ReDiff решает эту проблему, обучая модель не просто удалять шум, а активно находить и исправлять собственные ошибки. Обучение проходит в два этапа: сначала модель учится исправлять синтетические ошибки, затем в процессе онлайн-обучения модель корректирует свои собственные черновики, учась на исправлениях эксперта. Такой подход значительно улучшает связность и фактическую точность генерируемого контента, разрывая цепочку накапливающихся ошибок.'}, 'en': {'title': 'Refining Errors for Better AI Generation', 'desc': "ReDiff is a new framework designed to improve discrete diffusion models by addressing the errors that occur during the generation of content. It focuses on correcting mistakes made during the initial token generation, which can lead to a series of compounding errors. The framework employs a two-stage training process that first teaches the model to revise synthetic errors and then allows it to learn from its own mistakes through an online self-correction loop. This approach enhances the model's ability to produce coherent and factually accurate outputs, making it more effective than traditional methods."}, 'zh': {'title': '主动修正，提升生成质量的ReDiff框架', 'desc': 'ReDiff是一种增强精炼的扩散框架，旨在解决离散扩散模型在训练和推理之间的差异。该模型通过主动修正自身错误，提高生成内容的一致性和事实准确性。我们采用了两阶段的训练过程，首先训练模型修正合成错误，然后通过在线自我修正循环，让模型学习专家的修正。实验结果表明，ReDiff显著提升了生成内容的连贯性和准确性，超越了传统去噪方法。'}}}, {'id': 'https://huggingface.co/papers/2510.18212', 'title': 'A Definition of AGI', 'url': 'https://huggingface.co/papers/2510.18212', 'abstract': 'A quantifiable framework based on Cattell-Horn-Carroll theory evaluates AI systems across ten cognitive domains, revealing significant gaps in foundational cognitive abilities like long-term memory.  \t\t\t\t\tAI-generated summary \t\t\t\t The lack of a concrete definition for Artificial General Intelligence (AGI) obscures the gap between today\'s specialized AI and human-level cognition. This paper introduces a quantifiable framework to address this, defining AGI as matching the cognitive versatility and proficiency of a well-educated adult. To operationalize this, we ground our methodology in Cattell-Horn-Carroll theory, the most empirically validated model of human cognition. The framework dissects general intelligence into ten core cognitive domains-including reasoning, memory, and perception-and adapts established human psychometric batteries to evaluate AI systems. Application of this framework reveals a highly "jagged" cognitive profile in contemporary models. While proficient in knowledge-intensive domains, current AI systems have critical deficits in foundational cognitive machinery, particularly long-term memory storage. The resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify both rapid progress and the substantial gap remaining before AGI.', 'score': 23, 'issue_id': 6621, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': 'be8269623d74c8cf', 'authors': ['Dan Hendrycks', 'Dawn Song', 'Christian Szegedy', 'Honglak Lee', 'Yarin Gal', 'Erik Brynjolfsson', 'Sharon Li', 'Andy Zou', 'Lionel Levine', 'Bo Han', 'Jie Fu', 'Ziwei Liu', 'Jinwoo Shin', 'Kimin Lee', 'Mantas Mazeika', 'Long Phan', 'George Ingebretsen', 'Adam Khoja', 'Cihang Xie', 'Olawale Salaudeen', 'Matthias Hein', 'Kevin Zhao', 'Alexander Pan', 'David Duvenaud', 'Bo Li', 'Steve Omohundro', 'Gabriel Alfour', 'Max Tegmark', 'Kevin McGrew', 'Gary Marcus', 'Jaan Tallinn', 'Eric Schmidt', 'Yoshua Bengio'], 'affiliations': ['Beneficial AI Research', 'CSER', 'Carnegie Mellon University', 'Center for AI Safety', 'Conjecture', 'Cornell University', 'Gray Swan AI', 'HKUST', 'Hong Kong Baptist University', 'Institute for Applied Psychometrics', 'KAIST', 'LG AI Research', 'LawZero', 'Massachusetts Institute of Technology', 'Morph Labs', 'Nanyang Technological University', 'New York University', 'Stanford University', 'University of California, Berkeley', 'University of California, Santa Cruz', 'University of Chicago', 'University of Michigan', 'University of Oxford', 'University of Toronto', 'University of Tübingen', 'University of Washington', 'University of Wisconsin-Madison', 'Université de Montréal', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2510.18212.jpg', 'data': {'categories': ['#agi', '#benchmark', '#long_context', '#reasoning', '#math'], 'emoji': '🧠', 'ru': {'title': 'Измеряя разрыв до AGI через призму человеческой психометрии', 'desc': 'Статья предлагает количественную методику оценки AI-систем на основе теории Кеттелла-Хорна-Кэрролла, рассматривая десять когнитивных доменов человеческого интеллекта. Авторы адаптируют психометрические тесты для людей, чтобы измерить способности LLM в областях рассуждения, памяти и восприятия. Исследование показывает "зубчатый" профиль современных моделей: они сильны в задачах, требующих знаний, но имеют критические недостатки в базовых когнитивных функциях, особенно в долговременной памяти. По предложенной шкале GPT-4 достигает 27% на пути к AGI, а GPT-5 - 58%, что наглядно демонстрирует прогресс и оставшийся разрыв до человеческого уровня.'}, 'en': {'title': 'Bridging the Gap to Artificial General Intelligence', 'desc': 'This paper presents a framework to evaluate AI systems based on the Cattell-Horn-Carroll theory of intelligence, which breaks down general intelligence into ten cognitive domains. It aims to define Artificial General Intelligence (AGI) as the ability to perform across these domains like a well-educated adult. The evaluation shows that while current AI models excel in knowledge-heavy tasks, they significantly lack in foundational cognitive abilities, especially in long-term memory. The framework provides quantifiable AGI scores, highlighting both the advancements made and the considerable gaps that still exist in achieving true AGI.'}, 'zh': {'title': '量化框架揭示AI认知能力的差距', 'desc': '本文提出了一个基于Cattell-Horn-Carroll理论的量化框架，用于评估人工智能系统在十个认知领域的表现。研究发现，当前的AI系统在基础认知能力上存在显著差距，尤其是在长期记忆方面。通过将一般智力分解为推理、记忆和感知等核心认知领域，框架能够有效评估AI的认知能力。最终的AGI评分显示，尽管AI在知识密集型领域表现出色，但在实现与人类相当的认知灵活性和熟练度方面仍有很大提升空间。'}}}, {'id': 'https://huggingface.co/papers/2510.14901', 'title': 'Reasoning with Sampling: Your Base Model is Smarter Than You Think', 'url': 'https://huggingface.co/papers/2510.14901', 'abstract': "An iterative sampling algorithm enhances reasoning capabilities in base models without additional training, matching or outperforming reinforcement learning on single-shot tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Frontier reasoning models have exhibited incredible capabilities across a wide array of disciplines, driven by posttraining large language models (LLMs) with reinforcement learning (RL). However, despite the widespread success of this paradigm, much of the literature has been devoted to disentangling truly novel behaviors that emerge during RL but are not present in the base models. In our work, we approach this question from a different angle, instead asking whether comparable reasoning capabilites can be elicited from base models at inference time by pure sampling, without any additional training. Inspired by Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened distributions, we propose a simple iterative sampling algorithm leveraging the base models' own likelihoods. Over different base models, we show that our algorithm offers substantial boosts in reasoning that nearly match and even outperform those from RL on a wide variety of single-shot tasks, including MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in diversity over multiple samples that is characteristic of RL-posttraining. Crucially, our method does not require training, curated datasets, or a verifier, suggesting broad applicability beyond easily verifiable domains.", 'score': 21, 'issue_id': 6624, 'pub_date': '2025-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': 'eab418eb30e48c40', 'authors': ['Aayush Karan', 'Yilun Du'], 'affiliations': ['Harvard University'], 'pdf_title_img': 'assets/pdf/title_img/2510.14901.jpg', 'data': {'categories': ['#optimization', '#inference', '#math', '#rl', '#reasoning'], 'emoji': '🔄', 'ru': {'title': 'Рассуждения без обучения: сила итеративного сэмплирования', 'desc': 'Исследователи предложили итеративный алгоритм сэмплирования, который улучшает способности базовых LLM к рассуждениям без дополнительного обучения. Метод основан на техниках Markov chain Monte Carlo и использует собственные вероятности базовой модели для генерации ответов. Алгоритм показывает результаты, сравнимые или превосходящие reinforcement learning на задачах типа MATH500, HumanEval и GPQA. Ключевое преимущество — не требуется обучение, датасеты или верификатор, при этом сохраняется разнообразие генерируемых ответов.'}, 'en': {'title': 'Boosting Reasoning with Iterative Sampling!', 'desc': "This paper presents an iterative sampling algorithm that enhances the reasoning abilities of base models without the need for additional training. The approach leverages the models' own likelihoods, inspired by Markov chain Monte Carlo techniques, to generate improved reasoning outputs. The results demonstrate that this method can match or even surpass the performance of reinforcement learning on various single-shot tasks. Importantly, the algorithm maintains diversity in its outputs, avoiding common pitfalls associated with reinforcement learning post-training."}, 'zh': {'title': '无训练增强推理能力的迭代采样算法', 'desc': '本文提出了一种迭代采样算法，可以在不进行额外训练的情况下增强基础模型的推理能力。该算法利用基础模型自身的可能性，通过纯采样在推理时引发类似于强化学习的推理能力。实验结果表明，该算法在多个单次任务上，推理能力几乎可以与强化学习相媲美，甚至超越其表现。此外，该方法避免了强化学习后训练中样本多样性下降的问题，具有广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2510.21270', 'title': 'Sparser Block-Sparse Attention via Token Permutation', 'url': 'https://huggingface.co/papers/2510.21270', 'abstract': 'Permuted Block-Sparse Attention improves computational efficiency in large language models by enhancing block-level sparsity in the self-attention mechanism, achieving significant speedups without compromising accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling the context length of large language models (LLMs) offers significant benefits but is computationally expensive. This expense stems primarily from the self-attention mechanism, whose O(N^2) complexity with respect to sequence length presents a major bottleneck for both memory and latency. Fortunately, the attention matrix is often sparse, particularly for long sequences, suggesting an opportunity for optimization. Block-sparse attention has emerged as a promising solution that partitions sequences into blocks and skips computation for a subset of these blocks. However, the effectiveness of this method is highly dependent on the underlying attention patterns, which can lead to sub-optimal block-level sparsity. For instance, important key tokens for queries within a single block may be scattered across numerous other blocks, leading to computational redundancy. In this work, we propose Permuted Block-Sparse Attention (PBS-Attn), a plug-and-play method that leverages the permutation properties of attention to increase block-level sparsity and enhance the computational efficiency of LLM prefilling. We conduct comprehensive experiments on challenging real-world long-context datasets, demonstrating that PBS-Attn consistently outperforms existing block-sparse attention methods in model accuracy and closely matches the full attention baseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn achieves an end-to-end speedup of up to 2.75times in long-context prefilling, confirming its practical viability. Code available at https://github.com/xinghaow99/pbs-attn', 'score': 20, 'issue_id': 6622, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '37b575b4da897688', 'authors': ['Xinghao Wang', 'Pengyu Wang', 'Dong Zhang', 'Chenkun Tan', 'Shaojun Zhou', 'Zhaoxiang Liu', 'Shiguo Lian', 'Fangxu Liu', 'Kai Song', 'Xipeng Qiu'], 'affiliations': ['ByteDance', 'China Unicom', 'Fudan University', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2510.21270.jpg', 'data': {'categories': ['#optimization', '#long_context', '#training', '#architecture', '#inference'], 'emoji': '🔀', 'ru': {'title': 'Перестановка токенов для эффективного разреженного внимания в LLM', 'desc': 'Статья предлагает метод Permuted Block-Sparse Attention (PBS-Attn) для ускорения обработки длинных контекстов в больших языковых моделях. Проблема заключается в квадратичной сложности механизма self-attention, который становится узким местом при увеличении длины последовательности. PBS-Attn использует перестановку токенов для увеличения блочной разреженности матрицы внимания, что позволяет пропускать вычисления для целых блоков без потери точности. Эксперименты показывают ускорение до 2.75 раз на этапе prefilling при сохранении качества модели на уровне полного attention.'}, 'en': {'title': 'Boosting Efficiency with Permuted Block-Sparse Attention', 'desc': 'The paper introduces Permuted Block-Sparse Attention (PBS-Attn), a method designed to improve the efficiency of large language models (LLMs) by optimizing the self-attention mechanism. It addresses the computational challenges posed by the O(N^2) complexity of traditional attention methods, particularly for long sequences. By enhancing block-level sparsity through permutation properties, PBS-Attn reduces unnecessary computations while maintaining model accuracy. Experimental results show that PBS-Attn significantly speeds up processing times, achieving up to 2.75 times faster performance compared to existing block-sparse attention techniques.'}, 'zh': {'title': '提升大语言模型效率的块稀疏注意力', 'desc': 'Permuted Block-Sparse Attention（PBS-Attn）是一种提高大语言模型计算效率的方法，通过增强自注意力机制中的块级稀疏性来实现显著的速度提升，而不影响准确性。该方法通过对序列进行分块处理，跳过部分块的计算，从而优化了计算过程。研究表明，PBS-Attn在处理长上下文数据集时，能够在模型准确性上超越现有的块稀疏注意力方法，并且与完整注意力基线相近。通过定制的permute-FlashAttention内核，PBS-Attn在长上下文预填充中实现了最高2.75倍的端到端加速，证明了其实际可行性。'}}}, {'id': 'https://huggingface.co/papers/2510.20286', 'title': 'UI-Ins: Enhancing GUI Grounding with Multi-Perspective\n  Instruction-as-Reasoning', 'url': 'https://huggingface.co/papers/2510.20286', 'abstract': 'The Instruction-as-Reasoning paradigm enhances GUI grounding by treating instructions as dynamic pathways, improving performance through multi-perspective reasoning and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t GUI grounding, which maps natural-language instructions to actionable UI elements, is a core capability of GUI agents. Prior works largely treats instructions as a static proxy for user intent, overlooking the impact of instruction diversity and quality on grounding performance. Through a careful investigation of existing grounding datasets, we find a 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to a substantial 76% relative performance improvement. In this paper, we introduce the Instruction-as-Reasoning paradigm, treating instructions as dynamic analytical pathways that offer distinct perspectives and enabling the model to select the most effective pathway during reasoning. To achieve this, we propose a two-stage training framework: supervised fine-tuning (SFT) on synthesized, diverse instructions to instill multi-perspective reasoning, followed by reinforcement learning (RL) to optimize pathway selection and composition. Our resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results on five challenging grounding benchmarks and exhibit emergent reasoning, selectively composing and synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model demonstrates strong agentic potential, achieving a 74.1% success rate on AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals additional insights such as how reasoning can be formulated to enhance rather than hinder grounding performance, and how our method mitigates policy collapse in the SFT+RL framework. All code and model checkpoints will be publicly released in https://github.com/alibaba/UI-Ins.', 'score': 16, 'issue_id': 6621, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '7a550042831907d9', 'authors': ['Liangyu Chen', 'Hanzhang Zhou', 'Chenglin Cai', 'Jianan Zhang', 'Panrong Tong', 'Quyu Kong', 'Xu Zhang', 'Chen Liu', 'Yuqi Liu', 'Wenxuan Wang', 'Yue Wang', 'Qin Jin', 'Steven Hoi'], 'affiliations': ['CUHK', 'Renmin University of China', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.20286.jpg', 'data': {'categories': ['#training', '#rl', '#open_source', '#benchmark', '#agents', '#optimization', '#reasoning'], 'emoji': '🎯', 'ru': {'title': 'Инструкции как пути рассуждения: новый подход к пониманию интерфейсов', 'desc': 'Исследователи представили новую парадигму Instruction-as-Reasoning для GUI grounding - задачи сопоставления естественно-языковых инструкций с элементами пользовательского интерфейса. Вместо того чтобы рассматривать инструкции как статичные команды, модель использует их как динамические аналитические пути, выбирая наиболее эффективный подход к рассуждению. Обучение проходит в два этапа: сначала supervised fine-tuning на разнообразных синтезированных инструкциях, затем reinforcement learning для оптимизации выбора путей рассуждения. Модели UI-Ins-7B и UI-Ins-32B достигли state-of-the-art результатов на пяти бенчмарках, при этом UI-Ins-32B показала 87.3% точности на UI-I2E-Bench и 74.1% успешности в качестве агента на AndroidWorld.'}, 'en': {'title': 'Dynamic Pathways for Enhanced GUI Grounding', 'desc': 'This paper introduces the Instruction-as-Reasoning paradigm, which enhances GUI grounding by treating instructions as dynamic pathways for reasoning. It highlights the flaws in existing grounding datasets and demonstrates that leveraging instruction diversity can significantly improve performance. The authors propose a two-stage training framework that combines supervised fine-tuning and reinforcement learning to optimize the selection of effective reasoning pathways. Their models achieve state-of-the-art results on multiple benchmarks, showcasing improved grounding accuracy and emergent reasoning capabilities.'}, 'zh': {'title': '指令作为推理：提升GUI基础性能的动态路径', 'desc': '本论文提出了"指令作为推理"的范式，通过将指令视为动态分析路径，来增强图形用户界面（GUI）基础的性能。研究发现，现有数据集中指令的多样性和质量对基础性能有显著影响，且在推理时利用指令多样性可以提高性能达76%。我们提出了一个两阶段的训练框架，首先通过监督微调（SFT）来培养多角度推理，然后通过强化学习（RL）优化路径选择。最终模型在多个基准测试中表现出色，特别是在UI-I2E-Bench上达到了87.3%的最佳准确率。'}}}, {'id': 'https://huggingface.co/papers/2510.21697', 'title': 'Visual Diffusion Models are Geometric Solvers', 'url': 'https://huggingface.co/papers/2510.21697', 'abstract': 'Visual diffusion models can solve geometric problems by transforming noisy images into valid solutions, demonstrating a novel approach to geometric reasoning through image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper we show that visual diffusion models can serve as effective geometric solvers: they can directly reason about geometric problems by working in pixel space. We first demonstrate this on the Inscribed Square Problem, a long-standing problem in geometry that asks whether every Jordan curve contains four points forming a square. We then extend the approach to two other well-known hard geometric problems: the Steiner Tree Problem and the Simple Polygon Problem.   Our method treats each problem instance as an image and trains a standard visual diffusion model that transforms Gaussian noise into an image representing a valid approximate solution that closely matches the exact one. The model learns to transform noisy geometric structures into correct configurations, effectively recasting geometric reasoning as image generation.   Unlike prior work that necessitates specialized architectures and domain-specific adaptations when applying diffusion to parametric geometric representations, we employ a standard visual diffusion model that operates on the visual representation of the problem. This simplicity highlights a surprising bridge between generative modeling and geometric problem solving. Beyond the specific problems studied here, our results point toward a broader paradigm: operating in image space provides a general and practical framework for approximating notoriously hard problems, and opens the door to tackling a far wider class of challenging geometric tasks.', 'score': 13, 'issue_id': 6621, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'ca1111516b2085c3', 'authors': ['Nir Goren', 'Shai Yehezkel', 'Omer Dahary', 'Andrey Voynov', 'Or Patashnik', 'Daniel Cohen-Or'], 'affiliations': ['Google DeepMind', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2510.21697.jpg', 'data': {'categories': ['#cv', '#reasoning', '#multimodal', '#diffusion'], 'emoji': '🔷', 'ru': {'title': 'Геометрия через генерацию: диффузионные модели как решатели задач', 'desc': 'Исследователи показали, что визуальные диффузионные модели могут решать сложные геометрические задачи, работая напрямую с изображениями. Модель обучается преобразовывать шумные картинки в валидные геометрические решения, превращая геометрическое рассуждение в задачу генерации изображений. В отличие от предыдущих подходов, метод использует стандартные visual diffusion модели без специализированных архитектур. Подход был успешно применён к известным сложным задачам: проблеме вписанного квадрата, задаче Штейнера и проблеме простого многоугольника.'}, 'en': {'title': 'Transforming Noise into Geometry: A New Approach with Visual Diffusion Models', 'desc': 'This paper explores how visual diffusion models can effectively solve geometric problems by transforming noisy images into valid solutions. It demonstrates this capability through the Inscribed Square Problem, the Steiner Tree Problem, and the Simple Polygon Problem, treating each problem as an image. The model learns to convert Gaussian noise into images that represent approximate solutions, thereby framing geometric reasoning as a process of image generation. This approach simplifies the application of diffusion models to geometric tasks, suggesting a new paradigm for addressing complex geometric challenges using standard visual models.'}, 'zh': {'title': '视觉扩散模型：几何问题的新解法', 'desc': '本文展示了视觉扩散模型如何通过将噪声图像转化为有效解来解决几何问题。这种方法在像素空间中直接推理几何问题，首次应用于著名的内切正方形问题。我们的方法将每个问题实例视为图像，并训练标准的视觉扩散模型，将高斯噪声转化为接近精确解的有效近似解。我们的研究表明，图像空间的操作为近似解决复杂几何问题提供了一个通用且实用的框架。'}}}, {'id': 'https://huggingface.co/papers/2510.20206', 'title': 'RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via\n  Data Alignment and Test-Time Scaling', 'url': 'https://huggingface.co/papers/2510.20206', 'abstract': 'RAPO++ enhances text-to-video generation by optimizing user prompts through retrieval, iterative refinement, and LLM fine-tuning, improving semantic alignment, compositionality, and temporal coherence.  \t\t\t\t\tAI-generated summary \t\t\t\t Prompt design plays a crucial role in text-to-video (T2V) generation, yet user-provided prompts are often short, unstructured, and misaligned with training data, limiting the generative potential of diffusion-based T2V models. We present RAPO++, a cross-stage prompt optimization framework that unifies training-data--aligned refinement, test-time iterative scaling, and large language model (LLM) fine-tuning to substantially improve T2V generation without modifying the underlying generative backbone. In Stage 1, Retrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with semantically relevant modifiers retrieved from a relation graph and refactors them to match training distributions, enhancing compositionality and multi-object fidelity. Stage 2 introduces Sample-Specific Prompt Optimization (SSPO), a closed-loop mechanism that iteratively refines prompts using multi-source feedback -- including semantic alignment, spatial fidelity, temporal coherence, and task-specific signals such as optical flow -- yielding progressively improved video generation quality. Stage 3 leverages optimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing task-specific optimization patterns and enabling efficient, high-quality prompt generation even before inference. Extensive experiments across five state-of-the-art T2V models and five benchmarks demonstrate that RAPO++ achieves significant gains in semantic alignment, compositional reasoning, temporal stability, and physical plausibility, outperforming existing methods by large margins. Our results highlight RAPO++ as a model-agnostic, cost-efficient, and scalable solution that sets a new standard for prompt optimization in T2V generation. The code is available at https://github.com/Vchitect/RAPO.', 'score': 11, 'issue_id': 6622, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '0715013010b8fa21', 'authors': ['Bingjie Gao', 'Qianli Ma', 'Xiaoxue Wu', 'Shuai Yang', 'Guanzhou Lan', 'Haonan Zhao', 'Jiaxuan Chen', 'Qingyang Liu', 'Yu Qiao', 'Xinyuan Chen', 'Yaohui Wang', 'Li Niu'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory, Shanghai 201112, China', 'Shanghai Jiao Tong University, Shanghai 200240, China'], 'pdf_title_img': 'assets/pdf/title_img/2510.20206.jpg', 'data': {'categories': ['#video', '#multimodal', '#optimization', '#rag', '#diffusion', '#training'], 'emoji': '🎬', 'ru': {'title': 'Трёхэтапная оптимизация промптов для улучшения text-to-video генерации', 'desc': 'RAPO++ — это фреймворк для оптимизации промптов в генерации видео из текста, который работает в три этапа. Первый этап обогащает пользовательские промпты релевантными модификаторами через retrieval и приводит их к формату обучающих данных. Второй этап итеративно улучшает промпты на основе обратной связи по семантическому соответствию, пространственной точности и временной когерентности. Третий этап дообучает LLM на оптимизированных парах промптов, что позволяет генерировать качественные промпты ещё до inference без изменения самой генеративной модели.'}, 'en': {'title': 'Revolutionizing Text-to-Video Generation with RAPO++', 'desc': 'RAPO++ is a framework designed to improve text-to-video (T2V) generation by optimizing user prompts. It uses a three-stage process that includes retrieving relevant modifiers, refining prompts iteratively, and fine-tuning a large language model (LLM) to enhance the quality of generated videos. The framework focuses on improving semantic alignment, compositionality, and temporal coherence, which are crucial for creating realistic and coherent videos. Extensive testing shows that RAPO++ significantly outperforms existing methods, making it a versatile and efficient solution for T2V generation.'}, 'zh': {'title': 'RAPO++：文本到视频生成的新标准', 'desc': 'RAPO++ 是一种增强文本到视频生成的框架，通过优化用户提示来提高生成效果。它结合了检索、迭代精炼和大语言模型微调，改善了语义对齐、组合性和时间一致性。该框架分为三个阶段，首先通过检索相关修饰符来丰富用户提示，然后利用多源反馈迭代优化提示，最后微调重写的 LLM 以实现高效的提示生成。实验结果表明，RAPO++ 在多个基准测试中显著提升了生成视频的质量，成为文本到视频生成的新标准。'}}}, {'id': 'https://huggingface.co/papers/2510.20479', 'title': 'RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via\n  Hierarchical Model Merging', 'url': 'https://huggingface.co/papers/2510.20479', 'abstract': 'RECALL is a representation-aware framework for continual learning in large language models that merges models without historical data, preserving domain-general features and adapting to task-specific knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t We unveil that internal representations in large language models (LLMs) serve as reliable proxies of learned knowledge, and propose RECALL, a novel representation-aware model merging framework for continual learning without access to historical data. RECALL computes inter-model similarity from layer-wise hidden representations over clustered typical samples, and performs adaptive, hierarchical parameter fusion to align knowledge across models. This design enables the preservation of domain-general features in shallow layers while allowing task-specific adaptation in deeper layers. Unlike prior methods that require task labels or incur performance trade-offs, RECALL achieves seamless multi-domain integration and strong resistance to catastrophic forgetting. Extensive experiments across five NLP tasks and multiple continual learning scenarios show that RECALL outperforms baselines in both knowledge retention and generalization, providing a scalable and data-free solution for evolving LLMs.', 'score': 10, 'issue_id': 6622, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '0c4b0e43a28cdf2a', 'authors': ['Bowen Wang', 'Haiyuan Wan', 'Liwen Shi', 'Chen Yang', 'Peng He', 'Yue Ma', 'Haochen Han', 'Wenhao Li', 'Tiao Tan', 'Yongjian Li', 'Fangming Liu', 'Yifan Gong', 'Sheng Zhang'], 'affiliations': ['Huazhong University of Science and Technology', 'Peng Cheng Laboratory', 'School of Biomedical Engineering, Tsinghua University', 'Shenzhen International Graduate School, Tsinghua University', 'The Hong Kong University of Science and Technology, Guangzhou', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2510.20479.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#optimization', '#agents'], 'emoji': '🔄', 'ru': {'title': 'Слияние моделей через внутренние представления без потери памяти', 'desc': 'Статья представляет RECALL — новый метод continual learning для больших языковых моделей, который позволяет объединять модели без доступа к историческим данным обучения. Ключевая идея заключается в использовании внутренних представлений LLM как индикаторов полученных знаний и адаптивном слиянии параметров на разных уровнях сети. Метод сохраняет общие признаки в нижних слоях модели, позволяя при этом адаптировать верхние слои под специфику конкретных задач. Эксперименты показали, что RECALL эффективно решает проблему catastrophic forgetting и превосходит существующие методы в удержании знаний и обобщающей способности.'}, 'en': {'title': 'Seamless Learning with RECALL: Merging Knowledge Without History', 'desc': 'RECALL is a framework designed for continual learning in large language models (LLMs) that allows merging of models without needing past data. It leverages internal representations as indicators of learned knowledge and computes similarities between models based on their hidden layers. The framework uses adaptive parameter fusion to maintain general features while adapting to specific tasks. This approach not only prevents performance loss but also enhances knowledge retention and generalization across various natural language processing tasks.'}, 'zh': {'title': 'RECALL：无历史数据的持续学习新框架', 'desc': 'RECALL是一种针对大型语言模型的持续学习框架，能够在没有历史数据的情况下合并模型，保留领域通用特征并适应特定任务的知识。该框架利用大型语言模型中的内部表示作为学习知识的可靠代理，通过计算层级隐藏表示的模型间相似性，进行自适应的层次参数融合。RECALL的设计使得在浅层保留领域通用特征的同时，在深层进行特定任务的适应。与以往需要任务标签或存在性能折衷的方法不同，RECALL实现了无缝的多领域集成，并对灾难性遗忘具有强大的抵抗力。'}}}, {'id': 'https://huggingface.co/papers/2510.21223', 'title': 'Model Merging with Functional Dual Anchors', 'url': 'https://huggingface.co/papers/2510.21223', 'abstract': 'Functional Dual Anchors (FDAs) enhance model merging by aligning gradients with task vectors in the input-representation space, offering robustness and flexibility compared to parameter-space methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Model merging is an efficient post-training strategy for integrating knowledge from multiple finetuned checkpoints of a shared foundation model. Existing methods operate in the parameter space, combining task vectors to mitigate conflicts, but remain constrained by parameter inconsistencies. We propose Functional Dual Anchors (FDAs), a framework that instead models the input-representation space. FDAs are synthetic inputs whose induced gradients align with task vectors, capturing task-specific functional shifts relative to the pretrained model. This perspective bridges joint multi-task training and post-hoc merging, offering both robustness and flexibility. We further introduce a principled initialization scheme and show that FDAs are complementary to parameter-space model merging. Comprehensive experiments demonstrate the effectiveness of FDAs in model merging.', 'score': 9, 'issue_id': 6621, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '029e5f4701cf63dd', 'authors': ['Kexuan Shi', 'Yandong Wen', 'Weiyang Liu'], 'affiliations': ['The Chinese University of Hong Kong', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2510.21223.jpg', 'data': {'categories': ['#synthetic', '#architecture', '#training', '#optimization'], 'emoji': '⚓', 'ru': {'title': 'Функциональные якоря для умного объединения моделей', 'desc': 'Исследователи предложили новый подход к объединению fine-tuned моделей под названием Functional Dual Anchors (FDA). Вместо работы в пространстве параметров, метод работает в пространстве входов и представлений, используя синтетические входные данные как «якоря». Градиенты этих якорей выравниваются с task vectors, что позволяет эффективно захватывать специфичные для каждой задачи изменения относительно базовой модели. Метод показывает большую робастность по сравнению с классическими подходами и может их дополнять.'}, 'en': {'title': 'Enhancing Model Merging with Functional Dual Anchors', 'desc': 'Functional Dual Anchors (FDAs) improve the process of model merging by focusing on the input-representation space rather than the parameter space. This approach aligns gradients with task vectors, which helps to manage conflicts that arise when integrating knowledge from different finetuned models. FDAs act as synthetic inputs that reflect task-specific changes, enhancing the robustness and flexibility of the merging process. Our experiments show that FDAs work well alongside traditional parameter-space methods, making them a valuable addition to model merging techniques.'}, 'zh': {'title': '功能双锚：提升模型合并的鲁棒性与灵活性', 'desc': '功能双锚（FDAs）通过在输入表示空间中对齐梯度与任务向量，增强了模型合并的能力，相比于参数空间的方法，提供了更强的鲁棒性和灵活性。现有的模型合并方法主要在参数空间中操作，试图通过组合任务向量来减少冲突，但受到参数不一致性的限制。FDAs作为合成输入，其诱导的梯度与任务向量对齐，能够捕捉相对于预训练模型的任务特定功能变化。这种方法将联合多任务训练与后期合并相结合，展示了FDAs在模型合并中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2510.13251', 'title': 'Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs', 'url': 'https://huggingface.co/papers/2510.13251', 'abstract': 'Video Large Language Models (VideoLLMs) perform video question answering by initiating temporal reasoning through cross-frame interactions, followed by video-language integration, and generate answers using effective information pathways while suppressing unnecessary attention edges.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Large Language Models (VideoLLMs) extend the capabilities of vision-language models to spatiotemporal inputs, enabling tasks such as video question answering (VideoQA). Despite recent advances in VideoLLMs, their internal mechanisms on where and how they extract and propagate video and textual information remain less explored. In this study, we investigate the internal information flow of VideoLLMs using mechanistic interpretability techniques. Our analysis reveals consistent patterns across diverse VideoQA tasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame interactions in early-to-middle layers, (2) followed by progressive video-language integration in middle layers. This is facilitated by alignment between video representations and linguistic embeddings containing temporal concepts. (3) Upon completion of this integration, the model is ready to generate correct answers in middle-to-late layers. (4) Based on our analysis, we show that VideoLLMs can retain their VideoQA performance by selecting these effective information pathways while suppressing a substantial amount of attention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a blueprint on how VideoLLMs perform temporal reasoning and offer practical insights for improving model interpretability and downstream generalization. Our project page with the source code is available at https://map-the-flow.github.io', 'score': 9, 'issue_id': 6623, 'pub_date': '2025-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': 'acfa1fde05b861ca', 'authors': ['Minji Kim', 'Taekyung Kim', 'Bohyung Han'], 'affiliations': ['ECE & IPAI, Seoul National University', 'NAVER AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2510.13251.jpg', 'data': {'categories': ['#alignment', '#video', '#reasoning', '#interpretability', '#training', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Как видео-LLM понимают время: карта потоков информации', 'desc': 'Исследователи изучили внутренние механизмы работы видео-LLM (больших языковых моделей для видео) при ответах на вопросы о видеороликах. Оказалось, что модели работают в три этапа: сначала анализируют связи между кадрами в ранних слоях, затем объединяют видео и текстовую информацию в средних слоях, и наконец генерируют ответы в поздних слоях. Важное открытие: модели могут сохранять качество работы, используя только 42% связей внимания и отбрасывая остальные как ненужные. Это помогает понять, как AI рассуждает о временных событиях в видео, и открывает путь к более эффективным моделям.'}, 'en': {'title': 'Unlocking the Secrets of Video Question Answering with VideoLLMs', 'desc': 'This paper explores how Video Large Language Models (VideoLLMs) handle video question answering by analyzing their internal information flow. It reveals that these models use cross-frame interactions for temporal reasoning, integrating video and language representations effectively. The study shows that VideoLLMs can maintain performance by focusing on key information pathways while ignoring unnecessary attention connections. These insights enhance our understanding of VideoLLMs and suggest ways to improve their interpretability and generalization in various tasks.'}, 'zh': {'title': '视频问答的新视角：时间推理与信息流动', 'desc': '视频大型语言模型（VideoLLMs）通过跨帧交互启动时间推理，进而进行视频问答。研究表明，VideoLLMs在处理视频和文本信息时，采用了有效的信息流动路径，并抑制了不必要的注意力边缘。分析显示，时间推理在模型的早中层通过活跃的跨帧交互开始，随后在中层进行视频与语言的逐步整合。最终，模型在中后层生成正确答案，保持了良好的视频问答性能。'}}}, {'id': 'https://huggingface.co/papers/2510.21553', 'title': 'Document Understanding, Measurement, and Manipulation Using Category\n  Theory', 'url': 'https://huggingface.co/papers/2510.21553', 'abstract': 'Category theory is used to develop information-theoretic measures, summarization, and self-supervised improvement of large pretrained models through a mathematical framework of question-answer pairs and orthogonalization.  \t\t\t\t\tAI-generated summary \t\t\t\t We apply category theory to extract multimodal document structure which leads us to develop information theoretic measures, content summarization and extension, and self-supervised improvement of large pretrained models. We first develop a mathematical representation of a document as a category of question-answer pairs. Second, we develop an orthogonalization procedure to divide the information contained in one or more documents into non-overlapping pieces. The structures extracted in the first and second steps lead us to develop methods to measure and enumerate the information contained in a document. We also build on those steps to develop new summarization techniques, as well as to develop a solution to a new problem viz. exegesis resulting in an extension of the original document. Our question-answer pair methodology enables a novel rate distortion analysis of summarization techniques. We implement our techniques using large pretrained models, and we propose a multimodal extension of our overall mathematical framework. Finally, we develop a novel self-supervised method using RLVR to improve large pretrained models using consistency constraints such as composability and closure under certain operations that stem naturally from our category theoretic framework.', 'score': 3, 'issue_id': 6621, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'aa27a9cf9e256732', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#interpretability', '#training', '#rl', '#multimodal', '#survey', '#data', '#optimization', '#math'], 'emoji': '🔷', 'ru': {'title': 'Теория категорий для структурирования и улучшения больших языковых моделей', 'desc': 'Исследователи применили теорию категорий для анализа структуры документов через пары вопрос-ответ. Они разработали метод ортогонализации для разделения информации на непересекающиеся части и создали информационно-теоретические меры для оценки содержания документов. На основе этого подхода предложены новые методы суммаризации и расширения текстов, а также выполнен rate-distortion анализ. Авторы также представили self-supervised метод с использованием RLVR для улучшения LLM через ограничения согласованности, вытекающие из категорно-теоретического фреймворка.'}, 'en': {'title': 'Harnessing Category Theory for Enhanced Document Understanding and Model Improvement', 'desc': 'This paper uses category theory to create new ways to measure and summarize information in documents. It represents documents as categories of question-answer pairs, allowing for a structured analysis of their content. The authors introduce an orthogonalization method to separate information into distinct parts, which aids in developing effective summarization techniques. Additionally, they propose a self-supervised learning approach to enhance large pretrained models by applying consistency constraints derived from their mathematical framework.'}, 'zh': {'title': '利用范畴理论提升预训练模型的摘要与扩展能力', 'desc': '本文利用范畴理论来提取多模态文档结构，从而开发信息理论度量、内容摘要和扩展，以及大型预训练模型的自监督改进。我们首先将文档数学表示为问题-答案对的范畴。接着，我们开发了正交化程序，将一个或多个文档中的信息划分为不重叠的部分。这些结构的提取使我们能够测量和枚举文档中包含的信息，并提出新的摘要技术和文档扩展的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2510.20535', 'title': 'ARC-Encoder: learning compressed text representations for large language\n  models', 'url': 'https://huggingface.co/papers/2510.20535', 'abstract': 'An ARC-Encoder compresses context into continuous representations for LLMs, improving inference efficiency and performance across various scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent techniques such as retrieval-augmented generation or chain-of-thought reasoning have led to longer contexts and increased inference costs. Context compression techniques can reduce these costs, but the most effective approaches require fine-tuning the target model or even modifying its architecture. This can degrade its general abilities when not used for this specific purpose. Here we explore an alternative approach: an encoder that compresses the context into continuous representations which replace token embeddings in decoder LLMs. First, we perform a systematic study of training strategies and architecture choices for the encoder. Our findings led to the design of an Adaptable text Representations Compressor, named ARC-Encoder, which outputs x-times fewer continuous representations (typically x!in!{4,8}) than text tokens. We evaluate ARC-Encoder across a variety of LLM usage scenarios, ranging from in-context learning to context window extension, on both instruct and base decoders. Results show that ARC-Encoder achieves state-of-the-art performance on several benchmarks while improving computational efficiency at inference. Finally, we demonstrate that our models can be adapted to multiple decoders simultaneously, allowing a single encoder to generalize across different decoder LLMs. This makes ARC-Encoder a flexible and efficient solution for portable encoders that work seamlessly with multiple LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder , fine-tuning dataset and pretrained models are available at https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .', 'score': 3, 'issue_id': 6621, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '38daae856d0502b4', 'authors': ['Hippolyte Pilchen', 'Edouard Grave', 'Patrick Pérez'], 'affiliations': ['Kyutai, Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2510.20535.jpg', 'data': {'categories': ['#training', '#open_source', '#dataset', '#long_context', '#architecture', '#optimization', '#inference'], 'emoji': '🗜️', 'ru': {'title': 'Сжатие контекста без потери качества для эффективного inference', 'desc': 'Статья представляет ARC-Encoder — энкодер, который сжимает контекст в непрерывные представления для decoder LLM, уменьшая количество токенов в 4-8 раз. Это решение повышает эффективность inference без необходимости fine-tuning целевой модели или изменения её архитектуры, что сохраняет общие способности LLM. ARC-Encoder показывает state-of-the-art результаты в различных сценариях использования — от in-context learning до расширения контекстного окна. Ключевое преимущество — один энкодер может работать одновременно с несколькими разными decoder LLM, обеспечивая гибкость и портативность решения.'}, 'en': {'title': 'Efficient Context Compression for Enhanced LLM Performance', 'desc': 'The ARC-Encoder is a novel approach that compresses context into continuous representations for large language models (LLMs), enhancing both inference efficiency and performance. Unlike traditional methods that require fine-tuning or architectural changes, ARC-Encoder replaces token embeddings with fewer continuous representations, typically reducing the number of outputs by a factor of 4 to 8. This method has been systematically studied to optimize training strategies and architecture, resulting in state-of-the-art performance across various benchmarks. Additionally, ARC-Encoder is adaptable, allowing it to work with multiple decoder LLMs, making it a versatile solution for improving computational efficiency in diverse applications.'}, 'zh': {'title': 'ARC-Encoder：高效的上下文压缩解决方案', 'desc': 'ARC-Encoder是一种将上下文压缩为连续表示的编码器，旨在提高大语言模型（LLM）的推理效率和性能。与传统的上下文压缩技术不同，ARC-Encoder不需要对目标模型进行微调或架构修改，从而避免了对模型通用能力的影响。通过系统研究训练策略和架构选择，ARC-Encoder能够输出比文本标记少x倍的连续表示，显著提高计算效率。我们的实验表明，ARC-Encoder在多种使用场景下表现出色，能够灵活适应不同的解码器，成为一种高效的可移植编码器解决方案。'}}}, {'id': 'https://huggingface.co/papers/2510.21652', 'title': 'AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research\n  Suite', 'url': 'https://huggingface.co/papers/2510.21652', 'abstract': 'AstaBench provides a comprehensive benchmark suite for evaluating AI agents in scientific research, revealing that while progress has been made, AI still falls short in fully assisting scientific research.  \t\t\t\t\tAI-generated summary \t\t\t\t AI agents hold the potential to revolutionize scientific productivity by automating literature reviews, replicating experiments, analyzing data, and even proposing new directions of inquiry; indeed, there are now many such agents, ranging from general-purpose "deep research" systems to specialized science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of these agents is critical for progress. Yet existing benchmarks fall short on several fronts: they (1) fail to provide holistic, product-informed measures of real-world use cases such as science research; (2) lack reproducible agent tools necessary for a controlled comparison of core agentic capabilities; (3) do not account for confounding variables such as model cost and tool access; (4) do not provide standardized interfaces for quick agent prototyping and evaluation; and (5) lack comprehensive baseline agents necessary to identify true advances. In response, we define principles and tooling for more rigorously benchmarking agents. Using these, we present AstaBench, a suite that provides the first holistic measure of agentic ability to perform scientific research, comprising 2400+ problems spanning the entire scientific discovery process and multiple scientific domains, and including many problems inspired by actual user requests to deployed Asta agents. Our suite comes with the first scientific research environment with production-grade search tools that enable controlled, reproducible evaluation, better accounting for confounders. Alongside, we provide a comprehensive suite of nine science-optimized classes of Asta agents and numerous baselines. Our extensive evaluation of 57 agents across 22 agent classes reveals several interesting findings, most importantly that despite meaningful progress on certain individual aspects, AI remains far from solving the challenge of science research assistance.', 'score': 2, 'issue_id': 6621, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'e8b1cd801c13a100', 'authors': ['Jonathan Bragg', "Mike D'Arcy", 'Nishant Balepur', 'Dan Bareket', 'Bhavana Dalvi', 'Sergey Feldman', 'Dany Haddad', 'Jena D. Hwang', 'Peter Jansen', 'Varsha Kishore', 'Bodhisattwa Prasad Majumder', 'Aakanksha Naik', 'Sigal Rahamimov', 'Kyle Richardson', 'Amanpreet Singh', 'Harshit Surana', 'Aryeh Tiktinsky', 'Rosni Vasu', 'Guy Wiener', 'Chloe Anastasiades', 'Stefan Candra', 'Jason Dunkelberger', 'Dan Emery', 'Rob Evans', 'Malachi Hamada', 'Regan Huff', 'Rodney Kinney', 'Matt Latzke', 'Jaron Lochner', 'Ruben Lozano-Aguilera', 'Cecile Nguyen', 'Smita Rao', 'Amber Tanaka', 'Brooke Vlahos', 'Peter Clark', 'Doug Downey', 'Yoav Goldberg', 'Ashish Sabharwal', 'Daniel S. Weld'], 'affiliations': ['Allen Institute for AI', 'Bar-Ilan University', 'University of Arizona', 'University of Maryland', 'University of Washington', 'University of Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2510.21652.jpg', 'data': {'categories': ['#benchmark', '#agents', '#science'], 'emoji': '🔬', 'ru': {'title': 'AI в науке: ещё не замена учёным', 'desc': 'AstaBench представляет собой набор тестов для оценки возможностей AI агентов в научных исследованиях. Он выявляет, что, несмотря на прогресс, AI пока не может полностью заменить учёных. AstaBench включает более 2400 задач, охватывающих весь процесс научного открытия. Это позволяет более точно оценивать способности AI в научной среде.'}, 'en': {'title': "AstaBench: Elevating AI's Role in Scientific Research Evaluation", 'desc': 'AstaBench is a new benchmark suite designed to evaluate AI agents specifically in the context of scientific research. It addresses the shortcomings of existing benchmarks by providing a comprehensive set of over 2400 problems that reflect real-world scientific tasks. The suite includes tools for reproducible evaluations and accounts for various confounding factors, ensuring a more accurate assessment of AI capabilities. Despite advancements in AI, the evaluation shows that these agents still struggle to fully assist in scientific research, highlighting the need for further development.'}, 'zh': {'title': 'AstaBench：科学研究中的 AI 代理评估新标准', 'desc': 'AstaBench 是一个全面的基准测试套件，用于评估 AI 代理在科学研究中的表现。尽管 AI 在某些方面取得了进展，但仍未能完全支持科学研究。该套件提供了2400多个问题，涵盖整个科学发现过程，并包括多个科学领域的问题。通过严格的评估，AstaBench 旨在为科学研究提供更有效的 AI 代理评估工具。'}}}, {'id': 'https://huggingface.co/papers/2510.21447', 'title': 'PhysWorld: From Real Videos to World Models of Deformable Objects via\n  Physics-Aware Demonstration Synthesis', 'url': 'https://huggingface.co/papers/2510.21447', 'abstract': 'PhysWorld uses a simulator to generate diverse demonstrations for training a GNN-based world model, enabling accurate and fast predictions for deformable objects with competitive performance and faster inference speeds.  \t\t\t\t\tAI-generated summary \t\t\t\t Interactive world models that simulate object dynamics are crucial for robotics, VR, and AR. However, it remains a significant challenge to learn physics-consistent dynamics models from limited real-world video data, especially for deformable objects with spatially-varying physical properties. To overcome the challenge of data scarcity, we propose PhysWorld, a novel framework that utilizes a simulator to synthesize physically plausible and diverse demonstrations to learn efficient world models. Specifically, we first construct a physics-consistent digital twin within MPM simulator via constitutive model selection and global-to-local optimization of physical properties. Subsequently, we apply part-aware perturbations to the physical properties and generate various motion patterns for the digital twin, synthesizing extensive and diverse demonstrations. Finally, using these demonstrations, we train a lightweight GNN-based world model that is embedded with physical properties. The real video can be used to further refine the physical properties. PhysWorld achieves accurate and fast future predictions for various deformable objects, and also generalizes well to novel interactions. Experiments show that PhysWorld has competitive performance while enabling inference speeds 47 times faster than the recent state-of-the-art method, i.e., PhysTwin.', 'score': 2, 'issue_id': 6621, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'aee1bcbeeb33e55c', 'authors': ['Yu Yang', 'Zhilu Zhang', 'Xiang Zhang', 'Yihan Zeng', 'Hui Li', 'Wangmeng Zuo'], 'affiliations': ['Harbin Institute of Technology', 'Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2510.21447.jpg', 'data': {'categories': ['#dataset', '#agents', '#robotics', '#synthetic', '#graphs', '#inference'], 'emoji': '🌊', 'ru': {'title': 'Физический мир: симуляция для обучения моделей деформируемых объектов', 'desc': 'PhysWorld — это framework для создания интерактивных world models, которые симулируют динамику деформируемых объектов. Система использует MPM-симулятор для генерации разнообразных демонстраций с физически правдоподобными свойствами материалов, которые затем используются для обучения легковесной GNN-модели. Подход решает проблему нехватки реальных видеоданных путём создания цифрового двойника объекта с последующей генерацией вариаций его физических свойств. В результате модель обеспечивает точные предсказания будущих состояний объектов со скоростью inference в 47 раз быстрее, чем state-of-the-art метод PhysTwin.'}, 'en': {'title': 'Revolutionizing Predictions for Deformable Objects with PhysWorld', 'desc': 'PhysWorld is a framework designed to improve the training of Graph Neural Network (GNN)-based world models for predicting the behavior of deformable objects. It addresses the challenge of limited real-world data by using a simulator to create diverse and physically plausible demonstrations. By constructing a digital twin and applying part-aware perturbations, PhysWorld generates a wide range of motion patterns that enhance the learning process. The resulting model not only achieves accurate predictions but also operates significantly faster than previous methods, making it highly effective for applications in robotics and virtual environments.'}, 'zh': {'title': 'PhysWorld：快速准确的可变形物体预测', 'desc': 'PhysWorld是一个新颖的框架，利用模拟器生成多样化的演示数据，以训练基于图神经网络的世界模型。该模型能够准确快速地预测可变形物体的动态，尤其是在数据稀缺的情况下。通过构建物理一致的数字双胞胎，并对物理属性进行局部优化，PhysWorld能够合成丰富的运动模式。实验表明，PhysWorld在性能上具有竞争力，并且推理速度比最新的PhysTwin方法快47倍。'}}}, {'id': 'https://huggingface.co/papers/2510.20780', 'title': 'Are Large Reasoning Models Good Translation Evaluators? Analysis and\n  Performance Boost', 'url': 'https://huggingface.co/papers/2510.20780', 'abstract': 'Calibrating large reasoning models with synthetic human-like thinking trajectories improves their efficiency and performance in machine translation evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large reasoning models (LRMs) have introduced an intermediate "thinking" process prior to generating final answers, improving their reasoning capabilities on complex downstream tasks. However, the potential of LRMs as evaluators for machine translation (MT) quality remains underexplored. We provides the first systematic analysis of LRM-as-a-judge in MT evaluation. We identify key challenges, revealing LRMs require tailored evaluation materials, tend to "overthink" simpler instances and have issues with scoring mechanisms leading to overestimation. To address these, we propose to calibrate LRM thinking by training them on synthetic, human-like thinking trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this approach largely reduces thinking budgets by ~35x while concurrently improving evaluation performance across different LRM scales from 7B to 32B (e.g., R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These findings highlight the potential of efficiently calibrated LRMs to advance fine-grained automatic MT evaluation.', 'score': 2, 'issue_id': 6623, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '0e1d847c372418a9', 'authors': ['Runzhe Zhan', 'Zhihong Huang', 'Xinyi Yang', 'Lidia S. Chao', 'Min Yang', 'Derek F. Wong'], 'affiliations': ['NLP2CT Lab, Department of Computer and Information Science, University of Macau', 'Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2510.20780.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#synthetic', '#training', '#machine_translation', '#multimodal'], 'emoji': '🎯', 'ru': {'title': 'Калибровка моделей рассуждения для оценки машинного перевода', 'desc': 'Исследователи изучили применение больших моделей рассуждения (LRM) для оценки качества машинного перевода и обнаружили ряд проблем: модели склонны «переусложнять» простые примеры и завышать оценки. Для решения этих проблем предложен метод калибровки LRM путём обучения на синтетических траекториях рассуждений, имитирующих человеческое мышление. Эксперименты на бенчмарках WMT24 показали, что подход сокращает вычислительные затраты на рассуждение в 35 раз при одновременном улучшении качества оценки для моделей размером от 7B до 32B параметров. Например, модель R1-Distill-Qwen-7B показала улучшение корреляции на 8.7 процентных пунктов, что демонстрирует потенциал калиброванных LRM для автоматической оценки машинного перевода.'}, 'en': {'title': 'Calibrating LRMs for Smarter Machine Translation Evaluation', 'desc': 'This paper explores how calibrating large reasoning models (LRMs) with synthetic human-like thinking processes can enhance their efficiency and performance in evaluating machine translation (MT). The authors identify challenges faced by LRMs, such as the tendency to overthink simpler tasks and issues with scoring accuracy. By training LRMs on tailored evaluation materials that mimic human reasoning, they demonstrate a significant reduction in the computational resources needed for evaluation while improving the correlation of MT quality assessments. The results indicate that well-calibrated LRMs can effectively contribute to more nuanced and accurate automatic MT evaluation.'}, 'zh': {'title': '校准推理模型，提升机器翻译评估效率', 'desc': '本文探讨了如何通过合成的人类思维轨迹来校准大型推理模型（LRMs），以提高其在机器翻译评估中的效率和性能。研究发现，LRMs在评估机器翻译质量时面临一些挑战，包括需要定制的评估材料、对简单实例的过度思考以及评分机制的问题。通过在合成的思维轨迹上进行训练，本文提出了一种校准LRM思维的方法，实验结果表明这种方法可以将思维预算减少约35倍，同时提高评估性能。研究结果强调了高效校准的LRMs在细粒度自动机器翻译评估中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2510.17234', 'title': 'Taming Modality Entanglement in Continual Audio-Visual Segmentation', 'url': 'https://huggingface.co/papers/2510.17234', 'abstract': 'A novel Continual Audio-Visual Segmentation (CAVS) task addresses challenges in multi-modal continual learning through a Collision-based Multi-modal Rehearsal (CMR) framework, improving performance over single-modal methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, significant progress has been made in multi-modal continual learning, aiming to learn new tasks sequentially in multi-modal settings while preserving performance on previously learned ones. However, existing methods mainly focus on coarse-grained tasks, with limitations in addressing modality entanglement in fine-grained continual learning settings. To bridge this gap, we introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to continuously segment new classes guided by audio. Through comprehensive analysis, two critical challenges are identified: 1) multi-modal semantic drift, where a sounding objects is labeled as background in sequential tasks; 2) co-occurrence confusion, where frequent co-occurring classes tend to be confused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework is designed to address these challenges. Specifically, for multi-modal semantic drift, a Multi-modal Sample Selection (MSS) strategy is proposed to select samples with high modal consistency for rehearsal. Meanwhile, for co-occurence confusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed, allowing for the increase of rehearsal sample frequency of those confusable classes during training process. Moreover, we construct three audio-visual incremental scenarios to verify effectiveness of our method. Comprehensive experiments demonstrate that our method significantly outperforms single-modal continual learning methods.', 'score': 2, 'issue_id': 6622, 'pub_date': '2025-10-20', 'pub_date_card': {'ru': '20 октября', 'en': 'October 20', 'zh': '10月20日'}, 'hash': '7a6c5466d3f58bd9', 'authors': ['Yuyang Hong', 'Qi Yang', 'Tao Zhang', 'Zili Wang', 'Zhaojin Fu', 'Kun Ding', 'Bin Fan', 'Shiming Xiang'], 'affiliations': ['School of Intelligent Science and Technology, University of Science and Technology Beijing, Beijing 100083, China', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China, and also with the School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China'], 'pdf_title_img': 'assets/pdf/title_img/2510.17234.jpg', 'data': {'categories': ['#training', '#multimodal'], 'emoji': '🔊', 'ru': {'title': 'Непрерывное обучение с аудио и видео без забывания', 'desc': 'Статья представляет новую задачу непрерывной аудио-визуальной сегментации (CAVS), где модель учится последовательно сегментировать новые классы объектов под управлением звука, не забывая предыдущие. Авторы выявили две ключевые проблемы: мультимодальный семантический дрейф, когда звучащие объекты становятся фоном, и путаницу часто встречающихся вместе классов. Для решения предложен фреймворк CMR с умным выбором примеров для повторения на основе модальной согласованности и частоты столкновений классов. Эксперименты показывают значительное превосходство над одномодальными методами continual learning.'}, 'en': {'title': 'Enhancing Multi-Modal Learning with Collision-based Rehearsal', 'desc': 'This paper introduces a new task called Continual Audio-Visual Segmentation (CAVS) that focuses on learning from both audio and visual data over time. It addresses the challenges of multi-modal continual learning, particularly the issues of multi-modal semantic drift and co-occurrence confusion. To tackle these challenges, the authors propose a Collision-based Multi-modal Rehearsal (CMR) framework, which includes strategies for selecting samples that maintain modal consistency and increasing the frequency of rehearsal for confusing classes. Experimental results show that this approach significantly improves performance compared to traditional single-modal methods.'}, 'zh': {'title': '持续音视频分割：多模态学习的新突破', 'desc': '本文提出了一种新的持续音视频分割（CAVS）任务，旨在解决多模态持续学习中的挑战。通过引入基于碰撞的多模态重演（CMR）框架，本文提高了在新任务学习时对已学任务性能的保持。研究中识别了两个关键挑战：多模态语义漂移和共现混淆，并提出了相应的解决策略。实验结果表明，所提方法在性能上显著优于单模态持续学习方法。'}}}, {'id': 'https://huggingface.co/papers/2510.21581', 'title': 'Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video', 'url': 'https://huggingface.co/papers/2510.21581', 'abstract': "Foley Control uses a small cross-attention bridge to synchronize video and audio without retraining, achieving competitive alignment with fewer parameters and maintaining modularity.  \t\t\t\t\tAI-generated summary \t\t\t\t Foley Control is a lightweight approach to video-guided Foley that keeps pretrained single-modality models frozen and learns only a small cross-attention bridge between them. We connect V-JEPA2 video embeddings to a frozen Stable Audio Open DiT text-to-audio (T2A) model by inserting compact video cross-attention after the model's existing text cross-attention, so prompts set global semantics while video refines timing and local dynamics. The frozen backbones retain strong marginals (video; audio given text) and the bridge learns the audio-video dependency needed for synchronization -- without retraining the audio prior. To cut memory and stabilize training, we pool video tokens before conditioning. On curated video-audio benchmarks, Foley Control delivers competitive temporal and semantic alignment with far fewer trainable parameters than recent multi-modal systems, while preserving prompt-driven controllability and production-friendly modularity (swap/upgrade encoders or the T2A backbone without end-to-end retraining). Although we focus on Video-to-Foley, the same bridge design can potentially extend to other audio modalities (e.g., speech).", 'score': 1, 'issue_id': 6630, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '5e57c85ed10db64d', 'authors': ['Ciara Rowles', 'Varun Jampani', 'Simon Donné', 'Shimon Vainer', 'Julian Parker', 'Zach Evans'], 'affiliations': ['Meta'], 'pdf_title_img': 'assets/pdf/title_img/2510.21581.jpg', 'data': {'categories': ['#optimization', '#video', '#small_models', '#alignment', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Модульный мост между видео и звуком без переобучения моделей', 'desc': 'Foley Control — это легковесный метод для генерации звукового сопровождения видео, который использует замороженные предобученные модели и обучает лишь небольшой мост на основе cross-attention между ними. Система соединяет video embeddings из V-JEPA2 с замороженной audio diffusion моделью Stable Audio Open через компактный слой cross-attention, где текстовые промпты задают общую семантику, а видео уточняет тайминг и локальную динамику. Благодаря заморозке базовых моделей и обучению только моста, метод достигает конкурентной синхронизации аудио и видео с гораздо меньшим количеством обучаемых параметров, чем мультимодальные системы. Модульная архитектура позволяет легко заменять или обновлять компоненты без полного переобучения всей системы.'}, 'en': {'title': 'Efficient Video-Audio Synchronization with Minimal Parameters', 'desc': 'Foley Control is a novel method that synchronizes video and audio using a small cross-attention bridge, allowing for effective alignment without the need for retraining existing models. It leverages pretrained single-modality models, keeping them frozen while learning only the necessary connections for audio-video synchronization. By integrating video embeddings with a text-to-audio model, it ensures that video enhances the timing and dynamics of audio based on global semantic prompts. This approach not only reduces the number of trainable parameters but also maintains modularity, enabling easy upgrades and swaps of components without full retraining.'}, 'zh': {'title': '轻量级视频音频同步的新方法', 'desc': 'Foley Control是一种轻量级的视频引导Foley方法，它通过小型交叉注意力桥接来同步视频和音频，而无需重新训练。该方法保持了预训练的单模态模型不变，仅学习视频和音频之间的依赖关系。通过在现有文本交叉注意力后插入紧凑的视频交叉注意力，Foley Control能够在全局语义的基础上，利用视频来细化时序和局部动态。该方法在视频-音频基准测试中表现出竞争力的时间和语义对齐，同时参数量远低于最近的多模态系统。'}}}, {'id': 'https://huggingface.co/papers/2510.21057', 'title': 'Soft Instruction De-escalation Defense', 'url': 'https://huggingface.co/papers/2510.21057', 'abstract': 'SIC, an iterative prompt sanitization method, enhances the security of tool-augmented LLM agents by repeatedly inspecting and cleaning incoming data to prevent prompt injection attacks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an external environment; this makes them susceptible to prompt injections when dealing with untrusted data. To overcome this limitation, we propose SIC (Soft Instruction Control)-a simple yet effective iterative prompt sanitization loop designed for tool-augmented LLM agents. Our method repeatedly inspects incoming data for instructions that could compromise agent behavior. If such content is found, the malicious content is rewritten, masked, or removed, and the result is re-evaluated. The process continues until the input is clean or a maximum iteration limit is reached; if imperative instruction-like content remains, the agent halts to ensure security. By allowing multiple passes, our approach acknowledges that individual rewrites may fail but enables the system to catch and correct missed injections in later steps. Although immediately useful, worst-case analysis shows that SIC is not infallible; strong adversary can still get a 15% ASR by embedding non-imperative workflows. This nonetheless raises the bar.', 'score': 1, 'issue_id': 6630, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'e29c4154928b821d', 'authors': ['Nils Philipp Walter', 'Chawin Sitawarin', 'Jamie Hayes', 'David Stutz', 'Ilia Shumailov'], 'affiliations': ['AI Sequrity Company', 'CISPA Helmholtz Center for Information Security', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2510.21057.jpg', 'data': {'categories': ['#agents', '#security', '#data', '#inference'], 'emoji': '🛡️', 'ru': {'title': 'Итеративная очистка промптов для защиты LLM-агентов', 'desc': 'Статья представляет метод SIC (Soft Instruction Control) для защиты LLM-агентов от prompt injection атак при работе с внешними данными. Метод использует итеративный цикл санитизации, который многократно проверяет входящие данные на наличие вредоносных инструкций и переписывает, маскирует или удаляет их. Если после нескольких итераций подозрительный контент остаётся, агент останавливает работу для обеспечения безопасности. Несмотря на эффективность, метод не является полностью защищённым - продвинутые атаки с использованием non-imperative workflows всё ещё достигают 15% успеха.'}, 'en': {'title': 'SIC: Securing LLM Agents Against Prompt Injections', 'desc': "The paper introduces SIC, an iterative prompt sanitization method aimed at improving the security of tool-augmented Large Language Model (LLM) agents against prompt injection attacks. SIC works by continuously inspecting and cleaning incoming data to identify and neutralize potentially harmful instructions that could alter the agent's behavior. The method allows for multiple iterations of data evaluation, ensuring that any missed malicious content can be addressed in subsequent passes. While SIC significantly enhances security, it acknowledges that determined adversaries may still find ways to exploit the system, achieving a 15% attack success rate under certain conditions."}, 'zh': {'title': '提升LLM代理安全性的迭代清理方法', 'desc': 'SIC（软指令控制）是一种迭代的提示清理方法，旨在增强工具增强型大语言模型（LLM）代理的安全性。该方法通过反复检查和清理输入数据，防止提示注入攻击，从而保护代理的行为。SIC会识别并处理可能危害代理行为的恶意内容，确保输入数据的安全性。尽管在最坏情况下仍存在一定的风险，但SIC显著提高了系统的安全标准。'}}}, {'id': 'https://huggingface.co/papers/2510.20708', 'title': 'ALICE-LRI: A General Method for Lossless Range Image Generation for\n  Spinning LiDAR Sensors without Calibration Metadata', 'url': 'https://huggingface.co/papers/2510.20708', 'abstract': 'ALICE-LRI is a sensor-agnostic method that achieves lossless range image generation from spinning LiDAR point clouds, preserving all points and maintaining geometric accuracy in real-time.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D LiDAR sensors are essential for autonomous navigation, environmental monitoring, and precision mapping in remote sensing applications. To efficiently process the massive point clouds generated by these sensors, LiDAR data is often projected into 2D range images that organize points by their angular positions and distances. While these range image representations enable efficient processing, conventional projection methods suffer from fundamental geometric inconsistencies that cause irreversible information loss, compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDAR Intrinsic Calibration Estimation for Lossless Range Images), the first general, sensor-agnostic method that achieves lossless range image generation from spinning LiDAR point clouds without requiring manufacturer metadata or calibration files. Our algorithm automatically reverse-engineers the intrinsic geometry of any spinning LiDAR sensor by inferring critical parameters including laser beam configuration, angular distributions, and per-beam calibration corrections, enabling lossless projection and complete point cloud reconstruction with zero point loss. Comprehensive evaluation across the complete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfect point preservation, with zero points lost across all point clouds. Geometric accuracy is maintained well within sensor precision limits, establishing geometric losslessness with real-time performance. We also present a compression case study that validates substantial downstream benefits, demonstrating significant quality improvements in practical applications. This paradigm shift from approximate to lossless LiDAR projections opens new possibilities for high-precision remote sensing applications requiring complete geometric preservation.', 'score': 1, 'issue_id': 6632, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '2267826ebc8fe420', 'authors': ['Samuel Soutullo', 'Miguel Yermo', 'David L. Vilariño', 'Óscar G. Lorenzo', 'José C. Cabaleiro', 'Francisco F. Rivera'], 'affiliations': ['Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS), Rúa de Jenaro de la Fuente Domínguez, Santiago de Compostela, 15782, Coruña, Spain', 'Departamento de Electrónica Computación, Universidade de Santiago de Compostela, Rúa Lope Gómez de Marzoa, Santiago de Compostela, 15782, Coruña, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2510.20708.jpg', 'data': {'categories': ['#3d', '#dataset'], 'emoji': '📡', 'ru': {'title': 'Идеальная проекция LiDAR без потери информации', 'desc': 'ALICE-LRI — это метод, который создаёт 2D range-изображения из облаков точек вращающихся LiDAR-сенсоров без потери данных. Алгоритм автоматически определяет внутренние параметры любого LiDAR-датчика, включая конфигурацию лазерных лучей и калибровочные поправки, без использования метаданных производителя. Тестирование на датасетах KITTI и DurLAR показало нулевую потерю точек и сохранение геометрической точности в пределах погрешности сенсора. Метод работает в реальном времени и открывает новые возможности для высокоточных приложений в области дистанционного зондирования и автономной навигации.'}, 'en': {'title': 'Achieving Lossless LiDAR Projections for Precision Mapping', 'desc': 'ALICE-LRI is a novel method designed to generate lossless range images from spinning LiDAR point clouds, ensuring that no data points are lost during the process. This approach is sensor-agnostic, meaning it can work with any spinning LiDAR sensor without needing specific calibration data. By automatically determining the intrinsic parameters of the LiDAR sensor, ALICE-LRI maintains high geometric accuracy and allows for real-time processing of point clouds. The method has been validated on extensive datasets, showing perfect point preservation and significant improvements in the quality of downstream applications.'}, 'zh': {'title': '无损LiDAR范围图像生成的革命性方法', 'desc': 'ALICE-LRI是一种传感器无关的方法，可以从旋转的LiDAR点云中生成无损的范围图像，实时保持所有点和几何精度。该方法通过推断激光束配置、角度分布和每束激光的校正等关键参数，自动逆向工程任何旋转LiDAR传感器的内在几何特性。ALICE-LRI在KITTI和DurLAR数据集上的全面评估表明，所有点云中没有点丢失，几何精度保持在传感器精度范围内。该方法的无损投影为高精度遥感应用开辟了新的可能性，确保了完整的几何保留。'}}}, {'id': 'https://huggingface.co/papers/2510.11370', 'title': 'Stabilizing MoE Reinforcement Learning by Aligning Training and\n  Inference Routers', 'url': 'https://huggingface.co/papers/2510.11370', 'abstract': 'Rollout Routing Replay (R3) stabilizes reinforcement learning training in Mixture-of-Experts models by reducing discrepancies between training and inference routing behaviors.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has emerged as a crucial approach for enhancing the capabilities of large language models. However, in Mixture-of-Experts (MoE) models, the routing mechanism often introduces instability, even leading to catastrophic RL training collapse. We analyze the training-inference consistency of MoE models and identify a notable discrepancy in routing behaviors between the two phases. Moreover, even under identical conditions, the routing framework can yield divergent expert selections across repeated forward passes. To address this foundational inconsistency, we propose Rollout Routing Replay (R3), a method that records routing distributions from the inference engine and replays them during training. R3 significantly reduces training-inference policy KL divergence and mitigates extreme discrepancies without compromising training speed. Extensive experiments on various settings confirm that R3 succeeds in stabilizing RL training, preventing collapse and outperforming methods such as GSPO and TIS. We believe this work can offer a new solution for stabilizing RL in MoE models.', 'score': 1, 'issue_id': 6630, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': 'bffa313f30aff879', 'authors': ['Wenhan Ma', 'Hailin Zhang', 'Liang Zhao', 'Yifan Song', 'Yudong Wang', 'Zhifang Sui', 'Fuli Luo'], 'affiliations': ['LLM-Core Xiaomi', 'State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2510.11370.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#reasoning'], 'emoji': '🔄', 'ru': {'title': 'Стабилизация обучения MoE через воспроизведение роутинга', 'desc': 'Статья представляет метод Rollout Routing Replay (R3) для стабилизации обучения с подкреплением в Mixture-of-Experts моделях. Проблема заключается в несогласованности механизма роутинга между фазами обучения и инференса, что может приводить к катастрофическому коллапсу тренировки. R3 записывает распределения роутинга во время инференса и воспроизводит их при обучении, существенно снижая расхождение KL-дивергенции между политиками. Эксперименты подтверждают, что метод успешно стабилизирует RL-тренировку и превосходит альтернативные подходы типа GSPO и TIS.'}, 'en': {'title': 'Stabilizing RL Training in Mixture-of-Experts with R3', 'desc': 'The paper introduces Rollout Routing Replay (R3), a method designed to stabilize reinforcement learning (RL) training in Mixture-of-Experts (MoE) models. It addresses the issue of inconsistencies between the routing behaviors during training and inference, which can lead to training failures. By recording and replaying routing distributions from the inference phase during training, R3 minimizes the KL divergence between training and inference policies. The results show that R3 effectively prevents training collapse and outperforms existing methods, providing a promising solution for enhancing RL in MoE architectures.'}, 'zh': {'title': '稳定强化学习训练的新方法', 'desc': '本论文提出了一种名为Rollout Routing Replay（R3）的方法，旨在稳定混合专家模型中的强化学习训练。通过减少训练和推理阶段之间的路由行为差异，R3有效地解决了在强化学习过程中可能出现的不稳定性问题。我们分析了混合专家模型的训练与推理一致性，并发现两者之间存在显著的路由行为差异。实验结果表明，R3能够显著降低训练与推理策略之间的KL散度，从而防止训练崩溃，并在多种设置中优于其他方法。'}}}, {'id': 'https://huggingface.co/papers/2510.21440', 'title': 'Redefining Retrieval Evaluation in the Era of LLMs', 'url': 'https://huggingface.co/papers/2510.21440', 'abstract': 'A new metric, UDCG, is introduced to better evaluate Retrieval Augmented Generation systems by accounting for both the utility of relevant documents and the distraction of irrelevant ones, improving correlation with end-to-end answer accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR, assume that human users sequentially examine documents with diminishing attention to lower ranks. This assumption breaks down in Retrieval Augmented Generation (RAG) systems, where search results are consumed by Large Language Models (LLMs), which, unlike humans, process all retrieved documents as a whole rather than sequentially. Additionally, traditional IR metrics do not account for related but irrelevant documents that actively degrade generation quality, rather than merely being ignored. Due to these two major misalignments, namely human vs. machine position discount and human relevance vs. machine utility, classical IR metrics do not accurately predict RAG performance. We introduce a utility-based annotation schema that quantifies both the positive contribution of relevant passages and the negative impact of distracting ones. Building on this foundation, we propose UDCG (Utility and Distraction-aware Cumulative Gain), a metric using an LLM-oriented positional discount to directly optimize the correlation with the end-to-end answer accuracy. Experiments on five datasets and six LLMs demonstrate that UDCG improves correlation by up to 36% compared to traditional metrics. Our work provides a critical step toward aligning IR evaluation with LLM consumers and enables more reliable assessment of RAG components', 'score': 0, 'issue_id': 6635, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '8b9a52137045ce11', 'authors': ['Giovanni Trappolini', 'Florin Cuconasu', 'Simone Filice', 'Yoelle Maarek', 'Fabrizio Silvestri'], 'affiliations': ['Sapienza University of Rome', 'Technology Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2510.21440.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#rag', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'Новая метрика для оценки поиска в RAG-системах с учётом работы LLM', 'desc': 'Исследователи представили метрику UDCG для оценки систем Retrieval Augmented Generation, которая учитывает особенности работы LLM в отличие от людей. Традиционные метрики информационного поиска вроде nDCG предполагают последовательное изучение документов человеком, но LLM обрабатывают все результаты одновременно. UDCG учитывает не только полезность релевантных документов, но и негативное влияние отвлекающих нерелевантных материалов на качество генерации ответов. Эксперименты показали улучшение корреляции с точностью конечных ответов до 36% по сравнению с классическими метриками.'}, 'en': {'title': 'UDCG: A New Metric for Better RAG Evaluation', 'desc': 'This paper introduces a new evaluation metric called UDCG, designed specifically for Retrieval Augmented Generation (RAG) systems. Unlike traditional metrics that assume users examine documents sequentially, UDCG accounts for how Large Language Models (LLMs) process all retrieved documents simultaneously. It also addresses the issue of irrelevant documents that can negatively impact the quality of generated answers, which traditional metrics overlook. By incorporating both the utility of relevant documents and the distraction of irrelevant ones, UDCG significantly improves the correlation with actual answer accuracy in RAG systems.'}, 'zh': {'title': 'UDCG：提升检索增强生成系统评估的关键指标', 'desc': '本文提出了一种新的评估指标UDCG，用于更好地评估检索增强生成系统（RAG）。该指标考虑了相关文档的效用和无关文档的干扰，从而提高了与最终答案准确性的相关性。传统的信息检索指标无法有效反映机器学习模型的处理方式，因此无法准确预测RAG的性能。通过引入基于效用的注释方案，UDCG能够更好地评估检索结果的质量，提升了评估的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2510.21111', 'title': 'PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language\n  Models in Physical Environments', 'url': 'https://huggingface.co/papers/2510.21111', 'abstract': 'Active Visual Reasoning (AVR) extends visual reasoning to interactive, partially observable environments, requiring agents to sequentially gather and integrate information for coherent decision-making, as evaluated by the CLEVR-AVR benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual reasoning in multimodal large language models (MLLMs) has primarily been studied in static, fully observable settings, limiting their effectiveness in real-world environments where information is often incomplete due to occlusion or limited field of view. Humans, in contrast, actively explore and interact with their environment-moving, examining, and manipulating objects-to gather information through a closed-loop process integrating perception, reasoning, and action. Inspired by this human capability, we introduce the Active Visual Reasoning (AVR) task, extending visual reasoning to partially observable, interactive environments. AVR necessitates agents to: (1) actively acquire information via sequential physical actions, (2) integrate observations across multiple steps for coherent reasoning, and (3) dynamically adjust decisions based on evolving visual feedback. To rigorously evaluate AVR, we introduce CLEVR-AVR, a simulation benchmark featuring multi-round interactive environments designed to assess both reasoning correctness and information-gathering efficiency. We present AVR-152k, a large-scale dataset that offers rich Chain-of-Thought (CoT) annotations detailing iterative reasoning for uncertainty identification, action-conditioned information gain prediction, and information-maximizing action selection, crucial for training agents in a higher-order Markov Decision Process. Building on this, we develop PhysVLM-AVR, an MLLM achieving state-of-the-art performance on CLEVR-AVR, embodied reasoning (OpenEQA, RoboVQA), and passive visual reasoning (GeoMath, Geometry30K). Our analysis also reveals that current embodied MLLMs, despite detecting information incompleteness, struggle to actively acquire and integrate new information through interaction, highlighting a fundamental gap in active reasoning capabilities.', 'score': 0, 'issue_id': 6627, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'c9c37d0f923c45b0', 'authors': ['Weijie Zhou', 'Xuantang Xiong', 'Yi Peng', 'Manli Tao', 'Chaoyang Zhao', 'Honghui Dong', 'Ming Tang', 'Jinqiao Wang'], 'affiliations': ['Beijing Jiaotong University', 'Foundation Model Research Center, Institute of Automation, Chinese Academy of Sciences', 'ObjectEye Inc.', 'Tencent Robotics & Futian Laboratory, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2510.21111.jpg', 'data': {'categories': ['#dataset', '#cv', '#multimodal', '#agents', '#benchmark', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Научите AI активно смотреть и думать', 'desc': 'Статья представляет задачу Active Visual Reasoning (AVR), где AI-агенты должны активно исследовать частично наблюдаемые интерактивные среды, подобно людям. Авторы создали бенчмарк CLEVR-AVR и датасет AVR-152k с аннотациями Chain-of-Thought для обучения агентов последовательному сбору информации через физические действия. На основе этих данных разработана модель PhysVLM-AVR, показавшая state-of-the-art результаты на нескольких задачах визуального рассуждения. Исследование выявило, что современные multimodal LLM плохо справляются с активным сбором и интеграцией новой информации через взаимодействие со средой.'}, 'en': {'title': 'Empowering AI with Active Visual Reasoning', 'desc': 'Active Visual Reasoning (AVR) is a new approach that allows AI agents to make decisions in environments where they cannot see everything at once. Unlike traditional visual reasoning, which works in fully observable settings, AVR requires agents to actively explore and gather information through actions. This process involves integrating observations over time and adjusting decisions based on what they see, similar to how humans interact with their surroundings. The paper introduces a benchmark called CLEVR-AVR to evaluate these capabilities and presents a dataset and model that improve the performance of AI in these complex scenarios.'}, 'zh': {'title': '主动视觉推理：智能体的交互式决策新境界', 'desc': '主动视觉推理（AVR）将视觉推理扩展到交互式、部分可观察的环境中，要求智能体通过顺序收集和整合信息来进行连贯的决策。与静态、完全可观察的设置不同，AVR 需要智能体通过物理动作主动获取信息，并在多个步骤中整合观察结果以进行合理推理。为评估 AVR 的有效性，我们引入了 CLEVR-AVR 基准测试，设计了多轮交互环境以评估推理的正确性和信息收集的效率。此外，我们开发了 PhysVLM-AVR，这是一种在 CLEVR-AVR 上表现出色的多模态大语言模型。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (7)', '#agi (1)', '#alignment (4)', '#architecture (4)', '#audio', '#benchmark (6)', '#cv (4)', '#data (2)', '#dataset (6)', '#diffusion (4)', '#ethics', '#games (1)', '#graphs (1)', '#hallucinations (1)', '#healthcare', '#inference (5)', '#interpretability (2)', '#leakage', '#long_context (3)', '#low_resource', '#machine_translation (1)', '#math (3)', '#multilingual', '#multimodal (8)', '#open_source (3)', '#optimization (14)', '#plp', '#rag (2)', '#reasoning (9)', '#rl (7)', '#rlhf', '#robotics (1)', '#science (1)', '#security (1)', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic (3)', '#training (13)', '#transfer_learning (1)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-10-27 23:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-27 23:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-27 23:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    