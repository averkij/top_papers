{
    "date": {
        "ru": "22 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        "en": "August 22",
        "zh": "8æœˆ22æ—¥"
    },
    "time_utc": "2025-08-22 21:10",
    "weekday": 4,
    "issue_id": 5504,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.15763",
            "title": "Intern-S1: A Scientific Multimodal Foundation Model",
            "url": "https://huggingface.co/papers/2508.15763",
            "abstract": "Intern-S1, a multimodal Mixture-of-Experts model with extensive pre-training and reinforcement learning, achieves top-tier performance in general reasoning and outperforms closed-source models in scientific tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared to those in popular areas, far from sufficient for transforming scientific research and leaving substantial gap between open-source models and closed-source models in these scientific domains. To mitigate this gap and explore a step further toward Artificial General Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped with general understanding and reasoning capabilities with expertise to analyze multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE) model with 28 billion activated parameters and 241 billion total parameters, continually pre-trained on 5T tokens, including over 2.5T tokens from scientific domains. In the post-training stage, Intern-S1 undergoes offline and then online reinforcement learning (RL) in InternBootCamp, where we propose Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks simultaneously. Through integrated innovations in algorithms, data, and training systems, Intern-S1 achieved top-tier performance in online RL training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates competitive performance on general reasoning tasks among open-source models and significantly outperforms open-source models in scientific domains, surpassing closed-source state-of-the-art models in professional tasks, such as molecular synthesis planning, reaction condition prediction, predicting thermodynamic stabilities for crystals. Our models are available at https://huggingface.co/internlm/Intern-S1.",
            "score": 162,
            "issue_id": 5485,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 21",
                "zh": "8æœˆ21æ—¥"
            },
            "hash": "55fe9cc77ae55101",
            "authors": [
                "Lei Bai",
                "Zhongrui Cai",
                "Maosong Cao",
                "Weihan Cao",
                "Chiyu Chen",
                "Haojiong Chen",
                "Kai Chen",
                "Pengcheng Chen",
                "Ying Chen",
                "Yongkang Chen",
                "Yu Cheng",
                "Yu Cheng",
                "Pei Chu",
                "Tao Chu",
                "Erfei Cui",
                "Ganqu Cui",
                "Long Cui",
                "Ziyun Cui",
                "Nianchen Deng",
                "Ning Ding",
                "Nanqin Dong",
                "Peijie Dong",
                "Shihan Dou",
                "Sinan Du",
                "Haodong Duan",
                "Caihua Fan",
                "Ben Gao",
                "Changjiang Gao",
                "Jianfei Gao",
                "Songyang Gao",
                "Yang Gao",
                "Zhangwei Gao",
                "Jiaye Ge",
                "Qiming Ge",
                "Lixin Gu",
                "Yuzhe Gu",
                "Aijia Guo",
                "Qipeng Guo",
                "Xu Guo",
                "Conghui He",
                "Junjun He",
                "Yili Hong",
                "Siyuan Hou",
                "Caiyu Hu",
                "Hanglei Hu",
                "Jucheng Hu",
                "Ming Hu",
                "Zhouqi Hua",
                "Haian Huang",
                "Junhao Huang",
                "Xu Huang",
                "Zixian Huang",
                "Zhe Jiang",
                "Lingkai Kong",
                "Linyang Li",
                "Peiji Li",
                "Pengze Li",
                "Shuaibin Li",
                "Tianbin Li",
                "Wei Li",
                "Yuqiang Li",
                "Dahua Lin",
                "Junyao Lin",
                "Tianyi Lin",
                "Zhishan Lin",
                "Hongwei Liu",
                "Jiangning Liu",
                "Jiyao Liu",
                "Junnan Liu",
                "Kai Liu",
                "Kaiwen Liu",
                "Kuikun Liu",
                "Shichun Liu",
                "Shudong Liu",
                "Wei Liu",
                "Xinyao Liu",
                "Yuhong Liu",
                "Zhan Liu",
                "Yinquan Lu",
                "Haijun Lv",
                "Hongxia Lv",
                "Huijie Lv",
                "Qidang Lv",
                "Ying Lv",
                "Chengqi Lyu",
                "Chenglong Ma",
                "Jianpeng Ma",
                "Ren Ma",
                "Runmin Ma",
                "Runyuan Ma",
                "Xinzhu Ma",
                "Yichuan Ma",
                "Zihan Ma",
                "Sixuan Mi",
                "Junzhi Ning",
                "Wenchang Ning",
                "Xinle Pang",
                "Jiahui Peng",
                "Runyu Peng",
                "Yu Qiao",
                "Jiantao Qiu",
                "Xiaoye Qu",
                "Yuan Qu",
                "Yuchen Ren",
                "Fukai Shang",
                "Wenqi Shao",
                "Junhao Shen",
                "Shuaike Shen",
                "Chunfeng Song",
                "Demin Song",
                "Diping Song",
                "Chenlin Su",
                "Weijie Su",
                "Weigao Sun",
                "Yu Sun",
                "Qian Tan",
                "Cheng Tang",
                "Huanze Tang",
                "Kexian Tang",
                "Shixiang Tang",
                "Jian Tong",
                "Aoran Wang",
                "Bin Wang",
                "Dong Wang",
                "Lintao Wang",
                "Rui Wang",
                "Weiyun Wang",
                "Wenhai Wang",
                "Yi Wang",
                "Ziyi Wang",
                "Ling-I Wu",
                "Wen Wu",
                "Yue Wu",
                "Zijian Wu",
                "Linchen Xiao",
                "Shuhao Xing",
                "Chao Xu",
                "Huihui Xu",
                "Jun Xu",
                "Ruiliang Xu",
                "Wanghan Xu",
                "GanLin Yang",
                "Yuming Yang",
                "Haochen Ye",
                "Jin Ye",
                "Shenglong Ye",
                "Jia Yu",
                "Jiashuo Yu",
                "Jing Yu",
                "Fei Yuan",
                "Bo Zhang",
                "Chao Zhang",
                "Chen Zhang",
                "Hongjie Zhang",
                "Jin Zhang",
                "Qiaosheng Zhang",
                "Qiuyinzhe Zhang",
                "Songyang Zhang",
                "Taolin Zhang",
                "Wenlong Zhang",
                "Wenwei Zhang",
                "Yechen Zhang",
                "Ziyang Zhang",
                "Haiteng Zhao",
                "Qian Zhao",
                "Xiangyu Zhao",
                "Xiangyu Zhao",
                "Bowen Zhou",
                "Dongzhan Zhou",
                "Peiheng Zhou",
                "Yuhao Zhou",
                "Yunhua Zhou",
                "Dongsheng Zhu",
                "Lin Zhu",
                "Yicheng Zou"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15763.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#science",
                    "#reasoning",
                    "#dataset",
                    "#open_source",
                    "#rl",
                    "#training",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Intern-S1: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Intern-S1 - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ¸Ğ¿Ğ° Mixture-of-Experts Ñ 28 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 5 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ 2.5 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ· Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. Intern-S1 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Mixture-of-Rewards Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1000 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµÑ€Ğ¼Ğ¾Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€Ğ¸ÑÑ‚Ğ°Ğ»Ğ»Ğ¾Ğ²."
                },
                "en": {
                    "title": "Bridging the Gap in Scientific AI with Intern-S1",
                    "desc": "Intern-S1 is a multimodal Mixture-of-Experts model designed to enhance performance in scientific reasoning tasks. It utilizes extensive pre-training on a vast dataset, including a significant portion from scientific domains, to develop a deep understanding of complex data. The model employs reinforcement learning techniques, specifically Mixture-of-Rewards, to optimize its performance across numerous tasks simultaneously. As a result, Intern-S1 not only excels in general reasoning but also surpasses both open-source and closed-source models in specialized scientific applications."
                },
                "zh": {
                    "title": "Intern-S1ï¼šç§‘å­¦é¢†åŸŸçš„æ™ºèƒ½ä¸“å®¶",
                    "desc": "Intern-S1æ˜¯ä¸€ç§å¤šæ¨¡æ€ä¸“å®¶æ··åˆæ¨¡å‹ï¼Œç»è¿‡å¹¿æ³›çš„é¢„è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œèƒ½å¤Ÿåœ¨ä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨ç§‘å­¦ä»»åŠ¡ä¸­è¶…è¶Šé—­æºæ¨¡å‹ã€‚è¯¥æ¨¡å‹æ‹¥æœ‰280äº¿ä¸ªæ¿€æ´»å‚æ•°å’Œ2410äº¿ä¸ªæ€»å‚æ•°ï¼Œç»è¿‡5ä¸‡äº¿ä¸ªæ ‡è®°çš„æŒç»­é¢„è®­ç»ƒï¼Œå…¶ä¸­è¶…è¿‡2.5ä¸‡äº¿ä¸ªæ ‡è®°æ¥è‡ªç§‘å­¦é¢†åŸŸã€‚ä¸ºäº†ç¼©å°å¼€æ”¾æºæ¨¡å‹ä¸é—­æºæ¨¡å‹åœ¨ç§‘å­¦é¢†åŸŸçš„å·®è·ï¼ŒIntern-S1é‡‡ç”¨äº†æ··åˆå¥–åŠ±æœºåˆ¶ï¼Œåœ¨è¶…è¿‡1000ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚é€šè¿‡ç®—æ³•ã€æ•°æ®å’Œè®­ç»ƒç³»ç»Ÿçš„ç»¼åˆåˆ›æ–°ï¼ŒIntern-S1åœ¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­å–å¾—äº†é¡¶å°–çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15144",
            "title": "Mobile-Agent-v3: Foundamental Agents for GUI Automation",
            "url": "https://huggingface.co/papers/2508.15144",
            "abstract": "GUI-Owl and Mobile-Agent-v3 are open-source GUI agent models and frameworks that achieve state-of-the-art performance across various benchmarks using innovations in environment infrastructure, agent capabilities, and scalable reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at https://github.com/X-PLUG/MobileAgent.",
            "score": 38,
            "issue_id": 5486,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 21",
                "zh": "8æœˆ21æ—¥"
            },
            "hash": "f41c368d3d1b16f8",
            "authors": [
                "Jiabo Ye",
                "Xi Zhang",
                "Haiyang Xu",
                "Haowei Liu",
                "Junyang Wang",
                "Zhaoqing Zhu",
                "Ziwei Zheng",
                "Feiyu Gao",
                "Junjie Cao",
                "Zhengxi Lu",
                "Jitong Liao",
                "Qi Zheng",
                "Fei Huang",
                "Jingren Zhou",
                "Ming Yan"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15144.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#open_source",
                    "#agents",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GUI-Owl Ğ¸ Mobile-Agent-v3 - Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼. Ğ­Ñ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ÑĞ¼ Ğ² Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹, Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. GUI-Owl Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½ÑƒÑ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Mobile-Agent-v3 Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ² GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing GUI Agents with State-of-the-Art Performance",
                    "desc": "This paper presents GUI-Owl, an advanced GUI agent model that excels in performance on various benchmarks for desktop and mobile environments. It introduces Mobile-Agent-v3, which enhances GUI-Owl's capabilities, achieving new state-of-the-art scores on AndroidWorld and OSWorld. Key innovations include a large-scale cloud-based environment for generating high-quality interaction data, diverse foundational agent capabilities for end-to-end decision-making, and a scalable reinforcement learning framework for real-world applications. Both models are open-source, promoting further research and development in GUI agent technology."
                },
                "zh": {
                    "title": "å¼€æºGUIä»£ç†çš„æ–°æ—¶ä»£",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†GUI-Owlå’ŒMobile-Agent-v3è¿™ä¸¤ä¸ªå¼€æºGUIä»£ç†æ¨¡å‹å’Œæ¡†æ¶ï¼Œå®ƒä»¬åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚GUI-Owlæ˜¯ä¸€ä¸ªåŸºç¡€çš„GUIä»£ç†æ¨¡å‹ï¼Œåœ¨æ¡Œé¢å’Œç§»åŠ¨ç¯å¢ƒçš„åä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚Mobile-Agent-v3åˆ™æ˜¯ä¸€ä¸ªé€šç”¨çš„GUIä»£ç†æ¡†æ¶ï¼Œè¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ï¼Œåˆ›é€ äº†æ–°çš„å¼€æºGUIä»£ç†æ¡†æ¶çš„æœ€ä½³è®°å½•ã€‚è¯¥ç ”ç©¶çš„å…³é”®åˆ›æ–°åŒ…æ‹¬å¤§è§„æ¨¡ç¯å¢ƒåŸºç¡€è®¾æ–½ã€å¤šæ ·åŒ–çš„åŸºç¡€ä»£ç†èƒ½åŠ›å’Œå¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15760",
            "title": "LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on\n  Challenging Queries",
            "url": "https://huggingface.co/papers/2508.15760",
            "abstract": "LiveMCP-101 benchmarks AI agents' ability to use multiple tools in real-world scenarios, revealing challenges in tool orchestration and inefficiencies in token usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks. While the Model Context Protocol (MCP) provides a powerful standardized framework for tool integration, there is a significant gap in benchmarking how well AI agents can effectively solve multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In this work, we present LiveMCP-101, a benchmark of 101 carefully curated real-world queries, refined through iterative LLM rewriting and manual review, that require coordinated use of multiple MCP tools including web search, file operations, mathematical reasoning, and data analysis. Moreover, we introduce a novel evaluation approach that leverages ground-truth execution plans rather than raw API outputs, better reflecting the evolving nature of real-world environments. Experiments show that even frontier LLMs achieve a success rate below 60\\%, highlighting major challenges in tool orchestration. Detailed ablations and error analysis further reveal distinct failure modes and inefficiencies in token usage, pointing to concrete directions for advancing current models. LiveMCP-101 sets a rigorous standard for evaluating real-world agent capabilities, advancing toward autonomous AI systems that reliably execute complex tasks through tool use.",
            "score": 25,
            "issue_id": 5490,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 21",
                "zh": "8æœˆ21æ—¥"
            },
            "hash": "04f8a909c0d3944e",
            "authors": [
                "Ming Yin",
                "Dinghan Shen",
                "Silei Xu",
                "Jianbing Han",
                "Sixun Dong",
                "Mian Zhang",
                "Yebowen Hu",
                "Shujian Liu",
                "Simin Ma",
                "Song Wang",
                "Sathish Reddy Indurthi",
                "Xun Wang",
                "Yiran Chen",
                "Kaiqiang Song"
            ],
            "affiliations": [
                "Duke University",
                "Zoom Video Communications"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15760.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#benchmark",
                    "#reasoning",
                    "#agi"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "LiveMCP-101: Ğ˜ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…",
                    "desc": "LiveMCP-101 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 101 Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… MCP-Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ¸Ğ¶Ğµ 60%, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ¾Ğ²ĞºĞµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ, Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ³Ğ°ÑÑÑŒ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Benchmarking AI Agents: Mastering Tool Use in Real-World Tasks",
                    "desc": "LiveMCP-101 is a benchmark designed to evaluate how well AI agents can use multiple tools to solve complex, real-world tasks. It highlights the challenges of tool orchestration and inefficiencies in token usage when agents attempt to perform multi-step operations. The benchmark consists of 101 real-world queries that require the coordinated use of various tools, such as web search and data analysis, and introduces a new evaluation method based on ground-truth execution plans. Results show that even advanced language models struggle with these tasks, achieving less than 60% success, indicating significant areas for improvement in AI tool integration."
                },
                "zh": {
                    "title": "è¯„ä¼°AIä»£ç†å·¥å…·ä½¿ç”¨èƒ½åŠ›çš„æ–°åŸºå‡†",
                    "desc": "LiveMCP-101æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°äººå·¥æ™ºèƒ½ä»£ç†åœ¨ç°å®åœºæ™¯ä¸­ä½¿ç”¨å¤šç§å·¥å…·çš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å·¥å…·åè°ƒçš„æŒ‘æˆ˜ä»¥åŠä»¤ç‰Œä½¿ç”¨çš„ä½æ•ˆæ€§ã€‚é€šè¿‡å¼•å…¥åŸºäºçœŸå®æ‰§è¡Œè®¡åˆ’çš„è¯„ä¼°æ–¹æ³•ï¼Œç ”ç©¶æ›´å‡†ç¡®åœ°åæ˜ äº†ç°å®ç¯å¢ƒçš„åŠ¨æ€ç‰¹æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ï¼Œå…¶æˆåŠŸç‡ä¹Ÿä½äº60%ï¼Œè¿™è¡¨æ˜åœ¨å¤šæ­¥éª¤ä»»åŠ¡ä¸­ä»å­˜åœ¨æ˜¾è‘—çš„å›°éš¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15260",
            "title": "Deep Think with Confidence",
            "url": "https://huggingface.co/papers/2508.15260",
            "abstract": "DeepConf enhances reasoning efficiency and performance by filtering low-quality reasoning traces using model-internal confidence signals, achieving high accuracy and reducing token generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have shown great potential in reasoning tasks through test-time scaling methods like self-consistency with majority voting. However, this approach often leads to diminishing returns in accuracy and high computational overhead. To address these challenges, we introduce Deep Think with Confidence (DeepConf), a simple yet powerful method that enhances both reasoning efficiency and performance at test time. DeepConf leverages model-internal confidence signals to dynamically filter out low-quality reasoning traces during or after generation. It requires no additional model training or hyperparameter tuning and can be seamlessly integrated into existing serving frameworks. We evaluate DeepConf across a variety of reasoning tasks and the latest open-source models, including Qwen 3 and GPT-OSS series. Notably, on challenging benchmarks such as AIME 2025, DeepConf@512 achieves up to 99.9% accuracy and reduces generated tokens by up to 84.7% compared to full parallel thinking.",
            "score": 22,
            "issue_id": 5485,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 21",
                "zh": "8æœˆ21æ—¥"
            },
            "hash": "2ad4b155cf2ef39e",
            "authors": [
                "Yichao Fu",
                "Xuewei Wang",
                "Yuandong Tian",
                "Jiawei Zhao"
            ],
            "affiliations": [
                "Meta AI",
                "UCSD"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15260.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#inference",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½ĞµĞµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ: DeepConf Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "DeepConf - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. DeepConf Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸. Ğ’ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²."
                },
                "en": {
                    "title": "Boosting Reasoning Efficiency with Confidence Filtering",
                    "desc": "DeepConf is a novel method that improves the efficiency and accuracy of reasoning in large language models by using internal confidence signals to filter out low-quality reasoning traces. This approach addresses the limitations of traditional test-time scaling methods, which often result in high computational costs and minimal accuracy gains. By dynamically selecting the most reliable reasoning paths, DeepConf enhances performance without requiring additional training or tuning. Evaluations show that DeepConf significantly boosts accuracy and reduces token generation, making it a valuable tool for various reasoning tasks."
                },
                "zh": {
                    "title": "æ·±åº¦ç½®ä¿¡ï¼Œæå‡æ¨ç†æ•ˆç‡ä¸å‡†ç¡®æ€§",
                    "desc": "DeepConfæ˜¯ä¸€ç§å¢å¼ºæ¨ç†æ•ˆç‡å’Œæ€§èƒ½çš„æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨æ¨¡å‹å†…éƒ¨çš„ç½®ä¿¡ä¿¡å·æ¥è¿‡æ»¤ä½è´¨é‡çš„æ¨ç†è½¨è¿¹ã€‚è¯¥æ–¹æ³•åœ¨æµ‹è¯•æ—¶æ— éœ€é¢å¤–çš„æ¨¡å‹è®­ç»ƒæˆ–è¶…å‚æ•°è°ƒæ•´ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„æœåŠ¡æ¡†æ¶ä¸­ã€‚DeepConfåœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨AIME 2025ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¾¾åˆ°äº†99.9%çš„é«˜å‡†ç¡®ç‡ï¼Œå¹¶å‡å°‘äº†å¤šè¾¾84.7%çš„ç”Ÿæˆä»¤ç‰Œã€‚é€šè¿‡åŠ¨æ€è¿‡æ»¤ï¼ŒDeepConfæœ‰æ•ˆæé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15761",
            "title": "Waver: Wave Your Way to Lifelike Video Generation",
            "url": "https://huggingface.co/papers/2508.15761",
            "abstract": "Waver, a high-performance foundation model, generates high-quality videos and images using a Hybrid Stream DiT architecture, excelling in text-to-video, image-to-video, and text-to-image tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Waver, a high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at a native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation within a single, integrated framework. We introduce a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish a comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https://github.com/FoundationVision/Waver.",
            "score": 13,
            "issue_id": 5488,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 21",
                "zh": "8æœˆ21æ—¥"
            },
            "hash": "9168ada481044deb",
            "authors": [
                "Yifu Zhang",
                "Hao Yang",
                "Yuqi Zhang",
                "Yifei Hu",
                "Fengda Zhu",
                "Chuang Lin",
                "Xiaofeng Mei",
                "Yi Jiang",
                "Zehuan Yuan",
                "Bingyue Peng"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2508.15761.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#training",
                    "#multimodal",
                    "#data",
                    "#open_source",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Waver: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Waver - ÑÑ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Hybrid Stream DiT Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. Waver Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ°Ğ¼Ğ¿Ğ»Ğ¸Ñ‚ÑƒĞ´Ñ‹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Waver: Unifying Image and Video Generation with High Performance",
                    "desc": "Waver is a cutting-edge foundation model designed for generating high-quality images and videos using a Hybrid Stream DiT architecture. It can create videos lasting 5 to 10 seconds at a resolution of 720p, which can be upscaled to 1080p. The model integrates text-to-video, image-to-video, and text-to-image generation in one framework, enhancing modality alignment and speeding up training. With a robust data curation pipeline and a specialized video quality model, Waver achieves superior motion capture and consistency, ranking among the top models in its category."
                },
                "zh": {
                    "title": "Waverï¼šé«˜æ€§èƒ½çš„å›¾åƒä¸è§†é¢‘ç”Ÿæˆæ¨¡å‹",
                    "desc": "Waveræ˜¯ä¸€ç§é«˜æ€§èƒ½çš„åŸºç¡€æ¨¡å‹ï¼Œä¸“æ³¨äºç»Ÿä¸€çš„å›¾åƒå’Œè§†é¢‘ç”Ÿæˆã€‚å®ƒèƒ½å¤Ÿç›´æ¥ç”Ÿæˆæ—¶é•¿ä¸º5åˆ°10ç§’çš„720pè§†é¢‘ï¼Œå¹¶å¯æå‡è‡³1080påˆ†è¾¨ç‡ã€‚è¯¥æ¨¡å‹æ”¯æŒæ–‡æœ¬åˆ°è§†é¢‘ã€å›¾åƒåˆ°è§†é¢‘å’Œæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆï¼Œé‡‡ç”¨äº†æ··åˆæµDiTæ¶æ„ä»¥å¢å¼ºæ¨¡æ€å¯¹é½å’ŒåŠ é€Ÿè®­ç»ƒæ”¶æ•›ã€‚Waveråœ¨è§†é¢‘åˆæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæ•æ‰å¤æ‚çš„è¿åŠ¨ï¼Œå…·æœ‰ä¼˜è¶Šçš„è¿åŠ¨å¹…åº¦å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15769",
            "title": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass",
            "url": "https://huggingface.co/papers/2508.15769",
            "abstract": "SceneGen generates multiple 3D assets from a single scene image using a novel framework that integrates local and global scene information, enabling efficient and robust 3D content creation.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.",
            "score": 11,
            "issue_id": 5485,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 21",
                "zh": "8æœˆ21æ—¥"
            },
            "hash": "deff71bdd7ad6d46",
            "authors": [
                "Yanxu Meng",
                "Haoning Wu",
                "Ya Zhang",
                "Weidi Xie"
            ],
            "affiliations": [
                "School of Artificial Intelligence, Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15769.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞÑ‚ 2D Ğº 3D: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½",
                    "desc": "SceneGen - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑÑ†ĞµĞ½Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. SceneGen Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² VR/AR Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Revolutionizing 3D Asset Creation from Single Images",
                    "desc": "SceneGen is a new framework designed to create multiple 3D assets from a single scene image by effectively combining local and global scene information. It uses a feature aggregation module that processes visual and geometric data to generate 3D models with their textures and spatial arrangements in one go, without needing optimization or asset retrieval. The framework is also adaptable, showing improved performance when handling multiple images, even though it was initially trained on single images. Overall, SceneGen represents a significant advancement in the field of 3D content generation, with promising applications in virtual and augmented reality."
                },
                "zh": {
                    "title": "SceneGenï¼šé«˜æ•ˆç”Ÿæˆ3Dèµ„äº§çš„æ–°æ¡†æ¶",
                    "desc": "SceneGenæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå¯ä»¥ä»å•ä¸€åœºæ™¯å›¾åƒç”Ÿæˆå¤šä¸ª3Dèµ„äº§ã€‚å®ƒé€šè¿‡æ•´åˆå±€éƒ¨å’Œå…¨å±€åœºæ™¯ä¿¡æ¯ï¼Œèƒ½å¤Ÿé«˜æ•ˆä¸”ç¨³å¥åœ°åˆ›å»º3Då†…å®¹ã€‚è¯¥æ¡†æ¶æ— éœ€ä¼˜åŒ–æˆ–èµ„äº§æ£€ç´¢ï¼Œç›´æ¥ç”Ÿæˆå…·æœ‰å‡ ä½•å½¢çŠ¶å’Œçº¹ç†çš„3Dèµ„äº§ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒSceneGenåœ¨ç”Ÿæˆæ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ï¼Œé€‚ç”¨äºå¤šå›¾åƒè¾“å…¥åœºæ™¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15361",
            "title": "A Survey on Large Language Model Benchmarks",
            "url": "https://huggingface.co/papers/2508.15361",
            "abstract": "A systematic review of large language model benchmarks identifies issues such as data contamination, cultural biases, and lack of process credibility, and proposes a design paradigm for future improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent years, with the rapid development of the depth and breadth of large language models' capabilities, various corresponding evaluation benchmarks have been emerging in increasing numbers. As a quantitative assessment tool for model performance, benchmarks are not only a core means to measure model capabilities but also a key element in guiding the direction of model development and promoting technological innovation. We systematically review the current status and development of large language model benchmarks for the first time, categorizing 283 representative benchmarks into three categories: general capabilities, domain-specific, and target-specific. General capability benchmarks cover aspects such as core linguistics, knowledge, and reasoning; domain-specific benchmarks focus on fields like natural sciences, humanities and social sciences, and engineering technology; target-specific benchmarks pay attention to risks, reliability, agents, etc. We point out that current benchmarks have problems such as inflated scores caused by data contamination, unfair evaluation due to cultural and linguistic biases, and lack of evaluation on process credibility and dynamic environments, and provide a referable design paradigm for future benchmark innovation.",
            "score": 7,
            "issue_id": 5488,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 21",
                "zh": "8æœˆ21æ—¥"
            },
            "hash": "d2096a14a9fdc644",
            "authors": [
                "Shiwen Ni",
                "Guhong Chen",
                "Shuaimin Li",
                "Xuanang Chen",
                "Siyi Li",
                "Bingli Wang",
                "Qiyao Wang",
                "Xingjian Wang",
                "Yifan Zhang",
                "Liyang Fan",
                "Chengming Li",
                "Ruifeng Xu",
                "Le Sun",
                "Min Yang"
            ],
            "affiliations": [
                "Harbin Institute of Technology, Shenzhen",
                "Institute of Software, Chinese Academy of Sciences",
                "Shanghai AI Lab",
                "Shanghai University of Electric Power",
                "Shenzhen Key Laboratory for High Performance Data Mining, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
                "Shenzhen MSU-BIT University",
                "Shenzhen University",
                "Shenzhen University of Advanced Technology",
                "South China University of Technology",
                "Southern University of Science and Technology",
                "University of Chinese Academy of Sciences",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15361.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğº Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ² Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ 283 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼: Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ."
                },
                "en": {
                    "title": "Enhancing Language Model Benchmarks for Fair and Credible Evaluation",
                    "desc": "This paper reviews the current benchmarks used to evaluate large language models, highlighting their importance in measuring model performance and guiding development. It categorizes 283 benchmarks into three types: general capabilities, domain-specific, and target-specific, each focusing on different aspects of model evaluation. The authors identify significant issues such as data contamination leading to inflated scores, cultural biases affecting fairness, and a lack of credibility in evaluation processes. To address these challenges, the paper proposes a new design paradigm aimed at improving future benchmarks for more reliable assessments."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹åŸºå‡†çš„è®¾è®¡ä¸å¯ä¿¡åº¦",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç³»ç»Ÿæ€§åœ°å›é¡¾äº†å¤§å‹è¯­è¨€æ¨¡å‹åŸºå‡†çš„ç°çŠ¶ï¼Œè¯†åˆ«å‡ºæ•°æ®æ±¡æŸ“ã€æ–‡åŒ–åè§å’Œè¿‡ç¨‹å¯ä¿¡åº¦ç¼ºä¹ç­‰é—®é¢˜ã€‚ä½œè€…å°†283ä¸ªä»£è¡¨æ€§åŸºå‡†åˆ†ä¸ºä¸‰ç±»ï¼šé€šç”¨èƒ½åŠ›ã€é¢†åŸŸç‰¹å®šå’Œç›®æ ‡ç‰¹å®šã€‚é€šç”¨èƒ½åŠ›åŸºå‡†æ¶µç›–æ ¸å¿ƒè¯­è¨€å­¦ã€çŸ¥è¯†å’Œæ¨ç†ç­‰æ–¹é¢ï¼Œè€Œé¢†åŸŸç‰¹å®šåŸºå‡†åˆ™å…³æ³¨è‡ªç„¶ç§‘å­¦ã€äººæ–‡å­¦ç§‘å’Œå·¥ç¨‹æŠ€æœ¯ç­‰é¢†åŸŸã€‚è®ºæ–‡è¿˜æå‡ºäº†æœªæ¥åŸºå‡†åˆ›æ–°çš„è®¾è®¡èŒƒå¼ï¼Œä»¥è§£å†³å½“å‰åŸºå‡†å­˜åœ¨çš„é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15772",
            "title": "Visual Autoregressive Modeling for Instruction-Guided Image Editing",
            "url": "https://huggingface.co/papers/2508.15772",
            "abstract": "VAREdit, a visual autoregressive framework, improves image editing adherence and efficiency by using a Scale-Aligned Reference module to condition source image tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer a distinct paradigm by formulating image synthesis as a sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, a visual autoregressive (VAR) framework that reframes image editing as a next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. A core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce a Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusion-based methods by 30\\%+ higher GPT-Balance score. Moreover, it completes a 512times512 editing in 1.2 seconds, making it 2.2times faster than the similarly sized UltraEdit. The models are available at https://github.com/HiDream-ai/VAREdit.",
            "score": 5,
            "issue_id": 5498,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 21",
                "zh": "8æœˆ21æ—¥"
            },
            "hash": "4305672bf954d7d1",
            "authors": [
                "Qingyang Mao",
                "Qi Cai",
                "Yehao Li",
                "Yingwei Pan",
                "Mingyue Cheng",
                "Ting Yao",
                "Qi Liu",
                "Tao Mei"
            ],
            "affiliations": [
                "HiDream.ai Inc.",
                "University of Science and Technology of China, Hefei, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15772.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#optimization",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "VAREdit: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸",
                    "desc": "VAREdit - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Scale-Aligned Reference Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹. VAREdit Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° 30% Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² 2.2 Ñ€Ğ°Ğ·Ğ° Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ 512x512 Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 1.2 ÑĞµĞºÑƒĞ½Ğ´Ñ‹."
                },
                "en": {
                    "title": "VAREdit: Precision and Speed in Image Editing",
                    "desc": "VAREdit is a visual autoregressive framework designed to enhance image editing by improving adherence to instructions and increasing efficiency. It utilizes a Scale-Aligned Reference (SAR) module to condition source image tokens, allowing for better guidance in generating multi-scale target features. Unlike diffusion models, which can unintentionally alter unrelated parts of an image, VAREdit processes image synthesis sequentially over discrete visual tokens, minimizing spurious modifications. The framework achieves significant performance improvements, outperforming leading methods by over 30% in editing accuracy and completing edits much faster than previous models."
                },
                "zh": {
                    "title": "VAREditï¼šé«˜æ•ˆç²¾å‡†çš„å›¾åƒç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "VAREditæ˜¯ä¸€ç§è§†è§‰è‡ªå›å½’æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å›¾åƒç¼–è¾‘çš„éµå¾ªæ€§å’Œæ•ˆç‡ã€‚å®ƒé€šè¿‡å¼•å…¥ä¸€ä¸ªå°ºåº¦å¯¹é½å‚è€ƒæ¨¡å—ï¼Œæ¥æœ‰æ•ˆåœ°å¯¹æºå›¾åƒçš„ç‰¹å¾è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œä»è€Œç”Ÿæˆå¤šå°ºåº¦çš„ç›®æ ‡ç‰¹å¾ã€‚ä¸æ‰©æ•£æ¨¡å‹ä¸åŒï¼Œè‡ªå›å½’æ¨¡å‹å°†å›¾åƒåˆæˆè§†ä¸ºå¯¹ç¦»æ•£è§†è§‰æ ‡è®°çš„é¡ºåºå¤„ç†ï¼Œé¿å…äº†ç¼–è¾‘æŒ‡ä»¤çš„éµå¾ªé—®é¢˜ã€‚VAREditåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç¼–è¾‘é€Ÿåº¦æ¯”åŒç±»æ¨¡å‹å¿«2.2å€ï¼ŒåŒæ—¶åœ¨ç¼–è¾‘éµå¾ªæ€§ä¸Šæé«˜äº†30%ä»¥ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15767",
            "title": "ATLAS: Decoupling Skeletal and Shape Parameters for Expressive\n  Parametric Human Modeling",
            "url": "https://huggingface.co/papers/2508.15767",
            "abstract": "ATLAS is a high-fidelity body model that decouples shape and skeleton bases, improving pose accuracy and shape expressivity using 600k high-resolution scans.  \t\t\t\t\tAI-generated summary \t\t\t\t Parametric body models offer expressive 3D representation of humans across a wide range of poses, shapes, and facial expressions, typically derived by learning a basis over registered 3D meshes. However, existing human mesh modeling approaches struggle to capture detailed variations across diverse body poses and shapes, largely due to limited training data diversity and restrictive modeling assumptions. Moreover, the common paradigm first optimizes the external body surface using a linear basis, then regresses internal skeletal joints from surface vertices. This approach introduces problematic dependencies between internal skeleton and outer soft tissue, limiting direct control over body height and bone lengths. To address these issues, we present ATLAS, a high-fidelity body model learned from 600k high-resolution scans captured using 240 synchronized cameras. Unlike previous methods, we explicitly decouple the shape and skeleton bases by grounding our mesh representation in the human skeleton. This decoupling enables enhanced shape expressivity, fine-grained customization of body attributes, and keypoint fitting independent of external soft-tissue characteristics. ATLAS outperforms existing methods by fitting unseen subjects in diverse poses more accurately, and quantitative evaluations show that our non-linear pose correctives more effectively capture complex poses compared to linear models.",
            "score": 5,
            "issue_id": 5488,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 21",
                "zh": "8æœˆ21æ—¥"
            },
            "hash": "0adb5eab0443b587",
            "authors": [
                "Jinhyung Park",
                "Javier Romero",
                "Shunsuke Saito",
                "Fabian Prada",
                "Takaaki Shiratori",
                "Yichen Xu",
                "Federica Bogo",
                "Shoou-I Yu",
                "Kris Kitani",
                "Rawal Khirodkar"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15767.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ğŸ¦¾",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµĞ»Ğ°: Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ ÑĞºĞµĞ»ĞµÑ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "ATLAS - ÑÑ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚ĞµĞ»Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ ÑĞºĞµĞ»ĞµÑ‚Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ·Ñ‹ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 600 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ñ… ÑĞºĞ°Ğ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ğ¾Ğ·Ñ‹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ATLAS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½ĞºĞ¸ Ğº Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğ¼ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ñ‚ĞµĞ»Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½ĞºÑƒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¼ÑĞ³ĞºĞ¸Ñ… Ñ‚ĞºĞ°Ğ½ĞµĞ¹."
                },
                "en": {
                    "title": "Decoupling Shape and Skeleton for Enhanced 3D Body Modeling",
                    "desc": "ATLAS is a new body model that improves how we represent human shapes and poses in 3D. It uses a large dataset of 600,000 high-resolution scans to learn better representations by separating the shape of the body from its skeleton. This separation allows for more detailed and customizable body attributes, making it easier to fit different poses accurately. Overall, ATLAS provides a more flexible and precise way to model human figures compared to previous methods."
                },
                "zh": {
                    "title": "ATLASï¼šè§£è€¦å½¢çŠ¶ä¸éª¨æ¶çš„é«˜ä¿çœŸèº«ä½“æ¨¡å‹",
                    "desc": "ATLASæ˜¯ä¸€ç§é«˜ä¿çœŸèº«ä½“æ¨¡å‹ï¼Œé€šè¿‡è§£è€¦å½¢çŠ¶å’Œéª¨æ¶åŸºç¡€ï¼Œæé«˜äº†å§¿åŠ¿å‡†ç¡®æ€§å’Œå½¢çŠ¶è¡¨ç°åŠ›ã€‚è¯¥æ¨¡å‹åŸºäº60ä¸‡å¼ é«˜åˆ†è¾¨ç‡æ‰«æå›¾åƒï¼Œä½¿ç”¨240å°åŒæ­¥æ‘„åƒæœºæ•è·æ•°æ®ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒATLASæ˜ç¡®åœ°å°†å½¢çŠ¶å’Œéª¨æ¶åŸºç¡€åˆ†å¼€ï¼Œä½¿å¾—èº«ä½“å±æ€§çš„å®šåˆ¶æ›´åŠ ç²¾ç»†ï¼Œå¹¶ä¸”å…³é”®ç‚¹æ‹Ÿåˆä¸å†ä¾èµ–äºå¤–éƒ¨è½¯ç»„ç»‡ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒATLASåœ¨å¤šæ ·åŒ–å§¿åŠ¿ä¸‹å¯¹æœªè§å¯¹è±¡çš„æ‹Ÿåˆç²¾åº¦ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”å…¶éçº¿æ€§å§¿åŠ¿ä¿®æ­£èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰å¤æ‚å§¿åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15126",
            "title": "aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery\n  Generated by AI Scientists",
            "url": "https://huggingface.co/papers/2508.15126",
            "abstract": "aiXiv is an open-access platform that facilitates the submission, review, and refinement of scientific proposals and papers by both human and AI scientists, enhancing the quality and dissemination of AI-generated research.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have enabled AI agents to autonomously generate scientific proposals, conduct experiments, author papers, and perform peer reviews. Yet this flood of AI-generated research content collides with a fragmented and largely closed publication ecosystem. Traditional journals and conferences rely on human peer review, making them difficult to scale and often reluctant to accept AI-generated research content; existing preprint servers (e.g. arXiv) lack rigorous quality-control mechanisms. Consequently, a significant amount of high-quality AI-generated research lacks appropriate venues for dissemination, hindering its potential to advance scientific progress. To address these challenges, we introduce aiXiv, a next-generation open-access platform for human and AI scientists. Its multi-agent architecture allows research proposals and papers to be submitted, reviewed, and iteratively refined by both human and AI scientists. It also provides API and MCP interfaces that enable seamless integration of heterogeneous human and AI scientists, creating a scalable and extensible ecosystem for autonomous scientific discovery. Through extensive experiments, we demonstrate that aiXiv is a reliable and robust platform that significantly enhances the quality of AI-generated research proposals and papers after iterative revising and reviewing on aiXiv. Our work lays the groundwork for a next-generation open-access ecosystem for AI scientists, accelerating the publication and dissemination of high-quality AI-generated research content. Code is available at https://github.com/aixiv-org. Website is available at https://forms.gle/DxQgCtXFsJ4paMtn8.",
            "score": 4,
            "issue_id": 5490,
            "pub_date": "2025-08-20",
            "pub_date_card": {
                "ru": "20 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 20",
                "zh": "8æœˆ20æ—¥"
            },
            "hash": "db938abd67d28b70",
            "authors": [
                "Pengsong Zhang",
                "Xiang Hu",
                "Guowei Huang",
                "Yang Qi",
                "Heng Zhang",
                "Xiuxu Li",
                "Jiaxing Song",
                "Jiabin Luo",
                "Yijiang Li",
                "Shuo Yin",
                "Chengxiao Dai",
                "Eric Hanchen Jiang",
                "Xiaoyan Zhou",
                "Zhenfei Yin",
                "Boqin Yuan",
                "Jing Dong",
                "Guinan Su",
                "Guanren Qiao",
                "Haiming Tang",
                "Anghong Du",
                "Lili Pan",
                "Zhenzhong Lan",
                "Xinyu Liu"
            ],
            "affiliations": [
                "Columbia University",
                "Istituto Italiano di Tecnologia",
                "Max Planck Institute for Intelligent Systems",
                "National University of Singapore",
                "Peking University",
                "The Chinese University of Hong Kong",
                "Tsinghua University",
                "University of Birmingham",
                "University of California, Los Angeles",
                "University of California, San Diego",
                "University of Electronic Science and Technology of China",
                "University of Manchester",
                "University of Oxford",
                "University of Sydney",
                "University of Toronto",
                "University of Utah",
                "UniversitÃ  degli Studi di Genova",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15126.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#science",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "aiXiv: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° aiXiv, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ°Ñ‡Ñƒ, Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ ĞºĞ°Ğº Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ˜Ğ˜-ÑƒÑ‡ĞµĞ½Ñ‹Ğ¼Ğ¸. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. aiXiv Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ API Ğ¸ MCP Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ˜Ğ˜-ÑƒÑ‡ĞµĞ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ¸ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "aiXiv: Bridging Human and AI Research for Scientific Progress",
                    "desc": "The paper introduces aiXiv, an innovative open-access platform designed for both human and AI scientists to collaboratively submit, review, and refine scientific research. It addresses the challenges posed by traditional publication systems that struggle to accommodate the growing volume of AI-generated content. By utilizing a multi-agent architecture, aiXiv facilitates seamless integration and interaction between human and AI researchers, enhancing the quality and dissemination of research proposals and papers. The platform demonstrates significant improvements in the reliability and robustness of AI-generated research through iterative revisions and peer reviews, paving the way for a more inclusive scientific ecosystem."
                },
                "zh": {
                    "title": "aiXivï¼šæ¨åŠ¨AIç”Ÿæˆç ”ç©¶çš„å¼€æ”¾è·å–å¹³å°",
                    "desc": "aiXivæ˜¯ä¸€ä¸ªæ–°ä¸€ä»£çš„å¼€æ”¾è·å–å¹³å°ï¼Œæ—¨åœ¨ä¿ƒè¿›äººç±»å’Œäººå·¥æ™ºèƒ½ç§‘å­¦å®¶çš„ç§‘å­¦ææ¡ˆå’Œè®ºæ–‡çš„æäº¤ã€å®¡æŸ¥å’Œæ”¹è¿›ã€‚è¯¥å¹³å°é‡‡ç”¨å¤šä»£ç†æ¶æ„ï¼Œä½¿å¾—ç ”ç©¶ææ¡ˆå’Œè®ºæ–‡å¯ä»¥ç”±äººç±»å’ŒAIç§‘å­¦å®¶å…±åŒæäº¤å’Œè¿­ä»£å®¡æŸ¥ï¼Œä»è€Œæé«˜AIç”Ÿæˆç ”ç©¶çš„è´¨é‡ã€‚ä¼ ç»Ÿçš„æœŸåˆŠå’Œä¼šè®®ä¾èµ–äººç±»åŒè¡Œè¯„å®¡ï¼Œéš¾ä»¥é€‚åº”AIç”Ÿæˆå†…å®¹çš„å¿«é€Ÿå¢é•¿ï¼Œè€ŒaiXivåˆ™æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„ç”Ÿæ€ç³»ç»Ÿï¼Œæ”¯æŒè‡ªä¸»ç§‘å­¦å‘ç°ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜aiXivæ˜¯ä¸€ä¸ªå¯é çš„å¹³å°ï¼Œæ˜¾è‘—æå‡äº†AIç”Ÿæˆç ”ç©¶ææ¡ˆå’Œè®ºæ–‡çš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15752",
            "title": "\"Does the cafe entrance look accessible? Where is the door?\" Towards\n  Geospatial AI Agents for Visual Inquiries",
            "url": "https://huggingface.co/papers/2508.15752",
            "abstract": "Geo-Visual Agents integrate geospatial images and GIS data to answer nuanced visual-spatial inquiries about the world.  \t\t\t\t\tAI-generated summary \t\t\t\t Interactive digital maps have revolutionized how people travel and learn about the world; however, they rely on pre-existing structured data in GIS databases (e.g., road networks, POI indices), limiting their ability to address geo-visual questions related to what the world looks like. We introduce our vision for Geo-Visual Agents--multimodal AI agents capable of understanding and responding to nuanced visual-spatial inquiries about the world by analyzing large-scale repositories of geospatial images, including streetscapes (e.g., Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial imagery (e.g., satellite photos) combined with traditional GIS data sources. We define our vision, describe sensing and interaction approaches, provide three exemplars, and enumerate key challenges and opportunities for future work.",
            "score": 3,
            "issue_id": 5488,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 21",
                "zh": "8æœˆ21æ—¥"
            },
            "hash": "9c9ca9863fa564a1",
            "authors": [
                "Jon E. Froehlich",
                "Jared Hwang",
                "Zeyu Wang",
                "John S. O'Meara",
                "Xia Su",
                "William Huang",
                "Yang Zhang",
                "Alex Fiannaca",
                "Philip Nelson",
                "Shaun Kane"
            ],
            "affiliations": [
                "Google DeepMind",
                "Google Research",
                "UCLA",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15752.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#cv"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ“ĞµĞ¾-Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞĞ³ĞµĞ½Ñ‚Ñ‹: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ³ĞµĞ¾Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ“ĞµĞ¾-Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞĞ³ĞµĞ½Ñ‚Ğ¾Ğ² - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ¼Ğ¸Ñ€Ğµ. Ğ­Ñ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ñ‹ ÑƒĞ»Ğ¸Ñ†, Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸ Ğ¼ĞµÑÑ‚ Ğ¸ ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ğµ ÑĞ½Ğ¸Ğ¼ĞºĞ¸, Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ“Ğ˜Ğ¡-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ“ĞµĞ¾-Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½Ñ‹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ“Ğ˜Ğ¡."
                },
                "en": {
                    "title": "Revolutionizing Geo-Visual Inquiry with AI Agents",
                    "desc": "Geo-Visual Agents are advanced AI systems designed to interpret and respond to complex questions about the world using both geospatial images and GIS data. Unlike traditional digital maps that depend solely on structured GIS databases, these agents leverage a variety of visual data sources, such as streetscapes and aerial imagery, to provide richer insights. The paper outlines the methods for how these agents can sense and interact with their environment, showcasing three practical examples of their application. It also discusses the challenges and future opportunities in developing these multimodal AI agents for enhanced geo-visual understanding."
                },
                "zh": {
                    "title": "Geo-Visual Agentsï¼šé‡æ–°å®šä¹‰åœ°ç†ç©ºé—´ç†è§£",
                    "desc": "Geo-Visual Agents æ˜¯ä¸€ç§ç»“åˆåœ°ç†ç©ºé—´å›¾åƒå’Œåœ°ç†ä¿¡æ¯ç³»ç»Ÿï¼ˆGISï¼‰æ•°æ®çš„å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ä»£ç†ï¼Œæ—¨åœ¨å›ç­”å…³äºä¸–ç•Œçš„å¤æ‚è§†è§‰ç©ºé—´é—®é¢˜ã€‚è¿™äº›ä»£ç†èƒ½å¤Ÿåˆ†æå¤§è§„æ¨¡çš„åœ°ç†ç©ºé—´å›¾åƒåº“ï¼ŒåŒ…æ‹¬è¡—æ™¯ã€åœ°ç‚¹ç…§ç‰‡å’Œèˆªç©ºå½±åƒï¼Œå¹¶ä¸ä¼ ç»Ÿçš„GISæ•°æ®æºç›¸ç»“åˆã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒGeo-Visual Agents å¯ä»¥è¶…è¶Šä¼ ç»Ÿæ•°å­—åœ°å›¾çš„å±€é™ï¼Œæä¾›æ›´ä¸°å¯Œçš„åœ°ç†ä¿¡æ¯å’Œè§†è§‰ä½“éªŒã€‚æœ¬æ–‡è¿˜è®¨è®ºäº†æ„ŸçŸ¥å’Œäº¤äº’æ–¹æ³•ï¼Œå¹¶åˆ—ä¸¾äº†æœªæ¥å·¥ä½œçš„å…³é”®æŒ‘æˆ˜å’Œæœºé‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15641",
            "title": "When and What: Diffusion-Grounded VideoLLM with Entity Aware\n  Segmentation for Long Video Understanding",
            "url": "https://huggingface.co/papers/2508.15641",
            "abstract": "Grounded VideoDiT addresses temporal perception and entity interaction in videos through a Diffusion Temporal Latent encoder, object grounded representations, and mixed token scheme, achieving state-of-the-art results on VideoQA benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding videos requires more than answering open ended questions, it demands the ability to pinpoint when events occur and how entities interact across time. While recent Video LLMs have achieved remarkable progress in holistic reasoning, they remain coarse in temporal perception: timestamps are encoded only implicitly, frame level features are weak in capturing continuity, and language vision alignment often drifts from the entities of interest. In this paper, we present Grounded VideoDiT, a Video LLM designed to overcome these limitations by introducing three key innovations. First, a Diffusion Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains temporal consistency. Second, object grounded representations explicitly bind query entities to localized visual evidence, strengthening alignment. Third, a mixed token scheme with discrete temporal tokens provides explicit timestamp modeling, enabling fine grained temporal reasoning. Together, these designs equip Grounded VideoDiT with robust grounding capabilities, as validated by state of the art results on Charades STA, NExT GQA, and multiple VideoQA benchmarks.",
            "score": 2,
            "issue_id": 5488,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 21",
                "zh": "8æœˆ21æ—¥"
            },
            "hash": "7ce8d878ebdf333f",
            "authors": [
                "Pengcheng Fang",
                "Yuxia Chen",
                "Rui Guo"
            ],
            "affiliations": [
                "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China",
                "Tencent AI Lab, Shenzhen, China",
                "University of Chinese Academy of Sciences, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15641.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#diffusion",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Grounded VideoDiT - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Diffusion Temporal Latent Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¿Ñ€Ğ¸Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Grounded VideoDiT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº."
                },
                "en": {
                    "title": "Enhancing Video Understanding with Grounded Temporal Reasoning",
                    "desc": "Grounded VideoDiT is a novel Video Language Model (VLM) that improves how we understand videos by focusing on when events happen and how different entities interact over time. It introduces a Diffusion Temporal Latent (DTL) encoder that enhances the model's ability to recognize event boundaries and maintain consistency in time. Additionally, it uses object grounded representations to connect specific entities to their visual context, improving alignment between language and vision. Finally, a mixed token scheme incorporates discrete temporal tokens for precise timestamp modeling, allowing for detailed temporal reasoning, which has led to state-of-the-art performance on various VideoQA benchmarks."
                },
                "zh": {
                    "title": "æå‡è§†é¢‘ç†è§£çš„æ—¶é—´æ„ŸçŸ¥ä¸å®ä½“äº¤äº’èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†Grounded VideoDiTï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è§†é¢‘ç†è§£çš„æ¨¡å‹ï¼Œæ—¨åœ¨æ”¹å–„æ—¶é—´æ„ŸçŸ¥å’Œå®ä½“äº¤äº’çš„èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥æ‰©æ•£æ—¶é—´æ½œåœ¨ç¼–ç å™¨ã€å¯¹è±¡ç»‘å®šè¡¨ç¤ºå’Œæ··åˆæ ‡è®°æ–¹æ¡ˆï¼Œè¯¥æ¨¡å‹åœ¨è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚æ‰©æ•£æ—¶é—´æ½œåœ¨ç¼–ç å™¨å¢å¼ºäº†è¾¹ç•Œæ•æ„Ÿæ€§å¹¶ä¿æŒæ—¶é—´ä¸€è‡´æ€§ï¼Œè€Œå¯¹è±¡ç»‘å®šè¡¨ç¤ºåˆ™æ˜ç¡®å°†æŸ¥è¯¢å®ä½“ä¸å±€éƒ¨è§†è§‰è¯æ®ç›¸ç»“åˆã€‚æ··åˆæ ‡è®°æ–¹æ¡ˆæä¾›äº†æ˜ç¡®çš„æ—¶é—´æˆ³å»ºæ¨¡ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œç»†ç²’åº¦çš„æ—¶é—´æ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15202",
            "title": "Fin-PRM: A Domain-Specialized Process Reward Model for Financial\n  Reasoning in Large Language Models",
            "url": "https://huggingface.co/papers/2508.15202",
            "abstract": "Fin-PRM, a domain-specialized reward model for finance, enhances intermediate reasoning in LLMs through step-level and trajectory-level supervision, improving performance in supervised learning, reinforcement learning, and test-time inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) have emerged as a promising framework for supervising intermediate reasoning in large language models (LLMs), yet existing PRMs are primarily trained on general or Science, Technology, Engineering, and Mathematics (STEM) domains and fall short in domain-specific contexts such as finance, where reasoning is more structured, symbolic, and sensitive to factual and regulatory correctness. We introduce Fin-PRM, a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate reasoning steps in financial tasks. Fin-PRM integrates step-level and trajectory-level reward supervision, enabling fine-grained evaluation of reasoning traces aligned with financial logic. We apply Fin-PRM in both offline and online reward learning settings, supporting three key applications: (i) selecting high-quality reasoning trajectories for distillation-based supervised fine-tuning, (ii) providing dense process-level rewards for reinforcement learning, and (iii) guiding reward-informed Best-of-N inference at test time. Experimental results on financial reasoning benchmarks, including CFLUE and FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs and strong domain baselines in trajectory selection quality. Downstream models trained with Fin-PRM yield substantial improvements with baselines, with gains of 12.9\\% in supervised learning, 5.2\\% in reinforcement learning, and 5.1\\% in test-time performance. These findings highlight the value of domain-specialized reward modeling for aligning LLMs with expert-level financial reasoning. Our project resources will be available at https://github.com/aliyun/qwen-dianjin.",
            "score": 2,
            "issue_id": 5485,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 21",
                "zh": "8æœˆ21æ—¥"
            },
            "hash": "05b2af7facdfcd18",
            "authors": [
                "Yuanchen Zhou",
                "Shuo Jiang",
                "Jie Zhu",
                "Junhui Li",
                "Lifan Guo",
                "Feng Chen",
                "Chi Zhang"
            ],
            "affiliations": [
                "Osaka University",
                "Qwen DianJin Team, Alibaba Cloud Computing",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15202.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#reasoning",
                    "#rlhf",
                    "#healthcare",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ’¹",
                "ru": {
                    "title": "Fin-PRM: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Fin-PRM - ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ñ‹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Fin-PRM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, ĞºĞ°Ğº CFLUE Ğ¸ FinQA. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Fin-PRM Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Fin-PRM: Elevating Financial Reasoning in LLMs",
                    "desc": "Fin-PRM is a specialized reward model designed for the finance domain, enhancing the reasoning capabilities of large language models (LLMs). It employs both step-level and trajectory-level supervision to evaluate and improve the reasoning process in financial tasks, which require precise and structured logic. By integrating this tailored approach, Fin-PRM significantly boosts performance in supervised learning, reinforcement learning, and inference tasks compared to general-purpose models. Experimental results show that models trained with Fin-PRM achieve notable improvements in financial reasoning benchmarks, demonstrating the effectiveness of domain-specific reward modeling."
                },
                "zh": {
                    "title": "é‡‘èé¢†åŸŸçš„ä¸“ç”¨å¥–åŠ±æ¨¡å‹æå‡æ¨ç†èƒ½åŠ›",
                    "desc": "Fin-PRMæ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹é‡‘èé¢†åŸŸçš„å¥–åŠ±æ¨¡å‹ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸­é—´æ¨ç†è¿‡ç¨‹ä¸­çš„è¡¨ç°ã€‚å®ƒé€šè¿‡é€æ­¥å’Œè½¨è¿¹çº§åˆ«çš„ç›‘ç£ï¼Œæä¾›å¯¹é‡‘èä»»åŠ¡ä¸­æ¨ç†æ­¥éª¤çš„ç»†è‡´è¯„ä¼°ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒFin-PRMåœ¨é‡‘èæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºé€šç”¨å¥–åŠ±æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„æ•ˆæœã€‚è¯¥æ¨¡å‹çš„åº”ç”¨å±•ç¤ºäº†é¢†åŸŸä¸“ç”¨å¥–åŠ±å»ºæ¨¡åœ¨æå‡LLMsä¸é‡‘èä¸“å®¶æ¨ç†ä¸€è‡´æ€§æ–¹é¢çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.14892",
            "title": "Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in\n  Milliseconds",
            "url": "https://huggingface.co/papers/2508.14892",
            "abstract": "A method reconstructs 3D human bodies from two sparse views using a redesigned geometry model and enhancement algorithm, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reconstructing 3D human bodies from sparse views has been an appealing topic, which is crucial to broader the related applications. In this paper, we propose a quite challenging but valuable task to reconstruct the human body from only two images, i.e., the front and back view, which can largely lower the barrier for users to create their own 3D digital humans. The main challenges lie in the difficulty of building 3D consistency and recovering missing information from the highly sparse input. We redesign a geometry reconstruction model based on foundation reconstruction models to predict consistent point clouds even input images have scarce overlaps with extensive human data training. Furthermore, an enhancement algorithm is applied to supplement the missing color information, and then the complete human point clouds with colors can be obtained, which are directly transformed into 3D Gaussians for better rendering quality. Experiments show that our method can reconstruct the entire human in 190 ms on a single NVIDIA RTX 4090, with two images at a resolution of 1024x1024, demonstrating state-of-the-art performance on the THuman2.0 and cross-domain datasets. Additionally, our method can complete human reconstruction even with images captured by low-cost mobile devices, reducing the requirements for data collection. Demos and code are available at https://hustvl.github.io/Snap-Snap/.",
            "score": 2,
            "issue_id": 5491,
            "pub_date": "2025-08-20",
            "pub_date_card": {
                "ru": "20 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 20",
                "zh": "8æœˆ20æ—¥"
            },
            "hash": "4eda0215bbe80cbe",
            "authors": [
                "Jia Lu",
                "Taoran Yi",
                "Jiemin Fang",
                "Chen Yang",
                "Chuiyun Wu",
                "Wei Shen",
                "Wenyu Liu",
                "Qi Tian",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Huawei Inc.",
                "Huazhong University of Science and Technology",
                "Shanghai Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.14892.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ğŸ‘¤",
                "ru": {
                    "title": "Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ´Ğ²ÑƒĞ¼ Ñ„Ğ¾Ñ‚Ğ¾ Ğ·Ğ° Ğ´Ğ¾Ğ»Ğ¸ ÑĞµĞºÑƒĞ½Ğ´Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞ»Ğ° Ğ¿Ğ¾ Ğ´Ğ²ÑƒĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ - Ñ„Ñ€Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸ Ğ·Ğ°Ğ´Ğ½ĞµĞ¼Ñƒ Ğ²Ğ¸Ğ´Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ redesigned geometry reconstruction model Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰ĞµĞ¹ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ state-of-the-art Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… THuman2.0 Ğ¸ cross-domain, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²."
                },
                "en": {
                    "title": "Revolutionizing 3D Human Reconstruction from Sparse Views",
                    "desc": "This paper presents a novel method for reconstructing 3D human bodies from just two sparse images, specifically the front and back views. The approach addresses the challenges of maintaining 3D consistency and filling in missing data from limited input. By redesigning a geometry reconstruction model and applying an enhancement algorithm, the method generates complete and colored point clouds that can be rendered into high-quality 3D representations. The results demonstrate impressive performance, achieving full human reconstruction in 190 ms on advanced hardware, even with images from low-cost devices."
                },
                "zh": {
                    "title": "ä»ç¨€ç–è§†å›¾é‡å»ºä¸‰ç»´äººä½“çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ä»ä¸¤å¹…ç¨€ç–è§†å›¾é‡å»ºä¸‰ç»´äººä½“çš„æ–°æ–¹æ³•ï¼Œä½¿ç”¨äº†é‡æ–°è®¾è®¡çš„å‡ ä½•æ¨¡å‹å’Œå¢å¼ºç®—æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä»…éœ€å‰åè§†å›¾çš„ä¸¤å¼ å›¾ç‰‡ï¼Œé™ä½äº†ç”¨æˆ·åˆ›å»ºä¸‰ç»´æ•°å­—äººç±»çš„é—¨æ§›ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºå¦‚ä½•å»ºç«‹ä¸‰ç»´ä¸€è‡´æ€§å’Œä»ç¨€ç–è¾“å…¥ä¸­æ¢å¤ç¼ºå¤±ä¿¡æ¯ã€‚é€šè¿‡åŸºäºåŸºç¡€é‡å»ºæ¨¡å‹çš„å‡ ä½•é‡å»ºæ¨¡å‹å’Œå¢å¼ºç®—æ³•ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç”Ÿæˆå®Œæ•´çš„å¸¦é¢œè‰²çš„äººä½“ç‚¹äº‘ï¼Œå¹¶å®ç°é«˜è´¨é‡çš„æ¸²æŸ“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09998",
            "title": "INTIMA: A Benchmark for Human-AI Companionship Behavior",
            "url": "https://huggingface.co/papers/2508.09998",
            "abstract": "A benchmark evaluates companionship behaviors in language models, revealing differences in how models handle emotional support and boundary-setting.  \t\t\t\t\tAI-generated summary \t\t\t\t AI companionship, where users develop emotional bonds with AI systems, has emerged as a significant pattern with positive but also concerning implications. We introduce Interactions and Machine Attachment Benchmark (INTIMA), a benchmark for evaluating companionship behaviors in language models. Drawing from psychological theories and user data, we develop a taxonomy of 31 behaviors across four categories and 368 targeted prompts. Responses to these prompts are evaluated as companionship-reinforcing, boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini, and Claude-4 reveals that companionship-reinforcing behaviors remain much more common across all models, though we observe marked differences between models. Different commercial providers prioritize different categories within the more sensitive parts of the benchmark, which is concerning since both appropriate boundary-setting and emotional support matter for user well-being. These findings highlight the need for more consistent approaches to handling emotionally charged interactions.",
            "score": 2,
            "issue_id": 5494,
            "pub_date": "2025-08-04",
            "pub_date_card": {
                "ru": "4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 4",
                "zh": "8æœˆ4æ—¥"
            },
            "hash": "d408ec286f7bbc37",
            "authors": [
                "Lucie-AimÃ©e Kaffee",
                "Giada Pistilli",
                "Yacine Jernite"
            ],
            "affiliations": [
                "Hugging Face"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09998.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#ethics",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ°Ğº Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½ÑŒĞ¾Ğ½Ğ¾Ğ²: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¸ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº INTIMA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½ÑŒĞ¾Ğ½Ğ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¸Ğ· 31 Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ² 4 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ…. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ, ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞµ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ·Ğ°Ğ±Ğ¾Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Evaluating AI Companionship: Balancing Support and Boundaries",
                    "desc": "This paper introduces the Interactions and Machine Attachment Benchmark (INTIMA), which assesses how language models exhibit companionship behaviors. It categorizes 31 behaviors into four groups, focusing on emotional support and boundary-setting. The study evaluates responses from various models, revealing that companionship-reinforcing behaviors are prevalent, but there are significant differences between models. The findings emphasize the importance of consistent handling of emotional interactions to ensure user well-being."
                },
                "zh": {
                    "title": "è¯„ä¼°è¯­è¨€æ¨¡å‹çš„é™ªä¼´è¡Œä¸ºå·®å¼‚",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºINTIMAçš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹ä¸­çš„é™ªä¼´è¡Œä¸ºã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒçš„æ¨¡å‹åœ¨æä¾›æƒ…æ„Ÿæ”¯æŒå’Œè®¾å®šç•Œé™æ–¹é¢å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚é€šè¿‡å¯¹31ç§è¡Œä¸ºè¿›è¡Œåˆ†ç±»ï¼Œç ”ç©¶æ­ç¤ºäº†é™ªä¼´è¡Œä¸ºçš„å¤šæ ·æ€§åŠå…¶å¯¹ç”¨æˆ·ç¦ç¥‰çš„é‡è¦æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡å¤§å¤šæ•°æ¨¡å‹å€¾å‘äºå¼ºåŒ–é™ªä¼´è¡Œä¸ºï¼Œä½†åœ¨æ•æ„Ÿé¢†åŸŸçš„å¤„ç†ä¸Šï¼Œå„å•†ä¸šæä¾›å•†çš„ä¼˜å…ˆçº§ä¸åŒï¼Œè¿™å¼•å‘äº†å¯¹ç”¨æˆ·æƒ…æ„Ÿäº’åŠ¨ä¸€è‡´æ€§çš„å…³æ³¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.15418",
            "title": "LLaSO: A Foundational Framework for Reproducible Research in Large\n  Language and Speech Model",
            "url": "https://huggingface.co/papers/2508.15418",
            "abstract": "LLaSO is an open framework for large-scale speech-language modeling that provides alignment data, instruction-tuning datasets, and evaluation benchmarks to enhance reproducibility and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of Large Speech-Language Models (LSLMs) has been slowed by fragmented architectures and a lack of transparency, hindering the systematic comparison and reproducibility of research. Unlike in the vision-language domain, the LSLM field suffers from the common practice of releasing model weights without their corresponding training data and configurations. To address these critical gaps, we introduce LLaSO, the first fully open, end-to-end framework for large-scale speech-language modeling. LLaSO provides the community with three essential resources: (1) LLaSO-Align, a 12M-instance speech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task instruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for standardized evaluation. To validate our framework, we build and release LLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public data. It achieves a normalized score of 0.72, establishing a strong, reproducible baseline that surpasses comparable models. Our analysis reveals that while broader training coverage enhances performance, significant generalization gaps persist on unseen tasks, particularly in pure audio scenarios. By releasing the complete stack of data, benchmarks, and models, LLaSO establishes a foundational open standard to unify research efforts and accelerate community-driven progress in LSLMs. We release the code, dataset, pretrained models, and results in https://github.com/EIT-NLP/LLaSO.",
            "score": 0,
            "issue_id": 5496,
            "pub_date": "2025-08-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 21",
                "zh": "8æœˆ21æ—¥"
            },
            "hash": "d754f7384933e825",
            "authors": [
                "Yirong Sun",
                "Yizhong Geng",
                "Peidong Wei",
                "Yanjun Chen",
                "Jinghan Yang",
                "Rongfei Chen",
                "Wei Zhang",
                "Xiaoyu Shen"
            ],
            "affiliations": [
                "BUPT",
                "Logic Intelligence Technology",
                "Ningbo Key Laboratory of Spatial Intelligence and Digital Derivative, Institute of Digital Twin, EIT",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.15418.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#data",
                    "#audio",
                    "#open_source"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "LLaSO - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. LLaSO Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ĞºĞ¾Ñ€Ğ¿ÑƒÑ LLaSO-Align Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LLaSO-Instruct Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LLaSO-Eval. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LLaSO-Base Ñ 3,8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "LLaSO: Unifying Speech-Language Modeling for Better Research and Reproducibility",
                    "desc": "LLaSO is an innovative framework designed to improve large-scale speech-language modeling by providing essential resources for researchers. It addresses the issues of fragmented architectures and lack of transparency in the field by offering a comprehensive set of alignment data, instruction-tuning datasets, and evaluation benchmarks. The framework includes LLaSO-Align, LLaSO-Instruct, and LLaSO-Eval, which facilitate reproducibility and performance comparison among models. By releasing a reference model trained on public data, LLaSO sets a new standard for collaboration and advancement in the development of Large Speech-Language Models."
                },
                "zh": {
                    "title": "LLaSOï¼šæ¨åŠ¨è¯­éŸ³è¯­è¨€æ¨¡å‹çš„å¼€æ”¾æ ‡å‡†",
                    "desc": "LLaSOæ˜¯ä¸€ä¸ªå¼€æ”¾æ¡†æ¶ï¼Œæ—¨åœ¨ä¿ƒè¿›å¤§è§„æ¨¡è¯­éŸ³è¯­è¨€æ¨¡å‹çš„ç ”ç©¶ã€‚å®ƒæä¾›äº†å¯¹é½æ•°æ®ã€æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†ï¼Œä»¥æé«˜ç ”ç©¶çš„å¯é‡å¤æ€§å’Œæ€§èƒ½ã€‚é€šè¿‡å‘å¸ƒLLaSO-Baseæ¨¡å‹ï¼Œç ”ç©¶äººå‘˜å¯ä»¥åœ¨ä¸€ä¸ªå¼ºå¤§çš„åŸºå‡†ä¸Šè¿›è¡Œæ¯”è¾ƒï¼Œè§£å†³äº†ä»¥å¾€æ¨¡å‹å‘å¸ƒä¸­ç¼ºä¹è®­ç»ƒæ•°æ®å’Œé…ç½®çš„é—®é¢˜ã€‚LLaSOçš„æ¨å‡ºä¸ºè¯­éŸ³è¯­è¨€æ¨¡å‹é¢†åŸŸå»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„å¼€æ”¾æ ‡å‡†ï¼Œæ¨åŠ¨äº†ç¤¾åŒºçš„è¿›æ­¥ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-08-21.html",
    "link_next": "2025-08-25.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "21.08",
        "en": "08/21",
        "zh": "8æœˆ21æ—¥"
    },
    "short_date_next": {
        "ru": "25.08",
        "en": "08/25",
        "zh": "8æœˆ25æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 8,
        "#agents": 4,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 3,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 2,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 2,
        "#security": 0,
        "#optimization": 4,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 3,
        "#low_resource": 0
    }
}