{
    "date": {
        "ru": "30 октября",
        "en": "October 30",
        "zh": "10月30日"
    },
    "time_utc": "2025-10-30 02:28",
    "weekday": 3,
    "issue_id": 6690,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.25726",
            "title": "The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic,\n  and Long-Horizon Task Execution",
            "url": "https://huggingface.co/papers/2510.25726",
            "abstract": "Real-world language agents must handle complex, multi-step workflows across diverse Apps. For instance, an agent may manage emails by coordinating with calendars and file systems, or monitor a production database to detect anomalies and generate reports following an operating manual. However, existing language agent benchmarks often focus on narrow domains or simplified tasks that lack the diversity, realism, and long-horizon complexity required to evaluate agents' real-world performance. To address this gap, we introduce the Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering diverse Apps and tools, realistic environment setup, and reliable execution-based evaluation. Toolathlon spans 32 software applications and 604 tools, ranging from everyday platforms such as Google Calendar and Notion to professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools are based on a high-quality set of Model Context Protocol (MCP) servers that we may have revised or implemented ourselves. Unlike prior works, which primarily ensure functional realism but offer limited environment state diversity, we provide realistic initial environment states from real software, such as Canvas courses with dozens of students or real financial spreadsheets. This benchmark includes 108 manually sourced or crafted tasks in total, requiring interacting with multiple Apps over around 20 turns on average to complete. Each task is strictly verifiable through dedicated evaluation scripts. Comprehensive evaluation of SOTA models highlights their significant shortcomings: the best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate with 20.2 tool calling turns on average, while the top open-weights model DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development of more capable language agents for real-world, long-horizon task execution.",
            "score": 6,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "ee2a3f9cd44f7f99",
            "authors": [
                "Junlong Li",
                "Wenshuo Zhao",
                "Jian Zhao",
                "Weihao Zeng",
                "Haoze Wu",
                "Xiaochen Wang",
                "Rui Ge",
                "Yuxuan Cao",
                "Yuzhen Huang",
                "Wei Liu",
                "Junteng Liu",
                "Zhaochen Su",
                "Yiyang Guo",
                "Fan Zhou",
                "Lueyang Zhang",
                "Juan Michelini",
                "Xingyao Wang",
                "Xiang Yue",
                "Shuyan Zhou",
                "Graham Neubig",
                "Junxian He"
            ],
            "affiliations": [
                "All Hands AI",
                "Carnegie Mellon University",
                "Duke University",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25726.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#benchmark",
                    "#agi"
                ],
                "emoji": "🏅",
                "ru": {
                    "title": "Десятиборье для AI-агентов: новый стандарт проверки на реальных задачах",
                    "desc": "Исследователи представили Toolathlon — новый бенчмарк для оценки языковых агентов, способных выполнять сложные многошаговые задачи в реальных приложениях. Бенчмарк включает 32 программных приложения (от Google Calendar до Kubernetes), 604 инструмента и 108 задач, требующих в среднем 20 шагов взаимодействия с несколькими приложениями. В отличие от существующих бенчмарков, Toolathlon использует реалистичные начальные состояния окружения из настоящего софта и строгую автоматическую проверку результатов. Лучшая современная модель Claude-4.5-Sonnet достигает лишь 38.6% успешности, что демонстрирует значительные ограничения текущих AI-агентов в решении реальных задач."
                },
                "en": {
                    "title": "Toolathlon: Elevating Language Agents for Real-World Challenges",
                    "desc": "This paper introduces Toolathlon, a new benchmark designed to evaluate language agents in complex, multi-step workflows across various applications. Unlike previous benchmarks that focus on narrow tasks, Toolathlon includes 32 software applications and 604 tools, providing a realistic environment for testing. The benchmark features 108 tasks that require agents to interact with multiple apps over an average of 20 turns, ensuring a comprehensive evaluation of their capabilities. Initial results show that current state-of-the-art models struggle with these tasks, highlighting the need for improved language agents that can handle real-world scenarios effectively."
                },
                "zh": {
                    "title": "Toolathlon：评估语言代理的新基准",
                    "desc": "本论文介绍了一个新的基准测试工具，称为Tool Decathlon（Toolathlon），旨在评估语言代理在复杂多步骤工作流中的表现。该基准涵盖32个软件应用和604个工具，提供真实的环境设置和可靠的执行评估。与以往的研究不同，Toolathlon提供了多样化的环境状态和真实的任务场景，要求代理与多个应用进行交互。通过对现有模型的全面评估，发现它们在处理复杂任务时存在显著不足，Toolathlon的推出有望推动更强大的语言代理的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.19195",
            "title": "Rethinking Driving World Model as Synthetic Data Generator for\n  Perception Tasks",
            "url": "https://huggingface.co/papers/2510.19195",
            "abstract": "Dream4Drive is a synthetic data generation framework that enhances downstream perception tasks in autonomous driving by decomposing videos into 3D-aware guidance maps and rendering 3D assets, leading to improved performance across various training epochs.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in driving world models enable controllable generation of high-quality RGB videos or multimodal videos. Existing methods primarily focus on metrics related to generation quality and controllability. However, they often overlook the evaluation of downstream perception tasks, which are really crucial for the performance of autonomous driving. Existing methods usually leverage a training strategy that first pretrains on synthetic data and finetunes on real data, resulting in twice the epochs compared to the baseline (real data only). When we double the epochs in the baseline, the benefit of synthetic data becomes negligible. To thoroughly demonstrate the benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data generation framework designed for enhancing the downstream perception tasks. Dream4Drive first decomposes the input video into several 3D-aware guidance maps and subsequently renders the 3D assets onto these guidance maps. Finally, the driving world model is fine-tuned to produce the edited, multi-view photorealistic videos, which can be used to train the downstream perception models. Dream4Drive enables unprecedented flexibility in generating multi-view corner cases at scale, significantly boosting corner case perception in autonomous driving. To facilitate future research, we also contribute a large-scale 3D asset dataset named DriveObj3D, covering the typical categories in driving scenarios and enabling diverse 3D-aware video editing. We conduct comprehensive experiments to show that Dream4Drive can effectively boost the performance of downstream perception models under various training epochs. Page: https://wm-research.github.io/Dream4Drive/ GitHub Link: https://github.com/wm-research/Dream4Drive",
            "score": 6,
            "issue_id": 6690,
            "pub_date": "2025-10-22",
            "pub_date_card": {
                "ru": "22 октября",
                "en": "October 22",
                "zh": "10月22日"
            },
            "hash": "66a46c2275824bb0",
            "authors": [
                "Kai Zeng",
                "Zhanqian Wu",
                "Kaixin Xiong",
                "Xiaobao Wei",
                "Xiangyu Guo",
                "Zhenxin Zhu",
                "Kalok Ho",
                "Lijun Zhou",
                "Bohan Zeng",
                "Ming Lu",
                "Haiyang Sun",
                "Bing Wang",
                "Guang Chen",
                "Hangjun Ye",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Peking University",
                "Xiaomi EV"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.19195.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#3d",
                    "#dataset",
                    "#agents",
                    "#synthetic"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Генерация синтетических данных для обучения систем автопилота через 3D-рендеринг",
                    "desc": "Dream4Drive - это фреймворк для генерации синтетических данных, который улучшает задачи восприятия в автономном вождении. Метод разбивает видео на 3D-aware карты guidance и рендерит 3D-ассеты, создавая реалистичные мультивидовые видео для обучения моделей восприятия. В отличие от существующих подходов, Dream4Drive демонстрирует стабильное улучшение производительности при любом количестве эпох обучения, особенно эффективно генерируя corner cases в больших масштабах. Авторы также представили датасет DriveObj3D с 3D-ассетами типичных объектов для сценариев вождения."
                },
                "en": {
                    "title": "Enhancing Autonomous Driving Perception with Dream4Drive",
                    "desc": "Dream4Drive is a synthetic data generation framework aimed at improving perception tasks in autonomous driving. It works by breaking down videos into 3D-aware guidance maps and rendering 3D assets, which enhances the training of perception models. This approach allows for the generation of high-quality, multi-view photorealistic videos that can be used to train models more effectively than traditional methods. Additionally, Dream4Drive introduces a large-scale dataset, DriveObj3D, to support diverse 3D-aware video editing and further research in the field."
                },
                "zh": {
                    "title": "Dream4Drive：提升自动驾驶感知的合成数据生成框架",
                    "desc": "Dream4Drive是一个合成数据生成框架，旨在通过将视频分解为3D感知引导图并渲染3D资产，来增强自动驾驶中的下游感知任务。该框架能够生成高质量的多视角视频，从而显著提高角落案例的感知能力。通过引入DriveObj3D数据集，Dream4Drive为未来的研究提供了丰富的3D资产，支持多样化的3D感知视频编辑。实验结果表明，Dream4Drive在不同训练周期下有效提升了下游感知模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.23473",
            "title": "Video-Thinker: Sparking \"Thinking with Videos\" via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2510.23473",
            "abstract": "Video-Thinker, a multimodal large language model, autonomously reasons with videos using intrinsic grounding and captioning capabilities, achieving state-of-the-art performance on various video reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in image reasoning methods, particularly \"Thinking with Images\", have demonstrated remarkable success in Multimodal Large Language Models (MLLMs); however, this dynamic reasoning paradigm has not yet been extended to video reasoning tasks. In this paper, we propose Video-Thinker, which empowers MLLMs to think with videos by autonomously leveraging their intrinsic \"grounding\" and \"captioning\" capabilities to generate reasoning clues throughout the inference process. To spark this capability, we construct Video-Thinker-10K, a curated dataset featuring autonomous tool usage within chain-of-thought reasoning sequences. Our training strategy begins with Supervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group Relative Policy Optimization (GRPO) to strengthen this reasoning capability. Through this approach, Video-Thinker enables MLLMs to autonomously navigate grounding and captioning tasks for video reasoning, eliminating the need for constructing and calling external tools. Extensive experiments demonstrate that Video-Thinker achieves significant performance gains on both in-domain tasks and challenging out-of-domain video reasoning benchmarks, including Video-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B substantially outperforms existing baselines such as Video-R1 and establishes state-of-the-art performance among 7B-sized MLLMs.",
            "score": 4,
            "issue_id": 6690,
            "pub_date": "2025-10-27",
            "pub_date_card": {
                "ru": "27 октября",
                "en": "October 27",
                "zh": "10月27日"
            },
            "hash": "448c0141074addb0",
            "authors": [
                "Shijian Wang",
                "Jiarui Jin",
                "Xingjian Wang",
                "Linxin Song",
                "Runhao Fu",
                "Hecheng Wang",
                "Zongyuan Ge",
                "Yuan Lu",
                "Xuelian Cheng"
            ],
            "affiliations": [
                "Fudan University",
                "Monash University",
                "Southeast University",
                "University of Southern California",
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.23473.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#games",
                    "#dataset",
                    "#reasoning",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Научить LLM думать над видео через автономное рассуждение",
                    "desc": "Video-Thinker — это multimodal LLM, который умеет рассуждать над видео, используя встроенные возможности grounding (локализации объектов) и captioning (описания). Модель автономно генерирует промежуточные рассуждения в процессе inference, не требуя внешних инструментов. Для обучения создан датасет Video-Thinker-10K с примерами chain-of-thought рассуждений, а тренировка включает supervised fine-tuning и reinforcement learning через GRPO. Модель достигает state-of-the-art результатов среди 7B моделей на различных бенчмарках для video reasoning."
                },
                "en": {
                    "title": "Empowering Video Reasoning with Autonomous Thinking",
                    "desc": "Video-Thinker is a multimodal large language model designed to enhance video reasoning by utilizing its intrinsic grounding and captioning abilities. It introduces a new dataset, Video-Thinker-10K, which supports autonomous reasoning through chain-of-thought sequences. The model is trained using Supervised Fine-Tuning followed by Group Relative Policy Optimization to improve its reasoning skills. As a result, Video-Thinker achieves superior performance on various video reasoning benchmarks, outperforming existing models and setting new standards in the field."
                },
                "zh": {
                    "title": "视频推理的新突破：Video-Thinker",
                    "desc": "Video-Thinker是一种多模态大型语言模型，能够通过内在的基础和字幕能力自主进行视频推理。该模型在视频推理基准测试中表现出色，达到了最先进的性能。我们构建了Video-Thinker-10K数据集，以支持链式思维推理序列中的自主工具使用。通过监督微调和群体相对策略优化的训练策略，Video-Thinker能够有效地处理视频推理任务，提升了多模态大型语言模型的推理能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25590",
            "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
            "url": "https://huggingface.co/papers/2510.25590",
            "abstract": "Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying a uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition. We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in a single step. Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas. For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information. 3) Adaptive Velocity Decay Cache. Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process. We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o confirmed that semantic and perceptual fidelity were well preserved.",
            "score": 3,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "a5455877a0fcfb11",
            "authors": [
                "Pengtao Chen",
                "Xianfang Zeng",
                "Maosen Zhao",
                "Mingzhu Shen",
                "Peng Ye",
                "Bangyin Xiang",
                "Zhibo Wang",
                "Wei Cheng",
                "Gang Yu",
                "Tao Chen"
            ],
            "affiliations": [
                "Fudan University",
                "Imperial College London",
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25590.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#optimization"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Умное разделение регионов для быстрого редактирования изображений",
                    "desc": "Статья предлагает RegionE — фреймворк для ускорения редактирования изображений по текстовым инструкциям без дополнительного обучения. Ключевая идея заключается в разделении изображения на редактируемые и нередактируемые области, которые обрабатываются по-разному: для нередактируемых регионов используется одношаговое предсказание вместо многошагового денойзинга. Для ускорения обработки редактируемых областей применяются специальный Region-Instruction KV Cache и адаптивный кэш скорости с затуханием. Метод показал ускорение в 2-2.5 раза на современных моделях типа FLUX.1 и Qwen-Image-Edit с сохранением качества редактирования."
                },
                "en": {
                    "title": "Accelerating Image Editing with Region-Aware Techniques",
                    "desc": "This paper introduces RegionE, a new framework for instruction-based image editing (IIE) that improves efficiency by recognizing the differences between edited and unedited regions of an image. It employs an adaptive region partitioning method to identify these areas, allowing for faster predictions in unedited regions while maintaining detailed processing in edited areas. The framework also includes a Region-Instruction KV Cache to enhance local iterative denoising and an adaptive velocity decay cache to speed up the process further. Overall, RegionE significantly accelerates IIE tasks without requiring additional training, achieving notable performance improvements on existing models."
                },
                "zh": {
                    "title": "区域感知，提升图像编辑效率",
                    "desc": "最近，基于指令的图像编辑（IIE）受到了广泛关注。现有的IIE模型在处理图像时没有区分编辑区域和未编辑区域，导致生成过程效率低下。为此，我们提出了RegionE，一个自适应的区域感知生成框架，可以加速IIE任务而无需额外训练。RegionE通过自适应区域划分、区域感知生成和自适应速度衰减缓存等组件，提高了生成效率和质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24592",
            "title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence\n  Optimization",
            "url": "https://huggingface.co/papers/2510.24592",
            "abstract": "ReForm, a reflective autoformalization method, improves the semantic accuracy of formal statements generated from natural language mathematics through iterative refinement and semantic consistency evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoformalization, which translates natural language mathematics into machine-verifiable formal statements, is critical for using formal mathematical reasoning to solve math problems stated in natural language. While Large Language Models can generate syntactically correct formal statements, they often fail to preserve the original problem's semantic intent. This limitation arises from the LLM approaches' treating autoformalization as a simplistic translation task which lacks mechanisms for self-reflection and iterative refinement that human experts naturally employ. To address these issues, we propose ReForm, a Reflective Autoformalization method that tightly integrates semantic consistency evaluation into the autoformalization process. This enables the model to iteratively generate formal statements, assess its semantic fidelity, and self-correct identified errors through progressive refinement. To effectively train this reflective model, we introduce Prospective Bounded Sequence Optimization (PBSO), which employs different rewards at different sequence positions to ensure that the model develops both accurate autoformalization and correct semantic validations, preventing superficial critiques that would undermine the purpose of reflection. Extensive experiments across four autoformalization benchmarks demonstrate that ReForm achieves an average improvement of 17.2 percentage points over the strongest baselines. To further ensure evaluation reliability, we introduce ConsistencyCheck, a benchmark of 859 expert-annotated items that not only validates LLMs as judges but also reveals that autoformalization is inherently difficult: even human experts produce semantic errors in up to 38.5% of cases.",
            "score": 2,
            "issue_id": 6690,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "5f2d10bba8cf180d",
            "authors": [
                "Guoxin Chen",
                "Jing Wu",
                "Xinjie Chen",
                "Wayne Xin Zhao",
                "Ruihua Song",
                "Chengxi Li",
                "Kai Fan",
                "Dayiheng Liu",
                "Minpeng Liao"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Tongyi Lab, Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24592.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#math",
                    "#dataset",
                    "#reasoning",
                    "#optimization",
                    "#data"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Рефлексивная автоформализация математики с самопроверкой",
                    "desc": "Статья представляет ReForm — метод автоформализации, который переводит математические задачи на естественном языке в формальные верифицируемые утверждения. Ключевая проблема существующих LLM заключается в том, что они создают синтаксически корректные, но семантически неточные формальные выражения. ReForm решает это через итеративную генерацию с самопроверкой семантической согласованности и самокоррекцией ошибок, используя специальный метод обучения PBSO. Эксперименты показывают улучшение на 17.2 процентных пункта, при этом даже эксперты-люди допускают семантические ошибки в 38.5% случаев."
                },
                "en": {
                    "title": "ReForm: Enhancing Semantic Accuracy in Autoformalization",
                    "desc": "ReForm is a method designed to enhance the accuracy of converting natural language mathematics into formal statements that machines can verify. It addresses the common issue where Large Language Models (LLMs) generate statements that are syntactically correct but semantically flawed. By incorporating a process of iterative refinement and semantic consistency evaluation, ReForm allows the model to self-correct and improve its outputs. The method is trained using Prospective Bounded Sequence Optimization (PBSO), which helps ensure that the model not only produces accurate formalizations but also validates their semantic correctness."
                },
                "zh": {
                    "title": "反思性自动形式化：提升数学语义准确性",
                    "desc": "ReForm是一种反思性自动形式化方法，通过迭代优化和语义一致性评估，提高从自然语言数学生成的形式语句的语义准确性。传统的大型语言模型在生成形式语句时，虽然语法正确，但常常无法保留原问题的语义意图。ReForm通过将语义一致性评估紧密集成到自动形式化过程中，使模型能够迭代生成形式语句，并自我纠正识别出的错误。通过引入前瞻性有界序列优化（PBSO），ReForm确保模型在不同序列位置获得不同的奖励，从而有效训练出准确的自动形式化和正确的语义验证。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25772",
            "title": "VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context\n  Learning",
            "url": "https://huggingface.co/papers/2510.25772",
            "abstract": "Visual effects (VFX) are crucial to the expressive power of digital media, yet their creation remains a major challenge for generative AI. Prevailing methods often rely on the one-LoRA-per-effect paradigm, which is resource-intensive and fundamentally incapable of generalizing to unseen effects, thus limiting scalability and creation. To address this challenge, we introduce VFXMaster, the first unified, reference-based framework for VFX video generation. It recasts effect generation as an in-context learning task, enabling it to reproduce diverse dynamic effects from a reference video onto target content. In addition, it demonstrates remarkable generalization to unseen effect categories. Specifically, we design an in-context conditioning strategy that prompts the model with a reference example. An in-context attention mask is designed to precisely decouple and inject the essential effect attributes, allowing a single unified model to master the effect imitation without information leakage. In addition, we propose an efficient one-shot effect adaptation mechanism to boost generalization capability on tough unseen effects from a single user-provided video rapidly. Extensive experiments demonstrate that our method effectively imitates various categories of effect information and exhibits outstanding generalization to out-of-domain effects. To foster future research, we will release our code, models, and a comprehensive dataset to the community.",
            "score": 1,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "b29b7917819d7178",
            "authors": [
                "Baolu Li",
                "Yiming Zhang",
                "Qinghe Wang",
                "Liqian Ma",
                "Xiaoyu Shi",
                "Xintao Wang",
                "Pengfei Wan",
                "Zhenfei Yin",
                "Yunzhi Zhuge",
                "Huchuan Lu",
                "Xu Jia"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "Kling Team, Kuaishou Technology",
                "Oxford University",
                "ZMO AI Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25772.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#games",
                    "#dataset",
                    "#video"
                ],
                "emoji": "✨",
                "ru": {
                    "title": "Единая модель для копирования визуальных эффектов из примера",
                    "desc": "Исследователи представили VFXMaster — первую универсальную систему для генерации видео с визуальными эффектами на основе референсного видео. Вместо традиционного подхода с обучением отдельной LoRA для каждого эффекта, метод использует in-context learning, позволяя модели воспроизводить разнообразные динамические эффекты из примера на целевой контент. Специальная стратегия условного обучения с маской внимания точно извлекает и переносит атрибуты эффектов без утечки информации. Система демонстрирует впечатляющую способность к генерализации на невиданные категории эффектов благодаря механизму one-shot адаптации."
                },
                "en": {
                    "title": "VFXMaster: Unifying Visual Effects Generation with In-Context Learning",
                    "desc": "This paper presents VFXMaster, a novel framework for generating visual effects (VFX) in videos using generative AI. Unlike traditional methods that require separate models for each effect, VFXMaster employs a unified approach that leverages in-context learning to adapt effects from reference videos to new content. The framework includes an innovative attention mask to isolate and apply specific effect attributes, enhancing its ability to generalize to previously unseen effects. The authors also introduce a one-shot adaptation mechanism, allowing rapid learning from a single example, which significantly improves the model's versatility and efficiency in VFX creation."
                },
                "zh": {
                    "title": "VFXMaster：统一的视觉特效生成框架",
                    "desc": "本论文介绍了一种名为VFXMaster的统一框架，用于生成视觉特效（VFX）视频。该方法将特效生成视为上下文学习任务，能够从参考视频中复制多样的动态特效到目标内容上。VFXMaster展示了对未见特效类别的显著泛化能力，并设计了一种上下文条件策略，以精确解耦和注入特效属性。通过高效的一次性特效适应机制，该方法能够快速提升对难以见到的特效的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25741",
            "title": "Scaling Latent Reasoning via Looped Language Models",
            "url": "https://huggingface.co/papers/2510.25741",
            "abstract": "Modern LLMs are trained to \"think\" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Models (LoopLM) that instead build reasoning into the pre-training phase through (i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and 2.6B models enjoy superior performance that match the results of up to 12B SOTA LLMs across a wide range of benchmarks. Through controlled experiments, we show this advantage stems not from increased knowledge capacity, but from superior knowledge manipulation capabilities. We also show that LoopLM yields reasoning traces more aligned with final outputs than explicit CoT. We hope our results show the potential of LoopLM as a novel scaling direction in the reasoning era. Our model could be found in: http://ouro-llm.github.io.",
            "score": 1,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "33772a5a18bf991a",
            "authors": [
                "Rui-Jie Zhu",
                "Zixuan Wang",
                "Kai Hua",
                "Tianyu Zhang",
                "Ziniu Li",
                "Haoran Que",
                "Boyi Wei",
                "Zixin Wen",
                "Fan Yin",
                "He Xing",
                "Lu Li",
                "Jiajun Shi",
                "Kaijing Ma",
                "Shanda Li",
                "Taylor Kergan",
                "Andrew Smith",
                "Xingwei Qu",
                "Mude Hui",
                "Bohong Wu",
                "Qiyang Min",
                "Hongzhi Huang",
                "Xun Zhou",
                "Wei Ye",
                "Jiaheng Liu",
                "Jian Yang",
                "Yunfeng Shi",
                "Chenghua Lin",
                "Enduo Zhao",
                "Tianle Cai",
                "Ge Zhang",
                "Wenhao Huang",
                "Yoshua Bengio",
                "Jason Eshraghian"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Carnegie Mellon University",
                "Conscium",
                "M-A-P Core Contributors",
                "Mila - Quebec AI Institute",
                "Peking University",
                "Princeton University",
                "UC Santa Cruz",
                "University of Manchester",
                "University of Montreal",
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25741.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "🔁",
                "ru": {
                    "title": "Рассуждения внутри модели: обучение LLM думать в латентном пространстве",
                    "desc": "В статье представлена архитектура Ouro - семейство Looped Language Models (LoopLM), которые встраивают способность к рассуждению непосредственно на этапе предобучения, а не полагаются на chain-of-thought при инференсе. Модели используют итеративные вычисления в латентном пространстве с регуляризацией энтропии для динамического выделения вычислительной глубины, обучаясь на 7.7 триллионах токенов. Компактные модели Ouro размером 1.4B и 2.6B параметров показывают результаты, сравнимые с современными LLM до 12B параметров на широком спектре бенчмарков. Ключевое преимущество заключается не в увеличении объёма знаний, а в улучшенной способности манипулировать имеющимися знаниями через внутренние итеративные процессы."
                },
                "en": {
                    "title": "Ouro: Enhancing Reasoning in Language Models Through Pre-Training",
                    "desc": "This paper introduces Ouro, a new type of pre-trained language model called Looped Language Models (LoopLM) that enhances reasoning during the pre-training phase rather than relying solely on post-training text generation. Ouro incorporates iterative computation in latent space and uses an entropy-regularized objective to optimize how depth is allocated in reasoning tasks. The models, with sizes of 1.4B and 2.6B parameters, demonstrate performance comparable to larger state-of-the-art models, achieving better knowledge manipulation rather than just increased capacity. The findings suggest that LoopLM can provide reasoning traces that are more closely aligned with the final outputs compared to traditional chain-of-thought methods, indicating a promising new direction for scaling reasoning in language models."
                },
                "zh": {
                    "title": "循环语言模型：推理的新方向",
                    "desc": "现代的大型语言模型（LLM）主要通过显式文本生成进行“思考”，如链式推理（CoT），这使得推理过程被推迟到训练后，并未充分利用预训练数据。我们提出并开源了Ouro，这是一种预训练的循环语言模型（LoopLM），它通过在潜在空间中的迭代计算、熵正则化目标和扩展到7.7万亿个标记，将推理构建到预训练阶段。Ouro 1.4B和2.6B模型在多个基准测试中表现优越，能够与高达12B的最先进LLM的结果相匹配。我们的实验表明，这种优势并非来自知识容量的增加，而是来自更优的知识操作能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24824",
            "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling",
            "url": "https://huggingface.co/papers/2510.24824",
            "abstract": "Large Language Models (LLMs) are powerful but often too slow and costly for real-world use during inference. Looped transformers save on parameters by reusing the same weights for multiple computational steps, or \"loops.\" However, this approach has a major flaw: the loops run one after another, causing inference latency and memory requirements to increase with each added loop. This makes them impractical for fast applications. To solve this problem, we introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that delivers the performance benefits of a deep, looped model but with the low latency of a standard, non-looped model. PLT works using two key techniques. First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by computing different loops for different tokens at the same time, all within a single pass. Second, to prevent memory costs from growing, we use an Efficient Representation Enhancement strategy. This method shares the memory (KV cache) from the first loop with all other loops. It then uses a Gated Sliding-Window Attention (G-SWA) to combine this shared global information with local information, maintaining high accuracy. Our experiments show that PLT achieves the high accuracy of a traditional looped model but with almost no extra latency or memory cost compared to a standard transformer.",
            "score": 1,
            "issue_id": 6690,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "97530385a3672599",
            "authors": [
                "Bohong Wu",
                "Mengzhao Chen",
                "Xiang Luo",
                "Shen Yan",
                "Qifan Yu",
                "Fan Xia",
                "Tianqi Zhang",
                "Hongrui Zhan",
                "Zheng Zhong",
                "Xun Zhou",
                "Siyuan Qiao",
                "Xingyan Bin"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24824.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Параллельные петли для быстрых LLM без потери качества",
                    "desc": "Large Language Models часто слишком медленные и дорогие для практического применения. Looped трансформеры экономят параметры, переиспользуя одни и те же веса в нескольких вычислительных шагах, но это увеличивает латентность, так как петли выполняются последовательно. Авторы предложили Parallel Loop Transformer (PLT) — архитектуру, которая вычисляет разные петли для разных токенов параллельно и использует общий KV cache с Gated Sliding-Window Attention для эффективной работы с памятью. Эксперименты показали, что PLT достигает точности традиционных looped моделей практически без дополнительных затрат на латентность и память."
                },
                "en": {
                    "title": "Fast and Efficient Inference with Parallel Loop Transformers",
                    "desc": "The paper introduces the Parallel Loop Transformer (PLT), a new architecture designed to enhance the efficiency of Large Language Models (LLMs) during inference. PLT utilizes Cross-Loop Parallelism (CLP) to allow multiple loops to be processed simultaneously for different tokens, significantly reducing latency. Additionally, it employs an Efficient Representation Enhancement strategy to manage memory usage by sharing the key-value cache across loops. The results demonstrate that PLT maintains the accuracy of traditional looped models while minimizing latency and memory costs, making it suitable for real-time applications."
                },
                "zh": {
                    "title": "并行循环变换器：高效推理的新选择",
                    "desc": "大型语言模型（LLMs）在推理时通常速度慢且成本高，难以在实际应用中使用。循环变换器通过在多个计算步骤中重用相同的权重来节省参数，但其缺点是循环依赖导致推理延迟和内存需求增加。为了解决这个问题，我们提出了并行循环变换器（PLT），它结合了深度循环模型的性能优势和标准非循环模型的低延迟。PLT通过交叉循环并行性和高效表示增强策略来实现这一目标，从而在保持高准确率的同时，几乎没有额外的延迟或内存成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.24821",
            "title": "Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal\n  Perception and Generation",
            "url": "https://huggingface.co/papers/2510.24821",
            "abstract": "We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables highly efficient scaling (dramatically improving computational efficiency while significantly expanding model capacity) and empowers stronger unified multimodal intelligence across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in contextual ASR and highly competitive results in dialect-aware ASR. In image generation, Ming-Flash-Omni introduces high-fidelity text rendering and demonstrates marked gains in scene consistency and identity preservation during image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture.",
            "score": 1,
            "issue_id": 6690,
            "pub_date": "2025-10-28",
            "pub_date_card": {
                "ru": "28 октября",
                "en": "October 28",
                "zh": "10月28日"
            },
            "hash": "184d8de02508f6c1",
            "authors": [
                "Inclusion AI",
                ":",
                "Bowen Ma",
                "Cheng Zou",
                "Canxiang Yan",
                "Chunxiang Jin",
                "Chunjie Shen",
                "Dandan Zheng",
                "Fudong Wang",
                "Furong Xu",
                "GuangMing Yao",
                "Jun Zhou",
                "Jingdong Chen",
                "Jianing Li",
                "Jianxin Sun",
                "Jiajia Liu",
                "Jianjiang Zhu",
                "Jianping Jiang",
                "Jun Peng",
                "Kaixiang Ji",
                "Kaimeng Ren",
                "Libin Wang",
                "Lixiang Ru",
                "Longhua Tan",
                "Lan Wang",
                "Mochen Bai",
                "Ning Gao",
                "Qingpei Guo",
                "Qinglong Zhang",
                "Qiang Xu",
                "Rui Liu",
                "Ruijie Xiong",
                "Ruobing Zheng",
                "Sirui Gao",
                "Tianqi Li",
                "Tinghao Liu",
                "Weilong Chai",
                "Xinyu Xiao",
                "Xiaomei Wang",
                "Xiaolong Wang",
                "Xiao Lu",
                "Xiaoyu Li",
                "Xingning Dong",
                "Xuzheng Yu",
                "Yi Yuan",
                "Yuting Gao",
                "Yuting Xiao",
                "Yunxiao Sun",
                "Yipeng Chen",
                "Yifan Mao",
                "Yifei Wu",
                "Yongjie Lyu",
                "Ziping Ma",
                "Zhiqiang Fang",
                "Zhihao Qiu",
                "Ziyuan Huang",
                "Zizheng Yang",
                "Zhengyu He"
            ],
            "affiliations": [
                "Ant Group",
                "Inclusion AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.24821.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#architecture",
                    "#audio",
                    "#agi"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Единая мультимодальная модель с разреженной MoE-архитектурой для речи, текста и изображений",
                    "desc": "В статье представлена Ming-Flash-Omni — улучшенная мультимодальная модель на базе архитектуры Mixture-of-Experts со 100 миллиардами параметров, из которых только 6.1 миллиарда активны для каждого токена. Модель демонстрирует state-of-the-art результаты в распознавании речи (особенно в контекстном ASR), генерации изображений с высококачественным рендерингом текста и генеративной сегментации. Архитектура MoE обеспечивает эффективное масштабирование, значительно улучшая вычислительную эффективность при расширении возможностей модели. Система объединяет понимание и генерацию контента в области зрения, речи и языка в единой архитектуре, что является важным шагом к AGI."
                },
                "en": {
                    "title": "Ming-Flash-Omni: Pioneering Unified Multimodal Intelligence",
                    "desc": "Ming-Flash-Omni is an advanced machine learning model that enhances the capabilities of its predecessor, Ming-Omni, by utilizing a sparser Mixture-of-Experts (MoE) approach. With 100 billion parameters, it activates only 6.1 billion per token, allowing for efficient scaling and improved computational performance. This model excels in multimodal tasks, achieving top results in speech recognition, image generation, and generative segmentation, thereby pushing the boundaries towards Artificial General Intelligence (AGI). Its innovations include high-fidelity text rendering and enhanced spatial control, making it a significant advancement in unified multimodal intelligence."
                },
                "zh": {
                    "title": "Ming-Flash-Omni：迈向人工通用智能的多模态突破",
                    "desc": "我们提出了Ming-Flash-Omni，这是Ming-Omni的升级版，基于一种稀疏的专家混合模型（MoE）变体，具有1000亿个参数，其中每个token仅激活6.1亿个参数。这种架构实现了高效的扩展，显著提高了计算效率，同时大幅扩展了模型容量，推动了跨视觉、语音和语言的统一多模态智能，代表了向人工通用智能（AGI）迈出的重要一步。与前身相比，升级版在多模态理解和生成方面表现出显著的改进，尤其在语音识别和图像生成方面取得了最先进的性能。Ming-Flash-Omni还引入了生成分割，不仅在独立分割性能上表现出色，还增强了图像生成中的空间控制和编辑一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25758",
            "title": "TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological\n  Counseling",
            "url": "https://huggingface.co/papers/2510.25758",
            "abstract": "Large language models (LLMs) in psychological counseling have attracted increasing attention. However, existing approaches often lack emotional understanding, adaptive strategies, and the use of therapeutic methods across multiple sessions with long-term memory, leaving them far from real clinical practice. To address these critical gaps, we introduce TheraMind, a strategic and adaptive agent for longitudinal psychological counseling. The cornerstone of TheraMind is a novel dual-loop architecture that decouples the complex counseling process into an Intra-Session Loop for tactical dialogue management and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session Loop perceives the patient's emotional state to dynamically select response strategies while leveraging cross-session memory to ensure continuity. Crucially, the Cross-Session Loop empowers the agent with long-term adaptability by evaluating the efficacy of the applied therapy after each session and adjusting the method for subsequent interactions. We validate our approach in a high-fidelity simulation environment grounded in real clinical cases. Extensive evaluations show that TheraMind outperforms other methods, especially on multi-session metrics like Coherence, Flexibility, and Therapeutic Attunement, validating the effectiveness of its dual-loop design in emulating strategic, adaptive, and longitudinal therapeutic behavior. The code is publicly available at https://0mwwm0.github.io/TheraMind/.",
            "score": 0,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "e35ada54dde29562",
            "authors": [
                "He Hu",
                "Yucheng Zhou",
                "Chiyuan Ma",
                "Qianning Wang",
                "Zheng Zhang",
                "Fei Ma",
                "Laizhong Cui",
                "Qi Tian"
            ],
            "affiliations": [
                "Auckland University of Technology",
                "CUHK, Shenzhen",
                "College of Computer Science and Software Engineering, Shenzhen University",
                "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)",
                "SKL-IOTSC, CIS, University of Macau",
                "School of Psychology, South China Normal University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25758.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#open_source",
                    "#long_context",
                    "#alignment",
                    "#agents",
                    "#healthcare"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "TheraMind: новый подход в психологическом консультировании с использованием LLM",
                    "desc": "В статье рассматривается использование LLM в психологическом консультировании. Авторы представляют TheraMind, агент, который использует двойную петлевую архитектуру для улучшения адаптивности и стратегического планирования в терапии. Внутрисессионная петля управляет диалогом, учитывая эмоциональное состояние пациента, а межсессионная петля адаптирует терапию на основе долгосрочной памяти. Результаты показывают, что TheraMind превосходит другие методы по показателям многосессионной терапии, таким как когерентность и гибкость."
                },
                "en": {
                    "title": "TheraMind: Adaptive Longitudinal Counseling with Dual-Loop Architecture",
                    "desc": "TheraMind is a novel approach to psychological counseling using large language models (LLMs) that addresses key limitations in emotional understanding and long-term therapeutic strategies. It features a dual-loop architecture, consisting of an Intra-Session Loop for real-time dialogue management and a Cross-Session Loop for strategic planning across multiple sessions. This design allows TheraMind to adaptively respond to a patient's emotional state while maintaining continuity through long-term memory. Evaluations demonstrate that TheraMind significantly improves multi-session counseling metrics, showcasing its effectiveness in replicating adaptive therapeutic interactions."
                },
                "zh": {
                    "title": "TheraMind：智能心理咨询的未来",
                    "desc": "大型语言模型（LLMs）在心理咨询中的应用越来越受到关注，但现有方法往往缺乏情感理解和适应性策略，无法有效进行多次会话的治疗。为了解决这些问题，我们提出了TheraMind，一个用于长期心理咨询的战略性和适应性代理。TheraMind的核心是一个新颖的双循环架构，将复杂的咨询过程分为会话内循环和会话间循环，以实现战术对话管理和战略治疗规划。通过动态选择响应策略和评估治疗效果，TheraMind能够在多次会话中保持连贯性和适应性，显著提升了心理咨询的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.25409",
            "title": "BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains",
            "url": "https://huggingface.co/papers/2510.25409",
            "abstract": "The rapid advancement of large language models(LLMs) has intensified the need for domain and culture specific evaluation. Existing benchmarks are largely Anglocentric and domain-agnostic, limiting their applicability to India-centric contexts. To address this gap, we introduce BhashaBench V1, the first domain-specific, multi-task, bilingual benchmark focusing on critical Indic knowledge systems. BhashaBench V1 contains 74,166 meticulously curated question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from authentic government and domain-specific exams. It spans four major domains: Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs reveals significant domain and language specific performance gaps, with especially large disparities in low-resource domains. For instance, GPT-4o achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models consistently perform better on English content compared to Hindi across all domains. Subdomain-level analysis shows that areas such as Cyber Law, International Finance perform relatively well, while Panchakarma, Seed Science, and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive dataset for evaluating large language models across India's diverse knowledge domains. It enables assessment of models' ability to integrate domain-specific knowledge with bilingual understanding. All code, benchmarks, and resources are publicly available to support open research.",
            "score": 0,
            "issue_id": 6690,
            "pub_date": "2025-10-29",
            "pub_date_card": {
                "ru": "29 октября",
                "en": "October 29",
                "zh": "10月29日"
            },
            "hash": "825dd6c8c3ee41d8",
            "authors": [
                "Vijay Devane",
                "Mohd Nauman",
                "Bhargav Patel",
                "Aniket Mahendra Wakchoure",
                "Yogeshkumar Sant",
                "Shyam Pawar",
                "Viraj Thakur",
                "Ananya Godse",
                "Sunil Patra",
                "Neha Maurya",
                "Suraj Racha",
                "Nitish Kamal Singh",
                "Ajay Nagpal",
                "Piyush Sawarkar",
                "Kundeshwar Vijayrao Pundalik",
                "Rohit Saluja",
                "Ganesh Ramakrishnan"
            ],
            "affiliations": [
                "Indian Institute of Technology Bombay (IIT Bombay)",
                "Technology Innovation Hub (TIH) at IIT Bombay"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.25409.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#low_resource",
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#science"
                ],
                "emoji": "🇮🇳",
                "ru": {
                    "title": "BhashaBench V1: Оценка LLM в индийском контексте",
                    "desc": "В статье обсуждается создание BhashaBench V1, первого доменно-специфического, многоцелевого, двуязычного бенчмарка для оценки LLM в индийском контексте. BhashaBench V1 включает 74,166 пар вопросов и ответов на английском и хинди, охватывающих такие области, как сельское хозяйство, право, финансы и аюрведа. Исследование показывает значительные различия в производительности моделей в зависимости от языка и домена, особенно в малоресурсных областях. BhashaBench V1 предоставляет обширный набор данных для оценки способности моделей интегрировать доменные знания с двуязычным пониманием."
                },
                "en": {
                    "title": "BhashaBench V1: Bridging Language and Domain Gaps in AI Evaluation",
                    "desc": "This paper presents BhashaBench V1, a new benchmark designed to evaluate large language models (LLMs) in the context of Indian knowledge systems. It includes 74,166 question-answer pairs in both English and Hindi, covering four key domains: Agriculture, Legal, Finance, and Ayurveda. The evaluation reveals significant performance gaps between languages and domains, highlighting that models perform better on English content. BhashaBench V1 aims to enhance the assessment of LLMs by providing a domain-specific and bilingual framework, promoting more accurate evaluations in low-resource areas."
                },
                "zh": {
                    "title": "BhashaBench V1：评估印度知识体系的双语基准",
                    "desc": "随着大型语言模型（LLMs）的快速发展，针对特定领域和文化的评估需求日益增加。现有的基准测试主要集中在英语，缺乏对印度特定背景的适用性。为了解决这一问题，我们推出了BhashaBench V1，这是首个专注于重要印度知识体系的领域特定多任务双语基准。该基准包含74,166个精心策划的问答对，涵盖农业、法律、金融和阿育吠陀等四个主要领域，能够对大型语言模型进行细致的评估。"
                }
            }
        }
    ],
    "link_prev": "2025-10-29.html",
    "link_next": "2025-10-31.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "29.10",
        "en": "10/29",
        "zh": "10月29日"
    },
    "short_date_next": {
        "ru": "31.10",
        "en": "10/31",
        "zh": "10月31日"
    },
    "categories": {
        "#dataset": 6,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 3,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 4,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 2,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    }
}