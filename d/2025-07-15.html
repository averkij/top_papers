
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 12 papers. July 15.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">15 июля</span> | <span id="title-articles-count">12 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-07-14.html">⬅️ <span id="prev-date">14.07</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-07-16.html">➡️ <span id="next-date">16.07</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-07.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'};
        let feedDateNext = {'ru': '16.07', 'en': '07/16', 'zh': '7月16日'};
        let feedDatePrev = {'ru': '14.07', 'en': '07/14', 'zh': '7月14日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.09862', 'title': 'SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual\n  Dyadic Interactive Human Generation', 'url': 'https://huggingface.co/papers/2507.09862', 'abstract': 'A large-scale dataset named SpeakerVid-5M is introduced for audio-visual dyadic interactive virtual human generation, featuring diverse interactions and high-quality data for various virtual human tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid development of large-scale models has catalyzed significant breakthroughs in the digital human domain. These advanced methodologies offer high-fidelity solutions for avatar driving and rendering, leading academia to focus on the next major challenge: audio-visual dyadic interactive virtual human. To facilitate research in this emerging area, we present SpeakerVid-5M dataset, the first large-scale, high-quality dataset designed for audio-visual dyadic interactive virtual human generation. Totaling over 8,743 hours, SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It covers diverse scales and interaction types, including monadic talking, listening, and dyadic conversations. Crucially, the dataset is structured along two key dimensions: interaction type and data quality. First, it is categorized into four types (dialogue branch, single branch, listening branch and multi-turn branch) based on the interaction scenario. Second, it is stratified into a large-scale pre-training subset and a curated, high-quality subset for Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of 2D virtual human tasks. In addition, we provide an autoregressive (AR)-based video chat baseline trained on this data, accompanied by a dedicated set of metrics and test data to serve as a benchmark VidChatBench for future work. Both the dataset and the corresponding data processing code will be publicly released. Project page: https://dorniwang.github.io/SpeakerVid-5M/', 'score': 37, 'issue_id': 4814, 'pub_date': '2025-07-14', 'pub_date_card': {'ru': '14 июля', 'en': 'July 14', 'zh': '7月14日'}, 'hash': '67b82ab227be6ce9', 'authors': ['Youliang Zhang', 'Zhaoyang Li', 'Duomin Wang', 'Jiahe Zhang', 'Deyu Zhou', 'Zixin Yin', 'Xili Dai', 'Gang Yu', 'Xiu Li'], 'affiliations': ['StepFun', 'The Hong Kong University of Science and Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.09862.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'SpeakerVid-5M: Революция в создании интерактивных виртуальных людей', 'desc': 'Представлен крупномасштабный набор данных SpeakerVid-5M для генерации аудиовизуальных диадических интерактивных виртуальных людей. Датасет содержит более 5,2 миллиона видеоклипов с портретами людей, охватывающих различные типы взаимодействий. Структура набора данных организована по двум ключевым измерениям: типу взаимодействия и качеству данных. Авторы также предоставляют базовую модель видеочата на основе авторегрессии и набор метрик для оценки будущих работ в этой области.'}, 'en': {'title': 'Unlocking Interactive Virtual Humans with SpeakerVid-5M', 'desc': 'The paper introduces the SpeakerVid-5M dataset, a large-scale resource designed for generating audio-visual dyadic interactive virtual humans. It consists of over 8,743 hours of video data, featuring more than 5.2 million clips that capture various interaction types, such as monadic and dyadic conversations. The dataset is organized into two main categories: interaction type and data quality, allowing for effective pre-training and fine-tuning of models. Additionally, the authors present a baseline video chat model and a benchmark called VidChatBench to facilitate future research in this area.'}, 'zh': {'title': 'SpeakerVid-5M：音视频互动虚拟人的新里程碑', 'desc': '本文介绍了一个名为SpeakerVid-5M的大规模数据集，旨在生成音视频双向互动的虚拟人。该数据集包含超过8743小时的高质量视频片段，涵盖多种互动类型，如单向对话和双向对话。数据集按照互动类型和数据质量两个维度进行结构化，支持多种2D虚拟人任务。我们还提供了基于自回归模型的视频聊天基线，并设立了VidChatBench作为未来研究的基准。'}}}, {'id': 'https://huggingface.co/papers/2507.10532', 'title': 'Reasoning or Memorization? Unreliable Results of Reinforcement Learning\n  Due to Data Contamination', 'url': 'https://huggingface.co/papers/2507.10532', 'abstract': 'Research on enhancing LLM reasoning through RL reveals that accurate reward signals are crucial for performance improvement, and current benchmarks may be unreliable due to data contamination.  \t\t\t\t\tAI-generated summary \t\t\t\t The reasoning capabilities of large language models (LLMs) have been a longstanding focus of research. Recent works have further enhanced these capabilities using reinforcement learning (RL), with many new methods claiming significant improvements with minimal or no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance reasoning performance. However, these breakthroughs are mostly reported on the Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500, AMC, and AIME, while failing to achieve similar gains on other models like Llama, which warrants further investigation. Our analysis shows that although Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on large-scale web corpora makes it vulnerable to data contamination in popular benchmarks. As a result, results derived from these benchmarks may be unreliable. To address this, we introduce a generator that produces fully synthetic arithmetic problems of arbitrary length and difficulty, yielding a clean dataset we call RandomCalculation. Using these leakage-free datasets, we show that only accurate reward signals consistently improve performance, while noisy or incorrect signals do not. We advocate for evaluating RL methods on uncontaminated benchmarks and across diverse model families to ensure trustworthy conclusions.', 'score': 28, 'issue_id': 4818, 'pub_date': '2025-07-14', 'pub_date_card': {'ru': '14 июля', 'en': 'July 14', 'zh': '7月14日'}, 'hash': '39054d8e3e70cc7b', 'authors': ['Mingqi Wu', 'Zhihao Zhang', 'Qiaole Dong', 'Zhiheng Xi', 'Jun Zhao', 'Senjie Jin', 'Xiaoran Fan', 'Yuhao Zhou', 'Yanwei Fu', 'Qin Liu', 'Songyang Zhang', 'Qi Zhang'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'University of California, Davis'], 'pdf_title_img': 'assets/pdf/title_img/2507.10532.jpg', 'data': {'categories': ['#leakage', '#dataset', '#synthetic', '#benchmark', '#rl', '#reasoning'], 'emoji': '🧮', 'ru': {'title': 'Чистые данные - ключ к надежному обучению с подкреплением для LLM', 'desc': 'Исследование показывает, что для улучшения способностей больших языковых моделей (LLM) к рассуждению с помощью обучения с подкреплением критически важны точные сигналы вознаграждения. Обнаружено, что существующие бенчмарки могут быть ненадежными из-за загрязнения данных. Авторы предлагают новый чистый набор данных RandomCalculation для оценки методов обучения с подкреплением. Результаты подтверждают, что только точные сигналы вознаграждения последовательно улучшают производительность моделей.'}, 'en': {'title': 'Accurate Rewards: The Key to Enhancing LLM Reasoning', 'desc': 'This paper investigates how to improve the reasoning abilities of large language models (LLMs) using reinforcement learning (RL). It highlights the importance of accurate reward signals for enhancing performance, noting that current benchmarks may be flawed due to data contamination. The authors introduce a new dataset called RandomCalculation, which consists of synthetic arithmetic problems, to provide a cleaner evaluation environment. Their findings suggest that only precise reward signals lead to consistent improvements in reasoning, emphasizing the need for reliable benchmarks across different model families.'}, 'zh': {'title': '准确奖励信号是提升推理能力的关键', 'desc': '本研究探讨了通过强化学习（RL）提升大型语言模型（LLM）推理能力的重要性，强调准确的奖励信号对性能提升至关重要。当前的基准测试可能因数据污染而不可靠，导致研究结果的可信度下降。尽管Qwen2.5模型在数学推理方面表现出色，但其在大规模网络语料库上的预训练使其易受污染影响。为了解决这一问题，我们引入了一种生成器，创建了完全合成的算术问题数据集，证明只有准确的奖励信号才能持续提升模型性能。'}}}, {'id': 'https://huggingface.co/papers/2507.10548', 'title': 'EmbRACE-3K: Embodied Reasoning and Action in Complex Environments', 'url': 'https://huggingface.co/papers/2507.10548', 'abstract': "A new dataset, EmRACE-3K, evaluates vision-language models in embodied settings, showing limitations in spatial reasoning and long-horizon planning, and demonstrates improvements through supervised and reinforcement learning fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advanced vision-language models(VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings, which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from a first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass a wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as a multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to evaluate the embodied reasoning capabilities of VLMs across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the dataset's effectiveness in enabling the development of embodied reasoning capabilities.", 'score': 24, 'issue_id': 4817, 'pub_date': '2025-07-14', 'pub_date_card': {'ru': '14 июля', 'en': 'July 14', 'zh': '7月14日'}, 'hash': '046b6dd86b975bd1', 'authors': ['Mingxian Lin', 'Wei Huang', 'Yitang Li', 'Chengjie Jiang', 'Kui Wu', 'Fangwei Zhong', 'Shengju Qian', 'Xin Wang', 'Xiaojuan Qi'], 'affiliations': ['Beijing Normal University', 'LIGHTSPEED', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.10548.jpg', 'data': {'categories': ['#reasoning', '#long_context', '#multimodal', '#benchmark', '#rl', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'EmRACE-3K: Новый рубеж в обучении воплощенного ИИ', 'desc': 'Статья представляет новый набор данных EmRACE-3K для оценки моделей компьютерного зрения и обработки естественного языка в воплощенных средах. Исследование выявляет ограничения современных моделей в пространственных рассуждениях и долгосрочном планировании. Авторы демонстрируют улучшения производительности моделей через дообучение с учителем и обучение с подкреплением. EmRACE-3K содержит более 3000 задач, управляемых естественным языком, в разнообразных фотореалистичных средах.'}, 'en': {'title': 'Enhancing Embodied Reasoning with EmRACE-3K Dataset', 'desc': 'The paper introduces EmRACE-3K, a new dataset designed to evaluate vision-language models (VLMs) in embodied settings, where agents interact with environments in real-time. It highlights the limitations of current state-of-the-art models in spatial reasoning and long-horizon planning, particularly in open-environment interactions. The dataset includes over 3,000 tasks that require navigation, object manipulation, and multi-stage goal execution, providing a benchmark for assessing embodied reasoning capabilities. Fine-tuning a model using supervised and reinforcement learning on this dataset shows significant improvements, demonstrating its potential to enhance VLM performance in interactive scenarios.'}, 'zh': {'title': 'EmRACE-3K：提升视觉-语言模型的具身推理能力', 'desc': '本论文介绍了一个新的数据集EmRACE-3K，用于评估视觉-语言模型在具身环境中的表现。研究发现，当前的模型在空间推理和长远规划方面存在明显的局限性，尤其是在需要实时互动的场景中。通过使用EmRACE-3K数据集，研究者们建立了一个基准，评估模型在探索、动态空间语义推理和多阶段目标执行等方面的能力。最后，通过监督学习和强化学习的微调，显著提升了模型在这些挑战中的表现，证明了该数据集在发展具身推理能力方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2507.10524', 'title': 'Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation', 'url': 'https://huggingface.co/papers/2507.10524', 'abstract': 'Mixture-of-Recursions (MoR) achieves parameter and computational efficiency in large language models through shared layers and adaptive recursion depths, improving performance metrics and throughput.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.', 'score': 21, 'issue_id': 4818, 'pub_date': '2025-07-14', 'pub_date_card': {'ru': '14 июля', 'en': 'July 14', 'zh': '7月14日'}, 'hash': 'f28faa5fbfe7a41c', 'authors': ['Sangmin Bae', 'Yujin Kim', 'Reza Bayat', 'Sungnyun Kim', 'Jiyoun Ha', 'Tal Schuster', 'Adam Fisch', 'Hrayr Harutyunyan', 'Ziwei Ji', 'Aaron Courville', 'Se-Young Yun'], 'affiliations': ['Google Cloud', 'Google DeepMind', 'Google Research', 'KAIST AI', 'Mila', 'Université de Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2507.10524.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Эффективность больших языковых моделей через рекурсию и разделение параметров', 'desc': 'Статья представляет Mixture-of-Recursions (MoR) - новый подход к повышению эффективности больших языковых моделей. MoR объединяет разделение параметров и адаптивные вычисления в рамках рекурсивного трансформера. Метод использует общий стек слоев и динамически назначает разную глубину рекурсии для отдельных токенов. MoR демонстрирует улучшение метрик производительности и пропускной способности по сравнению с базовыми моделями при меньшем размере модели.'}, 'en': {'title': 'Efficient Language Modeling with Mixture-of-Recursions', 'desc': 'Mixture-of-Recursions (MoR) is a novel framework designed to enhance the efficiency of large language models by integrating parameter sharing and adaptive recursion depths. It utilizes a shared stack of layers across recursion steps, which reduces the number of parameters needed while maintaining performance. Additionally, MoR employs lightweight routers to assign different recursion depths to individual tokens, optimizing attention computation and memory access. This approach not only improves validation perplexity and few-shot accuracy but also increases throughput, making it a cost-effective solution for high-quality language modeling.'}, 'zh': {'title': '高效递归，提升语言模型性能', 'desc': 'Mixture-of-Recursions（MoR）是一种新颖的框架，通过共享层和自适应递归深度，在大型语言模型中实现参数和计算效率的提升。MoR在递归步骤中重用共享的层堆栈，从而提高参数效率，同时通过轻量级路由器动态分配不同的递归深度，实现自适应计算。该方法使得模型能够在特定递归深度中仅关注活跃的token，从而优化内存访问效率。实验结果表明，MoR在多个模型规模下显著降低了验证困惑度，并提高了少量样本的准确性，展示了在不增加成本的情况下实现大型模型质量的有效路径。'}}}, {'id': 'https://huggingface.co/papers/2507.10541', 'title': 'REST: Stress Testing Large Reasoning Models by Asking Multiple Problems\n  at Once', 'url': 'https://huggingface.co/papers/2507.10541', 'abstract': 'REST evaluates large reasoning models under simultaneous multi-context pressure, revealing performance differences not apparent in single-question tests and highlighting the importance of contextual priority allocation and cognitive load management.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent Large Reasoning Models (LRMs) have achieved remarkable progress on task-specific benchmarks, yet their evaluation methods remain constrained by isolated problem-solving paradigms. Existing benchmarks predominantly assess single-question reasoning through sequential testing, resulting critical limitations: (1) vulnerability to data contamination and less challenging (e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual creation of new questions with large human efforts, (2) failure to evaluate models under multi-context pressure, a key requirement for real-world deployment. To bridge this gap, we present REST (Reasoning Evaluation through Simultaneous Testing), a stress-testing framework that concurrently exposes LRMs to multiple problems simultaneously. Beyond basic reasoning, REST specifically evaluates several under-tested capabilities: contextual priority allocation, cross-problem interference resistance, and dynamic cognitive load management. Our evaluation reveals several striking findings: Even state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance degradation under stress testing. Crucially, REST demonstrates stronger discriminative power than existing benchmarks, revealing pronounced performance differences among models that exhibit similar, near-ceiling performance under single-question evaluations. Some key mechanistic insights emerge from our analysis: (1) the "overthinking trap" is a critical factor contributing to the performance degradation; (2) the models trained with "long2short" technique preserve more accuracy of their single-problem performance under REST, outperforming standard-trained counterparts. These results establish REST as a cost-efficient, future-proof evaluation paradigm that better reflects real-world reasoning demands while reducing reliance on continuous human annotation.', 'score': 20, 'issue_id': 4819, 'pub_date': '2025-07-14', 'pub_date_card': {'ru': '14 июля', 'en': 'July 14', 'zh': '7月14日'}, 'hash': '6e4271f0c17f7ec8', 'authors': ['Zhuoshi Pan', 'Qizhi Pei', 'Yu Li', 'Qiyao Sun', 'Zinan Tang', 'H. Vicky Zhao', 'Conghui He', 'Lijun Wu'], 'affiliations': ['OpenDataLab', 'Renmin University of China', 'Shanghai Artificial Intelligence Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.10541.jpg', 'data': {'categories': ['#benchmark', '#training', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'REST: стресс-тест для искусственного интеллекта', 'desc': 'REST - это новый метод оценки больших языковых моделей, который тестирует их способность решать несколько задач одновременно. Он выявляет различия в производительности моделей, которые не видны при стандартном тестировании с одиночными вопросами. REST оценивает такие важные способности моделей, как распределение приоритетов между задачами и управление когнитивной нагрузкой. Результаты показывают, что даже современные модели значительно хуже справляются с задачами в условиях стресс-тестирования по сравнению с обычными тестами.'}, 'en': {'title': 'REST: Stress Testing for Real-World Reasoning', 'desc': 'The paper introduces REST, a new framework for evaluating Large Reasoning Models (LRMs) under simultaneous multi-context conditions. Unlike traditional methods that test models on single questions, REST exposes them to multiple problems at once, revealing their performance under stress. This approach highlights important capabilities such as contextual priority allocation and cognitive load management, which are crucial for real-world applications. The findings show that even top models struggle under this pressure, indicating that current evaluation methods may not accurately reflect their true reasoning abilities.'}, 'zh': {'title': 'REST：多上下文压力下的推理评估新方法', 'desc': '这篇论文介绍了一种新的评估框架REST，用于同时测试大型推理模型（LRMs）。传统的评估方法主要集中在单一问题的顺序测试，导致模型在多上下文压力下的表现未被充分评估。REST通过同时暴露多个问题，揭示了模型在上下文优先分配和认知负荷管理方面的能力差异。研究结果表明，即使是最先进的模型在压力测试下也会显著下降，强调了这种新评估方法在真实世界应用中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2507.04404', 'title': 'LayerCake: Token-Aware Contrastive Decoding within Large Language Model\n  Layers', 'url': 'https://huggingface.co/papers/2507.04404', 'abstract': 'A token-aware, layer-localized contrastive decoding method improves factual accuracy in large language models by selectively suppressing attention to specific token types at their respective depths.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at natural language understanding and generation but remain vulnerable to factual errors, limiting their reliability in knowledge-intensive tasks. While decoding-time strategies provide a promising efficient solution without training, existing methods typically treat token-level and layer-level signals in isolation, overlooking the joint dynamics between them. In this work, we introduce a token-aware, layer-localized contrastive decoding method that aligns specific token types with their most influential transformer layers to improve factual generation. Through empirical attention analysis, we identify two key patterns: punctuation tokens receive dominant attention in early layers, while conceptual tokens govern semantic reasoning in intermediate layers. By selectively suppressing attention to these token types at their respective depths, we achieve the induction of controlled factual degradation and derive contrastive signals to guide the final factual decoding. Our method requires no additional training or model modification, and experiments demonstrate that our method consistently improves factuality across multiple LLMs and various benchmarks.', 'score': 16, 'issue_id': 4815, 'pub_date': '2025-07-06', 'pub_date_card': {'ru': '6 июля', 'en': 'July 6', 'zh': '7月6日'}, 'hash': '67be09e9b6db6fc6', 'authors': ['Jingze Zhu', 'Yongliang Wu', 'Wenbo Zhu', 'Jiawang Cao', 'Yanqiang Zheng', 'Jiawei Chen', 'Xu Yang', 'Bernt Schiele', 'Jonas Fischer', 'Xinting Hu'], 'affiliations': ['Growth, Xiaomi Corporation', 'Max Planck Institute for Informatics', 'Opus AI Research', 'Southeast University'], 'pdf_title_img': 'assets/pdf/title_img/2507.04404.jpg', 'data': {'categories': ['#training', '#hallucinations', '#benchmark', '#architecture', '#interpretability'], 'emoji': '🎯', 'ru': {'title': 'Повышение фактической точности языковых моделей через токен-ориентированное контрастное декодирование', 'desc': 'Статья представляет новый метод контрастного декодирования для улучшения фактической точности больших языковых моделей (LLM). Метод основан на избирательном подавлении внимания к определенным типам токенов на соответствующих глубинах трансформера. Авторы обнаружили, что знаки препинания получают доминирующее внимание на ранних слоях, а концептуальные токены управляют семантическими рассуждениями на промежуточных слоях. Эксперименты показывают, что предложенный метод последовательно улучшает фактическую точность различных LLM на нескольких эталонных наборах данных.'}, 'en': {'title': 'Enhancing Factual Accuracy in LLMs with Layer-Localized Attention', 'desc': 'This paper presents a new method called token-aware, layer-localized contrastive decoding to enhance the factual accuracy of large language models (LLMs). The approach focuses on managing attention to different types of tokens at specific layers of the model, which helps in reducing factual errors during text generation. By analyzing how punctuation and conceptual tokens are processed in different layers, the method effectively suppresses attention to these tokens when necessary. The results show that this technique improves the factuality of generated text without requiring any additional training or changes to the model architecture.'}, 'zh': {'title': '提升大型语言模型事实性的对比解码方法', 'desc': '本文提出了一种基于令牌感知和层局部对比解码的方法，以提高大型语言模型的事实准确性。该方法通过在不同层次上选择性地抑制特定类型令牌的注意力，来改善生成的事实内容。研究发现，标点符号在早期层中占主导地位，而概念令牌则在中间层中主导语义推理。通过这种方式，我们的方法在不需要额外训练或模型修改的情况下，显著提高了多个大型语言模型的事实性表现。'}}}, {'id': 'https://huggingface.co/papers/2507.09104', 'title': 'CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards', 'url': 'https://huggingface.co/papers/2507.09104', 'abstract': 'CompassJudger-2, a generalist judge model, achieves superior performance across multiple benchmarks through task-driven data curation, verifiable rewards, and a refined learning objective with margin policy gradient loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, the role of LLM-as-judge in evaluating large language models has gained prominence. However, current judge models suffer from narrow specialization and limited robustness, undermining their capacity for comprehensive evaluations. In this work, we present CompassJudger-2, a novel generalist judge model that overcomes these limitations via a task-driven, multi-domain data curation strategy. Central to our approach is supervising judgment tasks with verifiable rewards, guiding intrinsic critical reasoning through rejection sampling to foster robust, generalizable judgment capabilities. We introduce a refined learning objective with margin policy gradient loss to enhance performance. Empirically, CompassJudger-2 achieves superior results across multiple judge and reward benchmarks, and our 7B model demonstrates competitive judgment accuracy with significantly larger models like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a comprehensive benchmark evaluating cross-domain judgment accuracy and rank consistency to standardize judge model evaluation. These contributions advance robust, scalable LLM judgment and establish new performance and evaluation standards.', 'score': 13, 'issue_id': 4816, 'pub_date': '2025-07-12', 'pub_date_card': {'ru': '12 июля', 'en': 'July 12', 'zh': '7月12日'}, 'hash': '40a4c36902a53203', 'authors': ['Taolin Zhang', 'Maosong Cao', 'Alexander Lam', 'Songyang Zhang', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.09104.jpg', 'data': {'categories': ['#data', '#architecture', '#training', '#benchmark', '#dataset', '#agi', '#reasoning', '#optimization'], 'emoji': '⚖️', 'ru': {'title': 'CompassJudger-2: универсальный судья для ИИ-моделей', 'desc': 'CompassJudger-2 - это новая модель-судья общего назначения для оценки больших языковых моделей. Она использует многодоменную стратегию курации данных и обучение с подкреплением для развития надежных способностей к суждению. Модель применяет усовершенствованную целевую функцию с градиентным спуском по политике с маржой. CompassJudger-2 демонстрирует превосходные результаты на различных тестовых наборах для оценки моделей-судей.'}, 'en': {'title': 'Revolutionizing Model Evaluation with CompassJudger-2', 'desc': 'CompassJudger-2 is a versatile judge model designed to evaluate large language models more effectively. It improves upon previous models by using a task-driven approach to curate diverse data, ensuring robust evaluations across different domains. The model employs verifiable rewards and a refined learning objective with margin policy gradient loss to enhance its judgment capabilities. With its superior performance on various benchmarks, CompassJudger-2 sets new standards for accuracy and consistency in model evaluation.'}, 'zh': {'title': 'CompassJudger-2：通用评判模型的突破', 'desc': 'CompassJudger-2是一种通用评判模型，通过任务驱动的数据整理、可验证的奖励和改进的学习目标（边际策略梯度损失）在多个基准测试中表现优异。该模型克服了现有评判模型的专业化狭窄和鲁棒性不足的问题，能够进行全面的评估。我们的方法通过可验证的奖励来监督判断任务，并通过拒绝采样促进内在的批判性推理，从而增强判断能力。CompassJudger-2在多个评判和奖励基准测试中取得了优异的结果，展示了与更大模型的竞争性判断准确性。'}}}, {'id': 'https://huggingface.co/papers/2507.10065', 'title': 'MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second', 'url': 'https://huggingface.co/papers/2507.10065', 'abstract': 'MoVieS synthesizes 4D dynamic novel views from monocular videos using Gaussian primitives, enabling unified modeling of appearance, geometry, and motion with minimal task-specific supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic novel views from monocular videos in one second. MoVieS represents dynamic 3D scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising their time-varying motion. This allows, for the first time, the unified modeling of appearance, geometry and motion, and enables view synthesis, reconstruction and 3D point tracking within a single learning-based framework. By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS enables large-scale training on diverse datasets with minimal dependence on task-specific supervision. As a result, it also naturally supports a wide range of zero-shot applications, such as scene flow estimation and moving object segmentation. Extensive experiments validate the effectiveness and efficiency of MoVieS across multiple tasks, achieving competitive performance while offering several orders of magnitude speedups.', 'score': 10, 'issue_id': 4818, 'pub_date': '2025-07-14', 'pub_date_card': {'ru': '14 июля', 'en': 'July 14', 'zh': '7月14日'}, 'hash': '0083d2c68182097f', 'authors': ['Chenguo Lin', 'Yuchen Lin', 'Panwang Pan', 'Yifan Yu', 'Honglei Yan', 'Katerina Fragkiadaki', 'Yadong Mu'], 'affiliations': ['ByteDance', 'Carnegie Mellon University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2507.10065.jpg', 'data': {'categories': ['#video', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Единая модель для синтеза видов, реконструкции и отслеживания в 3D', 'desc': 'MoVieS - это новая модель, которая синтезирует динамические виды из монокулярных видео за одну секунду. Она использует сетки гауссовых примитивов для представления динамичных 3D-сцен, явно моделируя их движение во времени. Этот подход позволяет объединить моделирование внешнего вида, геометрии и движения в единой обучаемой системе. MoVieS поддерживает широкий спектр задач без дополнительного обучения, включая оценку потока сцены и сегментацию движущихся объектов.'}, 'en': {'title': 'Revolutionizing 4D View Synthesis with MoVieS', 'desc': 'MoVieS is a machine learning model that creates 4D views from single videos quickly and efficiently. It uses Gaussian primitives to represent dynamic 3D scenes, allowing it to capture changes in motion over time. This model combines the understanding of appearance, geometry, and motion into one framework, which simplifies the process of view synthesis and 3D reconstruction. MoVieS can be trained on large datasets with little specific guidance, making it versatile for various applications like scene flow estimation and moving object segmentation.'}, 'zh': {'title': 'MoVieS：统一建模动态场景的创新方法', 'desc': 'MoVieS是一种新型的前馈模型，可以从单目视频中合成4D动态新视图。它使用像素对齐的高斯原语来表示动态3D场景，并明确监督其随时间变化的运动。这种方法首次实现了外观、几何和运动的统一建模，支持在单一学习框架内进行视图合成、重建和3D点跟踪。MoVieS能够在多样化的数据集上进行大规模训练，且对特定任务的监督依赖最小，支持多种零样本应用。'}}}, {'id': 'https://huggingface.co/papers/2507.04218', 'title': 'DreamPoster: A Unified Framework for Image-Conditioned Generative Poster\n  Design', 'url': 'https://huggingface.co/papers/2507.04218', 'abstract': "DreamPoster generates high-quality posters from images and text prompts using a progressive training strategy and Seedream3.0 model, outperforming existing methods in usability.  \t\t\t\t\tAI-generated summary \t\t\t\t We present DreamPoster, a Text-to-Image generation framework that intelligently synthesizes high-quality posters from user-provided images and text prompts while maintaining content fidelity and supporting flexible resolution and layout outputs. Specifically, DreamPoster is built upon our T2I model, Seedream3.0 to uniformly process different poster generating types. For dataset construction, we propose a systematic data annotation pipeline that precisely annotates textual content and typographic hierarchy information within poster images, while employing comprehensive methodologies to construct paired datasets comprising source materials (e.g., raw graphics/text) and their corresponding final poster outputs. Additionally, we implement a progressive training strategy that enables the model to hierarchically acquire multi-task generation capabilities while maintaining high-quality generation. Evaluations on our testing benchmarks demonstrate DreamPoster's superiority over existing methods, achieving a high usability rate of 88.55\\%, compared to GPT-4o (47.56\\%) and SeedEdit3.0 (25.96\\%). DreamPoster will be online in Jimeng and other Bytedance Apps.", 'score': 4, 'issue_id': 4824, 'pub_date': '2025-07-06', 'pub_date_card': {'ru': '6 июля', 'en': 'July 6', 'zh': '7月6日'}, 'hash': '1445577894611bf7', 'authors': ['Xiwei Hu', 'Haokun Chen', 'Zhongqi Qi', 'Hui Zhang', 'Dexiang Hong', 'Jie Shao', 'Xinglong Wu'], 'affiliations': ['Fudan University', 'Intelligent Creation Lab, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2507.04218.jpg', 'data': {'categories': ['#training', '#synthetic', '#benchmark', '#dataset', '#cv', '#data'], 'emoji': '🎨', 'ru': {'title': 'DreamPoster: Интеллектуальное создание постеров с помощью ИИ', 'desc': 'DreamPoster - это система генерации изображений на основе текста, которая создает высококачественные постеры из пользовательских изображений и текстовых запросов. Она использует модель Seedream3.0 и прогрессивную стратегию обучения для обработки различных типов генерации постеров. DreamPoster применяет систематический конвейер аннотации данных для точной разметки текстового содержания и типографической иерархии в изображениях постеров. Согласно оценкам на тестовых наборах данных, DreamPoster превосходит существующие методы, достигая высокого уровня применимости в 88.55%.'}, 'en': {'title': 'Transforming Ideas into Stunning Posters with DreamPoster!', 'desc': 'DreamPoster is a novel framework for generating high-quality posters from images and text prompts, leveraging the advanced Seedream3.0 model. It employs a systematic data annotation pipeline to accurately capture textual content and typographic hierarchy, ensuring fidelity in the generated outputs. The model utilizes a progressive training strategy that enhances its ability to perform multiple tasks while maintaining high-quality results. Evaluations show that DreamPoster significantly outperforms existing methods in usability, achieving an impressive usability rate of 88.55%.'}, 'zh': {'title': 'DreamPoster：智能生成高质量海报的创新框架', 'desc': 'DreamPoster 是一个文本到图像生成框架，可以从用户提供的图像和文本提示中智能合成高质量的海报。它基于 Seedream3.0 模型，能够统一处理不同类型的海报生成。为了构建数据集，我们提出了一种系统的数据注释流程，精确标注海报图像中的文本内容和排版层次信息。通过逐步训练策略，DreamPoster 能够分层获取多任务生成能力，同时保持高质量的生成效果。'}}}, {'id': 'https://huggingface.co/papers/2507.08267', 'title': 'A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy\n  with SFT and Efficiency with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2507.08267', 'abstract': "A combination of extended supervised fine-tuning and reinforcement learning from online inference enhances the mathematical reasoning capabilities of large language models, achieving top-tier performance on benchmarks like the AI Mathematical Olympiad.  \t\t\t\t\tAI-generated summary \t\t\t\t Enhancing the mathematical reasoning of Large Language Models (LLMs) is a pivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a systematic methodology for combining them to maximize both accuracy and efficiency remains largely unexplored. This paper introduces a practical and effective training recipe that strategically integrates extended SFT with RL from online inference (GRPO). We posit that these methods play complementary, not competing, roles: a prolonged SFT phase first pushes the model's accuracy to its limits, after which a GRPO phase dramatically improves token efficiency while preserving this peak performance. Our experiments reveal that extending SFT for as many as 10 epochs is crucial for performance breakthroughs, and that the primary role of GRPO in this framework is to optimize solution length. The efficacy of our recipe is rigorously validated through top-tier performance on challenging benchmarks, including a high rank among over 2,200 teams in the strictly leak-free AI Mathematical Olympiad (AIMO). This work provides the community with a battle-tested blueprint for developing state-of-the-art mathematical reasoners that are both exceptionally accurate and practically efficient. To ensure full reproducibility and empower future research, we will open-source our entire framework, including all code, model checkpoints, and training configurations at https://github.com/analokmaus/kaggle-aimo2-fast-math-r1.", 'score': 3, 'issue_id': 4820, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': '2aec98226e1c5d62', 'authors': ['Hiroshi Yoshihara', 'Taiki Yamaguchi', 'Yuichi Inoue'], 'affiliations': ['Aillis Inc., Tokyo, Japan', 'Department of Health Policy and Public Health, Graduate School of Pharmaceutical Sciences, The University of Tokyo, Tokyo, Japan', 'Rist Inc., Kyoto, Japan', 'Sakana AI, Tokyo, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2507.08267.jpg', 'data': {'categories': ['#open_source', '#math', '#optimization', '#reasoning', '#training', '#benchmark'], 'emoji': '🧮', 'ru': {'title': 'Симбиоз SFT и RL: путь к совершенству в математическом мышлении ИИ', 'desc': 'Статья представляет эффективный метод обучения больших языковых моделей (LLM) для улучшения их математических способностей. Авторы комбинируют расширенное обучение с учителем (SFT) и обучение с подкреплением (RL) от онлайн-вывода. Этот подход позволяет достичь высокой точности и эффективности модели в решении математических задач. Результаты подтверждены высокими показателями на сложных бенчмарках, включая AI Mathematical Olympiad.'}, 'en': {'title': 'Boosting Mathematical Reasoning in LLMs with Smart Training Techniques', 'desc': "This paper presents a novel approach to enhance the mathematical reasoning abilities of large language models (LLMs) by combining extended supervised fine-tuning (SFT) with reinforcement learning from online inference (GRPO). The authors argue that these two training methods complement each other, with SFT initially maximizing the model's accuracy, followed by GRPO which improves efficiency in generating solutions. Their experiments demonstrate that extending the SFT phase significantly contributes to performance improvements, while GRPO focuses on optimizing the length of the generated solutions. The proposed methodology achieves outstanding results on benchmarks, including a high ranking in the AI Mathematical Olympiad, and the authors plan to share their framework to support further research in this area."}, 'zh': {'title': '提升大型语言模型的数学推理能力', 'desc': '本文探讨了如何通过扩展的监督微调和在线推理的强化学习相结合，提升大型语言模型的数学推理能力。研究表明，延长监督微调阶段可以显著提高模型的准确性，而随后引入的在线推理强化学习则能优化解决方案的长度和效率。实验结果显示，延长监督微调至多10个周期对于性能突破至关重要。该方法在AI数学奥林匹克等基准测试中表现优异，为开发高效且准确的数学推理模型提供了实用的蓝图。'}}}, {'id': 'https://huggingface.co/papers/2507.09074', 'title': 'Favicon Trojans: Executable Steganography Via Ico Alpha Channel\n  Exploitation', 'url': 'https://huggingface.co/papers/2507.09074', 'abstract': 'This paper presents a novel method of executable steganography using the alpha transparency layer of ICO image files to embed and deliver self-decompressing JavaScript payloads within web browsers. By targeting the least significant bit (LSB) of non-transparent alpha layer image values, the proposed method successfully conceals compressed JavaScript code inside a favicon image without affecting visual fidelity. Global web traffic loads 294 billion favicons daily and consume 0.9 petabytes of network bandwidth. A proof-of-concept implementation demonstrates that a 64x64 ICO image can embed up to 512 bytes uncompressed, or 0.8 kilobyte when using lightweight two-fold compression. On page load, a browser fetches the favicon as part of standard behavior, allowing an embedded loader script to extract and execute the payload entirely in memory using native JavaScript APIs and canvas pixel access. This creates a two-stage covert channel requiring no additional network or user requests. Testing across multiple browsers in both desktop and mobile environments confirms successful and silent execution of the embedded script. We evaluate the threat model, relate it to polymorphic phishing attacks that evade favicon-based detection, and analyze evasion of content security policies and antivirus scanners. We map nine example MITRE ATT&CK Framework objectives to single line JavaScript to execute arbitrarily in ICO files. Existing steganalysis and sanitization defenses are discussed, highlighting limitations in detecting or neutralizing alpha-channel exploits. The results demonstrate a stealthy and reusable attack surface that blurs traditional boundaries between static images and executable content. Because modern browsers report silent errors when developers specifically fail to load ICO files, this attack surface offers an interesting example of required web behaviors that in turn compromise security.', 'score': 2, 'issue_id': 4823, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': '4a098df2d28ece2f', 'authors': ['David Noever', 'Forrest McKee'], 'affiliations': ['PeopleTec, Inc., Huntsville, Alabama, USA'], 'pdf_title_img': 'assets/pdf/title_img/2507.09074.jpg', 'data': {'categories': ['#multimodal', '#audio', '#dataset', '#data', '#video', '#healthcare', '#security'], 'emoji': '🕵️', 'ru': {'title': 'Невидимый JavaScript: Стеганография в фавиконках', 'desc': 'Статья представляет новый метод исполняемой стеганографии, использующий альфа-канал прозрачности ICO-файлов для встраивания JavaScript-кода в веб-браузерах. Метод скрывает сжатый JavaScript в фавиконке, не влияя на её визуальное качество. Доказательство концепции показывает, что 64x64 ICO-изображение может вместить до 512 байт несжатого или 0.8 килобайт сжатого кода. Этот подход создает двухэтапный скрытый канал, не требующий дополнительных сетевых запросов, и успешно обходит системы безопасности и антивирусные сканеры.'}, 'en': {'title': 'Stealthy JavaScript Payloads Hidden in Favicons', 'desc': 'This paper introduces a new technique for executable steganography that uses the alpha transparency layer of ICO image files to hide JavaScript payloads. By manipulating the least significant bit of the alpha values, the method embeds compressed JavaScript code within favicon images without altering their appearance. The approach takes advantage of the fact that browsers automatically load favicons, allowing the hidden script to be executed in memory without additional network requests. The study also evaluates the implications for security, showing how this method can evade detection by existing defenses and contribute to sophisticated phishing attacks.'}, 'zh': {'title': '利用ICO图像实现隐蔽的JavaScript攻击', 'desc': '本文提出了一种新颖的可执行隐写术方法，利用ICO图像文件的透明度层嵌入和传递自解压的JavaScript负载。该方法通过针对非透明alpha层图像值的最低有效位（LSB），成功地在favicon图像中隐藏压缩的JavaScript代码，而不影响视觉效果。研究表明，浏览器在加载页面时会自动获取favicon，从而使嵌入的加载脚本能够在内存中提取和执行负载，形成一个无需额外网络请求的隐蔽通道。我们还评估了该方法的威胁模型，并分析了其对现有安全防护措施的规避能力。'}}}, {'id': 'https://huggingface.co/papers/2507.08924', 'title': 'From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for\n  LLM Evaluation', 'url': 'https://huggingface.co/papers/2507.08924', 'abstract': 'Korean expert-level benchmarks, KMMLU-Redux and KMMLU-Pro, are introduced to evaluate Large Language Models across academic and industrial domains in Korea.  \t\t\t\t\tAI-generated summary \t\t\t\t The development of Large Language Models (LLMs) requires robust benchmarks that encompass not only academic domains but also industrial fields to effectively evaluate their applicability in real-world scenarios. In this paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux, reconstructed from the existing KMMLU, consists of questions from the Korean National Technical Qualification exams, with critical errors removed to enhance reliability. KMMLU-Pro is based on Korean National Professional Licensure exams to reflect professional knowledge in Korea. Our experiments demonstrate that these benchmarks comprehensively represent industrial knowledge in Korea. We release our dataset publicly available.', 'score': 2, 'issue_id': 4819, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': 'f75ad1f8a8d2cda6', 'authors': ['Seokhee Hong', 'Sunkyoung Kim', 'Guijin Son', 'Soyeon Kim', 'Yeonjung Hong', 'Jinsik Lee'], 'affiliations': ['LG AI Research', 'OnelineAI'], 'pdf_title_img': 'assets/pdf/title_img/2507.08924.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#science', '#open_source'], 'emoji': '🇰🇷', 'ru': {'title': 'Новые корейские бенчмарки для оценки LLM в реальных сценариях', 'desc': 'В статье представлены два новых эталонных теста для оценки больших языковых моделей (LLM) на корейском языке: KMMLU-Redux и KMMLU-Pro. KMMLU-Redux основан на вопросах из корейских национальных технических квалификационных экзаменов, а KMMLU-Pro - на экзаменах для получения профессиональных лицензий. Эти тесты охватывают как академические, так и промышленные области знаний в Корее. Эксперименты показали, что данные бенчмарки комплексно отражают профессиональные знания в корейской индустрии.'}, 'en': {'title': 'Evaluating LLMs with Korean Expert Benchmarks', 'desc': 'This paper presents two new benchmarks, KMMLU-Redux and KMMLU-Pro, designed to assess Large Language Models (LLMs) in both academic and industrial contexts in Korea. KMMLU-Redux is an improved version of the original KMMLU, featuring questions from the Korean National Technical Qualification exams with errors corrected for better accuracy. KMMLU-Pro focuses on the Korean National Professional Licensure exams, ensuring that it captures essential professional knowledge. The authors provide evidence that these benchmarks effectively cover the necessary industrial knowledge in Korea and make the dataset publicly available for further research.'}, 'zh': {'title': '评估大型语言模型的新基准', 'desc': '本文介绍了两个用于评估大型语言模型（LLMs）的韩国专家级基准：KMMLU-Redux和KMMLU-Pro。这些基准不仅涵盖学术领域，还包括工业领域，以有效评估LLMs在实际场景中的适用性。KMMLU-Redux是从现有的KMMLU重建而来，包含韩国国家技术资格考试的问题，并去除了关键错误以提高可靠性。KMMLU-Pro则基于韩国国家职业执照考试，反映了韩国的专业知识。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents', '#agi (1)', '#alignment', '#architecture (3)', '#audio (1)', '#benchmark (9)', '#cv (1)', '#data (4)', '#dataset (7)', '#diffusion', '#ethics', '#games', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference', '#interpretability (1)', '#leakage (1)', '#long_context (1)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (2)', '#open_source (2)', '#optimization (4)', '#plp', '#rag', '#reasoning (5)', '#rl (2)', '#rlhf', '#robotics', '#science (1)', '#security (1)', '#small_models (1)', '#story_generation', '#survey', '#synthetic (2)', '#training (6)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-07-15 16:15',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-15 16:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-15 16:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    