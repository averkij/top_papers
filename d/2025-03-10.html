
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 23 papers. March 10.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">10 Ğ¼Ğ°Ñ€Ñ‚Ğ°</span> | <span id="title-articles-count">23 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-07.html">â¬…ï¸ <span id="prev-date">07.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-11.html">â¡ï¸ <span id="next-date">11.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '10 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 10', 'zh': '3æœˆ10æ—¥'};
        let feedDateNext = {'ru': '11.03', 'en': '03/11', 'zh': '3æœˆ11æ—¥'};
        let feedDatePrev = {'ru': '07.03', 'en': '03/07', 'zh': '3æœˆ7æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.05236', 'title': 'Unified Reward Model for Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2503.05236', 'abstract': 'Recent advances in human preference alignment have significantly enhanced multimodal generation and understanding. A key approach is training reward models to guide preference optimization. However, existing models are often task-specific, limiting their adaptability across diverse visual applications. We also argue that jointly learning to assess multiple tasks may foster a synergistic effect, where improved image understanding enhances image generation assessment, and refined image evaluation benefits video assessment through better frame analysis. To this end, this paper proposes UnifiedReward, the first unified reward model for multimodal understanding and generation assessment, enabling both pairwise ranking and pointwise scoring, which can be employed for vision model preference alignment. Specifically, (1) we first develop UnifiedReward on our constructed large-scale human preference dataset, including both image and video generation/understanding tasks. (2) Then, it is utilized to automatically construct high-quality preference pair data based on the vision models, fine-gradually filtering their outputs through pair ranking and point sifting. (3) Finally, these data are used for their preference alignment through Direct Preference Optimization (DPO). Experimental results demonstrate that joint learning to assess diverse visual tasks can lead to substantial mutual benefits and we apply our pipeline to both image and video understanding/generation tasks, significantly improving the performance in each domain.', 'score': 80, 'issue_id': 2607, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '6ebf61a6777b8e4d', 'authors': ['Yibin Wang', 'Yuhang Zang', 'Hao Li', 'Cheng Jin', 'Jiaqi Wang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.05236.jpg', 'data': {'categories': ['#multimodal', '#video', '#cv', '#alignment', '#dataset', '#rlhf', '#rag'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UnifiedReward - Ğ¿ĞµÑ€Ğ²ÑƒÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. UnifiedReward Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ—Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Direct Preference Optimization (DPO).'}, 'en': {'title': 'UnifiedReward: Enhancing Multimodal Learning through Joint Preference Alignment', 'desc': 'This paper introduces UnifiedReward, a novel reward model designed to enhance multimodal understanding and generation in machine learning. It addresses the limitation of existing task-specific models by enabling joint learning across various visual tasks, which improves both image and video assessments. The model is trained on a large-scale human preference dataset and utilizes techniques like pairwise ranking and pointwise scoring for effective preference alignment. Experimental results show that this unified approach leads to significant performance improvements in both image and video tasks, demonstrating the benefits of synergistic learning.'}, 'zh': {'title': 'ç»Ÿä¸€å¥–åŠ±æ¨¡å‹ï¼Œæå‡å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆ', 'desc': 'æœ€è¿‘åœ¨äººç±»åå¥½å¯¹é½æ–¹é¢çš„è¿›å±•æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ç”Ÿæˆå’Œç†è§£çš„èƒ½åŠ›ã€‚å…³é”®æ–¹æ³•æ˜¯è®­ç»ƒå¥–åŠ±æ¨¡å‹æ¥æŒ‡å¯¼åå¥½ä¼˜åŒ–ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸æ˜¯ç‰¹å®šäºä»»åŠ¡çš„ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒè§†è§‰åº”ç”¨ä¸­çš„é€‚åº”æ€§ã€‚æœ¬æ–‡æå‡ºäº†UnifiedRewardï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆè¯„ä¼°çš„ç»Ÿä¸€å¥–åŠ±æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶è¿›è¡Œæˆå¯¹æ’åå’Œé€ç‚¹è¯„åˆ†ï¼Œä»è€Œå®ç°è§†è§‰æ¨¡å‹çš„åå¥½å¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.21263', 'title': 'RuCCoD: Towards Automated ICD Coding in Russian', 'url': 'https://huggingface.co/papers/2502.21263', 'abstract': 'This study investigates the feasibility of automating clinical coding in Russian, a language with limited biomedical resources. We present a new dataset for ICD coding, which includes diagnosis fields from electronic health records (EHRs) annotated with over 10,000 entities and more than 1,500 unique ICD codes. This dataset serves as a benchmark for several state-of-the-art models, including BERT, LLaMA with LoRA, and RAG, with additional experiments examining transfer learning across domains (from PubMed abstracts to medical diagnosis) and terminologies (from UMLS concepts to ICD codes). We then apply the best-performing model to label an in-house EHR dataset containing patient histories from 2017 to 2021. Our experiments, conducted on a carefully curated test set, demonstrate that training with the automated predicted codes leads to a significant improvement in accuracy compared to manually annotated data from physicians. We believe our findings offer valuable insights into the potential for automating clinical coding in resource-limited languages like Russian, which could enhance clinical efficiency and data accuracy in these contexts.', 'score': 61, 'issue_id': 2617, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 28', 'zh': '2æœˆ28æ—¥'}, 'hash': '89ad8229208a4f98', 'authors': ['Aleksandr Nesterov', 'Andrey Sakhovskiy', 'Ivan Sviridov', 'Airat Valiev', 'Vladimir Makharev', 'Petr Anokhin', 'Galina Zubkova', 'Elena Tutubalina'], 'affiliations': ['AIRI, Moscow, Russia', 'HSE University, Moscow, Russia', 'ISP RAS Research Center for Trusted Artificial Intelligence, Moscow, Russia', 'Sber AI Lab, Moscow, Russia', 'Sber AI, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2502.21263.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#benchmark', '#science', '#low_resource', '#healthcare', '#training'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ĞœĞšĞ‘, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 10000 ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ 1500 ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¾Ğ² ĞœĞšĞ‘. ĞŸÑ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº BERT, LLaMA Ñ LoRA Ğ¸ RAG, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ°Ñ… Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ²Ñ€Ğ°Ñ‡ĞµĞ¹.'}, 'en': {'title': 'Automating Clinical Coding: A Leap for Russian Healthcare', 'desc': 'This paper explores the automation of clinical coding in the Russian language, which lacks extensive biomedical resources. It introduces a new dataset for ICD coding, featuring over 10,000 annotated entities and 1,500 unique ICD codes derived from electronic health records. The study benchmarks various advanced models, including BERT and LLaMA, and investigates transfer learning from different medical domains and terminologies. Results show that using automated coding significantly improves accuracy over traditional manual coding by physicians, highlighting the potential for enhanced clinical efficiency in resource-limited settings.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–ä¸´åºŠç¼–ç ï¼šæå‡ä¿„è¯­åŒ»ç–—æ•ˆç‡çš„å¸Œæœ›', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨ä¿„è¯­ä¸­è‡ªåŠ¨åŒ–ä¸´åºŠç¼–ç çš„å¯è¡Œæ€§ï¼Œä¿„è¯­çš„ç”Ÿç‰©åŒ»å­¦èµ„æºç›¸å¯¹æœ‰é™ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ICDç¼–ç æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰çš„è¯Šæ–­å­—æ®µï¼Œæ ‡æ³¨äº†è¶…è¿‡10,000ä¸ªå®ä½“å’Œ1,500å¤šä¸ªç‹¬ç‰¹çš„ICDä»£ç ã€‚é€šè¿‡å¯¹å¤šç§å…ˆè¿›æ¨¡å‹ï¼ˆå¦‚BERTã€LLaMAä¸LoRAã€RAGï¼‰çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥åŠè·¨é¢†åŸŸå’Œæœ¯è¯­çš„è¿ç§»å­¦ä¹ å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è‡ªåŠ¨é¢„æµ‹çš„ä»£ç è¿›è¡Œè®­ç»ƒï¼Œç›¸è¾ƒäºåŒ»ç”Ÿæ‰‹åŠ¨æ ‡æ³¨çš„æ•°æ®ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼Œæ˜¾ç¤ºäº†åœ¨èµ„æºæœ‰é™çš„è¯­è¨€ä¸­è‡ªåŠ¨åŒ–ä¸´åºŠç¼–ç çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05500', 'title': 'EuroBERT: Scaling Multilingual Encoders for European Languages', 'url': 'https://huggingface.co/papers/2503.05500', 'abstract': 'General-purpose multilingual vector representations, used in retrieval, regression and classification, are traditionally obtained from bidirectional encoder models. Despite their wide applicability, encoders have been recently overshadowed by advances in generative decoder-only models. However, many innovations driving this progress are not inherently tied to decoders. In this paper, we revisit the development of multilingual encoders through the lens of these advances, and introduce EuroBERT, a family of multilingual encoders covering European and widely spoken global languages. Our models outperform existing alternatives across a diverse range of tasks, spanning multilingual capabilities, mathematics, and coding, and natively supporting sequences of up to 8,192 tokens. We also examine the design decisions behind EuroBERT, offering insights into our dataset composition and training pipeline. We publicly release the EuroBERT models, including intermediate training checkpoints, together with our training framework.', 'score': 46, 'issue_id': 2613, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': 'befb9ffdb6a6cd73', 'authors': ['Nicolas Boizard', 'Hippolyte Gisserot-Boukhlef', 'Duarte M. Alves', 'AndrÃ© Martins', 'Ayoub Hammal', 'Caio Corro', 'CÃ©line Hudelot', 'Emmanuel Malherbe', 'Etienne Malaboeuf', 'Fanny Jourdan', 'Gabriel Hautreux', 'JoÃ£o Alves', 'Kevin El-Haddad', 'Manuel Faysse', 'Maxime Peyrard', 'Nuno M. Guerreiro', 'Patrick Fernandes', 'Ricardo Rei', 'Pierre Colombo'], 'affiliations': ['Artefact', 'CINES', 'CNRS', 'Carnegie Mellon University', 'Diabolocom', 'Equall', 'INSA Rennes', 'IRISA', 'IRT Saint Exupery', 'ISIA Lab', 'Illuin Technology', 'Instituto Superior Tecnico & Universidade de Lisboa (Lisbon ELLIS Unit)', 'Instituto de Telecomunicacoes', 'LISN', 'MICS, CentraleSupelec, Universite Paris-Saclay', 'Unbabel', 'Universite Grenoble Alpes, Grenoble INP, LIG', 'Universite Paris-Saclay'], 'pdf_title_img': 'assets/pdf/title_img/2503.05500.jpg', 'data': {'categories': ['#multilingual', '#open_source', '#architecture', '#training', '#dataset', '#long_context'], 'emoji': 'ğŸŒ', 'ru': {'title': 'EuroBERT: Ğ’Ğ¾Ğ·Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ EuroBERT - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ĞµĞ²Ñ€Ğ¾Ğ¿ĞµĞ¹ÑĞºĞ¸Ñ… Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². Ğ­Ñ‚Ğ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ EuroBERT Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ´Ğ¾ 8192 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ… Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ pipeline Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'EuroBERT: Advancing Multilingual Encoders for Diverse Tasks', 'desc': 'This paper presents EuroBERT, a new family of multilingual encoder models designed to improve performance in various tasks such as retrieval, regression, and classification. Unlike traditional models that rely heavily on bidirectional encoders, EuroBERT leverages recent advancements in generative models while maintaining the strengths of encoders. The models are capable of handling sequences of up to 8,192 tokens and demonstrate superior performance across multilingual tasks, mathematics, and coding challenges. The authors also provide insights into the dataset and training processes used to develop EuroBERT, along with public access to the models and training framework.'}, 'zh': {'title': 'EuroBERTï¼šæå‡å¤šè¯­è¨€å¤„ç†çš„æ–°é€‰æ‹©', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šè¯­è¨€ç¼–ç å™¨æ¨¡å‹ï¼Œç§°ä¸ºEuroBERTï¼Œæ—¨åœ¨æå‡å¤šè¯­è¨€æ£€ç´¢ã€å›å½’å’Œåˆ†ç±»ä»»åŠ¡çš„æ€§èƒ½ã€‚å°½ç®¡ç”Ÿæˆè§£ç å™¨æ¨¡å‹è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†æˆ‘ä»¬è®¤ä¸ºå¤šè¯­è¨€ç¼–ç å™¨ä»ç„¶å…·æœ‰é‡è¦ä»·å€¼ã€‚EuroBERTè¦†ç›–äº†æ¬§æ´²åŠå¹¿æ³›ä½¿ç”¨çš„å…¨çƒè¯­è¨€ï¼Œå¹¶åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜åˆ†äº«äº†EuroBERTçš„è®¾è®¡å†³ç­–ã€æ•°æ®é›†æ„æˆå’Œè®­ç»ƒæµç¨‹ï¼Œå¹¶å…¬å¼€å‘å¸ƒäº†æ¨¡å‹åŠå…¶è®­ç»ƒæ¡†æ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05085', 'title': 'S2S-Arena, Evaluating Speech2Speech Protocols on Instruction Following\n  with Paralinguistic Information', 'url': 'https://huggingface.co/papers/2503.05085', 'abstract': 'The rapid development of large language models (LLMs) has brought significant attention to speech models, particularly recent progress in speech2speech protocols supporting speech input and output. However, the existing benchmarks adopt automatic text-based evaluators for evaluating the instruction following ability of these models lack consideration for paralinguistic information in both speech understanding and generation. To address these issues, we introduce S2S-Arena, a novel arena-style S2S benchmark that evaluates instruction-following capabilities with paralinguistic information in both speech-in and speech-out across real-world tasks. We design 154 samples that fused TTS and live recordings in four domains with 21 tasks and manually evaluate existing popular speech models in an arena-style manner. The experimental results show that: (1) in addition to the superior performance of GPT-4o, the speech model of cascaded ASR, LLM, and TTS outperforms the jointly trained model after text-speech alignment in speech2speech protocols; (2) considering paralinguistic information, the knowledgeability of the speech model mainly depends on the LLM backbone, and the multilingual support of that is limited by the speech module; (3) excellent speech models can already understand the paralinguistic information in speech input, but generating appropriate audio with paralinguistic information is still a challenge.', 'score': 37, 'issue_id': 2619, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': 'aa9d1f284b6fe901', 'authors': ['Feng Jiang', 'Zhiyu Lin', 'Fan Bu', 'Yuhao Du', 'Benyou Wang', 'Haizhou Li'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2503.05085.jpg', 'data': {'categories': ['#audio', '#multilingual', '#benchmark'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'S2S-Arena: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ S2S-Arena - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ 154 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ° Ğ² 4 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ… Ñ 21 Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ñ… ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¸ Ğ¶Ğ¸Ğ²ÑƒÑ Ñ€ĞµÑ‡ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (ASR+LLM+TTS) Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… speech2speech. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸, Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹.'}, 'en': {'title': 'Enhancing Speech Models with Paralinguistic Insights', 'desc': "This paper presents S2S-Arena, a new benchmark for evaluating speech-to-speech (S2S) models that incorporates paralinguistic information, which includes elements like tone and emotion in speech. The authors highlight that current evaluation methods primarily rely on text-based metrics, which do not adequately assess the models' ability to understand and generate speech with these nuances. Through a series of 154 samples across various tasks, the study reveals that models like GPT-4o and cascaded ASR-LLM-TTS outperform others when considering paralinguistic factors. The findings indicate that while advanced speech models can comprehend paralinguistic cues, generating speech that accurately reflects these cues remains a significant challenge."}, 'zh': {'title': 'å¼•å…¥å‰¯è¯­è¨€ä¿¡æ¯çš„è¯­éŸ³æ¨¡å‹è¯„ä¼°æ–°æ ‡å‡†', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè¯­éŸ³æ¨¡å‹ä¹Ÿå—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œå°¤å…¶æ˜¯åœ¨è¯­éŸ³è¾“å…¥å’Œè¾“å‡ºçš„è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆspeech2speechï¼‰åè®®æ–¹é¢çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦ä¾èµ–è‡ªåŠ¨æ–‡æœ¬è¯„ä¼°å™¨æ¥è¯„ä¼°è¿™äº›æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œæœªèƒ½è€ƒè™‘è¯­éŸ³ç†è§£å’Œç”Ÿæˆä¸­çš„å‰¯è¯­è¨€ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†S2S-Arenaï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„ç«æŠ€åœºé£æ ¼çš„S2SåŸºå‡†ï¼Œè¯„ä¼°åœ¨çœŸå®ä»»åŠ¡ä¸­è¯­éŸ³è¾“å…¥å’Œè¾“å‡ºçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œå¹¶è€ƒè™‘å‰¯è¯­è¨€ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡GPT-4oè¡¨ç°ä¼˜è¶Šï¼Œä½†åœ¨è¯­éŸ³åˆ°è¯­éŸ³åè®®ä¸­ï¼Œçº§è”çš„ASRã€LLMå’ŒTTSè¯­éŸ³æ¨¡å‹åœ¨æ–‡æœ¬-è¯­éŸ³å¯¹é½åä¼˜äºè”åˆè®­ç»ƒæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05179', 'title': 'Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching', 'url': 'https://huggingface.co/papers/2503.05179', 'abstract': 'Recent advances in large language models have demonstrated remarkable reasoning capabilities through Chain of Thought (CoT) prompting, but often at the cost of excessive verbosity in their intermediate outputs, which increases computational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting framework that combines cognitive-inspired reasoning paradigms with linguistic constraints to minimize token usage while preserving reasoning accuracy. SoT is designed as a flexible framework that can incorporate any custom reasoning paradigms based on cognitive science, and we instantiate it with three such paradigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each tailored to different reasoning tasks and selected dynamically via a lightweight routing model. Through comprehensive evaluation across 15 reasoning datasets with multiple languages and multimodal scenarios, we demonstrate that SoT achieves token reductions of 76% with negligible accuracy impact. In certain domains like mathematical and multi-hop reasoning, it even improves accuracy while using significantly fewer tokens. Our code is publicly available: https://www.github.com/SimonAytes/SoT.', 'score': 31, 'issue_id': 2607, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': 'e02cb6f62715b753', 'authors': ['Simon A. Aytes', 'Jinheon Baek', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.05179.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#reasoning', '#multilingual', '#optimization', '#open_source', '#math', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Sketch-of-Thought (SoT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾-Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸. SoT Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹: Conceptual Chaining, Chunked Symbolism Ğ¸ Expert Lexicons, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 15 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SoT ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 76% Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ° Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ´Ğ°Ğ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞµĞµ.'}, 'en': {'title': 'Efficient Reasoning with Sketch-of-Thought', 'desc': 'This paper presents a new prompting framework called Sketch-of-Thought (SoT) that enhances reasoning in large language models while reducing the number of tokens used. SoT integrates cognitive-inspired reasoning methods with linguistic constraints to maintain accuracy without excessive verbosity. The framework is adaptable, allowing for the inclusion of various reasoning paradigms, which are dynamically selected based on the task at hand. Evaluation shows that SoT can reduce token usage by 76% with little to no loss in accuracy, and in some cases, it even improves performance in specific reasoning tasks.'}, 'zh': {'title': 'æ€ç»´è‰å›¾ï¼šé«˜æ•ˆæ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æç¤ºæ¡†æ¶ï¼Œç§°ä¸ºæ€ç»´è‰å›¾ï¼ˆSketch-of-Thoughtï¼ŒSoTï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘ä¸­é—´è¾“å‡ºçš„å†—é•¿æ€§ã€‚SoTç»“åˆäº†è®¤çŸ¥ç§‘å­¦çš„æ¨ç†èŒƒå¼å’Œè¯­è¨€çº¦æŸï¼Œä»¥æœ€å°åŒ–ä»¤ç‰Œä½¿ç”¨é‡ï¼ŒåŒæ—¶ä¿æŒæ¨ç†çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶çµæ´»ï¼Œå¯ä»¥æ ¹æ®è®¤çŸ¥ç§‘å­¦çš„ä¸åŒæ¨ç†èŒƒå¼è¿›è¡Œå®šåˆ¶ï¼Œå¹¶é€šè¿‡è½»é‡çº§è·¯ç”±æ¨¡å‹åŠ¨æ€é€‰æ‹©ã€‚é€šè¿‡åœ¨15ä¸ªæ¨ç†æ•°æ®é›†ä¸Šçš„å…¨é¢è¯„ä¼°ï¼ŒSoTå®ç°äº†76%çš„ä»¤ç‰Œå‡å°‘ï¼Œä¸”å¯¹å‡†ç¡®æ€§å½±å“å¾®ä¹å…¶å¾®ï¼Œç”šè‡³åœ¨æŸäº›é¢†åŸŸæé«˜äº†å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02130', 'title': 'Forgetting Transformer: Softmax Attention with a Forget Gate', 'url': 'https://huggingface.co/papers/2503.02130', 'abstract': 'An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism the Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer\'s superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a "Pro" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.', 'score': 16, 'issue_id': 2607, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '4c39f334b6c4ed28', 'authors': ['Zhixuan Lin', 'Evgenii Nikishin', 'Xu Owen He', 'Aaron Courville'], 'affiliations': ['MakerMaker AI', 'Mila & Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.02130.jpg', 'data': {'categories': ['#long_context', '#architecture', '#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Forgetting Transformer: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Forgetting Transformer (FoX), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ 'Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ' Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. FoX Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ° Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ¼ FlashAttention Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ FoX ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."}, 'en': {'title': 'Enhancing Transformers with Forgetting Attention for Superior Performance', 'desc': 'This paper introduces a new attention mechanism called Forgetting Attention, which integrates a forget gate into Transformer models. The Forgetting Transformer (FoX) leverages this mechanism to improve performance on various language modeling tasks, particularly those involving long contexts. FoX not only matches the performance of traditional Transformers on long-context tasks but also excels in short-context and length extrapolation tasks. Additionally, it is compatible with the FlashAttention algorithm and eliminates the need for positional embeddings, enhancing its efficiency and effectiveness.'}, 'zh': {'title': 'é—å¿˜å˜æ¢å™¨ï¼šæå‡é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡çš„åˆ©å™¨', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºé—å¿˜æ³¨æ„åŠ›ï¼ˆForgetting Attentionï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†é—å¿˜é—¨é›†æˆåˆ°Transformeræ¨¡å‹ä¸­ã€‚é€šè¿‡ä»¥æ•°æ®ä¸ºä¾èµ–çš„æ–¹å¼é™ä½æœªå½’ä¸€åŒ–æ³¨æ„åŠ›åˆ†æ•°ï¼Œé—å¿˜æ³¨æ„åŠ›ä½¿å¾—Transformeråœ¨é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡å’Œé•¿åº¦å¤–æ¨ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„Transformerã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªâ€œProâ€æ¨¡å—ï¼Œç»“åˆäº†é€’å½’åºåˆ—æ¨¡å‹ä¸­çš„ä¸€äº›å¸¸è§æ¶æ„ç»„ä»¶ï¼Œæ˜¾è‘—æå‡äº†FoXå’ŒTransformerçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒFoXåœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­ä¿æŒäº†Transformerçš„ä¼˜åŠ¿ï¼Œè¶…è¶Šäº†å…¶ä»–é€’å½’åºåˆ—æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05592', 'title': 'R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.05592', 'abstract': 'Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models~(LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their internal knowledge to solve problems, which can be inadequate for time-sensitive or knowledge-intensive questions, leading to inaccuracies and hallucinations. To address this, we propose R1-Searcher, a novel two-stage outcome-based RL approach designed to enhance the search capabilities of LLMs. This method allows LLMs to autonomously invoke external search systems to access additional knowledge during the reasoning process. Our framework relies exclusively on RL, without requiring process rewards or distillation for a cold start. % effectively generalizing to out-of-domain datasets and supporting both Base and Instruct models. Our experiments demonstrate that our method significantly outperforms previous strong RAG methods, even when compared to the closed-source GPT-4o-mini.', 'score': 12, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '6af1f8cd890c69ae', 'authors': ['Huatong Song', 'Jinhao Jiang', 'Yingqian Min', 'Jie Chen', 'Zhipeng Chen', 'Wayne Xin Zhao', 'Lei Fang', 'Ji-Rong Wen'], 'affiliations': ['DataCanvas', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.05592.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#rl', '#rag', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'R1-Searcher - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ñ… Ğ¸Ğ»Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ R1-Searcher Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ RAG, Ğ´Ğ°Ğ¶Ğµ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ GPT-4o-mini.'}, 'en': {'title': 'Enhancing LLM Reasoning with External Knowledge Search', 'desc': 'This paper introduces R1-Searcher, a new approach that improves the reasoning abilities of Large Language Models (LLMs) using reinforcement learning (RL). Unlike existing models that depend solely on their internal knowledge, R1-Searcher enables LLMs to access external search systems for additional information, which helps in answering complex and time-sensitive questions more accurately. The method operates in two stages and does not require initial rewards or distillation, making it easier to implement. Experimental results show that R1-Searcher outperforms previous retrieval-augmented generation (RAG) methods, demonstrating its effectiveness across various datasets.'}, 'zh': {'title': 'å¢å¼ºæ¨ç†èƒ½åŠ›ï¼ŒR1-SearcheråŠ©åŠ›LLMs', 'desc': 'ç°æœ‰çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚å°½ç®¡å®ƒä»¬åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†æ—¶é—´æ•æ„Ÿæˆ–çŸ¥è¯†å¯†é›†çš„é—®é¢˜æ—¶ï¼Œå¾€å¾€ä¾èµ–å†…éƒ¨çŸ¥è¯†ï¼Œå¯¼è‡´ä¸å‡†ç¡®å’Œå¹»è§‰ç°è±¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†R1-Searcherï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åŸºäºç»“æœçš„ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºLLMsçš„æœç´¢èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å…è®¸LLMsåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»è°ƒç”¨å¤–éƒ¨æœç´¢ç³»ç»Ÿï¼Œä»¥è·å–é¢å¤–çŸ¥è¯†ï¼Œä»è€Œæ˜¾è‘—æé«˜æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05639', 'title': 'VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play\n  Context Control', 'url': 'https://huggingface.co/papers/2503.05639', 'abstract': "Video inpainting, which aims to restore corrupted video content, has experienced substantial progress. Despite these advances, existing methods, whether propagating unmasked region pixels through optical flow and receptive field priors, or extending image-inpainting models temporally, face challenges in generating fully masked objects or balancing the competing objectives of background context preservation and foreground generation in one model, respectively. To address these limitations, we propose a novel dual-stream paradigm VideoPainter that incorporates an efficient context encoder (comprising only 6% of the backbone parameters) to process masked videos and inject backbone-aware background contextual cues to any pre-trained video DiT, producing semantically consistent content in a plug-and-play manner. This architectural separation significantly reduces the model's learning complexity while enabling nuanced integration of crucial background context. We also introduce a novel target region ID resampling technique that enables any-length video inpainting, greatly enhancing our practical applicability. Additionally, we establish a scalable dataset pipeline leveraging current vision understanding models, contributing VPData and VPBench to facilitate segmentation-based inpainting training and assessment, the largest video inpainting dataset and benchmark to date with over 390K diverse clips. Using inpainting as a pipeline basis, we also explore downstream applications including video editing and video editing pair data generation, demonstrating competitive performance and significant practical potential. Extensive experiments demonstrate VideoPainter's superior performance in both any-length video inpainting and editing, across eight key metrics, including video quality, mask region preservation, and textual coherence.", 'score': 11, 'issue_id': 2614, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '735dd34a3043623a', 'authors': ['Yuxuan Bian', 'Zhaoyang Zhang', 'Xuan Ju', 'Mingdeng Cao', 'Liangbin Xie', 'Ying Shan', 'Qiang Xu'], 'affiliations': ['Tencent ARC Lab, China', 'The Chinese University of Hong Kong, China', 'The University of Tokyo, Japan', 'University of Macau, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.05639.jpg', 'data': {'categories': ['#video', '#benchmark', '#dataset', '#games', '#architecture', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'VideoPainter: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° - VideoPainter. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DiT. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ñ€ĞµÑĞµĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ° ID Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ»ÑĞ±Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VPData Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VPBench Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ°.'}, 'en': {'title': 'Revolutionizing Video Inpainting with VideoPainter', 'desc': "This paper presents VideoPainter, a new approach to video inpainting that effectively restores missing video content. It introduces a dual-stream architecture that uses a lightweight context encoder to enhance background information while maintaining the integrity of foreground objects. The method also features a novel target region ID resampling technique, allowing for flexible inpainting of videos of any length. Additionally, the authors provide a comprehensive dataset and benchmark for training and evaluating video inpainting models, showcasing VideoPainter's strong performance across various metrics."}, 'zh': {'title': 'è§†é¢‘ä¿®å¤çš„æ–°çºªå…ƒï¼šVideoPainter', 'desc': 'è§†é¢‘ä¿®å¤æ—¨åœ¨æ¢å¤æŸåçš„è§†é¢‘å†…å®¹ï¼Œè¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç°æœ‰æ–¹æ³•åœ¨ç”Ÿæˆå®Œå…¨é®æŒ¡çš„ç‰©ä½“æˆ–å¹³è¡¡èƒŒæ™¯ä¿ç•™ä¸å‰æ™¯ç”Ÿæˆæ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒæµæ¶æ„VideoPainterï¼Œåˆ©ç”¨é«˜æ•ˆçš„ä¸Šä¸‹æ–‡ç¼–ç å™¨å¤„ç†é®æŒ¡è§†é¢‘ï¼Œå¹¶å°†èƒŒæ™¯ä¸Šä¸‹æ–‡ä¿¡æ¯æ³¨å…¥åˆ°é¢„è®­ç»ƒçš„è§†é¢‘æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„ç›®æ ‡åŒºåŸŸIDé‡é‡‡æ ·æŠ€æœ¯ï¼Œæ”¯æŒä»»æ„é•¿åº¦çš„è§†é¢‘ä¿®å¤ï¼Œæå¤§åœ°æå‡äº†å®é™…åº”ç”¨çš„å¯è¡Œæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05379', 'title': 'R1-Omni: Explainable Omni-Multimodal Emotion Recognition with\n  Reinforcing Learning', 'url': 'https://huggingface.co/papers/2503.05379', 'abstract': "In this work, we present the first application of Reinforcement Learning with Verifiable Reward (RLVR) to an Omni-multimodal large language model in the context of emotion recognition, a task where both visual and audio modalities play crucial roles. We leverage RLVR to optimize the Omni model, significantly enhancing its performance in three key aspects: reasoning capability, emotion recognition accuracy, and generalization ability. The introduction of RLVR not only improves the model's overall performance on in-distribution data but also demonstrates superior robustness when evaluated on out-of-distribution datasets. More importantly, the improved reasoning capability enables clear analysis of the contributions of different modalities, particularly visual and audio information, in the emotion recognition process. This provides valuable insights into the optimization of multimodal large language models.", 'score': 9, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '34c6afbd7ae83841', 'authors': ['Jiaxing Zhao', 'Xihan Wei', 'Liefeng Bo'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.05379.jpg', 'data': {'categories': ['#optimization', '#rl', '#multimodal', '#audio', '#cv', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'RLVR: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ (RLVR) Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹. RLVR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Omni-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ĞµÑ‘ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ĞºĞ»Ğ°Ğ´ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Emotion Recognition with RLVR in Multimodal Models', 'desc': "This paper introduces a novel approach called Reinforcement Learning with Verifiable Reward (RLVR) applied to an Omni-multimodal large language model for emotion recognition. The use of RLVR enhances the model's reasoning skills, accuracy in recognizing emotions, and its ability to generalize across different datasets. The model not only performs better on familiar data but also shows increased robustness when tested on new, unseen data. Additionally, the improved reasoning capability allows for a detailed understanding of how visual and audio inputs contribute to the emotion recognition task."}, 'zh': {'title': 'æƒ…æ„Ÿè¯†åˆ«ä¸­çš„å…¨æ¨¡æ€å¼ºåŒ–å­¦ä¹ æ–°çªç ´', 'desc': 'æœ¬ç ”ç©¶é¦–æ¬¡å°†å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰åº”ç”¨äºæƒ…æ„Ÿè¯†åˆ«çš„å…¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚åœ¨è¿™ä¸ªä»»åŠ¡ä¸­ï¼Œè§†è§‰å’ŒéŸ³é¢‘æ¨¡æ€èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚é€šè¿‡ä½¿ç”¨RLVRï¼Œæˆ‘ä»¬æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ã€æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ç­‰ä¸‰ä¸ªå…³é”®æ–¹é¢çš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒRLVRçš„å¼•å…¥ä¸ä»…æé«˜äº†æ¨¡å‹åœ¨åŒåˆ†å¸ƒæ•°æ®ä¸Šçš„æ•´ä½“æ€§èƒ½ï¼Œè¿˜åœ¨å¼‚åˆ†å¸ƒæ•°æ®é›†ä¸Šå±•ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05652', 'title': 'BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation\n  for Everyday Household Activities', 'url': 'https://huggingface.co/papers/2503.05652', 'abstract': "Real-world household tasks present significant challenges for mobile manipulation robots. An analysis of existing robotics benchmarks reveals that successful task performance hinges on three key whole-body control capabilities: bimanual coordination, stable and precise navigation, and extensive end-effector reachability. Achieving these capabilities requires careful hardware design, but the resulting system complexity further complicates visuomotor policy learning. To address these challenges, we introduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for whole-body manipulation in diverse household tasks. Built on a bimanual, wheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body teleoperation interface for data collection and a novel algorithm for learning whole-body visuomotor policies. We evaluate BRS on five challenging household tasks that not only emphasize the three core capabilities but also introduce additional complexities, such as long-range navigation, interaction with articulated and deformable objects, and manipulation in confined spaces. We believe that BRS's integrated robotic embodiment, data collection interface, and learning framework mark a significant step toward enabling real-world whole-body manipulation for everyday household tasks. BRS is open-sourced at https://behavior-robot-suite.github.io/", 'score': 8, 'issue_id': 2608, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '52a7efb2f40f020a', 'authors': ['Yunfan Jiang', 'Ruohan Zhang', 'Josiah Wong', 'Chen Wang', 'Yanjie Ze', 'Hang Yin', 'Cem Gokmen', 'Shuran Song', 'Jiajun Wu', 'Li Fei-Fei'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.05652.jpg', 'data': {'categories': ['#robotics', '#open_source', '#dataset', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BEHAVIOR Robot Suite (BRS) - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. BRS Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´Ğ²ÑƒÑ€ÑƒĞºĞ¾Ğ¼ ĞºĞ¾Ğ»ĞµÑĞ½Ğ¾Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğµ Ñ 4-Ğ¾ÑĞµĞ²Ñ‹Ğ¼ Ñ‚Ğ¾Ñ€ÑĞ¾Ğ¼ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ñ‚ĞµĞ»ĞµÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ¾Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±Ñ‹Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ¸Ğ¼Ğ°Ğ½ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑĞ³Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ². BRS Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ²Ğ¿ĞµÑ€ĞµĞ´ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ….'}, 'en': {'title': 'Empowering Robots for Everyday Household Tasks with BRS', 'desc': 'This paper presents the BEHAVIOR Robot Suite (BRS), a framework designed to enhance mobile manipulation robots for household tasks. It identifies three essential capabilities for effective task performance: bimanual coordination, stable navigation, and extensive reachability. The BRS integrates a teleoperation interface for data collection and a novel algorithm for learning visuomotor policies, addressing the complexities of hardware design and policy learning. The framework is evaluated on five challenging tasks that test these capabilities in real-world scenarios, aiming to improve robotic manipulation in everyday environments.'}, 'zh': {'title': 'å®ç°å®¶åº­ä»»åŠ¡çš„å…¨èº«æ“æ§æœºå™¨äºº', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†BEHAVIORæœºå™¨äººå¥—ä»¶ï¼ˆBRSï¼‰ï¼Œæ—¨åœ¨è§£å†³ç§»åŠ¨æ“ä½œæœºå™¨äººåœ¨å®¶åº­ä»»åŠ¡ä¸­é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒæˆåŠŸå®Œæˆä»»åŠ¡ä¾èµ–äºä¸‰é¡¹å…³é”®çš„å…¨èº«æ§åˆ¶èƒ½åŠ›ï¼šåŒæ‰‹åè°ƒã€ç¨³å®šç²¾ç¡®çš„å¯¼èˆªå’Œå¹¿æ³›çš„æœ«ç«¯æ‰§è¡Œå™¨å¯è¾¾æ€§ã€‚BRSæ¡†æ¶ç»“åˆäº†ä¸€ä¸ªåŒæ‰‹è½®å¼æœºå™¨äººå’Œ4è‡ªç”±åº¦çš„èº¯å¹²ï¼Œæä¾›äº†ä¸€ç§ç»æµé«˜æ•ˆçš„å…¨èº«é¥æ“ä½œæ¥å£ç”¨äºæ•°æ®æ”¶é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°ç®—æ³•ç”¨äºå­¦ä¹ å…¨èº«è§†è§‰è¿åŠ¨ç­–ç•¥ã€‚é€šè¿‡åœ¨äº”ä¸ªå¤æ‚çš„å®¶åº­ä»»åŠ¡ä¸Šè¯„ä¼°BRSï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿è·ç¦»å¯¼èˆªã€ä¸å¯åŠ¨å’Œå¯å˜å½¢ç‰©ä½“çš„äº¤äº’ä»¥åŠåœ¨ç‹­å°ç©ºé—´ä¸­çš„æ“ä½œèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05638', 'title': 'TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos\n  via Diffusion Models', 'url': 'https://huggingface.co/papers/2503.05638', 'abstract': 'We present TrajectoryCrafter, a novel approach to redirect camera trajectories for monocular videos. By disentangling deterministic view transformations from stochastic content generation, our method achieves precise control over user-specified camera trajectories. We propose a novel dual-stream conditional video diffusion model that concurrently integrates point cloud renders and source videos as conditions, ensuring accurate view transformations and coherent 4D content generation. Instead of leveraging scarce multi-view videos, we curate a hybrid training dataset combining web-scale monocular videos with static multi-view datasets, by our innovative double-reprojection strategy, significantly fostering robust generalization across diverse scenes. Extensive evaluations on multi-view and large-scale monocular videos demonstrate the superior performance of our method.', 'score': 8, 'issue_id': 2612, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '33c7af8e9a7df61e', 'authors': ['Mark YU', 'Wenbo Hu', 'Jinbo Xing', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG'], 'pdf_title_img': 'assets/pdf/title_img/2503.05638.jpg', 'data': {'categories': ['#dataset', '#3d', '#optimization', '#diffusion', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'TrajectoryCrafter - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´Ğ° Ğ¸ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ ÑƒÑĞ»Ğ¾Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ñ€ĞµĞ½Ğ´ĞµÑ€Ñ‹ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹.'}, 'en': {'title': 'Mastering Camera Movement in Monocular Videos', 'desc': 'TrajectoryCrafter is a new method designed to change camera paths in single-camera videos. It separates the predictable changes in view from the random elements in the video content, allowing for better control over how the camera moves. The approach uses a dual-stream conditional video diffusion model that combines 3D point cloud images and original videos to ensure smooth transitions and realistic video generation. By creating a unique training dataset that merges large amounts of single-camera videos with some multi-camera data, the method shows improved performance across various scenes.'}, 'zh': {'title': 'ç²¾å‡†æ§åˆ¶æ‘„åƒæœºè½¨è¿¹çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºTrajectoryCrafterçš„æ–°æ–¹æ³•ï¼Œç”¨äºé‡å®šå‘å•ç›®è§†é¢‘çš„æ‘„åƒæœºè½¨è¿¹ã€‚é€šè¿‡å°†ç¡®å®šæ€§çš„è§†å›¾å˜æ¢ä¸éšæœºå†…å®¹ç”Ÿæˆåˆ†ç¦»ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç²¾ç¡®æ§åˆ¶ç”¨æˆ·æŒ‡å®šçš„æ‘„åƒæœºè½¨è¿¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒæµæ¡ä»¶è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ŒåŒæ—¶æ•´åˆç‚¹äº‘æ¸²æŸ“å’Œæºè§†é¢‘ä½œä¸ºæ¡ä»¶ï¼Œç¡®ä¿å‡†ç¡®çš„è§†å›¾å˜æ¢å’Œä¸€è‡´çš„4Då†…å®¹ç”Ÿæˆã€‚é€šè¿‡åˆ›æ–°çš„åŒé‡é‡æŠ•å½±ç­–ç•¥ï¼Œæˆ‘ä»¬ç»“åˆäº†ç½‘ç»œè§„æ¨¡çš„å•ç›®è§†é¢‘å’Œé™æ€å¤šè§†è§’æ•°æ®é›†ï¼Œæ˜¾è‘—æé«˜äº†åœ¨ä¸åŒåœºæ™¯ä¸­çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04824', 'title': 'ProReflow: Progressive Reflow with Decomposed Velocity', 'url': 'https://huggingface.co/papers/2503.04824', 'abstract': 'Diffusion models have achieved significant progress in both image and video generation while still suffering from huge computation costs. As an effective solution, flow matching aims to reflow the diffusion process of diffusion models into a straight line for a few-step and even one-step generation. However, in this paper, we suggest that the original training pipeline of flow matching is not optimal and introduce two techniques to improve it. Firstly, we introduce progressive reflow, which progressively reflows the diffusion models in local timesteps until the whole diffusion progresses, reducing the difficulty of flow matching. Second, we introduce aligned v-prediction, which highlights the importance of direction matching in flow matching over magnitude matching. Experimental results on SDv1.5 and SDXL demonstrate the effectiveness of our method, for example, conducting on SDv1.5 achieves an FID of 10.70 on MSCOCO2014 validation set with only 4 sampling steps, close to our teacher model (32 DDIM steps, FID = 10.05).', 'score': 8, 'issue_id': 2616, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': '3380a5ba02714266', 'authors': ['Lei Ke', 'Haohang Xu', 'Xuefei Ning', 'Yu Li', 'Jiajun Li', 'Haoling Li', 'Yuxuan Lin', 'Dongsheng Jiang', 'Yujiu Yang', 'Linfeng Zhang'], 'affiliations': ['Huawei Inc.', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04824.jpg', 'data': {'categories': ['#training', '#cv', '#diffusion', '#optimization'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ 'progressive reflow', ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'aligned v-prediction', Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… SDv1.5 Ğ¸ SDXL Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."}, 'en': {'title': 'Streamlining Diffusion: Faster Generation with Flow Matching Enhancements', 'desc': 'This paper addresses the high computational costs associated with diffusion models used for image and video generation. It proposes flow matching as a method to streamline the diffusion process, allowing for faster generation in fewer steps. The authors introduce two enhancements: progressive reflow, which simplifies the flow matching by gradually adjusting local timesteps, and aligned v-prediction, which emphasizes the importance of direction over magnitude in flow matching. Experimental results show that their approach significantly improves performance, achieving competitive results with fewer sampling steps compared to traditional methods.'}, 'zh': {'title': 'ä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„æµåŒ¹é…è®­ç»ƒ', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è®¡ç®—æˆæœ¬ä»ç„¶å¾ˆé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒæµåŒ¹é…æŠ€æœ¯å°†æ‰©æ•£è¿‡ç¨‹é‡æ–°è°ƒæ•´ä¸ºç›´çº¿ï¼Œä»¥å®ç°å°‘æ­¥ç”šè‡³ä¸€æ­¥çš„ç”Ÿæˆã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§æ”¹è¿›æµåŒ¹é…è®­ç»ƒæµç¨‹çš„æŠ€æœ¯ï¼šé€æ­¥é‡æ–°æµåŠ¨å’Œå¯¹é½çš„vé¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆè´¨é‡ä¸Šæ¥è¿‘æ•™å¸ˆæ¨¡å‹ï¼ŒåŒæ—¶å¤§å¹…å‡å°‘äº†é‡‡æ ·æ­¥éª¤ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04872', 'title': 'TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation', 'url': 'https://huggingface.co/papers/2503.04872', 'abstract': 'The challenge of reducing the size of Large Language Models (LLMs) while maintaining their performance has gained significant attention. However, existing methods, such as model distillation and transfer learning, often fail to achieve high accuracy. To address this limitation, we introduce the Branch-Merge distillation approach, which enhances model compression through two phases: (1) the Branch Phase, where knowledge from a large teacher model is selectively distilled into specialized student models via domain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where these student models are merged to enable cross-domain knowledge transfer and improve generalization. We validate our distillation approach using DeepSeek-R1 as the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting merged model, TinyR1-32B-Preview, outperforms its counterpart DeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics (+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving near-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge distillation approach provides a scalable solution for creating smaller, high-performing LLMs with reduced computational cost and time.', 'score': 6, 'issue_id': 2609, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '94defd7f9d19776e', 'authors': ['Lin Sun', 'Guangxiang Zhao', 'Xiaoqi Jian', 'Yuhan Wu', 'Weihong Lin', 'Yongfu Zhu', 'Change Jia', 'Linglin Zhang', 'Jinzhu Wu', 'Junfeng Ran', 'Sai-er Hu', 'Zihan Jiang', 'Junting Zhou', 'Wenrui Liu', 'Bin Cui', 'Tong Yang', 'Xiangzheng Zhang'], 'affiliations': ['Peking University', 'Qiyuan Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.04872.jpg', 'data': {'categories': ['#small_models', '#training', '#transfer_learning', '#optimization'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ’ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Branch-Merge distillation. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ñ„Ğ°Ğ·: Branch, Ğ³Ğ´Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¸, Ğ¸ Merge, Ğ³Ğ´Ğµ ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ TinyR1-32B-Preview Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ…, Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… LLM Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Branch-Merge: Compressing LLMs Without Compromise!', 'desc': 'This paper presents a new method called Branch-Merge distillation to reduce the size of Large Language Models (LLMs) while keeping their performance high. It consists of two main phases: the Branch Phase, where knowledge from a large teacher model is distilled into smaller, specialized student models through supervised fine-tuning, and the Merge Phase, where these student models are combined to enhance knowledge transfer across different domains. The approach was tested using specific models and showed that the merged model, TinyR1-32B-Preview, outperformed the individual student model in various tasks, including Mathematics, Coding, and Science. Overall, this method offers an effective way to create smaller LLMs that maintain strong performance and are more efficient in terms of computational resources.'}, 'zh': {'title': 'åˆ†æ”¯åˆå¹¶è’¸é¦ï¼šé«˜æ•ˆå‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹è’¸é¦æ–¹æ³•ï¼Œç§°ä¸ºåˆ†æ”¯åˆå¹¶è’¸é¦ï¼Œæ—¨åœ¨åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹çš„ä½“ç§¯ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šåˆ†æ”¯é˜¶æ®µé€šè¿‡é¢†åŸŸç‰¹å®šçš„ç›‘ç£å¾®è°ƒå°†çŸ¥è¯†ä»å¤§å‹æ•™å¸ˆæ¨¡å‹é€‰æ‹©æ€§åœ°è’¸é¦åˆ°ä¸“é—¨çš„å­¦ç”Ÿæ¨¡å‹ä¸­ï¼›åˆå¹¶é˜¶æ®µåˆ™å°†è¿™äº›å­¦ç”Ÿæ¨¡å‹åˆå¹¶ï¼Œä»¥å®ç°è·¨é¢†åŸŸçŸ¥è¯†è½¬ç§»å¹¶æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆå¹¶åçš„æ¨¡å‹TinyR1-32B-Previewåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¶å¯¹åº”çš„å­¦ç”Ÿæ¨¡å‹DeepSeek-R1-Distill-Qwen-32Bã€‚è¯¥æ–¹æ³•ä¸ºåˆ›å»ºæ›´å°ä¸”é«˜æ€§èƒ½çš„è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œé™ä½äº†è®¡ç®—æˆæœ¬å’Œæ—¶é—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05447', 'title': 'Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts', 'url': 'https://huggingface.co/papers/2503.05447', 'abstract': 'Linear Sequence Modeling (LSM) like linear attention, state space models and linear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant architectural improvements. In this paper, we introduce Linear-MoE, a production-level system for modeling and training large-scale models that integrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules for linear-complexity sequence modeling and MoE layers for sparsely activation, aiming to offer high performance with efficient training. The Linear-MoE system comprises: 1) Modeling subsystem, which provides a unified framework supporting all instances of LSM. and 2) Training subsystem, which facilitates efficient training by incorporating various advanced parallelism technologies, particularly Sequence Parallelism designed for Linear-MoE models. Additionally, we explore hybrid models that combine Linear-MoE layers with standard Transformer-MoE layers with its Sequence Parallelism to further enhance model flexibility and performance. Evaluations on two model series, A0.3B-2B and A1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining competitive performance on various benchmarks, showcasing its potential as a next-generation foundational model architecture. Code: https://github.com/OpenSparseLLMs/Linear-MoE.', 'score': 5, 'issue_id': 2614, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '3975a97b4236e791', 'authors': ['Weigao Sun', 'Disen Lan', 'Tong Zhu', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Shanghai AI Laboratory', 'Soochow University', 'South China University of Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.05447.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Linear-MoE: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Linear-MoE - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ÑƒÑ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ (LSM) Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE). Linear-MoE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° LSM-Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ MoE-ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸, ÑÑ‚Ñ€ĞµĞ¼ÑÑÑŒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ÑƒÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² LSM, Ğ¸ Ğ¿Ğ¾Ğ´ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ°. ĞÑ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ÑĞµÑ€Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Linear-MoE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Efficient Modeling with Linear-MoE: Merging LSM and MoE for High Performance', 'desc': 'This paper presents Linear-MoE, a new system that combines Linear Sequence Modeling (LSM) with Mixture-of-Experts (MoE) to improve the efficiency and performance of large-scale models. Linear-MoE utilizes linear-complexity sequence modeling from LSM and the sparsity benefits of MoE layers, allowing for effective training and high performance. The system includes a modeling subsystem for LSM and a training subsystem that employs advanced parallelism techniques, particularly Sequence Parallelism. Experiments show that Linear-MoE achieves better efficiency while delivering competitive results on various benchmarks, indicating its promise as a foundational model architecture.'}, 'zh': {'title': 'çº¿æ€§-MoEï¼šé«˜æ•ˆçš„åºåˆ—å»ºæ¨¡ä¸è®­ç»ƒæ–°æ¶æ„', 'desc': 'çº¿æ€§åºåˆ—å»ºæ¨¡ï¼ˆLSMï¼‰å’Œä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰æœ€è¿‘æˆä¸ºé‡è¦çš„æ¶æ„æ”¹è¿›ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLinear-MoEçš„ç³»ç»Ÿï¼Œå®ƒå°†LSMä¸MoEç»“åˆï¼Œç”¨äºå»ºæ¨¡å’Œè®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹ã€‚Linear-MoEåˆ©ç”¨LSMæ¨¡å—çš„çº¿æ€§å¤æ‚åº¦åºåˆ—å»ºæ¨¡ä¼˜åŠ¿å’ŒMoEå±‚çš„ç¨€ç–æ¿€æ´»ç‰¹æ€§ï¼Œæ—¨åœ¨æä¾›é«˜æ€§èƒ½å’Œé«˜æ•ˆè®­ç»ƒã€‚é€šè¿‡å¯¹A0.3B-2Bå’ŒA1B-7Bæ¨¡å‹ç³»åˆ—çš„è¯„ä¼°ï¼ŒLinear-MoEåœ¨ä¿æŒç«äº‰æ€§èƒ½çš„åŒæ—¶å®ç°äº†æ•ˆç‡æå‡ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºä¸‹ä¸€ä»£åŸºç¡€æ¨¡å‹æ¶æ„çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04548', 'title': 'An Empirical Study on Eliciting and Improving R1-like Reasoning Models', 'url': 'https://huggingface.co/papers/2503.04548', 'abstract': 'In this report, we present the third technical report on the development of slow-thinking models as part of the STILL project. As the technical pathway becomes clearer, scaling RL training has become a central technique for implementing such reasoning models. We systematically experiment with and document the effects of various factors influencing RL training, conducting experiments on both base models and fine-tuned models. Specifically, we demonstrate that our RL training approach consistently improves the Qwen2.5-32B base models, enhancing both response length and test accuracy. Furthermore, we show that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already achieved a high performance level, it can be further refined through RL training, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we also explore the use of tool manipulation, finding that it significantly boosts the reasoning performance of large reasoning models. This approach achieves a remarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its effectiveness in enhancing model capabilities. We release our resources at the STILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.', 'score': 5, 'issue_id': 2615, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': 'defc053b9079a4a2', 'authors': ['Zhipeng Chen', 'Yingqian Min', 'Beichen Zhang', 'Jie Chen', 'Jinhao Jiang', 'Daixuan Cheng', 'Wayne Xin Zhao', 'Zheng Liu', 'Xu Miao', 'Yang Lu', 'Lei Fang', 'Zhongyuan Wang', 'Ji-Rong Wen'], 'affiliations': ['BAAI', 'DataCanvas', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04548.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°Ğ»Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ‘Ñ‹Ğ»Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-32B, ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 86.67% Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… AIME 2024.'}, 'en': {'title': 'Enhancing Reasoning with Reinforcement Learning and Tool Manipulation', 'desc': 'This paper discusses advancements in slow-thinking models within the STILL project, focusing on reinforcement learning (RL) training techniques. The authors conduct systematic experiments to analyze how different factors affect RL training, leading to improvements in the Qwen2.5-32B base models. They demonstrate that even high-performing models can be further enhanced through RL training, achieving notable accuracy on benchmark tasks. Additionally, the study highlights the benefits of tool manipulation in improving reasoning performance, achieving impressive results on the AIME 2024 challenge.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡æ¨ç†æ¨¡å‹çš„èƒ½åŠ›', 'desc': 'æœ¬æŠ¥å‘Šä»‹ç»äº†STILLé¡¹ç›®ä¸­æ…¢æ€ç»´æ¨¡å‹å‘å±•çš„ç¬¬ä¸‰ä¸ªæŠ€æœ¯æŠ¥å‘Šã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°å®éªŒå¹¶è®°å½•äº†å½±å“å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„å„ç§å› ç´ ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹ä¸Šçš„å®éªŒã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒRLè®­ç»ƒæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜Qwen2.5-32BåŸºç¡€æ¨¡å‹çš„å“åº”é•¿åº¦å’Œæµ‹è¯•å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°å·¥å…·æ“ä½œçš„ä½¿ç”¨æ˜¾è‘—æå‡äº†å¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†æ€§èƒ½ï¼Œè¾¾åˆ°äº†86.67%çš„å‡†ç¡®ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04359', 'title': 'LONGCODEU: Benchmarking Long-Context Language Models on Long Code\n  Understanding', 'url': 'https://huggingface.co/papers/2503.04359', 'abstract': "Current advanced long-context language models offer great potential for real-world software engineering applications. However, progress in this critical domain remains hampered by a fundamental limitation: the absence of a rigorous evaluation framework for long code understanding. To gap this obstacle, we propose a long code understanding benchmark LONGCODEU from four aspects (8 tasks) to evaluate LCLMs' long code understanding ability required for practical applications, including code unit perception, intra-code unit understanding, inter-code unit relation understanding, and long code documentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6 general models and 3 code models). Our experimental results reveal key limitations in current LCLMs' capabilities for long code understanding. Particularly, the performance of LCLMs drops dramatically when the long code length is greater than 32K, falling far short of their claimed 128K-1M context windows. In the four aspects, inter-code unit relation understanding is the most challenging for LCLMs. Our study provides valuable insights for optimizing LCLMs and driving advancements in software engineering.", 'score': 5, 'issue_id': 2617, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '808bf0135ca11113', 'authors': ['Jia Li', 'Xuyuan Guo', 'Lei Li', 'Kechi Zhang', 'Ge Li', 'Jia Li', 'Zhengwei Tao', 'Fang Liu', 'Chongyang Tao', 'Yuqi Zhu', 'Zhi Jin'], 'affiliations': ['Key Lab of High Confidence Software Technology (Peking University), MoE, School of Computer Science, Peking University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04359.jpg', 'data': {'categories': ['#dataset', '#long_context', '#benchmark', '#optimization'], 'emoji': 'ğŸ“', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº LONGCODEU Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ (LCLM) Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 8 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² 4 Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ñ…: Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 9 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… LCLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğµ ĞºĞ¾Ğ´Ğ° Ğ±Ğ¾Ğ»ĞµĞµ 32K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Bridging the Gap in Long Code Understanding for LCLMs', 'desc': 'This paper addresses the challenges faced by advanced long-context language models (LCLMs) in understanding long code, which is crucial for software engineering. It introduces a new benchmark called LONGCODEU, designed to evaluate LCLMs across eight tasks that cover various aspects of long code comprehension. The study evaluates nine popular LCLMs and finds significant performance drops when handling code longer than 32K tokens, indicating limitations in their capabilities. The findings highlight that understanding relationships between code units is particularly difficult for these models, providing insights for future improvements in LCLMs.'}, 'zh': {'title': 'æå‡é•¿ä»£ç ç†è§£èƒ½åŠ›çš„å…³é”®è¯„ä¼°åŸºå‡†', 'desc': 'å½“å‰å…ˆè¿›çš„é•¿æ–‡æœ¬è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹åº”ç”¨ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç¼ºä¹ä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶é™åˆ¶äº†è¿™ä¸€é¢†åŸŸçš„è¿›å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºLONGCODEUçš„é•¿ä»£ç ç†è§£åŸºå‡†ï¼Œæ¶µç›–äº†å››ä¸ªæ–¹é¢çš„å…«ä¸ªä»»åŠ¡ï¼Œä»¥è¯„ä¼°é•¿ä»£ç ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰çš„é•¿ä»£ç è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¶…è¿‡32Kçš„é•¿ä»£ç æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨ç†è§£ä»£ç å•å…ƒä¹‹é—´çš„å…³ç³»æ–¹é¢æœ€å…·æŒ‘æˆ˜æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04808', 'title': 'Learning from Failures in Multi-Attempt Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.04808', 'abstract': "Recent advancements in reinforcement learning (RL) for large language models (LLMs), exemplified by DeepSeek R1, have shown that even a simple question-answering task can substantially improve an LLM's reasoning capabilities. In this work, we extend this approach by modifying the task into a multi-attempt setting. Instead of generating a single response per question, the model is given multiple attempts, with feedback provided after incorrect responses. The multi-attempt task encourages the model to refine its previous attempts and improve search efficiency. Experimental results show that even a small LLM trained on a multi-attempt task achieves significantly higher accuracy when evaluated with more attempts, improving from 45.6% with 1 attempt to 52.5% with 2 attempts on the math benchmark. In contrast, the same LLM trained on a standard single-turn task exhibits only a marginal improvement, increasing from 42.3% to 43.2% when given more attempts during evaluation. The results indicate that, compared to the standard single-turn task, an LLM trained on a multi-attempt task achieves slightly better performance on math benchmarks while also learning to refine its responses more effectively based on user feedback. Full code is available at https://github.com/DualityRL/multi-attempt", 'score': 5, 'issue_id': 2609, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': 'fb2db794d0ea3c11', 'authors': ['Stephen Chung', 'Wenyu Du', 'Jie Fu'], 'affiliations': ['DualityRL', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.04808.jpg', 'data': {'categories': ['#optimization', '#rl', '#math', '#training', '#rlhf', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ğ½ĞµĞ´Ñ€ÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¿Ğ¾ÑĞ»Ğµ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ½Ğ¾ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ²Ğ¾Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ.'}, 'en': {'title': 'Enhancing LLMs with Multi-Attempt Learning', 'desc': "This paper explores how modifying reinforcement learning tasks can enhance the reasoning abilities of large language models (LLMs). By implementing a multi-attempt question-answering framework, the model receives feedback on incorrect answers, allowing it to improve its responses iteratively. Experimental results demonstrate that even a small LLM can achieve better accuracy on math benchmarks when trained with this multi-attempt approach, compared to traditional single-turn tasks. The findings suggest that providing multiple attempts and feedback significantly aids in refining the model's performance."}, 'zh': {'title': 'å¤šæ¬¡å°è¯•ï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­åº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡å¤šæ¬¡å°è¯•çš„ä»»åŠ¡è®¾ç½®æ¥æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„å•æ¬¡å›ç­”ä¸åŒï¼Œæ¨¡å‹åœ¨æ¯ä¸ªé—®é¢˜ä¸Šå¯ä»¥è¿›è¡Œå¤šæ¬¡å°è¯•ï¼Œå¹¶åœ¨é”™è¯¯å›ç­”åè·å¾—åé¦ˆã€‚è¿™ç§å¤šæ¬¡å°è¯•çš„ä»»åŠ¡è®¾ç½®ä¿ƒä½¿æ¨¡å‹æ”¹è¿›ä¹‹å‰çš„å›ç­”ï¼Œä»è€Œæé«˜æœç´¢æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯å°å‹LLMï¼Œåœ¨å¤šæ¬¡å°è¯•çš„ä»»åŠ¡è®­ç»ƒä¸‹ï¼Œå…¶å‡†ç¡®ç‡æ˜¾è‘—æé«˜ï¼Œæ˜¾ç¤ºå‡ºå¤šæ¬¡å°è¯•å¯¹æ¨¡å‹å­¦ä¹ å’Œåé¦ˆçš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01713', 'title': 'SAGE: A Framework of Precise Retrieval for RAG', 'url': 'https://huggingface.co/papers/2503.01713', 'abstract': 'Retrieval-augmented generation (RAG) has demonstrated significant proficiency in conducting question-answering (QA) tasks within a specified corpus. Nonetheless, numerous failure instances of RAG in QA still exist. These failures are not solely attributable to the limitations of Large Language Models (LLMs); instead, they predominantly arise from the retrieval of inaccurate information for LLMs due to two limitations: (1) Current RAG methods segment the corpus without considering semantics, making it difficult to find relevant context due to impaired correlation between questions and the segments. (2) There is a trade-off between missing essential context with fewer context retrieved and getting irrelevant context with more context retrieved.   In this paper, we introduce a RAG framework (SAGE), to overcome these limitations. First, to address the segmentation issue without considering semantics, we propose to train a semantic segmentation model. This model is trained to segment the corpus into semantically complete chunks. Second, to ensure that only the most relevant chunks are retrieved while the irrelevant ones are ignored, we design a chunk selection algorithm to dynamically select chunks based on the decreasing speed of the relevance score, leading to a more relevant selection. Third, to further ensure the precision of the retrieved chunks, we propose letting LLMs assess whether retrieved chunks are excessive or lacking and then adjust the amount of context accordingly. Experiments show that SAGE outperforms baselines by 61.25% in the quality of QA on average. Moreover, by avoiding retrieving noisy context, SAGE lowers the cost of the tokens consumed in LLM inference and achieves a 49.41% enhancement in cost efficiency on average. Additionally, our work offers valuable insights for boosting RAG.', 'score': 4, 'issue_id': 2613, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': 'dc1c8022a96cab3e', 'authors': ['Jintao Zhang', 'Guoliang Li', 'Jinyang Su'], 'affiliations': ['Department of Computer Science Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01713.jpg', 'data': {'categories': ['#rag', '#optimization', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SAGE: Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ RAG Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SAGE Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ retrieval-augmented generation (RAG) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SAGE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 61.25% Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ½Ğ° 49.41% Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² RAG Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'SAGE: Smarter Retrieval for Better Question-Answering', 'desc': 'This paper presents a new framework called SAGE to improve retrieval-augmented generation (RAG) for question-answering tasks. It addresses two main issues: the ineffective segmentation of the corpus that ignores semantics and the trade-off between retrieving too little or too much context. SAGE introduces a semantic segmentation model to create meaningful chunks and a dynamic chunk selection algorithm to ensure only the most relevant information is retrieved. The results show that SAGE significantly enhances QA quality and cost efficiency compared to existing methods.'}, 'zh': {'title': 'æå‡é—®ç­”è´¨é‡çš„æ™ºèƒ½æ£€ç´¢æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ï¼ˆSAGEï¼‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰RAGæ–¹æ³•åœ¨é—®ç­”ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚é¦–å…ˆï¼ŒSAGEé€šè¿‡è®­ç»ƒè¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼Œå°†è¯­æ–™åº“åˆ†å‰²æˆè¯­ä¹‰å®Œæ•´çš„å—ï¼Œä»¥æé«˜ç›¸å…³æ€§ã€‚å…¶æ¬¡ï¼Œè®¾è®¡äº†ä¸€ç§åŠ¨æ€é€‰æ‹©ç®—æ³•ï¼Œæ ¹æ®ç›¸å…³æ€§å¾—åˆ†çš„ä¸‹é™é€Ÿåº¦é€‰æ‹©æœ€ç›¸å…³çš„å—ï¼Œä»è€Œé¿å…æ— å…³ä¿¡æ¯çš„å¹²æ‰°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAGEåœ¨é—®ç­”è´¨é‡ä¸Šæ¯”åŸºçº¿æé«˜äº†61.25%ï¼Œå¹¶ä¸”åœ¨æˆæœ¬æ•ˆç‡ä¸Šæå‡äº†49.41%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05132', 'title': 'R1-Zero\'s "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model', 'url': 'https://huggingface.co/papers/2503.05132', 'abstract': 'Recently DeepSeek R1 demonstrated how reinforcement learning with simple rule-based incentives can enable autonomous development of complex reasoning in large language models, characterized by the "aha moment", in which the model manifest self-reflection and increased response length during training. However, attempts to extend this success to multimodal reasoning often failed to reproduce these key characteristics. In this report, we present the first successful replication of these emergent characteristics for multimodal reasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying reinforcement learning directly on the SAT dataset, our model achieves 59.47% accuracy on CVBench, outperforming the base model by approximately ~30% and exceeding both SFT setting by ~2%. In addition, we share our failed attempts and insights in attempting to achieve R1-like reasoning using RL with instruct models. aiming to shed light on the challenges involved. Our key observations include: (1) applying RL on instruct model often results in trivial reasoning trajectories, and (2) naive length reward are ineffective in eliciting reasoning capabilities. The project code is available at https://github.com/turningpoint-ai/VisualThinker-R1-Zero', 'score': 3, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '760e01cf2a414aeb', 'authors': ['Hengguang Zhou', 'Xirui Li', 'Ruochen Wang', 'Minhao Cheng', 'Tianyi Zhou', 'Cho-Jui Hsieh'], 'affiliations': ['Pennsylvania State University', 'University of California, LA', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2503.05132.jpg', 'data': {'categories': ['#rl', '#multimodal', '#training', '#reasoning', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ· DeepSeek R1 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ LLM Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹ "ÑĞ²Ñ€Ğ¸ĞºĞ¸". ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑƒĞ´Ğ°Ğ²Ğ°Ğ»Ğ¾ÑÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¶Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ¸ ÑÑ‚Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ SFT, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ»ÑÑ‚ÑÑ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰Ğ¸Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unlocking Multimodal Reasoning with Reinforcement Learning', 'desc': "This paper discusses the application of reinforcement learning (RL) to enhance multimodal reasoning in large language models, specifically using the Qwen2-VL-2B model. The authors successfully replicated the 'aha moment' phenomenon, where the model demonstrates self-reflection and improved response length during training, achieving a notable accuracy of 59.47% on the CVBench dataset. They also share insights from their unsuccessful attempts to replicate similar reasoning capabilities using instruct models, highlighting challenges such as trivial reasoning paths and ineffective reward structures. The findings suggest that while RL can significantly improve reasoning in multimodal contexts, careful consideration of reward mechanisms is crucial for success."}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ åŠ©åŠ›å¤šæ¨¡æ€æ¨ç†çš„çªç ´', 'desc': 'æœ€è¿‘ï¼ŒDeepSeek R1å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç®€å•çš„åŸºäºè§„åˆ™çš„æ¿€åŠ±æ¥å®ç°å¼ºåŒ–å­¦ä¹ ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»å‘å±•å¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚è¿™ç§èƒ½åŠ›åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°ä¸ºâ€œæç„¶å¤§æ‚Ÿâ€çš„æ—¶åˆ»ï¼Œæ¨¡å‹ä¼šè‡ªæˆ‘åæ€å¹¶å¢åŠ å“åº”é•¿åº¦ã€‚ç„¶è€Œï¼Œå°è¯•å°†è¿™ç§æˆåŠŸæ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†æ—¶ï¼Œå¾€å¾€æ— æ³•é‡ç°è¿™äº›å…³é”®ç‰¹å¾ã€‚åœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æˆåŠŸå¤åˆ¶äº†è¿™äº›ç‰¹å¾ï¼Œå¹¶åœ¨éSFTçš„2Bæ¨¡å‹ä¸Šå®ç°äº†å¤šæ¨¡æ€æ¨ç†çš„è¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.05315', 'title': 'LoRACode: LoRA Adapters for Code Embeddings', 'url': 'https://huggingface.co/papers/2503.05315', 'abstract': 'Code embeddings are essential for semantic code search; however, current approaches often struggle to capture the precise syntactic and contextual nuances inherent in code. Open-source models such as CodeBERT and UniXcoder exhibit limitations in scalability and efficiency, while high-performing proprietary systems impose substantial computational costs. We introduce a parameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to construct task-specific adapters for code retrieval. Our approach reduces the number of trainable parameters to less than two percent of the base model, enabling rapid fine-tuning on extensive code corpora (2 million samples in 25 minutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in Mean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code search tasks across multiple programming languages. Distinction in task-wise and language-wise adaptation helps explore the sensitivity of code retrieval for syntactical and linguistic variations.', 'score': 2, 'issue_id': 2610, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'}, 'hash': '95dca112be949ba8', 'authors': ['Saumya Chaturvedi', 'Aman Chadha', 'Laurent Bindschaedler'], 'affiliations': ['AWS GenAI Santa Clara, CA, USA', 'Max Planck Institute for Software Systems Saarbrucken, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.05315.jpg', 'data': {'categories': ['#data', '#open_source', '#training', '#optimization', '#plp'], 'emoji': 'ğŸ”', 'ru': {'title': 'LoRA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ğ´Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Low-Rank Adaptation (LoRA). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ¼Ğ°Ñ… ĞºĞ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ MRR Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Code2Code Ğ¸ Text2Code. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ğ´Ğ° Ğº ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸ÑĞ¼.'}, 'en': {'title': 'Efficient Code Retrieval with Low-Rank Adaptation', 'desc': 'This paper addresses the challenges of semantic code search by improving how code embeddings are generated. It highlights the limitations of existing models like CodeBERT and UniXcoder in terms of scalability and efficiency. The authors propose a new method using Low-Rank Adaptation (LoRA) to create task-specific adapters, significantly reducing the number of trainable parameters. Their approach allows for quick fine-tuning on large code datasets, resulting in notable improvements in retrieval performance across various programming languages.'}, 'zh': {'title': 'é«˜æ•ˆä»£ç æ£€ç´¢çš„ä½ç§©é€‚åº”æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä½ç§©é€‚åº”ï¼ˆLoRAï¼‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œç”¨äºæ„å»ºç‰¹å®šä»»åŠ¡çš„ä»£ç æ£€ç´¢é€‚é…å™¨ã€‚è¯¥æ–¹æ³•å°†å¯è®­ç»ƒå‚æ•°å‡å°‘åˆ°åŸºç¡€æ¨¡å‹çš„ä¸åˆ°2%ï¼Œä½¿å¾—åœ¨å¤§è§„æ¨¡ä»£ç è¯­æ–™åº“ä¸Šè¿›è¡Œå¿«é€Ÿå¾®è°ƒæˆä¸ºå¯èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä»£ç åˆ°ä»£ç çš„æ£€ç´¢ä»»åŠ¡ä¸­ï¼Œå¹³å‡å€’æ•°æ’åï¼ˆMRRï¼‰æé«˜äº†9.1%ï¼Œè€Œæ–‡æœ¬åˆ°ä»£ç çš„æ£€ç´¢ä»»åŠ¡æé«˜äº†86.69%ã€‚é€šè¿‡ä»»åŠ¡å’Œè¯­è¨€çš„é€‚åº”æ€§åŒºåˆ†ï¼Œæœ¬æ–‡æ¢è®¨äº†ä»£ç æ£€ç´¢å¯¹è¯­æ³•å’Œè¯­è¨€å˜ä½“çš„æ•æ„Ÿæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.18968', 'title': 'Know You First and Be You Better: Modeling Human-Like User Simulators\n  via Implicit Profiles', 'url': 'https://huggingface.co/papers/2502.18968', 'abstract': 'User simulators are crucial for replicating human interactions with dialogue systems, supporting both collaborative training and automatic evaluation, especially for large language models (LLMs). However, existing simulators often rely solely on text utterances, missing implicit user traits such as personality, speaking style, and goals. In contrast, persona-based methods lack generalizability, as they depend on predefined profiles of famous individuals or archetypes. To address these challenges, we propose User Simulator with implicit Profiles (USP), a framework that infers implicit user profiles from human-machine conversations and uses them to generate more personalized and realistic dialogues. We first develop an LLM-driven extractor with a comprehensive profile schema. Then, we refine the simulation through conditional supervised fine-tuning and reinforcement learning with cycle consistency, optimizing it at both the utterance and conversation levels. Finally, we adopt a diverse profile sampler to capture the distribution of real-world user profiles. Experimental results demonstrate that USP outperforms strong baselines in terms of authenticity and diversity while achieving comparable performance in consistency. Furthermore, dynamic multi-turn evaluations based on USP strongly align with mainstream benchmarks, demonstrating its effectiveness in real-world applications.', 'score': 2, 'issue_id': 2619, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '32f11f6f4b295fc5', 'authors': ['Kuang Wang', 'Xianfei Li', 'Shenghao Yang', 'Li Zhou', 'Feng Jiang', 'Haizhou Li'], 'affiliations': ['Shenzhen Research Institute of Big Data', 'Shenzhen University of Advanced Technology', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2502.18968.jpg', 'data': {'categories': ['#rl', '#benchmark', '#training', '#optimization', '#games', '#agents', '#interpretability'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ - User Simulator with implicit Profiles (USP). USP Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ USP Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Dialogue Systems with Implicit User Profiles', 'desc': 'This paper introduces the User Simulator with implicit Profiles (USP), a novel framework designed to enhance dialogue systems by incorporating implicit user traits like personality and goals. Unlike traditional simulators that focus only on text, USP infers user profiles from actual human-machine interactions, allowing for more personalized dialogue generation. The framework employs a large language model (LLM) to extract user profiles and utilizes conditional supervised fine-tuning and reinforcement learning to improve dialogue quality. Experimental results show that USP significantly improves the authenticity and diversity of generated dialogues while maintaining consistency, making it effective for real-world applications.'}, 'zh': {'title': 'éšå¼ç”¨æˆ·æ¡£æ¡ˆï¼Œæå‡å¯¹è¯çœŸå®æ„Ÿ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨æˆ·æ¨¡æ‹Ÿå™¨ï¼ˆUSPï¼‰ï¼Œæ—¨åœ¨é€šè¿‡éšå¼ç”¨æˆ·ç‰¹å¾ç”Ÿæˆæ›´ä¸ªæ€§åŒ–å’ŒçœŸå®çš„å¯¹è¯ã€‚ç°æœ‰çš„æ¨¡æ‹Ÿå™¨é€šå¸¸åªä¾èµ–æ–‡æœ¬ï¼Œå¿½è§†äº†ç”¨æˆ·çš„ä¸ªæ€§ã€è¯´è¯é£æ ¼å’Œç›®æ ‡ç­‰éšæ€§ç‰¹å¾ã€‚USPé€šè¿‡ä»äººæœºå¯¹è¯ä¸­æ¨æ–­éšå¼ç”¨æˆ·æ¡£æ¡ˆï¼Œç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼Œä¼˜åŒ–å¯¹è¯ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUSPåœ¨çœŸå®æ€§å’Œå¤šæ ·æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶åœ¨ä¸€è‡´æ€§ä¸Šè¡¨ç°ç›¸å½“ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04504', 'title': 'AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM', 'url': 'https://huggingface.co/papers/2503.04504', 'abstract': 'Video anomaly detection (VAD) is crucial for video analysis and surveillance in computer vision. However, existing VAD models rely on learned normal patterns, which makes them difficult to apply to diverse environments. Consequently, users should retrain models or develop separate AI models for new environments, which requires expertise in machine learning, high-performance hardware, and extensive data collection, limiting the practical usability of VAD. To address these challenges, this study proposes customizable video anomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers user-defined text as an abnormal event and detects frames containing a specified event in a video. We effectively implemented AnyAnomaly using a context-aware visual question answering without fine-tuning the large vision language model. To validate the effectiveness of the proposed model, we constructed C-VAD datasets and demonstrated the superiority of AnyAnomaly. Furthermore, our approach showed competitive performance on VAD benchmark datasets, achieving state-of-the-art results on the UBnormal dataset and outperforming other methods in generalization across all datasets. Our code is available online at github.com/SkiddieAhn/Paper-AnyAnomaly.', 'score': 1, 'issue_id': 2613, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '7dbd20628d3eb105', 'authors': ['Sunghyun Ahn', 'Youngwan Jo', 'Kijung Lee', 'Sein Kwon', 'Inpyo Hong', 'Sanghyun Park'], 'affiliations': ['Yonsei University, Seoul, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2503.04504.jpg', 'data': {'categories': ['#open_source', '#optimization', '#dataset', '#benchmark', '#cv', '#video'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ C-VAD. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ AnyAnomaly, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ AnyAnomaly Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… C-VAD Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹.'}, 'en': {'title': 'Customizable Anomaly Detection for Diverse Video Environments', 'desc': 'This paper introduces a new approach to video anomaly detection (VAD) called customizable video anomaly detection (C-VAD). Unlike traditional VAD models that require retraining for different environments, C-VAD allows users to define what constitutes an abnormal event using text input. The AnyAnomaly model leverages a context-aware visual question answering system, enabling it to detect specified events without the need for extensive fine-tuning. The results show that AnyAnomaly not only performs well on custom datasets but also achieves state-of-the-art results on established VAD benchmarks, demonstrating its versatility and effectiveness.'}, 'zh': {'title': 'å¯å®šåˆ¶çš„è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼Œè½»æ¾åº”å¯¹å¤šæ ·ç¯å¢ƒ', 'desc': 'è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„è§†é¢‘åˆ†æå’Œç›‘æ§ä¸­è‡³å…³é‡è¦ã€‚ç°æœ‰çš„VADæ¨¡å‹ä¾èµ–äºå­¦ä¹ åˆ°çš„æ­£å¸¸æ¨¡å¼ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨ä¸åŒç¯å¢ƒä¸­çš„åº”ç”¨å˜å¾—å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†å¯å®šåˆ¶çš„è§†é¢‘å¼‚å¸¸æ£€æµ‹ï¼ˆC-VADï¼‰æŠ€æœ¯å’ŒAnyAnomalyæ¨¡å‹ï¼Œå…è®¸ç”¨æˆ·å®šä¹‰å¼‚å¸¸äº‹ä»¶å¹¶æ£€æµ‹è§†é¢‘ä¸­çš„ç›¸å…³å¸§ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨UBnormalæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå±•ç¤ºäº†å…¶åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šçš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01840', 'title': 'EAGLE-3: Scaling up Inference Acceleration of Large Language Models via\n  Training-Time Test', 'url': 'https://huggingface.co/papers/2503.01840', 'abstract': "The sequential nature of modern LLMs makes them expensive and slow, and speculative sampling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top-layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE's feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. The code is available at https://github.com/SafeAILab/EAGLE.", 'score': 1, 'issue_id': 2619, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': 'cf0ee90637c3e71a', 'authors': ['Yuhui Li', 'Fangyun Wei', 'Chao Zhang', 'Hongyang Zhang'], 'affiliations': ['Microsoft Research', 'Peking University', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.01840.jpg', 'data': {'categories': ['#training', '#inference', '#optimization', '#reasoning'], 'emoji': 'ğŸš€', 'ru': {'title': 'EAGLE-3: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EAGLE-3, Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ²ĞµÑ€ÑĞ¸Ğ¹, EAGLE-3 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ EAGLE-3 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 6,5 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ² 1,4 Ñ€Ğ°Ğ·Ğ° Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ EAGLE-2. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ°Ğº Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'EAGLE-3: Speeding Up LLMs with Direct Token Prediction', 'desc': 'This paper presents EAGLE-3, an advanced model that improves the efficiency of large language models (LLMs) by shifting from feature prediction to direct token prediction. By utilizing multi-layer feature fusion instead of relying solely on top-layer features, EAGLE-3 enhances performance and allows for better utilization of larger training datasets. The authors demonstrate that EAGLE-3 achieves significant speed improvements, with a speedup ratio of up to 6.5 times compared to previous methods. The results indicate that EAGLE-3 not only accelerates inference but also improves model intelligence, making it a valuable advancement in the field of machine learning.'}, 'zh': {'title': 'EAGLE-3ï¼šæå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†é€Ÿåº¦çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¡ºåºç‰¹æ€§ä½¿å…¶åœ¨æ¨ç†æ—¶æˆæœ¬é«˜ä¸”é€Ÿåº¦æ…¢ï¼Œæ¨æµ‹é‡‡æ ·æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚EAGLEæ–¹æ³•åœ¨ç‰¹å¾å±‚é¢è¿›è¡Œè‡ªå›å½’ï¼Œåˆ©ç”¨ç›®æ ‡æ¨¡å‹çš„é¡¶å±‚ç‰¹å¾ï¼Œå–å¾—æ¯”ä¼ ç»Ÿæ¨æµ‹é‡‡æ ·æ›´å¥½çš„æ•ˆæœã€‚å°½ç®¡åœ¨LLMç¤¾åŒºä¸­ï¼Œæ‰©å¤§è®­ç»ƒæ•°æ®ä»¥æé«˜æ¨¡å‹æ™ºèƒ½çš„è¶‹åŠ¿æ—¥ç›Šå¢é•¿ï¼Œä½†æˆ‘ä»¬å‘ç°å¯¹äºEAGLEæ¥è¯´ï¼Œæ‰©å¤§æ•°æ®çš„æ•ˆæœæœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†EAGLE-3ï¼Œæ”¾å¼ƒç‰¹å¾é¢„æµ‹ï¼Œé‡‡ç”¨ç›´æ¥çš„æ ‡è®°é¢„æµ‹ï¼Œå¹¶é€šè¿‡è®­ç»ƒæ—¶æµ‹è¯•çš„æŠ€æœ¯å®ç°å¤šå±‚ç‰¹å¾èåˆï¼Œä»è€Œæ˜¾è‘—æå‡æ€§èƒ½ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (1)', '#agi', '#alignment (1)', '#architecture (4)', '#audio (2)', '#benchmark (6)', '#cv (4)', '#data (1)', '#dataset (9)', '#diffusion (2)', '#ethics', '#games (2)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (1)', '#interpretability (1)', '#leakage', '#long_context (3)', '#low_resource (1)', '#machine_translation', '#math (2)', '#multilingual (3)', '#multimodal (4)', '#open_source (5)', '#optimization (18)', '#plp (1)', '#rag (3)', '#reasoning (8)', '#rl (6)', '#rlhf (2)', '#robotics (1)', '#science (1)', '#security', '#small_models (1)', '#story_generation', '#survey', '#synthetic', '#training (14)', '#transfer_learning (2)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-03-10 15:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-10 15:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-10 15:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    