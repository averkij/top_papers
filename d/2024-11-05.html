
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 23 papers. November 5.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            margin-top: 10px;
            margin-bottom: 10px;
            display: block;
            border-radius: 5px;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">5 –Ω–æ—è–±—Ä—è</span> | <span id="title-articles-count">23 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-11-04.html">‚¨ÖÔ∏è <span id="prev-date">04.11</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-11-06.html">‚û°Ô∏è <span id="next-date">06.11</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-11.html">üìà <span id='top-month-label'>–ú–µ—Å—è—Ü</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '5 –Ω–æ—è–±—Ä—è', 'en': 'November 5', 'zh': '11Êúà5Êó•'};
        let feedDateNext = {'ru': '06.11', 'en': '11/06', 'zh': '11Êúà6Êó•'};
        let feedDatePrev = {'ru': '04.11', 'en': '11/04', 'zh': '11Êúà4Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '–°—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'Published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.24024', 'title': 'AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents', 'url': 'https://huggingface.co/papers/2410.24024', 'abstract': 'Autonomous agents have become increasingly important for interacting with the real world. Android agents, in particular, have been recently a frequently-mentioned interaction method. However, existing studies for training and evaluating Android agents lack systematic research on both open-source and closed-source models. In this work, we propose AndroidLab as a systematic Android agent framework. It includes an operation environment with different modalities, action space, and a reproducible benchmark. It supports both large language models (LLMs) and multimodal models (LMMs) in the same action space. AndroidLab benchmark includes predefined Android virtual devices and 138 tasks across nine apps built on these devices. By using the AndroidLab environment, we develop an Android Instruction dataset and train six open-source LLMs and LMMs, lifting the average success rates from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMs. AndroidLab is open-sourced and publicly available at https://github.com/THUDM/Android-Lab.', 'score': 48, 'issue_id': 422, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 31', 'zh': '10Êúà31Êó•'}, 'hash': '4ba16ad433c7511f', 'authors': ['Yifan Xu', 'Xiao Liu', 'Xueqiao Sun', 'Siyi Cheng', 'Hao Yu', 'Hanyu Lai', 'Shudan Zhang', 'Dan Zhang', 'Jie Tang', 'Yuxiao Dong'], 'affiliations': ['Tsinghua University', 'Peking University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24024.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#training', '#dataset', '#open_source', '#games', '#agents'], 'emoji': 'ü§ñ', 'ru': {'title': 'AndroidLab: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ Android-–∞–≥–µ–Ω—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AndroidLab - —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å—Ä–µ–¥—É –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –±–∞–∑–µ Android. –≠—Ç–∞ —Å—Ä–µ–¥–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏, –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º –¥–µ–π—Å—Ç–≤–∏–π –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–º —ç—Ç–∞–ª–æ–Ω–æ–º. AndroidLab –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM), —Ç–∞–∫ –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (LMM) –≤ –æ–¥–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–µ–π—Å—Ç–≤–∏–π. –° –ø–æ–º–æ—â—å—é AndroidLab –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Android Instruction –∏ –æ–±—É—á–∏–ª–∏ —à–µ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—Å–∏–≤ –∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'Empowering Android Agents with AndroidLab: A New Benchmark Framework', 'desc': 'This paper introduces AndroidLab, a comprehensive framework designed for training and evaluating Android agents. It provides a structured environment that includes various modalities and a defined action space, allowing for systematic testing of both open-source and closed-source models. The framework supports large language models (LLMs) and multimodal models (LMMs), facilitating their development and benchmarking across 138 tasks in nine different applications. By utilizing AndroidLab, the authors significantly improved the success rates of LLMs and LMMs, demonstrating its effectiveness in enhancing the performance of Android agents.'}, 'zh': {'title': 'AndroidLabÔºöÊèêÂçáÂÆâÂçì‰ª£ÁêÜÁöÑËÆ≠ÁªÉ‰∏éËØÑ‰º∞', 'desc': 'Ëá™‰∏ª‰ª£ÁêÜÂú®‰∏éÁé∞ÂÆû‰∏ñÁïå‰∫íÂä®‰∏≠ÂèòÂæóË∂äÊù•Ë∂äÈáçË¶ÅÔºåÂ∞§ÂÖ∂ÊòØÂÆâÂçì‰ª£ÁêÜ„ÄÇÁé∞ÊúâÁöÑÂÆâÂçì‰ª£ÁêÜËÆ≠ÁªÉÂíåËØÑ‰º∞Á†îÁ©∂Áº∫‰πèÁ≥ªÁªüÊÄßÔºåÊó†Ê≥ïÂÖ®Èù¢ÊØîËæÉÂºÄÊ∫êÂíåÈó≠Ê∫êÊ®°Âûã„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜAndroidLabÔºåËøôÊòØ‰∏Ä‰∏™Á≥ªÁªüÂåñÁöÑÂÆâÂçì‰ª£ÁêÜÊ°ÜÊû∂ÔºåÊîØÊåÅÂ§öÁßçÊìç‰ΩúÁéØÂ¢ÉÂíåÂä®‰ΩúÁ©∫Èó¥„ÄÇÈÄöËøáAndroidLabÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜÂÆâÂçìÊåá‰ª§Êï∞ÊçÆÈõÜÔºåÂπ∂ÊòæËëóÊèêÈ´ò‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂíåÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÊàêÂäüÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.02355', 'title': '"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization', 'url': 'https://huggingface.co/papers/2411.02355', 'abstract': 'Despite the popularity of large language model (LLM) quantization for inference acceleration, significant uncertainty remains regarding the accuracy-performance trade-offs associated with various quantization formats. We present a comprehensive empirical study of quantized accuracy, evaluating popular quantization formats (FP8, INT8, INT4) across academic benchmarks and real-world tasks, on the entire Llama-3.1 model family. Additionally, our study examines the difference in text generated by quantized models versus their uncompressed counterparts. Beyond benchmarks, we also present a couple of quantization improvements which allowed us to obtain state-of-the-art accuracy recovery results. Our investigation, encompassing over 500,000 individual evaluations, yields several key findings: (1) FP8 weight and activation quantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and activation quantization (W8A8-INT), when properly tuned, incurs surprisingly low 1-3% accuracy degradation, and (3) INT4 weight-only quantization (W4A16-INT) is competitive with 8-bit integer weight and activation quantization. To address the question of the "best" format for a given deployment environment, we conduct inference performance analysis using the popular open-source vLLM framework on various GPU architectures. We find that W4A16 offers the best cost-efficiency for synchronous deployments, and for asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel in asynchronous "continuous batching" deployment of mid- and large-size models on high-end GPUs. Our results provide a set of practical guidelines for deploying quantized LLMs across scales and performance requirements.', 'score': 44, 'issue_id': 431, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 –Ω–æ—è–±—Ä—è', 'en': 'November 4', 'zh': '11Êúà4Êó•'}, 'hash': '8cc3629c4f7e76a1', 'authors': ['Eldar Kurtic', 'Alexandre Marques', 'Shubhra Pandit', 'Mark Kurtz', 'Dan Alistarh'], 'affiliations': ['Neural Magic', 'Institute of Science and Technology Austria'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02355.jpg', 'data': {'categories': ['#benchmark', '#inference', '#optimization', '#training', '#open_source'], 'emoji': 'üî¨', 'ru': {'title': '–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ LLM: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é', 'desc': '–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è (FP8, INT8, INT4) –Ω–∞ —Å–µ–º–µ–π—Å—Ç–≤–µ –º–æ–¥–µ–ª–µ–π Llama-3.1, –æ—Ü–µ–Ω–∏–≤–∞—è –∏—Ö —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ FP8 –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –±–µ–∑–æ—à–∏–±–æ—á–Ω–æ –¥–ª—è –≤—Å–µ—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π, –∞ INT8 –ø—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –¥–∞–µ—Ç —Å–Ω–∏–∂–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—Å–µ–≥–æ –Ω–∞ 1-3%. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö GPU –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã—Ö LLM.'}, 'en': {'title': 'Optimizing Quantization for Efficient Large Language Model Deployment', 'desc': 'This paper investigates the accuracy and performance trade-offs of different quantization formats for large language models (LLMs), specifically focusing on FP8, INT8, and INT4 quantization methods. The authors conducted extensive evaluations on the Llama-3.1 model family, analyzing over 500,000 instances to determine the impact of quantization on model accuracy and text generation. Key findings indicate that FP8 quantization is lossless, while INT8 can achieve minimal accuracy loss with proper tuning, and INT4 quantization remains competitive. The study also provides practical guidelines for selecting the optimal quantization format based on deployment scenarios and GPU architectures.'}, 'zh': {'title': 'ÈáèÂåñÊ®°ÂûãÁöÑÊúÄ‰Ω≥ÈÄâÊã©‰∏éÊÄßËÉΩ‰ºòÂåñ', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÈáèÂåñÂØπÊé®ÁêÜÂä†ÈÄüÁöÑÂΩ±ÂìçÔºåÁâπÂà´ÂÖ≥Ê≥®‰∏çÂêåÈáèÂåñÊ†ºÂºèÔºàÂ¶ÇFP8„ÄÅINT8„ÄÅINT4ÔºâÂú®ÂáÜÁ°ÆÊÄßÂíåÊÄßËÉΩ‰πãÈó¥ÁöÑÊùÉË°°„ÄÇÊàë‰ª¨ÂØπLlama-3.1Ê®°ÂûãÁ≥ªÂàóËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑÂÆûËØÅÁ†îÁ©∂ÔºåËØÑ‰º∞‰∫ÜÈáèÂåñÊ®°ÂûãÂú®Â≠¶ÊúØÂü∫ÂáÜÂíåÂÆûÈôÖ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåFP8ÈáèÂåñÂú®ÊâÄÊúâÊ®°ÂûãËßÑÊ®°‰∏äÈÉΩÊòØÊó†ÊçüÁöÑÔºåËÄåÁªèËøáÈÄÇÂΩìË∞É‰ºòÁöÑINT8ÈáèÂåñ‰ªÖÊúâ1-3%ÁöÑÂáÜÁ°ÆÊÄß‰∏ãÈôç„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏Ä‰∫õÈáèÂåñÊîπËøõÊñπÊ≥ïÔºåÂ∏ÆÂä©ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÂáÜÁ°ÆÊÄßÊÅ¢Â§çÁªìÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.02337', 'title': 'WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning', 'url': 'https://huggingface.co/papers/2411.02337', 'abstract': "Large language models (LLMs) have shown remarkable potential as autonomous agents, particularly in web-based tasks. However, existing LLM web agents heavily rely on expensive proprietary LLM APIs, while open LLMs lack the necessary decision-making capabilities. This paper introduces WebRL, a self-evolving online curriculum reinforcement learning framework designed to train high-performance web agents using open LLMs. WebRL addresses three key challenges in building LLM web agents, including the scarcity of training tasks, sparse feedback signals, and policy distribution drift in online learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4 models into proficient web agents. On WebArena-Lite, WebRL improves the success rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B. These open models significantly surpass the performance of GPT-4-Turbo (17.6%) and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's effectiveness in bridging the gap between open and proprietary LLM-based web agents, paving the way for more accessible and powerful autonomous web interaction systems.", 'score': 36, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 –Ω–æ—è–±—Ä—è', 'en': 'November 4', 'zh': '11Êúà4Êó•'}, 'hash': 'a9af7d20e52c1f39', 'authors': ['Zehan Qi', 'Xiao Liu', 'Iat Long Iong', 'Hanyu Lai', 'Xueqiao Sun', 'Xinyue Yang', 'Jiadai Sun', 'Yu Yang', 'Shuntian Yao', 'Tianjie Zhang', 'Wei Xu', 'Jie Tang', 'Yuxiao Dong'], 'affiliations': ['Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02337.jpg', 'data': {'categories': ['#small_models', '#reasoning', '#rl', '#benchmark', '#optimization', '#training', '#open_source', '#agents', '#architecture'], 'emoji': 'üï∏Ô∏è', 'ru': {'title': 'WebRL: –û—Ç–∫—Ä—ã—Ç—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –≤ –≤–µ–±-–∑–∞–¥–∞—á–∞—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç WebRL - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. WebRL —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Ö–≤–∞—Ç–∫–∏ –æ–±—É—á–∞—é—â–∏—Ö –∑–∞–¥–∞—á, —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∏ —Å–º–µ—â–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ –≤ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∏–π —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω, –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ WebRL –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π Llama-3.1 –∏ GLM-4 –≤ –∑–∞–¥–∞—á–∞—Ö –≤–µ–±-–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, –ø—Ä–µ–≤–∑–æ–π–¥—è –¥–∞–∂–µ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ GPT-4.'}, 'en': {'title': 'Empowering Open LLMs for Superior Web Performance with WebRL', 'desc': 'This paper presents WebRL, a novel framework that enhances open large language models (LLMs) for web-based tasks through reinforcement learning. It tackles challenges such as limited training tasks, sparse feedback, and policy drift by implementing a self-evolving curriculum that generates new tasks from failures, a robust outcome-supervised reward model, and adaptive learning strategies. The framework significantly improves the performance of open models like Llama-3.1 and GLM-4, achieving success rates that surpass proprietary models like GPT-4-Turbo. Overall, WebRL demonstrates a promising approach to making powerful web agents more accessible by leveraging open-source LLMs.'}, 'zh': {'title': 'WebRLÔºöÂºÄÊîæLLMÁöÑËá™ÊàëËøõÂåñÁΩëÁªú‰ª£ÁêÜËÆ≠ÁªÉÊ°ÜÊû∂', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÁΩëÁªú‰ªªÂä°‰∏≠Â±ïÁé∞‰∫ÜÂá∫Ëâ≤ÁöÑÊΩúÂäõÔºå‰ΩÜÁé∞ÊúâÁöÑLLMÁΩëÁªú‰ª£ÁêÜ‰æùËµñÊòÇË¥µÁöÑ‰∏ìÊúâAPIÔºåËÄåÂºÄÊîæÁöÑLLMÁº∫‰πèÂÜ≥Á≠ñËÉΩÂäõ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜWebRLÔºå‰∏Ä‰∏™Ëá™ÊàëËøõÂåñÁöÑÂú®Á∫øËØæÁ®ãÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊó®Âú®Âà©Áî®ÂºÄÊîæÁöÑLLMËÆ≠ÁªÉÈ´òÊÄßËÉΩÁöÑÁΩëÁªú‰ª£ÁêÜ„ÄÇWebRLËß£ÂÜ≥‰∫ÜÊûÑÂª∫LLMÁΩëÁªú‰ª£ÁêÜÁöÑ‰∏â‰∏™ÂÖ≥ÈîÆÊåëÊàòÔºåÂåÖÊã¨ËÆ≠ÁªÉ‰ªªÂä°Á®ÄÁº∫„ÄÅÂèçÈ¶à‰ø°Âè∑Á®ÄÁñèÂíåÂú®Á∫øÂ≠¶‰π†‰∏≠ÁöÑÁ≠ñÁï•ÂàÜÂ∏ÉÊºÇÁßª„ÄÇÈÄöËøáWebRLÔºåÊàë‰ª¨Â∞ÜÂºÄÊîæÁöÑLlama-3.1ÂíåGLM-4Ê®°ÂûãËΩ¨Âèò‰∏∫È´òÊïàÁöÑÁΩëÁªú‰ª£ÁêÜÔºåÊòæËëóÊèêÈ´ò‰∫ÜÂÆÉ‰ª¨ÁöÑÊàêÂäüÁéáÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑ‰∏ìÊúâÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.02385', 'title': 'How Far is Video Generation from World Model: A Physical Law Perspective', 'url': 'https://huggingface.co/papers/2411.02385', 'abstract': 'OpenAI\'s Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization. We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws. This provides an unlimited supply of data for large-scale experimentation and enables quantitative evaluation of whether the generated videos adhere to physical laws. We trained diffusion-based video generation models to predict object movements based on initial frames. Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios. Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit "case-based" generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color > size > velocity > shape. Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora\'s broader success. See our project page at https://phyworld.github.io', 'score': 32, 'issue_id': 421, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 –Ω–æ—è–±—Ä—è', 'en': 'November 4', 'zh': '11Êúà4Êó•'}, 'hash': '771395452deb397f', 'authors': ['Bingyi Kang', 'Yang Yue', 'Rui Lu', 'Zhijie Lin', 'Yang Zhao', 'Kaixin Wang', 'Gao Huang', 'Jiashi Feng'], 'affiliations': ['Bytedance Research', 'Tsinghua University', 'Technion'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02385.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#benchmark', '#cv', '#video', '#training', '#open_source', '#3d'], 'emoji': 'üé•', 'ru': {'title': '–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–∏ –Ω–µ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –∑–∞–∫–æ–Ω—ã –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑—É—á–∞—Ç—å —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –∑–∞–∫–æ–Ω—ã –∏–∑ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ 2D —Å–∏–º—É–ª—è—Ü–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–¥–µ–æ, —É–ø—Ä–∞–≤–ª—è–µ–º—ã—Ö –∑–∞–∫–æ–Ω–∞–º–∏ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π –º–µ—Ö–∞–Ω–∏–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –∏–¥–µ–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ —Ä–∞–º–∫–∞—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –Ω–æ –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–µ–π –Ω–∞ –Ω–æ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –Ω–µ –∞–±—Å—Ç—Ä–∞–≥–∏—Ä—É—é—Ç –æ–±—â–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞, –∞ —Å–∫–æ—Ä–µ–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –æ–±–æ–±—â–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤.'}, 'en': {'title': 'Unlocking Video Generation: Beyond Scaling to Understand Physics', 'desc': 'This paper investigates the ability of video generation models to learn and predict physical laws from visual data without human input. The authors created a 2D simulation environment to generate videos based on classical mechanics, allowing for extensive testing of model performance. They found that while the models excelled in familiar scenarios, they struggled with new, unseen situations, indicating a reliance on specific examples rather than generalizing physical principles. The study concludes that simply increasing model size is not enough for these systems to grasp fundamental laws of physics, highlighting the need for improved generalization techniques.'}, 'zh': {'title': 'ËßÜÈ¢ëÁîüÊàêÊ®°Âûã‰∏éÁâ©ÁêÜÊ≥ïÂàôÁöÑÊé¢Á¥¢', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂú®Â≠¶‰π†Áâ©ÁêÜÊ≥ïÂàôÊñπÈù¢ÁöÑÊΩúÂäõ„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏Ä‰∏™‰∫åÁª¥Ê®°ÊãüÊµãËØïÂπ≥Âè∞ÔºåÁî®‰∫éÁîüÊàêÂèóÁªèÂÖ∏ÂäõÂ≠¶Ê≥ïÂàôÊîØÈÖçÁöÑËßÜÈ¢ëÊï∞ÊçÆ„ÄÇÈÄöËøáÂØπÊ®°ÂûãÂú®‰∏çÂêåÂú∫ÊôØ‰∏ãÁöÑË°®Áé∞ËøõË°åËØÑ‰º∞ÔºåÊàë‰ª¨ÂèëÁé∞Ê®°ÂûãÂú®Â∑≤Áü•ÂàÜÂ∏ÉÂÜÖË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®Êú™Áü•ÂàÜÂ∏É‰∏≠ÂàôÂá∫Áé∞Â§±Ë¥•„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊ®°ÂûãÂú®Êé®ÂπøÊñ∞Ê°à‰æãÊó∂Ôºå‰ºòÂÖàËÄÉËôëÁöÑÂõ†Á¥†‰æùÊ¨°‰∏∫È¢úËâ≤„ÄÅÂ§ßÂ∞è„ÄÅÈÄüÂ∫¶ÂíåÂΩ¢Áä∂ÔºåËÄå‰∏çÊòØÊäΩË±°Âá∫‰∏ÄËà¨ÁöÑÁâ©ÁêÜËßÑÂàô„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.02336', 'title': 'MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D', 'url': 'https://huggingface.co/papers/2411.02336', 'abstract': 'Texturing is a crucial step in the 3D asset production workflow, which enhances the visual appeal and diversity of 3D assets. Despite recent advancements in Text-to-Texture (T2T) generation, existing methods often yield subpar results, primarily due to local discontinuities, inconsistencies across multiple views, and their heavy dependence on UV unwrapping outcomes. To tackle these challenges, we propose a novel generation-refinement 3D texturing framework called MVPaint, which can generate high-resolution, seamless textures while emphasizing multi-view consistency. MVPaint mainly consists of three key modules. 1) Synchronized Multi-view Generation (SMG). Given a 3D mesh model, MVPaint first simultaneously generates multi-view images by employing an SMG model, which leads to coarse texturing results with unpainted parts due to missing observations. 2) Spatial-aware 3D Inpainting (S3I). To ensure complete 3D texturing, we introduce the S3I method, specifically designed to effectively texture previously unobserved areas. 3) UV Refinement (UVR). Furthermore, MVPaint employs a UVR module to improve the texture quality in the UV space, which first performs a UV-space Super-Resolution, followed by a Spatial-aware Seam-Smoothing algorithm for revising spatial texturing discontinuities caused by UV unwrapping. Moreover, we establish two T2T evaluation benchmarks: the Objaverse T2T benchmark and the GSO T2T benchmark, based on selected high-quality 3D meshes from the Objaverse dataset and the entire GSO dataset, respectively. Extensive experimental results demonstrate that MVPaint surpasses existing state-of-the-art methods. Notably, MVPaint could generate high-fidelity textures with minimal Janus issues and highly enhanced cross-view consistency.', 'score': 23, 'issue_id': 432, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 –Ω–æ—è–±—Ä—è', 'en': 'November 4', 'zh': '11Êúà4Êó•'}, 'hash': '0f7de0fb0e5cdd86', 'authors': ['Wei Cheng', 'Juncheng Mu', 'Xianfang Zeng', 'Xin Chen', 'Anqi Pang', 'Chi Zhang', 'Zhibin Wang', 'Bin Fu', 'Gang Yu', 'Ziwei Liu', 'Liang Pan'], 'affiliations': ['Tencent PCG', 'Shanghai AI Laboratory', 'S-Lab, NTU', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02336.jpg', 'data': {'categories': ['#benchmark', '#cv', '#optimization', '#dataset', '#games', '#3d'], 'emoji': 'üé®', 'ru': {'title': 'MVPaint: —Ä–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º —Ç–µ–∫—Å—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–∏ 3D-–º–æ–¥–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ MVPaint –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç—É—Ä –¥–ª—è 3D-–º–æ–¥–µ–ª–µ–π. –°–∏—Å—Ç–µ–º–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –º–æ–¥—É–ª–µ–π: —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ 3D-–∏–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥–∞ –∏ —É—Ç–æ—á–Ω–µ–Ω–∏—è UV-—Ä–∞–∑–≤–µ—Ä—Ç–∫–∏. MVPaint —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ä–∞–∑—Ä—ã–≤–æ–≤, –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É —Ä–∞–∫—É—Ä—Å–∞–º–∏ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç UV-—Ä–∞–∑–≤–µ—Ä—Ç–∫–∏, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ —Ç–µ–∫—Å—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–ª–∏ –¥–≤–∞ –Ω–æ–≤—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç—É—Ä –ø–æ —Ç–µ–∫—Å—Ç—É.'}, 'en': {'title': 'MVPaint: Revolutionizing 3D Texturing with Multi-View Consistency', 'desc': 'This paper introduces MVPaint, a new framework for generating high-quality textures for 3D models. It addresses common issues in Text-to-Texture (T2T) generation, such as local discontinuities and inconsistencies across different views. MVPaint consists of three main components: Synchronized Multi-view Generation for initial texture creation, Spatial-aware 3D Inpainting for filling in unpainted areas, and UV Refinement for enhancing texture quality in UV space. The framework outperforms existing methods, providing seamless textures with improved visual consistency across multiple perspectives.'}, 'zh': {'title': 'MVPaintÔºöÊèêÂçá3DÁ∫πÁêÜÁîüÊàêÁöÑ‰∏Ä‰ΩìÂåñËß£ÂÜ≥ÊñπÊ°à', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ3DÁ∫πÁêÜÁîüÊàê‰∏é‰ºòÂåñÊ°ÜÊû∂MVPaintÔºåÊó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊñáÊú¨Âà∞Á∫πÁêÜÁîüÊàêÊñπÊ≥ï‰∏≠ÁöÑÂ±ÄÈÉ®‰∏çËøûÁª≠ÊÄßÂíåÂ§öËßÜÂõæ‰∏ÄËá¥ÊÄßÈóÆÈ¢ò„ÄÇMVPaintÂåÖÂê´‰∏â‰∏™‰∏ªË¶ÅÊ®°ÂùóÔºöÂêåÊ≠•Â§öËßÜÂõæÁîüÊàêÔºàSMGÔºâ„ÄÅÁ©∫Èó¥ÊÑüÁü•3D‰øÆË°•ÔºàS3IÔºâÂíåUV‰ºòÂåñÔºàUVRÔºâÔºåËÉΩÂ§üÁîüÊàêÈ´òÂàÜËæ®Áéá„ÄÅÊó†ÁºùÁöÑÁ∫πÁêÜ„ÄÇÈÄöËøáSMGÊ®°ÂùóÔºåMVPaintÂèØ‰ª•ÂêåÊó∂ÁîüÊàêÂ§öËßÜÂõæÂõæÂÉèÔºåËÄåS3IÊ®°ÂùóÂàô‰∏ìÊ≥®‰∫éÂ°´Ë°•Êú™ËßÇÂØüÂà∞ÁöÑÂå∫Âüü„ÄÇÊúÄÂêéÔºåUVRÊ®°ÂùóÈÄöËøáË∂ÖÂàÜËæ®ÁéáÂíåÁºùÂêàÂπ≥ÊªëÁÆóÊ≥ïÊù•ÊèêÂçáUVÁ©∫Èó¥‰∏≠ÁöÑÁ∫πÁêÜË¥®ÈáèÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéMVPaintÂú®Á∫πÁêÜÁîüÊàêÊñπÈù¢‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.00860', 'title': 'Survey of Cultural Awareness in Language Models: Text and Beyond', 'url': 'https://huggingface.co/papers/2411.00860', 'abstract': 'Large-scale deployment of large language models (LLMs) in various applications, such as chatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure inclusivity. Culture has been widely studied in psychology and anthropology, and there has been a recent surge in research on making LLMs more culturally inclusive in LLMs that goes beyond multilinguality and builds on findings from psychology and anthropology. In this paper, we survey efforts towards incorporating cultural awareness into text-based and multimodal LLMs. We start by defining cultural awareness in LLMs, taking the definitions of culture from anthropology and psychology as a point of departure. We then examine methodologies adopted for creating cross-cultural datasets, strategies for cultural inclusion in downstream tasks, and methodologies that have been used for benchmarking cultural awareness in LLMs. Further, we discuss the ethical implications of cultural alignment, the role of Human-Computer Interaction in driving cultural inclusion in LLMs, and the role of cultural alignment in driving social science research. We finally provide pointers to future research based on our findings about gaps in the literature.', 'score': 23, 'issue_id': 425, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 30', 'zh': '10Êúà30Êó•'}, 'hash': '8e4a6348db0215e3', 'authors': ['Siddhesh Pawar', 'Junyeong Park', 'Jiho Jin', 'Arnav Arora', 'Junho Myung', 'Srishti Yadav', 'Faiz Ghifari Haznitrama', 'Inhwa Song', 'Alice Oh', 'Isabelle Augenstein'], 'affiliations': ['University of Copenhagen, Denmark', 'KAIST, Republic of Korea', 'KAIST, Republic of Korea', 'KAIST, Republic of Korea', 'University of Copenhagen, Denmark', 'University of Copenhagen, Denmark', 'KAIST, Republic of Korea', 'KAIST, Republic of Korea', 'KAIST, Republic of Korea', 'University of Copenhagen, Denmark'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00860.jpg', 'data': {'categories': ['#science', '#benchmark', '#multilingual', '#multimodal', '#ethics', '#training', '#dataset', '#survey', '#architecture', '#alignment'], 'emoji': 'üåç', 'ru': {'title': '–ö—É–ª—å—Ç—É—Ä–Ω–∞—è –∏–Ω–∫–ª—é–∑–∏–≤–Ω–æ—Å—Ç—å –≤ —ç–ø–æ—Ö—É –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –≤–Ω–µ–¥—Ä–µ–Ω–∏—é –∫—É–ª—å—Ç—É—Ä–Ω–æ–π –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM). –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ —Å–æ–∑–¥–∞–Ω–∏—è –∫—Ä–æ—Å—Å-–∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤–∫–ª—é—á–µ–Ω–∏—è –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –û–±—Å—É–∂–¥–∞—é—Ç—Å—è —ç—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –∫—É–ª—å—Ç—É—Ä–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ LLM –∏ —Ä–æ–ª—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ–∫–∞ —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º –≤ —ç—Ç–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ. –°—Ç–∞—Ç—å—è —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –∫—É–ª—å—Ç—É—Ä–Ω–æ–π –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.'}, 'en': {'title': 'Enhancing Cultural Sensitivity in Language Models', 'desc': 'This paper explores the importance of cultural sensitivity in large language models (LLMs) used in applications like chatbots. It reviews existing research on how to make LLMs more inclusive by integrating insights from psychology and anthropology. The authors define cultural awareness in LLMs and discuss methods for creating diverse datasets and evaluating cultural inclusivity. Additionally, they highlight ethical considerations and suggest future research directions to enhance cultural alignment in LLMs.'}, 'zh': {'title': 'ËÆ©Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊõ¥ÂÖ∑ÊñáÂåñÊïèÊÑüÊÄß', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ËûçÂÖ•ÊñáÂåñÊïèÊÑüÊÄßÁöÑÈáçË¶ÅÊÄßÔºå‰ª•Á°Æ‰øùÁî®Êà∑ÁöÑÂåÖÂÆπÊÄß„ÄÇÊàë‰ª¨È¶ñÂÖàÂÆö‰πâ‰∫ÜLLMs‰∏≠ÁöÑÊñáÂåñÊÑèËØÜÔºåÂπ∂Âü∫‰∫é‰∫∫Á±ªÂ≠¶ÂíåÂøÉÁêÜÂ≠¶ÁöÑÂÆö‰πâËøõË°åËÆ®ËÆ∫„ÄÇÊé•ÁùÄÔºåÊàë‰ª¨ÂàÜÊûê‰∫ÜÂàõÂª∫Ë∑®ÊñáÂåñÊï∞ÊçÆÈõÜÁöÑÊñπÊ≥ï„ÄÅÂú®‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÂÆûÁé∞ÊñáÂåñÂåÖÂÆπÁöÑÁ≠ñÁï•Ôºå‰ª•ÂèäÁî®‰∫éËØÑ‰º∞LLMsÊñáÂåñÊÑèËØÜÁöÑÊñπÊ≥ïËÆ∫„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ËÆ®ËÆ∫‰∫ÜÊñáÂåñÂØπÈΩêÁöÑ‰º¶ÁêÜÂΩ±Âìç„ÄÅ‰∫∫Êú∫‰∫§‰∫íÂú®Êé®Âä®ÊñáÂåñÂåÖÂÆπ‰∏≠ÁöÑ‰ΩúÁî®Ôºå‰ª•ÂèäÊú™Êù•Á†îÁ©∂ÁöÑÊñπÂêë„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.02395', 'title': 'Training-free Regional Prompting for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.02395', 'abstract': 'Diffusion models have demonstrated excellent capabilities in text-to-image generation. Their semantic understanding (i.e., prompt following) ability has also been greatly improved with large language models (e.g., T5, Llama). However, existing models cannot perfectly handle long and complex text prompts, especially when the text prompts contain various objects with numerous attributes and interrelated spatial relationships. While many regional prompting methods have been proposed for UNet-based models (SD1.5, SDXL), but there are still no implementations based on the recent Diffusion Transformer (DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and implement regional prompting for FLUX.1 based on attention manipulation, which enables DiT with fined-grained compositional text-to-image generation capability in a training-free manner. Code is available at https://github.com/antonioo-c/Regional-Prompting-FLUX.', 'score': 23, 'issue_id': 423, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 –Ω–æ—è–±—Ä—è', 'en': 'November 4', 'zh': '11Êúà4Êó•'}, 'hash': '2a0401bfd2cb136b', 'authors': ['Anthony Chen', 'Jianjin Xu', 'Wenzhao Zheng', 'Gaole Dai', 'Yida Wang', 'Renrui Zhang', 'Haofan Wang', 'Shanghang Zhang'], 'affiliations': ['Peking University', 'InstantX Team', 'Carnegie Mellon University', 'UC Berkeley', 'Li Auto Inc.', 'CUHK'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02395.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#open_source', '#cv'], 'emoji': 'üé®', 'ru': {'title': '–¢–æ—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Å–ª–æ–∂–Ω—ã–º —Ç–µ–∫—Å—Ç–∞–º: –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è DiT', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞ –¥–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Diffusion Transformer (DiT), –ø—Ä–∏–º–µ–Ω–∏–º—ã–π –∫ –º–æ–¥–µ–ª—è–º FLUX.1. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Å–ª–æ–∂–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º –æ–ø–∏—Å–∞–Ω–∏—è–º –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ê–≤—Ç–æ—Ä—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª–∏ –º–µ—Ç–æ–¥ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏–µ–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç DiT –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ —Å–ª–µ–¥–æ–≤–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–∞–º. –†–∞–±–æ—Ç–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π.'}, 'en': {'title': 'Enhancing Text-to-Image Generation with Regional Prompting in Diffusion Transformers', 'desc': 'This paper discusses the limitations of current diffusion models in generating images from long and complex text prompts. It highlights the challenges these models face when dealing with multiple objects and their attributes in a spatial context. The authors propose a new method called regional prompting for the Diffusion Transformer (DiT) architecture, specifically for the FLUX.1 model, which enhances its ability to generate images based on detailed text descriptions without requiring additional training. This approach utilizes attention manipulation to achieve fine-grained compositional generation.'}, 'zh': {'title': 'Âå∫ÂüüÊèêÁ§∫ÊèêÂçáÊâ©Êï£Ê®°ÂûãÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêËÉΩÂäõ', 'desc': 'Êâ©Êï£Ê®°ÂûãÂú®ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÁêÜËß£ËØ≠‰πâÊñπÈù¢ÂæóÂà∞‰∫ÜÂæàÂ§ßÊèêÂçá„ÄÇÂ∞ΩÁÆ°Â∑≤ÊúâËÆ∏Â§öÂå∫ÂüüÊèêÁ§∫ÊñπÊ≥ïË¢´ÊèêÂá∫Ôºå‰ΩÜÁé∞ÊúâÊ®°Âûã‰ªçÊó†Ê≥ïÂÆåÁæéÂ§ÑÁêÜÈïø‰∏îÂ§çÊùÇÁöÑÊñáÊú¨ÊèêÁ§∫„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊ≥®ÊÑèÂäõÊìç‰ΩúÁöÑÂå∫ÂüüÊèêÁ§∫ÊñπÊ≥ïÔºå‰∏ìÈó®ÈíàÂØπFLUX.1Êû∂ÊûÑËøõË°åÂÆûÁé∞„ÄÇËØ•ÊñπÊ≥ï‰ΩøÂæóÊâ©Êï£ÂèòÊç¢Âô®ËÉΩÂ§üÂú®Êó†ÈúÄËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÔºåÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑÁªÑÂêàÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.02265', 'title': 'Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent', 'url': 'https://huggingface.co/papers/2411.02265', 'abstract': "In this paper, we introduce Hunyuan-Large, which is currently the largest open-source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior performance across various benchmarks including language understanding and generation, logical reasoning, mathematical problem-solving, coding, long-context, and aggregated tasks, where it outperforms LLama3.1-70B and exhibits comparable performance when compared to the significantly larger LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale synthetic data that is orders larger than in previous literature, a mixed expert routing strategy, a key-value cache compression technique, and an expert-specific learning rate strategy. Additionally, we also investigate the scaling laws and learning rate schedule of mixture of experts models, providing valuable insights and guidances for future model development and optimization. The code and checkpoints of Hunyuan-Large are released to facilitate future innovations and applications.   Codes: https://github.com/Tencent/Hunyuan-Large   Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large", 'score': 23, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 –Ω–æ—è–±—Ä—è', 'en': 'November 4', 'zh': '11Êúà4Êó•'}, 'hash': '775afc5ff4d7fbdf', 'authors': ['Xingwu Sun', 'Yanfeng Chen', 'Yiqing Huang', 'Ruobing Xie', 'Jiaqi Zhu', 'Kai Zhang', 'Shuaipeng Li', 'Zhen Yang', 'Jonny Han', 'Xiaobo Shu', 'Jiahao Bu', 'Zhongzhi Chen', 'Xuemeng Huang', 'Fengzong Lian', 'Saiyong Yang', 'Jianfeng Yan', 'Yuyuan Zeng', 'Xiaoqin Ren', 'Chao Yu', 'Lulu Wu', 'Yue Mao', 'Jun Xia', 'Tao Yang', 'Suncong Zheng', 'Kan Wu', 'Dian Jiao', 'Jinbao Xue', 'Xipeng Zhang', 'Decheng Wu', 'Kai Liu', 'Dengpeng Wu', 'Guanghui Xu', 'Shaohua Chen', 'Shuang Chen', 'Xiao Feng', 'Yigeng Hong', 'Junqiang Zheng', 'Chengcheng Xu', 'Zongwei Li', 'Xiong Kuang', 'Jianglu Hu', 'Yiqi Chen', 'Yuchi Deng', 'Guiyang Li', 'Ao Liu', 'Chenchen Zhang', 'Shihui Hu', 'Zilong Zhao', 'Zifan Wu', 'Yao Ding', 'Weichao Wang', 'Han Liu', 'Roberts Wang', 'Hao Fei', 'Peijie Yu', 'Ze Zhao', 'Xun Cao', 'Hai Wang', 'Fusheng Xiang', 'Mengyuan Huang', 'Zhiyuan Xiong', 'Bin Hu', 'Xuebin Hou', 'Lei Jiang', 'Jianqiang Ma', 'Jiajia Wu', 'Yaping Deng', 'Yi Shen', 'Qian Wang', 'Weijie Liu', 'Jie Liu', 'Meng Chen', 'Liang Dong', 'Weiwen Jia', 'Hu Chen', 'Feifei Liu', 'Rui Yuan', 'Huilin Xu', 'Zhenxiang Yan', 'Tengfei Cao', 'Zhichao Hu', 'Xinhua Feng', 'Dong Du', 'Tinghao Yu', 'Yangyu Tao', 'Feng Zhang', 'Jianchen Zhu', 'Chengzhong Xu', 'Xirui Li', 'Chong Zha', 'Wen Ouyang', 'Yinben Xia', 'Xiang Li', 'Zekun He', 'Rongpeng Chen', 'Jiawei Song', 'Ruibin Chen', 'Fan Jiang', 'Chongqing Zhao', 'Bo Wang', 'Hao Gong', 'Rong Gan', 'Winston Hu', 'Zhanhui Kang', 'Yong Yang', 'Yuhong Liu', 'Di Wang', 'Jie Jiang'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02265.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#synthetic', '#benchmark', '#optimization', '#math', '#plp', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': 'üß†', 'ru': {'title': 'Hunyuan-Large: –ì–∏–≥–∞–Ω—Ç—Å–∫–∏–π —à–∞–≥ –≤–ø–µ—Ä–µ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Hunyuan-Large - –∫—Ä—É–ø–Ω–µ–π—à—É—é –æ—Ç–∫—Ä—ã—Ç—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â—É—é 389 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —è–∑—ã–∫–∞, –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ. –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ Hunyuan-Large –≤–∫–ª—é—á–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å–º–µ—à–∞–Ω–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –∏—Å—Å–ª–µ–¥—É—é—Ç –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–µ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤.'}, 'en': {'title': 'Unlocking New Frontiers in AI with Hunyuan-Large', 'desc': 'Hunyuan-Large is a groundbreaking Transformer-based mixture of experts model with an impressive 389 billion parameters, designed to process up to 256K tokens. It demonstrates exceptional performance in various tasks such as language understanding, logical reasoning, and coding, surpassing the capabilities of LLama3.1-70B and rivaling the larger LLama3.1-405B. The model employs innovative techniques like large-scale synthetic data generation, a mixed expert routing strategy, and expert-specific learning rates to enhance its efficiency. Furthermore, the paper explores scaling laws and learning rate schedules, offering insights for future advancements in model optimization.'}, 'zh': {'title': 'Hunyuan-LargeÔºöË∂ÖÂ§ßËßÑÊ®°‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÁöÑÂàõÊñ∞‰πãË∑Ø', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜHunyuan-LargeÔºåËøôÊòØÁõÆÂâçÊúÄÂ§ßÁöÑÂºÄÊ∫êÂü∫‰∫éTransformerÁöÑ‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÔºåÊã•Êúâ3890‰∫ø‰∏™ÂèÇÊï∞Âíå520‰∫ø‰∏™ÊøÄÊ¥ªÂèÇÊï∞ÔºåËÉΩÂ§üÂ§ÑÁêÜÂ§öËææ256KÁöÑtoken„ÄÇÊàë‰ª¨ÂØπHunyuan-LargeÂú®ËØ≠Ë®ÄÁêÜËß£‰∏éÁîüÊàê„ÄÅÈÄªËæëÊé®ÁêÜ„ÄÅÊï∞Â≠¶ÈóÆÈ¢òËß£ÂÜ≥„ÄÅÁºñÁ†Å„ÄÅÈïø‰∏ä‰∏ãÊñáÂíåËÅöÂêà‰ªªÂä°Á≠âÂ§ö‰∏™Âü∫ÂáÜ‰∏äÁöÑ‰ºòË∂äÊÄßËÉΩËøõË°å‰∫ÜÂÖ®Èù¢ËØÑ‰º∞ÔºåÁªìÊûúÊòæÁ§∫ÂÖ∂‰ºò‰∫éLLama3.1-70BÔºåÂπ∂‰∏îÂú®‰∏éÊõ¥Â§ßÊ®°ÂûãLLama3.1-405BÁöÑÊØîËæÉ‰∏≠Ë°®Áé∞Áõ∏ÂΩì„ÄÇHunyuan-LargeÁöÑÂÖ≥ÈîÆÂÆûË∑µÂåÖÊã¨Â§ßËßÑÊ®°ÂêàÊàêÊï∞ÊçÆ„ÄÅÊ∑∑Âêà‰∏ìÂÆ∂Ë∑ØÁî±Á≠ñÁï•„ÄÅÈîÆÂÄºÁºìÂ≠òÂéãÁº©ÊäÄÊúØÂíå‰∏ìÂÆ∂ÁâπÂÆöÂ≠¶‰π†ÁéáÁ≠ñÁï•„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ËøòÁ†îÁ©∂‰∫Ü‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÁöÑÊâ©Â±ïËßÑÂæãÂíåÂ≠¶‰π†ÁéáË∞ÉÂ∫¶Ôºå‰∏∫Êú™Êù•Ê®°ÂûãÁöÑÂºÄÂèëÂíå‰ºòÂåñÊèê‰æõ‰∫ÜÂÆùË¥µÁöÑËßÅËß£ÂíåÊåáÂØº„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.02397', 'title': 'Adaptive Caching for Faster Video Generation with Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.02397', 'abstract': 'Generating temporally-consistent high-fidelity videos can be computationally expensive, especially over longer temporal spans. More-recent Diffusion Transformers (DiTs) -- despite making significant headway in this context -- have only heightened such challenges as they rely on larger models and heavier attention mechanisms, resulting in slower inference speeds. In this paper, we introduce a training-free method to accelerate video DiTs, termed Adaptive Caching (AdaCache), which is motivated by the fact that "not all videos are created equal": meaning, some videos require fewer denoising steps to attain a reasonable quality than others. Building on this, we not only cache computations through the diffusion process, but also devise a caching schedule tailored to each video generation, maximizing the quality-latency trade-off. We further introduce a Motion Regularization (MoReg) scheme to utilize video information within AdaCache, essentially controlling the compute allocation based on motion content. Altogether, our plug-and-play contributions grant significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video generation) without sacrificing the generation quality, across multiple video DiT baselines.', 'score': 20, 'issue_id': 421, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 –Ω–æ—è–±—Ä—è', 'en': 'November 4', 'zh': '11Êúà4Êó•'}, 'hash': 'd7fa22d791789900', 'authors': ['Kumara Kahatapitiya', 'Haozhe Liu', 'Sen He', 'Ding Liu', 'Menglin Jia', 'Chenyang Zhang', 'Michael S. Ryoo', 'Tian Xie'], 'affiliations': ['Meta AI', 'Stony Brook University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02397.jpg', 'data': {'categories': ['#diffusion', '#inference', '#video', '#optimization', '#architecture'], 'emoji': 'üöÄ', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–∫–æ—Ä—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ AdaCache –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ (DiT) –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. AdaCache –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –∫—ç—à–∏—Ä—É–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏, —É—á–∏—Ç—ã–≤–∞—è, —á—Ç–æ —Ä–∞–∑–Ω—ã–º –≤–∏–¥–µ–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è. –¢–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å—Ö–µ–º–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è (MoReg) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ. –ú–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∏–¥–µ–æ (–¥–æ 4.7 —Ä–∞–∑ –Ω–∞ Open-Sora 720p) –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞.'}, 'en': {'title': 'Accelerating Video Generation with Adaptive Caching!', 'desc': 'This paper presents a method called Adaptive Caching (AdaCache) to improve the speed of video generation using Diffusion Transformers (DiTs). The authors argue that different videos have varying requirements for denoising steps, allowing for a more efficient computation process. By caching computations and creating a tailored caching schedule for each video, they enhance the balance between quality and speed. Additionally, they introduce a Motion Regularization (MoReg) technique to optimize resource allocation based on the motion content of the video, achieving significant speed improvements without compromising quality.'}, 'zh': {'title': 'Âä†ÈÄüËßÜÈ¢ëÁîüÊàêÔºåÊèêÂçáË¥®Èáè‰∏éÊïàÁéáÔºÅ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Ëá™ÈÄÇÂ∫îÁºìÂ≠òÔºàAdaCacheÔºâÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÂä†ÈÄüËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÊâ©Êï£ÂèòÊç¢Âô®ÔºàDiTsÔºâ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÁºìÂ≠òËÆ°ÁÆóËøáÁ®ãÔºåÈíàÂØπÊØè‰∏™ËßÜÈ¢ëÁîüÊàêÂà∂ÂÆöÁºìÂ≠òËÆ°ÂàíÔºå‰ªéËÄå‰ºòÂåñË¥®Èáè‰∏éÂª∂ËøüÁöÑÂπ≥Ë°°„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜËøêÂä®Ê≠£ÂàôÂåñÔºàMoRegÔºâÊñπÊ°àÔºåÊ†πÊçÆËßÜÈ¢ë‰∏≠ÁöÑËøêÂä®ÂÜÖÂÆπÊù•ÊéßÂà∂ËÆ°ÁÆóÂàÜÈÖç„ÄÇÊï¥‰ΩìËÄåË®ÄÔºåËøô‰∫õÂàõÊñ∞ÊòæËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜÈÄüÂ∫¶ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÁîüÊàêË¥®Èáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.02319', 'title': 'GenXD: Generating Any 3D and 4D Scenes', 'url': 'https://huggingface.co/papers/2411.02319', 'abstract': "Recent developments in 2D visual generation have been remarkably successful. However, 3D and 4D generation remain challenging in real-world applications due to the lack of large-scale 4D data and effective model design. In this paper, we propose to jointly investigate general 3D and 4D generation by leveraging camera and object movements commonly observed in daily life. Due to the lack of real-world 4D data in the community, we first propose a data curation pipeline to obtain camera poses and object motion strength from videos. Based on this pipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K. By leveraging all the 3D and 4D data, we develop our framework, GenXD, which allows us to produce any 3D or 4D scene. We propose multiview-temporal modules, which disentangle camera and object movements, to seamlessly learn from both 3D and 4D data. Additionally, GenXD employs masked latent conditions to support a variety of conditioning views. GenXD can generate videos that follow the camera trajectory as well as consistent 3D views that can be lifted into 3D representations. We perform extensive evaluations across various real-world and synthetic datasets, demonstrating GenXD's effectiveness and versatility compared to previous methods in 3D and 4D generation.", 'score': 19, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 –Ω–æ—è–±—Ä—è', 'en': 'November 4', 'zh': '11Êúà4Êó•'}, 'hash': '51444eddaf6bbbe7', 'authors': ['Yuyang Zhao', 'Chung-Ching Lin', 'Kevin Lin', 'Zhiwen Yan', 'Linjie Li', 'Zhengyuan Yang', 'Jianfeng Wang', 'Gim Hee Lee', 'Lijuan Wang'], 'affiliations': ['National University of Singapore', 'Microsoft Corporation'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02319.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#cv', '#video', '#data', '#dataset', '#3d', '#architecture'], 'emoji': 'üé•', 'ru': {'title': 'GenXD: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä 3D –∏ 4D —Å—Ü–µ–Ω', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D –∏ 4D —Å—Ü–µ–Ω –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º GenXD. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö 4D —Å—Ü–µ–Ω CamVid-30K, –∏—Å–ø–æ–ª—å–∑—É—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–≤–∏–∂–µ–Ω–∏–∏ –∫–∞–º–µ—Ä—ã –∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ –≤–∏–¥–µ–æ. GenXD –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º—É–ª—å—Ç–∏—Ä–∞–∫—É—Ä—Å–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–π –∫–∞–º–µ—Ä—ã –∏ –æ–±—ä–µ–∫—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è –≥–∏–±–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–¥–µ–æ —Å –∑–∞–¥–∞–Ω–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–µ–π –∫–∞–º–µ—Ä—ã –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ 3D –≤–∏–¥—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ 3D-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è.'}, 'en': {'title': 'Unlocking 3D and 4D Generation with GenXD', 'desc': 'This paper addresses the challenges of generating 3D and 4D visual content, which are more complex than 2D generation due to limited data and model design issues. The authors introduce a new dataset called CamVid-30K, created by extracting camera poses and object movements from videos, which helps in training models effectively. They present a framework named GenXD that utilizes multiview-temporal modules to differentiate between camera and object movements, enabling the generation of realistic 3D and 4D scenes. Extensive evaluations show that GenXD outperforms existing methods, demonstrating its capability to create coherent videos and 3D representations from diverse inputs.'}, 'zh': {'title': 'Á™ÅÁ†¥3D‰∏é4DÁîüÊàêÁöÑÁì∂È¢à', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫Ü3DÂíå4DËßÜËßâÁîüÊàêÁöÑÊåëÊàòÔºåÂ∞§ÂÖ∂ÊòØÂú®Áº∫‰πèÂ§ßËßÑÊ®°4DÊï∞ÊçÆÁöÑÊÉÖÂÜµ‰∏ã„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊï∞ÊçÆÊï¥ÁêÜÊµÅÁ®ãÔºå‰ªéËßÜÈ¢ë‰∏≠Ëé∑ÂèñÁõ∏Êú∫ÂßøÊÄÅÂíåÁâ©‰ΩìËøêÂä®Âº∫Â∫¶ÔºåÂπ∂ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™Â§ßÂûãÁöÑ4DÂú∫ÊôØÊï∞ÊçÆÈõÜCamVid-30K„ÄÇÂü∫‰∫éËøô‰∫õÊï∞ÊçÆÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜGenXDÊ°ÜÊû∂ÔºåËÉΩÂ§üÁîüÊàê‰ªªÊÑè3DÊàñ4DÂú∫ÊôØ„ÄÇÈÄöËøáÂ§öËßÜËßíÊó∂Èó¥Ê®°ÂùóÔºåGenXDËÉΩÂ§üÊúâÊïàÂú∞Â≠¶‰π†Áõ∏Êú∫ÂíåÁâ©‰ΩìÁöÑËøêÂä®ÔºåÂπ∂ÁîüÊàê‰∏éÁõ∏Êú∫ËΩ®Ëøπ‰∏ÄËá¥ÁöÑËßÜÈ¢ëÂíåÂèØÊèêÂçá‰∏∫3DË°®Á§∫ÁöÑËßÜÂõæ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.02394', 'title': 'AutoVFX: Physically Realistic Video Editing from Natural Language Instructions', 'url': 'https://huggingface.co/papers/2411.02394', 'abstract': "Modern visual effects (VFX) software has made it possible for skilled artists to create imagery of virtually anything. However, the creation process remains laborious, complex, and largely inaccessible to everyday users. In this work, we present AutoVFX, a framework that automatically creates realistic and dynamic VFX videos from a single video and natural language instructions. By carefully integrating neural scene modeling, LLM-based code generation, and physical simulation, AutoVFX is able to provide physically-grounded, photorealistic editing effects that can be controlled directly using natural language instructions. We conduct extensive experiments to validate AutoVFX's efficacy across a diverse spectrum of videos and instructions. Quantitative and qualitative results suggest that AutoVFX outperforms all competing methods by a large margin in generative quality, instruction alignment, editing versatility, and physical plausibility.", 'score': 15, 'issue_id': 435, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 –Ω–æ—è–±—Ä—è', 'en': 'November 4', 'zh': '11Êúà4Êó•'}, 'hash': '96ed53359fbc24a5', 'authors': ['Hao-Yu Hsu', 'Zhi-Hao Lin', 'Albert Zhai', 'Hongchi Xia', 'Shenlong Wang'], 'affiliations': ['University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02394.jpg', 'data': {'categories': ['#reasoning', '#cv', '#video', '#multimodal', '#games', '#architecture', '#alignment'], 'emoji': 'üé¨', 'ru': {'title': 'AutoVFX: –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–µ—Ü—ç—Ñ—Ñ–µ–∫—Ç–æ–≤ —Å–∏–ª–æ–π –º—ã—Å–ª–∏', 'desc': 'AutoVFX - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –≤–∏–¥–µ–æ —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–¥–Ω–æ–≥–æ –≤–∏–¥–µ–æ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ. –û–Ω–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ü–µ–Ω, –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ. AutoVFX –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º –ø–æ–≤–µ–¥–µ–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–Ω–∞–ª–æ–≥–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Transforming VFX Creation with Natural Language and AI', 'desc': 'AutoVFX is a novel framework designed to simplify the creation of visual effects (VFX) by allowing users to generate realistic videos from a single input video and natural language commands. It combines advanced techniques such as neural scene modeling, large language model (LLM)-based code generation, and physical simulation to produce high-quality, dynamic effects. The framework is evaluated through extensive experiments, demonstrating superior performance in generative quality, alignment with user instructions, versatility in editing, and adherence to physical realism. Overall, AutoVFX makes sophisticated VFX creation accessible to a broader audience, reducing the complexity of traditional methods.'}, 'zh': {'title': 'Ëá™Âä®ÂåñËßÜËßâÁâπÊïàÔºåËΩªÊùæÂàõ‰ΩúÁúüÂÆûÂΩ±ÂÉè', 'desc': 'Áé∞‰ª£ËßÜËßâÁâπÊïàËΩØ‰ª∂‰ΩøÂæóÁÜüÁªÉÁöÑËâ∫ÊúØÂÆ∂ËÉΩÂ§üÂàõÈÄ†Âá†‰πé‰ªª‰ΩïÂõæÂÉèÔºå‰ΩÜÂàõ‰ΩúËøáÁ®ã‰ªçÁÑ∂ÁπÅÁêê‰∏îÂ§çÊùÇÔºåÊôÆÈÄöÁî®Êà∑Èöæ‰ª•Êé•Ëß¶„ÄÇÊú¨ÊñáÊèêÂá∫‰∫ÜAutoVFXÔºå‰∏Ä‰∏™Ê°ÜÊû∂ÂèØ‰ª•Ê†πÊçÆÂçï‰∏™ËßÜÈ¢ëÂíåËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§Ëá™Âä®ÂàõÂª∫ÈÄºÁúü‰∏îÂä®ÊÄÅÁöÑËßÜËßâÁâπÊïàËßÜÈ¢ë„ÄÇÈÄöËøáÁ≤æÂøÉÊï¥ÂêàÁ•ûÁªèÂú∫ÊôØÂª∫Ê®°„ÄÅÂü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰ª£Á†ÅÁîüÊàêÂíåÁâ©ÁêÜ‰ªøÁúüÔºåAutoVFXËÉΩÂ§üÊèê‰æõÁâ©ÁêÜÂü∫Á°ÄÁöÑ„ÄÅÁÖßÁâáÁ∫ßÁúüÂÆûÊÑüÁöÑÁºñËæëÊïàÊûúÔºåÂπ∂ÂèØ‰ª•ÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§Áõ¥Êé•ÊéßÂà∂„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåAutoVFXÂú®ÁîüÊàêË¥®Èáè„ÄÅÊåá‰ª§ÂØπÈΩê„ÄÅÁºñËæëÂ§öÊ†∑ÊÄßÂíåÁâ©ÁêÜÂêàÁêÜÊÄßÊñπÈù¢ÊòæËëó‰ºò‰∫éÊâÄÊúâÁ´û‰∫âÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.00836', 'title': 'DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models', 'url': 'https://huggingface.co/papers/2411.00836', 'abstract': "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that SOTA VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010 generated concrete questions. Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. Our analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning.", 'score': 15, 'issue_id': 420, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 29', 'zh': '10Êúà29Êó•'}, 'hash': 'e73ae00a5621a2b9', 'authors': ['Chengke Zou', 'Xingang Guo', 'Rui Yang', 'Junyu Zhang', 'Bin Hu', 'Huan Zhang'], 'affiliations': ['University of Illinois at Urbana-Champaign', 'University of California, Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00836.jpg', 'data': {'categories': ['#reasoning', '#synthetic', '#benchmark', '#cv', '#graphs', '#optimization', '#math', '#dataset', '#architecture'], 'emoji': 'üßÆ', 'ru': {'title': 'DynaMath: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ò–ò', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DynaMath - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π Vision-Language Models (VLM) –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ VLM, —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-4V, —á–∞—Å—Ç–æ –Ω–µ —Å–ø–æ—Å–æ–±–Ω—ã –ø—Ä–∏–º–µ–Ω—è—Ç—å —à–∞–≥–∏ —Ä–µ—à–µ–Ω–∏—è –∫ –ø–æ—Ö–æ–∂–∏–º –∑–∞–¥–∞—á–∞–º —Å –Ω–µ–±–æ–ª—å—à–∏–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏. DynaMath –≤–∫–ª—é—á–∞–µ—Ç 501 –∏—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å –≤ –≤–∏–¥–µ Python-–ø—Ä–æ–≥—Ä–∞–º–º, –ø–æ–∑–≤–æ–ª—è—é—â–∏—Ö –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞—Ü–∏–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ 14 —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö VLM –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –∏—Ö —Ç–æ—á–Ω–æ—Å—Ç—å –≤ —Ö—É–¥—à–µ–º —Å–ª—É—á–∞–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∏–∂–µ —Å—Ä–µ–¥–Ω–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Enhancing VLMs: Unveiling Mathematical Reasoning Limitations with DynaMath', 'desc': "This paper explores the limitations of Vision-Language Models (VLMs) in performing mathematical reasoning tasks that involve visual elements. It highlights that while humans can adapt their problem-solving strategies to slight changes in problems, current state-of-the-art VLMs like GPT-4o struggle with this adaptability. To address this issue, the authors introduce DynaMath, a dynamic benchmark that generates a wide variety of mathematical questions to rigorously test VLMs' reasoning capabilities. The findings reveal that VLMs exhibit significantly lower accuracy in worst-case scenarios compared to average cases, underscoring the importance of evaluating their robustness in mathematical reasoning."}, 'zh': {'title': 'ÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÂ∞§ÂÖ∂ÊòØÂú®ËßÜËßâ‰∏ä‰∏ãÊñáÁöÑÂΩ±Âìç‰∏ã„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°‰∫∫Á±ªËÉΩÂ§üÁÅµÊ¥ªÂ∫îÂØπÁõ∏‰ººÈóÆÈ¢òÁöÑÂèòÂåñÔºåÂΩìÂâçÁöÑÊúÄÂÖàËøõÊ®°ÂûãÂ¶ÇGPT-4oÂú®Èù¢ÂØπËøô‰∫õÂèòÂåñÊó∂Âç¥Ë°®Áé∞‰∏ç‰Ω≥ÔºåÊòæÁ§∫Âá∫ÂÖ∂Êï∞Â≠¶Êé®ÁêÜËÉΩÂäõÁöÑÂ±ÄÈôêÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜDynaMathÔºå‰∏Ä‰∏™Âä®ÊÄÅËßÜËßâÊï∞Â≠¶Âü∫ÂáÜÔºåÊó®Âú®Ê∑±ÂÖ•ËØÑ‰º∞VLMsÁöÑÊé®ÁêÜÁ®≥ÂÅ•ÊÄß„ÄÇÈÄöËøáÂØπ501‰∏™È´òË¥®ÈáèÁßçÂ≠êÈóÆÈ¢òÁöÑËá™Âä®ÁîüÊàêÔºåDynaMathËÉΩÂ§üËØÑ‰º∞Ê®°ÂûãÂú®‰∏çÂêåËæìÂÖ•Êù°‰ª∂‰∏ãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÁªìÊûúÊòæÁ§∫Ê®°ÂûãÂú®ÊúÄÂùèÊÉÖÂÜµ‰∏ãÁöÑÂáÜÁ°ÆÁéáÊòæËëó‰Ωé‰∫éÂπ≥ÂùáÊÉÖÂÜµ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.01747', 'title': 'DynaSaur: Large Language Agents Beyond Predefined Actions', 'url': 'https://huggingface.co/papers/2411.01747', 'abstract': 'Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly-scoped environments, we argue that it presents two major challenges when deploying LLM agents in real-world scenarios: (1) selecting from a fixed set of actions significantly restricts the planning and acting capabilities of LLM agents, and (2) this approach requires substantial human effort to enumerate and implement all possible actions, which becomes impractical in complex environments with a vast number of potential actions. In this work, we propose an LLM agent framework that enables the dynamic creation and composition of actions in an online manner. In this framework, the agent interacts with the environment by generating and executing programs written in a general-purpose programming language at each step. Furthermore, generated actions are accumulated over time for future reuse. Our extensive experiments on the GAIA benchmark demonstrate that this framework offers significantly greater flexibility and outperforms previous methods. Notably, it allows an LLM agent to recover in scenarios where no relevant action exists in the predefined set or when existing actions fail due to unforeseen edge cases. At the time of writing, we hold the top position on the GAIA public leaderboard. Our code can be found in https://github.com/adobe-research/dynasaur{https://github.com/adobe-research/dynasaur}.', 'score': 13, 'issue_id': 423, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 –Ω–æ—è–±—Ä—è', 'en': 'November 4', 'zh': '11Êúà4Êó•'}, 'hash': '772b11b15cab80a0', 'authors': ['Dang Nguyen', 'Viet Dac Lai', 'Seunghyun Yoon', 'Ryan A. Rossi', 'Handong Zhao', 'Ruiyi Zhang', 'Puneet Mathur', 'Nedim Lipka', 'Yu Wang', 'Trung Bui', 'Franck Dernoncourt', 'Tianyi Zhou'], 'affiliations': ['University of Maryland', 'Adobe Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01747.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#agi', '#plp', '#open_source', '#agents', '#architecture'], 'emoji': 'ü§ñ', 'ru': {'title': '–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –±–∞–∑–µ LLM: —à–∞–≥ –∫ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–º—É –ò–ò', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –∏ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è –≤ —Ä–µ–∂–∏–º–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–∏—Å—Ç–µ–º —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –¥–µ–π—Å—Ç–≤–∏–π, –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–π –ø–æ–¥—Ö–æ–¥ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–≥—Ä–∞–º–º—ã –Ω–∞ —è–∑—ã–∫–µ –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥–æ–π. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –∏—Ö –º–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª—å—à—É—é –≥–∏–±–∫–æ—Å—Ç—å –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ GAIA. –û—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö, –≥–¥–µ –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∏–ª–∏ –Ω–µ–ø—Ä–∏–º–µ–Ω–∏–º—ã.'}, 'en': {'title': 'Empowering LLM Agents with Dynamic Action Generation', 'desc': 'This paper introduces a new framework for large language model (LLM) agents that allows them to dynamically create and execute actions in real-time, rather than relying on a fixed set of predefined actions. This approach enhances the planning and acting capabilities of LLM agents, making them more adaptable to complex and unpredictable environments. By generating programs in a general-purpose programming language, the agents can respond to unforeseen situations and accumulate useful actions for future tasks. The experiments conducted on the GAIA benchmark show that this framework significantly outperforms traditional methods, providing greater flexibility and effectiveness in real-world applications.'}, 'zh': {'title': 'Âä®ÊÄÅÂàõÂª∫‰∏éÁªÑÂêàÂä®‰ΩúÁöÑLLM‰ª£ÁêÜÊ°ÜÊû∂', 'desc': 'Áé∞ÊúâÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜÁ≥ªÁªüÈÄöÂ∏∏Âú®ÊØè‰∏ÄÊ≠•‰ªéÂõ∫ÂÆöÁöÑÈ¢ÑÂÆö‰πâÂä®‰ΩúÈõÜ‰∏≠ÈÄâÊã©Âä®‰Ωú„ÄÇËøôÁßçÊñπÊ≥ïÂú®Â∞ÅÈó≠ÁöÑ„ÄÅÁã≠Á™ÑÁöÑÁéØÂ¢É‰∏≠ÊúâÊïàÔºå‰ΩÜÂú®Áé∞ÂÆû‰∏ñÁïå‰∏≠Â∫îÁî®Êó∂Èù¢‰∏¥‰∏§‰∏™‰∏ªË¶ÅÊåëÊàòÔºö‰∏ÄÊòØ‰ªéÂõ∫ÂÆöÂä®‰ΩúÈõÜ‰∏≠ÈÄâÊã©ÈôêÂà∂‰∫ÜLLM‰ª£ÁêÜÁöÑËßÑÂàíÂíåÊâßË°åËÉΩÂäõÔºå‰∫åÊòØÈúÄË¶ÅÂ§ßÈáè‰∫∫ÂäõÊù•Âàó‰∏æÂíåÂÆûÁé∞ÊâÄÊúâÂèØËÉΩÁöÑÂä®‰ΩúÔºåËøôÂú®Â§çÊùÇÁéØÂ¢É‰∏≠ÂèòÂæó‰∏çÂàáÂÆûÈôÖ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçLLM‰ª£ÁêÜÊ°ÜÊû∂ÔºåÂÖÅËÆ∏Âú®Á∫øÂä®ÊÄÅÂàõÂª∫ÂíåÁªÑÂêàÂä®‰ΩúÔºå‰ª£ÁêÜÈÄöËøáÁîüÊàêÂíåÊâßË°åÈÄöÁî®ÁºñÁ®ãËØ≠Ë®ÄÁºñÂÜôÁöÑÁ®ãÂ∫è‰∏éÁéØÂ¢É‰∫íÂä®„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåËØ•Ê°ÜÊû∂Êèê‰æõ‰∫ÜÊõ¥Â§ßÁöÑÁÅµÊ¥ªÊÄßÔºåÂπ∂Âú®GAIAÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫é‰ª•ÂæÄÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.02327', 'title': 'PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance', 'url': 'https://huggingface.co/papers/2411.02327', 'abstract': "The past year has witnessed the significant advancement of video-based large language models. However, the challenge of developing a unified model for both short and long video understanding remains unresolved. Most existing video LLMs cannot handle hour-long videos, while methods custom for long videos tend to be ineffective for shorter videos and images. In this paper, we identify the key issue as the redundant content in videos. To address this, we propose a novel pooling strategy that simultaneously achieves token compression and instruction-aware visual feature aggregation. Our model is termed Prompt-guided Pooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three core components: the CLIP-based visual-prompt alignment that extracts visual information relevant to the user's instructions, the prompt-guided pooling that compresses the visual sequence to arbitrary scales using convolution-style pooling, and the clip context extension designed for lengthy prompt common in visual dialogue. Moreover, our codebase also integrates the most advanced video Direct Preference Optimization (DPO) and visual interleave training. Extensive experiments have validated the performance of our model. With superior throughput and only 1024 visual context, PPLLaVA achieves better results on image benchmarks as a video LLM, while achieving state-of-the-art performance across various video benchmarks, excelling in tasks ranging from caption generation to multiple-choice questions, and handling video lengths from seconds to hours. Codes have been available at https://github.com/farewellthree/PPLLaVA.", 'score': 11, 'issue_id': 432, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 –Ω–æ—è–±—Ä—è', 'en': 'November 4', 'zh': '11Êúà4Êó•'}, 'hash': 'ed9d356ccf75d780', 'authors': ['Ruyang Liu', 'Haoran Tang', 'Haibo Liu', 'Yixiao Ge', 'Ying Shan', 'Chen Li', 'Jiankun Yang'], 'affiliations': ['Peking University', 'Applied Research Center (ARC), Tencent PCG', 'Peng Cheng Laboratory'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02327.jpg', 'data': {'categories': ['#long_context', '#rlhf', '#benchmark', '#video', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': 'üé•', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –ª—é–±–æ–π –¥–ª–∏–Ω—ã —Å –ø–æ–º–æ—â—å—é –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø—É–ª–∏–Ω–≥–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å PPLLaVA –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–∫ –∫–æ—Ä–æ—Ç–∫–∏—Ö, —Ç–∞–∫ –∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ. –ö–ª—é—á–µ–≤–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø—É–ª–∏–Ω–≥–∞, –∫–æ—Ç–æ—Ä–∞—è —Å–∂–∏–º–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å —É—á–µ—Ç–æ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ú–æ–¥–µ–ª—å –≤–∫–ª—é—á–∞–µ—Ç –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ CLIP, –ø—É–ª–∏–Ω–≥ —Å —É—á–µ—Ç–æ–º –ø–æ–¥—Å–∫–∞–∑–æ–∫ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫–ª–∏–ø–æ–≤. PPLLaVA –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ —Ä–∞–∑–Ω–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Unified Video Understanding with PPLLaVA', 'desc': 'This paper presents a new model called Prompt-guided Pooling LLaVA (PPLLaVA) that improves video understanding for both short and long videos. The authors identify redundant content in videos as a key challenge and propose a pooling strategy that compresses tokens while aggregating visual features based on user instructions. PPLLaVA includes components for visual-prompt alignment, convolution-style pooling, and context extension for lengthy prompts. The model demonstrates superior performance across various benchmarks, effectively handling video lengths from seconds to hours.'}, 'zh': {'title': 'ËßÜÈ¢ëÁêÜËß£ÁöÑÊñ∞Á™ÅÁ†¥ÔºöPPLLaVAÊ®°Âûã', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜÈ¢ëÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåÁß∞‰∏∫PPLLaVAÔºåÊó®Âú®Ëß£ÂÜ≥Áü≠ËßÜÈ¢ëÂíåÈïøËßÜÈ¢ëÁêÜËß£ÁöÑÁªü‰∏ÄÊ®°ÂûãÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÂèëÁé∞ËßÜÈ¢ë‰∏≠ÁöÑÂÜó‰ΩôÂÜÖÂÆπÊòØ‰∏ªË¶ÅÊåëÊàòÔºåÂõ†Ê≠§ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ±†ÂåñÁ≠ñÁï•ÔºåÂÆûÁé∞‰∫Ü‰ª§ÁâåÂéãÁº©ÂíåÊåá‰ª§ÊÑüÁü•ÁöÑËßÜËßâÁâπÂæÅËÅöÂêà„ÄÇPPLLaVAÂåÖÂê´‰∏â‰∏™Ê†∏ÂøÉÁªÑ‰ª∂ÔºåÂàÜÂà´ÊòØÂü∫‰∫éCLIPÁöÑËßÜËßâÊèêÁ§∫ÂØπÈΩê„ÄÅÊèêÁ§∫ÂºïÂØºÊ±†ÂåñÂíåÂâ™Ëæë‰∏ä‰∏ãÊñáÊâ©Â±ï„ÄÇÁªèËøáÂπøÊ≥õÂÆûÈ™åÈ™åËØÅÔºåPPLLaVAÂú®Â§ÑÁêÜ‰ªéÂá†ÁßíÂà∞Âá†Â∞èÊó∂ÁöÑËßÜÈ¢ëÊó∂ÔºåË°®Áé∞Âá∫Ëâ≤ÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑËßÜÈ¢ëÂü∫ÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.02335', 'title': 'Sparsing Law: Towards Large Language Models with Greater Activation Sparsity', 'url': 'https://huggingface.co/papers/2411.02335', 'abstract': 'Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL-p% sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., 1-sparsity ratio) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable.', 'score': 11, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 –Ω–æ—è–±—Ä—è', 'en': 'November 4', 'zh': '11Êúà4Êó•'}, 'hash': '3c9152f4d267fc7b', 'authors': ['Yuqi Luo', 'Chenyang Song', 'Xu Han', 'Yingfa Chen', 'Chaojun Xiao', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02335.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#interpretability'], 'emoji': 'üß†', 'ru': {'title': '–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤ LLM: –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É PPL-p% –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —à–∏—Ä–∏–Ω—ã –∏ –≥–ª—É–±–∏–Ω—ã –º–æ–¥–µ–ª–∏ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–∞—Ü–∏–π, –∞ –º–∞—Å—à—Ç–∞–± –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–∞–±–æ–µ –≤–ª–∏—è–Ω–∏–µ.'}, 'en': {'title': 'Unlocking Efficiency: The Power of Activation Sparsity in LLMs', 'desc': 'This paper investigates activation sparsity in decoder-only Transformer-based large language models (LLMs), focusing on how weakly-contributed activation outputs can be reduced for efficiency. The authors introduce a new metric called PPL-p% sparsity to quantitatively measure activation sparsity across different activation functions. Their experiments reveal that while ReLU and SiLU functions perform similarly, they exhibit opposite trends in training-time sparsity, with ReLU being more efficient. Additionally, the study finds that activation sparsity is largely unaffected by the parameter scale, suggesting that deeper architectures may enhance efficiency without requiring more parameters.'}, 'zh': {'title': 'ÊøÄÊ¥ªÁ®ÄÁñèÊÄßÔºöÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊïàÁéáÁöÑÂÖ≥ÈîÆ', 'desc': 'ÊøÄÊ¥ªÁ®ÄÁñèÊÄßÊåáÁöÑÊòØÂú®ÊøÄÊ¥ªËæìÂá∫‰∏≠Â≠òÂú®Â§ßÈáèË¥°ÁåÆËæÉÂº±ÁöÑÂÖÉÁ¥†ÔºåËøô‰∫õÂÖÉÁ¥†ÂèØ‰ª•Ë¢´Ê∂àÈô§Ôºå‰ªéËÄåÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑËÆ∏Â§öÈáçË¶ÅÂ∫îÁî®ÊúâÁõä„ÄÇÊú¨ÊñáÂØπÂü∫‰∫éËß£Á†ÅÂô®ÁöÑTransformer LLM‰∏≠ÁöÑÊøÄÊ¥ªÁ®ÄÁñèÊÄßËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑÂÆöÈáèÁ†îÁ©∂ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊøÄÊ¥ªÁ®ÄÁñèÊÄßÂ∫¶ÈáèÊ†áÂáÜPPL-p%Á®ÄÁñèÊÄßÔºåÈÄÇÁî®‰∫é‰ªª‰ΩïÊøÄÊ¥ªÂáΩÊï∞„ÄÇÁ†îÁ©∂ÂèëÁé∞Ôºå‰∏çÂêåÁöÑÊøÄÊ¥ªÂáΩÊï∞Âú®ÊÄßËÉΩ‰∏äÁõ∏‰ººÔºå‰ΩÜÂú®ËÆ≠ÁªÉÊó∂Èó¥ÁöÑÁ®ÄÁñèÊÄßË∂ãÂäø‰∏äÂç¥Áõ∏ÂèçÔºåReLUÊøÄÊ¥ªÂáΩÊï∞Âú®Âà©Áî®Êõ¥Â§öËÆ≠ÁªÉÊï∞ÊçÆÊñπÈù¢Êõ¥‰∏∫È´òÊïà„ÄÇÊúÄÂêéÔºåÁ†îÁ©∂Ë°®ÊòéÔºåÊøÄÊ¥ªÁ®ÄÁñèÊÄßÁöÑÊûÅÈôêÂÄº‰∏éÂèÇÊï∞ËßÑÊ®°ÁöÑÂèòÂåñÂÖ≥Á≥ª‰∏çÂ§ßÔºåËøô‰∏∫ÊèêÈ´òLLMsÁöÑÊïàÁéáÂíåÂèØËß£ÈáäÊÄßÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÂêØÁ§∫„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.01798', 'title': 'SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF', 'url': 'https://huggingface.co/papers/2411.01798', 'abstract': "In Large Language Model (LLM) development, Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning models with human values and preferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence between the current policy and a frozen initial policy as a reference, which is added as a penalty in policy optimization algorithms like Proximal Policy Optimization (PPO). While this constraint prevents models from deviating too far from the initial checkpoint, it limits exploration of the reward landscape, reducing the model's ability to discover higher-quality solutions. As a result, policy optimization is often trapped in a narrow region of the parameter space, leading to suboptimal alignment and performance. This paper presents SALSA (Soup-based Alignment Learning for Stronger Adaptation), a novel approach designed to overcome these limitations by creating a more flexible and better located reference model through weight-space averaging of two independent supervised fine-tuned (SFT) models. This model soup allows for larger deviation in KL divergence and exploring a promising region of the solution space without sacrificing stability. By leveraging this more robust reference model, SALSA fosters better exploration, achieving higher rewards and improving model robustness, out-of-distribution generalization, and performance. We validate the effectiveness of SALSA through extensive experiments on popular open models (Llama2-7B, Mistral-7B, and Gemma-2B) across various benchmarks (MT-Bench, Arena-Hard, UltraFeedback), where it consistently surpasses PPO by fostering deeper exploration and achieving superior alignment in LLMs.", 'score': 8, 'issue_id': 435, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 –Ω–æ—è–±—Ä—è', 'en': 'November 4', 'zh': '11Êúà4Êó•'}, 'hash': 'e5a60efe079c6136', 'authors': ['Atoosa Chegini', 'Hamid Kazemi', 'Iman Mirzadeh', 'Dong Yin', 'Maxwell Horton', 'Moin Nabi', 'Mehrdad Farajtabar', 'Keivan Alizadeh'], 'affiliations': ['University of Maryland', 'Apple'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01798.jpg', 'data': {'categories': ['#small_models', '#rl', '#rlhf', '#benchmark', '#optimization', '#training', '#open_source', '#alignment'], 'emoji': 'üß†', 'ru': {'title': 'SALSA: –ì–∏–±–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ SALSA –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –æ—Ç –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ —á–µ–ª–æ–≤–µ–∫–∞ (RLHF). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ PPO, –∏—Å–ø–æ–ª—å–∑—É—é—â–µ–≥–æ KL-–¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—é —Å –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω–æ–π –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª—å—é, SALSA —Å–æ–∑–¥–∞–µ—Ç –±–æ–ª–µ–µ –≥–∏–±–∫—É—é –æ–ø–æ—Ä–Ω—É—é –º–æ–¥–µ–ª—å –ø—É—Ç–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤ –¥–≤—É—Ö –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —à–∏—Ä–æ–∫—É—é –æ–±–ª–∞—Å—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–µ –∂–µ—Ä—Ç–≤—É—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å—é. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ SALSA –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç PPO –ø–æ –º–Ω–æ–≥–∏–º –º–µ—Ç—Ä–∏–∫–∞–º, –≤–∫–ª—é—á–∞—è –Ω–∞–≥—Ä–∞–¥—ã, –æ–±–æ–±—â–µ–Ω–∏–µ –≤–Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ –æ–±—â—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'SALSA: Enhancing LLM Alignment through Flexible Exploration', 'desc': "This paper introduces SALSA, a new method for improving the alignment of Large Language Models (LLMs) with human preferences using Reinforcement Learning from Human Feedback (RLHF). Traditional RLHF methods use a fixed reference policy, which can restrict the model's ability to explore and find better solutions due to the penalty imposed by Kullback-Leibler (KL) divergence. SALSA addresses this issue by averaging weights from two independently fine-tuned models, creating a more flexible reference that allows for greater exploration of the reward landscape. The results show that SALSA enhances model performance, robustness, and generalization across various benchmarks compared to traditional methods like Proximal Policy Optimization (PPO)."}, 'zh': {'title': 'SALSAÔºöÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂØπÈΩê‰∏éÊé¢Á¥¢ÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Âú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂºÄÂèë‰∏≠ÔºåÂü∫‰∫é‰∫∫Á±ªÂèçÈ¶àÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLHFÔºâÂØπ‰∫é‰ΩøÊ®°Âûã‰∏é‰∫∫Á±ª‰ª∑ÂÄºËßÇÂíåÂÅèÂ•Ω‰øùÊåÅ‰∏ÄËá¥Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ‰º†ÁªüÁöÑRLHF‰æùËµñ‰∫éÂΩìÂâçÁ≠ñÁï•‰∏éÂÜªÁªìÂàùÂßãÁ≠ñÁï•‰πãÈó¥ÁöÑKullback-LeiblerÔºàKLÔºâÊï£Â∫¶‰Ωú‰∏∫ÂèÇËÄÉÔºåËøôÂú®Á≠ñÁï•‰ºòÂåñÁÆóÊ≥ï‰∏≠‰Ωú‰∏∫ÊÉ©ÁΩöÈ°π‰ΩøÁî®„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïSALSAÔºàÂü∫‰∫éÊ®°ÂûãÈõÜÂêàÁöÑÂØπÈΩêÂ≠¶‰π†ÔºâÔºåÈÄöËøáÂØπ‰∏§‰∏™Áã¨Á´ãÁöÑÁõëÁù£ÂæÆË∞ÉÊ®°ÂûãËøõË°åÊùÉÈáçÁ©∫Èó¥Âπ≥ÂùáÔºåÂàõÂª∫‰∏Ä‰∏™Êõ¥ÁÅµÊ¥ªÁöÑÂèÇËÄÉÊ®°ÂûãÔºå‰ªéËÄåÂÖãÊúç‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇSALSAÈÄöËøáÊõ¥Â•ΩÁöÑÊé¢Á¥¢ËÉΩÂäõÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÂíåÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÂÖ∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.00785', 'title': 'IGOR: Image-GOal Representations are the Atomic Control Units for Foundation Models in Embodied AI', 'url': 'https://huggingface.co/papers/2411.00785', 'abstract': 'We introduce Image-GOal Representations (IGOR), aiming to learn a unified, semantically consistent action space across human and various robots. Through this unified latent action space, IGOR enables knowledge transfer among large-scale robot and human activity data. We achieve this by compressing visual changes between an initial image and its goal state into latent actions. IGOR allows us to generate latent action labels for internet-scale video data. This unified latent action space enables the training of foundation policy and world models across a wide variety of tasks performed by both robots and humans. We demonstrate that: (1) IGOR learns a semantically consistent action space for both human and robots, characterizing various possible motions of objects representing the physical interaction knowledge; (2) IGOR can "migrate" the movements of the object in the one video to other videos, even across human and robots, by jointly using the latent action model and world model; (3) IGOR can learn to align latent actions with natural language through the foundation policy model, and integrate latent actions with a low-level policy model to achieve effective robot control. We believe IGOR opens new possibilities for human-to-robot knowledge transfer and control.', 'score': 8, 'issue_id': 434, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 –æ–∫—Ç—è–±—Ä—è', 'en': 'October 17', 'zh': '10Êúà17Êó•'}, 'hash': 'a7e2824d0e474c0b', 'authors': ['Xiaoyu Chen', 'Junliang Guo', 'Tianyu He', 'Chuheng Zhang', 'Pushi Zhang', 'Derek Cathera Yang', 'Li Zhao', 'Jiang Bian'], 'affiliations': ['Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00785.jpg', 'data': {'categories': ['#science', '#cv', '#graphs', '#video', '#multimodal', '#training', '#dataset', '#robotics', '#transfer_learning', '#architecture'], 'emoji': 'ü§ñ', 'ru': {'title': '–ï–¥–∏–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –ª—é–¥–µ–π –∏ —Ä–æ–±–æ—Ç–æ–≤', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç IGOR - —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –µ–¥–∏–Ω–æ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –ª—é–¥–µ–π –∏ —Ä–æ–±–æ—Ç–æ–≤. IGOR —Å–∂–∏–º–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ–∂–¥—É –Ω–∞—á–∞–ª—å–Ω—ã–º –∏ —Ü–µ–ª–µ–≤—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –≤ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç–∫–∏ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ –µ–¥–∏–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å –±–∞–∑–æ–≤—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ –º–æ–¥–µ–ª–∏ –º–∏—Ä–∞ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á, –≤—ã–ø–æ–ª–Ω—è–µ–º—ã—Ö –∫–∞–∫ —Ä–æ–±–æ—Ç–∞–º–∏, —Ç–∞–∫ –∏ –ª—é–¥—å–º–∏. IGOR –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–Ω–æ—Å–∞ –¥–≤–∏–∂–µ–Ω–∏–π –æ–±—ä–µ–∫—Ç–æ–≤ –º–µ–∂–¥—É –≤–∏–¥–µ–æ, –¥–∞–∂–µ –º–µ–∂–¥—É –ª—é–¥—å–º–∏ –∏ —Ä–æ–±–æ—Ç–∞–º–∏, –∞ —Ç–∞–∫–∂–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —è–∑—ã–∫–æ–º –∏ –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —Ä–æ–±–æ—Ç–∞–º–∏.'}, 'en': {'title': 'Unified Action Space for Human-Robot Interaction', 'desc': 'The paper presents Image-GOal Representations (IGOR), a framework designed to create a unified action space that is semantically consistent for both humans and robots. By compressing the visual changes between an initial image and its goal state into latent actions, IGOR facilitates knowledge transfer across diverse datasets of human and robot activities. This approach allows for the generation of latent action labels from large-scale video data, enabling the training of foundational policy and world models for various tasks. Ultimately, IGOR enhances the ability to transfer movement knowledge between humans and robots, aligning actions with natural language for improved robot control.'}, 'zh': {'title': 'Áªü‰∏ÄÂä®‰ΩúÁ©∫Èó¥ÔºåËøûÊé•‰∫∫Á±ª‰∏éÊú∫Âô®‰∫∫', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜÂõæÂÉèÁõÆÊ†áË°®Á§∫ÔºàIGORÔºâÔºåÊó®Âú®Â≠¶‰π†‰∏Ä‰∏™Áªü‰∏Ä‰∏îËØ≠‰πâ‰∏ÄËá¥ÁöÑÂä®‰ΩúÁ©∫Èó¥ÔºåÈÄÇÁî®‰∫é‰∫∫Á±ªÂíåÂêÑÁßçÊú∫Âô®‰∫∫„ÄÇÈÄöËøáËøô‰∏™Áªü‰∏ÄÁöÑÊΩúÂú®Âä®‰ΩúÁ©∫Èó¥ÔºåIGORËÉΩÂ§üÂú®Â§ßËßÑÊ®°Êú∫Âô®‰∫∫Âíå‰∫∫Á±ªÊ¥ªÂä®Êï∞ÊçÆ‰πãÈó¥ËøõË°åÁü•ËØÜËΩ¨Áßª„ÄÇÊàë‰ª¨ÈÄöËøáÂéãÁº©ÂàùÂßãÂõæÂÉè‰∏éÁõÆÊ†áÁä∂ÊÄÅ‰πãÈó¥ÁöÑËßÜËßâÂèòÂåñÊù•ÂÆûÁé∞Ëøô‰∏ÄÁÇπÔºå‰ªéËÄåÁîüÊàêÊΩúÂú®Âä®‰ΩúÊ†áÁ≠æ„ÄÇIGOR‰∏∫Êú∫Âô®‰∫∫ÊéßÂà∂Âíå‰∫∫Êú∫‰∫§‰∫íÂºÄËæü‰∫ÜÊñ∞ÁöÑÂèØËÉΩÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.00918', 'title': 'LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models', 'url': 'https://huggingface.co/papers/2411.00918', 'abstract': 'Mixture of Experts (MoEs) plays an important role in the development of more efficient and effective large language models (LLMs). Due to the enormous resource requirements, studying large scale MoE algorithms remain in-accessible to many researchers. This work develops LibMoE, a comprehensive and modular framework to streamline the research, training, and evaluation of MoE algorithms. Built upon three core principles: (i) modular design, (ii) efficient training; (iii) comprehensive evaluation, LibMoE brings MoE in LLMs more accessible to a wide range of researchers by standardizing the training and evaluation pipelines. Using LibMoE, we extensively benchmarked five state-of-the-art MoE algorithms over three different LLMs and 11 datasets under the zero-shot setting. The results show that despite the unique characteristics, all MoE algorithms perform roughly similar when averaged across a wide range of tasks. With the modular design and extensive evaluation, we believe LibMoE will be invaluable for researchers to make meaningful progress towards the next generation of MoE and LLMs. Project page: https://fsoft-aic.github.io/fsoft-LibMoE.github.io.', 'score': 8, 'issue_id': 420, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 –Ω–æ—è–±—Ä—è', 'en': 'November 1', 'zh': '11Êúà1Êó•'}, 'hash': 'a406640433a3de34', 'authors': ['Nam V. Nguyen', 'Thong T. Doan', 'Luong Tran', 'Van Nguyen', 'Quang Pham'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00918.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': 'üß†', 'ru': {'title': 'LibMoE: –î–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π Mixture of Experts –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LibMoE - –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –º–æ–¥—É–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ Mixture of Experts (MoE) –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). LibMoE –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ç—Ä–µ—Ö –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö: –º–æ–¥—É–ª—å–Ω—ã–π –¥–∏–∑–∞–π–Ω, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –æ—Ü–µ–Ω–∫–∞. –ò—Å–ø–æ–ª—å–∑—É—è LibMoE, –∞–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –æ–±—à–∏—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—è—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ MoE –Ω–∞ —Ç—Ä–µ—Ö —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LLM –∏ 11 –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–µ–∂–∏–º–µ zero-shot. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –≤—Å–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã MoE –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–∏ –ø–æ —à–∏—Ä–æ–∫–æ–º—É —Å–ø–µ–∫—Ç—Ä—É –∑–∞–¥–∞—á.'}, 'en': {'title': 'LibMoE: Streamlining Mixture of Experts for Large Language Models', 'desc': 'This paper introduces LibMoE, a framework designed to facilitate research on Mixture of Experts (MoE) algorithms for large language models (LLMs). It emphasizes a modular design, efficient training processes, and comprehensive evaluation methods to make MoE more accessible to researchers. The authors benchmark five leading MoE algorithms across three LLMs and 11 datasets, revealing that their performance is generally similar across various tasks. LibMoE aims to standardize the training and evaluation of MoE, helping researchers advance the development of future LLMs.'}, 'zh': {'title': 'LibMoEÔºöËÆ©Ê∑∑Âêà‰∏ìÂÆ∂ÁÆóÊ≥ïÊõ¥Êòì‰∫éÁ†îÁ©∂ÂíåÂ∫îÁî®', 'desc': 'Ê∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÈ´òÊïàÂíåÊúâÊïàÂèëÂ±ï‰∏≠ÊâÆÊºîÁùÄÈáçË¶ÅËßíËâ≤„ÄÇÁî±‰∫éËµÑÊ∫êÈúÄÊ±ÇÂ∑®Â§ßÔºåËÆ∏Â§öÁ†îÁ©∂ËÄÖÈöæ‰ª•Á†îÁ©∂Â§ßËßÑÊ®°ÁöÑMoEÁÆóÊ≥ï„ÄÇÊú¨ÊñáÂºÄÂèë‰∫ÜLibMoEÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ®Èù¢‰∏îÊ®°ÂùóÂåñÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÁÆÄÂåñMoEÁÆóÊ≥ïÁöÑÁ†îÁ©∂„ÄÅËÆ≠ÁªÉÂíåËØÑ‰º∞„ÄÇÈÄöËøáÊ®°ÂùóÂåñËÆæËÆ°„ÄÅÈ´òÊïàËÆ≠ÁªÉÂíåÂÖ®Èù¢ËØÑ‰º∞ÔºåLibMoE‰ΩøÂæóMoEÂú®LLMs‰∏≠ÁöÑÂ∫îÁî®ÂØπÊõ¥Â§öÁ†îÁ©∂ËÄÖÂèòÂæóÂèØÂèä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.00743', 'title': 'Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models', 'url': 'https://huggingface.co/papers/2411.00743', 'abstract': 'Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability methods. Sparse Autoencoders (SAEs) have emerged as a promising tool for disentangling FM representations, but they struggle to capture rare, yet crucial concepts in the data. We introduce Specialized Sparse Autoencoders (SSAEs), designed to illuminate these elusive dark matter features by focusing on specific subdomains. We present a practical recipe for training SSAEs, demonstrating the efficacy of dense retrieval for data selection and the benefits of Tilted Empirical Risk Minimization as a training objective to improve concept recall. Our evaluation of SSAEs on standard metrics, such as downstream perplexity and L_0 sparsity, show that they effectively capture subdomain tail concepts, exceeding the capabilities of general-purpose SAEs. We showcase the practical utility of SSAEs in a case study on the Bias in Bios dataset, where SSAEs achieve a 12.5\\% increase in worst-group classification accuracy when applied to remove spurious gender information. SSAEs provide a powerful new lens for peering into the inner workings of FMs in subdomains.', 'score': 6, 'issue_id': 421, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 –Ω–æ—è–±—Ä—è', 'en': 'November 1', 'zh': '11Êúà1Êó•'}, 'hash': 'af234e3c99f935ec', 'authors': ['Aashiq Muhamed', 'Mona Diab', 'Virginia Smith'], 'affiliations': ['Language Technologies Institute, Carnegie Mellon University', 'Machine Learning Department, Carnegie Mellon University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00743.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#ethics', '#data', '#training', '#dataset', '#architecture'], 'emoji': 'üîç', 'ru': {'title': '–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç —Å–∫—Ä—ã—Ç—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã –≤ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π - –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä—ã (SSAE). SSAE —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø–æ–¥–¥–æ–º–µ–Ω–∞—Ö –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã—è–≤–ª—è—é—Ç —Ä–µ–¥–∫–∏–µ, –Ω–æ –≤–∞–∂–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã –≤ –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–ª–æ—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Tilted Empirical Risk Minimization –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å SSAE –ø–æ–∫–∞–∑–∞–Ω–∞ –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫–∞—Ö –∏ –≤ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö Bias in Bios.'}, 'en': {'title': 'Illuminating Hidden Concepts in Foundation Models with SSAEs', 'desc': 'This paper discusses the challenges of understanding and mitigating risks in foundation models (FMs) through effective interpretability methods. It introduces Specialized Sparse Autoencoders (SSAEs), which are designed to better identify rare but important concepts in data by focusing on specific subdomains. The authors provide a training approach that utilizes dense retrieval for data selection and Tilted Empirical Risk Minimization to enhance concept recall. The results show that SSAEs outperform traditional Sparse Autoencoders in capturing these critical features, as demonstrated in a case study that improved classification accuracy by addressing bias in gender information.'}, 'zh': {'title': '‰∏ìÊ≥®Â≠êÈ¢ÜÂüüÁöÑÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºöÊè≠Á§∫Âü∫Á°ÄÊ®°ÂûãÁöÑÊΩúÂú®ÁâπÂæÅ', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂü∫Á°ÄÊ®°ÂûãÔºàFMsÔºâÊΩúÂú®È£éÈô©ÁöÑÁêÜËß£‰∏éÁºìËß£ÔºåÂº∫Ë∞É‰∫ÜÊúâÊïàÁöÑÂèØËß£ÈáäÊÄßÊñπÊ≥ïÁöÑÈáçË¶ÅÊÄß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®ÔºàSSAEsÔºâÔºåÊó®Âú®Êè≠Á§∫Êï∞ÊçÆ‰∏≠Á®ÄÊúâ‰ΩÜÈáçË¶ÅÁöÑÊ¶ÇÂøµÔºåÁâπÂà´ÂÖ≥Ê≥®ÁâπÂÆöÂ≠êÈ¢ÜÂüü„ÄÇÈÄöËøáÂØÜÈõÜÊ£ÄÁ¥¢ÂíåÂÄæÊñúÁªèÈ™åÈ£éÈô©ÊúÄÂ∞èÂåñÁ≠âÊñπÊ≥ïÔºåÊàë‰ª¨Â±ïÁ§∫‰∫ÜSSAEsÂú®ÊçïÊçâÂ≠êÈ¢ÜÂüüÂ∞æÈÉ®Ê¶ÇÂøµÊñπÈù¢ÁöÑ‰ºòÂäø„ÄÇÊ°à‰æãÁ†îÁ©∂Ë°®ÊòéÔºåSSAEsÂú®ÂéªÈô§ËôöÂÅáÊÄßÂà´‰ø°ÊÅØÊó∂ÔºåÂàÜÁ±ªÂáÜÁ°ÆÁéáÊèêÈ´ò‰∫Ü12.5%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.00359', 'title': 'Constrained Diffusion Implicit Models', 'url': 'https://huggingface.co/papers/2411.00359', 'abstract': 'This paper describes an efficient algorithm for solving noisy linear inverse problems using pretrained diffusion models. Extending the paradigm of denoising diffusion implicit models (DDIM), we propose constrained diffusion implicit models (CDIM) that modify the diffusion updates to enforce a constraint upon the final output. For noiseless inverse problems, CDIM exactly satisfies the constraints; in the noisy case, we generalize CDIM to satisfy an exact constraint on the residual distribution of the noise. Experiments across a variety of tasks and metrics show strong performance of CDIM, with analogous inference acceleration to unconstrained DDIM: 10 to 50 times faster than previous conditional diffusion methods. We demonstrate the versatility of our approach on many problems including super-resolution, denoising, inpainting, deblurring, and 3D point cloud reconstruction.', 'score': 5, 'issue_id': 434, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 –Ω–æ—è–±—Ä—è', 'en': 'November 1', 'zh': '11Êúà1Êó•'}, 'hash': 'e85bdeb4e14858fd', 'authors': ['Vivek Jayaram', 'Ira Kemelmacher-Shlizerman', 'Steven M. Seitz', 'John Thickstun'], 'affiliations': ['University of Washington', 'Cornell University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00359.jpg', 'data': {'categories': ['#diffusion', '#cv', '#inference', '#optimization', '#3d', '#architecture'], 'emoji': 'üî¨', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω—ã—Ö –∑–∞–¥–∞—á —Å –ø–æ–º–æ—â—å—é –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞—à—É–º–ª–µ–Ω–Ω—ã—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö –æ–±—Ä–∞—Ç–Ω—ã—Ö –∑–∞–¥–∞—á —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç constrained diffusion implicit models (CDIM), –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É—é—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞ –∫–æ–Ω–µ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. CDIM —Ç–æ—á–Ω–æ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º –¥–ª—è –∑–∞–¥–∞—á –±–µ–∑ —à—É–º–∞, –∞ –¥–ª—è –∑–∞—à—É–º–ª–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –æ–±–æ–±—â–∞–µ—Ç—Å—è –¥–ª—è —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏—è —Ç–æ—á–Ω–æ–≥–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ —à—É–º–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å CDIM —Å —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º –≤—ã–≤–æ–¥–∞ –≤ 10-50 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —É—Å–ª–æ–≤–Ω—ã–º–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Accelerating Inverse Problem Solutions with Constrained Diffusion Models', 'desc': "This paper introduces a new algorithm called Constrained Diffusion Implicit Models (CDIM) for tackling noisy linear inverse problems. CDIM builds on the existing Denoising Diffusion Implicit Models (DDIM) by incorporating constraints into the diffusion process, ensuring that the final output adheres to specific requirements. In scenarios without noise, CDIM perfectly meets these constraints, while in noisy situations, it adapts to maintain an exact constraint on the noise's residual distribution. The results demonstrate that CDIM not only performs well across various tasks like super-resolution and denoising but also accelerates inference significantly, achieving speeds 10 to 50 times faster than traditional methods."}, 'zh': {'title': 'È´òÊïàËß£ÂÜ≥Â∏¶Âô™Â£∞Á∫øÊÄßÈÄÜÈóÆÈ¢òÁöÑÁ∫¶ÊùüÊâ©Êï£Ê®°Âûã', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈ´òÊïàÁöÑÁÆóÊ≥ïÔºåÁî®‰∫éËß£ÂÜ≥Â∏¶Âô™Â£∞ÁöÑÁ∫øÊÄßÈÄÜÈóÆÈ¢òÔºåÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑÊâ©Êï£Ê®°Âûã„ÄÇÊàë‰ª¨Êâ©Â±ï‰∫ÜÂéªÂô™Êâ©Êï£ÈöêÂºèÊ®°ÂûãÔºàDDIMÔºâÁöÑËåÉÂºèÔºåÊèêÂá∫‰∫ÜÁ∫¶ÊùüÊâ©Êï£ÈöêÂºèÊ®°ÂûãÔºàCDIMÔºâÔºåÈÄöËøá‰øÆÊîπÊâ©Êï£Êõ¥Êñ∞Êù•Âº∫Âà∂ÊúÄÁªàËæìÂá∫Êª°Ë∂≥Á∫¶ÊùüÊù°‰ª∂„ÄÇÂú®Êó†Âô™Â£∞ÁöÑÈÄÜÈóÆÈ¢ò‰∏≠ÔºåCDIMËÉΩÂ§üÁ≤æÁ°ÆÊª°Ë∂≥Á∫¶ÊùüÔºõËÄåÂú®ÊúâÂô™Â£∞ÁöÑÊÉÖÂÜµ‰∏ãÔºåÊàë‰ª¨Â∞ÜCDIMÊé®ÂπøÂà∞Êª°Ë∂≥Âô™Â£∞ÊÆãÂ∑ÆÂàÜÂ∏ÉÁöÑÁ≤æÁ°ÆÁ∫¶Êùü„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCDIMÂú®Â§öÁßç‰ªªÂä°ÂíåÊåáÊ†á‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂÖ∂Êé®ÁêÜÈÄüÂ∫¶ÊØî‰πãÂâçÁöÑÊù°‰ª∂Êâ©Êï£ÊñπÊ≥ïÂø´10Âà∞50ÂÄçÔºåÂ±ïÁ§∫‰∫ÜÊàë‰ª¨ÊñπÊ≥ïÂú®Ë∂ÖÂàÜËæ®Áéá„ÄÅÂéªÂô™„ÄÅ‰øÆÂ§ç„ÄÅÂéªÊ®°Á≥äÂíå3DÁÇπ‰∫ëÈáçÂª∫Á≠âÈóÆÈ¢ò‰∏äÁöÑÂ§öÊ†∑ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.00492', 'title': 'Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models', 'url': 'https://huggingface.co/papers/2411.00492', 'abstract': 'We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple experts, aggregating their responses, and selecting the best among individual and aggregated responses. This process is performed in a single chain of thoughts through our seven carefully designed subtasks derived from the Nominal Group Technique (Ven and Delbecq, 1974), a well-established decision-making framework. Our evaluations demonstrate that Multi-expert Prompting significantly outperforms ExpertPrompting and comparable baselines in enhancing the truthfulness, factuality, informativeness, and usefulness of responses while reducing toxicity and hurtfulness. It further achieves state-of-the-art truthfulness by outperforming the best baseline by 8.69% with ChatGPT. Multi-expert Prompting is efficient, explainable, and highly adaptable to diverse scenarios, eliminating the need for manual prompt construction.', 'score': 5, 'issue_id': 433, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 –Ω–æ—è–±—Ä—è', 'en': 'November 1', 'zh': '11Êúà1Êó•'}, 'hash': 'd4857ddaa86c9914', 'authors': ['Do Xuan Long', 'Duong Ngoc Yen', 'Anh Tuan Luu', 'Kenji Kawaguchi', 'Min-Yen Kan', 'Nancy F. Chen'], 'affiliations': ['National University of Singapore', 'Institute for Infocomm Research (I2R), A*STAR', 'Nanyang Technological University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00492.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#interpretability', '#architecture', '#alignment'], 'emoji': 'üß†', 'ru': {'title': '–ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑—É–º —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –ò–ò', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Multi-expert Prompting, —É–ª—É—á—à–∞—é—â–∏–π –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —Å–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö –æ—Ç–≤–µ—Ç—ã –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –ü—Ä–æ—Ü–µ—Å—Å —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –≤ –≤–∏–¥–µ –µ–¥–∏–Ω–æ–π —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–º–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –ø–æ–¥–∑–∞–¥–∞—á, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –º–µ—Ç–æ–¥–∏–∫–µ Nominal Group Technique. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Multi-expert Prompting –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ —Ä—è–¥—É –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤, –≤–∫–ª—é—á–∞—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤.'}, 'en': {'title': 'Harnessing Collective Expertise for Superior Language Model Responses', 'desc': 'Multi-expert Prompting is an advanced technique that enhances the performance of large language models (LLMs) by simulating the input of multiple experts. It aggregates responses from these simulated experts and selects the best one, ensuring that the output is more accurate and informative. This method is structured around seven subtasks inspired by the Nominal Group Technique, which aids in effective decision-making. Evaluations show that this approach significantly improves the truthfulness and usefulness of LLM responses while minimizing negative outputs like toxicity.'}, 'zh': {'title': 'Â§ö‰∏ìÂÆ∂ÊèêÁ§∫ÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÁöÑÂÖ®Êñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ¢ûÂº∫ÊñπÊ≥ïÔºåÁß∞‰∏∫Â§ö‰∏ìÂÆ∂ÊèêÁ§∫ÔºàMulti-expert PromptingÔºâÔºåÊó®Âú®ÊîπÂñÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÁîüÊàêÊïàÊûú„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊ®°ÊãüÂ§ö‰∏™‰∏ìÂÆ∂Êù•ÊåáÂØºLLMÊâßË°åËæìÂÖ•Êåá‰ª§ÔºåËÅöÂêàÂêÑ‰∏™‰∏ìÂÆ∂ÁöÑÂìçÂ∫îÔºåÂπ∂ÈÄâÊã©ÊúÄ‰Ω≥ÁöÑÂçï‰∏™ÂíåËÅöÂêàÂìçÂ∫î„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏É‰∏™Â≠ê‰ªªÂä°ÔºåÂü∫‰∫éÊàêÁÜüÁöÑÂÜ≥Á≠ñÊ°ÜÊû∂‚Äî‚ÄîÂêç‰πâÂ∞èÁªÑÊäÄÊúØÔºàNominal Group TechniqueÔºâÔºå‰ª•ÂÆûÁé∞Ëøô‰∏ÄËøáÁ®ã„ÄÇËØÑ‰º∞ÁªìÊûúË°®ÊòéÔºåÂ§ö‰∏ìÂÆ∂ÊèêÁ§∫Âú®ÊèêÈ´òÂìçÂ∫îÁöÑÁúüÂÆûÊÄß„ÄÅ‰∫ãÂÆûÊÄß„ÄÅ‰ø°ÊÅØÈáèÂíåÂÆûÁî®ÊÄßÊñπÈù¢ÊòæËëó‰ºò‰∫é‰∏ìÂÆ∂ÊèêÁ§∫ÔºàExpertPromptingÔºâÂèäÂÖ∂‰ªñÂü∫Á∫øÔºåÂêåÊó∂ÂáèÂ∞ë‰∫ÜÊúâÂÆ≥ÊÄßÂíåÊîªÂáªÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.01106', 'title': 'LoRA-Contextualizing Adaptation of Large Multimodal Models for Long Document Understanding', 'url': 'https://huggingface.co/papers/2411.01106', 'abstract': 'Large multimodal models (LMMs) have recently shown great progress in text-rich image understanding, yet they still struggle with complex, multi-page, visually-rich documents. Traditional methods using document parsers for retrieval-augmented generation suffer from performance and efficiency limitations, while directly presenting all pages to LMMs leads to inefficiencies, especially with lengthy documents. In this work, we present a novel framework named LoRA-Contextualizing Adaptation of Large multimodal models (LoCAL), which broadens the capabilities of any LMM to support long-document understanding. We demonstrate that LMMs can effectively serve as multimodal retrievers, fetching relevant pages to answer user questions based on these pages. LoCAL is implemented with two specific LMM adapters: one for evidence page retrieval and another for question answering. Empirical results show state-of-the-art performance on public benchmarks, demonstrating the effectiveness of LoCAL.', 'score': 4, 'issue_id': 433, 'pub_date': '2024-11-02', 'pub_date_card': {'ru': '2 –Ω–æ—è–±—Ä—è', 'en': 'November 2', 'zh': '11Êúà2Êó•'}, 'hash': '9ac41ff2238a6f8d', 'authors': ['Jian Chen', 'Ruiyi Zhang', 'Yufan Zhou', 'Tong Yu', 'Franck Dernoncourt', 'Jiuxiang Gu', 'Ryan A. Rossi', 'Changyou Chen', 'Tong Sun'], 'affiliations': ['University at Buffalo', 'Adobe Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01106.jpg', 'data': {'categories': ['#science', '#rag', '#benchmark', '#multimodal', '#architecture', '#long_context'], 'emoji': 'üìÑ', 'ru': {'title': 'LoCAL: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é LMM', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ LoCAL –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–æ–ª—å—à–∏–º–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LMM). LoCAL –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ –∞–¥–∞–ø—Ç–µ—Ä–∞ LMM: –æ–¥–∏–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü, –¥—Ä—É–≥–æ–π –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –º–Ω–æ–≥–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å LoCAL –Ω–∞ –ø—É–±–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.'}, 'en': {'title': 'Enhancing Long-Document Understanding with LoCAL', 'desc': 'This paper introduces LoCAL, a new framework designed to enhance large multimodal models (LMMs) for understanding long and complex documents. Traditional methods struggle with efficiency when processing multiple pages, but LoCAL allows LMMs to act as multimodal retrievers, selecting relevant pages to answer questions. The framework includes two specialized adapters: one for retrieving evidence pages and another for answering questions based on those pages. Empirical results indicate that LoCAL achieves state-of-the-art performance on public benchmarks, showcasing its effectiveness in long-document comprehension.'}, 'zh': {'title': 'ÊèêÂçáÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÈïøÊñáÊ°£ÁêÜËß£ËÉΩÂäõ', 'desc': 'Â§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMsÔºâÂú®ÁêÜËß£ÊñáÊú¨‰∏∞ÂØåÁöÑÂõæÂÉèÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂú®Â§ÑÁêÜÂ§çÊùÇÁöÑÂ§öÈ°µËßÜËßâÊñáÊ°£Êó∂‰ªçÁÑ∂Èù¢‰∏¥ÊåëÊàò„ÄÇ‰º†ÁªüÁöÑÊñáÊ°£Ëß£ÊûêÊñπÊ≥ïÂú®Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê‰∏≠Â≠òÂú®ÊÄßËÉΩÂíåÊïàÁéáÁöÑÈôêÂà∂ÔºåËÄåÁõ¥Êé•Â∞ÜÊâÄÊúâÈ°µÈù¢ÂëàÁé∞ÁªôLMMsÂàôÂØºËá¥ÊïàÁéá‰Ωé‰∏ãÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜËæÉÈïøÊñáÊ°£Êó∂„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞Ê°ÜÊû∂ÔºåÁß∞‰∏∫LoRA-‰∏ä‰∏ãÊñáÈÄÇÂ∫îÁöÑÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLoCALÔºâÔºåÂÆÉÊâ©Â±ï‰∫Ü‰ªª‰ΩïLMMÊîØÊåÅÈïøÊñáÊ°£ÁêÜËß£ÁöÑËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLoCALÂú®ÂÖ¨ÂÖ±Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2411.01192', 'title': 'Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks', 'url': 'https://huggingface.co/papers/2411.01192', 'abstract': 'We introduce Swan, a family of embedding models centred around the Arabic language, addressing both small-scale and large-scale use cases. Swan includes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on ArMistral, a pretrained Arabic large language model. To evaluate these models, we propose ArabicMTEB, a comprehensive benchmark suite that assesses cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance, covering eight diverse tasks and spanning 94 datasets. Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks, while the Swan-Small consistently surpasses Multilingual-E5 base. Our extensive evaluations demonstrate that Swan models are both dialectally and culturally aware, excelling across various Arabic domains while offering significant monetary efficiency. This work significantly advances the field of Arabic language modelling and provides valuable resources for future research and applications in Arabic natural language processing. Our models and benchmark will be made publicly accessible for research.', 'score': 3, 'issue_id': 427, 'pub_date': '2024-11-02', 'pub_date_card': {'ru': '2 –Ω–æ—è–±—Ä—è', 'en': 'November 2', 'zh': '11Êúà2Êó•'}, 'hash': 'df042a726644eac5', 'authors': ['Gagan Bhatia', 'El Moatez Billah Nagoudi', 'Abdellah El Mekki', 'Fakhraddin Alwajih', 'Muhammad Abdul-Mageed'], 'affiliations': ['MBZUAI', 'The University of British Columbia', 'Invertible AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01192.jpg', 'data': {'categories': ['#small_models', '#benchmark', '#multilingual', '#dataset', '#open_source', '#low_resource', '#architecture'], 'emoji': 'ü¶¢', 'ru': {'title': 'Swan: –ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—Ä–∞–±—Å–∫–æ–≥–æ —è–∑—ã–∫–∞', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –º–æ–¥–µ–ª–∏ —Å–µ–º–µ–π—Å—Ç–≤–∞ Swan –¥–ª—è –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –∞—Ä–∞–±—Å–∫–æ–º —è–∑—ã–∫–µ. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞: Swan-Small –Ω–∞ –æ—Å–Ω–æ–≤–µ ARBERTv2 –∏ Swan-Large –Ω–∞ –±–∞–∑–µ ArMistral. –î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–∑–¥–∞–Ω –±–µ–Ω—á–º–∞—Ä–∫ ArabicMTEB, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π 8 –∑–∞–¥–∞—á –∏ 94 –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. Swan-Large –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Multilingual-E5-large –ø–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤—É –∞—Ä–∞–±—Å–∫–∏—Ö –∑–∞–¥–∞—á, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –¥–∏–∞–ª–µ–∫—Ç–Ω—É—é –∏ –∫—É–ª—å—Ç—É—Ä–Ω—É—é –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'Swan: Advancing Arabic Language Embeddings for Diverse Applications', 'desc': 'This paper presents Swan, a new family of embedding models specifically designed for the Arabic language, which includes two versions: Swan-Small and Swan-Large. Swan-Small is based on ARBERTv2, while Swan-Large utilizes the ArMistral model, a large pretrained Arabic language model. The authors introduce ArabicMTEB, a benchmark suite that evaluates the performance of these models across various Arabic text tasks, ensuring they are effective in different dialects and cultural contexts. The results show that Swan-Large outperforms existing models in most Arabic tasks, highlighting its efficiency and effectiveness in Arabic natural language processing.'}, 'zh': {'title': 'SwanÔºöÈòøÊãâ‰ºØËØ≠ÂµåÂÖ•Ê®°ÂûãÁöÑÂàõÊñ∞‰πãË∑Ø', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜSwanÔºå‰∏Ä‰∏™‰ª•ÈòøÊãâ‰ºØËØ≠‰∏∫‰∏≠ÂøÉÁöÑÂµåÂÖ•Ê®°ÂûãÂÆ∂ÊóèÔºåÈÄÇÁî®‰∫éÂ∞èËßÑÊ®°ÂíåÂ§ßËßÑÊ®°ÁöÑÂ∫îÁî®Âú∫ÊôØ„ÄÇSwanÂåÖÊã¨‰∏§‰∏™Âèò‰ΩìÔºöÂü∫‰∫éARBERTv2ÁöÑSwan-SmallÂíåÂü∫‰∫éÈ¢ÑËÆ≠ÁªÉÈòøÊãâ‰ºØÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãArMistralÁöÑSwan-Large„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜArabicMTEBÔºå‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂü∫ÂáÜÂ•ó‰ª∂ÔºåÁî®‰∫éËØÑ‰º∞ÈòøÊãâ‰ºØÊñáÊú¨ÂµåÂÖ•Âú®Ë∑®ËØ≠Ë®Ä„ÄÅÂ§öÊñπË®Ä„ÄÅÂ§öÈ¢ÜÂüüÂíåÂ§öÊñáÂåñÊñπÈù¢ÁöÑË°®Áé∞ÔºåÊ∂µÁõñÂÖ´‰∏™‰∏çÂêå‰ªªÂä°Âíå94‰∏™Êï∞ÊçÆÈõÜ„ÄÇSwan-LargeÂú®Â§ßÂ§öÊï∞ÈòøÊãâ‰ºØ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜMultilingual-E5-largeÔºåËÄåSwan-Small‰πüÂßãÁªà‰ºò‰∫éMultilingual-E5 base„ÄÇ'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (4)', '#agents (3)', '#agi (1)', '#alignment (4)', '#architecture (18)', '#audio', '#benchmark (15)', '#cv (8)', '#data (2)', '#dataset (10)', '#diffusion (5)', '#ethics (2)', '#games (3)', '#graphs (2)', '#hallucinations', '#healthcare', '#inference (3)', '#interpretability (3)', '#leakage', '#long_context (3)', '#low_resource (1)', '#machine_translation', '#math (2)', '#multilingual (2)', '#multimodal (5)', '#open_source (11)', '#optimization (12)', '#plp (2)', '#rag (1)', '#reasoning (6)', '#rl (2)', '#rlhf (2)', '#robotics (1)', '#science (3)', '#security', '#small_models (3)', '#story_generation', '#survey (1)', '#synthetic (4)', '#training (12)', '#transfer_learning (1)', '#video (6)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">üìù ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <img class="article-pdf-title-img" src="https://hfday.ru/${item['pdf_title_img']}" />
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2024-11-05 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-11-05 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-11-05 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    