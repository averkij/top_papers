
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 23 papers. November 5.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">5 ноября</span> | <span id="title-articles-count">23 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-11-04.html">⬅️ <span id="prev-date">04.11</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-11-06.html">➡️ <span id="next-date">06.11</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-11.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'};
        let feedDateNext = {'ru': '06.11', 'en': '11/06', 'zh': '11月6日'};
        let feedDatePrev = {'ru': '04.11', 'en': '11/04', 'zh': '11月4日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.24024', 'title': 'AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents', 'url': 'https://huggingface.co/papers/2410.24024', 'abstract': 'Autonomous agents have become increasingly important for interacting with the real world. Android agents, in particular, have been recently a frequently-mentioned interaction method. However, existing studies for training and evaluating Android agents lack systematic research on both open-source and closed-source models. In this work, we propose AndroidLab as a systematic Android agent framework. It includes an operation environment with different modalities, action space, and a reproducible benchmark. It supports both large language models (LLMs) and multimodal models (LMMs) in the same action space. AndroidLab benchmark includes predefined Android virtual devices and 138 tasks across nine apps built on these devices. By using the AndroidLab environment, we develop an Android Instruction dataset and train six open-source LLMs and LMMs, lifting the average success rates from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMs. AndroidLab is open-sourced and publicly available at https://github.com/THUDM/Android-Lab.', 'score': 48, 'issue_id': 422, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '4ba16ad433c7511f', 'authors': ['Yifan Xu', 'Xiao Liu', 'Xueqiao Sun', 'Siyi Cheng', 'Hao Yu', 'Hanyu Lai', 'Shudan Zhang', 'Dan Zhang', 'Jie Tang', 'Yuxiao Dong'], 'affiliations': ['Tsinghua University', 'Peking University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24024.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#training', '#dataset', '#open_source', '#games', '#agents'], 'emoji': '🤖', 'ru': {'title': 'AndroidLab: Революция в обучении Android-агентов', 'desc': 'Статья представляет AndroidLab - систематическую среду для разработки и оценки агентов на базе Android. Эта среда включает в себя операционное окружение с различными модальностями, пространством действий и воспроизводимым эталоном. AndroidLab поддерживает как большие языковые модели (LLM), так и мультимодальные модели (LMM) в одном пространстве действий. С помощью AndroidLab авторы разработали набор данных Android Instruction и обучили шесть моделей с открытым исходным кодом, значительно повысив их эффективность.'}, 'en': {'title': 'Empowering Android Agents with AndroidLab: A New Benchmark Framework', 'desc': 'This paper introduces AndroidLab, a comprehensive framework designed for training and evaluating Android agents. It provides a structured environment that includes various modalities and a defined action space, allowing for systematic testing of both open-source and closed-source models. The framework supports large language models (LLMs) and multimodal models (LMMs), facilitating their development and benchmarking across 138 tasks in nine different applications. By utilizing AndroidLab, the authors significantly improved the success rates of LLMs and LMMs, demonstrating its effectiveness in enhancing the performance of Android agents.'}, 'zh': {'title': 'AndroidLab：提升安卓代理的训练与评估', 'desc': '自主代理在与现实世界互动中变得越来越重要，尤其是安卓代理。现有的安卓代理训练和评估研究缺乏系统性，无法全面比较开源和闭源模型。我们提出了AndroidLab，这是一个系统化的安卓代理框架，支持多种操作环境和动作空间。通过AndroidLab，我们开发了安卓指令数据集，并显著提高了大语言模型和多模态模型的成功率。'}}}, {'id': 'https://huggingface.co/papers/2411.02355', 'title': '"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization', 'url': 'https://huggingface.co/papers/2411.02355', 'abstract': 'Despite the popularity of large language model (LLM) quantization for inference acceleration, significant uncertainty remains regarding the accuracy-performance trade-offs associated with various quantization formats. We present a comprehensive empirical study of quantized accuracy, evaluating popular quantization formats (FP8, INT8, INT4) across academic benchmarks and real-world tasks, on the entire Llama-3.1 model family. Additionally, our study examines the difference in text generated by quantized models versus their uncompressed counterparts. Beyond benchmarks, we also present a couple of quantization improvements which allowed us to obtain state-of-the-art accuracy recovery results. Our investigation, encompassing over 500,000 individual evaluations, yields several key findings: (1) FP8 weight and activation quantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and activation quantization (W8A8-INT), when properly tuned, incurs surprisingly low 1-3% accuracy degradation, and (3) INT4 weight-only quantization (W4A16-INT) is competitive with 8-bit integer weight and activation quantization. To address the question of the "best" format for a given deployment environment, we conduct inference performance analysis using the popular open-source vLLM framework on various GPU architectures. We find that W4A16 offers the best cost-efficiency for synchronous deployments, and for asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel in asynchronous "continuous batching" deployment of mid- and large-size models on high-end GPUs. Our results provide a set of practical guidelines for deploying quantized LLMs across scales and performance requirements.', 'score': 44, 'issue_id': 431, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '8cc3629c4f7e76a1', 'authors': ['Eldar Kurtic', 'Alexandre Marques', 'Shubhra Pandit', 'Mark Kurtz', 'Dan Alistarh'], 'affiliations': ['Neural Magic', 'Institute of Science and Technology Austria'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02355.jpg', 'data': {'categories': ['#benchmark', '#inference', '#optimization', '#training', '#open_source'], 'emoji': '🔬', 'ru': {'title': 'Оптимальное квантование LLM: баланс между точностью и производительностью', 'desc': 'Это исследование посвящено квантованию больших языковых моделей (LLM) для ускорения вывода. Авторы провели комплексный эмпирический анализ различных форматов квантования (FP8, INT8, INT4) на семействе моделей Llama-3.1, оценивая их точность на академических тестах и реальных задачах. Результаты показывают, что квантование FP8 практически безошибочно для всех размеров моделей, а INT8 при правильной настройке дает снижение точности всего на 1-3%. Исследование также включает анализ производительности на различных GPU архитектурах, предоставляя практические рекомендации по развертыванию квантованных LLM.'}, 'en': {'title': 'Optimizing Quantization for Efficient Large Language Model Deployment', 'desc': 'This paper investigates the accuracy and performance trade-offs of different quantization formats for large language models (LLMs), specifically focusing on FP8, INT8, and INT4 quantization methods. The authors conducted extensive evaluations on the Llama-3.1 model family, analyzing over 500,000 instances to determine the impact of quantization on model accuracy and text generation. Key findings indicate that FP8 quantization is lossless, while INT8 can achieve minimal accuracy loss with proper tuning, and INT4 quantization remains competitive. The study also provides practical guidelines for selecting the optimal quantization format based on deployment scenarios and GPU architectures.'}, 'zh': {'title': '量化模型的最佳选择与性能优化', 'desc': '本文研究了大语言模型（LLM）量化对推理加速的影响，特别关注不同量化格式（如FP8、INT8、INT4）在准确性和性能之间的权衡。我们对Llama-3.1模型系列进行了全面的实证研究，评估了量化模型在学术基准和实际任务中的表现。研究发现，FP8量化在所有模型规模上都是无损的，而经过适当调优的INT8量化仅有1-3%的准确性下降。此外，我们还提出了一些量化改进方法，帮助实现了最先进的准确性恢复结果。'}}}, {'id': 'https://huggingface.co/papers/2411.02337', 'title': 'WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning', 'url': 'https://huggingface.co/papers/2411.02337', 'abstract': "Large language models (LLMs) have shown remarkable potential as autonomous agents, particularly in web-based tasks. However, existing LLM web agents heavily rely on expensive proprietary LLM APIs, while open LLMs lack the necessary decision-making capabilities. This paper introduces WebRL, a self-evolving online curriculum reinforcement learning framework designed to train high-performance web agents using open LLMs. WebRL addresses three key challenges in building LLM web agents, including the scarcity of training tasks, sparse feedback signals, and policy distribution drift in online learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4 models into proficient web agents. On WebArena-Lite, WebRL improves the success rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B. These open models significantly surpass the performance of GPT-4-Turbo (17.6%) and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's effectiveness in bridging the gap between open and proprietary LLM-based web agents, paving the way for more accessible and powerful autonomous web interaction systems.", 'score': 36, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': 'a9af7d20e52c1f39', 'authors': ['Zehan Qi', 'Xiao Liu', 'Iat Long Iong', 'Hanyu Lai', 'Xueqiao Sun', 'Xinyue Yang', 'Jiadai Sun', 'Yu Yang', 'Shuntian Yao', 'Tianjie Zhang', 'Wei Xu', 'Jie Tang', 'Yuxiao Dong'], 'affiliations': ['Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02337.jpg', 'data': {'categories': ['#small_models', '#reasoning', '#rl', '#benchmark', '#optimization', '#training', '#open_source', '#agents', '#architecture'], 'emoji': '🕸️', 'ru': {'title': 'WebRL: Открытые языковые модели превосходят проприетарные в веб-задачах', 'desc': 'Статья представляет WebRL - фреймворк обучения с подкреплением для создания веб-агентов на основе открытых языковых моделей. WebRL решает проблемы нехватки обучающих задач, разреженной обратной связи и смещения распределения политики в онлайн-обучении. Фреймворк включает самоэволюционирующий учебный план, модель вознаграждения на основе результатов и адаптивные стратегии обучения с подкреплением. Применение WebRL значительно улучшило производительность открытых моделей Llama-3.1 и GLM-4 в задачах веб-взаимодействия, превзойдя даже проприетарные модели GPT-4.'}, 'en': {'title': 'Empowering Open LLMs for Superior Web Performance with WebRL', 'desc': 'This paper presents WebRL, a novel framework that enhances open large language models (LLMs) for web-based tasks through reinforcement learning. It tackles challenges such as limited training tasks, sparse feedback, and policy drift by implementing a self-evolving curriculum that generates new tasks from failures, a robust outcome-supervised reward model, and adaptive learning strategies. The framework significantly improves the performance of open models like Llama-3.1 and GLM-4, achieving success rates that surpass proprietary models like GPT-4-Turbo. Overall, WebRL demonstrates a promising approach to making powerful web agents more accessible by leveraging open-source LLMs.'}, 'zh': {'title': 'WebRL：开放LLM的自我进化网络代理训练框架', 'desc': '大型语言模型（LLMs）在网络任务中展现了出色的潜力，但现有的LLM网络代理依赖昂贵的专有API，而开放的LLM缺乏决策能力。本文提出了WebRL，一个自我进化的在线课程强化学习框架，旨在利用开放的LLM训练高性能的网络代理。WebRL解决了构建LLM网络代理的三个关键挑战，包括训练任务稀缺、反馈信号稀疏和在线学习中的策略分布漂移。通过WebRL，我们将开放的Llama-3.1和GLM-4模型转变为高效的网络代理，显著提高了它们的成功率，超越了现有的专有模型。'}}}, {'id': 'https://huggingface.co/papers/2411.02385', 'title': 'How Far is Video Generation from World Model: A Physical Law Perspective', 'url': 'https://huggingface.co/papers/2411.02385', 'abstract': 'OpenAI\'s Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization. We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws. This provides an unlimited supply of data for large-scale experimentation and enables quantitative evaluation of whether the generated videos adhere to physical laws. We trained diffusion-based video generation models to predict object movements based on initial frames. Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios. Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit "case-based" generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color > size > velocity > shape. Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora\'s broader success. See our project page at https://phyworld.github.io', 'score': 32, 'issue_id': 421, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '771395452deb397f', 'authors': ['Bingyi Kang', 'Yang Yue', 'Rui Lu', 'Zhijie Lin', 'Yang Zhao', 'Kaixin Wang', 'Gao Huang', 'Jiashi Feng'], 'affiliations': ['Bytedance Research', 'Tsinghua University', 'Technion'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02385.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#benchmark', '#cv', '#video', '#training', '#open_source', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Генеративные видеомодели не раскрывают физические законы при масштабировании', 'desc': 'Исследование оценивает способность моделей генерации видео изучать фундаментальные физические законы из визуальных данных. Авторы разработали 2D симуляцию для создания видео, управляемых законами классической механики. Эксперименты показали, что модели достигают идеальной генерализации в рамках распределения, но не справляются с экстраполяцией на новые сценарии. Результаты указывают на то, что модели не абстрагируют общие физические правила, а скорее демонстрируют обобщение на основе конкретных примеров.'}, 'en': {'title': 'Unlocking Video Generation: Beyond Scaling to Understand Physics', 'desc': 'This paper investigates the ability of video generation models to learn and predict physical laws from visual data without human input. The authors created a 2D simulation environment to generate videos based on classical mechanics, allowing for extensive testing of model performance. They found that while the models excelled in familiar scenarios, they struggled with new, unseen situations, indicating a reliance on specific examples rather than generalizing physical principles. The study concludes that simply increasing model size is not enough for these systems to grasp fundamental laws of physics, highlighting the need for improved generalization techniques.'}, 'zh': {'title': '视频生成模型与物理法则的探索', 'desc': '本文探讨了视频生成模型在学习物理法则方面的潜力。我们开发了一个二维模拟测试平台，用于生成受经典力学法则支配的视频数据。通过对模型在不同场景下的表现进行评估，我们发现模型在已知分布内表现良好，但在未知分布中则出现失败。研究表明，模型在推广新案例时，优先考虑的因素依次为颜色、大小、速度和形状，而不是抽象出一般的物理规则。'}}}, {'id': 'https://huggingface.co/papers/2411.02336', 'title': 'MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D', 'url': 'https://huggingface.co/papers/2411.02336', 'abstract': 'Texturing is a crucial step in the 3D asset production workflow, which enhances the visual appeal and diversity of 3D assets. Despite recent advancements in Text-to-Texture (T2T) generation, existing methods often yield subpar results, primarily due to local discontinuities, inconsistencies across multiple views, and their heavy dependence on UV unwrapping outcomes. To tackle these challenges, we propose a novel generation-refinement 3D texturing framework called MVPaint, which can generate high-resolution, seamless textures while emphasizing multi-view consistency. MVPaint mainly consists of three key modules. 1) Synchronized Multi-view Generation (SMG). Given a 3D mesh model, MVPaint first simultaneously generates multi-view images by employing an SMG model, which leads to coarse texturing results with unpainted parts due to missing observations. 2) Spatial-aware 3D Inpainting (S3I). To ensure complete 3D texturing, we introduce the S3I method, specifically designed to effectively texture previously unobserved areas. 3) UV Refinement (UVR). Furthermore, MVPaint employs a UVR module to improve the texture quality in the UV space, which first performs a UV-space Super-Resolution, followed by a Spatial-aware Seam-Smoothing algorithm for revising spatial texturing discontinuities caused by UV unwrapping. Moreover, we establish two T2T evaluation benchmarks: the Objaverse T2T benchmark and the GSO T2T benchmark, based on selected high-quality 3D meshes from the Objaverse dataset and the entire GSO dataset, respectively. Extensive experimental results demonstrate that MVPaint surpasses existing state-of-the-art methods. Notably, MVPaint could generate high-fidelity textures with minimal Janus issues and highly enhanced cross-view consistency.', 'score': 23, 'issue_id': 432, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '0f7de0fb0e5cdd86', 'authors': ['Wei Cheng', 'Juncheng Mu', 'Xianfang Zeng', 'Xin Chen', 'Anqi Pang', 'Chi Zhang', 'Zhibin Wang', 'Bin Fu', 'Gang Yu', 'Ziwei Liu', 'Liang Pan'], 'affiliations': ['Tencent PCG', 'Shanghai AI Laboratory', 'S-Lab, NTU', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02336.jpg', 'data': {'categories': ['#benchmark', '#cv', '#optimization', '#dataset', '#games', '#3d'], 'emoji': '🎨', 'ru': {'title': 'MVPaint: революция в автоматическом текстурировании 3D-моделей', 'desc': 'В статье представлена новая система MVPaint для генерации высококачественных текстур для 3D-моделей. Система состоит из трех ключевых модулей: синхронизированной многоракурсной генерации, пространственно-ориентированного 3D-инпейнтинга и уточнения UV-развертки. MVPaint решает проблемы локальных разрывов, несогласованности между ракурсами и зависимости от UV-развертки, характерные для существующих методов текстурирования. Авторы также создали два новых бенчмарка для оценки качества генерации текстур по тексту.'}, 'en': {'title': 'MVPaint: Revolutionizing 3D Texturing with Multi-View Consistency', 'desc': 'This paper introduces MVPaint, a new framework for generating high-quality textures for 3D models. It addresses common issues in Text-to-Texture (T2T) generation, such as local discontinuities and inconsistencies across different views. MVPaint consists of three main components: Synchronized Multi-view Generation for initial texture creation, Spatial-aware 3D Inpainting for filling in unpainted areas, and UV Refinement for enhancing texture quality in UV space. The framework outperforms existing methods, providing seamless textures with improved visual consistency across multiple perspectives.'}, 'zh': {'title': 'MVPaint：提升3D纹理生成的一体化解决方案', 'desc': '本文提出了一种新的3D纹理生成与优化框架MVPaint，旨在解决现有文本到纹理生成方法中的局部不连续性和多视图一致性问题。MVPaint包含三个主要模块：同步多视图生成（SMG）、空间感知3D修补（S3I）和UV优化（UVR），能够生成高分辨率、无缝的纹理。通过SMG模块，MVPaint可以同时生成多视图图像，而S3I模块则专注于填补未观察到的区域。最后，UVR模块通过超分辨率和缝合平滑算法来提升UV空间中的纹理质量，实验结果表明MVPaint在纹理生成方面优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2411.00860', 'title': 'Survey of Cultural Awareness in Language Models: Text and Beyond', 'url': 'https://huggingface.co/papers/2411.00860', 'abstract': 'Large-scale deployment of large language models (LLMs) in various applications, such as chatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure inclusivity. Culture has been widely studied in psychology and anthropology, and there has been a recent surge in research on making LLMs more culturally inclusive in LLMs that goes beyond multilinguality and builds on findings from psychology and anthropology. In this paper, we survey efforts towards incorporating cultural awareness into text-based and multimodal LLMs. We start by defining cultural awareness in LLMs, taking the definitions of culture from anthropology and psychology as a point of departure. We then examine methodologies adopted for creating cross-cultural datasets, strategies for cultural inclusion in downstream tasks, and methodologies that have been used for benchmarking cultural awareness in LLMs. Further, we discuss the ethical implications of cultural alignment, the role of Human-Computer Interaction in driving cultural inclusion in LLMs, and the role of cultural alignment in driving social science research. We finally provide pointers to future research based on our findings about gaps in the literature.', 'score': 23, 'issue_id': 425, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '8e4a6348db0215e3', 'authors': ['Siddhesh Pawar', 'Junyeong Park', 'Jiho Jin', 'Arnav Arora', 'Junho Myung', 'Srishti Yadav', 'Faiz Ghifari Haznitrama', 'Inhwa Song', 'Alice Oh', 'Isabelle Augenstein'], 'affiliations': ['University of Copenhagen, Denmark', 'KAIST, Republic of Korea', 'KAIST, Republic of Korea', 'KAIST, Republic of Korea', 'University of Copenhagen, Denmark', 'University of Copenhagen, Denmark', 'KAIST, Republic of Korea', 'KAIST, Republic of Korea', 'KAIST, Republic of Korea', 'University of Copenhagen, Denmark'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00860.jpg', 'data': {'categories': ['#science', '#benchmark', '#multilingual', '#multimodal', '#ethics', '#training', '#dataset', '#survey', '#architecture', '#alignment'], 'emoji': '🌍', 'ru': {'title': 'Культурная инклюзивность в эпоху больших языковых моделей', 'desc': 'Эта статья посвящена внедрению культурной осведомленности в крупномасштабные языковые модели (LLM). Авторы рассматривают методологии создания кросс-культурных датасетов и стратегии включения культурных аспектов в задачи обработки естественного языка. Обсуждаются этические последствия культурной адаптации LLM и роль взаимодействия человека с компьютером в этом процессе. Статья также предлагает направления для будущих исследований в области культурной осведомленности искусственного интеллекта.'}, 'en': {'title': 'Enhancing Cultural Sensitivity in Language Models', 'desc': 'This paper explores the importance of cultural sensitivity in large language models (LLMs) used in applications like chatbots. It reviews existing research on how to make LLMs more inclusive by integrating insights from psychology and anthropology. The authors define cultural awareness in LLMs and discuss methods for creating diverse datasets and evaluating cultural inclusivity. Additionally, they highlight ethical considerations and suggest future research directions to enhance cultural alignment in LLMs.'}, 'zh': {'title': '让大型语言模型更具文化敏感性', 'desc': '本论文探讨了在大型语言模型（LLMs）中融入文化敏感性的重要性，以确保用户的包容性。我们首先定义了LLMs中的文化意识，并基于人类学和心理学的定义进行讨论。接着，我们分析了创建跨文化数据集的方法、在下游任务中实现文化包容的策略，以及用于评估LLMs文化意识的方法论。最后，我们讨论了文化对齐的伦理影响、人机交互在推动文化包容中的作用，以及未来研究的方向。'}}}, {'id': 'https://huggingface.co/papers/2411.02395', 'title': 'Training-free Regional Prompting for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.02395', 'abstract': 'Diffusion models have demonstrated excellent capabilities in text-to-image generation. Their semantic understanding (i.e., prompt following) ability has also been greatly improved with large language models (e.g., T5, Llama). However, existing models cannot perfectly handle long and complex text prompts, especially when the text prompts contain various objects with numerous attributes and interrelated spatial relationships. While many regional prompting methods have been proposed for UNet-based models (SD1.5, SDXL), but there are still no implementations based on the recent Diffusion Transformer (DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and implement regional prompting for FLUX.1 based on attention manipulation, which enables DiT with fined-grained compositional text-to-image generation capability in a training-free manner. Code is available at https://github.com/antonioo-c/Regional-Prompting-FLUX.', 'score': 23, 'issue_id': 423, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '2a0401bfd2cb136b', 'authors': ['Anthony Chen', 'Jianjin Xu', 'Wenzhao Zheng', 'Gaole Dai', 'Yida Wang', 'Renrui Zhang', 'Haofan Wang', 'Shanghang Zhang'], 'affiliations': ['Peking University', 'InstantX Team', 'Carnegie Mellon University', 'UC Berkeley', 'Li Auto Inc.', 'CUHK'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02395.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#open_source', '#cv'], 'emoji': '🎨', 'ru': {'title': 'Точная генерация изображений по сложным текстам: новый метод для DiT', 'desc': 'Статья представляет новый метод регионального промптинга для архитектуры Diffusion Transformer (DiT), применимый к моделям FLUX.1. Этот подход позволяет улучшить генерацию изображений по сложным текстовым описаниям без дополнительного обучения модели. Авторы реализовали метод манипуляции вниманием, что позволяет DiT более точно следовать детальным композиционным промптам. Работа направлена на преодоление ограничений существующих моделей в обработке длинных текстовых описаний с множеством объектов и пространственных отношений.'}, 'en': {'title': 'Enhancing Text-to-Image Generation with Regional Prompting in Diffusion Transformers', 'desc': 'This paper discusses the limitations of current diffusion models in generating images from long and complex text prompts. It highlights the challenges these models face when dealing with multiple objects and their attributes in a spatial context. The authors propose a new method called regional prompting for the Diffusion Transformer (DiT) architecture, specifically for the FLUX.1 model, which enhances its ability to generate images based on detailed text descriptions without requiring additional training. This approach utilizes attention manipulation to achieve fine-grained compositional generation.'}, 'zh': {'title': '区域提示提升扩散模型的文本到图像生成能力', 'desc': '扩散模型在文本到图像生成方面表现出色，尤其是在理解语义方面得到了很大提升。尽管已有许多区域提示方法被提出，但现有模型仍无法完美处理长且复杂的文本提示。本文提出了一种基于注意力操作的区域提示方法，专门针对FLUX.1架构进行实现。该方法使得扩散变换器能够在无需训练的情况下，实现细粒度的组合文本到图像生成能力。'}}}, {'id': 'https://huggingface.co/papers/2411.02265', 'title': 'Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent', 'url': 'https://huggingface.co/papers/2411.02265', 'abstract': "In this paper, we introduce Hunyuan-Large, which is currently the largest open-source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior performance across various benchmarks including language understanding and generation, logical reasoning, mathematical problem-solving, coding, long-context, and aggregated tasks, where it outperforms LLama3.1-70B and exhibits comparable performance when compared to the significantly larger LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale synthetic data that is orders larger than in previous literature, a mixed expert routing strategy, a key-value cache compression technique, and an expert-specific learning rate strategy. Additionally, we also investigate the scaling laws and learning rate schedule of mixture of experts models, providing valuable insights and guidances for future model development and optimization. The code and checkpoints of Hunyuan-Large are released to facilitate future innovations and applications.   Codes: https://github.com/Tencent/Hunyuan-Large   Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large", 'score': 23, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '775afc5ff4d7fbdf', 'authors': ['Xingwu Sun', 'Yanfeng Chen', 'Yiqing Huang', 'Ruobing Xie', 'Jiaqi Zhu', 'Kai Zhang', 'Shuaipeng Li', 'Zhen Yang', 'Jonny Han', 'Xiaobo Shu', 'Jiahao Bu', 'Zhongzhi Chen', 'Xuemeng Huang', 'Fengzong Lian', 'Saiyong Yang', 'Jianfeng Yan', 'Yuyuan Zeng', 'Xiaoqin Ren', 'Chao Yu', 'Lulu Wu', 'Yue Mao', 'Jun Xia', 'Tao Yang', 'Suncong Zheng', 'Kan Wu', 'Dian Jiao', 'Jinbao Xue', 'Xipeng Zhang', 'Decheng Wu', 'Kai Liu', 'Dengpeng Wu', 'Guanghui Xu', 'Shaohua Chen', 'Shuang Chen', 'Xiao Feng', 'Yigeng Hong', 'Junqiang Zheng', 'Chengcheng Xu', 'Zongwei Li', 'Xiong Kuang', 'Jianglu Hu', 'Yiqi Chen', 'Yuchi Deng', 'Guiyang Li', 'Ao Liu', 'Chenchen Zhang', 'Shihui Hu', 'Zilong Zhao', 'Zifan Wu', 'Yao Ding', 'Weichao Wang', 'Han Liu', 'Roberts Wang', 'Hao Fei', 'Peijie Yu', 'Ze Zhao', 'Xun Cao', 'Hai Wang', 'Fusheng Xiang', 'Mengyuan Huang', 'Zhiyuan Xiong', 'Bin Hu', 'Xuebin Hou', 'Lei Jiang', 'Jianqiang Ma', 'Jiajia Wu', 'Yaping Deng', 'Yi Shen', 'Qian Wang', 'Weijie Liu', 'Jie Liu', 'Meng Chen', 'Liang Dong', 'Weiwen Jia', 'Hu Chen', 'Feifei Liu', 'Rui Yuan', 'Huilin Xu', 'Zhenxiang Yan', 'Tengfei Cao', 'Zhichao Hu', 'Xinhua Feng', 'Dong Du', 'Tinghao Yu', 'Yangyu Tao', 'Feng Zhang', 'Jianchen Zhu', 'Chengzhong Xu', 'Xirui Li', 'Chong Zha', 'Wen Ouyang', 'Yinben Xia', 'Xiang Li', 'Zekun He', 'Rongpeng Chen', 'Jiawei Song', 'Ruibin Chen', 'Fan Jiang', 'Chongqing Zhao', 'Bo Wang', 'Hao Gong', 'Rong Gan', 'Winston Hu', 'Zhanhui Kang', 'Yong Yang', 'Yuhong Liu', 'Di Wang', 'Jie Jiang'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02265.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#synthetic', '#benchmark', '#optimization', '#math', '#plp', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Hunyuan-Large: Гигантский шаг вперед в области моделей смеси экспертов', 'desc': 'Статья представляет Hunyuan-Large - крупнейшую открытую модель на основе трансформеров с архитектурой смеси экспертов, содержащую 389 миллиардов параметров. Модель демонстрирует превосходную производительность в различных задачах, включая понимание и генерацию языка, логические рассуждения и программирование. Ключевые особенности Hunyuan-Large включают использование масштабных синтетических данных и смешанную стратегию маршрутизации экспертов. Авторы также исследуют законы масштабирования и графики скорости обучения для моделей смеси экспертов.'}, 'en': {'title': 'Unlocking New Frontiers in AI with Hunyuan-Large', 'desc': 'Hunyuan-Large is a groundbreaking Transformer-based mixture of experts model with an impressive 389 billion parameters, designed to process up to 256K tokens. It demonstrates exceptional performance in various tasks such as language understanding, logical reasoning, and coding, surpassing the capabilities of LLama3.1-70B and rivaling the larger LLama3.1-405B. The model employs innovative techniques like large-scale synthetic data generation, a mixed expert routing strategy, and expert-specific learning rates to enhance its efficiency. Furthermore, the paper explores scaling laws and learning rate schedules, offering insights for future advancements in model optimization.'}, 'zh': {'title': 'Hunyuan-Large：超大规模专家混合模型的创新之路', 'desc': '本文介绍了Hunyuan-Large，这是目前最大的开源基于Transformer的专家混合模型，拥有3890亿个参数和520亿个激活参数，能够处理多达256K的token。我们对Hunyuan-Large在语言理解与生成、逻辑推理、数学问题解决、编码、长上下文和聚合任务等多个基准上的优越性能进行了全面评估，结果显示其优于LLama3.1-70B，并且在与更大模型LLama3.1-405B的比较中表现相当。Hunyuan-Large的关键实践包括大规模合成数据、混合专家路由策略、键值缓存压缩技术和专家特定学习率策略。此外，我们还研究了专家混合模型的扩展规律和学习率调度，为未来模型的开发和优化提供了宝贵的见解和指导。'}}}, {'id': 'https://huggingface.co/papers/2411.02397', 'title': 'Adaptive Caching for Faster Video Generation with Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.02397', 'abstract': 'Generating temporally-consistent high-fidelity videos can be computationally expensive, especially over longer temporal spans. More-recent Diffusion Transformers (DiTs) -- despite making significant headway in this context -- have only heightened such challenges as they rely on larger models and heavier attention mechanisms, resulting in slower inference speeds. In this paper, we introduce a training-free method to accelerate video DiTs, termed Adaptive Caching (AdaCache), which is motivated by the fact that "not all videos are created equal": meaning, some videos require fewer denoising steps to attain a reasonable quality than others. Building on this, we not only cache computations through the diffusion process, but also devise a caching schedule tailored to each video generation, maximizing the quality-latency trade-off. We further introduce a Motion Regularization (MoReg) scheme to utilize video information within AdaCache, essentially controlling the compute allocation based on motion content. Altogether, our plug-and-play contributions grant significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video generation) without sacrificing the generation quality, across multiple video DiT baselines.', 'score': 20, 'issue_id': 421, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': 'd7fa22d791789900', 'authors': ['Kumara Kahatapitiya', 'Haozhe Liu', 'Sen He', 'Ding Liu', 'Menglin Jia', 'Chenyang Zhang', 'Michael S. Ryoo', 'Tian Xie'], 'affiliations': ['Meta AI', 'Stony Brook University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02397.jpg', 'data': {'categories': ['#diffusion', '#inference', '#video', '#optimization', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Адаптивное кэширование ускоряет генерацию видео без потери качества', 'desc': 'Статья представляет метод AdaCache для ускорения видео-диффузионных трансформеров (DiT) без переобучения. AdaCache адаптивно кэширует вычисления в процессе диффузии, учитывая, что разным видео требуется разное количество шагов шумоподавления. Также предложена схема регуляризации движения (MoReg) для оптимизации распределения вычислений на основе содержания движения в видео. Метод значительно ускоряет генерацию видео (до 4.7 раз на Open-Sora 720p) без потери качества.'}, 'en': {'title': 'Accelerating Video Generation with Adaptive Caching!', 'desc': 'This paper presents a method called Adaptive Caching (AdaCache) to improve the speed of video generation using Diffusion Transformers (DiTs). The authors argue that different videos have varying requirements for denoising steps, allowing for a more efficient computation process. By caching computations and creating a tailored caching schedule for each video, they enhance the balance between quality and speed. Additionally, they introduce a Motion Regularization (MoReg) technique to optimize resource allocation based on the motion content of the video, achieving significant speed improvements without compromising quality.'}, 'zh': {'title': '加速视频生成，提升质量与效率！', 'desc': '本论文提出了一种名为自适应缓存（AdaCache）的方法，用于加速视频生成中的扩散变换器（DiTs）。该方法通过缓存计算过程，针对每个视频生成制定缓存计划，从而优化质量与延迟的平衡。我们还引入了运动正则化（MoReg）方案，根据视频中的运动内容来控制计算分配。整体而言，这些创新显著提高了推理速度，同时保持了生成质量。'}}}, {'id': 'https://huggingface.co/papers/2411.02319', 'title': 'GenXD: Generating Any 3D and 4D Scenes', 'url': 'https://huggingface.co/papers/2411.02319', 'abstract': "Recent developments in 2D visual generation have been remarkably successful. However, 3D and 4D generation remain challenging in real-world applications due to the lack of large-scale 4D data and effective model design. In this paper, we propose to jointly investigate general 3D and 4D generation by leveraging camera and object movements commonly observed in daily life. Due to the lack of real-world 4D data in the community, we first propose a data curation pipeline to obtain camera poses and object motion strength from videos. Based on this pipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K. By leveraging all the 3D and 4D data, we develop our framework, GenXD, which allows us to produce any 3D or 4D scene. We propose multiview-temporal modules, which disentangle camera and object movements, to seamlessly learn from both 3D and 4D data. Additionally, GenXD employs masked latent conditions to support a variety of conditioning views. GenXD can generate videos that follow the camera trajectory as well as consistent 3D views that can be lifted into 3D representations. We perform extensive evaluations across various real-world and synthetic datasets, demonstrating GenXD's effectiveness and versatility compared to previous methods in 3D and 4D generation.", 'score': 19, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '51444eddaf6bbbe7', 'authors': ['Yuyang Zhao', 'Chung-Ching Lin', 'Kevin Lin', 'Zhiwen Yan', 'Linjie Li', 'Zhengyuan Yang', 'Jianfeng Wang', 'Gim Hee Lee', 'Lijuan Wang'], 'affiliations': ['National University of Singapore', 'Microsoft Corporation'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02319.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#cv', '#video', '#data', '#dataset', '#3d', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'GenXD: Универсальный генератор 3D и 4D сцен', 'desc': 'Исследователи представили новый подход к генерации 3D и 4D сцен под названием GenXD. Они создали большой набор данных реальных 4D сцен CamVid-30K, используя специальный конвейер для извлечения информации о движении камеры и объектов из видео. GenXD использует мультиракурсно-временные модули для разделения движений камеры и объектов, а также маскированные латентные условия для гибкого задания входных ракурсов. Метод позволяет генерировать видео с заданной траекторией камеры и согласованные 3D виды, которые можно преобразовать в 3D-представления.'}, 'en': {'title': 'Unlocking 3D and 4D Generation with GenXD', 'desc': 'This paper addresses the challenges of generating 3D and 4D visual content, which are more complex than 2D generation due to limited data and model design issues. The authors introduce a new dataset called CamVid-30K, created by extracting camera poses and object movements from videos, which helps in training models effectively. They present a framework named GenXD that utilizes multiview-temporal modules to differentiate between camera and object movements, enabling the generation of realistic 3D and 4D scenes. Extensive evaluations show that GenXD outperforms existing methods, demonstrating its capability to create coherent videos and 3D representations from diverse inputs.'}, 'zh': {'title': '突破3D与4D生成的瓶颈', 'desc': '本文探讨了3D和4D视觉生成的挑战，尤其是在缺乏大规模4D数据的情况下。我们提出了一种数据整理流程，从视频中获取相机姿态和物体运动强度，并引入了一个大型的4D场景数据集CamVid-30K。基于这些数据，我们开发了GenXD框架，能够生成任意3D或4D场景。通过多视角时间模块，GenXD能够有效地学习相机和物体的运动，并生成与相机轨迹一致的视频和可提升为3D表示的视图。'}}}, {'id': 'https://huggingface.co/papers/2411.02394', 'title': 'AutoVFX: Physically Realistic Video Editing from Natural Language Instructions', 'url': 'https://huggingface.co/papers/2411.02394', 'abstract': "Modern visual effects (VFX) software has made it possible for skilled artists to create imagery of virtually anything. However, the creation process remains laborious, complex, and largely inaccessible to everyday users. In this work, we present AutoVFX, a framework that automatically creates realistic and dynamic VFX videos from a single video and natural language instructions. By carefully integrating neural scene modeling, LLM-based code generation, and physical simulation, AutoVFX is able to provide physically-grounded, photorealistic editing effects that can be controlled directly using natural language instructions. We conduct extensive experiments to validate AutoVFX's efficacy across a diverse spectrum of videos and instructions. Quantitative and qualitative results suggest that AutoVFX outperforms all competing methods by a large margin in generative quality, instruction alignment, editing versatility, and physical plausibility.", 'score': 15, 'issue_id': 435, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '96ed53359fbc24a5', 'authors': ['Hao-Yu Hsu', 'Zhi-Hao Lin', 'Albert Zhai', 'Hongchi Xia', 'Shenlong Wang'], 'affiliations': ['University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02394.jpg', 'data': {'categories': ['#reasoning', '#cv', '#video', '#multimodal', '#games', '#architecture', '#alignment'], 'emoji': '🎬', 'ru': {'title': 'AutoVFX: Создание спецэффектов силой мысли', 'desc': 'AutoVFX - это новая система, которая автоматически создает реалистичные видео с визуальными эффектами на основе одного видео и текстовых инструкций на естественном языке. Она объединяет нейронное моделирование сцен, генерацию кода с помощью больших языковых моделей и физическое моделирование. AutoVFX позволяет создавать фотореалистичные эффекты с физически корректным поведением. Эксперименты показали, что система значительно превосходит аналоги по качеству генерации, соответствию инструкциям, универсальности редактирования и физической достоверности.'}, 'en': {'title': 'Transforming VFX Creation with Natural Language and AI', 'desc': 'AutoVFX is a novel framework designed to simplify the creation of visual effects (VFX) by allowing users to generate realistic videos from a single input video and natural language commands. It combines advanced techniques such as neural scene modeling, large language model (LLM)-based code generation, and physical simulation to produce high-quality, dynamic effects. The framework is evaluated through extensive experiments, demonstrating superior performance in generative quality, alignment with user instructions, versatility in editing, and adherence to physical realism. Overall, AutoVFX makes sophisticated VFX creation accessible to a broader audience, reducing the complexity of traditional methods.'}, 'zh': {'title': '自动化视觉特效，轻松创作真实影像', 'desc': '现代视觉特效软件使得熟练的艺术家能够创造几乎任何图像，但创作过程仍然繁琐且复杂，普通用户难以接触。本文提出了AutoVFX，一个框架可以根据单个视频和自然语言指令自动创建逼真且动态的视觉特效视频。通过精心整合神经场景建模、基于大语言模型的代码生成和物理仿真，AutoVFX能够提供物理基础的、照片级真实感的编辑效果，并可以通过自然语言指令直接控制。大量实验表明，AutoVFX在生成质量、指令对齐、编辑多样性和物理合理性方面显著优于所有竞争方法。'}}}, {'id': 'https://huggingface.co/papers/2411.00836', 'title': 'DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models', 'url': 'https://huggingface.co/papers/2411.00836', 'abstract': "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that SOTA VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010 generated concrete questions. Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. Our analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning.", 'score': 15, 'issue_id': 420, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'e73ae00a5621a2b9', 'authors': ['Chengke Zou', 'Xingang Guo', 'Rui Yang', 'Junyu Zhang', 'Bin Hu', 'Huan Zhang'], 'affiliations': ['University of Illinois at Urbana-Champaign', 'University of California, Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00836.jpg', 'data': {'categories': ['#reasoning', '#synthetic', '#benchmark', '#cv', '#graphs', '#optimization', '#math', '#dataset', '#architecture'], 'emoji': '🧮', 'ru': {'title': 'DynaMath: Новый подход к оценке математических способностей ИИ', 'desc': 'Статья представляет DynaMath - динамический визуальный математический бенчмарк для оценки робастности рассуждений Vision-Language Models (VLM) в задачах математики. Исследователи обнаружили, что современные VLM, такие как GPT-4V, часто не способны применять шаги решения к похожим задачам с небольшими изменениями. DynaMath включает 501 исходный вопрос в виде Python-программ, позволяющих генерировать множество вариаций для тестирования обобщающей способности моделей. Результаты оценки 14 современных VLM показали, что их точность в худшем случае значительно ниже средней точности.'}, 'en': {'title': 'Enhancing VLMs: Unveiling Mathematical Reasoning Limitations with DynaMath', 'desc': "This paper explores the limitations of Vision-Language Models (VLMs) in performing mathematical reasoning tasks that involve visual elements. It highlights that while humans can adapt their problem-solving strategies to slight changes in problems, current state-of-the-art VLMs like GPT-4o struggle with this adaptability. To address this issue, the authors introduce DynaMath, a dynamic benchmark that generates a wide variety of mathematical questions to rigorously test VLMs' reasoning capabilities. The findings reveal that VLMs exhibit significantly lower accuracy in worst-case scenarios compared to average cases, underscoring the importance of evaluating their robustness in mathematical reasoning."}, 'zh': {'title': '提升视觉语言模型的数学推理能力', 'desc': '本文探讨了视觉语言模型（VLMs）在数学推理任务中的表现，尤其是在视觉上下文的影响下。研究发现，尽管人类能够灵活应对相似问题的变化，当前的最先进模型如GPT-4o在面对这些变化时却表现不佳，显示出其数学推理能力的局限性。为了解决这一问题，本文提出了DynaMath，一个动态视觉数学基准，旨在深入评估VLMs的推理稳健性。通过对501个高质量种子问题的自动生成，DynaMath能够评估模型在不同输入条件下的泛化能力，结果显示模型在最坏情况下的准确率显著低于平均情况。'}}}, {'id': 'https://huggingface.co/papers/2411.01747', 'title': 'DynaSaur: Large Language Agents Beyond Predefined Actions', 'url': 'https://huggingface.co/papers/2411.01747', 'abstract': 'Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly-scoped environments, we argue that it presents two major challenges when deploying LLM agents in real-world scenarios: (1) selecting from a fixed set of actions significantly restricts the planning and acting capabilities of LLM agents, and (2) this approach requires substantial human effort to enumerate and implement all possible actions, which becomes impractical in complex environments with a vast number of potential actions. In this work, we propose an LLM agent framework that enables the dynamic creation and composition of actions in an online manner. In this framework, the agent interacts with the environment by generating and executing programs written in a general-purpose programming language at each step. Furthermore, generated actions are accumulated over time for future reuse. Our extensive experiments on the GAIA benchmark demonstrate that this framework offers significantly greater flexibility and outperforms previous methods. Notably, it allows an LLM agent to recover in scenarios where no relevant action exists in the predefined set or when existing actions fail due to unforeseen edge cases. At the time of writing, we hold the top position on the GAIA public leaderboard. Our code can be found in https://github.com/adobe-research/dynasaur{https://github.com/adobe-research/dynasaur}.', 'score': 13, 'issue_id': 423, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '772b11b15cab80a0', 'authors': ['Dang Nguyen', 'Viet Dac Lai', 'Seunghyun Yoon', 'Ryan A. Rossi', 'Handong Zhao', 'Ruiyi Zhang', 'Puneet Mathur', 'Nedim Lipka', 'Yu Wang', 'Trung Bui', 'Franck Dernoncourt', 'Tianyi Zhou'], 'affiliations': ['University of Maryland', 'Adobe Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01747.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#agi', '#plp', '#open_source', '#agents', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Динамическое создание действий для агентов на базе LLM: шаг к адаптивному ИИ', 'desc': 'Эта статья представляет новую структуру агентов на основе больших языковых моделей (LLM), которая позволяет динамически создавать и комбинировать действия в режиме реального времени. В отличие от существующих систем с фиксированным набором действий, предлагаемый подход генерирует и выполняет программы на языке общего назначения на каждом шаге взаимодействия с окружающей средой. Авторы демонстрируют, что их метод обеспечивает большую гибкость и превосходит предыдущие подходы в экспериментах на бенчмарке GAIA. Особенно эффективно система работает в сценариях, где предопределенные действия отсутствуют или неприменимы.'}, 'en': {'title': 'Empowering LLM Agents with Dynamic Action Generation', 'desc': 'This paper introduces a new framework for large language model (LLM) agents that allows them to dynamically create and execute actions in real-time, rather than relying on a fixed set of predefined actions. This approach enhances the planning and acting capabilities of LLM agents, making them more adaptable to complex and unpredictable environments. By generating programs in a general-purpose programming language, the agents can respond to unforeseen situations and accumulate useful actions for future tasks. The experiments conducted on the GAIA benchmark show that this framework significantly outperforms traditional methods, providing greater flexibility and effectiveness in real-world applications.'}, 'zh': {'title': '动态创建与组合动作的LLM代理框架', 'desc': '现有的大型语言模型（LLM）代理系统通常在每一步从固定的预定义动作集中选择动作。这种方法在封闭的、狭窄的环境中有效，但在现实世界中应用时面临两个主要挑战：一是从固定动作集中选择限制了LLM代理的规划和执行能力，二是需要大量人力来列举和实现所有可能的动作，这在复杂环境中变得不切实际。我们提出了一种LLM代理框架，允许在线动态创建和组合动作，代理通过生成和执行通用编程语言编写的程序与环境互动。我们的实验表明，该框架提供了更大的灵活性，并在GAIA基准测试中表现优于以往方法。'}}}, {'id': 'https://huggingface.co/papers/2411.02327', 'title': 'PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance', 'url': 'https://huggingface.co/papers/2411.02327', 'abstract': "The past year has witnessed the significant advancement of video-based large language models. However, the challenge of developing a unified model for both short and long video understanding remains unresolved. Most existing video LLMs cannot handle hour-long videos, while methods custom for long videos tend to be ineffective for shorter videos and images. In this paper, we identify the key issue as the redundant content in videos. To address this, we propose a novel pooling strategy that simultaneously achieves token compression and instruction-aware visual feature aggregation. Our model is termed Prompt-guided Pooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three core components: the CLIP-based visual-prompt alignment that extracts visual information relevant to the user's instructions, the prompt-guided pooling that compresses the visual sequence to arbitrary scales using convolution-style pooling, and the clip context extension designed for lengthy prompt common in visual dialogue. Moreover, our codebase also integrates the most advanced video Direct Preference Optimization (DPO) and visual interleave training. Extensive experiments have validated the performance of our model. With superior throughput and only 1024 visual context, PPLLaVA achieves better results on image benchmarks as a video LLM, while achieving state-of-the-art performance across various video benchmarks, excelling in tasks ranging from caption generation to multiple-choice questions, and handling video lengths from seconds to hours. Codes have been available at https://github.com/farewellthree/PPLLaVA.", 'score': 11, 'issue_id': 432, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': 'ed9d356ccf75d780', 'authors': ['Ruyang Liu', 'Haoran Tang', 'Haibo Liu', 'Yixiao Ge', 'Ying Shan', 'Chen Li', 'Jiankun Yang'], 'affiliations': ['Peking University', 'Applied Research Center (ARC), Tencent PCG', 'Peng Cheng Laboratory'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02327.jpg', 'data': {'categories': ['#long_context', '#rlhf', '#benchmark', '#video', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'Универсальная обработка видео любой длины с помощью инновационного пулинга', 'desc': 'Статья представляет новую модель PPLLaVA для обработки как коротких, так и длинных видео. Ключевая инновация заключается в использовании стратегии пулинга, которая сжимает токены и агрегирует визуальные признаки с учетом инструкций. Модель включает выравнивание визуальных подсказок на основе CLIP, пулинг с учетом подсказок и расширение контекста клипов. PPLLaVA демонстрирует высокую производительность на различных бенчмарках для изображений и видео разной длительности.'}, 'en': {'title': 'Unified Video Understanding with PPLLaVA', 'desc': 'This paper presents a new model called Prompt-guided Pooling LLaVA (PPLLaVA) that improves video understanding for both short and long videos. The authors identify redundant content in videos as a key challenge and propose a pooling strategy that compresses tokens while aggregating visual features based on user instructions. PPLLaVA includes components for visual-prompt alignment, convolution-style pooling, and context extension for lengthy prompts. The model demonstrates superior performance across various benchmarks, effectively handling video lengths from seconds to hours.'}, 'zh': {'title': '视频理解的新突破：PPLLaVA模型', 'desc': '本论文提出了一种新的视频大语言模型，称为PPLLaVA，旨在解决短视频和长视频理解的统一模型问题。我们发现视频中的冗余内容是主要挑战，因此提出了一种新的池化策略，实现了令牌压缩和指令感知的视觉特征聚合。PPLLaVA包含三个核心组件，分别是基于CLIP的视觉提示对齐、提示引导池化和剪辑上下文扩展。经过广泛实验验证，PPLLaVA在处理从几秒到几小时的视频时，表现出色，超越了现有的视频基准。'}}}, {'id': 'https://huggingface.co/papers/2411.02335', 'title': 'Sparsing Law: Towards Large Language Models with Greater Activation Sparsity', 'url': 'https://huggingface.co/papers/2411.02335', 'abstract': 'Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL-p% sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., 1-sparsity ratio) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable.', 'score': 11, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '3c9152f4d267fc7b', 'authors': ['Yuqi Luo', 'Chenyang Song', 'Xu Han', 'Yingfa Chen', 'Chaojun Xiao', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02335.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Разреженность активаций в LLM: ключ к эффективности и интерпретируемости', 'desc': 'Статья исследует разреженность активаций в больших языковых моделях (LLM). Авторы предлагают новую метрику PPL-p% для измерения разреженности активаций. Исследование показывает, что различные функции активации демонстрируют противоположные тренды разреженности во время обучения. Обнаружено, что соотношение ширины и глубины модели влияет на разреженность активаций, а масштаб параметров модели оказывает слабое влияние.'}, 'en': {'title': 'Unlocking Efficiency: The Power of Activation Sparsity in LLMs', 'desc': 'This paper investigates activation sparsity in decoder-only Transformer-based large language models (LLMs), focusing on how weakly-contributed activation outputs can be reduced for efficiency. The authors introduce a new metric called PPL-p% sparsity to quantitatively measure activation sparsity across different activation functions. Their experiments reveal that while ReLU and SiLU functions perform similarly, they exhibit opposite trends in training-time sparsity, with ReLU being more efficient. Additionally, the study finds that activation sparsity is largely unaffected by the parameter scale, suggesting that deeper architectures may enhance efficiency without requiring more parameters.'}, 'zh': {'title': '激活稀疏性：提升大型语言模型效率的关键', 'desc': '激活稀疏性指的是在激活输出中存在大量贡献较弱的元素，这些元素可以被消除，从而对大型语言模型（LLMs）的许多重要应用有益。本文对基于解码器的Transformer LLM中的激活稀疏性进行了全面的定量研究，提出了一种新的激活稀疏性度量标准PPL-p%稀疏性，适用于任何激活函数。研究发现，不同的激活函数在性能上相似，但在训练时间的稀疏性趋势上却相反，ReLU激活函数在利用更多训练数据方面更为高效。最后，研究表明，激活稀疏性的极限值与参数规模的变化关系不大，这为提高LLMs的效率和可解释性提供了重要的启示。'}}}, {'id': 'https://huggingface.co/papers/2411.01798', 'title': 'SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF', 'url': 'https://huggingface.co/papers/2411.01798', 'abstract': "In Large Language Model (LLM) development, Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning models with human values and preferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence between the current policy and a frozen initial policy as a reference, which is added as a penalty in policy optimization algorithms like Proximal Policy Optimization (PPO). While this constraint prevents models from deviating too far from the initial checkpoint, it limits exploration of the reward landscape, reducing the model's ability to discover higher-quality solutions. As a result, policy optimization is often trapped in a narrow region of the parameter space, leading to suboptimal alignment and performance. This paper presents SALSA (Soup-based Alignment Learning for Stronger Adaptation), a novel approach designed to overcome these limitations by creating a more flexible and better located reference model through weight-space averaging of two independent supervised fine-tuned (SFT) models. This model soup allows for larger deviation in KL divergence and exploring a promising region of the solution space without sacrificing stability. By leveraging this more robust reference model, SALSA fosters better exploration, achieving higher rewards and improving model robustness, out-of-distribution generalization, and performance. We validate the effectiveness of SALSA through extensive experiments on popular open models (Llama2-7B, Mistral-7B, and Gemma-2B) across various benchmarks (MT-Bench, Arena-Hard, UltraFeedback), where it consistently surpasses PPO by fostering deeper exploration and achieving superior alignment in LLMs.", 'score': 8, 'issue_id': 435, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': 'e5a60efe079c6136', 'authors': ['Atoosa Chegini', 'Hamid Kazemi', 'Iman Mirzadeh', 'Dong Yin', 'Maxwell Horton', 'Moin Nabi', 'Mehrdad Farajtabar', 'Keivan Alizadeh'], 'affiliations': ['University of Maryland', 'Apple'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01798.jpg', 'data': {'categories': ['#small_models', '#rl', '#rlhf', '#benchmark', '#optimization', '#training', '#open_source', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'SALSA: Гибкое обучение для лучшей адаптации языковых моделей', 'desc': 'Статья представляет новый подход SALSA для обучения больших языковых моделей с подкреплением от обратной связи человека (RLHF). В отличие от традиционного метода PPO, использующего KL-дивергенцию с замороженной исходной моделью, SALSA создает более гибкую опорную модель путем усреднения весов двух независимо обученных моделей. Это позволяет исследовать более широкую область пространства параметров, не жертвуя стабильностью. Эксперименты показывают, что SALSA превосходит PPO по многим метрикам, включая награды, обобщение вне распределения и общую производительность.'}, 'en': {'title': 'SALSA: Enhancing LLM Alignment through Flexible Exploration', 'desc': "This paper introduces SALSA, a new method for improving the alignment of Large Language Models (LLMs) with human preferences using Reinforcement Learning from Human Feedback (RLHF). Traditional RLHF methods use a fixed reference policy, which can restrict the model's ability to explore and find better solutions due to the penalty imposed by Kullback-Leibler (KL) divergence. SALSA addresses this issue by averaging weights from two independently fine-tuned models, creating a more flexible reference that allows for greater exploration of the reward landscape. The results show that SALSA enhances model performance, robustness, and generalization across various benchmarks compared to traditional methods like Proximal Policy Optimization (PPO)."}, 'zh': {'title': 'SALSA：提升大型语言模型对齐与探索的创新方法', 'desc': '在大型语言模型（LLM）的开发中，基于人类反馈的强化学习（RLHF）对于使模型与人类价值观和偏好保持一致至关重要。传统的RLHF依赖于当前策略与冻结初始策略之间的Kullback-Leibler（KL）散度作为参考，这在策略优化算法中作为惩罚项使用。本文提出了一种新方法SALSA（基于模型集合的对齐学习），通过对两个独立的监督微调模型进行权重空间平均，创建一个更灵活的参考模型，从而克服了传统方法的局限性。SALSA通过更好的探索能力，显著提高了模型的鲁棒性和性能，验证了其在多个基准测试中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2411.00785', 'title': 'IGOR: Image-GOal Representations are the Atomic Control Units for Foundation Models in Embodied AI', 'url': 'https://huggingface.co/papers/2411.00785', 'abstract': 'We introduce Image-GOal Representations (IGOR), aiming to learn a unified, semantically consistent action space across human and various robots. Through this unified latent action space, IGOR enables knowledge transfer among large-scale robot and human activity data. We achieve this by compressing visual changes between an initial image and its goal state into latent actions. IGOR allows us to generate latent action labels for internet-scale video data. This unified latent action space enables the training of foundation policy and world models across a wide variety of tasks performed by both robots and humans. We demonstrate that: (1) IGOR learns a semantically consistent action space for both human and robots, characterizing various possible motions of objects representing the physical interaction knowledge; (2) IGOR can "migrate" the movements of the object in the one video to other videos, even across human and robots, by jointly using the latent action model and world model; (3) IGOR can learn to align latent actions with natural language through the foundation policy model, and integrate latent actions with a low-level policy model to achieve effective robot control. We believe IGOR opens new possibilities for human-to-robot knowledge transfer and control.', 'score': 8, 'issue_id': 434, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'a7e2824d0e474c0b', 'authors': ['Xiaoyu Chen', 'Junliang Guo', 'Tianyu He', 'Chuheng Zhang', 'Pushi Zhang', 'Derek Cathera Yang', 'Li Zhao', 'Jiang Bian'], 'affiliations': ['Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00785.jpg', 'data': {'categories': ['#science', '#cv', '#graphs', '#video', '#multimodal', '#training', '#dataset', '#robotics', '#transfer_learning', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Единое пространство действий для людей и роботов', 'desc': 'Исследователи представляют IGOR - систему для создания единого семантически согласованного пространства действий для людей и роботов. IGOR сжимает визуальные изменения между начальным и целевым изображением в латентные действия, что позволяет генерировать метки действий для масштабных видеоданных. Это единое латентное пространство действий позволяет обучать базовые политики и модели мира для разнообразных задач, выполняемых как роботами, так и людьми. IGOR демонстрирует возможность переноса движений объектов между видео, даже между людьми и роботами, а также интеграцию с естественным языком и низкоуровневым управлением роботами.'}, 'en': {'title': 'Unified Action Space for Human-Robot Interaction', 'desc': 'The paper presents Image-GOal Representations (IGOR), a framework designed to create a unified action space that is semantically consistent for both humans and robots. By compressing the visual changes between an initial image and its goal state into latent actions, IGOR facilitates knowledge transfer across diverse datasets of human and robot activities. This approach allows for the generation of latent action labels from large-scale video data, enabling the training of foundational policy and world models for various tasks. Ultimately, IGOR enhances the ability to transfer movement knowledge between humans and robots, aligning actions with natural language for improved robot control.'}, 'zh': {'title': '统一动作空间，连接人类与机器人', 'desc': '本文介绍了图像目标表示（IGOR），旨在学习一个统一且语义一致的动作空间，适用于人类和各种机器人。通过这个统一的潜在动作空间，IGOR能够在大规模机器人和人类活动数据之间进行知识转移。我们通过压缩初始图像与目标状态之间的视觉变化来实现这一点，从而生成潜在动作标签。IGOR为机器人控制和人机交互开辟了新的可能性。'}}}, {'id': 'https://huggingface.co/papers/2411.00918', 'title': 'LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models', 'url': 'https://huggingface.co/papers/2411.00918', 'abstract': 'Mixture of Experts (MoEs) plays an important role in the development of more efficient and effective large language models (LLMs). Due to the enormous resource requirements, studying large scale MoE algorithms remain in-accessible to many researchers. This work develops LibMoE, a comprehensive and modular framework to streamline the research, training, and evaluation of MoE algorithms. Built upon three core principles: (i) modular design, (ii) efficient training; (iii) comprehensive evaluation, LibMoE brings MoE in LLMs more accessible to a wide range of researchers by standardizing the training and evaluation pipelines. Using LibMoE, we extensively benchmarked five state-of-the-art MoE algorithms over three different LLMs and 11 datasets under the zero-shot setting. The results show that despite the unique characteristics, all MoE algorithms perform roughly similar when averaged across a wide range of tasks. With the modular design and extensive evaluation, we believe LibMoE will be invaluable for researchers to make meaningful progress towards the next generation of MoE and LLMs. Project page: https://fsoft-aic.github.io/fsoft-LibMoE.github.io.', 'score': 8, 'issue_id': 420, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'a406640433a3de34', 'authors': ['Nam V. Nguyen', 'Thong T. Doan', 'Luong Tran', 'Van Nguyen', 'Quang Pham'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00918.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'LibMoE: Демократизация исследований Mixture of Experts в больших языковых моделях', 'desc': 'Статья представляет LibMoE - комплексную модульную систему для исследования, обучения и оценки алгоритмов Mixture of Experts (MoE) в больших языковых моделях (LLM). LibMoE основан на трех принципах: модульный дизайн, эффективное обучение и всесторонняя оценка. Используя LibMoE, авторы провели обширное сравнение пяти современных алгоритмов MoE на трех различных LLM и 11 наборах данных в режиме zero-shot. Результаты показали, что, несмотря на уникальные характеристики, все алгоритмы MoE показывают примерно одинаковые результаты при усреднении по широкому спектру задач.'}, 'en': {'title': 'LibMoE: Streamlining Mixture of Experts for Large Language Models', 'desc': 'This paper introduces LibMoE, a framework designed to facilitate research on Mixture of Experts (MoE) algorithms for large language models (LLMs). It emphasizes a modular design, efficient training processes, and comprehensive evaluation methods to make MoE more accessible to researchers. The authors benchmark five leading MoE algorithms across three LLMs and 11 datasets, revealing that their performance is generally similar across various tasks. LibMoE aims to standardize the training and evaluation of MoE, helping researchers advance the development of future LLMs.'}, 'zh': {'title': 'LibMoE：让混合专家算法更易于研究和应用', 'desc': '混合专家（MoE）在大型语言模型（LLMs）的高效和有效发展中扮演着重要角色。由于资源需求巨大，许多研究者难以研究大规模的MoE算法。本文开发了LibMoE，这是一个全面且模块化的框架，旨在简化MoE算法的研究、训练和评估。通过模块化设计、高效训练和全面评估，LibMoE使得MoE在LLMs中的应用对更多研究者变得可及。'}}}, {'id': 'https://huggingface.co/papers/2411.00743', 'title': 'Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models', 'url': 'https://huggingface.co/papers/2411.00743', 'abstract': 'Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability methods. Sparse Autoencoders (SAEs) have emerged as a promising tool for disentangling FM representations, but they struggle to capture rare, yet crucial concepts in the data. We introduce Specialized Sparse Autoencoders (SSAEs), designed to illuminate these elusive dark matter features by focusing on specific subdomains. We present a practical recipe for training SSAEs, demonstrating the efficacy of dense retrieval for data selection and the benefits of Tilted Empirical Risk Minimization as a training objective to improve concept recall. Our evaluation of SSAEs on standard metrics, such as downstream perplexity and L_0 sparsity, show that they effectively capture subdomain tail concepts, exceeding the capabilities of general-purpose SAEs. We showcase the practical utility of SSAEs in a case study on the Bias in Bios dataset, where SSAEs achieve a 12.5\\% increase in worst-group classification accuracy when applied to remove spurious gender information. SSAEs provide a powerful new lens for peering into the inner workings of FMs in subdomains.', 'score': 6, 'issue_id': 421, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'af234e3c99f935ec', 'authors': ['Aashiq Muhamed', 'Mona Diab', 'Virginia Smith'], 'affiliations': ['Language Technologies Institute, Carnegie Mellon University', 'Machine Learning Department, Carnegie Mellon University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00743.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#ethics', '#data', '#training', '#dataset', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Специализированные автоэнкодеры раскрывают скрытые концепты в фундаментальных моделях', 'desc': 'Статья представляет новый метод интерпретации фундаментальных моделей - Специализированные разреженные автоэнкодеры (SSAE). SSAE фокусируются на конкретных поддоменах и эффективно выявляют редкие, но важные концепты в данных. Авторы демонстрируют преимущества плотного поиска для выбора данных и использования Tilted Empirical Risk Minimization в качестве целевой функции обучения. Эффективность SSAE показана на стандартных метриках и в практическом исследовании на наборе данных Bias in Bios.'}, 'en': {'title': 'Illuminating Hidden Concepts in Foundation Models with SSAEs', 'desc': 'This paper discusses the challenges of understanding and mitigating risks in foundation models (FMs) through effective interpretability methods. It introduces Specialized Sparse Autoencoders (SSAEs), which are designed to better identify rare but important concepts in data by focusing on specific subdomains. The authors provide a training approach that utilizes dense retrieval for data selection and Tilted Empirical Risk Minimization to enhance concept recall. The results show that SSAEs outperform traditional Sparse Autoencoders in capturing these critical features, as demonstrated in a case study that improved classification accuracy by addressing bias in gender information.'}, 'zh': {'title': '专注子领域的稀疏自编码器：揭示基础模型的潜在特征', 'desc': '本论文探讨了基础模型（FMs）潜在风险的理解与缓解，强调了有效的可解释性方法的重要性。我们提出了一种新的稀疏自编码器（SSAEs），旨在揭示数据中稀有但重要的概念，特别关注特定子领域。通过密集检索和倾斜经验风险最小化等方法，我们展示了SSAEs在捕捉子领域尾部概念方面的优势。案例研究表明，SSAEs在去除虚假性别信息时，分类准确率提高了12.5%。'}}}, {'id': 'https://huggingface.co/papers/2411.00359', 'title': 'Constrained Diffusion Implicit Models', 'url': 'https://huggingface.co/papers/2411.00359', 'abstract': 'This paper describes an efficient algorithm for solving noisy linear inverse problems using pretrained diffusion models. Extending the paradigm of denoising diffusion implicit models (DDIM), we propose constrained diffusion implicit models (CDIM) that modify the diffusion updates to enforce a constraint upon the final output. For noiseless inverse problems, CDIM exactly satisfies the constraints; in the noisy case, we generalize CDIM to satisfy an exact constraint on the residual distribution of the noise. Experiments across a variety of tasks and metrics show strong performance of CDIM, with analogous inference acceleration to unconstrained DDIM: 10 to 50 times faster than previous conditional diffusion methods. We demonstrate the versatility of our approach on many problems including super-resolution, denoising, inpainting, deblurring, and 3D point cloud reconstruction.', 'score': 5, 'issue_id': 434, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'e85bdeb4e14858fd', 'authors': ['Vivek Jayaram', 'Ira Kemelmacher-Shlizerman', 'Steven M. Seitz', 'John Thickstun'], 'affiliations': ['University of Washington', 'Cornell University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00359.jpg', 'data': {'categories': ['#diffusion', '#cv', '#inference', '#optimization', '#3d', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'Ускоренное решение обратных задач с помощью ограниченных диффузионных моделей', 'desc': 'Статья описывает эффективный алгоритм для решения зашумленных линейных обратных задач с использованием предобученных диффузионных моделей. Авторы предлагают constrained diffusion implicit models (CDIM), которые модифицируют обновления диффузии для обеспечения ограничений на конечный результат. CDIM точно удовлетворяет ограничениям для задач без шума, а для зашумленных задач обобщается для удовлетворения точного ограничения на распределение остаточного шума. Эксперименты показывают высокую производительность CDIM с ускорением вывода в 10-50 раз по сравнению с предыдущими условными диффузионными методами.'}, 'en': {'title': 'Accelerating Inverse Problem Solutions with Constrained Diffusion Models', 'desc': "This paper introduces a new algorithm called Constrained Diffusion Implicit Models (CDIM) for tackling noisy linear inverse problems. CDIM builds on the existing Denoising Diffusion Implicit Models (DDIM) by incorporating constraints into the diffusion process, ensuring that the final output adheres to specific requirements. In scenarios without noise, CDIM perfectly meets these constraints, while in noisy situations, it adapts to maintain an exact constraint on the noise's residual distribution. The results demonstrate that CDIM not only performs well across various tasks like super-resolution and denoising but also accelerates inference significantly, achieving speeds 10 to 50 times faster than traditional methods."}, 'zh': {'title': '高效解决带噪声线性逆问题的约束扩散模型', 'desc': '本文提出了一种高效的算法，用于解决带噪声的线性逆问题，利用预训练的扩散模型。我们扩展了去噪扩散隐式模型（DDIM）的范式，提出了约束扩散隐式模型（CDIM），通过修改扩散更新来强制最终输出满足约束条件。在无噪声的逆问题中，CDIM能够精确满足约束；而在有噪声的情况下，我们将CDIM推广到满足噪声残差分布的精确约束。实验结果表明，CDIM在多种任务和指标上表现出色，其推理速度比之前的条件扩散方法快10到50倍，展示了我们方法在超分辨率、去噪、修复、去模糊和3D点云重建等问题上的多样性。'}}}, {'id': 'https://huggingface.co/papers/2411.00492', 'title': 'Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models', 'url': 'https://huggingface.co/papers/2411.00492', 'abstract': 'We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple experts, aggregating their responses, and selecting the best among individual and aggregated responses. This process is performed in a single chain of thoughts through our seven carefully designed subtasks derived from the Nominal Group Technique (Ven and Delbecq, 1974), a well-established decision-making framework. Our evaluations demonstrate that Multi-expert Prompting significantly outperforms ExpertPrompting and comparable baselines in enhancing the truthfulness, factuality, informativeness, and usefulness of responses while reducing toxicity and hurtfulness. It further achieves state-of-the-art truthfulness by outperforming the best baseline by 8.69% with ChatGPT. Multi-expert Prompting is efficient, explainable, and highly adaptable to diverse scenarios, eliminating the need for manual prompt construction.', 'score': 5, 'issue_id': 433, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'd4857ddaa86c9914', 'authors': ['Do Xuan Long', 'Duong Ngoc Yen', 'Anh Tuan Luu', 'Kenji Kawaguchi', 'Min-Yen Kan', 'Nancy F. Chen'], 'affiliations': ['National University of Singapore', 'Institute for Infocomm Research (I2R), A*STAR', 'Nanyang Technological University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00492.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#interpretability', '#architecture', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Коллективный разум экспертов для улучшения ответов ИИ', 'desc': 'В статье представлен метод Multi-expert Prompting, улучшающий генерацию текста большими языковыми моделями (LLM). Этот подход симулирует работу нескольких экспертов, объединяет их ответы и выбирает лучший результат. Процесс реализуется в виде единой цепочки рассуждений с использованием семи специально разработанных подзадач, основанных на методике Nominal Group Technique. Эксперименты показывают, что Multi-expert Prompting значительно превосходит базовые методы по ряду критериев, включая достоверность и информативность ответов.'}, 'en': {'title': 'Harnessing Collective Expertise for Superior Language Model Responses', 'desc': 'Multi-expert Prompting is an advanced technique that enhances the performance of large language models (LLMs) by simulating the input of multiple experts. It aggregates responses from these simulated experts and selects the best one, ensuring that the output is more accurate and informative. This method is structured around seven subtasks inspired by the Nominal Group Technique, which aids in effective decision-making. Evaluations show that this approach significantly improves the truthfulness and usefulness of LLM responses while minimizing negative outputs like toxicity.'}, 'zh': {'title': '多专家提示：提升语言模型生成的全新方法', 'desc': '本文提出了一种新的增强方法，称为多专家提示（Multi-expert Prompting），旨在改善大型语言模型（LLM）的生成效果。该方法通过模拟多个专家来指导LLM执行输入指令，聚合各个专家的响应，并选择最佳的单个和聚合响应。我们设计了七个子任务，基于成熟的决策框架——名义小组技术（Nominal Group Technique），以实现这一过程。评估结果表明，多专家提示在提高响应的真实性、事实性、信息量和实用性方面显著优于专家提示（ExpertPrompting）及其他基线，同时减少了有害性和攻击性。'}}}, {'id': 'https://huggingface.co/papers/2411.01106', 'title': 'LoRA-Contextualizing Adaptation of Large Multimodal Models for Long Document Understanding', 'url': 'https://huggingface.co/papers/2411.01106', 'abstract': 'Large multimodal models (LMMs) have recently shown great progress in text-rich image understanding, yet they still struggle with complex, multi-page, visually-rich documents. Traditional methods using document parsers for retrieval-augmented generation suffer from performance and efficiency limitations, while directly presenting all pages to LMMs leads to inefficiencies, especially with lengthy documents. In this work, we present a novel framework named LoRA-Contextualizing Adaptation of Large multimodal models (LoCAL), which broadens the capabilities of any LMM to support long-document understanding. We demonstrate that LMMs can effectively serve as multimodal retrievers, fetching relevant pages to answer user questions based on these pages. LoCAL is implemented with two specific LMM adapters: one for evidence page retrieval and another for question answering. Empirical results show state-of-the-art performance on public benchmarks, demonstrating the effectiveness of LoCAL.', 'score': 4, 'issue_id': 433, 'pub_date': '2024-11-02', 'pub_date_card': {'ru': '2 ноября', 'en': 'November 2', 'zh': '11月2日'}, 'hash': '9ac41ff2238a6f8d', 'authors': ['Jian Chen', 'Ruiyi Zhang', 'Yufan Zhou', 'Tong Yu', 'Franck Dernoncourt', 'Jiuxiang Gu', 'Ryan A. Rossi', 'Changyou Chen', 'Tong Sun'], 'affiliations': ['University at Buffalo', 'Adobe Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01106.jpg', 'data': {'categories': ['#science', '#rag', '#benchmark', '#multimodal', '#architecture', '#long_context'], 'emoji': '📄', 'ru': {'title': 'LoCAL: Революция в понимании длинных документов с помощью LMM', 'desc': 'Статья представляет новый фреймворк LoCAL для улучшения понимания длинных документов большими мультимодальными моделями (LMM). LoCAL использует два адаптера LMM: один для поиска релевантных страниц, другой для ответов на вопросы. Этот подход позволяет эффективно обрабатывать сложные многостраничные документы, преодолевая ограничения традиционных методов. Эмпирические результаты показывают превосходную производительность LoCAL на публичных бенчмарках.'}, 'en': {'title': 'Enhancing Long-Document Understanding with LoCAL', 'desc': 'This paper introduces LoCAL, a new framework designed to enhance large multimodal models (LMMs) for understanding long and complex documents. Traditional methods struggle with efficiency when processing multiple pages, but LoCAL allows LMMs to act as multimodal retrievers, selecting relevant pages to answer questions. The framework includes two specialized adapters: one for retrieving evidence pages and another for answering questions based on those pages. Empirical results indicate that LoCAL achieves state-of-the-art performance on public benchmarks, showcasing its effectiveness in long-document comprehension.'}, 'zh': {'title': '提升大型多模态模型的长文档理解能力', 'desc': '大型多模态模型（LMMs）在理解文本丰富的图像方面取得了显著进展，但在处理复杂的多页视觉文档时仍然面临挑战。传统的文档解析方法在检索增强生成中存在性能和效率的限制，而直接将所有页面呈现给LMMs则导致效率低下，尤其是在处理较长文档时。我们提出了一种新框架，称为LoRA-上下文适应的大型多模态模型（LoCAL），它扩展了任何LMM支持长文档理解的能力。实验结果表明，LoCAL在公共基准测试中表现出色，证明了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2411.01192', 'title': 'Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks', 'url': 'https://huggingface.co/papers/2411.01192', 'abstract': 'We introduce Swan, a family of embedding models centred around the Arabic language, addressing both small-scale and large-scale use cases. Swan includes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on ArMistral, a pretrained Arabic large language model. To evaluate these models, we propose ArabicMTEB, a comprehensive benchmark suite that assesses cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance, covering eight diverse tasks and spanning 94 datasets. Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks, while the Swan-Small consistently surpasses Multilingual-E5 base. Our extensive evaluations demonstrate that Swan models are both dialectally and culturally aware, excelling across various Arabic domains while offering significant monetary efficiency. This work significantly advances the field of Arabic language modelling and provides valuable resources for future research and applications in Arabic natural language processing. Our models and benchmark will be made publicly accessible for research.', 'score': 3, 'issue_id': 427, 'pub_date': '2024-11-02', 'pub_date_card': {'ru': '2 ноября', 'en': 'November 2', 'zh': '11月2日'}, 'hash': 'df042a726644eac5', 'authors': ['Gagan Bhatia', 'El Moatez Billah Nagoudi', 'Abdellah El Mekki', 'Fakhraddin Alwajih', 'Muhammad Abdul-Mageed'], 'affiliations': ['MBZUAI', 'The University of British Columbia', 'Invertible AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01192.jpg', 'data': {'categories': ['#small_models', '#benchmark', '#multilingual', '#dataset', '#open_source', '#low_resource', '#architecture'], 'emoji': '🦢', 'ru': {'title': 'Swan: Прорыв в обработке арабского языка', 'desc': 'Представлены модели семейства Swan для встраивания текстов на арабском языке. Разработаны два варианта: Swan-Small на основе ARBERTv2 и Swan-Large на базе ArMistral. Для оценки создан бенчмарк ArabicMTEB, охватывающий 8 задач и 94 набора данных. Swan-Large превосходит Multilingual-E5-large по большинству арабских задач, демонстрируя диалектную и культурную осведомленность.'}, 'en': {'title': 'Swan: Advancing Arabic Language Embeddings for Diverse Applications', 'desc': 'This paper presents Swan, a new family of embedding models specifically designed for the Arabic language, which includes two versions: Swan-Small and Swan-Large. Swan-Small is based on ARBERTv2, while Swan-Large utilizes the ArMistral model, a large pretrained Arabic language model. The authors introduce ArabicMTEB, a benchmark suite that evaluates the performance of these models across various Arabic text tasks, ensuring they are effective in different dialects and cultural contexts. The results show that Swan-Large outperforms existing models in most Arabic tasks, highlighting its efficiency and effectiveness in Arabic natural language processing.'}, 'zh': {'title': 'Swan：阿拉伯语嵌入模型的创新之路', 'desc': '本文介绍了Swan，一个以阿拉伯语为中心的嵌入模型家族，适用于小规模和大规模的应用场景。Swan包括两个变体：基于ARBERTv2的Swan-Small和基于预训练阿拉伯大型语言模型ArMistral的Swan-Large。我们提出了ArabicMTEB，一个全面的基准套件，用于评估阿拉伯文本嵌入在跨语言、多方言、多领域和多文化方面的表现，涵盖八个不同任务和94个数据集。Swan-Large在大多数阿拉伯任务中表现优异，超越了Multilingual-E5-large，而Swan-Small也始终优于Multilingual-E5 base。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (4)', '#agents (3)', '#agi (1)', '#alignment (4)', '#architecture (18)', '#audio', '#benchmark (15)', '#cv (8)', '#data (2)', '#dataset (10)', '#diffusion (5)', '#ethics (2)', '#games (3)', '#graphs (2)', '#hallucinations', '#healthcare', '#inference (3)', '#interpretability (3)', '#leakage', '#long_context (3)', '#low_resource (1)', '#machine_translation', '#math (2)', '#multilingual (2)', '#multimodal (5)', '#open_source (11)', '#optimization (12)', '#plp (2)', '#rag (1)', '#reasoning (6)', '#rl (2)', '#rlhf (2)', '#robotics (1)', '#science (3)', '#security', '#small_models (3)', '#story_generation', '#survey (1)', '#synthetic (4)', '#training (12)', '#transfer_learning (1)', '#video (6)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-11-05 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-11-05 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-11-05 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    