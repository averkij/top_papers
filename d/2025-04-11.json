{
    "date": {
        "ru": "11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 11",
        "zh": "4æœˆ11æ—¥"
    },
    "time_utc": "2025-04-11 04:13",
    "weekday": 4,
    "issue_id": 3184,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.07956",
            "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning",
            "url": "https://huggingface.co/papers/2504.07956",
            "abstract": "The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task.",
            "score": 18,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "88860725e51f3629",
            "authors": [
                "Yukun Qi",
                "Yiming Zhao",
                "Yu Zeng",
                "Xikun Bao",
                "Wenxuan Huang",
                "Lin Chen",
                "Zehui Chen",
                "Jie Zhao",
                "Zhongang Qi",
                "Feng Zhao"
            ],
            "affiliations": [
                "East China Normal University",
                "Huawei Noahs Ark Lab",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07956.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "VCR-Bench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VCR-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 859 Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 1034 Ğ¿Ğ°Ñ€Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. VCR-Bench Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¾Ğ² Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°."
                },
                "en": {
                    "title": "VCR-Bench: Evaluating Video Reasoning in LVLMs",
                    "desc": "This paper introduces VCR-Bench, a new benchmark for evaluating Video Chain-of-Thought (CoT) reasoning in large vision-language models (LVLMs). It addresses the lack of rigorous evaluation frameworks for assessing how well these models can reason about video content. The benchmark includes 859 videos and 1,034 annotated question-answer pairs, each with a stepwise CoT rationale linked to perception or reasoning capabilities. Experiments reveal that current LVLMs struggle with video reasoning, particularly in processing temporal-spatial information, highlighting the need for improved models in this area."
                },
                "zh": {
                    "title": "VCR-Benchï¼šè§†é¢‘æ¨ç†çš„æ–°æ ‡å‡†",
                    "desc": "é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†çš„è¿›æ­¥æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸€ä¸ªä¸¥æ ¼çš„è§†é¢‘CoTæ¨ç†è¯„ä¼°æ¡†æ¶ï¼Œç°æœ‰çš„è§†é¢‘åŸºå‡†æ— æ³•å……åˆ†è¯„ä¼°æ¨ç†è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†VCR-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°LVLMsåœ¨è§†é¢‘é“¾å¼æ€ç»´æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡859ä¸ªè§†é¢‘å’Œ1034å¯¹é«˜è´¨é‡é—®ç­”å¯¹ï¼ŒVCR-Benchä¸ºæ¯ä¸ªé—®ç­”å¯¹æä¾›äº†é€æ­¥çš„CoTæ¨ç†ä¾æ®ï¼Œæ­ç¤ºäº†å½“å‰LVLMsåœ¨å¤æ‚è§†é¢‘æ¨ç†ä¸­çš„å…³é”®ç“¶é¢ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07128",
            "title": "DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning",
            "url": "https://huggingface.co/papers/2504.07128",
            "abstract": "Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly \"thinking\" about a problem before providing an answer. This reasoning process is publicly available to the user, creating endless opportunities for studying the reasoning behaviour of the model and opening up the field of Thoughtology. Starting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning, our analyses on DeepSeek-R1 investigate the impact and controllability of thought length, management of long or confusing contexts, cultural and safety concerns, and the status of DeepSeek-R1 vis-\\`a-vis cognitive phenomena, such as human-like language processing and world modelling. Our findings paint a nuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning, where extra inference time can impair model performance. Furthermore, we find a tendency for DeepSeek-R1 to persistently ruminate on previously explored problem formulations, obstructing further exploration. We also note strong safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning counterpart, which can also compromise safety-aligned LLMs.",
            "score": 14,
            "issue_id": 3184,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "6317a88ae7643fe2",
            "authors": [
                "Sara Vera MarjanoviÄ‡",
                "Arkil Patel",
                "Vaibhav Adlakha",
                "Milad Aghajohari",
                "Parishad BehnamGhader",
                "Mehar Bhatia",
                "Aditi Khandelwal",
                "Austin Kraft",
                "Benno Krojer",
                "Xing Han LÃ¹",
                "Nicholas Meade",
                "Dongchan Shin",
                "Amirhossein Kazemnejad",
                "Gaurav Kamath",
                "Marius Mosbach",
                "Karolina StaÅ„czak",
                "Siva Reddy"
            ],
            "affiliations": [
                "McGill University",
                "Mila Quebec AI Institute",
                "University of Copenhagen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07128.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#ethics",
                    "#inference",
                    "#reasoning",
                    "#long_context",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "DeepSeek-R1: Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑˆĞ¸Ğ½ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ DeepSeek-R1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¸ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñƒ DeepSeek-R1 ĞµÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒÑ…ÑƒĞ´ÑˆĞ°Ñ‚ÑŒÑÑ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ÑÑ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°ÑÑ‚Ñ€ĞµĞ²Ğ°Ñ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ½ĞµĞµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ»Ğ°Ğ½Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ±ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "DeepSeek-R1: Revolutionizing Reasoning in Language Models",
                    "desc": "The paper introduces DeepSeek-R1, a Large Reasoning Model that enhances the way language models (LLMs) tackle complex problems by generating multi-step reasoning chains. This model allows users to observe the reasoning process, fostering a new area of research called Thoughtology. The study examines various aspects of DeepSeek-R1, including the effects of reasoning length, context management, and safety issues, revealing that excessive inference time can negatively impact performance. Additionally, the findings highlight the model's tendency to dwell on previous problem formulations, which can hinder its ability to explore new solutions and raise safety concerns compared to traditional LLMs."
                },
                "zh": {
                    "title": "æ·±åº¦æ¨ç†ï¼Œæ€ç»´çš„æœªæ¥",
                    "desc": "DeepSeek-R1æ˜¯ä¸€ç§å¤§å‹æ¨ç†æ¨¡å‹ï¼Œæ ‡å¿—ç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶çš„æ ¹æœ¬è½¬å˜ã€‚å®ƒé€šè¿‡åˆ›å»ºè¯¦ç»†çš„å¤šæ­¥éª¤æ¨ç†é“¾æ¥â€œæ€è€ƒâ€é—®é¢˜ï¼Œè€Œä¸æ˜¯ç›´æ¥ç»™å‡ºç­”æ¡ˆã€‚è¿™ç§æ¨ç†è¿‡ç¨‹å¯¹ç”¨æˆ·æ˜¯å…¬å¼€çš„ï¼Œä¸ºç ”ç©¶æ¨¡å‹çš„æ¨ç†è¡Œä¸ºæä¾›äº†æ— é™å¯èƒ½ï¼Œå¹¶å¼€å¯äº†æ€ç»´å­¦ï¼ˆThoughtologyï¼‰é¢†åŸŸã€‚æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºï¼ŒDeepSeek-R1åœ¨æ¨ç†æ—¶å­˜åœ¨ä¸€ä¸ªâ€œç”œèœœç‚¹â€ï¼Œè¿‡é•¿çš„æ¨ç†æ—¶é—´å¯èƒ½ä¼šå½±å“æ¨¡å‹çš„è¡¨ç°ï¼ŒåŒæ—¶å®ƒåœ¨å¤„ç†å·²æ¢ç´¢çš„é—®é¢˜æ—¶å®¹æ˜“é™·å…¥åå¤æ€è€ƒï¼Œå½±å“è¿›ä¸€æ­¥çš„æ¢ç´¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07943",
            "title": "HoloPart: Generative 3D Part Amodal Segmentation",
            "url": "https://huggingface.co/papers/2504.07943",
            "abstract": "3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Inspired by 2D amodal segmentation, we introduce this novel task to the 3D domain and propose a practical, two-stage approach, addressing the key challenges of inferring occluded 3D geometry, maintaining global shape consistency, and handling diverse shapes with limited training data. First, we leverage existing 3D part segmentation to obtain initial, incomplete part segments. Second, we introduce HoloPart, a novel diffusion-based model, to complete these segments into full 3D parts. HoloPart utilizes a specialized architecture with local attention to capture fine-grained part geometry and global shape context attention to ensure overall shape consistency. We introduce new benchmarks based on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart significantly outperforms state-of-the-art shape completion methods. By incorporating HoloPart with existing segmentation techniques, we achieve promising results on 3D part amodal segmentation, opening new avenues for applications in geometry editing, animation, and material assignment.",
            "score": 13,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "4cc1401ee10a171e",
            "authors": [
                "Yunhan Yang",
                "Yuan-Chen Guo",
                "Yukun Huang",
                "Zi-Xin Zou",
                "Zhipeng Yu",
                "Yangguang Li",
                "Yan-Pei Cao",
                "Xihui Liu"
            ],
            "affiliations": [
                "The University of Hong Kong",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07943.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#architecture",
                    "#diffusion",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸: Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ 3D-Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ HoloPart Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ 3D-Ñ‡Ğ°ÑÑ‚Ğ¸. HoloPart Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking Hidden Shapes: HoloPart for 3D Amodal Segmentation",
                    "desc": "This paper addresses the challenge of 3D part amodal segmentation, which involves identifying complete parts of a 3D shape even when some parts are hidden. Current methods only work with visible surfaces, limiting their effectiveness. The authors propose a two-stage approach that first uses existing segmentation techniques to identify incomplete parts, followed by a novel diffusion-based model called HoloPart to complete these segments. HoloPart employs a specialized architecture to ensure both detailed part geometry and overall shape consistency, achieving superior results on new benchmarks compared to existing methods."
                },
                "zh": {
                    "title": "çªç ´3Dåˆ†å‰²ï¼šHoloPartæ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„3Déƒ¨åˆ†æ— æ¨¡æ€åˆ†å‰²ä»»åŠ¡ï¼Œæ—¨åœ¨å°†3Då½¢çŠ¶åˆ†è§£ä¸ºå®Œæ•´ä¸”å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„éƒ¨åˆ†ï¼Œå³ä½¿åœ¨è¢«é®æŒ¡çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°ã€‚ç°æœ‰çš„3Déƒ¨åˆ†åˆ†å‰²æ–¹æ³•ä»…èƒ½è¯†åˆ«å¯è§çš„è¡¨é¢ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å®ç”¨çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œé¦–å…ˆåˆ©ç”¨ç°æœ‰çš„3Déƒ¨åˆ†åˆ†å‰²è·å–åˆæ­¥çš„ä¸å®Œæ•´éƒ¨åˆ†ï¼Œç„¶åå¼•å…¥HoloPartæ¨¡å‹ï¼Œé€šè¿‡æ‰©æ•£æ–¹æ³•å®Œæˆè¿™äº›éƒ¨åˆ†ã€‚HoloParté‡‡ç”¨äº†ä¸“é—¨çš„æ¶æ„ï¼Œç»“åˆå±€éƒ¨æ³¨æ„åŠ›å’Œå…¨å±€å½¢çŠ¶ä¸€è‡´æ€§ï¼Œæ˜¾è‘—æå‡äº†3Déƒ¨åˆ†æ— æ¨¡æ€åˆ†å‰²çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07960",
            "title": "VisualCloze: A Universal Image Generation Framework via Visual\n  In-Context Learning",
            "url": "https://huggingface.co/papers/2504.07960",
            "abstract": "Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures.",
            "score": 11,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "a8b5331ac40d6a3d",
            "authors": [
                "Zhong-Yu Li",
                "Ruoyi Du",
                "Juncheng Yan",
                "Le Zhuo",
                "Zhen Li",
                "Peng Gao",
                "Zhanyu Ma",
                "Ming-Ming Cheng"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "Tsinghua University",
                "VCIP, CS, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07960.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#graphs",
                    "#transfer_learning",
                    "#diffusion",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "VisualCloze: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "VisualCloze - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Graph200K - Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. VisualCloze Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ñ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "VisualCloze: Bridging Tasks with Visual Learning in Image Generation",
                    "desc": "This paper introduces VisualCloze, a universal image generation framework that overcomes the limitations of task-specific models by supporting a variety of in-domain tasks and generalizing to unseen tasks. It addresses challenges such as task instruction ambiguity and sparse task distributions by utilizing visual in-context learning, which allows models to learn from visual examples rather than language instructions. The authors also present Graph200K, a dataset that enhances task density and facilitates knowledge transfer across related tasks. Additionally, they demonstrate that their unified image generation approach aligns with image infilling, enabling the use of pre-trained models for improved generative performance."
                },
                "zh": {
                    "title": "VisualClozeï¼šé€šç”¨å›¾åƒç”Ÿæˆçš„æ–°æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVisualClozeçš„é€šç”¨å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å­˜åœ¨çš„æ•ˆç‡å’Œé€šç”¨æ€§é—®é¢˜ã€‚ä¸ä¼ ç»Ÿä¾èµ–è¯­è¨€æŒ‡ä»¤çš„æ–¹æ³•ä¸åŒï¼ŒVisualClozeé€šè¿‡è§†è§‰ç¤ºä¾‹è¿›è¡Œä»»åŠ¡è¯†åˆ«ï¼Œä»è€Œå‡å°‘äº†ä»»åŠ¡æ¨¡ç³Šæ€§å’Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†å¢å¼ºä»»åŠ¡ä¹‹é—´çš„å¯è½¬ç§»çŸ¥è¯†ï¼Œæˆ‘ä»¬å¼•å…¥äº†Graph200Kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é€šè¿‡å›¾ç»“æ„å»ºç«‹äº†å¤šç§ç›¸å…³ä»»åŠ¡ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„å›¾åƒç”Ÿæˆæ–¹æ³•ä¸å›¾åƒå¡«å……å…·æœ‰ä¸€è‡´çš„ç›®æ ‡ï¼Œä»è€Œèƒ½å¤Ÿåˆ©ç”¨é¢„è®­ç»ƒå¡«å……æ¨¡å‹çš„å¼ºç”Ÿæˆå…ˆéªŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07957",
            "title": "MM-IFEngine: Towards Multimodal Instruction Following",
            "url": "https://huggingface.co/papers/2504.07957",
            "abstract": "The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2%), MIA (+7.6%), and IFEval (+12.3%). The full data and evaluation code will be released on https://github.com/SYuan03/MM-IFEngine.",
            "score": 8,
            "issue_id": 3184,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "dbfc6cfeb60e05d7",
            "authors": [
                "Shengyuan Ding",
                "Shenxi Wu",
                "Xiangyu Zhao",
                "Yuhang Zang",
                "Haodong Duan",
                "Xiaoyi Dong",
                "Pan Zhang",
                "Yuhang Cao",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "CPII under InnoHK",
                "Fudan University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiaotong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07957.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#dataset",
                    "#data",
                    "#optimization",
                    "#open_source",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MM-IFEngine - Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MM-IFInstruct-23k Ğ¸ MM-IFDPO-23k Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MM-IFEval Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Instruction Following in MLLMs with MM-IFEngine",
                    "desc": "This paper introduces MM-IFEngine, a new method for generating high-quality image-instruction pairs to improve the instruction-following ability of Multi-modal Large Language Models (MLLMs). The authors create a large dataset called MM-IFInstruct-23k, which is designed for Supervised Fine-Tuning (SFT) and an extended version for Direct Preference Optimization (DPO). They also present MM-IFEval, a benchmark that evaluates MLLMs on complex tasks with both output and input constraints. Experiments show that fine-tuning on their datasets significantly enhances performance on various instruction-following benchmarks."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšèƒ½åŠ›è¯„ä¼°æ–¹æ³•ï¼Œç§°ä¸ºMM-IFEngineã€‚è¯¥æ–¹æ³•ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒ-æŒ‡ä»¤å¯¹ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®é›†MM-IFInstruct-23kï¼Œé€‚ç”¨äºç›‘ç£å¾®è°ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–ã€‚è®ºæ–‡è¿˜æå‡ºäº†MM-IFEvalï¼Œä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€åŸºå‡†ï¼ŒåŒ…å«è¾“å‡ºå“åº”å’Œè¾“å…¥å›¾åƒçš„çº¦æŸã€‚é€šè¿‡å®éªŒï¼Œå¾®è°ƒåçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07830",
            "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in\n  Multi-Agent Simulations",
            "url": "https://huggingface.co/papers/2504.07830",
            "abstract": "We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.",
            "score": 8,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "85dbaddf009300e0",
            "authors": [
                "Genglin Liu",
                "Salman Rahman",
                "Elisa Kreiss",
                "Marzyeh Ghassemi",
                "Saadia Gabriel"
            ],
            "affiliations": [
                "MIT CSAIL",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07830.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#graphs",
                    "#reasoning",
                    "#games",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "Ğ¦Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ´ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¾Ğ¼ Ğ˜Ğ˜",
                    "desc": "MOSAIC - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ Ğ²Ğ¾Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "MOSAIC: Simulating Social Networks to Combat Misinformation",
                    "desc": "The paper introduces MOSAIC, an innovative open-source framework for simulating social networks using generative language agents. These agents predict user behaviors like liking and sharing content, allowing researchers to study how deception emerges in online interactions. By creating detailed user personas, the framework facilitates large-scale simulations of content spread and user engagement. The study evaluates various content moderation strategies, revealing that they can effectively reduce misinformation while enhancing user interaction."
                },
                "zh": {
                    "title": "MOSAICï¼šç¤¾äº¤ç½‘ç»œè¡Œä¸ºæ¨¡æ‹Ÿä¸å†…å®¹å®¡æ ¸æ–°æ¢ç´¢",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼€æºç¤¾äº¤ç½‘ç»œæ¨¡æ‹Ÿæ¡†æ¶MOSAICï¼Œåˆ©ç”¨ç”Ÿæˆè¯­è¨€ä»£ç†é¢„æµ‹ç”¨æˆ·è¡Œä¸ºï¼Œå¦‚ç‚¹èµã€åˆ†äº«å’Œæ ‡è®°å†…å®¹ã€‚è¯¥æ¨¡æ‹Ÿç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†å’Œæœ‰å‘ç¤¾äº¤å›¾ï¼Œåˆ†ææ–°å‡ºç°çš„æ¬ºéª—è¡Œä¸ºï¼Œå¸®åŠ©ç†è§£ç”¨æˆ·å¦‚ä½•åˆ¤æ–­åœ¨çº¿ç¤¾äº¤å†…å®¹çš„çœŸå®æ€§ã€‚é€šè¿‡æ„å»ºå¤šæ ·åŒ–çš„ç»†ç²’åº¦ç”¨æˆ·ç”»åƒï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿæ”¯æŒå¤§è§„æ¨¡çš„å¤šä»£ç†æ¨¡æ‹Ÿï¼Œæ¨¡æ‹Ÿå†…å®¹ä¼ æ’­å’Œç”¨æˆ·å‚ä¸çš„åŠ¨æ€ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸‰ç§ä¸åŒçš„å†…å®¹å®¡æ ¸ç­–ç•¥ï¼Œå‘ç°å®ƒä»¬ä¸ä»…èƒ½å‡ç¼“è™šå‡ä¿¡æ¯çš„ä¼ æ’­ï¼Œè¿˜èƒ½æé«˜ç”¨æˆ·å‚ä¸åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07964",
            "title": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing",
            "url": "https://huggingface.co/papers/2504.07964",
            "abstract": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time optimization methods to re-weight or \"re-mixing\" the experts in different layers jointly for each test sample. Since the test sample's ground truth is unknown, we propose to optimize a surrogate objective defined by the sample's \"successful neighbors\" from a reference set of samples. We introduce three surrogates and algorithms based on mode-finding, kernel regression, and the average loss of similar reference samples/tasks. To reduce the cost of optimizing whole pathways, we apply our algorithms merely to the core experts' mixing weights in critical layers, which enjoy similar performance but save significant computation. This leads to \"Critical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE LLMs and examine it on six widely-used benchmarks. It consistently improves the base model by 7-15% in accuracy and outperforms widely used test-time learning baselines, e.g., in-context learning and prompt/prefix tuning, by a large margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to outperform LLMs of 7-9B parameters, hence improving MoE's advantages on efficiency. Our thorough ablation study further sheds novel insights on achieving test-time improvement on MoE.",
            "score": 6,
            "issue_id": 3184,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "1f6d28b26eec9879",
            "authors": [
                "Zhongyang Li",
                "Ziyue Li",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Johns Hopkins University",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07964.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE LLM) Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 10-20%. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ C3PO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ĞµÑĞ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ½ÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° 'ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… ÑĞ¾ÑĞµĞ´ÑÑ…' Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ C3PO Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ MoE LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 7-15% Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Optimize Expert Pathways for Better Performance!",
                    "desc": "This paper addresses the issue of sub-optimal expert pathways in Mixture-of-Experts (MoE) Large Language Models (LLMs), which can lead to a significant accuracy gap during inference. The authors propose a new method called C3PO, which optimizes the mixing weights of core experts in critical layers for each test sample using surrogate objectives based on similar reference samples. By focusing on optimizing only the essential components, C3PO achieves notable accuracy improvements of 7-15% over baseline models while maintaining computational efficiency. The results demonstrate that C3PO allows smaller MoE models to outperform larger LLMs, highlighting its effectiveness in enhancing model performance at lower resource costs."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ··åˆä¸“å®¶æ¨¡å‹çš„å…³é”®è·¯å¾„",
                    "desc": "æ··åˆä¸“å®¶ï¼ˆMoEï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸“å®¶è·¯å¾„é€‰æ‹©ä¸Šå­˜åœ¨æ˜¾è‘—çš„ä¼˜åŒ–ä¸è¶³ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œç®€å•çš„ä¸“å®¶é€‰æ‹©æ–¹æ³•åœ¨é¢„è®­ç»ƒé˜¶æ®µä¼šå¯¼è‡´10-20%çš„å‡†ç¡®ç‡å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯ä¸ªæµ‹è¯•æ ·æœ¬çš„ä¸åŒå±‚æ¬¡çš„ä¸“å®¶è¿›è¡Œé‡æ–°åŠ æƒæˆ–â€œé‡æ–°æ··åˆâ€ã€‚è¿™ç§æ–¹æ³•ç§°ä¸ºâ€œå…³é”®å±‚ã€æ ¸å¿ƒä¸“å®¶ã€åä½œè·¯å¾„ä¼˜åŒ–ï¼ˆC3POï¼‰â€ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å‡†ç¡®ç‡æå‡ï¼Œå¹¶ä¸”åœ¨è®¡ç®—æ•ˆç‡ä¸Šä¼˜äºä¼ ç»Ÿçš„å­¦ä¹ æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07934",
            "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual\n  Reasoning Self-Improvement",
            "url": "https://huggingface.co/papers/2504.07934",
            "abstract": "In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL.",
            "score": 5,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "419fda5d0a4bcadf",
            "authors": [
                "Xiyao Wang",
                "Zhengyuan Yang",
                "Chao Feng",
                "Hongjin Lu",
                "Linjie Li",
                "Chung-Ching Lin",
                "Kevin Lin",
                "Furong Huang",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Microsoft",
                "University of Maryland, College Park",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07934.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#data",
                    "#reasoning",
                    "#training",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RFT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ (MCTS) Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ThinkLite-VL, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 11 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ 7B Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning with Fewer Samples through Smart Data Selection",
                    "desc": "This paper introduces a novel method to improve visual reasoning in models using fewer training samples through self-improvement techniques, avoiding knowledge distillation. The authors emphasize the importance of sample difficulty during reinforcement fine-tuning (RFT), suggesting that challenging samples can enhance reasoning capabilities even with limited data. They propose a unique application of Monte Carlo Tree Search (MCTS) to quantify sample difficulty, allowing for effective data filtering. The resulting model, ThinkLite-VL, demonstrates a 7% performance increase over its predecessor using only 11k samples, achieving state-of-the-art results in various benchmarks."
                },
                "zh": {
                    "title": "ç”¨å°‘é‡æ ·æœ¬æå‡è§†è§‰æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªæˆ‘æ”¹è¿›æ¥å¢å¼ºè§†è§‰æ¨ç†ï¼Œä¸”æ‰€éœ€çš„è®­ç»ƒæ ·æœ¬æ˜¾è‘—å‡å°‘ï¼Œä¸ä¾èµ–çŸ¥è¯†è’¸é¦ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰è¿‡ç¨‹ä¸­ï¼Œè®­ç»ƒæ•°æ®çš„éš¾åº¦è‡³å…³é‡è¦ï¼Œé€‚å½“å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬å¯ä»¥æ˜¾è‘—æå‡æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹å¼ï¼Œåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥é‡åŒ–æ ·æœ¬çš„éš¾åº¦ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„æ•°æ®ç­›é€‰ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹ThinkLite-VLåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½¿ç”¨ä»…11kä¸ªè®­ç»ƒæ ·æœ¬ï¼Œå¹³å‡æ€§èƒ½æå‡äº†7%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.04974",
            "title": "Towards Visual Text Grounding of Multimodal Large Language Model",
            "url": "https://huggingface.co/papers/2504.04974",
            "abstract": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to their complex layouts and textual content. However, current benchmarks do not fully address these challenges, as they mostly focus on visual grounding on natural images, rather than text-rich document images. Thus, to bridge this gap, we introduce TRIG, a novel task with a newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering. Specifically, we propose an OCR-LLM-human interaction pipeline to create 800 manually annotated question-answer pairs as a benchmark and a large-scale training set of 90$ synthetic data based on four diverse datasets. A comprehensive evaluation of various MLLMs on our proposed benchmark exposes substantial limitations in their grounding capability on text-rich images. In addition, we propose two simple and effective TRIG methods based on general instruction tuning and plug-and-play efficient embedding, respectively. By finetuning MLLMs on our synthetic dataset, they promisingly improve spatial reasoning and grounding capabilities.",
            "score": 1,
            "issue_id": 3184,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 7",
                "zh": "4æœˆ7æ—¥"
            },
            "hash": "f382ad07e678a558",
            "authors": [
                "Ming Li",
                "Ruiyi Zhang",
                "Jian Chen",
                "Jiuxiang Gu",
                "Yufan Zhou",
                "Franck Dernoncourt",
                "Wanrong Zhu",
                "Tianyi Zhou",
                "Tong Sun"
            ],
            "affiliations": [
                "Adobe Research",
                "University at Buffalo",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.04974.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#dataset",
                    "#synthetic",
                    "#transfer_learning",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ TRIG Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 800 Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸ 90 Ñ‚Ñ‹ÑÑÑ‡ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑ†ĞµĞ½ĞºĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… MLLM Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞµ Ğº Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ… Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ plug-and-play."
                },
                "en": {
                    "title": "Enhancing MLLMs for Text-Rich Image Grounding",
                    "desc": "This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in understanding text-rich images, such as documents and infographics. The authors introduce a new task called TRIG, which focuses on improving the grounding of text in these complex images for better document question-answering. They create a benchmark dataset with 800 annotated question-answer pairs and 90 synthetic data samples to evaluate MLLMs' performance. Additionally, the paper presents two methods to enhance MLLMs' spatial reasoning and grounding abilities through fine-tuning on the new dataset."
                },
                "zh": {
                    "title": "æå‡æ–‡æ¡£å›¾åƒçš„æ–‡æœ¬å®šä½èƒ½åŠ›",
                    "desc": "å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†åœ¨è§†è§‰æ–‡æœ¬å®šä½æ–¹é¢ä»ç„¶å­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ–‡æœ¬ä¸°å¯Œçš„æ–‡æ¡£å›¾åƒä¸­ã€‚æ–‡æ¡£å›¾åƒå¦‚æ‰«æè¡¨å•å’Œä¿¡æ¯å›¾è¡¨ç”±äºå…¶å¤æ‚çš„å¸ƒå±€å’Œæ–‡æœ¬å†…å®¹ï¼Œå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TRIGä»»åŠ¡ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ–°çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œä»¥è¯„ä¼°å’Œæå‡MLLMsåœ¨æ–‡æ¡£é—®ç­”ä¸­çš„æ–‡æœ¬ä¸°å¯Œå›¾åƒå®šä½èƒ½åŠ›ã€‚é€šè¿‡å¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç©ºé—´æ¨ç†å’Œå®šä½èƒ½åŠ›ä¸Šæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-10.html",
    "link_next": "2025-04-14.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "10.04",
        "en": "04/10",
        "zh": "4æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "14.04",
        "en": "04/14",
        "zh": "4æœˆ14æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 5,
        "#transfer_learning": 2,
        "#graphs": 2,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æ‰©æ•£å˜å‹å™¨åœ¨ç”Ÿæˆè´¨é‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†éœ€è¦æ›´é•¿çš„è®­ç»ƒè¿­ä»£å’Œå¤šæ¬¡æ¨ç†æ­¥éª¤ã€‚æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­ï¼Œæ‰©æ•£å˜å‹å™¨ç¼–ç å™ªå£°è¾“å…¥ä»¥æå–ä½é¢‘è¯­ä¹‰æˆåˆ†ï¼Œç„¶åè§£ç é«˜é¢‘æˆåˆ†ã€‚è¿™ç§æ–¹æ¡ˆå¯¼è‡´ä¼˜åŒ–å›°å¢ƒï¼šç¼–ç ä½é¢‘è¯­ä¹‰éœ€è¦å‡å°‘é«˜é¢‘æˆåˆ†ï¼Œé€ æˆè¯­ä¹‰ç¼–ç å’Œé«˜é¢‘è§£ç ä¹‹é—´çš„çŸ›ç›¾ã€‚æˆ‘ä»¬æå‡ºä¸€ç§æ–°çš„è§£è€¦æ‰©æ•£å˜å‹å™¨ï¼ˆDDTï¼‰ï¼Œå…·æœ‰ä¸“é—¨çš„æ¡ä»¶ç¼–ç å™¨å’Œé€Ÿåº¦è§£ç å™¨ã€‚å®éªŒæ˜¾ç¤ºï¼Œæ›´å¤§çš„ç¼–ç å™¨éšç€æ¨¡å‹è§„æ¨¡å¢åŠ æé«˜æ€§èƒ½ã€‚åœ¨ImageNet 256x256ä¸Šï¼ŒDDT-XL/2è¾¾åˆ°1.31 FIDï¼ˆè®­ç»ƒæ”¶æ•›é€Ÿåº¦è¿‘ä¹å¿«4å€ï¼‰ï¼›åœ¨ImageNet 512x512ä¸Šï¼Œè¾¾åˆ°1.28 FIDã€‚æ­¤å¤–ï¼Œè§£è€¦æ¶æ„æé«˜æ¨ç†é€Ÿåº¦ï¼Œå…è®¸ç›¸é‚»å»å™ªæ­¥éª¤é—´å…±äº«è‡ªæˆ‘æ¡ä»¶ã€‚æˆ‘ä»¬æå‡ºä¸€ç§æ–°çš„ç»Ÿè®¡åŠ¨æ€è§„åˆ’æ–¹æ³•æ¥æœ€å°åŒ–æ€§èƒ½ä¸‹é™ã€‚",
        "title": "DDT: Decoupled Diffusion Transformer",
        "pinyin": "æ‰©æ•£å˜å‹å™¨åœ¨ç”Ÿæˆè´¨é‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†éœ€è¦æ›´é•¿çš„è®­ç»ƒè¿­ä»£å’Œå¤šæ¬¡æ¨ç†æ­¥éª¤ã€‚æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­ï¼Œæ‰©æ•£å˜å‹å™¨ç¼–ç å™ªå£°è¾“å…¥ä»¥æå–ä½é¢‘è¯­ä¹‰æˆåˆ†ï¼Œç„¶åè§£ç é«˜é¢‘æˆåˆ†ã€‚è¿™ç§æ–¹æ¡ˆå¯¼è‡´ä¼˜åŒ–å›°å¢ƒï¼šç¼–ç ä½é¢‘è¯­ä¹‰éœ€è¦å‡å°‘é«˜é¢‘æˆåˆ†ï¼Œé€ æˆè¯­ä¹‰ç¼–ç å’Œé«˜é¢‘è§£ç ä¹‹é—´çš„çŸ›ç›¾ã€‚æˆ‘ä»¬æå‡ºä¸€ç§æ–°çš„è§£è€¦æ‰©æ•£å˜å‹å™¨ï¼ˆDDTï¼‰ï¼Œå…·æœ‰ä¸“é—¨çš„æ¡ä»¶ç¼–ç å™¨å’Œé€Ÿåº¦è§£ç å™¨ã€‚å®éªŒæ˜¾ç¤ºï¼Œæ›´å¤§çš„ç¼–ç å™¨éšç€æ¨¡å‹è§„æ¨¡å¢åŠ æé«˜æ€§èƒ½ã€‚åœ¨ImageNet 256x256ä¸Šï¼ŒDDT-XL/2è¾¾åˆ°1.31 FIDï¼ˆè®­ç»ƒæ”¶æ•›é€Ÿåº¦è¿‘ä¹å¿«4å€ï¼‰ï¼›åœ¨ImageNet 512x512ä¸Šï¼Œè¾¾åˆ°1.28 FIDã€‚æ­¤å¤–ï¼Œè§£è€¦æ¶æ„æé«˜æ¨ç†é€Ÿåº¦ï¼Œå…è®¸ç›¸é‚»å»å™ªæ­¥éª¤é—´å…±äº«è‡ªæˆ‘æ¡ä»¶ã€‚æˆ‘ä»¬æå‡ºä¸€ç§æ–°çš„ç»Ÿè®¡åŠ¨æ€è§„åˆ’æ–¹æ³•æ¥æœ€å°åŒ–æ€§èƒ½ä¸‹é™ã€‚\n\nkuÃ² sÃ n biÃ n yÄ qÃ¬ zÃ i shÄ“ng chÃ©ng zhÃ¬ liÃ ng shÃ ng biÇo xiÃ n chÅ« sÃ¨, dÃ n xÅ« yÃ o gÃ¨ng chÃ¡ng de xÃ¹n liÃ n diÃ© dÇi hÃ© duÅ cÃ¬ tuÄ« lÇ bÃ¹ zhÃ²u. mÄ›i gÃ¨ qÃ¹ zÃ o bÃ¹ zhÃ²u zhÅng, kuÃ² sÃ n biÃ n yÄ qÃ¬ biÄn mÇ shÄ“ng yÄ«n yÃ¹n tÇ yÇ tÃ­ qÇ” dÄ« pÃ­n yÇ” yÃ¬ chÃ©ng fÄ“n, rÃ¡n hÃ²u jiÄ› mÇ gÄo pÃ­n chÃ©ng fÄ“n. zhÃ¨ zhÇ’ng fÄng Ã n dÇo zhÃ¬ yÃ²u huÃ  kÃ¹n jÃ¬ng: biÄn mÇ dÄ« pÃ­n yÇ” yÃ¬ xÅ« yÃ o jiÇn shÇo gÄo pÃ­n chÃ©ng fÄ“n, zÃ o chÃ©ng yÇ” yÃ¬ biÄn mÇ hÃ© gÄo pÃ­n jiÄ› mÇ zhÄ« jiÄn de mÃ¡o dÃ¹n. wÇ’ men tÃ­ chÅ« yÄ« zhÇ’ng xÄ«n de jiÄ› kÇ’u kuÃ² sÃ n biÃ n yÄ qÃ¬ (DDT), jÃ¹ yÇ’u zhuÄn mÃ©n de tiÃ¡o jiÃ n biÄn mÇ qÃ¬ hÃ© sÃ¹ dÃ¹ jiÄ› mÇ qÃ¬. shÃ­ yÃ n shÃ¬ zhÃ¹, gÃ¨ng dÃ  de biÄn mÇ qÃ¬ suÃ­ zhÄ› mÃ³ xÃ­ng guÄ« mÃ³ zÄ“ng jiÄ tÃ­ gÄo xÃ­ng nÃ©ng. zÃ i ImageNet 256x256 shÃ ng, DDT-XL/2 dÃ¡ dÃ o 1.31 FID (xÃ¹n liÃ n shÅu liÇn sÃ¹ dÃ¹ jÃ¬n hÅ« kuÃ i 4 bÃ¨i); zÃ i ImageNet 512x512 shÃ ng, dÃ¡ dÃ o 1.28 FID. cÇ wÃ i, jiÄ› kÇ’u jiÃ  gÃ²u tÃ­ gÄo tuÄ« lÇ sÃ¹ dÃ¹, yÇ”n xÇ” xiÄng lÃ­n qÃ¹ zÃ o bÃ¹ zhÃ²u jiÄn gÃ²ng xiÇng zÃ¬ wÇ’ tiÃ¡o jiÃ n. wÇ’ men tÃ­ chÅ« yÄ« zhÇ’ng xÄ«n de tÇ’ng jÃ¬ dÃ²ng tÃ i guÄ« huÃ  fÄng fÇ lÃ¡i zuÃ¬ shÇo huÃ  xÃ­ng nÃ©ng xiÃ  jiÃ ng.",
        "vocab": "[{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ² sÃ n', 'trans': 'diffusion'},\n{'word': 'å˜å‹å™¨', 'pinyin': 'biÃ n yÄ qÃ¬', 'trans': 'transformer'},\n{'word': 'è´¨é‡', 'pinyin': 'zhÃ¬ liÃ ng', 'trans': 'quality'},\n{'word': 'å‡ºè‰²', 'pinyin': 'chÅ« sÃ¨', 'trans': 'outstanding'},\n{'word': 'è¿­ä»£', 'pinyin': 'diÃ© dÃ i', 'trans': 'iteration'},\n{'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'inference'},\n{'word': 'å»å™ª', 'pinyin': 'qÃ¹ zÃ o', 'trans': 'denoising'},\n{'word': 'ç¼–ç ', 'pinyin': 'biÄn mÇ', 'trans': 'encode'},\n{'word': 'è¯­ä¹‰', 'pinyin': 'yÇ” yÃ¬', 'trans': 'semantic'},\n{'word': 'æˆåˆ†', 'pinyin': 'chÃ©ng fÃ¨n', 'trans': 'component'},\n{'word': 'è§£ç ', 'pinyin': 'jiÄ› mÇ', 'trans': 'decode'},\n{'word': 'ä¼˜åŒ–', 'pinyin': 'yÅu huÃ ', 'trans': 'optimization'},\n{'word': 'å›°å¢ƒ', 'pinyin': 'kÃ¹n jÃ¬ng', 'trans': 'dilemma'},\n{'word': 'å‡å°‘', 'pinyin': 'jiÇn shÇo', 'trans': 'reduce'},\n{'word': 'çŸ›ç›¾', 'pinyin': 'mÃ¡o dÃ¹n', 'trans': 'contradiction'},\n{'word': 'è§£è€¦', 'pinyin': 'jiÄ› Ç’u', 'trans': 'decouple'},\n{'word': 'æ¡ä»¶', 'pinyin': 'tiÄo jiÃ n', 'trans': 'condition'},\n{'word': 'é€Ÿåº¦', 'pinyin': 'sÃ¹ dÃ¹', 'trans': 'speed'},\n{'word': 'è§„æ¨¡', 'pinyin': 'guÄ« mÃ³', 'trans': 'scale'},\n{'word': 'æ”¶æ•›', 'pinyin': 'shÅu liÇn', 'trans': 'convergence'},\n{'word': 'æ¶æ„', 'pinyin': 'jiÃ  gÃ²u', 'trans': 'architecture'},\n{'word': 'å…±äº«', 'pinyin': 'gÃ²ng xiÇng', 'trans': 'share'},\n{'word': 'è‡ªæˆ‘', 'pinyin': 'zÃ¬ wÇ’', 'trans': 'self'},\n{'word': 'åŠ¨æ€', 'pinyin': 'dÃ²ng tÃ i', 'trans': 'dynamic'},\n{'word': 'è§„åˆ’', 'pinyin': 'guÄ« huÃ ', 'trans': 'planning'},\n{'word': 'æœ€å°åŒ–', 'pinyin': 'zuÃ¬ xiÇo huÃ ', 'trans': 'minimize'},\n{'word': 'ä¸‹é™', 'pinyin': 'xiÃ  jiÃ ng', 'trans': 'decrease'}]",
        "trans": "The diffusion transformer performs excellently in terms of generation quality but requires longer training iterations and multiple inference steps. In each denoising step, the diffusion transformer encodes noisy input to extract low-frequency semantic components and then decodes high-frequency components. This scheme leads to an optimization dilemma: encoding low-frequency semantics requires reducing high-frequency components, creating a conflict between semantic encoding and high-frequency decoding. We propose a new decoupled diffusion transformer (DDT) with dedicated conditional encoders and velocity decoders. Experiments show that larger encoders improve performance as the model scale increases. On ImageNet 256x256, DDT-XL/2 achieves 1.31 FID (with training convergence speed nearly 4 times faster); on ImageNet 512x512, it achieves 1.28 FID. Additionally, the decoupled architecture increases inference speed, allowing adjacent denoising steps to share self-conditioning. We propose a new statistical dynamic programming method to minimize performance degradation.",
        "update_ts": "2025-04-10 09:12"
    }
}