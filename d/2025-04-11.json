{
    "date": {
        "ru": "11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 11",
        "zh": "4æœˆ11æ—¥"
    },
    "time_utc": "2025-04-11 16:13",
    "weekday": 4,
    "issue_id": 3196,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.07491",
            "title": "Kimi-VL Technical Report",
            "url": "https://huggingface.co/papers/2504.07491",
            "abstract": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as a general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including college-level image and video comprehension, OCR, mathematical reasoning, and multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With a 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong long-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8 on MathVision, and 71.3 on MathVista while maintaining the compact 2.8B activated LLM parameters, setting a new standard for efficient multimodal thinking models. Code and models are publicly accessible at https://github.com/MoonshotAI/Kimi-VL.",
            "score": 64,
            "issue_id": 3185,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "e1d1e4add50955e8",
            "authors": [
                "Kimi Team",
                "Angang Du",
                "Bohong Yin",
                "Bowei Xing",
                "Bowen Qu",
                "Bowen Wang",
                "Cheng Chen",
                "Chenlin Zhang",
                "Chenzhuang Du",
                "Chu Wei",
                "Congcong Wang",
                "Dehao Zhang",
                "Dikang Du",
                "Dongliang Wang",
                "Enming Yuan",
                "Enzhe Lu",
                "Fang Li",
                "Flood Sung",
                "Guangda Wei",
                "Guokun Lai",
                "Han Zhu",
                "Hao Ding",
                "Hao Hu",
                "Hao Yang",
                "Hao Zhang",
                "Haoning Wu",
                "Haotian Yao",
                "Haoyu Lu",
                "Heng Wang",
                "Hongcheng Gao",
                "Huabin Zheng",
                "Jiaming Li",
                "Jianlin Su",
                "Jianzhou Wang",
                "Jiaqi Deng",
                "Jiezhong Qiu",
                "Jin Xie",
                "Jinhong Wang",
                "Jingyuan Liu",
                "Junjie Yan",
                "Kun Ouyang",
                "Liang Chen",
                "Lin Sui",
                "Longhui Yu",
                "Mengfan Dong",
                "Mengnan Dong",
                "Nuo Xu",
                "Pengyu Cheng",
                "Qizheng Gu",
                "Runjie Zhou",
                "Shaowei Liu",
                "Sihan Cao",
                "Tao Yu",
                "Tianhui Song",
                "Tongtong Bai",
                "Wei Song",
                "Weiran He",
                "Weixiao Huang",
                "Weixin Xu",
                "Xiaokun Yuan",
                "Xingcheng Yao",
                "Xingzhe Wu",
                "Xinxing Zu",
                "Xinyu Zhou",
                "Xinyuan Wang",
                "Y. Charles",
                "Yan Zhong",
                "Yang Li",
                "Yangyang Hu",
                "Yanru Chen",
                "Yejie Wang",
                "Yibo Liu",
                "Yibo Miao",
                "Yidao Qin",
                "Yimin Chen",
                "Yiping Bao",
                "Yiqin Wang",
                "Yongsheng Kang",
                "Yuanxin Liu",
                "Yulun Du",
                "Yuxin Wu",
                "Yuzhi Wang",
                "Yuzi Yan",
                "Zaida Zhou",
                "Zhaowei Li",
                "Zhejun Jiang",
                "Zheng Zhang",
                "Zhilin Yang",
                "Zhiqi Huang",
                "Zihao Huang",
                "Zijia Zhao",
                "Ziwei Chen"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.07491.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#agents",
                    "#rl",
                    "#multimodal",
                    "#long_context",
                    "#reasoning",
                    "#small_models",
                    "#training",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Kimi-VL: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Kimi-VL - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Mixture-of-Experts (MoE) Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Kimi-VL Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾ĞºĞ½Ğ¾Ğ¼ Ğ² 128K Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Kimi-VL-Thinking, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Kimi-VL: Efficient Multimodal Mastery with Long-Context Reasoning",
                    "desc": "Kimi-VL is a cutting-edge Mixture-of-Experts vision-language model that efficiently combines multimodal reasoning and long-context understanding while using only 2.8 billion parameters in its language decoder. It excels in various complex tasks, including multi-turn interactions and advanced image and video comprehension, outperforming other leading models in several areas. The model features a 128K extended context window, allowing it to process long inputs effectively, and its native-resolution vision encoder enhances its ability to interpret high-resolution visuals. Additionally, the Kimi-VL-Thinking variant improves long-horizon reasoning through supervised fine-tuning and reinforcement learning, setting a new benchmark for efficient multimodal models."
                },
                "zh": {
                    "title": "Kimi-VLï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€æ¨ç†æ–°æ ‡å‡†",
                    "desc": "Kimi-VLæ˜¯ä¸€ç§é«˜æ•ˆçš„å¼€æºæ··åˆä¸“å®¶ï¼ˆMoEï¼‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå…·å¤‡å…ˆè¿›çš„å¤šæ¨¡æ€æ¨ç†å’Œé•¿æ–‡æœ¬ç†è§£èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ä»»åŠ¡å’Œå„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿä¸é¡¶å°–æ¨¡å‹ç›¸åª²ç¾ã€‚Kimi-VLè¿˜å…·å¤‡å¤„ç†é•¿ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†å¤šè¾¾128Kçš„è¾“å…¥ï¼Œé€‚ç”¨äºå¤æ‚çš„è§†è§‰ç†è§£ä»»åŠ¡ã€‚é€šè¿‡é•¿é“¾æ€ç»´çš„ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼ŒKimi-VL-Thinkingè¿›ä¸€æ­¥æå‡äº†é•¿è¿œæ¨ç†èƒ½åŠ›ï¼Œè®¾å®šäº†é«˜æ•ˆå¤šæ¨¡æ€æ€ç»´æ¨¡å‹çš„æ–°æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07956",
            "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning",
            "url": "https://huggingface.co/papers/2504.07956",
            "abstract": "The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task.",
            "score": 32,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "88860725e51f3629",
            "authors": [
                "Yukun Qi",
                "Yiming Zhao",
                "Yu Zeng",
                "Xikun Bao",
                "Wenxuan Huang",
                "Lin Chen",
                "Zehui Chen",
                "Jie Zhao",
                "Zhongang Qi",
                "Feng Zhao"
            ],
            "affiliations": [
                "East China Normal University",
                "Huawei Noahs Ark Lab",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07956.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "VCR-Bench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VCR-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 859 Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ 1034 Ğ¿Ğ°Ñ€Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. VCR-Bench Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¾Ğ² Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°."
                },
                "en": {
                    "title": "VCR-Bench: Evaluating Video Reasoning in LVLMs",
                    "desc": "This paper introduces VCR-Bench, a new benchmark for evaluating Video Chain-of-Thought (CoT) reasoning in large vision-language models (LVLMs). It addresses the lack of rigorous evaluation frameworks for assessing how well these models can reason about video content. The benchmark includes 859 videos and 1,034 annotated question-answer pairs, each with a stepwise CoT rationale linked to perception or reasoning capabilities. Experiments reveal that current LVLMs struggle with video reasoning, particularly in processing temporal-spatial information, highlighting the need for improved models in this area."
                },
                "zh": {
                    "title": "VCR-Benchï¼šè§†é¢‘æ¨ç†çš„æ–°æ ‡å‡†",
                    "desc": "é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†çš„è¿›æ­¥æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹ä¸€ä¸ªä¸¥æ ¼çš„è§†é¢‘CoTæ¨ç†è¯„ä¼°æ¡†æ¶ï¼Œç°æœ‰çš„è§†é¢‘åŸºå‡†æ— æ³•å……åˆ†è¯„ä¼°æ¨ç†è¿‡ç¨‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†VCR-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°LVLMsåœ¨è§†é¢‘é“¾å¼æ€ç»´æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡859ä¸ªè§†é¢‘å’Œ1034å¯¹é«˜è´¨é‡é—®ç­”å¯¹ï¼ŒVCR-Benchä¸ºæ¯ä¸ªé—®ç­”å¯¹æä¾›äº†é€æ­¥çš„CoTæ¨ç†ä¾æ®ï¼Œæ­ç¤ºäº†å½“å‰LVLMsåœ¨å¤æ‚è§†é¢‘æ¨ç†ä¸­çš„å…³é”®ç“¶é¢ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07960",
            "title": "VisualCloze: A Universal Image Generation Framework via Visual\n  In-Context Learning",
            "url": "https://huggingface.co/papers/2504.07960",
            "abstract": "Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures.",
            "score": 29,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "a8b5331ac40d6a3d",
            "authors": [
                "Zhong-Yu Li",
                "Ruoyi Du",
                "Juncheng Yan",
                "Le Zhuo",
                "Zhen Li",
                "Peng Gao",
                "Zhanyu Ma",
                "Ming-Ming Cheng"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "Tsinghua University",
                "VCIP, CS, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07960.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#graphs",
                    "#transfer_learning",
                    "#diffusion",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "VisualCloze: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "VisualCloze - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Graph200K - Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. VisualCloze Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ñ… Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "VisualCloze: Bridging Tasks with Visual Learning in Image Generation",
                    "desc": "This paper introduces VisualCloze, a universal image generation framework that overcomes the limitations of task-specific models by supporting a variety of in-domain tasks and generalizing to unseen tasks. It addresses challenges such as task instruction ambiguity and sparse task distributions by utilizing visual in-context learning, which allows models to learn from visual examples rather than language instructions. The authors also present Graph200K, a dataset that enhances task density and facilitates knowledge transfer across related tasks. Additionally, they demonstrate that their unified image generation approach aligns with image infilling, enabling the use of pre-trained models for improved generative performance."
                },
                "zh": {
                    "title": "VisualClozeï¼šé€šç”¨å›¾åƒç”Ÿæˆçš„æ–°æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVisualClozeçš„é€šç”¨å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å½“å‰å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å­˜åœ¨çš„æ•ˆç‡å’Œé€šç”¨æ€§é—®é¢˜ã€‚ä¸ä¼ ç»Ÿä¾èµ–è¯­è¨€æŒ‡ä»¤çš„æ–¹æ³•ä¸åŒï¼ŒVisualClozeé€šè¿‡è§†è§‰ç¤ºä¾‹è¿›è¡Œä»»åŠ¡è¯†åˆ«ï¼Œä»è€Œå‡å°‘äº†ä»»åŠ¡æ¨¡ç³Šæ€§å’Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†å¢å¼ºä»»åŠ¡ä¹‹é—´çš„å¯è½¬ç§»çŸ¥è¯†ï¼Œæˆ‘ä»¬å¼•å…¥äº†Graph200Kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é€šè¿‡å›¾ç»“æ„å»ºç«‹äº†å¤šç§ç›¸å…³ä»»åŠ¡ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„å›¾åƒç”Ÿæˆæ–¹æ³•ä¸å›¾åƒå¡«å……å…·æœ‰ä¸€è‡´çš„ç›®æ ‡ï¼Œä»è€Œèƒ½å¤Ÿåˆ©ç”¨é¢„è®­ç»ƒå¡«å……æ¨¡å‹çš„å¼ºç”Ÿæˆå…ˆéªŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07128",
            "title": "DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning",
            "url": "https://huggingface.co/papers/2504.07128",
            "abstract": "Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly \"thinking\" about a problem before providing an answer. This reasoning process is publicly available to the user, creating endless opportunities for studying the reasoning behaviour of the model and opening up the field of Thoughtology. Starting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning, our analyses on DeepSeek-R1 investigate the impact and controllability of thought length, management of long or confusing contexts, cultural and safety concerns, and the status of DeepSeek-R1 vis-\\`a-vis cognitive phenomena, such as human-like language processing and world modelling. Our findings paint a nuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning, where extra inference time can impair model performance. Furthermore, we find a tendency for DeepSeek-R1 to persistently ruminate on previously explored problem formulations, obstructing further exploration. We also note strong safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning counterpart, which can also compromise safety-aligned LLMs.",
            "score": 26,
            "issue_id": 3184,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "6317a88ae7643fe2",
            "authors": [
                "Sara Vera MarjanoviÄ‡",
                "Arkil Patel",
                "Vaibhav Adlakha",
                "Milad Aghajohari",
                "Parishad BehnamGhader",
                "Mehar Bhatia",
                "Aditi Khandelwal",
                "Austin Kraft",
                "Benno Krojer",
                "Xing Han LÃ¹",
                "Nicholas Meade",
                "Dongchan Shin",
                "Amirhossein Kazemnejad",
                "Gaurav Kamath",
                "Marius Mosbach",
                "Karolina StaÅ„czak",
                "Siva Reddy"
            ],
            "affiliations": [
                "McGill University",
                "Mila Quebec AI Institute",
                "University of Copenhagen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07128.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#ethics",
                    "#inference",
                    "#reasoning",
                    "#long_context",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "DeepSeek-R1: Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑˆĞ¸Ğ½ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ DeepSeek-R1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¸ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñƒ DeepSeek-R1 ĞµÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒÑ…ÑƒĞ´ÑˆĞ°Ñ‚ÑŒÑÑ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ÑÑ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°ÑÑ‚Ñ€ĞµĞ²Ğ°Ñ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ½ĞµĞµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ»Ğ°Ğ½Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ±ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "DeepSeek-R1: Revolutionizing Reasoning in Language Models",
                    "desc": "The paper introduces DeepSeek-R1, a Large Reasoning Model that enhances the way language models (LLMs) tackle complex problems by generating multi-step reasoning chains. This model allows users to observe the reasoning process, fostering a new area of research called Thoughtology. The study examines various aspects of DeepSeek-R1, including the effects of reasoning length, context management, and safety issues, revealing that excessive inference time can negatively impact performance. Additionally, the findings highlight the model's tendency to dwell on previous problem formulations, which can hinder its ability to explore new solutions and raise safety concerns compared to traditional LLMs."
                },
                "zh": {
                    "title": "æ·±åº¦æ¨ç†ï¼Œæ€ç»´çš„æœªæ¥",
                    "desc": "DeepSeek-R1æ˜¯ä¸€ç§å¤§å‹æ¨ç†æ¨¡å‹ï¼Œæ ‡å¿—ç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶çš„æ ¹æœ¬è½¬å˜ã€‚å®ƒé€šè¿‡åˆ›å»ºè¯¦ç»†çš„å¤šæ­¥éª¤æ¨ç†é“¾æ¥â€œæ€è€ƒâ€é—®é¢˜ï¼Œè€Œä¸æ˜¯ç›´æ¥ç»™å‡ºç­”æ¡ˆã€‚è¿™ç§æ¨ç†è¿‡ç¨‹å¯¹ç”¨æˆ·æ˜¯å…¬å¼€çš„ï¼Œä¸ºç ”ç©¶æ¨¡å‹çš„æ¨ç†è¡Œä¸ºæä¾›äº†æ— é™å¯èƒ½ï¼Œå¹¶å¼€å¯äº†æ€ç»´å­¦ï¼ˆThoughtologyï¼‰é¢†åŸŸã€‚æˆ‘ä»¬çš„åˆ†ææ˜¾ç¤ºï¼ŒDeepSeek-R1åœ¨æ¨ç†æ—¶å­˜åœ¨ä¸€ä¸ªâ€œç”œèœœç‚¹â€ï¼Œè¿‡é•¿çš„æ¨ç†æ—¶é—´å¯èƒ½ä¼šå½±å“æ¨¡å‹çš„è¡¨ç°ï¼ŒåŒæ—¶å®ƒåœ¨å¤„ç†å·²æ¢ç´¢çš„é—®é¢˜æ—¶å®¹æ˜“é™·å…¥åå¤æ€è€ƒï¼Œå½±å“è¿›ä¸€æ­¥çš„æ¢ç´¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07957",
            "title": "MM-IFEngine: Towards Multimodal Instruction Following",
            "url": "https://huggingface.co/papers/2504.07957",
            "abstract": "The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2%), MIA (+7.6%), and IFEval (+12.3%). The full data and evaluation code will be released on https://github.com/SYuan03/MM-IFEngine.",
            "score": 24,
            "issue_id": 3184,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "dbfc6cfeb60e05d7",
            "authors": [
                "Shengyuan Ding",
                "Shenxi Wu",
                "Xiangyu Zhao",
                "Yuhang Zang",
                "Haodong Duan",
                "Xiaoyi Dong",
                "Pan Zhang",
                "Yuhang Cao",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "CPII under InnoHK",
                "Fudan University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiaotong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07957.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#dataset",
                    "#data",
                    "#optimization",
                    "#open_source",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MM-IFEngine - Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MM-IFInstruct-23k Ğ¸ MM-IFDPO-23k Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MM-IFEval Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Instruction Following in MLLMs with MM-IFEngine",
                    "desc": "This paper introduces MM-IFEngine, a new method for generating high-quality image-instruction pairs to improve the instruction-following ability of Multi-modal Large Language Models (MLLMs). The authors create a large dataset called MM-IFInstruct-23k, which is designed for Supervised Fine-Tuning (SFT) and an extended version for Direct Preference Optimization (DPO). They also present MM-IFEval, a benchmark that evaluates MLLMs on complex tasks with both output and input constraints. Experiments show that fine-tuning on their datasets significantly enhances performance on various instruction-following benchmarks."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æŒ‡ä»¤è·Ÿéšèƒ½åŠ›è¯„ä¼°æ–¹æ³•ï¼Œç§°ä¸ºMM-IFEngineã€‚è¯¥æ–¹æ³•ç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒ-æŒ‡ä»¤å¯¹ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®é›†MM-IFInstruct-23kï¼Œé€‚ç”¨äºç›‘ç£å¾®è°ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–ã€‚è®ºæ–‡è¿˜æå‡ºäº†MM-IFEvalï¼Œä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ¨¡æ€åŸºå‡†ï¼ŒåŒ…å«è¾“å‡ºå“åº”å’Œè¾“å…¥å›¾åƒçš„çº¦æŸã€‚é€šè¿‡å®éªŒï¼Œå¾®è°ƒåçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07943",
            "title": "HoloPart: Generative 3D Part Amodal Segmentation",
            "url": "https://huggingface.co/papers/2504.07943",
            "abstract": "3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Inspired by 2D amodal segmentation, we introduce this novel task to the 3D domain and propose a practical, two-stage approach, addressing the key challenges of inferring occluded 3D geometry, maintaining global shape consistency, and handling diverse shapes with limited training data. First, we leverage existing 3D part segmentation to obtain initial, incomplete part segments. Second, we introduce HoloPart, a novel diffusion-based model, to complete these segments into full 3D parts. HoloPart utilizes a specialized architecture with local attention to capture fine-grained part geometry and global shape context attention to ensure overall shape consistency. We introduce new benchmarks based on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart significantly outperforms state-of-the-art shape completion methods. By incorporating HoloPart with existing segmentation techniques, we achieve promising results on 3D part amodal segmentation, opening new avenues for applications in geometry editing, animation, and material assignment.",
            "score": 21,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "4cc1401ee10a171e",
            "authors": [
                "Yunhan Yang",
                "Yuan-Chen Guo",
                "Yukun Huang",
                "Zi-Xin Zou",
                "Zhipeng Yu",
                "Yangguang Li",
                "Yan-Pei Cao",
                "Xihui Liu"
            ],
            "affiliations": [
                "The University of Hong Kong",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07943.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#architecture",
                    "#diffusion",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸: Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ 3D-Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ HoloPart Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ 3D-Ñ‡Ğ°ÑÑ‚Ğ¸. HoloPart Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ñ‹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking Hidden Shapes: HoloPart for 3D Amodal Segmentation",
                    "desc": "This paper addresses the challenge of 3D part amodal segmentation, which involves identifying complete parts of a 3D shape even when some parts are hidden. Current methods only work with visible surfaces, limiting their effectiveness. The authors propose a two-stage approach that first uses existing segmentation techniques to identify incomplete parts, followed by a novel diffusion-based model called HoloPart to complete these segments. HoloPart employs a specialized architecture to ensure both detailed part geometry and overall shape consistency, achieving superior results on new benchmarks compared to existing methods."
                },
                "zh": {
                    "title": "çªç ´3Dåˆ†å‰²ï¼šHoloPartæ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„3Déƒ¨åˆ†æ— æ¨¡æ€åˆ†å‰²ä»»åŠ¡ï¼Œæ—¨åœ¨å°†3Då½¢çŠ¶åˆ†è§£ä¸ºå®Œæ•´ä¸”å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„éƒ¨åˆ†ï¼Œå³ä½¿åœ¨è¢«é®æŒ¡çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°ã€‚ç°æœ‰çš„3Déƒ¨åˆ†åˆ†å‰²æ–¹æ³•ä»…èƒ½è¯†åˆ«å¯è§çš„è¡¨é¢ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å®ç”¨çš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œé¦–å…ˆåˆ©ç”¨ç°æœ‰çš„3Déƒ¨åˆ†åˆ†å‰²è·å–åˆæ­¥çš„ä¸å®Œæ•´éƒ¨åˆ†ï¼Œç„¶åå¼•å…¥HoloPartæ¨¡å‹ï¼Œé€šè¿‡æ‰©æ•£æ–¹æ³•å®Œæˆè¿™äº›éƒ¨åˆ†ã€‚HoloParté‡‡ç”¨äº†ä¸“é—¨çš„æ¶æ„ï¼Œç»“åˆå±€éƒ¨æ³¨æ„åŠ›å’Œå…¨å±€å½¢çŠ¶ä¸€è‡´æ€§ï¼Œæ˜¾è‘—æå‡äº†3Déƒ¨åˆ†æ— æ¨¡æ€åˆ†å‰²çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07964",
            "title": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing",
            "url": "https://huggingface.co/papers/2504.07964",
            "abstract": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time optimization methods to re-weight or \"re-mixing\" the experts in different layers jointly for each test sample. Since the test sample's ground truth is unknown, we propose to optimize a surrogate objective defined by the sample's \"successful neighbors\" from a reference set of samples. We introduce three surrogates and algorithms based on mode-finding, kernel regression, and the average loss of similar reference samples/tasks. To reduce the cost of optimizing whole pathways, we apply our algorithms merely to the core experts' mixing weights in critical layers, which enjoy similar performance but save significant computation. This leads to \"Critical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE LLMs and examine it on six widely-used benchmarks. It consistently improves the base model by 7-15% in accuracy and outperforms widely used test-time learning baselines, e.g., in-context learning and prompt/prefix tuning, by a large margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to outperform LLMs of 7-9B parameters, hence improving MoE's advantages on efficiency. Our thorough ablation study further sheds novel insights on achieving test-time improvement on MoE.",
            "score": 14,
            "issue_id": 3184,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "1f6d28b26eec9879",
            "authors": [
                "Zhongyang Li",
                "Ziyue Li",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Johns Hopkins University",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07964.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE LLM) Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 10-20%. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ C3PO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ĞµÑĞ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ½ÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° 'ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… ÑĞ¾ÑĞµĞ´ÑÑ…' Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ C3PO Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ MoE LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 7-15% Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Optimize Expert Pathways for Better Performance!",
                    "desc": "This paper addresses the issue of sub-optimal expert pathways in Mixture-of-Experts (MoE) Large Language Models (LLMs), which can lead to a significant accuracy gap during inference. The authors propose a new method called C3PO, which optimizes the mixing weights of core experts in critical layers for each test sample using surrogate objectives based on similar reference samples. By focusing on optimizing only the essential components, C3PO achieves notable accuracy improvements of 7-15% over baseline models while maintaining computational efficiency. The results demonstrate that C3PO allows smaller MoE models to outperform larger LLMs, highlighting its effectiveness in enhancing model performance at lower resource costs."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ··åˆä¸“å®¶æ¨¡å‹çš„å…³é”®è·¯å¾„",
                    "desc": "æ··åˆä¸“å®¶ï¼ˆMoEï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸“å®¶è·¯å¾„é€‰æ‹©ä¸Šå­˜åœ¨æ˜¾è‘—çš„ä¼˜åŒ–ä¸è¶³ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œç®€å•çš„ä¸“å®¶é€‰æ‹©æ–¹æ³•åœ¨é¢„è®­ç»ƒé˜¶æ®µä¼šå¯¼è‡´10-20%çš„å‡†ç¡®ç‡å·®è·ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯ä¸ªæµ‹è¯•æ ·æœ¬çš„ä¸åŒå±‚æ¬¡çš„ä¸“å®¶è¿›è¡Œé‡æ–°åŠ æƒæˆ–â€œé‡æ–°æ··åˆâ€ã€‚è¿™ç§æ–¹æ³•ç§°ä¸ºâ€œå…³é”®å±‚ã€æ ¸å¿ƒä¸“å®¶ã€åä½œè·¯å¾„ä¼˜åŒ–ï¼ˆC3POï¼‰â€ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å‡†ç¡®ç‡æå‡ï¼Œå¹¶ä¸”åœ¨è®¡ç®—æ•ˆç‡ä¸Šä¼˜äºä¼ ç»Ÿçš„å­¦ä¹ æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07830",
            "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in\n  Multi-Agent Simulations",
            "url": "https://huggingface.co/papers/2504.07830",
            "abstract": "We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.",
            "score": 12,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "85dbaddf009300e0",
            "authors": [
                "Genglin Liu",
                "Salman Rahman",
                "Elisa Kreiss",
                "Marzyeh Ghassemi",
                "Saadia Gabriel"
            ],
            "affiliations": [
                "MIT CSAIL",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07830.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#graphs",
                    "#reasoning",
                    "#games",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "Ğ¦Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ´ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¾Ğ¼ Ğ˜Ğ˜",
                    "desc": "MOSAIC - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ Ğ²Ğ¾Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "MOSAIC: Simulating Social Networks to Combat Misinformation",
                    "desc": "The paper introduces MOSAIC, an innovative open-source framework for simulating social networks using generative language agents. These agents predict user behaviors like liking and sharing content, allowing researchers to study how deception emerges in online interactions. By creating detailed user personas, the framework facilitates large-scale simulations of content spread and user engagement. The study evaluates various content moderation strategies, revealing that they can effectively reduce misinformation while enhancing user interaction."
                },
                "zh": {
                    "title": "MOSAICï¼šç¤¾äº¤ç½‘ç»œè¡Œä¸ºæ¨¡æ‹Ÿä¸å†…å®¹å®¡æ ¸æ–°æ¢ç´¢",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼€æºç¤¾äº¤ç½‘ç»œæ¨¡æ‹Ÿæ¡†æ¶MOSAICï¼Œåˆ©ç”¨ç”Ÿæˆè¯­è¨€ä»£ç†é¢„æµ‹ç”¨æˆ·è¡Œä¸ºï¼Œå¦‚ç‚¹èµã€åˆ†äº«å’Œæ ‡è®°å†…å®¹ã€‚è¯¥æ¨¡æ‹Ÿç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†å’Œæœ‰å‘ç¤¾äº¤å›¾ï¼Œåˆ†ææ–°å‡ºç°çš„æ¬ºéª—è¡Œä¸ºï¼Œå¸®åŠ©ç†è§£ç”¨æˆ·å¦‚ä½•åˆ¤æ–­åœ¨çº¿ç¤¾äº¤å†…å®¹çš„çœŸå®æ€§ã€‚é€šè¿‡æ„å»ºå¤šæ ·åŒ–çš„ç»†ç²’åº¦ç”¨æˆ·ç”»åƒï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿæ”¯æŒå¤§è§„æ¨¡çš„å¤šä»£ç†æ¨¡æ‹Ÿï¼Œæ¨¡æ‹Ÿå†…å®¹ä¼ æ’­å’Œç”¨æˆ·å‚ä¸çš„åŠ¨æ€ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸‰ç§ä¸åŒçš„å†…å®¹å®¡æ ¸ç­–ç•¥ï¼Œå‘ç°å®ƒä»¬ä¸ä»…èƒ½å‡ç¼“è™šå‡ä¿¡æ¯çš„ä¼ æ’­ï¼Œè¿˜èƒ½æé«˜ç”¨æˆ·å‚ä¸åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07951",
            "title": "Scaling Laws for Native Multimodal Models Scaling Laws for Native\n  Multimodal Models",
            "url": "https://huggingface.co/papers/2504.07951",
            "abstract": "Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)--those trained from the ground up on all modalities--and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders. On the contrary, early-fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows for models that learn modality-specific weights, significantly enhancing performance.",
            "score": 10,
            "issue_id": 3190,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "66dadd5828f14551",
            "authors": [
                "Mustafa Shukor",
                "Enrico Fini",
                "Victor Guilherme Turrisi da Costa",
                "Matthieu Cord",
                "Joshua Susskind",
                "Alaaeldin El-Nouby"
            ],
            "affiliations": [
                "Apple",
                "Sorbonne University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07951.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#optimization",
                    "#agi",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ½Ğ½ĞµĞ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¼Ğ¸Ñ€ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ğ¾Ğ·Ğ´Ğ½ĞµĞ³Ğ¾ Ğ¸ Ñ€Ğ°Ğ½Ğ½ĞµĞ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° 457 Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ñ€Ğ°Ğ½Ğ½ĞµĞ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½Ğµ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ğ¼, Ğ° Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¸Ñ…. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (Mixture of Experts) Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ²ĞµÑĞ°Ğ¼, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Early-Fusion Models Outperform Late-Fusion in Multimodal Learning",
                    "desc": "This paper explores the effectiveness of native multimodal models (NMMs) compared to late-fusion architectures that combine pre-trained components. The authors conducted a large-scale study with 457 models to analyze the performance of early-fusion versus late-fusion approaches. They found that early-fusion models, which integrate modalities from the start, outperform late-fusion models in terms of efficiency and training ease. Additionally, by using Mixture of Experts (MoEs), the early-fusion models can learn specific weights for different modalities, further boosting their performance."
                },
                "zh": {
                    "title": "æ—©èåˆæ¶æ„æ›´èƒœä¸€ç­¹ï¼",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€æ¨¡å‹çš„æ¶æ„è®¾è®¡ï¼Œç‰¹åˆ«æ˜¯åŸç”Ÿå¤šæ¨¡æ€æ¨¡å‹ï¼ˆNMMsï¼‰ï¼Œè¿™äº›æ¨¡å‹ä»ä¸€å¼€å§‹å°±é’ˆå¯¹æ‰€æœ‰æ¨¡æ€è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å¯¹457ä¸ªä¸åŒæ¶æ„å’Œè®­ç»ƒç»„åˆçš„æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›çš„è§„æ¨¡æ³•åˆ™ç ”ç©¶ã€‚ç»“æœæ˜¾ç¤ºï¼Œåèåˆæ¶æ„å¹¶æ²¡æœ‰æ¯”æ—©èåˆæ¶æ„å…·æœ‰å›ºæœ‰ä¼˜åŠ¿ï¼Œåè€…åœ¨è¾ƒä½çš„å‚æ•°æ•°é‡ä¸‹è¡¨ç°æ›´å¼ºï¼Œè®­ç»ƒæ•ˆç‡æ›´é«˜ï¼Œéƒ¨ç½²æ›´ç®€å•ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œç»“åˆä¸“å®¶æ··åˆï¼ˆMoEsï¼‰å¯ä»¥è®©æ¨¡å‹å­¦ä¹ ç‰¹å®šæ¨¡æ€çš„æƒé‡ï¼Œä»è€Œæ˜¾è‘—æå‡æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07934",
            "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual\n  Reasoning Self-Improvement",
            "url": "https://huggingface.co/papers/2504.07934",
            "abstract": "In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL.",
            "score": 8,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "419fda5d0a4bcadf",
            "authors": [
                "Xiyao Wang",
                "Zhengyuan Yang",
                "Chao Feng",
                "Hongjin Lu",
                "Linjie Li",
                "Chung-Ching Lin",
                "Kevin Lin",
                "Furong Huang",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Microsoft",
                "University of Maryland, College Park",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07934.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#data",
                    "#reasoning",
                    "#training",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RFT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾ (MCTS) Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ThinkLite-VL, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 11 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ 7B Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning with Fewer Samples through Smart Data Selection",
                    "desc": "This paper introduces a novel method to improve visual reasoning in models using fewer training samples through self-improvement techniques, avoiding knowledge distillation. The authors emphasize the importance of sample difficulty during reinforcement fine-tuning (RFT), suggesting that challenging samples can enhance reasoning capabilities even with limited data. They propose a unique application of Monte Carlo Tree Search (MCTS) to quantify sample difficulty, allowing for effective data filtering. The resulting model, ThinkLite-VL, demonstrates a 7% performance increase over its predecessor using only 11k samples, achieving state-of-the-art results in various benchmarks."
                },
                "zh": {
                    "title": "ç”¨å°‘é‡æ ·æœ¬æå‡è§†è§‰æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªæˆ‘æ”¹è¿›æ¥å¢å¼ºè§†è§‰æ¨ç†ï¼Œä¸”æ‰€éœ€çš„è®­ç»ƒæ ·æœ¬æ˜¾è‘—å‡å°‘ï¼Œä¸ä¾èµ–çŸ¥è¯†è’¸é¦ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰è¿‡ç¨‹ä¸­ï¼Œè®­ç»ƒæ•°æ®çš„éš¾åº¦è‡³å…³é‡è¦ï¼Œé€‚å½“å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬å¯ä»¥æ˜¾è‘—æå‡æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹å¼ï¼Œåˆ©ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥é‡åŒ–æ ·æœ¬çš„éš¾åº¦ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„æ•°æ®ç­›é€‰ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹ThinkLite-VLåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½¿ç”¨ä»…11kä¸ªè®­ç»ƒæ ·æœ¬ï¼Œå¹³å‡æ€§èƒ½æå‡äº†7%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.04974",
            "title": "Towards Visual Text Grounding of Multimodal Large Language Model",
            "url": "https://huggingface.co/papers/2504.04974",
            "abstract": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to their complex layouts and textual content. However, current benchmarks do not fully address these challenges, as they mostly focus on visual grounding on natural images, rather than text-rich document images. Thus, to bridge this gap, we introduce TRIG, a novel task with a newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering. Specifically, we propose an OCR-LLM-human interaction pipeline to create 800 manually annotated question-answer pairs as a benchmark and a large-scale training set of 90$ synthetic data based on four diverse datasets. A comprehensive evaluation of various MLLMs on our proposed benchmark exposes substantial limitations in their grounding capability on text-rich images. In addition, we propose two simple and effective TRIG methods based on general instruction tuning and plug-and-play efficient embedding, respectively. By finetuning MLLMs on our synthetic dataset, they promisingly improve spatial reasoning and grounding capabilities.",
            "score": 3,
            "issue_id": 3184,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 7",
                "zh": "4æœˆ7æ—¥"
            },
            "hash": "f382ad07e678a558",
            "authors": [
                "Ming Li",
                "Ruiyi Zhang",
                "Jian Chen",
                "Jiuxiang Gu",
                "Yufan Zhou",
                "Franck Dernoncourt",
                "Wanrong Zhu",
                "Tianyi Zhou",
                "Tong Sun"
            ],
            "affiliations": [
                "Adobe Research",
                "University at Buffalo",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.04974.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#dataset",
                    "#synthetic",
                    "#transfer_learning",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ TRIG Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 800 Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸ 90 Ñ‚Ñ‹ÑÑÑ‡ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑ†ĞµĞ½ĞºĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… MLLM Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞµ Ğº Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ… Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ plug-and-play."
                },
                "en": {
                    "title": "Enhancing MLLMs for Text-Rich Image Grounding",
                    "desc": "This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in understanding text-rich images, such as documents and infographics. The authors introduce a new task called TRIG, which focuses on improving the grounding of text in these complex images for better document question-answering. They create a benchmark dataset with 800 annotated question-answer pairs and 90 synthetic data samples to evaluate MLLMs' performance. Additionally, the paper presents two methods to enhance MLLMs' spatial reasoning and grounding abilities through fine-tuning on the new dataset."
                },
                "zh": {
                    "title": "æå‡æ–‡æ¡£å›¾åƒçš„æ–‡æœ¬å®šä½èƒ½åŠ›",
                    "desc": "å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†åœ¨è§†è§‰æ–‡æœ¬å®šä½æ–¹é¢ä»ç„¶å­˜åœ¨æ˜¾è‘—çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ–‡æœ¬ä¸°å¯Œçš„æ–‡æ¡£å›¾åƒä¸­ã€‚æ–‡æ¡£å›¾åƒå¦‚æ‰«æè¡¨å•å’Œä¿¡æ¯å›¾è¡¨ç”±äºå…¶å¤æ‚çš„å¸ƒå±€å’Œæ–‡æœ¬å†…å®¹ï¼Œå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TRIGä»»åŠ¡ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ–°çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œä»¥è¯„ä¼°å’Œæå‡MLLMsåœ¨æ–‡æ¡£é—®ç­”ä¸­çš„æ–‡æœ¬ä¸°å¯Œå›¾åƒå®šä½èƒ½åŠ›ã€‚é€šè¿‡å¯¹MLLMsè¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç©ºé—´æ¨ç†å’Œå®šä½èƒ½åŠ›ä¸Šæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07961",
            "title": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction",
            "url": "https://huggingface.co/papers/2504.07961",
            "abstract": "We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic prior captured by such video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, depth, and ray maps. It uses a new multi-modal alignment algorithm to align and fuse these modalities, as well as multiple sliding windows, at inference time, thus obtaining robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods, including recent methods such as MonST3R, which are also designed to handle dynamic scenes.",
            "score": 1,
            "issue_id": 3193,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 10",
                "zh": "4æœˆ10æ—¥"
            },
            "hash": "4a0d7f1cda574212",
            "authors": [
                "Zeren Jiang",
                "Chuanxia Zheng",
                "Iro Laina",
                "Diane Larlus",
                "Andrea Vedaldi"
            ],
            "affiliations": [
                "Naver Labs Europe",
                "Visual Geometry Group, University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07961.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#benchmark",
                    "#multimodal",
                    "#long_context",
                    "#diffusion",
                    "#synthetic"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Geo4D: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Geo4D - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ¹Ğ¼ĞµÑ€Ñ‹, Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑÑŒ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ zero-shot. Geo4D Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Geo4D Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "Revolutionizing 3D Reconstruction with Geo4D",
                    "desc": "Geo4D is a novel approach that adapts video diffusion models for creating 3D reconstructions from single-camera videos of moving scenes. It effectively utilizes the dynamic information captured by these models, allowing it to be trained solely on synthetic datasets while still performing well on real-world data without additional training. The method predicts various geometric representations, including point, depth, and ray maps, and employs a unique multi-modal alignment technique to integrate these representations during the reconstruction process. Through extensive testing, Geo4D demonstrates superior performance compared to existing video depth estimation techniques, particularly in dynamic environments."
                },
                "zh": {
                    "title": "Geo4Dï¼šåŠ¨æ€åœºæ™¯çš„4Dé‡å»ºæ–°æ–¹æ³•",
                    "desc": "Geo4Dæ˜¯ä¸€ç§å°†è§†é¢‘æ‰©æ•£æ¨¡å‹ç”¨äºå•ç›®3Dé‡å»ºåŠ¨æ€åœºæ™¯çš„æ–¹æ³•ã€‚å®ƒåˆ©ç”¨è§†é¢‘æ¨¡å‹æ•æ‰åˆ°çš„å¼ºåŠ¨æ€å…ˆéªŒï¼Œä»…ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶èƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹å¾ˆå¥½åœ°æ¨å¹¿åˆ°çœŸå®æ•°æ®ã€‚Geo4Dé¢„æµ‹å¤šç§äº’è¡¥çš„å‡ ä½•æ¨¡æ€ï¼ŒåŒ…æ‹¬ç‚¹å›¾ã€æ·±åº¦å›¾å’Œå…‰çº¿å›¾ã€‚é€šè¿‡æ–°çš„å¤šæ¨¡æ€å¯¹é½ç®—æ³•å’Œå¤šä¸ªæ»‘åŠ¨çª—å£ï¼ŒGeo4Dåœ¨æ¨ç†æ—¶å¯¹è¿™äº›æ¨¡æ€è¿›è¡Œå¯¹é½å’Œèåˆï¼Œä»è€Œå®ç°é•¿è§†é¢‘çš„ç¨³å¥å’Œå‡†ç¡®çš„4Dé‡å»ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06801",
            "title": "MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular\n  Detection",
            "url": "https://huggingface.co/papers/2504.06801",
            "abstract": "Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, it's particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, a novel system that considers the 3D scene content to create realistic augmentations. Specifically, given a background scene, MonoPlace3D learns a distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient.",
            "score": 1,
            "issue_id": 3188,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 9",
                "zh": "4æœˆ9æ—¥"
            },
            "hash": "bf3e9622523967d2",
            "authors": [
                "Rishubh Parihar",
                "Srinjay Sarkar",
                "Sarthak Vora",
                "Jogendra Kundu",
                "R. Venkatesh Babu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.06801.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ 3D Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸",
                    "desc": "MonoPlace3D - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… 3D Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ². ĞĞ½Ğ° ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ 3D ÑÑ†ĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… 3D Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ ÑÑ‚Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… KITTI Ğ¸ NuScenes Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… 3D Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Monocular 3D Detection with Realistic Object Placement",
                    "desc": "This paper addresses the limitations of current monocular 3D detectors due to the lack of diverse real-world datasets. It highlights the importance of not just realistic object appearance but also the correct placement of objects in 3D scenes for effective training. The authors introduce MonoPlace3D, a system that learns to generate realistic object placements based on the content of the background scene. Their experiments show that MonoPlace3D enhances the performance of existing monocular 3D detectors on standard datasets like KITTI and NuScenes, demonstrating improved accuracy and data efficiency."
                },
                "zh": {
                    "title": "MonoPlace3Dï¼šæå‡å•ç›®3Dæ£€æµ‹çš„çœŸå®æ„Ÿå¢å¼º",
                    "desc": "å½“å‰çš„å•ç›®3Dæ£€æµ‹å™¨å—åˆ°çœŸå®ä¸–ç•Œæ•°æ®é›†å¤šæ ·æ€§å’Œè§„æ¨¡çš„é™åˆ¶ã€‚è™½ç„¶æ•°æ®å¢å¼ºæœ‰åŠ©äºæ”¹å–„æ¨¡å‹æ€§èƒ½ï¼Œä½†åœ¨æˆ·å¤–åœºæ™¯ä¸­ç”ŸæˆçœŸå®æ„Ÿçš„å¢å¼ºæ•°æ®å°¤å…¶å›°éš¾ã€‚å¤§å¤šæ•°åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ä¸“æ³¨äºé€šè¿‡æ”¹è¿›æ¸²æŸ“æŠ€æœ¯æ¥æé«˜ç‰©ä½“å¤–è§‚çš„çœŸå®æ„Ÿï¼Œè€Œæˆ‘ä»¬å‘ç°ç‰©ä½“çš„æ”¾ç½®ä½ç½®å’Œæ–¹å¼å¯¹è®­ç»ƒæœ‰æ•ˆçš„3Då•ç›®æ£€æµ‹å™¨åŒæ ·é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MonoPlace3Dç³»ç»Ÿï¼Œå®ƒè€ƒè™‘3Dåœºæ™¯å†…å®¹æ¥åˆ›å»ºçœŸå®çš„å¢å¼ºæ•°æ®ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ç°æœ‰å•ç›®3Dæ£€æµ‹å™¨çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06752",
            "title": "Compass Control: Multi Object Orientation Control for Text-to-Image\n  Generation",
            "url": "https://huggingface.co/papers/2504.06752",
            "abstract": "Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware compass tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study.",
            "score": 1,
            "issue_id": 3188,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 9",
                "zh": "4æœˆ9æ—¥"
            },
            "hash": "d73e0b8851ffd9e3",
            "authors": [
                "Rishubh Parihar",
                "Vaibhav Agrawal",
                "Sachidanand VS",
                "R. Venkatesh Babu"
            ],
            "affiliations": [
                "IIIT Hyderabad",
                "IISc Bangalore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06752.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d",
                    "#training",
                    "#diffusion",
                    "#synthetic"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹-ĞºĞ¾Ğ¼Ğ¿Ğ°ÑÑ‹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ€Ñ‚ Ğ¿ĞµÑ€ĞµĞºÑ€ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Mastering Object Orientation in Text-to-Image Generation",
                    "desc": "This paper presents a novel approach to enhance text-to-image diffusion models by enabling precise control over the orientation of multiple objects in generated scenes. The authors introduce orientation-aware compass tokens that are conditioned on the object orientation, allowing for better manipulation of object placement and alignment. A lightweight encoder network predicts these tokens based on input orientations, addressing challenges like poor control and object entanglement during training. The proposed method demonstrates strong generalization capabilities, achieving state-of-the-art results in orientation control and text alignment through extensive evaluations and user studies."
                },
                "zh": {
                    "title": "ç²¾ç¡®æ§åˆ¶å¤šå¯¹è±¡æ–¹å‘çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥æ§åˆ¶æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å¤šå¯¹è±¡çš„æ–¹å‘æ§åˆ¶ã€‚é€šè¿‡å¼•å…¥æ–¹å‘æ„ŸçŸ¥çš„æŒ‡å—é’ˆæ ‡è®°ï¼Œæ¨¡å‹èƒ½å¤Ÿä¸ºæ¯ä¸ªå¯¹è±¡æä¾›ç²¾ç¡®çš„æ–¹å‘æ§åˆ¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆå¤æ‚å¯¹è±¡å’Œå¤šå¯¹è±¡åœºæ™¯æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»“åˆä¸ªæ€§åŒ–æ–¹æ³•åï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨å¤šç§ä¸Šä¸‹æ–‡ä¸­ç²¾ç¡®æ§åˆ¶æ–°å¯¹è±¡çš„æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05579",
            "title": "TAPNext: Tracking Any Point (TAP) as Next Token Prediction",
            "url": "https://huggingface.co/papers/2504.05579",
            "abstract": "Tracking Any Point (TAP) in a video is a challenging computer vision problem with many demonstrated applications in robotics, video editing, and 3D reconstruction. Existing methods for TAP rely heavily on complex tracking-specific inductive biases and heuristics, limiting their generality and potential for scaling. To address these challenges, we present TAPNext, a new approach that casts TAP as sequential masked token decoding. Our model is causal, tracks in a purely online fashion, and removes tracking-specific inductive biases. This enables TAPNext to run with minimal latency, and removes the temporal windowing required by many existing state of art trackers. Despite its simplicity, TAPNext achieves a new state-of-the-art tracking performance among both online and offline trackers. Finally, we present evidence that many widely used tracking heuristics emerge naturally in TAPNext through end-to-end training.",
            "score": 1,
            "issue_id": 3192,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 8",
                "zh": "4æœˆ8æ—¥"
            },
            "hash": "27a89bfc540d93f8",
            "authors": [
                "Artem Zholus",
                "Carl Doersch",
                "Yi Yang",
                "Skanda Koppula",
                "Viorica Patraucean",
                "Xu Owen He",
                "Ignacio Rocco",
                "Mehdi S. M. Sajjadi",
                "Sarath Chandar",
                "Ross Goroshin"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "Chandar Research Lab",
                "Google DeepMind",
                "Mila - Quebec AI Institute",
                "Polytechnique Montreal",
                "University College London",
                "UniversitÃ© de MontrÃ©al"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05579.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture",
                    "#video",
                    "#robotics"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "TAPNext: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "TAPNext - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ‚Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ° Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ. TAPNext Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ñ‚Ñ€ĞµĞºĞµÑ€Ğ¾Ğ², Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑĞ²Ğ¾Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ñƒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ² TAPNext Ñ‡ĞµÑ€ĞµĞ· ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Revolutionizing Video Tracking with TAPNext",
                    "desc": "The paper introduces TAPNext, a novel method for tracking any point (TAP) in videos, which is crucial for applications like robotics and video editing. Unlike traditional methods that depend on complex rules and biases, TAPNext simplifies the process by using sequential masked token decoding. This approach allows for real-time tracking without the need for temporal windowing, resulting in lower latency. TAPNext not only achieves superior tracking performance compared to existing methods but also shows that common tracking heuristics can be learned through end-to-end training."
                },
                "zh": {
                    "title": "TAPNextï¼šç®€åŒ–è§†é¢‘è·Ÿè¸ªçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘è·Ÿè¸ªæ–¹æ³•ï¼Œç§°ä¸ºTAPNextï¼Œæ—¨åœ¨è§£å†³è·Ÿè¸ªä»»æ„ç‚¹ï¼ˆTAPï¼‰çš„é—®é¢˜ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒTAPNextå°†TAPè§†ä¸ºé¡ºåºæ©ç ä»¤ç‰Œè§£ç ï¼Œæ¶ˆé™¤äº†å¤æ‚çš„è·Ÿè¸ªç‰¹å®šåè§ã€‚è¯¥æ¨¡å‹å…·æœ‰å› æœæ€§ï¼Œèƒ½å¤Ÿåœ¨çº¿å®æ—¶è·Ÿè¸ªï¼Œä¸”å»¶è¿Ÿæä½ã€‚å°½ç®¡æ–¹æ³•ç®€å•ï¼ŒTAPNextåœ¨åœ¨çº¿å’Œç¦»çº¿è·Ÿè¸ªå™¨ä¸­éƒ½è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„è·Ÿè¸ªæ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-10.html",
    "link_next": "2025-04-14.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "10.04",
        "en": "04/10",
        "zh": "4æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "14.04",
        "en": "04/14",
        "zh": "4æœˆ14æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 7,
        "#agents": 2,
        "#cv": 5,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 4,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 1,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 6,
        "#transfer_learning": 2,
        "#graphs": 2,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 4,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº†Kimi-VLï¼Œä¸€ä¸ªé«˜æ•ˆçš„å¼€æºæ··åˆä¸“å®¶è§†è§‰è¯­è¨€æ¨¡å‹ã€‚å®ƒåœ¨å¤šæ¨¡æ€æ¨ç†ã€é•¿ä¸Šä¸‹æ–‡ç†è§£å’Œå¼ºå¤§çš„ä»£ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä»…æ¿€æ´»è¯­è¨€è§£ç å™¨ä¸­çš„2.8Bå‚æ•°ã€‚Kimi-VLåœ¨å¤šè½®ä»£ç†ä»»åŠ¡å’Œå¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†å’Œé«˜åˆ†è¾¨ç‡è§†è§‰è¾“å…¥ç†è§£æ–¹é¢å–å¾—è¿›å±•ã€‚åŸºäºKimi-VLï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†Kimi-VL-Thinkingï¼Œå…·æœ‰å¼ºå¤§çš„é•¿æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚ä»£ç å’Œæ¨¡å‹åœ¨https://github.com/MoonshotAI/Kimi-VLå…¬å¼€å¯ç”¨ã€‚",
        "title": "Kimi-VL Technical Report",
        "pinyin": "æˆ‘ä»¬ä»‹ç»äº†Kimi-VLï¼Œä¸€ä¸ªé«˜æ•ˆçš„å¼€æºæ··åˆä¸“å®¶è§†è§‰è¯­è¨€æ¨¡å‹ã€‚\nWÇ’men jiÃ¨shÃ o le Kimi-VL, yÄ«gÃ¨ gÄoxiÃ o de kÄiyuÃ¡n hÃ¹nhÃ© zhuÄnjiÄ shÃ¬jiuÃ© yÇ”yÃ¡n mÃ³xÃ­ng.\n\nå®ƒåœ¨å¤šæ¨¡æ€æ¨ç†ã€é•¿ä¸Šä¸‹æ–‡ç†è§£å’Œå¼ºå¤§çš„ä»£ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä»…æ¿€æ´»è¯­è¨€è§£ç å™¨ä¸­çš„2.8Bå‚æ•°ã€‚\nTÄ zÃ i duÅ mÃ³shÃ¬ tuÄ«lÇ, chÃ¡ng shÃ ngxÃ¬awÃ©n lÇjiÄ› hÃ© qiÃ¡ngdÃ  de dÃ ilÇ nÃ©nglÃ¬ fÄngmiÃ n biÇoxiÃ n chÅ«sÃ¨, jÇn jÄ«huÃ³ yÇ”yÃ¡n jiÄ›mÇqÃ¬ zhÅng de 2.8B cÄnshÃ¹.\n\nKimi-VLåœ¨å¤šè½®ä»£ç†ä»»åŠ¡å’Œå¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†å’Œé«˜åˆ†è¾¨ç‡è§†è§‰è¾“å…¥ç†è§£æ–¹é¢å–å¾—è¿›å±•ã€‚\nKimi-VL zÃ i duÅ lÃºn dÃ ilÇ rÃ¨nwÃ¹ hÃ© duÅ zhÇ’ng shÃ¬jiuÃ© yÇ”yÃ¡n rÃ¨nwÃ¹ zhÅng biÇoxiÃ n yÅuyÃ¬, bÃ¬ng zÃ i chÃ¡ng shÃ ngxÃ¬awÃ©n chÇ”lÇ hÃ© gÄo fÄ“nbiÃ nlÇœ shÃ¬jiuÃ© shÅ«rÃ¹ lÇjiÄ› fÄngmiÃ n qÇ”dÃ© jÃ¬nzhÃ n.\n\nåŸºäºKimi-VLï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†Kimi-VL-Thinkingï¼Œå…·æœ‰å¼ºå¤§çš„é•¿æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚\nJÄ«yÃº Kimi-VL, wÇ’men hÃ¡i kÄifÄ le Kimi-VL-Thinking, jÃ¹yÇ’u qiÃ¡ngdÃ  de chÃ¡ng shÃ­jiÄn tuÄ«lÇ nÃ©nglÃ¬.\n\nä»£ç å’Œæ¨¡å‹åœ¨https://github.com/MoonshotAI/Kimi-VLå…¬å¼€å¯ç”¨ã€‚\nDÃ imÇ hÃ© mÃ³xÃ­ng zÃ i https://github.com/MoonshotAI/Kimi-VL gÅngkÄi kÄ›yÃ²ng.",
        "vocab": "[\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"é«˜æ•ˆ\", \"pinyin\": \"gÄo xiÃ o\", \"trans\": \"efficient\"},\n    {\"word\": \"å¼€æº\", \"pinyin\": \"kÄi yuÃ¡n\", \"trans\": \"open-source\"},\n    {\"word\": \"æ··åˆ\", \"pinyin\": \"hÃ¹n hÃ©\", \"trans\": \"hybrid\"},\n    {\"word\": \"ä¸“å®¶\", \"pinyin\": \"zhuÄn jiÄ\", \"trans\": \"expert\"},\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬ juÃ©\", \"trans\": \"visual\"},\n    {\"word\": \"è¯­è¨€\", \"pinyin\": \"yÇ” yÃ¡n\", \"trans\": \"language\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"é•¿ä¸Šä¸‹æ–‡\", \"pinyin\": \"chÃ¡ng shÃ ng xiÃ  wÃ©n\", \"trans\": \"long context\"},\n    {\"word\": \"ç†è§£\", \"pinyin\": \"lÇ jiÄ›\", \"trans\": \"understanding\"},\n    {\"word\": \"å¼ºå¤§\", \"pinyin\": \"qiÃ¡ng dÃ \", \"trans\": \"powerful\"},\n    {\"word\": \"ä»£ç†\", \"pinyin\": \"dÃ i lÇ\", \"trans\": \"agent\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©ng lÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"æ¿€æ´»\", \"pinyin\": \"jÄ« huÃ³\", \"trans\": \"activate\"},\n    {\"word\": \"è§£ç å™¨\", \"pinyin\": \"jiÄ› mÇ qÃ¬\", \"trans\": \"decoder\"},\n    {\"word\": \"å‚æ•°\", \"pinyin\": \"cÄn shÃ¹\", \"trans\": \"parameter\"},\n    {\"word\": \"å¤šè½®\", \"pinyin\": \"duÅ lÃºn\", \"trans\": \"multi-turn\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wÃ¹\", \"trans\": \"task\"},\n    {\"word\": \"ä¼˜å¼‚\", \"pinyin\": \"yÅu yÃ¬\", \"trans\": \"excellent\"},\n    {\"word\": \"è¿›å±•\", \"pinyin\": \"jÃ¬n zhÇn\", \"trans\": \"progress\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ” lÇ\", \"trans\": \"process\"},\n    {\"word\": \"é«˜åˆ†è¾¨ç‡\", \"pinyin\": \"gÄo fÄ“n biÃ n lÇœ\", \"trans\": \"high resolution\"},\n    {\"word\": \"è¾“å…¥\", \"pinyin\": \"shÅ« rÃ¹\", \"trans\": \"input\"},\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ« yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"å¼€å‘\", \"pinyin\": \"kÄi fÄ\", \"trans\": \"develop\"},\n    {\"word\": \"é•¿æ—¶é—´\", \"pinyin\": \"chÃ¡ng shÃ­ jiÄn\", \"trans\": \"long-term\"},\n    {\"word\": \"å…¬å¼€\", \"pinyin\": \"gÅng kÄi\", \"trans\": \"public\"},\n    {\"word\": \"å¯ç”¨\", \"pinyin\": \"kÄ› yÃ²ng\", \"trans\": \"available\"}\n]",
        "trans": "We introduce Kimi-VL, an efficient open-source hybrid expert visual language model. It excels in multimodal reasoning, long-context understanding, and strong agent capabilities, activating only 2.8B parameters in the language decoder. Kimi-VL performs exceptionally well in multi-turn agent tasks and various visual language tasks, making progress in long-context processing and high-resolution visual input understanding. Based on Kimi-VL, we also developed Kimi-VL-Thinking, which has strong long-term reasoning capabilities. The code and model are publicly available at https://github.com/MoonshotAI/Kimi-VL.",
        "update_ts": "2025-04-11 09:11"
    }
}