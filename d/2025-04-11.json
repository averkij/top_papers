{
    "date": {
        "ru": "11 апреля",
        "en": "April 11",
        "zh": "4月11日"
    },
    "time_utc": "2025-04-11 16:13",
    "weekday": 4,
    "issue_id": 3196,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.07491",
            "title": "Kimi-VL Technical Report",
            "url": "https://huggingface.co/papers/2504.07491",
            "abstract": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as a general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including college-level image and video comprehension, OCR, mathematical reasoning, and multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With a 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong long-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8 on MathVision, and 71.3 on MathVista while maintaining the compact 2.8B activated LLM parameters, setting a new standard for efficient multimodal thinking models. Code and models are publicly accessible at https://github.com/MoonshotAI/Kimi-VL.",
            "score": 64,
            "issue_id": 3185,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "e1d1e4add50955e8",
            "authors": [
                "Kimi Team",
                "Angang Du",
                "Bohong Yin",
                "Bowei Xing",
                "Bowen Qu",
                "Bowen Wang",
                "Cheng Chen",
                "Chenlin Zhang",
                "Chenzhuang Du",
                "Chu Wei",
                "Congcong Wang",
                "Dehao Zhang",
                "Dikang Du",
                "Dongliang Wang",
                "Enming Yuan",
                "Enzhe Lu",
                "Fang Li",
                "Flood Sung",
                "Guangda Wei",
                "Guokun Lai",
                "Han Zhu",
                "Hao Ding",
                "Hao Hu",
                "Hao Yang",
                "Hao Zhang",
                "Haoning Wu",
                "Haotian Yao",
                "Haoyu Lu",
                "Heng Wang",
                "Hongcheng Gao",
                "Huabin Zheng",
                "Jiaming Li",
                "Jianlin Su",
                "Jianzhou Wang",
                "Jiaqi Deng",
                "Jiezhong Qiu",
                "Jin Xie",
                "Jinhong Wang",
                "Jingyuan Liu",
                "Junjie Yan",
                "Kun Ouyang",
                "Liang Chen",
                "Lin Sui",
                "Longhui Yu",
                "Mengfan Dong",
                "Mengnan Dong",
                "Nuo Xu",
                "Pengyu Cheng",
                "Qizheng Gu",
                "Runjie Zhou",
                "Shaowei Liu",
                "Sihan Cao",
                "Tao Yu",
                "Tianhui Song",
                "Tongtong Bai",
                "Wei Song",
                "Weiran He",
                "Weixiao Huang",
                "Weixin Xu",
                "Xiaokun Yuan",
                "Xingcheng Yao",
                "Xingzhe Wu",
                "Xinxing Zu",
                "Xinyu Zhou",
                "Xinyuan Wang",
                "Y. Charles",
                "Yan Zhong",
                "Yang Li",
                "Yangyang Hu",
                "Yanru Chen",
                "Yejie Wang",
                "Yibo Liu",
                "Yibo Miao",
                "Yidao Qin",
                "Yimin Chen",
                "Yiping Bao",
                "Yiqin Wang",
                "Yongsheng Kang",
                "Yuanxin Liu",
                "Yulun Du",
                "Yuxin Wu",
                "Yuzhi Wang",
                "Yuzi Yan",
                "Zaida Zhou",
                "Zhaowei Li",
                "Zhejun Jiang",
                "Zheng Zhang",
                "Zhilin Yang",
                "Zhiqi Huang",
                "Zihao Huang",
                "Zijia Zhao",
                "Ziwei Chen"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.07491.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#agents",
                    "#rl",
                    "#multimodal",
                    "#long_context",
                    "#reasoning",
                    "#small_models",
                    "#training",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Kimi-VL: Эффективная мультимодальная модель с расширенными возможностями рассуждений",
                    "desc": "Kimi-VL - это эффективная модель машинного обучения, основанная на архитектуре Mixture-of-Experts (MoE) для обработки визуальной и текстовой информации. Модель демонстрирует высокую производительность в различных сложных задачах, включая многоэтапное взаимодействие с агентами, понимание изображений и видео, оптическое распознавание символов и математические рассуждения. Kimi-VL обладает расширенным контекстным окном в 128K и способна обрабатывать сверхвысокое разрешение визуальных входных данных. Авторы также представляют улучшенную версию модели, Kimi-VL-Thinking, которая демонстрирует сильные способности к долгосрочным рассуждениям при сохранении компактного размера активированных параметров языковой модели."
                },
                "en": {
                    "title": "Kimi-VL: Efficient Multimodal Mastery with Long-Context Reasoning",
                    "desc": "Kimi-VL is a cutting-edge Mixture-of-Experts vision-language model that efficiently combines multimodal reasoning and long-context understanding while using only 2.8 billion parameters in its language decoder. It excels in various complex tasks, including multi-turn interactions and advanced image and video comprehension, outperforming other leading models in several areas. The model features a 128K extended context window, allowing it to process long inputs effectively, and its native-resolution vision encoder enhances its ability to interpret high-resolution visuals. Additionally, the Kimi-VL-Thinking variant improves long-horizon reasoning through supervised fine-tuning and reinforcement learning, setting a new benchmark for efficient multimodal models."
                },
                "zh": {
                    "title": "Kimi-VL：高效的多模态推理新标准",
                    "desc": "Kimi-VL是一种高效的开源混合专家（MoE）视觉语言模型（VLM），具备先进的多模态推理和长文本理解能力。该模型在多轮对话任务和各种视觉语言任务中表现出色，能够与顶尖模型相媲美。Kimi-VL还具备处理长上下文的能力，能够处理多达128K的输入，适用于复杂的视觉理解任务。通过长链思维的监督微调和强化学习，Kimi-VL-Thinking进一步提升了长远推理能力，设定了高效多模态思维模型的新标准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07956",
            "title": "VCR-Bench: A Comprehensive Evaluation Framework for Video\n  Chain-of-Thought Reasoning",
            "url": "https://huggingface.co/papers/2504.07956",
            "abstract": "The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task.",
            "score": 32,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "88860725e51f3629",
            "authors": [
                "Yukun Qi",
                "Yiming Zhao",
                "Yu Zeng",
                "Xikun Bao",
                "Wenxuan Huang",
                "Lin Chen",
                "Zehui Chen",
                "Jie Zhao",
                "Zhongang Qi",
                "Feng Zhao"
            ],
            "affiliations": [
                "East China Normal University",
                "Huawei Noahs Ark Lab",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07956.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "VCR-Bench: новый стандарт оценки рассуждений ИИ по видео",
                    "desc": "В статье представлен новый бенчмарк VCR-Bench для оценки способностей больших визуально-языковых моделей к рассуждениям на основе цепочки мыслей при анализе видео. Бенчмарк включает 859 видео и 1034 пары вопрос-ответ с пошаговыми обоснованиями, помеченными для оценки восприятия и рассуждений. Эксперименты показали существенные ограничения современных моделей, особенно в обработке пространственно-временной информации. VCR-Bench призван стать стандартизированным инструментом для выявления недостатков в сложных задачах видеоанализа."
                },
                "en": {
                    "title": "VCR-Bench: Evaluating Video Reasoning in LVLMs",
                    "desc": "This paper introduces VCR-Bench, a new benchmark for evaluating Video Chain-of-Thought (CoT) reasoning in large vision-language models (LVLMs). It addresses the lack of rigorous evaluation frameworks for assessing how well these models can reason about video content. The benchmark includes 859 videos and 1,034 annotated question-answer pairs, each with a stepwise CoT rationale linked to perception or reasoning capabilities. Experiments reveal that current LVLMs struggle with video reasoning, particularly in processing temporal-spatial information, highlighting the need for improved models in this area."
                },
                "zh": {
                    "title": "VCR-Bench：视频推理的新标准",
                    "desc": "链式思维（CoT）推理的进步显著提升了大型语言模型（LLMs）和大型视觉语言模型（LVLMs）的能力。然而，目前缺乏一个严格的视频CoT推理评估框架，现有的视频基准无法充分评估推理过程。为此，我们提出了VCR-Bench，这是一个新颖的基准，旨在全面评估LVLMs在视频链式思维推理方面的能力。通过859个视频和1034对高质量问答对，VCR-Bench为每个问答对提供了逐步的CoT推理依据，揭示了当前LVLMs在复杂视频推理中的关键瓶颈。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07960",
            "title": "VisualCloze: A Universal Image Generation Framework via Visual\n  In-Context Learning",
            "url": "https://huggingface.co/papers/2504.07960",
            "abstract": "Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures.",
            "score": 29,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "a8b5331ac40d6a3d",
            "authors": [
                "Zhong-Yu Li",
                "Ruoyi Du",
                "Juncheng Yan",
                "Le Zhuo",
                "Zhen Li",
                "Peng Gao",
                "Zhanyu Ma",
                "Ming-Ming Cheng"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "Tsinghua University",
                "VCIP, CS, Nankai University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07960.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#graphs",
                    "#transfer_learning",
                    "#diffusion",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "VisualCloze: универсальная генерация изображений с визуальным контекстным обучением",
                    "desc": "VisualCloze - это универсальная система генерации изображений, использующая визуальное обучение в контексте для идентификации задач. Она поддерживает широкий спектр задач, включая генерализацию на новые задачи и обратную генерацию. Система использует Graph200K - графовую структуру данных для создания взаимосвязанных задач и улучшения переноса знаний. VisualCloze объединяет генерацию изображений с их дополнением, используя сильные генеративные приоры предобученных моделей."
                },
                "en": {
                    "title": "VisualCloze: Bridging Tasks with Visual Learning in Image Generation",
                    "desc": "This paper introduces VisualCloze, a universal image generation framework that overcomes the limitations of task-specific models by supporting a variety of in-domain tasks and generalizing to unseen tasks. It addresses challenges such as task instruction ambiguity and sparse task distributions by utilizing visual in-context learning, which allows models to learn from visual examples rather than language instructions. The authors also present Graph200K, a dataset that enhances task density and facilitates knowledge transfer across related tasks. Additionally, they demonstrate that their unified image generation approach aligns with image infilling, enabling the use of pre-trained models for improved generative performance."
                },
                "zh": {
                    "title": "VisualCloze：通用图像生成的新框架",
                    "desc": "本论文介绍了一种名为VisualCloze的通用图像生成框架，旨在解决当前图像生成任务中存在的效率和通用性问题。与传统依赖语言指令的方法不同，VisualCloze通过视觉示例进行任务识别，从而减少了任务模糊性和提高了泛化能力。为了增强任务之间的可转移知识，我们引入了Graph200K数据集，该数据集通过图结构建立了多种相关任务。最后，我们发现我们的图像生成方法与图像填充具有一致的目标，从而能够利用预训练填充模型的强生成先验。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07128",
            "title": "DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning",
            "url": "https://huggingface.co/papers/2504.07128",
            "abstract": "Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly \"thinking\" about a problem before providing an answer. This reasoning process is publicly available to the user, creating endless opportunities for studying the reasoning behaviour of the model and opening up the field of Thoughtology. Starting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning, our analyses on DeepSeek-R1 investigate the impact and controllability of thought length, management of long or confusing contexts, cultural and safety concerns, and the status of DeepSeek-R1 vis-\\`a-vis cognitive phenomena, such as human-like language processing and world modelling. Our findings paint a nuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning, where extra inference time can impair model performance. Furthermore, we find a tendency for DeepSeek-R1 to persistently ruminate on previously explored problem formulations, obstructing further exploration. We also note strong safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning counterpart, which can also compromise safety-aligned LLMs.",
            "score": 26,
            "issue_id": 3184,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "6317a88ae7643fe2",
            "authors": [
                "Sara Vera Marjanović",
                "Arkil Patel",
                "Vaibhav Adlakha",
                "Milad Aghajohari",
                "Parishad BehnamGhader",
                "Mehar Bhatia",
                "Aditi Khandelwal",
                "Austin Kraft",
                "Benno Krojer",
                "Xing Han Lù",
                "Nicholas Meade",
                "Dongchan Shin",
                "Amirhossein Kazemnejad",
                "Gaurav Kamath",
                "Marius Mosbach",
                "Karolina Stańczak",
                "Siva Reddy"
            ],
            "affiliations": [
                "McGill University",
                "Mila Quebec AI Institute",
                "University of Copenhagen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07128.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#ethics",
                    "#inference",
                    "#reasoning",
                    "#long_context",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "DeepSeek-R1: мышление машин через призму рассуждений",
                    "desc": "Статья описывает новую модель машинного обучения DeepSeek-R1, которая использует многоступенчатые цепочки рассуждений для решения сложных задач. Исследователи анализируют влияние длины рассуждений, управление длинными контекстами, культурные и этические аспекты, а также сравнивают модель с когнитивными процессами человека. Результаты показывают, что у DeepSeek-R1 есть оптимальное время рассуждения, при превышении которого производительность модели может ухудшаться. Также отмечается тенденция модели застревать на ранее исследованных формулировках проблем и уязвимости в плане безопасности по сравнению с моделями без рассуждений."
                },
                "en": {
                    "title": "DeepSeek-R1: Revolutionizing Reasoning in Language Models",
                    "desc": "The paper introduces DeepSeek-R1, a Large Reasoning Model that enhances the way language models (LLMs) tackle complex problems by generating multi-step reasoning chains. This model allows users to observe the reasoning process, fostering a new area of research called Thoughtology. The study examines various aspects of DeepSeek-R1, including the effects of reasoning length, context management, and safety issues, revealing that excessive inference time can negatively impact performance. Additionally, the findings highlight the model's tendency to dwell on previous problem formulations, which can hinder its ability to explore new solutions and raise safety concerns compared to traditional LLMs."
                },
                "zh": {
                    "title": "深度推理，思维的未来",
                    "desc": "DeepSeek-R1是一种大型推理模型，标志着大语言模型（LLM）在处理复杂问题时的根本转变。它通过创建详细的多步骤推理链来“思考”问题，而不是直接给出答案。这种推理过程对用户是公开的，为研究模型的推理行为提供了无限可能，并开启了思维学（Thoughtology）领域。我们的分析显示，DeepSeek-R1在推理时存在一个“甜蜜点”，过长的推理时间可能会影响模型的表现，同时它在处理已探索的问题时容易陷入反复思考，影响进一步的探索。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07957",
            "title": "MM-IFEngine: Towards Multimodal Instruction Following",
            "url": "https://huggingface.co/papers/2504.07957",
            "abstract": "The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2%), MIA (+7.6%), and IFEval (+12.3%). The full data and evaluation code will be released on https://github.com/SYuan03/MM-IFEngine.",
            "score": 24,
            "issue_id": 3184,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "dbfc6cfeb60e05d7",
            "authors": [
                "Shengyuan Ding",
                "Shenxi Wu",
                "Xiangyu Zhao",
                "Yuhang Zang",
                "Haodong Duan",
                "Xiaoyi Dong",
                "Pan Zhang",
                "Yuhang Cao",
                "Dahua Lin",
                "Jiaqi Wang"
            ],
            "affiliations": [
                "CPII under InnoHK",
                "Fudan University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiaotong University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07957.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#dataset",
                    "#data",
                    "#optimization",
                    "#open_source",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Улучшение способности мультимодальных ИИ-моделей следовать инструкциям",
                    "desc": "Статья представляет MM-IFEngine - пайплайн для создания высококачественных пар изображение-инструкция для обучения мультимодальных языковых моделей. На основе этого пайплайна созданы наборы данных MM-IFInstruct-23k и MM-IFDPO-23k для обучения с учителем и прямой оптимизации предпочтений соответственно. Также предложен бенчмарк MM-IFEval для оценки способности моделей следовать сложным мультимодальным инструкциям. Эксперименты показали значительное улучшение результатов на различных бенчмарках после дообучения моделей на созданных наборах данных."
                },
                "en": {
                    "title": "Enhancing Instruction Following in MLLMs with MM-IFEngine",
                    "desc": "This paper introduces MM-IFEngine, a new method for generating high-quality image-instruction pairs to improve the instruction-following ability of Multi-modal Large Language Models (MLLMs). The authors create a large dataset called MM-IFInstruct-23k, which is designed for Supervised Fine-Tuning (SFT) and an extended version for Direct Preference Optimization (DPO). They also present MM-IFEval, a benchmark that evaluates MLLMs on complex tasks with both output and input constraints. Experiments show that fine-tuning on their datasets significantly enhances performance on various instruction-following benchmarks."
                },
                "zh": {
                    "title": "提升多模态指令跟随能力的创新方法",
                    "desc": "这篇论文介绍了一种新的多模态指令跟随能力评估方法，称为MM-IFEngine。该方法生成高质量的图像-指令对，创建了一个大规模的训练数据集MM-IFInstruct-23k，适用于监督微调和直接偏好优化。论文还提出了MM-IFEval，一个具有挑战性的多模态基准，包含输出响应和输入图像的约束。通过实验，微调后的多模态大语言模型在多个基准测试中表现出显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07943",
            "title": "HoloPart: Generative 3D Part Amodal Segmentation",
            "url": "https://huggingface.co/papers/2504.07943",
            "abstract": "3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Inspired by 2D amodal segmentation, we introduce this novel task to the 3D domain and propose a practical, two-stage approach, addressing the key challenges of inferring occluded 3D geometry, maintaining global shape consistency, and handling diverse shapes with limited training data. First, we leverage existing 3D part segmentation to obtain initial, incomplete part segments. Second, we introduce HoloPart, a novel diffusion-based model, to complete these segments into full 3D parts. HoloPart utilizes a specialized architecture with local attention to capture fine-grained part geometry and global shape context attention to ensure overall shape consistency. We introduce new benchmarks based on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart significantly outperforms state-of-the-art shape completion methods. By incorporating HoloPart with existing segmentation techniques, we achieve promising results on 3D part amodal segmentation, opening new avenues for applications in geometry editing, animation, and material assignment.",
            "score": 21,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "4cc1401ee10a171e",
            "authors": [
                "Yunhan Yang",
                "Yuan-Chen Guo",
                "Yukun Huang",
                "Zi-Xin Zou",
                "Zhipeng Yu",
                "Yangguang Li",
                "Yan-Pei Cao",
                "Xihui Liu"
            ],
            "affiliations": [
                "The University of Hong Kong",
                "VAST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07943.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#architecture",
                    "#diffusion",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🧩",
                "ru": {
                    "title": "Революция в 3D-сегментации: видеть невидимое",
                    "desc": "Статья представляет новый подход к амодальной сегментации трехмерных частей объектов. Авторы предлагают двухэтапный метод: сначала используется существующая сегментация 3D-частей для получения начальных неполных сегментов, затем применяется новая модель HoloPart на основе диффузии для завершения этих сегментов в полные 3D-части. HoloPart использует специализированную архитектуру с локальным и глобальным вниманием для захвата геометрии частей и обеспечения общей согласованности формы. Результаты показывают, что предложенный метод значительно превосходит современные методы завершения форм и открывает новые возможности для приложений в редактировании геометрии, анимации и назначении материалов."
                },
                "en": {
                    "title": "Unlocking Hidden Shapes: HoloPart for 3D Amodal Segmentation",
                    "desc": "This paper addresses the challenge of 3D part amodal segmentation, which involves identifying complete parts of a 3D shape even when some parts are hidden. Current methods only work with visible surfaces, limiting their effectiveness. The authors propose a two-stage approach that first uses existing segmentation techniques to identify incomplete parts, followed by a novel diffusion-based model called HoloPart to complete these segments. HoloPart employs a specialized architecture to ensure both detailed part geometry and overall shape consistency, achieving superior results on new benchmarks compared to existing methods."
                },
                "zh": {
                    "title": "突破3D分割：HoloPart模型的创新之路",
                    "desc": "本文提出了一种新的3D部分无模态分割任务，旨在将3D形状分解为完整且具有语义意义的部分，即使在被遮挡的情况下也能实现。现有的3D部分分割方法仅能识别可见的表面，限制了其应用。我们提出了一种实用的两阶段方法，首先利用现有的3D部分分割获取初步的不完整部分，然后引入HoloPart模型，通过扩散方法完成这些部分。HoloPart采用了专门的架构，结合局部注意力和全局形状一致性，显著提升了3D部分无模态分割的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07964",
            "title": "C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization\n  for Test-Time Expert Re-Mixing",
            "url": "https://huggingface.co/papers/2504.07964",
            "abstract": "Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time optimization methods to re-weight or \"re-mixing\" the experts in different layers jointly for each test sample. Since the test sample's ground truth is unknown, we propose to optimize a surrogate objective defined by the sample's \"successful neighbors\" from a reference set of samples. We introduce three surrogates and algorithms based on mode-finding, kernel regression, and the average loss of similar reference samples/tasks. To reduce the cost of optimizing whole pathways, we apply our algorithms merely to the core experts' mixing weights in critical layers, which enjoy similar performance but save significant computation. This leads to \"Critical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE LLMs and examine it on six widely-used benchmarks. It consistently improves the base model by 7-15% in accuracy and outperforms widely used test-time learning baselines, e.g., in-context learning and prompt/prefix tuning, by a large margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to outperform LLMs of 7-9B parameters, hence improving MoE's advantages on efficiency. Our thorough ablation study further sheds novel insights on achieving test-time improvement on MoE.",
            "score": 14,
            "issue_id": 3184,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "1f6d28b26eec9879",
            "authors": [
                "Zhongyang Li",
                "Ziyue Li",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Johns Hopkins University",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07964.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Оптимизация путей экспертов для повышения эффективности языковых моделей",
                    "desc": "Исследование показывает, что модели языка на основе смеси экспертов (MoE LLM) имеют значительный потенциал для улучшения точности на 10-20%. Авторы разработали новый метод оптимизации C3PO, который переоценивает веса экспертов во время вывода для каждого тестового примера. Метод использует суррогатную целевую функцию, основанную на 'успешных соседях' из эталонного набора. Применение C3PO к современным MoE LLM показало улучшение точности на 7-15% и превзошло базовые методы обучения во время тестирования."
                },
                "en": {
                    "title": "Optimize Expert Pathways for Better Performance!",
                    "desc": "This paper addresses the issue of sub-optimal expert pathways in Mixture-of-Experts (MoE) Large Language Models (LLMs), which can lead to a significant accuracy gap during inference. The authors propose a new method called C3PO, which optimizes the mixing weights of core experts in critical layers for each test sample using surrogate objectives based on similar reference samples. By focusing on optimizing only the essential components, C3PO achieves notable accuracy improvements of 7-15% over baseline models while maintaining computational efficiency. The results demonstrate that C3PO allows smaller MoE models to outperform larger LLMs, highlighting its effectiveness in enhancing model performance at lower resource costs."
                },
                "zh": {
                    "title": "优化混合专家模型的关键路径",
                    "desc": "混合专家（MoE）的大型语言模型（LLMs）在专家路径选择上存在显著的优化不足。我们的研究发现，简单的专家选择方法在预训练阶段会导致10-20%的准确率差距。为了解决这个问题，我们提出了一种新的测试时优化方法，通过对每个测试样本的不同层次的专家进行重新加权或“重新混合”。这种方法称为“关键层、核心专家、协作路径优化（C3PO）”，在多个基准测试中显示出显著的准确率提升，并且在计算效率上优于传统的学习方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07830",
            "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in\n  Multi-Agent Simulations",
            "url": "https://huggingface.co/papers/2504.07830",
            "abstract": "We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.",
            "score": 12,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "85dbaddf009300e0",
            "authors": [
                "Genglin Liu",
                "Salman Rahman",
                "Elisa Kreiss",
                "Marzyeh Ghassemi",
                "Saadia Gabriel"
            ],
            "affiliations": [
                "MIT CSAIL",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07830.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#graphs",
                    "#reasoning",
                    "#games",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "Цифровое общество под микроскопом ИИ",
                    "desc": "MOSAIC - это новая система моделирования социальных сетей с открытым исходным кодом, использующая языковые модели для имитации поведения пользователей. Она сочетает агентов на основе больших языковых моделей с направленным социальным графом для анализа поведения, связанного с обманом. Система позволяет проводить многоагентные симуляции, моделирующие распространение контента и динамику взаимодействия пользователей. Исследователи оценили три стратегии модерации контента и обнаружили, что они снижают распространение недостоверной информации и повышают вовлеченность пользователей."
                },
                "en": {
                    "title": "MOSAIC: Simulating Social Networks to Combat Misinformation",
                    "desc": "The paper introduces MOSAIC, an innovative open-source framework for simulating social networks using generative language agents. These agents predict user behaviors like liking and sharing content, allowing researchers to study how deception emerges in online interactions. By creating detailed user personas, the framework facilitates large-scale simulations of content spread and user engagement. The study evaluates various content moderation strategies, revealing that they can effectively reduce misinformation while enhancing user interaction."
                },
                "zh": {
                    "title": "MOSAIC：社交网络行为模拟与内容审核新探索",
                    "desc": "我们提出了一种新颖的开源社交网络模拟框架MOSAIC，利用生成语言代理预测用户行为，如点赞、分享和标记内容。该模拟结合了大型语言模型（LLM）代理和有向社交图，分析新出现的欺骗行为，帮助理解用户如何判断在线社交内容的真实性。通过构建多样化的细粒度用户画像，我们的系统支持大规模的多代理模拟，模拟内容传播和用户参与的动态。我们评估了三种不同的内容审核策略，发现它们不仅能减缓虚假信息的传播，还能提高用户参与度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07951",
            "title": "Scaling Laws for Native Multimodal Models Scaling Laws for Native\n  Multimodal Models",
            "url": "https://huggingface.co/papers/2504.07951",
            "abstract": "Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)--those trained from the ground up on all modalities--and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders. On the contrary, early-fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows for models that learn modality-specific weights, significantly enhancing performance.",
            "score": 10,
            "issue_id": 3190,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "66dadd5828f14551",
            "authors": [
                "Mustafa Shukor",
                "Enrico Fini",
                "Victor Guilherme Turrisi da Costa",
                "Matthieu Cord",
                "Joshua Susskind",
                "Alaaeldin El-Nouby"
            ],
            "affiliations": [
                "Apple",
                "Sorbonne University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07951.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#optimization",
                    "#agi",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективные мультимодальные модели: преимущества раннего слияния",
                    "desc": "Это исследование посвящено разработке мультимодальных моделей машинного обучения, способных воспринимать мир через различные типы данных. Авторы сравнивают архитектуры позднего и раннего слияния модальностей, проведя масштабное исследование на 457 моделях. Результаты показывают, что архитектуры раннего слияния не уступают поздним, а при меньшем количестве параметров даже превосходят их. Применение метода смеси экспертов (Mixture of Experts) позволяет моделям эффективно обучаться модально-специфичным весам, значительно улучшая производительность."
                },
                "en": {
                    "title": "Early-Fusion Models Outperform Late-Fusion in Multimodal Learning",
                    "desc": "This paper explores the effectiveness of native multimodal models (NMMs) compared to late-fusion architectures that combine pre-trained components. The authors conducted a large-scale study with 457 models to analyze the performance of early-fusion versus late-fusion approaches. They found that early-fusion models, which integrate modalities from the start, outperform late-fusion models in terms of efficiency and training ease. Additionally, by using Mixture of Experts (MoEs), the early-fusion models can learn specific weights for different modalities, further boosting their performance."
                },
                "zh": {
                    "title": "早融合架构更胜一筹！",
                    "desc": "本研究探讨了多模态模型的架构设计，特别是原生多模态模型（NMMs），这些模型从一开始就针对所有模态进行训练。我们对457个不同架构和训练组合的模型进行了广泛的规模法则研究。结果显示，后融合架构并没有比早融合架构具有固有优势，后者在较低的参数数量下表现更强，训练效率更高，部署更简单。我们还发现，结合专家混合（MoEs）可以让模型学习特定模态的权重，从而显著提升性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07934",
            "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual\n  Reasoning Self-Improvement",
            "url": "https://huggingface.co/papers/2504.07934",
            "abstract": "In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL.",
            "score": 8,
            "issue_id": 3183,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "419fda5d0a4bcadf",
            "authors": [
                "Xiyao Wang",
                "Zhengyuan Yang",
                "Chao Feng",
                "Hongjin Lu",
                "Linjie Li",
                "Chung-Ching Lin",
                "Kevin Lin",
                "Furong Huang",
                "Lijuan Wang"
            ],
            "affiliations": [
                "Microsoft",
                "University of Maryland, College Park",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07934.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#data",
                    "#reasoning",
                    "#training",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Меньше данных, больше мышления: революция в визуальном рассуждении ИИ",
                    "desc": "В этой статье представлен эффективный метод улучшения визуального рассуждения с использованием значительно меньшего количества обучающих примеров, основанный на самосовершенствовании без передачи знаний. Ключевой идеей является важность сложности обучающих данных при тонкой настройке с подкреплением (RFT). Авторы предлагают новый способ использования поиска по дереву Монте-Карло (MCTS) для количественной оценки сложности примеров и эффективной фильтрации данных. Разработанная модель ThinkLite-VL, обученная на отфильтрованном наборе из 11 тысяч примеров, превосходит существующие модели визуально-языкового рассуждения уровня 7B и достигает лучших результатов на ряде бенчмарков."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning with Fewer Samples through Smart Data Selection",
                    "desc": "This paper introduces a novel method to improve visual reasoning in models using fewer training samples through self-improvement techniques, avoiding knowledge distillation. The authors emphasize the importance of sample difficulty during reinforcement fine-tuning (RFT), suggesting that challenging samples can enhance reasoning capabilities even with limited data. They propose a unique application of Monte Carlo Tree Search (MCTS) to quantify sample difficulty, allowing for effective data filtering. The resulting model, ThinkLite-VL, demonstrates a 7% performance increase over its predecessor using only 11k samples, achieving state-of-the-art results in various benchmarks."
                },
                "zh": {
                    "title": "用少量样本提升视觉推理能力",
                    "desc": "本文提出了一种有效的方法，通过自我改进来增强视觉推理，且所需的训练样本显著减少，不依赖知识蒸馏。我们发现，在强化微调（RFT）过程中，训练数据的难度至关重要，适当具有挑战性的样本可以显著提升推理能力。我们提出了一种新颖的方式，利用蒙特卡洛树搜索（MCTS）来量化样本的难度，从而实现有效的数据筛选。最终，我们的模型ThinkLite-VL在八个基准测试中表现出色，使用仅11k个训练样本，平均性能提升了7%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.04974",
            "title": "Towards Visual Text Grounding of Multimodal Large Language Model",
            "url": "https://huggingface.co/papers/2504.04974",
            "abstract": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to their complex layouts and textual content. However, current benchmarks do not fully address these challenges, as they mostly focus on visual grounding on natural images, rather than text-rich document images. Thus, to bridge this gap, we introduce TRIG, a novel task with a newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering. Specifically, we propose an OCR-LLM-human interaction pipeline to create 800 manually annotated question-answer pairs as a benchmark and a large-scale training set of 90$ synthetic data based on four diverse datasets. A comprehensive evaluation of various MLLMs on our proposed benchmark exposes substantial limitations in their grounding capability on text-rich images. In addition, we propose two simple and effective TRIG methods based on general instruction tuning and plug-and-play efficient embedding, respectively. By finetuning MLLMs on our synthetic dataset, they promisingly improve spatial reasoning and grounding capabilities.",
            "score": 3,
            "issue_id": 3184,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "f382ad07e678a558",
            "authors": [
                "Ming Li",
                "Ruiyi Zhang",
                "Jian Chen",
                "Jiuxiang Gu",
                "Yufan Zhou",
                "Franck Dernoncourt",
                "Wanrong Zhu",
                "Tianyi Zhou",
                "Tong Sun"
            ],
            "affiliations": [
                "Adobe Research",
                "University at Buffalo",
                "University of Maryland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.04974.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#dataset",
                    "#synthetic",
                    "#transfer_learning",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "📄",
                "ru": {
                    "title": "Улучшение понимания текста на изображениях документов для мультимодальных ИИ",
                    "desc": "Статья представляет новую задачу TRIG для оценки и улучшения способностей мультимодальных языковых моделей (MLLM) в работе с текстом на изображениях документов. Авторы создали набор данных из 800 вручную размеченных пар вопрос-ответ и 90 тысяч синтетических примеров для обучения моделей. Оценка существующих MLLM на этом бенчмарке выявила значительные ограничения в их способности к пространственному рассуждению и привязке к тексту на изображениях. Предложены два метода для улучшения этих способностей: обучение на инструкциях и эффективное встраивание plug-and-play."
                },
                "en": {
                    "title": "Enhancing MLLMs for Text-Rich Image Grounding",
                    "desc": "This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in understanding text-rich images, such as documents and infographics. The authors introduce a new task called TRIG, which focuses on improving the grounding of text in these complex images for better document question-answering. They create a benchmark dataset with 800 annotated question-answer pairs and 90 synthetic data samples to evaluate MLLMs' performance. Additionally, the paper presents two methods to enhance MLLMs' spatial reasoning and grounding abilities through fine-tuning on the new dataset."
                },
                "zh": {
                    "title": "提升文档图像的文本定位能力",
                    "desc": "尽管多模态大型语言模型（MLLMs）已经取得了一定进展，但在视觉文本定位方面仍然存在显著的局限性，尤其是在文本丰富的文档图像中。文档图像如扫描表单和信息图表由于其复杂的布局和文本内容，带来了重大挑战。为了解决这一问题，我们提出了TRIG任务，并设计了一个新的指令数据集，以评估和提升MLLMs在文档问答中的文本丰富图像定位能力。通过对MLLMs进行微调，我们的方法在空间推理和定位能力上显示出显著的改进。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.07961",
            "title": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction",
            "url": "https://huggingface.co/papers/2504.07961",
            "abstract": "We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic prior captured by such video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, depth, and ray maps. It uses a new multi-modal alignment algorithm to align and fuse these modalities, as well as multiple sliding windows, at inference time, thus obtaining robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods, including recent methods such as MonST3R, which are also designed to handle dynamic scenes.",
            "score": 1,
            "issue_id": 3193,
            "pub_date": "2025-04-10",
            "pub_date_card": {
                "ru": "10 апреля",
                "en": "April 10",
                "zh": "4月10日"
            },
            "hash": "4a0d7f1cda574212",
            "authors": [
                "Zeren Jiang",
                "Chuanxia Zheng",
                "Iro Laina",
                "Diane Larlus",
                "Andrea Vedaldi"
            ],
            "affiliations": [
                "Naver Labs Europe",
                "Visual Geometry Group, University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.07961.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#benchmark",
                    "#multimodal",
                    "#long_context",
                    "#diffusion",
                    "#synthetic"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Geo4D: революция в 3D-реконструкции динамических сцен из видео",
                    "desc": "Geo4D - это метод для адаптации моделей диффузии видео к задаче монокулярной 3D-реконструкции динамических сцен. Он использует сильные динамические праймеры, захваченные видеомоделями, и может обучаться только на синтетических данных, хорошо обобщаясь на реальные данные в режиме zero-shot. Geo4D предсказывает несколько взаимодополняющих геометрических модальностей и использует новый алгоритм мультимодального выравнивания для их объединения. Эксперименты показывают, что Geo4D значительно превосходит современные методы оценки глубины видео для динамических сцен."
                },
                "en": {
                    "title": "Revolutionizing 3D Reconstruction with Geo4D",
                    "desc": "Geo4D is a novel approach that adapts video diffusion models for creating 3D reconstructions from single-camera videos of moving scenes. It effectively utilizes the dynamic information captured by these models, allowing it to be trained solely on synthetic datasets while still performing well on real-world data without additional training. The method predicts various geometric representations, including point, depth, and ray maps, and employs a unique multi-modal alignment technique to integrate these representations during the reconstruction process. Through extensive testing, Geo4D demonstrates superior performance compared to existing video depth estimation techniques, particularly in dynamic environments."
                },
                "zh": {
                    "title": "Geo4D：动态场景的4D重建新方法",
                    "desc": "Geo4D是一种将视频扩散模型用于单目3D重建动态场景的方法。它利用视频模型捕捉到的强动态先验，仅使用合成数据进行训练，并能在零样本情况下很好地推广到真实数据。Geo4D预测多种互补的几何模态，包括点图、深度图和光线图。通过新的多模态对齐算法和多个滑动窗口，Geo4D在推理时对这些模态进行对齐和融合，从而实现长视频的稳健和准确的4D重建。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06801",
            "title": "MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular\n  Detection",
            "url": "https://huggingface.co/papers/2504.06801",
            "abstract": "Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, it's particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, a novel system that considers the 3D scene content to create realistic augmentations. Specifically, given a background scene, MonoPlace3D learns a distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient.",
            "score": 1,
            "issue_id": 3188,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "bf3e9622523967d2",
            "authors": [
                "Rishubh Parihar",
                "Srinjay Sarkar",
                "Sarthak Vora",
                "Jogendra Kundu",
                "R. Venkatesh Babu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.06801.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Умное размещение синтетических объектов для улучшения монокулярной 3D детекции",
                    "desc": "MonoPlace3D - это новая система для создания реалистичных аугментаций данных для монокулярных 3D детекторов. Она учитывает содержимое 3D сцены для определения правдоподобного расположения объектов. Система обучается распределению возможных 3D ограничивающих рамок, а затем рендерит и размещает реалистичные объекты согласно этому распределению. Эксперименты на наборах данных KITTI и NuScenes показали значительное повышение точности существующих монокулярных 3D детекторов при высокой эффективности использования данных."
                },
                "en": {
                    "title": "Enhancing Monocular 3D Detection with Realistic Object Placement",
                    "desc": "This paper addresses the limitations of current monocular 3D detectors due to the lack of diverse real-world datasets. It highlights the importance of not just realistic object appearance but also the correct placement of objects in 3D scenes for effective training. The authors introduce MonoPlace3D, a system that learns to generate realistic object placements based on the content of the background scene. Their experiments show that MonoPlace3D enhances the performance of existing monocular 3D detectors on standard datasets like KITTI and NuScenes, demonstrating improved accuracy and data efficiency."
                },
                "zh": {
                    "title": "MonoPlace3D：提升单目3D检测的真实感增强",
                    "desc": "当前的单目3D检测器受到真实世界数据集多样性和规模的限制。虽然数据增强有助于改善模型性能，但在户外场景中生成真实感的增强数据尤其困难。大多数合成数据生成方法专注于通过改进渲染技术来提高物体外观的真实感，而我们发现物体的放置位置和方式对训练有效的3D单目检测器同样重要。为了解决这一问题，我们提出了MonoPlace3D系统，它考虑3D场景内容来创建真实的增强数据，从而显著提高了现有单目3D检测器的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.06752",
            "title": "Compass Control: Multi Object Orientation Control for Text-to-Image\n  Generation",
            "url": "https://huggingface.co/papers/2504.06752",
            "abstract": "Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware compass tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study.",
            "score": 1,
            "issue_id": 3188,
            "pub_date": "2025-04-09",
            "pub_date_card": {
                "ru": "9 апреля",
                "en": "April 9",
                "zh": "4月9日"
            },
            "hash": "d73e0b8851ffd9e3",
            "authors": [
                "Rishubh Parihar",
                "Vaibhav Agrawal",
                "Sachidanand VS",
                "R. Venkatesh Babu"
            ],
            "affiliations": [
                "IIIT Hyderabad",
                "IISc Bangalore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.06752.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d",
                    "#training",
                    "#diffusion",
                    "#synthetic"
                ],
                "emoji": "🧭",
                "ru": {
                    "title": "Точное управление ориентацией множественных объектов в генеративных моделях изображений",
                    "desc": "Эта статья представляет новый метод управления ориентацией объектов в генеративных моделях изображений на основе диффузии. Авторы предлагают использовать специальные токены-компасы для каждого объекта, которые кодируют информацию об ориентации. Модель обучается на синтетическом наборе данных с процедурно сгенерированными сценами. Для улучшения контроля и предотвращения смешивания объектов применяется ограничение карт перекрестного внимания."
                },
                "en": {
                    "title": "Mastering Object Orientation in Text-to-Image Generation",
                    "desc": "This paper presents a novel approach to enhance text-to-image diffusion models by enabling precise control over the orientation of multiple objects in generated scenes. The authors introduce orientation-aware compass tokens that are conditioned on the object orientation, allowing for better manipulation of object placement and alignment. A lightweight encoder network predicts these tokens based on input orientations, addressing challenges like poor control and object entanglement during training. The proposed method demonstrates strong generalization capabilities, achieving state-of-the-art results in orientation control and text alignment through extensive evaluations and user studies."
                },
                "zh": {
                    "title": "精确控制多对象方向的创新方法",
                    "desc": "本文提出了一种新的方法来控制文本到图像的扩散模型，特别是多对象的方向控制。通过引入方向感知的指南针标记，模型能够为每个对象提供精确的方向控制。研究表明，该模型在生成复杂对象和多对象场景时表现出强大的泛化能力。结合个性化方法后，模型能够在多种上下文中精确控制新对象的方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05579",
            "title": "TAPNext: Tracking Any Point (TAP) as Next Token Prediction",
            "url": "https://huggingface.co/papers/2504.05579",
            "abstract": "Tracking Any Point (TAP) in a video is a challenging computer vision problem with many demonstrated applications in robotics, video editing, and 3D reconstruction. Existing methods for TAP rely heavily on complex tracking-specific inductive biases and heuristics, limiting their generality and potential for scaling. To address these challenges, we present TAPNext, a new approach that casts TAP as sequential masked token decoding. Our model is causal, tracks in a purely online fashion, and removes tracking-specific inductive biases. This enables TAPNext to run with minimal latency, and removes the temporal windowing required by many existing state of art trackers. Despite its simplicity, TAPNext achieves a new state-of-the-art tracking performance among both online and offline trackers. Finally, we present evidence that many widely used tracking heuristics emerge naturally in TAPNext through end-to-end training.",
            "score": 1,
            "issue_id": 3192,
            "pub_date": "2025-04-08",
            "pub_date_card": {
                "ru": "8 апреля",
                "en": "April 8",
                "zh": "4月8日"
            },
            "hash": "27a89bfc540d93f8",
            "authors": [
                "Artem Zholus",
                "Carl Doersch",
                "Yi Yang",
                "Skanda Koppula",
                "Viorica Patraucean",
                "Xu Owen He",
                "Ignacio Rocco",
                "Mehdi S. M. Sajjadi",
                "Sarath Chandar",
                "Ross Goroshin"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "Chandar Research Lab",
                "Google DeepMind",
                "Mila - Quebec AI Institute",
                "Polytechnique Montreal",
                "University College London",
                "Université de Montréal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05579.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#architecture",
                    "#video",
                    "#robotics"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "TAPNext: Революция в отслеживании точек видео без специализированных предубеждений",
                    "desc": "TAPNext - это новый подход к отслеживанию точек в видео, который представляет эту задачу как последовательное декодирование маскированных токенов. Модель работает в причинном режиме, отслеживает в режиме реального времени и устраняет специфические для трекинга индуктивные предубеждения. TAPNext достигает нового уровня производительности среди онлайн и офлайн трекеров, несмотря на свою простоту. Исследование показывает, что многие широко используемые эвристики трекинга естественным образом возникают в TAPNext через сквозное обучение."
                },
                "en": {
                    "title": "Revolutionizing Video Tracking with TAPNext",
                    "desc": "The paper introduces TAPNext, a novel method for tracking any point (TAP) in videos, which is crucial for applications like robotics and video editing. Unlike traditional methods that depend on complex rules and biases, TAPNext simplifies the process by using sequential masked token decoding. This approach allows for real-time tracking without the need for temporal windowing, resulting in lower latency. TAPNext not only achieves superior tracking performance compared to existing methods but also shows that common tracking heuristics can be learned through end-to-end training."
                },
                "zh": {
                    "title": "TAPNext：简化视频跟踪的新方法",
                    "desc": "本文介绍了一种新的视频跟踪方法，称为TAPNext，旨在解决跟踪任意点（TAP）的问题。与现有方法不同，TAPNext将TAP视为顺序掩码令牌解码，消除了复杂的跟踪特定偏见。该模型具有因果性，能够在线实时跟踪，且延迟极低。尽管方法简单，TAPNext在在线和离线跟踪器中都达到了新的最先进的跟踪性能。"
                }
            }
        }
    ],
    "link_prev": "2025-04-10.html",
    "link_next": "2025-04-14.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "10.04",
        "en": "04/10",
        "zh": "4月10日"
    },
    "short_date_next": {
        "ru": "14.04",
        "en": "04/14",
        "zh": "4月14日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 7,
        "#agents": 2,
        "#cv": 5,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 4,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 1,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 6,
        "#transfer_learning": 2,
        "#graphs": 2,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 4,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "我们介绍了Kimi-VL，一个高效的开源混合专家视觉语言模型。它在多模态推理、长上下文理解和强大的代理能力方面表现出色，仅激活语言解码器中的2.8B参数。Kimi-VL在多轮代理任务和多种视觉语言任务中表现优异，并在长上下文处理和高分辨率视觉输入理解方面取得进展。基于Kimi-VL，我们还开发了Kimi-VL-Thinking，具有强大的长时间推理能力。代码和模型在https://github.com/MoonshotAI/Kimi-VL公开可用。",
        "title": "Kimi-VL Technical Report",
        "pinyin": "我们介绍了Kimi-VL，一个高效的开源混合专家视觉语言模型。\nWǒmen jièshào le Kimi-VL, yīgè gāoxiào de kāiyuán hùnhé zhuānjiā shìjiué yǔyán móxíng.\n\n它在多模态推理、长上下文理解和强大的代理能力方面表现出色，仅激活语言解码器中的2.8B参数。\nTā zài duō móshì tuīlǐ, cháng shàngxìawén lǐjiě hé qiángdà de dàilǐ nénglì fāngmiàn biǎoxiàn chūsè, jǐn jīhuó yǔyán jiěmǎqì zhōng de 2.8B cānshù.\n\nKimi-VL在多轮代理任务和多种视觉语言任务中表现优异，并在长上下文处理和高分辨率视觉输入理解方面取得进展。\nKimi-VL zài duō lún dàilǐ rènwù hé duō zhǒng shìjiué yǔyán rènwù zhōng biǎoxiàn yōuyì, bìng zài cháng shàngxìawén chǔlǐ hé gāo fēnbiànlǜ shìjiué shūrù lǐjiě fāngmiàn qǔdé jìnzhàn.\n\n基于Kimi-VL，我们还开发了Kimi-VL-Thinking，具有强大的长时间推理能力。\nJīyú Kimi-VL, wǒmen hái kāifā le Kimi-VL-Thinking, jùyǒu qiángdà de cháng shíjiān tuīlǐ nénglì.\n\n代码和模型在https://github.com/MoonshotAI/Kimi-VL公开可用。\nDàimǎ hé móxíng zài https://github.com/MoonshotAI/Kimi-VL gōngkāi kěyòng.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"高效\", \"pinyin\": \"gāo xiào\", \"trans\": \"efficient\"},\n    {\"word\": \"开源\", \"pinyin\": \"kāi yuán\", \"trans\": \"open-source\"},\n    {\"word\": \"混合\", \"pinyin\": \"hùn hé\", \"trans\": \"hybrid\"},\n    {\"word\": \"专家\", \"pinyin\": \"zhuān jiā\", \"trans\": \"expert\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"语言\", \"pinyin\": \"yǔ yán\", \"trans\": \"language\"},\n    {\"word\": \"模型\", \"pinyin\": \"mó xíng\", \"trans\": \"model\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"长上下文\", \"pinyin\": \"cháng shàng xià wén\", \"trans\": \"long context\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understanding\"},\n    {\"word\": \"强大\", \"pinyin\": \"qiáng dà\", \"trans\": \"powerful\"},\n    {\"word\": \"代理\", \"pinyin\": \"dài lǐ\", \"trans\": \"agent\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"激活\", \"pinyin\": \"jī huó\", \"trans\": \"activate\"},\n    {\"word\": \"解码器\", \"pinyin\": \"jiě mǎ qì\", \"trans\": \"decoder\"},\n    {\"word\": \"参数\", \"pinyin\": \"cān shù\", \"trans\": \"parameter\"},\n    {\"word\": \"多轮\", \"pinyin\": \"duō lún\", \"trans\": \"multi-turn\"},\n    {\"word\": \"任务\", \"pinyin\": \"rèn wù\", \"trans\": \"task\"},\n    {\"word\": \"优异\", \"pinyin\": \"yōu yì\", \"trans\": \"excellent\"},\n    {\"word\": \"进展\", \"pinyin\": \"jìn zhǎn\", \"trans\": \"progress\"},\n    {\"word\": \"处理\", \"pinyin\": \"chǔ lǐ\", \"trans\": \"process\"},\n    {\"word\": \"高分辨率\", \"pinyin\": \"gāo fēn biàn lǜ\", \"trans\": \"high resolution\"},\n    {\"word\": \"输入\", \"pinyin\": \"shū rù\", \"trans\": \"input\"},\n    {\"word\": \"基于\", \"pinyin\": \"jī yú\", \"trans\": \"based on\"},\n    {\"word\": \"开发\", \"pinyin\": \"kāi fā\", \"trans\": \"develop\"},\n    {\"word\": \"长时间\", \"pinyin\": \"cháng shí jiān\", \"trans\": \"long-term\"},\n    {\"word\": \"公开\", \"pinyin\": \"gōng kāi\", \"trans\": \"public\"},\n    {\"word\": \"可用\", \"pinyin\": \"kě yòng\", \"trans\": \"available\"}\n]",
        "trans": "We introduce Kimi-VL, an efficient open-source hybrid expert visual language model. It excels in multimodal reasoning, long-context understanding, and strong agent capabilities, activating only 2.8B parameters in the language decoder. Kimi-VL performs exceptionally well in multi-turn agent tasks and various visual language tasks, making progress in long-context processing and high-resolution visual input understanding. Based on Kimi-VL, we also developed Kimi-VL-Thinking, which has strong long-term reasoning capabilities. The code and model are publicly available at https://github.com/MoonshotAI/Kimi-VL.",
        "update_ts": "2025-04-11 09:11"
    }
}