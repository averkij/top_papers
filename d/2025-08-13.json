{
    "date": {
        "ru": "13 августа",
        "en": "August 13",
        "zh": "8月13日"
    },
    "time_utc": "2025-08-13 04:23",
    "weekday": 2,
    "issue_id": 5319,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.08086",
            "title": "Matrix-3D: Omnidirectional Explorable 3D World Generation",
            "url": "https://huggingface.co/papers/2508.08086",
            "abstract": "Matrix-3D generates wide-coverage 3D worlds from single images or text using panoramic video diffusion and reconstruction models.  \t\t\t\t\tAI-generated summary \t\t\t\t Explorable 3D world generation from a single image or text prompt forms a cornerstone of spatial intelligence. Recent works utilize video model to achieve wide-scope and generalizable 3D world generation. However, existing approaches often suffer from a limited scope in the generated scenes. In this work, we propose Matrix-3D, a framework that utilize panoramic representation for wide-coverage omnidirectional explorable 3D world generation that combines conditional video generation and panoramic 3D reconstruction. We first train a trajectory-guided panoramic video diffusion model that employs scene mesh renders as condition, to enable high-quality and geometrically consistent scene video generation. To lift the panorama scene video to 3D world, we propose two separate methods: (1) a feed-forward large panorama reconstruction model for rapid 3D scene reconstruction and (2) an optimization-based pipeline for accurate and detailed 3D scene reconstruction. To facilitate effective training, we also introduce the Matrix-Pano dataset, the first large-scale synthetic collection comprising 116K high-quality static panoramic video sequences with depth and trajectory annotations. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance in panoramic video generation and 3D world generation. See more in https://matrix-3d.github.io.",
            "score": 32,
            "issue_id": 5317,
            "pub_date": "2025-08-11",
            "pub_date_card": {
                "ru": "11 августа",
                "en": "August 11",
                "zh": "8月11日"
            },
            "hash": "70a9a4327de06f04",
            "authors": [
                "Zhongqi Yang",
                "Wenhang Ge",
                "Yuqi Li",
                "Jiaqi Chen",
                "Haoyuan Li",
                "Mengyin An",
                "Fei Kang",
                "Hua Xue",
                "Baixin Xu",
                "Yuyang Yin",
                "Eric Li",
                "Yang Liu",
                "Yikai Wang",
                "Hao-Xiang Guo",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology (Guangzhou)",
                "Institute of Computing Technology, Chinese Academy of Sciences",
                "School of Artificial Intelligence, Beijing Normal University",
                "Skywork AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.08086.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#diffusion",
                    "#3d",
                    "#synthetic"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Генерация исследуемых 3D-миров из одного изображения или текста",
                    "desc": "Matrix-3D - это система для создания исследуемых трехмерных миров на основе одного изображения или текстового запроса. Она использует панорамную видеодиффузию и модели реконструкции для генерации широкоохватных 3D-сцен. Система включает в себя условную генерацию видео с учетом траектории и два метода реконструкции 3D: быстрый прямой и точный оптимизационный. Для обучения был создан набор данных Matrix-Pano из 116 тысяч синтетических панорамных видеопоследовательностей."
                },
                "en": {
                    "title": "Transforming Images into Immersive 3D Worlds",
                    "desc": "Matrix-3D is a novel framework designed to create expansive 3D environments from a single image or text input by leveraging advanced panoramic video diffusion and reconstruction techniques. It addresses the limitations of previous methods by employing a trajectory-guided video diffusion model that generates high-quality scene videos, ensuring geometric consistency. The framework includes two distinct approaches for converting panoramic videos into 3D worlds: a fast feed-forward model for quick reconstructions and an optimization-based method for detailed accuracy. Additionally, the introduction of the Matrix-Pano dataset, which contains a large collection of panoramic video sequences with depth and trajectory data, supports effective training and enhances the overall performance of the system."
                },
                "zh": {
                    "title": "从单图像生成全景 3D 世界的创新框架",
                    "desc": "Matrix-3D 是一个生成广泛覆盖的 3D 世界的框架，能够从单张图像或文本提示中生成可探索的 3D 世界。该框架结合了条件视频生成和全景 3D 重建，利用全景表示来实现全方位的 3D 世界生成。我们首先训练了一个基于轨迹引导的全景视频扩散模型，以场景网格渲染作为条件，从而实现高质量和几何一致的场景视频生成。为了将全景场景视频提升为 3D 世界，我们提出了两种方法：快速的前馈全景重建模型和基于优化的精确 3D 场景重建流程。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05748",
            "title": "WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent",
            "url": "https://huggingface.co/papers/2508.05748",
            "abstract": "WebWatcher, a multimodal agent with enhanced visual-language reasoning, outperforms existing agents in complex visual and textual information retrieval tasks using synthetic trajectories and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Web agents such as Deep Research have demonstrated superhuman cognitive abilities, capable of solving highly challenging information-seeking problems. However, most research remains primarily text-centric, overlooking visual information in the real world. This makes multimodal Deep Research highly challenging, as such agents require much stronger reasoning abilities in perception, logic, knowledge, and the use of more sophisticated tools compared to text-based agents. To address this limitation, we introduce WebWatcher, a multi-modal Agent for Deep Research equipped with enhanced visual-language reasoning capabilities. It leverages high-quality synthetic multimodal trajectories for efficient cold start training, utilizes various tools for deep reasoning, and further enhances generalization through reinforcement learning. To better evaluate the capabilities of multimodal agents, we propose BrowseComp-VL, a benchmark with BrowseComp-style that requires complex information retrieval involving both visual and textual information. Experimental results show that WebWatcher significantly outperforms proprietary baseline, RAG workflow and open-source agents in four challenging VQA benchmarks, which paves the way for solving complex multimodal information-seeking tasks.",
            "score": 25,
            "issue_id": 5318,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 августа",
                "en": "August 7",
                "zh": "8月7日"
            },
            "hash": "937594202a5b31b7",
            "authors": [
                "Xinyu Geng",
                "Peng Xia",
                "Zhen Zhang",
                "Xinyu Wang",
                "Qiuchen Wang",
                "Ruixue Ding",
                "Chenxi Wang",
                "Jialong Wu",
                "Yida Zhao",
                "Kuan Li",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05748.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#synthetic",
                    "#multimodal",
                    "#agents",
                    "#rl",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "WebWatcher: Мультимодальный агент для глубокого исследования визуальной и текстовой информации",
                    "desc": "WebWatcher - это мультимодальный агент с улучшенными возможностями визуально-языкового рассуждения. Он превосходит существующие агенты в сложных задачах поиска визуальной и текстовой информации, используя синтетические траектории и обучение с подкреплением. WebWatcher использует высококачественные синтетические мультимодальные траектории для эффективного холодного старта обучения и применяет различные инструменты для глубокого рассуждения. Для оценки возможностей мультимодальных агентов авторы предлагают бенчмарк BrowseComp-VL, требующий сложного поиска информации с использованием как визуальных, так и текстовых данных."
                },
                "en": {
                    "title": "WebWatcher: Revolutionizing Multimodal Information Retrieval",
                    "desc": "WebWatcher is a multimodal agent designed to improve the retrieval of complex visual and textual information. It utilizes synthetic trajectories and reinforcement learning to enhance its visual-language reasoning capabilities. Unlike traditional text-centric agents, WebWatcher effectively integrates visual data, allowing for better reasoning and problem-solving. The introduction of the BrowseComp-VL benchmark further validates its superior performance in multimodal tasks compared to existing agents."
                },
                "zh": {
                    "title": "WebWatcher：超越文本的多模态智能体",
                    "desc": "WebWatcher是一种多模态智能体，具备增强的视觉语言推理能力，能够在复杂的视觉和文本信息检索任务中超越现有智能体。该系统利用高质量的合成多模态轨迹进行高效的冷启动训练，并通过强化学习进一步提升其泛化能力。WebWatcher在多个视觉问答基准测试中表现优异，显示出其在处理复杂多模态信息检索任务方面的潜力。为评估多模态智能体的能力，本文还提出了BrowseComp-VL基准，专注于视觉和文本信息的复杂检索。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.07976",
            "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL",
            "url": "https://huggingface.co/papers/2508.07976",
            "abstract": "ASearcher is an open-source project that uses scalable asynchronous RL training to enhance search agents, achieving high performance on QA tasks with long-horizon search capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. <=10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns and output tokens exceeding 150k during training time. With a simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We open-source our models, training data, and codes in https://github.com/inclusionAI/ASearcher.",
            "score": 15,
            "issue_id": 5318,
            "pub_date": "2025-08-11",
            "pub_date_card": {
                "ru": "11 августа",
                "en": "August 11",
                "zh": "8月11日"
            },
            "hash": "22a9c1c0433f2764",
            "authors": [
                "Jiaxuan Gao",
                "Wei Fu",
                "Minyang Xie",
                "Shusheng Xu",
                "Chuyi He",
                "Zhiyu Mei",
                "Banghua Zhu",
                "Yi Wu"
            ],
            "affiliations": [
                "Ant Research, RL Lab",
                "IIIS, Tsinghua University",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.07976.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#agents",
                    "#rl",
                    "#long_context",
                    "#open_source"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "ASearcher: Революция в обучении поисковых ИИ-агентов",
                    "desc": "ASearcher - это проект с открытым исходным кодом, использующий масштабируемое асинхронное обучение с подкреплением для улучшения поисковых агентов. Он достигает высокой производительности в задачах вопросно-ответного типа с возможностью долгосрочного поиска. Ключевые особенности включают полностью асинхронное обучение с подкреплением и агент на основе языковой модели, который автономно создает качественные наборы данных вопросов и ответов. ASearcher превосходит существующие открытые агенты на 32 миллиарда параметров в бенчмарках xBench и GAIA."
                },
                "en": {
                    "title": "Unlocking Expert-Level Search Intelligence with ASearcher",
                    "desc": "ASearcher is an innovative open-source project that enhances search agents through scalable asynchronous reinforcement learning (RL) training. It addresses the limitations of existing methods by enabling long-horizon search capabilities, allowing agents to learn complex strategies over extended interactions. The project introduces a prompt-based large language model (LLM) agent that autonomously generates high-quality question-answer pairs, significantly improving the dataset for training. As a result, ASearcher achieves impressive performance metrics, outperforming previous open-source agents in various QA tasks."
                },
                "zh": {
                    "title": "ASearcher：提升搜索智能的开源强化学习项目",
                    "desc": "ASearcher是一个开源项目，利用可扩展的异步强化学习（RL）训练来增强搜索代理的能力，特别是在长时间搜索的问答任务中表现出色。该项目的关键贡献包括可扩展的完全异步RL训练，能够在保持高训练效率的同时进行长时间搜索。通过基于提示的LLM代理，ASearcher能够自主合成高质量的问答，创建大规模的问答数据集。经过RL训练，ASearcher的代理在多个基准测试中显著提高了性能，展示了其在复杂搜索任务中的强大能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09138",
            "title": "Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language\n  Models",
            "url": "https://huggingface.co/papers/2508.09138",
            "abstract": "Two methods, Temporal Self-Consistency Voting and Temporal Consistency Reinforcement, improve diffusion large language models by leveraging temporal consistency in intermediate predictions.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (dLLMs) generate text through iterative denoising, yet current decoding strategies discard rich intermediate predictions in favor of the final output. Our work here reveals a critical phenomenon, temporal oscillation, where correct answers often emerge in the middle process, but are overwritten in later denoising steps. To address this issue, we introduce two complementary methods that exploit temporal consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output; and 2) a post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a measure of semantic stability across intermediate predictions, as a reward signal to encourage stable generations. Empirical results across multiple benchmarks demonstrate the effectiveness of our approach. Using the negative TSE reward alone, we observe a remarkable average improvement of 24.7% on the Countdown dataset over an existing dLLM. Combined with the accuracy reward, we achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown, respectively. Our findings underscore the untapped potential of temporal dynamics in dLLMs and offer two simple yet effective tools to harness them.",
            "score": 12,
            "issue_id": 5317,
            "pub_date": "2025-08-12",
            "pub_date_card": {
                "ru": "12 августа",
                "en": "August 12",
                "zh": "8月12日"
            },
            "hash": "fc4661402e36332d",
            "authors": [
                "Wen Wang",
                "Bozhen Fang",
                "Chenchen Jing",
                "Yongliang Shen",
                "Yangyi Shen",
                "Qiuyu Wang",
                "Hao Ouyang",
                "Hao Chen",
                "Chunhua Shen"
            ],
            "affiliations": [
                "Ant Group",
                "Stanford University",
                "Zhejiang University",
                "Zhejiang University of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09138.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "⏳",
                "ru": {
                    "title": "Использование временной динамики для улучшения диффузионных языковых моделей",
                    "desc": "Статья представляет два метода улучшения диффузионных больших языковых моделей (dLLM): Временное самосогласованное голосование и Усиление временной согласованности. Эти методы используют временную согласованность в промежуточных предсказаниях модели для повышения качества генерации текста. Авторы обнаружили феномен временных осцилляций, когда правильные ответы часто появляются в середине процесса, но перезаписываются на последних шагах. Эмпирические результаты показывают значительное улучшение производительности на нескольких бенчмарках, включая GSM8K, MATH500, SVAMP и Countdown."
                },
                "en": {
                    "title": "Harnessing Temporal Consistency for Better Language Model Outputs",
                    "desc": "This paper presents two innovative methods to enhance diffusion large language models (dLLMs) by focusing on the temporal consistency of their predictions. The authors identify a problem called temporal oscillation, where valuable intermediate outputs are lost during the final denoising process. To combat this, they propose Temporal Self-Consistency Voting, which aggregates predictions from various steps to find the most reliable output, and Temporal Consistency Reinforcement, which uses a reward based on Temporal Semantic Entropy to promote stable outputs. Their experiments show significant improvements in performance across several benchmarks, highlighting the importance of leveraging temporal dynamics in language model generation."
                },
                "zh": {
                    "title": "利用时间一致性提升大语言模型的性能",
                    "desc": "本文提出了两种方法，时间自一致性投票和时间一致性强化，旨在通过利用中间预测的时间一致性来改进扩散大语言模型（dLLMs）。研究发现，在去噪的过程中，正确答案往往在中间步骤中出现，但在后续步骤中被覆盖。为了解决这个问题，提出的时间自一致性投票方法在测试时聚合多个去噪步骤的预测，以选择最一致的输出；而时间一致性强化则使用时间语义熵作为奖励信号，鼓励生成稳定的结果。实验证明，这些方法在多个基准测试中显著提高了模型的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.08088",
            "title": "HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating\n  Local and Web Searches",
            "url": "https://huggingface.co/papers/2508.08088",
            "abstract": "HierSearch, a hierarchical agentic deep search framework using hierarchical RL, improves performance in multi-source retrieval tasks by coordinating local and Web search agents and refining knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, large reasoning models have demonstrated strong mathematical and coding abilities, and deep search leverages their reasoning capabilities in challenging information retrieval tasks. Existing deep search works are generally limited to a single knowledge source, either local or the Web. However, enterprises often require private deep search systems that can leverage search tools over both local and the Web corpus. Simply training an agent equipped with multiple search tools using flat reinforcement learning (RL) is a straightforward idea, but it has problems such as low training data efficiency and poor mastery of complex tools. To address the above issue, we propose a hierarchical agentic deep search framework, HierSearch, trained with hierarchical RL. At the low level, a local deep search agent and a Web deep search agent are trained to retrieve evidence from their corresponding domains. At the high level, a planner agent coordinates low-level agents and provides the final answer. Moreover, to prevent direct answer copying and error propagation, we design a knowledge refiner that filters out hallucinations and irrelevant evidence returned by low-level agents. Experiments show that HierSearch achieves better performance compared to flat RL, and outperforms various deep search and multi-source retrieval-augmented generation baselines in six benchmarks across general, finance, and medical domains.",
            "score": 10,
            "issue_id": 5317,
            "pub_date": "2025-08-11",
            "pub_date_card": {
                "ru": "11 августа",
                "en": "August 11",
                "zh": "8月11日"
            },
            "hash": "7e2ae00381bb3360",
            "authors": [
                "Jiejun Tan",
                "Zhicheng Dou",
                "Yan Yu",
                "Jiehan Cheng",
                "Qiang Ju",
                "Jian Xie",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "Baichuan Intelligent Technology",
                "Gaoling School of Artificial Intelligence, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.08088.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#rag",
                    "#reasoning",
                    "#agents",
                    "#benchmark",
                    "#rl",
                    "#healthcare"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Иерархический глубокий поиск для эффективного многоисточникового извлечения информации",
                    "desc": "HierSearch - это иерархическая система глубокого поиска, использующая иерархическое обучение с подкреплением для улучшения многоисточникового поиска. Она координирует локальных и веб-агентов поиска, а также уточняет полученные знания. HierSearch превосходит плоское обучение с подкреплением и другие методы в шести тестах в общей, финансовой и медицинской областях. Система предотвращает прямое копирование ответов и распространение ошибок с помощью специального фильтра знаний."
                },
                "en": {
                    "title": "HierSearch: Elevating Multi-Source Retrieval with Hierarchical RL",
                    "desc": "HierSearch is a hierarchical framework that enhances multi-source information retrieval by using hierarchical reinforcement learning (RL). It consists of local and Web search agents that work together to gather relevant information from different sources. A planner agent oversees these agents to ensure accurate and coherent answers, while a knowledge refiner filters out incorrect or irrelevant data. This approach significantly improves performance over traditional flat RL methods and outperforms existing deep search systems across various domains."
                },
                "zh": {
                    "title": "层次化智能深度搜索，提升多源检索性能",
                    "desc": "HierSearch是一种层次化的智能深度搜索框架，利用层次化强化学习来提升多源检索任务的性能。该框架通过协调本地搜索代理和网络搜索代理，优化知识获取过程。与传统的单一知识源深度搜索不同，HierSearch能够同时利用本地和网络数据，满足企业对私有深度搜索系统的需求。此外，HierSearch还设计了知识精炼器，以过滤低级代理返回的错误信息和无关证据，从而提高最终答案的准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05615",
            "title": "Test-Time Reinforcement Learning for GUI Grounding via Region\n  Consistency",
            "url": "https://huggingface.co/papers/2508.05615",
            "abstract": "GUI-RC and GUI-RCPO enhance GUI grounding accuracy by leveraging spatial consistency and reinforcement learning without additional training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical User Interface (GUI) grounding, the task of mapping natural language instructions to precise screen coordinates, is fundamental to autonomous GUI agents. While existing methods achieve strong performance through extensive supervised training or reinforcement learning with labeled rewards, they remain constrained by the cost and availability of pixel-level annotations. We observe that when models generate multiple predictions for the same GUI element, the spatial overlap patterns reveal implicit confidence signals that can guide more accurate localization. Leveraging this insight, we propose GUI-RC (Region Consistency), a test-time scaling method that constructs spatial voting grids from multiple sampled predictions to identify consensus regions where models show highest agreement. Without any training, GUI-RC improves accuracy by 2-3% across various architectures on ScreenSpot benchmarks. We further introduce GUI-RCPO (Region Consistency Policy Optimization), which transforms these consistency patterns into rewards for test-time reinforcement learning. By computing how well each prediction aligns with the collective consensus, GUI-RCPO enables models to iteratively refine their outputs on unlabeled data during inference. Extensive experiments demonstrate the generality of our approach: GUI-RC boosts Qwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO further improves it to 85.14% through self-supervised optimization. Our approach reveals the untapped potential of test-time scaling and test-time reinforcement learning for GUI grounding, offering a promising path toward more robust and data-efficient GUI agents.",
            "score": 7,
            "issue_id": 5317,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 августа",
                "en": "August 7",
                "zh": "8月7日"
            },
            "hash": "b571750771c4ba60",
            "authors": [
                "Yong Du",
                "Yuchen Yan",
                "Fei Tang",
                "Zhengxi Lu",
                "Chang Zong",
                "Weiming Lu",
                "Shengpei Jiang",
                "Yongliang Shen"
            ],
            "affiliations": [
                "Central South University",
                "SF Technology",
                "Zhejiang University",
                "Zhejiang University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05615.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#agents",
                    "#rlhf",
                    "#rl"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "Повышение точности ИИ-агентов для GUI без дополнительных данных",
                    "desc": "В статье представлены методы GUI-RC и GUI-RCPO для повышения точности привязки графического интерфейса без использования дополнительных обучающих данных. GUI-RC использует пространственную согласованность для идентификации областей с наибольшим согласием модели. GUI-RCPO применяет обучение с подкреплением во время вывода, используя сигналы согласованности в качестве награды. Эксперименты показывают значительное улучшение точности на бенчмарке ScreenSpot для различных архитектур моделей."
                },
                "en": {
                    "title": "Enhancing GUI Grounding with Spatial Consistency and Reinforcement Learning",
                    "desc": "This paper presents GUI-RC and GUI-RCPO, two innovative methods that improve the accuracy of GUI grounding by utilizing spatial consistency and reinforcement learning without needing extra training data. GUI grounding involves translating natural language commands into specific screen coordinates, which is crucial for autonomous GUI agents. The authors propose a technique called GUI-RC that uses spatial voting grids to identify areas of agreement among multiple predictions, enhancing localization accuracy. Additionally, GUI-RCPO applies reinforcement learning to refine predictions based on how well they align with the consensus, leading to significant performance improvements on benchmark tests."
                },
                "zh": {
                    "title": "提升GUI定位准确性的创新方法",
                    "desc": "本文提出了GUI-RC和GUI-RCPO两种方法，以提高图形用户界面（GUI）定位的准确性。这些方法利用空间一致性和强化学习，而无需额外的训练数据。GUI-RC通过构建空间投票网格，从多个预测中识别共识区域，从而在测试时提高准确性。GUI-RCPO则将一致性模式转化为奖励，允许模型在推理过程中对未标记数据进行自我优化。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05399",
            "title": "UNCAGE: Contrastive Attention Guidance for Masked Generative\n  Transformers in Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2508.05399",
            "abstract": "UNCAGE, a training-free method using contrastive attention guidance, enhances compositional fidelity in text-to-image generation by prioritizing the unmasking of object-representing tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) generation has been actively studied using Diffusion Models and Autoregressive Models. Recently, Masked Generative Transformers have gained attention as an alternative to Autoregressive Models to overcome the inherent limitations of causal attention and autoregressive decoding through bidirectional attention and parallel decoding, enabling efficient and high-quality image generation. However, compositional T2I generation remains challenging, as even state-of-the-art Diffusion Models often fail to accurately bind attributes and achieve proper text-image alignment. While Diffusion Models have been extensively studied for this issue, Masked Generative Transformers exhibit similar limitations but have not been explored in this context. To address this, we propose Unmasking with Contrastive Attention Guidance (UNCAGE), a novel training-free method that improves compositional fidelity by leveraging attention maps to prioritize the unmasking of tokens that clearly represent individual objects. UNCAGE consistently improves performance in both quantitative and qualitative evaluations across multiple benchmarks and metrics, with negligible inference overhead. Our code is available at https://github.com/furiosa-ai/uncage.",
            "score": 4,
            "issue_id": 5317,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 августа",
                "en": "August 7",
                "zh": "8月7日"
            },
            "hash": "2e4f45b5a135f498",
            "authors": [
                "Wonjun Kang",
                "Byeongkeun Ahn",
                "Minjae Lee",
                "Kevin Galim",
                "Seunghyuk Oh",
                "Hyung Il Koo",
                "Nam Ik Cho"
            ],
            "affiliations": [
                "Ajou University",
                "FuriosaAI",
                "Independent Researcher",
                "Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05399.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#diffusion",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Точная композиция в генерации изображений через умное раскрытие токенов",
                    "desc": "UNCAGE - это новый метод без дополнительного обучения, улучшающий композиционную точность в генерации изображений по тексту. Он использует контрастное управление вниманием для приоритизации раскрытия токенов, представляющих отдельные объекты. Метод применяется к маскированным генеративным трансформерам, которые ранее не исследовались в контексте композиционной генерации. UNCAGE показывает улучшение производительности по количественным и качественным оценкам на различных бенчмарках."
                },
                "en": {
                    "title": "Enhancing Text-to-Image Generation with UNCAGE",
                    "desc": "The paper introduces UNCAGE, a novel method that enhances the quality of text-to-image (T2I) generation without requiring additional training. It utilizes contrastive attention guidance to focus on unmasking tokens that represent distinct objects, improving compositional fidelity in generated images. This approach addresses the limitations of existing models, particularly in accurately binding attributes and achieving text-image alignment. The results show that UNCAGE outperforms previous methods in both quantitative and qualitative assessments while maintaining low computational overhead."
                },
                "zh": {
                    "title": "UNCAGE：提升文本到图像生成的组合保真度",
                    "desc": "UNCAGE是一种无训练的方法，利用对比注意力引导来增强文本到图像生成中的组合保真度。该方法通过优先解码清晰表示单个对象的标记，来改善文本与图像的对齐。尽管现有的扩散模型在组合生成方面存在挑战，UNCAGE在多个基准测试中表现出色，提升了生成质量。该方法在推理时几乎没有额外开销，展示了其高效性和实用性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09101",
            "title": "AutoCodeBench: Large Language Models are Automatic Code Benchmark\n  Generators",
            "url": "https://huggingface.co/papers/2508.09101",
            "abstract": "AutoCodeGen creates a large-scale, multilingual code generation benchmark, AutoCodeBench, to evaluate LLMs on diverse and complex tasks without manual annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, with code generation emerging as a key area of focus. While numerous benchmarks have been proposed to evaluate their code generation abilities, these benchmarks face several critical limitations. First, they often rely on manual annotations, which are time-consuming and difficult to scale across different programming languages and problem complexities. Second, most existing benchmarks focus primarily on Python, while the few multilingual benchmarks suffer from limited difficulty and uneven language distribution. To address these challenges, we propose AutoCodeGen, an automated method for generating high-difficulty multilingual code generation datasets without manual annotations. AutoCodeGen ensures the correctness and completeness of test cases by generating test inputs with LLMs and obtaining test outputs through a multilingual sandbox, while achieving high data quality through reverse-order problem generation and multiple filtering steps. Using this novel method, we introduce AutoCodeBench, a large-scale code generation benchmark comprising 3,920 problems evenly distributed across 20 programming languages. It is specifically designed to evaluate LLMs on challenging, diverse, and practical multilingual tasks. We evaluate over 30 leading open-source and proprietary LLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The results show that even the most advanced LLMs struggle with the complexity, diversity, and multilingual nature of these tasks. Besides, we introduce AutoCodeBench-Complete, specifically designed for base models to assess their few-shot code generation capabilities. We hope the AutoCodeBench series will serve as a valuable resource and inspire the community to focus on more challenging and practical multilingual code generation scenarios.",
            "score": 2,
            "issue_id": 5318,
            "pub_date": "2025-08-12",
            "pub_date_card": {
                "ru": "12 августа",
                "en": "August 12",
                "zh": "8月12日"
            },
            "hash": "6dd7cffc5f881c6a",
            "authors": [
                "Jason Chou",
                "Ao Liu",
                "Yuchi Deng",
                "Zhiying Zeng",
                "Tao Zhang",
                "Haotian Zhu",
                "Jianwei Cai",
                "Yue Mao",
                "Chenchen Zhang",
                "Lingyun Tan",
                "Ziyan Xu",
                "Bohui Zhai",
                "Hengyi Liu",
                "Speed Zhu",
                "Wiggin Zhou",
                "Fengzong Lian"
            ],
            "affiliations": [
                "Hunyuan Team, Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09101.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multilingual",
                    "#games",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "AutoCodeBench: автоматизированный многоязычный бенчмарк для оценки генерации кода ИИ",
                    "desc": "AutoCodeGen представляет собой метод автоматического создания многоязычного бенчмарка для оценки генерации кода языковыми моделями без ручной аннотации. Он создает набор данных AutoCodeBench, содержащий 3920 задач на 20 языках программирования. AutoCodeBench позволяет оценивать способности больших языковых моделей решать сложные и разнообразные задачи кодирования на разных языках. Результаты показывают, что даже самые продвинутые модели испытывают трудности с комплексностью и многоязычностью этих задач."
                },
                "en": {
                    "title": "Revolutionizing Code Generation Evaluation with AutoCodeBench",
                    "desc": "The paper introduces AutoCodeGen, a method for creating a large-scale, multilingual benchmark called AutoCodeBench to evaluate the code generation capabilities of Large Language Models (LLMs). Unlike existing benchmarks that rely on manual annotations and focus mainly on Python, AutoCodeBench offers a diverse set of 3,920 problems across 20 programming languages, ensuring a balanced difficulty level. The benchmark is generated automatically, ensuring high data quality through advanced techniques like reverse-order problem generation and multiple filtering steps. The evaluation of over 30 LLMs on this benchmark reveals that even the most advanced models face challenges with the complexity and diversity of the tasks, highlighting the need for more rigorous testing in multilingual code generation."
                },
                "zh": {
                    "title": "自动化多语言代码生成基准的创新",
                    "desc": "AutoCodeGen 是一种自动化方法，用于生成高难度的多语言代码生成数据集，而无需人工注释。它通过大型语言模型（LLMs）生成测试输入，并通过多语言沙箱获取测试输出，从而确保测试用例的正确性和完整性。我们推出的 AutoCodeBench 是一个大规模的代码生成基准，包含3920个问题，均匀分布在20种编程语言中，旨在评估 LLMs 在复杂、多样和实际的多语言任务上的表现。研究结果表明，即使是最先进的 LLMs 在这些任务的复杂性和多样性面前也面临挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.08791",
            "title": "Feedback-Driven Tool-Use Improvements in Large Language Models via\n  Automated Build Environments",
            "url": "https://huggingface.co/papers/2508.08791",
            "abstract": "An automated pipeline for constructing training environments and a verifiable reward mechanism enhance large language models' tool-use performance without compromising general capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective tool use is essential for large language models (LLMs) to interact meaningfully with their environment. However, progress is limited by the lack of efficient reinforcement learning (RL) frameworks specifically designed for tool use, due to challenges in constructing stable training environments and designing verifiable reward mechanisms. To address this, we propose an automated environment construction pipeline, incorporating scenario decomposition, document generation, function integration, complexity scaling, and localized deployment. This enables the creation of high-quality training environments that provide detailed and measurable feedback without relying on external tools. Additionally, we introduce a verifiable reward mechanism that evaluates both the precision of tool use and the completeness of task execution. When combined with trajectory data collected from the constructed environments, this mechanism integrates seamlessly with standard RL algorithms to facilitate feedback-driven model training. Experiments on LLMs of varying scales demonstrate that our approach significantly enhances the models' tool-use performance without degrading their general capabilities, regardless of inference modes or training algorithms. Our analysis suggests that these gains result from improved context understanding and reasoning, driven by updates to the lower-layer MLP parameters in models.",
            "score": 1,
            "issue_id": 5319,
            "pub_date": "2025-08-12",
            "pub_date_card": {
                "ru": "12 августа",
                "en": "August 12",
                "zh": "8月12日"
            },
            "hash": "d6c6aa0f12a1c869",
            "authors": [
                "Junjie Ye",
                "Changhao Jiang",
                "Zhengyin Du",
                "Yufei Xu",
                "Xuesong Yao",
                "Zhiheng Xi",
                "Xiaoran Fan",
                "Qi Zhang",
                "Xuanjing Huang",
                "Jiecao Chen"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.08791.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "🛠️",
                "ru": {
                    "title": "Автоматизация обучения LLM эффективному использованию инструментов",
                    "desc": "Предложена автоматизированная система для создания обучающих сред и механизма проверяемого вознаграждения, улучшающая способности больших языковых моделей (LLM) использовать инструменты. Система включает декомпозицию сценариев, генерацию документов, интеграцию функций и масштабирование сложности. Введен механизм вознаграждения, оценивающий точность использования инструментов и полноту выполнения задач. Эксперименты показали значительное улучшение навыков использования инструментов LLM без ухудшения общих возможностей."
                },
                "en": {
                    "title": "Enhancing Tool Use in LLMs with Automated Training Environments",
                    "desc": "This paper presents a new method to improve how large language models (LLMs) use tools effectively. It introduces an automated system for creating training environments that are stable and provide clear feedback, which is crucial for reinforcement learning (RL). Additionally, the authors propose a verifiable reward mechanism that assesses how well the models use tools and complete tasks. The results show that this approach enhances the tool-use abilities of LLMs without harming their overall performance."
                },
                "zh": {
                    "title": "提升工具使用性能的自动化训练环境",
                    "desc": "本文提出了一种自动化的训练环境构建管道，旨在提升大型语言模型（LLMs）在工具使用方面的表现，同时保持其通用能力不受影响。该管道通过场景分解、文档生成、功能集成、复杂度扩展和本地部署等步骤，创建高质量的训练环境，并提供详细的可测量反馈。我们还引入了一种可验证的奖励机制，评估工具使用的准确性和任务执行的完整性。实验结果表明，该方法显著提升了模型的工具使用性能，且不影响其整体能力。"
                }
            }
        }
    ],
    "link_prev": "2025-08-12.html",
    "link_next": "2025-08-14.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "12.08",
        "en": "08/12",
        "zh": "8月12日"
    },
    "short_date_next": {
        "ru": "14.08",
        "en": "08/14",
        "zh": "8月14日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 5,
        "#agents": 4,
        "#cv": 1,
        "#rl": 6,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}