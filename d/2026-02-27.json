{
    "date": {
        "ru": "27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 27",
        "zh": "2æœˆ27æ—¥"
    },
    "time_utc": "2026-02-27 07:42",
    "weekday": 4,
    "issue_id": 1254,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2602.23152",
            "title": "The Trinity of Consistency as a Defining Principle for General World Models",
            "url": "https://huggingface.co/papers/2602.23152",
            "abstract": "Abstract World Models require three consistency principlesâ€”modal, spatial, and temporalâ€”for general artificial intelligence, with a proposed benchmark evaluating multimodal learning systems.  \t\t\t\t\tAI-generated summary The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.",
            "score": 64,
            "issue_id": 1251,
            "pub_date": "2026-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "39883320a2294485",
            "authors": [
                "Jingxuan Wei",
                "Siyuan Li",
                "Yuhang Xu",
                "Zheng Sun",
                "Junjie Jiang",
                "Hexuan Jin",
                "Caijun Jia",
                "Honghao He",
                "Xinglong Xu",
                "Xi bai",
                "Chang Yu",
                "Yumou Liu",
                "Junnan Zhu",
                "Xuanhe Zhou",
                "Jintao Chen",
                "Xiaobin Hu",
                "Shancheng Pang",
                "Bihui Yu",
                "Ran He",
                "Zhen Lei",
                "Stan Z. Li",
                "Conghui He",
                "Shuicheng Yan",
                "Cheng Tan"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2602.23152.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ¡Ğ²ÑÑ‚Ğ°Ñ Ğ¢Ñ€Ğ¾Ğ¸Ñ†Ğ° ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ… ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ (ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…), Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ (Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ) Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ (Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ñ€Ğ¾Ğ·Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ğº ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¼ Ğº ÑĞ¸Ğ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¼Ğ¸Ñ€Ğ°. Ğ”Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CoW-Bench, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¿ÑƒÑ‚Ğ¸ Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Building Intelligent World Models with Consistency Principles",
                    "desc": "This paper discusses the importance of three consistency principlesâ€”modal, spatial, and temporalâ€”for developing World Models in artificial intelligence. These models are essential for simulating and understanding physical laws, which is crucial for achieving Artificial General Intelligence. The authors introduce CoW-Bench, a benchmark designed to evaluate multimodal learning systems, particularly in video generation and reasoning tasks. By establishing a theoretical framework based on these consistency principles, the paper aims to guide future advancements in creating more integrated and effective World Models."
                },
                "zh": {
                    "title": "æ„å»ºé€šç”¨ä¸–ç•Œæ¨¡å‹çš„ä¸‰å¤§ä¸€è‡´æ€§åŸåˆ™",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ„å»ºèƒ½å¤Ÿå­¦ä¹ ã€æ¨¡æ‹Ÿå’Œæ¨ç†ç‰©ç†æ³•åˆ™çš„ä¸–ç•Œæ¨¡å‹ï¼Œè¿™æ˜¯å®ç°é€šç”¨äººå·¥æ™ºèƒ½çš„åŸºç¡€æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºï¼Œä¸–ç•Œæ¨¡å‹å¿…é¡»éµå¾ªä¸‰ç§ä¸€è‡´æ€§åŸåˆ™ï¼šæ¨¡æ€ä¸€è‡´æ€§ã€ç©ºé—´ä¸€è‡´æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚é€šè¿‡è¿™äº›åŸåˆ™ï¼Œæˆ‘ä»¬ç³»ç»Ÿå›é¡¾äº†å¤šæ¨¡æ€å­¦ä¹ çš„å‘å±•å†ç¨‹ï¼Œæ­ç¤ºäº†ä»åˆ†æ•£çš„ä¸“ç”¨æ¨¡å—åˆ°ç»Ÿä¸€æ¶æ„çš„æ¼”å˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†CoW-BenchåŸºå‡†ï¼Œä¸“æ³¨äºå¤šå¸§æ¨ç†å’Œç”Ÿæˆåœºæ™¯ï¼Œä»¥è¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹å’Œç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.22859",
            "title": "From Blind Spots to Gains: Diagnostic-Driven Iterative Training for Large Multimodal Models",
            "url": "https://huggingface.co/papers/2602.22859",
            "abstract": "Abstract Diagnostic-driven Progressive Evolution enables continuous improvement of large multimodal models through iterative diagnosis and targeted data generation guided by identified weaknesses.  \t\t\t\t\tAI-generated summary As Large Multimodal Models (LMMs) scale up and reinforcement learning (RL) methods mature, LMMs have made notable progress in complex reasoning and decision making. Yet training still relies on static data and fixed recipes, making it difficult to diagnose capability blind spots or provide dynamic, targeted reinforcement. Motivated by findings that test driven error exposure and feedback based correction outperform repetitive practice, we propose Diagnostic-driven Progressive Evolution (DPE), a spiral loop where diagnosis steers data generation and reinforcement, and each iteration re-diagnoses the updated model to drive the next round of targeted improvement. DPE has two key components. First, multiple agents annotate and quality control massive unlabeled multimodal data, using tools such as web search and image editing to produce diverse, realistic samples. Second, DPE attributes failures to specific weaknesses, dynamically adjusts the data mixture, and guides agents to generate weakness focused data for targeted reinforcement. Experiments on Qwen3-VL-8B-Instruct and Qwen2.5-VL-7B-Instruct show stable, continual gains across eleven benchmarks, indicating DPE as a scalable paradigm for continual LMM training under open task distributions. Our code, models, and data are publicly available at https://github.com/hongruijia/DPE.",
            "score": 40,
            "issue_id": 1254,
            "pub_date": "2026-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "a1163e2331a241ac",
            "authors": [
                "Hongrui Jia",
                "Chaoya Jiang",
                "Shikun Zhang",
                "Wei Ye"
            ],
            "affiliations": [
                "Peking University",
                "Shandong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.22859.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents",
                    "#rl",
                    "#dataset",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ğ¿Ğ¸Ñ€Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ°: Ğ¸Ğ· ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑĞ¸Ğ»Ñƒ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Diagnostic-driven Progressive Evolution (DPE) Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾ÑÑ‚Ğ°Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾Ğ´Ğ¸Ğ½Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Iterative Improvement for Multimodal Models through Targeted Diagnosis",
                    "desc": "The paper introduces Diagnostic-driven Progressive Evolution (DPE), a method for improving large multimodal models (LMMs) through iterative diagnosis and targeted data generation. DPE focuses on identifying weaknesses in the model's performance and dynamically generating data to address these specific issues, rather than relying on static datasets. This approach involves multiple agents who annotate and quality control data, ensuring a diverse and realistic sample generation. Experiments demonstrate that DPE leads to consistent improvements across various benchmarks, showcasing its effectiveness for ongoing training of LMMs in changing task environments."
                },
                "zh": {
                    "title": "è¯Šæ–­é©±åŠ¨çš„æ¸è¿›æ¼”åŒ–ï¼šæŒç»­æ”¹è¿›å¤šæ¨¡æ€æ¨¡å‹çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè¯Šæ–­é©±åŠ¨çš„æ¸è¿›æ¼”åŒ–ï¼ˆDPEï¼‰çš„æ–¹æ³•ï¼Œç”¨äºæŒç»­æ”¹è¿›å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£è¯Šæ–­å’Œé’ˆå¯¹æ€§æ•°æ®ç”Ÿæˆï¼Œè§£å†³æ¨¡å‹çš„èƒ½åŠ›ç›²ç‚¹ã€‚DPEçš„ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†æ˜¯å¤šä¸ªä»£ç†å¯¹å¤§é‡æœªæ ‡è®°çš„å¤šæ¨¡æ€æ•°æ®è¿›è¡Œæ³¨é‡Šå’Œè´¨é‡æ§åˆ¶ï¼Œä»¥åŠåŠ¨æ€è°ƒæ•´æ•°æ®ç»„åˆä»¥é’ˆå¯¹ç‰¹å®šå¼±ç‚¹ç”Ÿæˆæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDPEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ç¨³å®šçš„æŒç»­æå‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å¼€æ”¾ä»»åŠ¡åˆ†å¸ƒä¸‹çš„å¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.22897",
            "title": "OmniGAIA: Towards Native Omni-Modal AI Agents",
            "url": "https://huggingface.co/papers/2602.22897",
            "abstract": "Abstract OmniGAIA benchmark evaluates multi-modal agents on complex reasoning tasks across video, audio, and image modalities, while OmniAtlas agent improves tool-use capabilities through hindsight-guided tree exploration and OmniDPO fine-tuning.  \t\t\t\t\tAI-generated summary Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.",
            "score": 32,
            "issue_id": 1251,
            "pub_date": "2026-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "04a55d6820c02809",
            "authors": [
                "Xiaoxi Li",
                "Wenxiang Jiao",
                "Jiarui Jin",
                "Shijian Wang",
                "Guanting Dong",
                "Jiajie Jin",
                "Hao Wang",
                "Yinuo Wang",
                "Ji-Rong Wen",
                "Yuan Lu",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Renmin University of China",
                "Southeast University",
                "Tsinghua University",
                "Xiaohongshu Inc.",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.22897.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#audio",
                    "#video",
                    "#training",
                    "#multimodal",
                    "#rlhf",
                    "#benchmark"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ Ğ±Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğº Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ¼Ğ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ OmniGAIA â€” ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ OmniAtlas â€” Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ²ÑĞµÑ… Ñ‚Ñ€Ñ‘Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ…, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ hindsight-guided tree exploration, Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· OmniDPO Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑˆĞ°Ğ³ Ğ²Ğ¿ĞµÑ€Ñ‘Ğ´ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Bridging the Gap in Multi-Modal AI with OmniGAIA and OmniAtlas",
                    "desc": "The paper introduces OmniGAIA, a benchmark for evaluating multi-modal agents that can process and reason across video, audio, and image data. It highlights the limitations of current models that only handle bi-modal interactions, emphasizing the need for a unified approach to achieve general AI capabilities. The OmniGAIA benchmark uses a novel omni-modal event graph to create complex queries that require deep reasoning and tool usage. Additionally, the paper presents OmniAtlas, an agent designed to improve tool-use through a unique training method that combines hindsight-guided exploration and fine-tuning techniques."
                },
                "zh": {
                    "title": "è¿ˆå‘ä¸‹ä¸€ä»£å¤šæ¨¡æ€AIåŠ©æ‰‹çš„é‡Œç¨‹ç¢‘",
                    "desc": "OmniGAIAåŸºå‡†æµ‹è¯•è¯„ä¼°å¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨è§†é¢‘ã€éŸ³é¢‘å’Œå›¾åƒç­‰å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ä¸ºäº†æå‡å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼ŒOmniAtlasæ™ºèƒ½ä½“é€šè¿‡å›é¡¾å¼•å¯¼çš„æ ‘æ¢ç´¢å’ŒOmniDPOå¾®è°ƒè¿›è¡Œæ”¹è¿›ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€äº‹ä»¶å›¾æ–¹æ³•ï¼Œåˆæˆäº†éœ€è¦è·¨æ¨¡æ€æ¨ç†å’Œå¤–éƒ¨å·¥å…·é›†æˆçš„å¤æ‚æŸ¥è¯¢ã€‚æ­¤é¡¹å·¥ä½œä¸ºä¸‹ä¸€ä»£åŸç”Ÿå¤šæ¨¡æ€AIåŠ©æ‰‹åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.22766",
            "title": "Imagination Helps Visual Reasoning, But Not Yet in Latent Space",
            "url": "https://huggingface.co/papers/2602.22766",
            "abstract": "Abstract Research reveals that latent visual reasoning in multimodal models suffers from input-latent and latent-answer disconnects, leading to the proposal of CapImagine, a text-based approach that outperforms complex latent-space methods.  \t\t\t\t\tAI-generated summary Latent visual reasoning aims to mimic human's imagination process by meditating through hidden states of Multimodal Large Language Models. While recognized as a promising paradigm for visual reasoning, the underlying mechanisms driving its effectiveness remain unclear. Motivated to demystify the true source of its efficacy, we investigate the validity of latent reasoning using Causal Mediation Analysis. We model the process as a causal chain: the input as the treatment, the latent tokens as the mediator, and the final answer as the outcome. Our findings uncover two critical disconnections: (a) Input-Latent Disconnect: dramatic perturbations on the input result in negligible changes to the latent tokens, suggesting that latent tokens do not effectively attend to the input sequence. (b) Latent-Answer Disconnect: perturbations on the latent tokens yield minimal impact on the final answer, indicating the limited causal effect latent tokens imposing on the outcome. Furthermore, extensive probing analysis reveals that latent tokens encode limited visual information and exhibit high similarity. Consequently, we challenge the necessity of latent reasoning and propose a straightforward alternative named CapImagine, which teaches the model to explicitly imagine using text. Experiments on vision-centric benchmarks show that CapImagine significantly outperforms complex latent-space baselines, highlighting the superior potential of visual reasoning through explicit imagination.",
            "score": 25,
            "issue_id": 1251,
            "pub_date": "2026-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "204384dc2b63309b",
            "authors": [
                "You Li",
                "Chi Chen",
                "Yanghao Li",
                "Fanhu Zeng",
                "Kaiyu Huang",
                "Jinan Xu",
                "Maosong Sun"
            ],
            "affiliations": [
                "School of Computer Science and Technology, Beijing Jiaotong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.22766.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¯Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾: Ğ¾Ñ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµĞ´Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ğ´Ğ²Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ°: Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ° Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ»Ğ°Ğ±Ğ¾ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ CapImagine â€” Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ²Ğ½Ğ¾ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ½Ğ¾Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ."
                },
                "en": {
                    "title": "CapImagine: Simplifying Visual Reasoning through Explicit Imagination",
                    "desc": "This paper investigates the limitations of latent visual reasoning in multimodal models, identifying two main disconnects: the Input-Latent Disconnect and the Latent-Answer Disconnect. The Input-Latent Disconnect shows that changes in input do not significantly affect the latent tokens, while the Latent-Answer Disconnect indicates that these tokens have little influence on the final output. To address these issues, the authors propose CapImagine, a simpler text-based approach that enhances visual reasoning by focusing on explicit imagination rather than complex latent representations. Experimental results demonstrate that CapImagine outperforms traditional latent-space methods, suggesting a more effective way to achieve visual reasoning in AI models."
                },
                "zh": {
                    "title": "é€šè¿‡æ˜ç¡®æƒ³è±¡æå‡è§†è§‰æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€æ¨¡å‹ä¸­æ½œåœ¨è§†è§‰æ¨ç†çš„ä¸è¶³ï¼Œå‘ç°è¾“å…¥ä¸æ½œåœ¨çŠ¶æ€ã€æ½œåœ¨çŠ¶æ€ä¸ç­”æ¡ˆä¹‹é—´å­˜åœ¨æ–­è£‚ã€‚æˆ‘ä»¬é€šè¿‡å› æœä¸­ä»‹åˆ†æéªŒè¯äº†æ½œåœ¨æ¨ç†çš„æœ‰æ•ˆæ€§ï¼Œå‘ç°è¾“å…¥çš„å˜åŒ–å¯¹æ½œåœ¨æ ‡è®°å½±å“å¾®å°ï¼Œè€Œæ½œåœ¨æ ‡è®°å¯¹æœ€ç»ˆç­”æ¡ˆçš„å½±å“ä¹Ÿæœ‰é™ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCapImagineçš„æ–‡æœ¬åŸºç¡€æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¿›è¡Œè§†è§‰æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCapImagineåœ¨è§†è§‰åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºå¤æ‚çš„æ½œåœ¨ç©ºé—´æ–¹æ³•ï¼Œå±•ç¤ºäº†é€šè¿‡æ˜ç¡®æƒ³è±¡è¿›è¡Œè§†è§‰æ¨ç†çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.23008",
            "title": "Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization",
            "url": "https://huggingface.co/papers/2602.23008",
            "abstract": "Abstract EMPOÂ² is a hybrid reinforcement learning framework that enhances exploration for large language model agents by integrating memory mechanisms with on- and off-policy updates, demonstrating improved performance and adaptability in complex environments.  \t\t\t\t\tAI-generated summary Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO^2), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO^2 achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO^2 demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO^2 as a promising framework for building more exploratory and generalizable LLM-based agents.",
            "score": 21,
            "issue_id": 1251,
            "pub_date": "2026-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "97d76d03a487b0fd",
            "authors": [
                "Zeyuan Liu",
                "Jeonghye Kim",
                "Xufang Luo",
                "Dongsheng Li",
                "Yuqing Yang"
            ],
            "affiliations": [
                "KAIST",
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.23008.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸĞ°Ğ¼ÑÑ‚ÑŒ Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "EMPOÂ² â€” ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½- Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ScienceWorld Ğ¸ WebShop Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. EMPOÂ² Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ±ĞµĞ· ĞµÑ‘ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Exploration in AI with EMPOÂ²",
                    "desc": "EMPOÂ² is a new framework that improves how large language model agents explore their environments using reinforcement learning. It combines memory mechanisms with both on-policy and off-policy updates, allowing agents to adapt better in complex situations. This approach helps agents discover new states more effectively than previous methods, which relied solely on pre-existing knowledge. The results show significant performance gains in various tasks, making EMPOÂ² a strong candidate for developing more flexible and capable AI agents."
                },
                "zh": {
                    "title": "EMPOÂ²ï¼šå¢å¼ºæ¢ç´¢èƒ½åŠ›çš„æ··åˆå¼ºåŒ–å­¦ä¹ æ¡†æ¶",
                    "desc": "EMPOÂ²æ˜¯ä¸€ç§æ··åˆå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆè®°å¿†æœºåˆ¶å’Œç­–ç•¥æ›´æ–°ï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†çš„æ¢ç´¢èƒ½åŠ›ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä»¥å¾€å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­å‘ç°æ–°çŠ¶æ€çš„ä¸è¶³ã€‚EMPOÂ²åˆ©ç”¨è®°å¿†è¿›è¡Œæ¢ç´¢ï¼Œå¹¶ç»“åˆåœ¨çº¿å’Œç¦»çº¿ç­–ç•¥æ›´æ–°ï¼Œä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœ‰è®°å¿†å’Œæ— è®°å¿†çš„æƒ…å†µä¸‹éƒ½èƒ½è¡¨ç°å‡ºè‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEMPOÂ²åœ¨å¤šä¸ªæµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨æ–°ä»»åŠ¡é€‚åº”æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.21760",
            "title": "Accelerating Diffusion via Hybrid Data-Pipeline Parallelism Based on Conditional Guidance Scheduling",
            "url": "https://huggingface.co/papers/2602.21760",
            "abstract": "Abstract A hybrid parallelism framework for diffusion models that combines condition-based partitioning and adaptive pipeline scheduling to reduce inference latency while maintaining image quality across different architectures.  \t\t\t\t\tAI-generated summary Diffusion models have achieved remarkable progress in high-fidelity image, video, and audio generation, yet inference remains computationally expensive. Nevertheless, current diffusion acceleration methods based on distributed parallelism suffer from noticeable generation artifacts and fail to achieve substantial acceleration proportional to the number of GPUs. Therefore, we propose a hybrid parallelism framework that combines a novel data parallel strategy, condition-based partitioning, with an optimal pipeline scheduling method, adaptive parallelism switching, to reduce generation latency and achieve high generation quality in conditional diffusion models. The key ideas are to (i) leverage the conditional and unconditional denoising paths as a new data-partitioning perspective and (ii) adaptively enable optimal pipeline parallelism according to the denoising discrepancy between these two paths. Our framework achieves 2.31times and 2.07times latency reductions on SDXL and SD3, respectively, using two NVIDIA RTX~3090 GPUs, while preserving image quality. This result confirms the generality of our approach across U-Net-based diffusion models and DiT-based flow-matching architectures. Our approach also outperforms existing methods in acceleration under high-resolution synthesis settings. Code is available at https://github.com/kaist-dmlab/Hybridiff.",
            "score": 8,
            "issue_id": 1252,
            "pub_date": "2026-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "72280e178117d00e",
            "authors": [
                "Euisoo Jung",
                "Byunghyun Kim",
                "Hyunjin Kim",
                "Seonghye Cho",
                "Jae-Gil Lee"
            ],
            "affiliations": [
                "School of Computing, KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.21760.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#diffusion"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ°. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿ÑƒÑ‚ÑĞ¼ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ GPU Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² 2.31x Ğ¸ 2.07x Ñ€Ğ°Ğ· Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… SDXL Ğ¸ SD3 ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ²ÑƒÑ… GPU, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ U-Net Ğ¸ DiT, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Accelerating Diffusion Models with Hybrid Parallelism",
                    "desc": "This paper presents a hybrid parallelism framework designed to enhance the efficiency of diffusion models during inference. By integrating condition-based partitioning with adaptive pipeline scheduling, the framework significantly reduces latency while ensuring high image quality. The approach utilizes both conditional and unconditional denoising paths for effective data partitioning and optimizes pipeline parallelism based on the discrepancies in denoising. The results demonstrate substantial latency reductions on various architectures, confirming the framework's effectiveness across different diffusion model types."
                },
                "zh": {
                    "title": "æ··åˆå¹¶è¡Œæ¡†æ¶ï¼šåŠ é€Ÿæ‰©æ•£æ¨¡å‹æ¨ç†",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ··åˆå¹¶è¡Œæ¡†æ¶ï¼Œç”¨äºæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ¡ä»¶åˆ†åŒºå’Œè‡ªé€‚åº”ç®¡é“è°ƒåº¦æ¥å‡å°‘æ¨ç†å»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ã€‚ç°æœ‰çš„æ‰©æ•£åŠ é€Ÿæ–¹æ³•åœ¨åˆ†å¸ƒå¼å¹¶è¡Œä¸­å­˜åœ¨ç”Ÿæˆä¼ªå½±çš„é—®é¢˜ï¼Œä¸”åŠ é€Ÿæ•ˆæœæœªèƒ½ä¸GPUæ•°é‡æˆæ­£æ¯”ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†æ–°é¢–çš„æ•°æ®å¹¶è¡Œç­–ç•¥å’Œæœ€ä½³ç®¡é“è°ƒåº¦æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½äº†ç”Ÿæˆå»¶è¿Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä½¿ç”¨ä¸¤å—NVIDIA RTX 3090 GPUæ—¶ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨SDXLå’ŒSD3ä¸Šåˆ†åˆ«å®ç°äº†2.31å€å’Œ2.07å€çš„å»¶è¿Ÿå‡å°‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.22594",
            "title": "Causal Motion Diffusion Models for Autoregressive Motion Generation",
            "url": "https://huggingface.co/papers/2602.22594",
            "abstract": "Abstract Causal Motion Diffusion Models introduce a unified framework for autoregressive motion generation using a causal diffusion transformer in a semantically aligned latent space, enabling fast, high-quality text-to-motion synthesis with improved temporal smoothness.  \t\t\t\t\tAI-generated summary Recent advances in motion diffusion models have substantially improved the realism of human motion synthesis. However, existing approaches either rely on full-sequence diffusion models with bidirectional generation, which limits temporal causality and real-time applicability, or autoregressive models that suffer from instability and cumulative errors. In this work, we present Causal Motion Diffusion Models (CMDM), a unified framework for autoregressive motion generation based on a causal diffusion transformer that operates in a semantically aligned latent space. CMDM builds upon a Motion-Language-Aligned Causal VAE (MAC-VAE), which encodes motion sequences into temporally causal latent representations. On top of this latent representation, an autoregressive diffusion transformer is trained using causal diffusion forcing to perform temporally ordered denoising across motion frames. To achieve fast inference, we introduce a frame-wise sampling schedule with causal uncertainty, where each subsequent frame is predicted from partially denoised previous frames. The resulting framework supports high-quality text-to-motion generation, streaming synthesis, and long-horizon motion generation at interactive rates. Experiments on HumanML3D and SnapMoGen demonstrate that CMDM outperforms existing diffusion and autoregressive models in both semantic fidelity and temporal smoothness, while substantially reducing inference latency.",
            "score": 3,
            "issue_id": 1251,
            "pub_date": "2026-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "382deb34ba8b7a79",
            "authors": [
                "Qing Yu",
                "Akihisa Watanabe",
                "Kent Fujiwara"
            ],
            "affiliations": [
                "LY Corporation",
                "Waseda University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.22594.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#training",
                    "#multimodal",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Causal Motion Diffusion Models Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° Ğ½Ğ° Motion-Language-Aligned Causal VAE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ°Ğ»Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Motion Generation with Causal Diffusion!",
                    "desc": "Causal Motion Diffusion Models (CMDM) provide a new way to generate human motion by using a causal diffusion transformer in a structured latent space. This approach allows for autoregressive motion generation, which means it predicts each motion frame based on previous ones, ensuring smooth and realistic transitions. By utilizing a Motion-Language-Aligned Causal VAE, CMDM encodes motion sequences into a format that respects temporal causality, improving the quality of generated motions. The framework also introduces a fast sampling method that enhances real-time performance, making it suitable for applications like text-to-motion synthesis and interactive motion generation."
                },
                "zh": {
                    "title": "å› æœè¿åŠ¨ç”Ÿæˆçš„é«˜æ•ˆæ–°æ¡†æ¶",
                    "desc": "å› æœè¿åŠ¨æ‰©æ•£æ¨¡å‹ï¼ˆCMDMï¼‰æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è‡ªå›å½’è¿åŠ¨ç”Ÿæˆæ¡†æ¶ï¼Œåˆ©ç”¨å› æœæ‰©æ•£å˜æ¢å™¨åœ¨è¯­ä¹‰å¯¹é½çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œå¿«é€Ÿã€é«˜è´¨é‡çš„æ–‡æœ¬åˆ°è¿åŠ¨åˆæˆã€‚è¯¥æ¨¡å‹é€šè¿‡è¿åŠ¨-è¯­è¨€å¯¹é½çš„å› æœå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆMAC-VAEï¼‰å°†è¿åŠ¨åºåˆ—ç¼–ç ä¸ºæ—¶é—´å› æœçš„æ½œåœ¨è¡¨ç¤ºã€‚CMDMåœ¨æ­¤åŸºç¡€ä¸Šè®­ç»ƒè‡ªå›å½’æ‰©æ•£å˜æ¢å™¨ï¼Œä½¿ç”¨å› æœæ‰©æ•£å¼ºåˆ¶è¿›è¡Œè¿åŠ¨å¸§çš„æ—¶é—´æœ‰åºå»å™ªã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCMDMåœ¨è¯­ä¹‰ä¿çœŸåº¦å’Œæ—¶é—´å¹³æ»‘æ€§æ–¹é¢ä¼˜äºç°æœ‰æ¨¡å‹ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘æ¨ç†å»¶è¿Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.17594",
            "title": "AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games",
            "url": "https://huggingface.co/papers/2602.17594",
            "abstract": "Abstract AI systems were evaluated across a diverse set of human-designed games to assess general intelligence, revealing significant gaps in performance compared to human players, particularly in complex cognitive tasks.  \t\t\t\t\tAI-generated summary Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play all conceivable human games, in comparison to human players with the same level of experience, time, or other resources. We define a \"human game\" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the \"Multiverse of Human Games\". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.",
            "score": 3,
            "issue_id": 1252,
            "pub_date": "2026-02-19",
            "pub_date_card": {
                "ru": "19 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 19",
                "zh": "2æœˆ19æ—¥"
            },
            "hash": "25f0c4bc8dd0ba04",
            "authors": [
                "Lance Ying",
                "Ryan Truong",
                "Prafull Sharma",
                "Kaiya Ivy Zhao",
                "Nathan Cloos",
                "Kelsey R. Allen",
                "Thomas L. Griffiths",
                "Katherine M. Collins",
                "JosÃ© HernÃ¡ndez-Orallo",
                "Phillip Isola",
                "Samuel J. Gershman",
                "Joshua B. Tenenbaum"
            ],
            "affiliations": [
                "Harvard University",
                "MIT",
                "Princeton University",
                "Universitat PolitÃ¨cnica de ValÃ¨ncia",
                "University of British Columbia",
                "University of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.17594.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#agi",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "ĞÑ‚ ÑƒĞ·ĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğº Ğ¾Ğ±Ñ‰ĞµĞ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ: Ğ¾Ñ†ĞµĞ½ĞºĞ° AI Ñ‡ĞµÑ€ĞµĞ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ³Ñ€Ñ‹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° AI-ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ³Ñ€Ñ‹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ»ÑĞ´ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ AI GameStore, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞµĞ¼ÑŒ frontier Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 100 Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 10% ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ AI-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking AI's Potential Through Human Games",
                    "desc": "This paper evaluates AI systems' general intelligence by testing them on a wide variety of human-designed games. It highlights the limitations of traditional AI benchmarks, which often focus on narrow tasks and quickly become outdated. The authors propose a new evaluation framework called the 'Multiverse of Human Games', which encompasses all conceivable games created for human enjoyment. They introduce the AI GameStore, a platform that generates and tests new games, revealing that current AI models perform significantly worse than humans, particularly in complex cognitive tasks."
                },
                "zh": {
                    "title": "é€šè¿‡æ¸¸æˆè¯„ä¼°AIçš„é€šç”¨æ™ºèƒ½",
                    "desc": "æœ¬ç ”ç©¶è¯„ä¼°äº†äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨å¤šç§äººç±»è®¾è®¡çš„æ¸¸æˆä¸­çš„è¡¨ç°ï¼Œä»¥è¡¡é‡å…¶é€šç”¨æ™ºèƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼ŒAIåœ¨å¤æ‚è®¤çŸ¥ä»»åŠ¡ä¸­ä¸äººç±»ç©å®¶ç›¸æ¯”å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æˆ‘ä»¬æå‡ºé€šè¿‡â€œäººç±»æ¸¸æˆâ€çš„æ–¹å¼æ¥è¯„ä¼°AIçš„é€šç”¨æ™ºèƒ½ï¼Œå®šä¹‰äººç±»æ¸¸æˆä¸ºäººç±»è®¾è®¡çš„ä¾›äººç±»ç©çš„æ¸¸æˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†AI GameStoreå¹³å°ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å’Œäººç±»å‚ä¸è€…ç”Ÿæˆæ–°çš„æ¸¸æˆï¼Œå¹¶å¯¹ç°æœ‰æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œä»¥æ¨åŠ¨æœºå™¨å‘äººç±»æ™ºèƒ½çš„è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.23058",
            "title": "GeoWorld: Geometric World Models",
            "url": "https://huggingface.co/papers/2602.23058",
            "abstract": "Abstract GeoWorld addresses limitations in energy-based predictive world models by utilizing hyperbolic geometry to preserve latent state structures and improve long-horizon prediction performance.  \t\t\t\t\tAI-generated summary Energy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. Project website: https://steve-zeyu-zhang.github.io/GeoWorld.",
            "score": 1,
            "issue_id": 1251,
            "pub_date": "2026-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "caf312a37dacc510",
            "authors": [
                "Zeyu Zhang",
                "Danning Li",
                "Ian Reid",
                "Richard Hartley"
            ],
            "affiliations": [
                "ANU",
                "MBZUAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.23058.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#video",
                    "#architecture",
                    "#rl",
                    "#long_context"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ¿ĞµÑ€Ğ±Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "GeoWorld â€” ÑÑ‚Ğ¾ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ±Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Hyperbolic JEPA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ· ĞµĞ²ĞºĞ»Ğ¸Ğ´Ğ¾Ğ²Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ³Ğ¸Ğ¿ĞµÑ€Ğ±Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¾Ğ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Geometric Reinforcement Learning Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ³Ğ¸Ğ¿ĞµÑ€Ğ±Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 3% Ğ´Ğ»Ñ Ñ‚Ñ€Ñ‘Ñ…ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ° 2% Ğ´Ğ»Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ SOTA Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ V-JEPA 2."
                },
                "en": {
                    "title": "Harnessing Hyperbolic Geometry for Enhanced Predictive Modeling",
                    "desc": "GeoWorld is a novel approach that enhances energy-based predictive world models by employing hyperbolic geometry. This method effectively maintains the geometric and hierarchical relationships of latent states, which is crucial for accurate long-horizon predictions. By utilizing a Hyperbolic JEPA, GeoWorld maps latent representations from traditional Euclidean space to hyperbolic manifolds, addressing the limitations of previous models. Additionally, it incorporates Geometric Reinforcement Learning to facilitate stable multi-step planning, resulting in improved success rates in planning tasks compared to existing methods."
                },
                "zh": {
                    "title": "GeoWorldï¼šæå‡é•¿æ—¶é—´é¢„æµ‹çš„å‡ ä½•ä¸–ç•Œæ¨¡å‹",
                    "desc": "GeoWorld æ˜¯ä¸€ç§æ–°çš„é¢„æµ‹ä¸–ç•Œæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³åŸºäºèƒ½é‡çš„æ¨¡å‹åœ¨é•¿æ—¶é—´é¢„æµ‹ä¸­çš„å±€é™æ€§ã€‚å®ƒåˆ©ç”¨åŒæ›²å‡ ä½•æ¥ä¿æŒæ½œåœ¨çŠ¶æ€ç»“æ„ï¼Œä»è€Œæé«˜é¢„æµ‹æ€§èƒ½ã€‚é€šè¿‡å°†æ½œåœ¨è¡¨ç¤ºä»æ¬§å‡ é‡Œå¾—ç©ºé—´æ˜ å°„åˆ°åŒæ›²æµå½¢ï¼ŒGeoWorld èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰çŠ¶æ€ä¹‹é—´çš„å‡ ä½•å’Œå±‚æ¬¡å…³ç³»ã€‚æ­¤å¤–ï¼ŒGeoWorld è¿˜å¼•å…¥äº†å‡ ä½•å¼ºåŒ–å­¦ä¹ ï¼Œæ”¯æŒåœ¨åŒæ›²æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œç¨³å®šçš„å¤šæ­¥è§„åˆ’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.23259",
            "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving",
            "url": "https://huggingface.co/papers/2602.23259",
            "abstract": "Abstract A risk-aware framework for autonomous driving that uses world modeling and risk evaluation to generalize beyond expert demonstrations without requiring explicit expert supervision.  \t\t\t\t\tAI-generated summary With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.",
            "score": 0,
            "issue_id": 1251,
            "pub_date": "2026-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "ee846dffcbfc5595",
            "authors": [
                "Jiangxin Sun",
                "Feng Xue",
                "Teng Long",
                "Chang Liu",
                "Jian-Fang Hu",
                "Wei-Shi Zheng",
                "Nicu Sebe"
            ],
            "affiliations": [
                "Sun Yat-sen University",
                "University of Trento"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.23259.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#training",
                    "#robotics"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ±ĞµĞ· ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RaWMPC â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€Ğ¸ÑĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ÑÑ… Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ‚ÑŒ Ğ¸Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ±ĞµĞ· ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€Ğ° Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ğ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ² ÑĞµÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ñ€ĞµĞ´ĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Driving Safely Without Expert Guidance",
                    "desc": "This paper presents a new framework called Risk-aware World Model Predictive Control (RaWMPC) for autonomous driving that enhances decision-making without relying on expert supervision. It addresses the limitations of traditional imitation learning by allowing the system to generalize better to rare or unseen driving scenarios. RaWMPC uses a world model to predict the outcomes of various driving actions and selects those with lower risk through a systematic evaluation of potential hazards. The framework also includes a self-evaluation method to transfer risk-avoidance skills from the world model to a generative action proposal network, improving performance in both familiar and unfamiliar driving situations."
                },
                "zh": {
                    "title": "æ— ç›‘ç£çš„å®‰å…¨è‡ªåŠ¨é©¾é©¶å†³ç­–",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é£é™©æ„ŸçŸ¥æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿåœ¨æ²¡æœ‰ä¸“å®¶ç›‘ç£çš„æƒ…å†µä¸‹çš„å†³ç­–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åä¸ºé£é™©æ„ŸçŸ¥ä¸–ç•Œæ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆRaWMPCï¼‰ï¼Œé€šè¿‡æ„å»ºä¸–ç•Œæ¨¡å‹æ¥é¢„æµ‹å¤šç§å€™é€‰åŠ¨ä½œçš„åæœï¼Œå¹¶é€šè¿‡æ˜ç¡®çš„é£é™©è¯„ä¼°é€‰æ‹©ä½é£é™©åŠ¨ä½œã€‚RaWMPCè¿˜è®¾è®¡äº†ä¸€ç§é£é™©æ„ŸçŸ¥äº¤äº’ç­–ç•¥ï¼Œä½¿ä¸–ç•Œæ¨¡å‹èƒ½å¤Ÿé¢„æµ‹å±é™©é©¾é©¶è¡Œä¸ºçš„ç»“æœï¼Œä»è€Œé¿å…ç¾éš¾æ€§åæœã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä¸€ç§è‡ªæˆ‘è¯„ä¼°è’¸é¦æ–¹æ³•ï¼Œå°†é£é™©è§„é¿èƒ½åŠ›ä»è®­ç»ƒå¥½çš„ä¸–ç•Œæ¨¡å‹æç‚¼åˆ°ç”ŸæˆåŠ¨ä½œæè®®ç½‘ç»œä¸­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.23165",
            "title": "DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation",
            "url": "https://huggingface.co/papers/2602.23165",
            "abstract": "Abstract DyaDiT is a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals by capturing interaction dynamics between two speakers.  \t\t\t\t\tAI-generated summary Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.",
            "score": 0,
            "issue_id": 1251,
            "pub_date": "2026-02-26",
            "pub_date_card": {
                "ru": "26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 26",
                "zh": "2æœˆ26æ—¥"
            },
            "hash": "50ad743b5135a38e",
            "authors": [
                "Yichen Peng",
                "Jyun-Ting Song",
                "Siyeol Jung",
                "Ruofan Liu",
                "Haiyang Liu",
                "Xuangeng Chu",
                "Ruicong Liu",
                "Erwin Wu",
                "Hideki Koike",
                "Kris Kitani"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Institute of Science Tokyo",
                "Shanda AI Research Tokyo",
                "The University of Tokyo",
                "UNIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.23165.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#audio",
                    "#video",
                    "#multimodal",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¶ĞµÑÑ‚Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ…",
                    "desc": "DyaDiT â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¶ĞµÑÑ‚Ñ‹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ´Ğ²ÑƒÑ… Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ…, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Seamless Interaction Dataset Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´ ÑÑ‚ĞµÑ€ĞµĞ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¶ĞµÑÑ‚Ñ‹ ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ DyaDiT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ĞµĞ½ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑƒĞ¼ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¶ĞµÑÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Human Interaction with Context-Aware Motion Generation",
                    "desc": "DyaDiT is a novel multi-modal diffusion transformer designed to generate realistic human motion based on dyadic audio signals, which represent interactions between two speakers. Unlike traditional methods that focus on a single speaker's motion, DyaDiT captures the dynamics of both speakers to create contextually appropriate gestures. It utilizes a motion dictionary to incorporate motion priors and can leverage the gestures of the conversational partner for more responsive interactions. Evaluations show that DyaDiT outperforms existing techniques in both objective metrics and user preference, indicating its effectiveness in generating socially engaging motions."
                },
                "zh": {
                    "title": "DyaDiTï¼šç”Ÿæˆè‡ªç„¶ç¤¾äº¤äº’åŠ¨çš„å¤šæ¨¡æ€å˜æ¢å™¨",
                    "desc": "DyaDiTæ˜¯ä¸€ç§å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨ï¼Œèƒ½å¤Ÿæ ¹æ®åŒäººéŸ³é¢‘ä¿¡å·ç”Ÿæˆåˆé€‚çš„äººç±»åŠ¨ä½œã€‚å®ƒé€šè¿‡æ•æ‰ä¸¤ä½è¯´è¯è€…ä¹‹é—´çš„äº’åŠ¨åŠ¨æ€ï¼Œæ¥å®ç°æ›´è‡ªç„¶çš„ç¤¾äº¤äº’åŠ¨ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒDyaDiTè€ƒè™‘äº†ç¤¾äº¤ä¸Šä¸‹æ–‡ï¼Œå¹¶èåˆäº†ä¸¤ä½è¯´è¯è€…çš„ä¿¡æ¯ï¼Œä»¥ç”Ÿæˆæ›´å…·å“åº”æ€§çš„åŠ¨ä½œã€‚ç»è¿‡è¯„ä¼°ï¼ŒDyaDiTåœ¨è¿åŠ¨ç”Ÿæˆçš„æ ‡å‡†æŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶è·å¾—ç”¨æˆ·çš„é«˜åº¦åå¥½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.22479",
            "title": "Efficient Continual Learning in Language Models via Thalamically Routed Cortical Columns",
            "url": "https://huggingface.co/papers/2602.22479",
            "abstract": "Abstract TRCÂ² addresses continual learning challenges in language models through a sparse, chunk-parallel architectural design that enables rapid adaptation without catastrophic forgetting.  \t\t\t\t\tAI-generated summary Continual learning is a core requirement for deployed language models, yet standard training and fine-tuning pipelines remain brittle under non-stationary data. Online updates often induce catastrophic forgetting, while methods that improve stability frequently increase latency, memory footprint, or dense computation in ways that do not scale well to long contexts. We introduce TRC^{2} (Thalamically Routed Cortical Columns), a decoder-only backbone that addresses continual learning at the architectural level. TRC^{2} combines sparse thalamic routing over cortical columns with mechanisms for modulation, prediction, memory, and feedback, together with a fast corrective pathway that supports rapid adaptation without destabilizing slower parameters. The resulting block is sparse and chunk-parallel, enabling efficient training and inference while preserving clean ablations of each subsystem. We instantiate a reproducible training and evaluation stack and a continual-learning harness that measures proxy forgetting under streaming domain shifts. Across language modeling and continual learning benchmarks, TRC^{2} improves the stability-plasticity tradeoff at comparable compute, enabling rapid on-stream adaptation while preserving previously acquired behavior.",
            "score": 0,
            "issue_id": 1252,
            "pub_date": "2026-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "34268f2c0f845ed2",
            "authors": [
                "Afshin Khadangi"
            ],
            "affiliations": [
                "SnT, University of Luxembourg",
                "W&B"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.22479.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° TRCÂ² Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ñ€ĞºĞ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ñ‹ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ±ĞµĞ· ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ², Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TRCÂ² ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞµÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "TRCÂ²: Adapting Language Models Without Forgetting",
                    "desc": "The paper introduces TRCÂ², a new architecture designed to improve continual learning in language models. It uses a sparse and chunk-parallel design that allows the model to adapt quickly to new information without losing previously learned knowledge, a problem known as catastrophic forgetting. TRCÂ² incorporates advanced mechanisms for memory, prediction, and feedback, which help maintain stability while allowing for rapid updates. The architecture has been tested and shows better performance in adapting to changing data while keeping computational efficiency in mind."
                },
                "zh": {
                    "title": "TRCÂ²ï¼šé«˜æ•ˆçš„æŒç»­å­¦ä¹ æ¶æ„",
                    "desc": "TRCÂ²æ˜¯ä¸€ç§é’ˆå¯¹è¯­è¨€æ¨¡å‹æŒç»­å­¦ä¹ æŒ‘æˆ˜çš„æ¶æ„è®¾è®¡ï¼Œé‡‡ç”¨ç¨€ç–å’Œå—å¹¶è¡Œçš„æ–¹å¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿé€‚åº”è€Œä¸å‘ç”Ÿç¾éš¾æ€§é—å¿˜ã€‚ä¼ ç»Ÿçš„è®­ç»ƒæ–¹æ³•åœ¨å¤„ç†éé™æ€æ•°æ®æ—¶è¡¨ç°ä¸ä½³ï¼Œåœ¨çº¿æ›´æ–°å¾€å¾€å¯¼è‡´æ¨¡å‹é—å¿˜ä¹‹å‰å­¦åˆ°çš„çŸ¥è¯†ã€‚TRCÂ²é€šè¿‡ç¨€ç–çš„ä¸˜è„‘è·¯ç”±å’Œå¤šç§æœºåˆ¶ï¼ˆå¦‚è°ƒåˆ¶ã€é¢„æµ‹ã€è®°å¿†å’Œåé¦ˆï¼‰ç›¸ç»“åˆï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆçš„è§£ç å™¨æ¶æ„ã€‚è¯¥æ–¹æ³•åœ¨è¯­è¨€å»ºæ¨¡å’ŒæŒç»­å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ”¹å–„äº†ç¨³å®šæ€§ä¸çµæ´»æ€§çš„å¹³è¡¡ï¼Œæ”¯æŒå¿«é€Ÿé€‚åº”è€Œä¸å½±å“å·²æœ‰çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.22437",
            "title": "veScale-FSDP: Flexible and High-Performance FSDP at Scale",
            "url": "https://huggingface.co/papers/2602.22437",
            "abstract": "Abstract veScale-FSDP introduces a redesigned fully sharded data parallel system with flexible sharding and structure-aware planning to improve scalability and efficiency for large-scale model training.  \t\t\t\t\tAI-generated summary Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDP's fixed element- or row-wise sharding formats conflict with the block-structured computations. In addition, today's implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, a redesigned FSDP system that couples a flexible sharding format, RaggedShard, with a structure-aware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As a result, veScale-FSDP achieves 5~66% higher throughput and 16~30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs.",
            "score": 0,
            "issue_id": 1251,
            "pub_date": "2026-02-25",
            "pub_date_card": {
                "ru": "25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 25",
                "zh": "2æœˆ25æ—¥"
            },
            "hash": "23df41059c4853fd",
            "authors": [
                "Zezhou Wang",
                "Youjie Li",
                "Zhiqi Lin",
                "Jiacheng Yang",
                "Cong Xie",
                "Guanyu Feng",
                "Zheng Zhong",
                "Ziyue Huang",
                "Hongyu Zhu",
                "Zhi Zhang",
                "Yanghua Peng",
                "Xin Liu"
            ],
            "affiliations": [
                "Bytedance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.22437.jpg",
            "data": {
                "categories": [],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ¾Ğµ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Fully Sharded Data Parallel (FSDP) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ veScale-FSDP, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ ÑˆĞ°Ñ€Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ RaggedShard Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Shampoo Ğ¸ Muon. ĞĞ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 5-66% Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° 16-30% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ FSDP. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ´Ğ¾ Ğ´ĞµÑÑÑ‚ĞºĞ¾Ğ² Ñ‚Ñ‹ÑÑÑ‡ GPU Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing Large-Scale Model Training with veScale-FSDP",
                    "desc": "veScale-FSDP is a new system designed to enhance the Fully Sharded Data Parallel (FSDP) approach for training large-scale machine learning models. It introduces a flexible sharding format called RaggedShard, which allows for better handling of complex training methods like block-wise quantization and non-element-wise optimizers. This system improves both communication and memory efficiency, enabling it to scale effectively across tens of thousands of GPUs. As a result, veScale-FSDP achieves significantly higher throughput and lower memory usage compared to previous FSDP implementations."
                },
                "zh": {
                    "title": "veScale-FSDPï¼šæå‡å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡ä¸çµæ´»æ€§",
                    "desc": "veScale-FSDPæ˜¯ä¸€ç§é‡æ–°è®¾è®¡çš„å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚å®ƒç»“åˆäº†çµæ´»çš„åˆ†ç‰‡æ ¼å¼å’Œç»“æ„æ„ŸçŸ¥çš„è§„åˆ’ç®—æ³•ï¼Œæ”¯æŒå—çŠ¶é‡åŒ–å’Œéå…ƒç´ çº§ä¼˜åŒ–å™¨ã€‚ä¸ç°æœ‰çš„FSDPç³»ç»Ÿç›¸æ¯”ï¼ŒveScale-FSDPåœ¨ååé‡ä¸Šæé«˜äº†5%åˆ°66%ï¼Œå†…å­˜ä½¿ç”¨é‡é™ä½äº†16%åˆ°30%ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿæœ‰æ•ˆåœ°æ‰©å±•åˆ°æ•°ä¸‡å°GPUï¼Œæ»¡è¶³å¤§è§„æ¨¡è®­ç»ƒçš„éœ€æ±‚ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-02-26.html",
    "link_next": "2026-03-02.html",
    "link_month": "2026-02.html",
    "short_date_prev": {
        "ru": "26.02",
        "en": "02/26",
        "zh": "2æœˆ26æ—¥"
    },
    "short_date_next": {
        "ru": "02.03",
        "en": "03/02",
        "zh": "3æœˆ2æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 6,
        "#agents": 4,
        "#cv": 0,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 2,
        "#video": 5,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 1,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}