
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 16 papers. November 12.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }        
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">12 Ğ½Ğ¾ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">16 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-11-11.html">â¬…ï¸ <span id="prev-date">11.11</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-11-13.html">â¡ï¸ <span id="next-date">13.11</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-11.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'};
        let feedDateNext = {'ru': '13.11', 'en': '11/13', 'zh': '11æœˆ13æ—¥'};
        let feedDatePrev = {'ru': '11.11', 'en': '11/11', 'zh': '11æœˆ11æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'Published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2411.07232', 'title': 'Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models', 'url': 'https://huggingface.co/papers/2411.07232', 'abstract': 'Adding Object into images based on text instructions is a challenging task in semantic image editing, requiring a balance between preserving the original scene and seamlessly integrating the new object in a fitting location. Despite extensive efforts, existing models often struggle with this balance, particularly with finding a natural location for adding an object in complex scenes. We introduce Add-it, a training-free approach that extends diffusion models\' attention mechanisms to incorporate information from three key sources: the scene image, the text prompt, and the generated image itself. Our weighted extended-attention mechanism maintains structural consistency and fine details while ensuring natural object placement. Without task-specific fine-tuning, Add-it achieves state-of-the-art results on both real and generated image insertion benchmarks, including our newly constructed "Additing Affordance Benchmark" for evaluating object placement plausibility, outperforming supervised methods. Human evaluations show that Add-it is preferred in over 80% of cases, and it also demonstrates improvements in various automated metrics.', 'score': 41, 'issue_id': 523, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '5e344b551de578a9', 'data': {'categories': ['#diffusion', '#dataset', '#cv', '#benchmark'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Add-it: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Add-it Ğ´Ğ»Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· ÑÑ†ĞµĞ½Ñ‹, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Add-it Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ĞµĞ½ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 80% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¿Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ»ÑĞ´ĞµĞ¹.'}, 'en': {'title': 'Seamless Object Insertion with Add-it: No Fine-Tuning Needed!', 'desc': "This paper presents Add-it, a novel approach for adding objects to images based on text instructions, addressing the challenge of maintaining the original scene's integrity while ensuring the new object is placed naturally. The method leverages diffusion models' attention mechanisms, integrating information from the scene image, text prompt, and generated image to achieve seamless object insertion. By employing a weighted extended-attention mechanism, Add-it preserves structural consistency and fine details, resulting in more plausible object placements. Remarkably, Add-it does not require task-specific fine-tuning and outperforms existing supervised methods on various benchmarks, including a new evaluation standard for object placement plausibility."}, 'zh': {'title': 'æ— ç¼å›¾åƒç¼–è¾‘çš„æ–°çªç ´', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAdd-itçš„æ–¹æ³•ï¼Œç”¨äºæ ¹æ®æ–‡æœ¬æŒ‡ä»¤å°†ç‰©ä½“æ·»åŠ åˆ°å›¾åƒä¸­ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆåœºæ™¯å›¾åƒã€æ–‡æœ¬æç¤ºå’Œç”Ÿæˆå›¾åƒçš„ä¿¡æ¯ï¼Œä»¥å®ç°è‡ªç„¶çš„ç‰©ä½“æ”¾ç½®ã€‚Add-itåœ¨ä¸è¿›è¡Œç‰¹å®šä»»åŠ¡å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†å›¾åƒæ’å…¥åŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›ç»“æœï¼Œå¹¶åœ¨80%ä»¥ä¸Šçš„æƒ…å†µä¸‹è¢«äººç±»è¯„ä¼°è€…æ‰€åå¥½ã€‚è¯¥æ–¹æ³•ä¿æŒäº†ç»“æ„ä¸€è‡´æ€§å’Œç»†èŠ‚ï¼ŒåŒæ—¶ç¡®ä¿äº†ç‰©ä½“çš„è‡ªç„¶ä½ç½®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07199', 'title': 'OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision', 'url': 'https://huggingface.co/papers/2411.07199', 'abstract': 'Instruction-guided image editing methods have demonstrated significant potential by training diffusion models on automatically synthesized or manually annotated image editing pairs. However, these methods remain far from practical, real-life applications. We identify three primary challenges contributing to this gap. Firstly, existing models have limited editing skills due to the biased synthesis process. Secondly, these methods are trained with datasets with a high volume of noise and artifacts. This is due to the application of simple filtering methods like CLIP-score. Thirdly, all these datasets are restricted to a single low resolution and fixed aspect ratio, limiting the versatility to handle real-world use cases. In this paper, we present \\omniedit, which is an omnipotent editor to handle seven different image editing tasks with any aspect ratio seamlessly. Our contribution is in four folds: (1) \\omniedit is trained by utilizing the supervision from seven different specialist models to ensure task coverage. (2) we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality. (3) we propose a new editing architecture called EditNet to greatly boost the editing success rate, (4) we provide images with different aspect ratios to ensure that our model can handle any image in the wild. We have curated a test set containing images of different aspect ratios, accompanied by diverse instructions to cover different tasks. Both automatic evaluation and human evaluations demonstrate that \\omniedit can significantly outperform all the existing models. Our code, dataset and model will be available at https://tiger-ai-lab.github.io/OmniEdit/', 'score': 36, 'issue_id': 522, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '89d7bedc1b5241ac', 'data': {'categories': ['#dataset', '#data', '#optimization', '#cv', '#architecture', '#open_source', '#diffusion'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'OmniEdit: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑˆÑƒĞ¼Ğ½Ñ‹Ğµ Ğ¸ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ OmniEdit, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞµĞ¼ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ»ÑĞ±Ñ‹Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½. OmniEdit Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ ÑĞµĞ¼Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ EditNet Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'OmniEdit: The All-in-One Image Editing Solution', 'desc': "This paper introduces \textit{omniedit}, a versatile image editing model designed to tackle multiple editing tasks with varying aspect ratios. The authors address key challenges in existing methods, such as biased synthesis, noisy datasets, and fixed resolutions, which limit practical applications. By leveraging supervision from multiple specialist models and employing advanced importance sampling techniques, \textit{omniedit} enhances data quality and editing performance. The proposed EditNet architecture further improves the model's success rate, making it a powerful tool for real-world image editing scenarios."}, 'zh': {'title': 'å…¨èƒ½å›¾åƒç¼–è¾‘ï¼Œæ‰“ç ´ç°å®åº”ç”¨çš„é™åˆ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸º\textit{omniedit}çš„å…¨èƒ½å›¾åƒç¼–è¾‘å™¨ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„å±€é™æ€§ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åè§åˆæˆè¿‡ç¨‹å¯¼è‡´çš„ç¼–è¾‘èƒ½åŠ›æœ‰é™ã€è®­ç»ƒæ•°æ®é›†ä¸­çš„å™ªå£°å’Œä¼ªå½±é—®é¢˜ï¼Œä»¥åŠæ•°æ®é›†çš„ä½åˆ†è¾¨ç‡å’Œå›ºå®šå®½é«˜æ¯”é™åˆ¶ã€‚é€šè¿‡åˆ©ç”¨ä¸ƒä¸ªä¸åŒä¸“ä¸šæ¨¡å‹çš„ç›‘ç£ï¼Œ\textit{omniedit}èƒ½å¤Ÿå¤„ç†ä¸ƒç§ä¸åŒçš„å›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œå¹¶ä¸”æ”¯æŒä»»æ„å®½é«˜æ¯”ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œ\textit{omniedit}åœ¨è‡ªåŠ¨è¯„ä¼°å’Œäººå·¥è¯„ä¼°ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07140', 'title': 'Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models', 'url': 'https://huggingface.co/papers/2411.07140', 'abstract': 'New LLM evaluation benchmarks are important to align with the rapid development of Large Language Models (LLMs). In this work, we present Chinese SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions, and Chinese SimpleQA mainly has five properties (i.e., Chinese, Diverse, High-quality, Static, Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6 major topics with 99 diverse subtopics. Second, we conduct a comprehensive quality control process to achieve high-quality questions and answers, where the reference answers are static and cannot be changed over time. Third, following SimpleQA, the questions and answers are very short, and the grading process is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we perform a comprehensive evaluation on the factuality abilities of existing LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to better understand the Chinese factuality abilities of their models and facilitate the growth of foundation models.', 'score': 29, 'issue_id': 522, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': 'ffca97b13123516b', 'data': {'categories': ['#benchmark', '#dataset', '#alignment', '#low_resource', '#multilingual'], 'emoji': 'ğŸ‡¨ğŸ‡³', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Chinese SimpleQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 6 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ‚ĞµĞ¼ Ğ¸ 99 Ğ¿Ğ¾Ğ´Ñ‚ĞµĞ¼, Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Chinese SimpleQA Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ API OpenAI Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ²Ğ¾Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼.'}, 'en': {'title': 'Empowering Chinese LLMs with SimpleQA Factuality Benchmark', 'desc': 'This paper introduces Chinese SimpleQA, a new benchmark designed to evaluate the factuality of Large Language Models (LLMs) specifically for the Chinese language. It features a diverse set of questions across six major topics, ensuring high-quality and static reference answers for consistency in evaluation. The benchmark emphasizes short questions and answers, making the grading process straightforward and efficient, particularly using the OpenAI API. The authors aim for Chinese SimpleQA to help developers assess and improve the factuality capabilities of their models in the Chinese context.'}, 'zh': {'title': 'ä¸­æ–‡SimpleQAï¼šæå‡è¯­è¨€æ¨¡å‹äº‹å®èƒ½åŠ›çš„åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸­æ–‡SimpleQAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢è¯„ä¼°è¯­è¨€æ¨¡å‹å›ç­”çŸ­é—®é¢˜çš„äº‹å®èƒ½åŠ›çš„åŸºå‡†ã€‚è¯¥åŸºå‡†ä¸“æ³¨äºä¸­æ–‡ï¼Œæ¶µç›–å…­ä¸ªä¸»è¦ä¸»é¢˜å’Œ99ä¸ªå¤šæ ·åŒ–çš„å­ä¸»é¢˜ã€‚æˆ‘ä»¬é€šè¿‡ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶è¿‡ç¨‹ï¼Œç¡®ä¿é—®é¢˜å’Œç­”æ¡ˆçš„é«˜è´¨é‡ï¼Œå¹¶ä¸”å‚è€ƒç­”æ¡ˆæ˜¯é™æ€çš„ï¼Œä¸ä¼šéšæ—¶é—´å˜åŒ–ã€‚å¸Œæœ›ä¸­æ–‡SimpleQAèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°ç†è§£å…¶æ¨¡å‹çš„ä¸­æ–‡äº‹å®èƒ½åŠ›ï¼Œä¿ƒè¿›åŸºç¡€æ¨¡å‹çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.06176', 'title': 'M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework', 'url': 'https://huggingface.co/papers/2411.06176', 'abstract': 'The ability to understand and answer questions over documents can be useful in many business and practical applications. However, documents often contain lengthy and diverse multimodal contents such as texts, figures, and tables, which are very time-consuming for humans to read thoroughly. Hence, there is an urgent need to develop effective and automated methods to aid humans in this task. In this work, we introduce M-LongDoc, a benchmark of 851 samples, and an automated framework to evaluate the performance of large multimodal models. We further propose a retrieval-aware tuning approach for efficient and effective multimodal document reading. Compared to existing works, our benchmark consists of more recent and lengthy documents with hundreds of pages, while also requiring open-ended solutions and not just extractive answers. To our knowledge, our training framework is the first to directly address the retrieval setting for multimodal long documents. To enable tuning open-source models, we construct a training corpus in a fully automatic manner for the question-answering task over such documents. Experiments show that our tuning approach achieves a relative improvement of 4.6% for the correctness of model responses, compared to the baseline open-source models. Our data, code, and models are available at https://multimodal-documents.github.io.', 'score': 28, 'issue_id': 521, 'pub_date': '2024-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': '950719af940fd8d0', 'data': {'categories': ['#benchmark', '#long_context', '#dataset', '#multimodal', '#open_source', '#training'], 'emoji': 'ğŸ“„', 'ru': {'title': 'M-LongDoc: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ M-LongDoc - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 851 Ğ¾Ğ±Ñ€Ğ°Ğ·ĞµÑ† ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ…, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 4.6% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼.'}, 'en': {'title': 'Enhancing Multimodal Document Understanding with M-LongDoc', 'desc': 'This paper presents M-LongDoc, a benchmark designed to evaluate large multimodal models on lengthy documents that include text, figures, and tables. The authors introduce a retrieval-aware tuning method that enhances the efficiency and effectiveness of multimodal document reading, particularly for open-ended question-answering tasks. Unlike previous benchmarks, M-LongDoc features more recent and extensive documents, requiring models to generate comprehensive answers rather than just extractive responses. Experimental results indicate that the proposed tuning approach improves the accuracy of model responses by 4.6% compared to existing baseline models.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ–‡æ¡£ç†è§£çš„æ•ˆç‡ä¸æ•ˆæœ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºM-LongDocçš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«851ä¸ªæ ·æœ¬ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ–‡æ¡£ç†è§£å’Œé—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç”±äºæ–‡æ¡£é€šå¸¸åŒ…å«æ–‡æœ¬ã€å›¾å½¢å’Œè¡¨æ ¼ç­‰å¤šç§å†…å®¹ï¼Œäººå·¥é˜…è¯»è€—æ—¶è¾ƒé•¿ï¼Œå› æ­¤éœ€è¦å¼€å‘æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–æ–¹æ³•æ¥è¾…åŠ©äººç±»ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ£€ç´¢æ„ŸçŸ¥çš„è°ƒä¼˜æ–¹æ³•ï¼Œä»¥æé«˜å¤šæ¨¡æ€æ–‡æ¡£é˜…è¯»çš„æ•ˆç‡å’Œæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡å‹å“åº”çš„æ­£ç¡®æ€§ä¸Šç›¸è¾ƒäºåŸºçº¿å¼€æºæ¨¡å‹æœ‰4.6%çš„ç›¸å¯¹æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05830', 'title': 'GitChameleon: Unmasking the Version-Switching Capabilities of Code Generation Models', 'url': 'https://huggingface.co/papers/2411.05830', 'abstract': "The rapid evolution of software libraries presents a significant challenge for code generation models, which must adapt to frequent version updates while maintaining compatibility with previous versions. Existing code completion benchmarks often overlook this dynamic aspect, and the one that does consider it relies on static code prediction tasks without execution-based evaluation, offering a limited perspective on a model's practical usability. To address this gap, we introduce \\GitChameleon{}, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests.  is designed to rigorously assess the ability of modern large language models (LLMs) to generate version-specific code that is not only syntactically correct but also functionally accurate upon execution. Our comprehensive evaluations reveal that state-of-the-art LLMs struggle with this task; for instance, GPT-4o achieves a pass@10 of only 39.9\\% (43.7\\% when provided with error feedback), highlighting the complexity of the problem and the limitations of current models. By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries,  serves as a critical tool to advance the development of more adaptable and reliable code generation models. For facilitation for further exploration of version-conditioned code generation, we make our code repository publicly accessible at https://github.com/NizarIslah/GitChameleon.", 'score': 18, 'issue_id': 528, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': '86b9530c6bdf431e', 'data': {'categories': ['#benchmark', '#optimization', '#open_source', '#plp', '#dataset'], 'emoji': 'ğŸ¦', 'ru': {'title': 'GitChameleon: Ğ¢ĞµÑÑ‚ Ğ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²ĞµÑ€ÑĞ¸ÑĞ¼ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GitChameleon - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹ Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑ€ÑĞ¸ÑĞ¼Ğ¸ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 116 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Python Ñ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ ÑĞ½Ğ¸Ñ‚-Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹ - GPT-4 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»Ğ¸ÑˆÑŒ 39.9% ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· 10 Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº. GitChameleon Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Adapting Code Generation to Evolving Libraries with GitChameleon', 'desc': 'This paper addresses the challenges faced by code generation models due to the rapid evolution of software libraries. It introduces \textit{GitChameleon}, a new dataset with 116 Python code completion problems that are specifically tied to different library versions and include executable unit tests. The dataset allows for a more rigorous evaluation of large language models (LLMs) in generating code that is both syntactically correct and functionally accurate. The findings indicate that current state-of-the-art models, like GPT-4o, struggle with this task, underscoring the need for improved adaptability in code generation systems.'}, 'zh': {'title': 'åº”å¯¹è½¯ä»¶åº“ç‰ˆæœ¬æ›´æ–°çš„ä»£ç ç”ŸæˆæŒ‘æˆ˜', 'desc': 'éšç€è½¯ä»¶åº“çš„å¿«é€Ÿå‘å±•ï¼Œä»£ç ç”Ÿæˆæ¨¡å‹é¢ä¸´ç€é€‚åº”é¢‘ç¹ç‰ˆæœ¬æ›´æ–°çš„æŒ‘æˆ˜ï¼ŒåŒæ—¶è¿˜éœ€ä¿æŒä¸æ—§ç‰ˆæœ¬çš„å…¼å®¹æ€§ã€‚ç°æœ‰çš„ä»£ç è¡¥å…¨åŸºå‡†æµ‹è¯•å¾€å¾€å¿½è§†äº†è¿™ä¸€åŠ¨æ€ç‰¹æ€§ï¼Œè€Œè€ƒè™‘åˆ°è¿™ä¸€ç‚¹çš„æµ‹è¯•åˆä¾èµ–äºé™æ€ä»£ç é¢„æµ‹ä»»åŠ¡ï¼Œç¼ºä¹åŸºäºæ‰§è¡Œçš„è¯„ä¼°ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å®é™…å¯ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†\textit{GitChameleon}ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ‰‹åŠ¨æ•´ç†æ•°æ®é›†ï¼ŒåŒ…å«116ä¸ªPythonä»£ç è¡¥å…¨é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½åŸºäºç‰¹å®šçš„åº“ç‰ˆæœ¬ï¼Œå¹¶é™„æœ‰å¯æ‰§è¡Œçš„å•å…ƒæµ‹è¯•ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆç‰ˆæœ¬ç‰¹å®šä»£ç æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†è¿™ä¸€é—®é¢˜çš„å¤æ‚æ€§å’Œç°æœ‰æ¨¡å‹çš„å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07126', 'title': 'Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models', 'url': 'https://huggingface.co/papers/2411.07126', 'abstract': 'We introduce Edify Image, a family of diffusion models capable of generating photorealistic image content with pixel-perfect accuracy. Edify Image utilizes cascaded pixel-space diffusion models trained using a novel Laplacian diffusion process, in which image signals at different frequency bands are attenuated at varying rates. Edify Image supports a wide range of applications, including text-to-image synthesis, 4K upsampling, ControlNets, 360 HDR panorama generation, and finetuning for image customization.', 'score': 18, 'issue_id': 521, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': 'a7486a925b416669', 'data': {'categories': ['#3d', '#diffusion', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ¤Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Edify Image - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ»Ğ°Ğ¿Ğ»Ğ°ÑĞ¾Ğ²ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ°Ñ… Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°ÑÑ‚ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ. Edify Image Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ°Ğ¿ÑĞºĞµĞ¹Ğ»Ğ¸Ğ½Ğ³ Ğ´Ğ¾ 4K, ControlNets Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼ HDR 360Â°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ´Ğ»Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Edify Image: Revolutionizing Photorealistic Image Generation with Precision', 'desc': 'Edify Image is a new set of diffusion models designed to create highly realistic images with precise detail. It employs a unique Laplacian diffusion process that adjusts the diffusion rates for different frequency bands of image signals. This allows for versatile applications such as generating images from text, enhancing image resolution to 4K, and creating panoramic images. Additionally, it offers customization options through finetuning, making it adaptable for various image generation tasks.'}, 'zh': {'title': 'Edify Imageï¼šç”ŸæˆçœŸå®æ„Ÿå›¾åƒçš„æ–°çªç ´', 'desc': 'Edify Imageæ˜¯ä¸€ç§æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆåƒç´ çº§ç²¾ç¡®çš„çœŸå®æ„Ÿå›¾åƒå†…å®¹ã€‚å®ƒé‡‡ç”¨çº§è”åƒç´ ç©ºé—´æ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æ–°é¢–çš„æ‹‰æ™®æ‹‰æ–¯æ‰©æ•£è¿‡ç¨‹è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿä»¥ä¸åŒçš„é€Ÿç‡è¡°å‡ä¸åŒé¢‘ç‡å¸¦çš„å›¾åƒä¿¡å·ã€‚è¯¥æ¨¡å‹æ”¯æŒå¤šç§åº”ç”¨ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€4Kè¶…åˆ†è¾¨ç‡ã€ControlNetsã€360 HDRå…¨æ™¯ç”Ÿæˆä»¥åŠå›¾åƒå®šåˆ¶çš„å¾®è°ƒã€‚Edify Imageåœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå±•ç°äº†å¼ºå¤§çš„çµæ´»æ€§å’Œé«˜è´¨é‡çš„è¾“å‡ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.06208', 'title': 'IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization', 'url': 'https://huggingface.co/papers/2411.06208', 'abstract': 'In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces TRACE, a benchmark for improving and evaluating the complex instructionfollowing ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and outof-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.', 'score': 14, 'issue_id': 521, 'pub_date': '2024-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': 'de83b5a8e14da36e', 'data': {'categories': ['#benchmark', '#dataset', '#alignment', '#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'TRACE Ğ¸ IOPO: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TRACE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ IOPO (Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ²Ğ¾Ğ´Ğ°-Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. TRACE Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 120 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸ 1000 Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing LLMs with TRACE and IOPO for Complex Instructions', 'desc': 'This paper addresses the challenge of large language models (LLMs) in following complex instructions, which is becoming increasingly important as their applications grow. It introduces TRACE, a benchmark designed to enhance and evaluate the ability of LLMs to handle complex instructions, featuring a substantial dataset of 120K training examples and 1K evaluation cases. The authors propose a novel alignment method called IOPO (Input-Output Preference Optimization), which focuses on both input and output preferences to improve LLM responses. Experimental results demonstrate that IOPO significantly enhances performance on both in-domain and out-of-domain datasets, outperforming existing methods like SFT and DPO.'}, 'zh': {'title': 'æå‡å¤æ‚æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢†åŸŸï¼Œæ¨¡å‹å‡†ç¡®éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨æŒ‡ä»¤å¤æ‚æ€§è¿…é€Ÿå¢åŠ çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æå‡ºäº†TRACEï¼Œä¸€ä¸ªç”¨äºæé«˜å’Œè¯„ä¼°å¤æ‚æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„åŸºå‡†ï¼ŒåŒ…å«12ä¸‡æ¡è®­ç»ƒæ•°æ®å’Œ1000æ¡è¯„ä¼°æ•°æ®ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†IOPOï¼ˆè¾“å…¥-è¾“å‡ºåå¥½ä¼˜åŒ–ï¼‰å¯¹é½æ–¹æ³•ï¼Œè€ƒè™‘äº†è¾“å…¥å’Œè¾“å‡ºåå¥½å¯¹ï¼Œå¸®åŠ©LLMså¿«é€Ÿå¯¹é½å“åº”åå¥½å¹¶æ·±å…¥æ¢ç´¢æŒ‡ä»¤åå¥½ã€‚é€šè¿‡åœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†IOPOçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºåœ¨é¢†åŸŸå†…æ•°æ®ä¸Šåˆ†åˆ«æé«˜äº†8.15%å’Œ2.18%ï¼Œåœ¨é¢†åŸŸå¤–æ•°æ®ä¸Šæé«˜äº†6.29%å’Œ3.13%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07231', 'title': 'Watermark Anything with Localized Messages', 'url': 'https://huggingface.co/papers/2411.07231', 'abstract': 'Image watermarking methods are not tailored to handle small watermarked areas. This restricts applications in real-world scenarios where parts of the image may come from different sources or have been edited. We introduce a deep-learning model for localized image watermarking, dubbed the Watermark Anything Model (WAM). The WAM embedder imperceptibly modifies the input image, while the extractor segments the received image into watermarked and non-watermarked areas and recovers one or several hidden messages from the areas found to be watermarked. The models are jointly trained at low resolution and without perceptual constraints, then post-trained for imperceptibility and multiple watermarks. Experiments show that WAM is competitive with state-of-the art methods in terms of imperceptibility and robustness, especially against inpainting and splicing, even on high-resolution images. Moreover, it offers new capabilities: WAM can locate watermarked areas in spliced images and extract distinct 32-bit messages with less than 1 bit error from multiple small regions - no larger than 10% of the image surface - even for small 256times 256 images.', 'score': 9, 'issue_id': 524, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '0f19ada656cd9116', 'data': {'categories': ['#training', '#cv', '#dataset'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'WAM: Ğ›Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ»ÑĞ±Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ´ÑĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°ĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ Watermark Anything Model (WAM). WAM ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ½ĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¸Ñ…, Ğ´Ğ°Ğ¶Ğµ ĞµÑĞ»Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ±Ñ‹Ğ»Ğ¾ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¾ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ WAM ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ½ĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ²ÑÑ‚Ğ°Ğ²Ğ¾Ğº Ğ¸ ÑĞºĞ»ĞµĞµĞº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Watermark Anything: Revolutionizing Localized Image Watermarking', 'desc': 'The paper presents a novel deep-learning model called the Watermark Anything Model (WAM) designed for localized image watermarking. Unlike traditional methods, WAM effectively handles small watermarked areas, making it suitable for images with diverse sources or edits. The model consists of an embedder that subtly alters the image and an extractor that identifies watermarked regions to recover hidden messages. Experimental results demonstrate that WAM achieves high imperceptibility and robustness against common image manipulations, outperforming existing techniques, especially in challenging scenarios involving splicing and inpainting.'}, 'zh': {'title': 'å±€éƒ¨æ°´å°çš„æ–°çªç ´ï¼šæ°´å°ä»»ä½•æ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç§°ä¸ºæ°´å°ä»»ä½•æ¨¡å‹ï¼ˆWAMï¼‰ï¼Œç”¨äºå±€éƒ¨å›¾åƒæ°´å°ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸æ˜¾è‘—æ”¹å˜å›¾åƒçš„æƒ…å†µä¸‹ï¼ŒåµŒå…¥æ°´å°å¹¶æå–éšè—ä¿¡æ¯ã€‚WAMé€šè¿‡è”åˆè®­ç»ƒå’ŒåæœŸè®­ç»ƒï¼Œç¡®ä¿æ°´å°çš„éšè”½æ€§å’Œå¤šé‡æ°´å°çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWAMåœ¨éšè”½æ€§å’Œé²æ£’æ€§æ–¹é¢ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼Œå°¤å…¶åœ¨å¤„ç†æ‹¼æ¥å’Œä¿®å¤æ—¶è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07180', 'title': 'Counterfactual Generation from Language Models', 'url': 'https://huggingface.co/papers/2411.07180', 'abstract': "Understanding and manipulating the causal generation mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgery -- e.g., model ablations or manipulation of linear subspaces tied to specific concepts -- to intervene on these models. To understand the impact of interventions precisely, it is useful to examine counterfactuals -- e.g., how a given sentence would have appeared had it been generated by the model following a specific intervention. We highlight that counterfactual reasoning is conceptually distinct from interventions, as articulated in Pearl's causal hierarchy. Based on this observation, we propose a framework for generating true string counterfactuals by reformulating language models as Generalized Structural-equation. Models using the Gumbel-max trick. This allows us to model the joint distribution over original strings and their counterfactuals resulting from the same instantiation of the sampling noise. We develop an algorithm based on hindsight Gumbel sampling that allows us to infer the latent noise variables and generate counterfactuals of observed strings. Our experiments demonstrate that the approach produces meaningful counterfactuals while at the same time showing that commonly used intervention techniques have considerable undesired side effects.", 'score': 4, 'issue_id': 521, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '6b57fa07bdf242ce', 'data': {'categories': ['#math', '#interpretability', '#reasoning', '#training', '#data'], 'emoji': 'ğŸ”€', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ñ€ÑĞº Ğ“ÑƒĞ¼Ğ±ĞµĞ»Ñ-Ğ¼Ğ°ĞºÑĞ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº Ğ¸ Ğ¸Ñ… ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ Ğ“ÑƒĞ¼Ğ±ĞµĞ»Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº.'}, 'en': {'title': 'Harnessing Counterfactuals for Better Control of Language Models', 'desc': 'This paper focuses on understanding how to control language models by manipulating their causal generation mechanisms. It critiques existing methods like representation surgery, which alter model behavior but may not provide precise insights into their effects. The authors introduce a new framework that uses counterfactual reasoning to generate true string counterfactuals, distinguishing it from traditional interventions. Their approach employs Generalized Structural-equation Models and Gumbel-max sampling to effectively model the relationship between original strings and their counterfactuals, revealing the limitations of current intervention techniques.'}, 'zh': {'title': 'æŒæ¡è¯­è¨€æ¨¡å‹çš„å› æœç”Ÿæˆæœºåˆ¶', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨è¯­è¨€æ¨¡å‹ä¸­ç†è§£å’Œæ“æ§å› æœç”Ÿæˆæœºåˆ¶çš„é‡è¦æ€§ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦ä¾èµ–äºè¡¨ç¤ºæ‰‹æœ¯ç­‰æŠ€æœ¯æ¥å¹²é¢„æ¨¡å‹ï¼Œä½†æˆ‘ä»¬å¼ºè°ƒåäº‹å®æ¨ç†ä¸å¹²é¢„æ˜¯ä¸åŒçš„æ¦‚å¿µã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œé€šè¿‡å°†è¯­è¨€æ¨¡å‹é‡æ„ä¸ºå¹¿ä¹‰ç»“æ„æ–¹ç¨‹æ¨¡å‹ï¼Œç”ŸæˆçœŸå®çš„å­—ç¬¦ä¸²åäº‹å®ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæœ‰æ„ä¹‰çš„åäº‹å®ï¼ŒåŒæ—¶æ­ç¤ºäº†å¸¸ç”¨å¹²é¢„æŠ€æœ¯çš„æ˜¾è‘—å‰¯ä½œç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.06481', 'title': 'KMM: Key Frame Mask Mamba for Extended Motion Generation', 'url': 'https://huggingface.co/papers/2411.06481', 'abstract': "Human motion generation is a cut-edge area of research in generative computer vision, with promising applications in video creation, game development, and robotic manipulation. The recent Mamba architecture shows promising results in efficiently modeling long and complex sequences, yet two significant challenges remain: Firstly, directly applying Mamba to extended motion generation is ineffective, as the limited capacity of the implicit memory leads to memory decay. Secondly, Mamba struggles with multimodal fusion compared to Transformers, and lack alignment with textual queries, often confusing directions (left or right) or omitting parts of longer text queries. To address these challenges, our paper presents three key contributions: Firstly, we introduce KMM, a novel architecture featuring Key frame Masking Modeling, designed to enhance Mamba's focus on key actions in motion segments. This approach addresses the memory decay problem and represents a pioneering method in customizing strategic frame-level masking in SSMs. Additionally, we designed a contrastive learning paradigm for addressing the multimodal fusion problem in Mamba and improving the motion-text alignment. Finally, we conducted extensive experiments on the go-to dataset, BABEL, achieving state-of-the-art performance with a reduction of more than 57% in FID and 70% parameters compared to previous state-of-the-art methods. See project website: https://steve-zeyu-zhang.github.io/KMM", 'score': 3, 'issue_id': 524, 'pub_date': '2024-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '970f532cd0007e6c', 'data': {'categories': ['#cv', '#optimization', '#games', '#multimodal', '#long_context', '#architecture', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'KMM: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Mamba', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ KMM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Mamba. KMM Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Mamba, Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ BABEL Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.'}, 'en': {'title': 'Enhancing Motion Generation with Key Frame Masking', 'desc': "This paper focuses on improving human motion generation using a new architecture called KMM, which stands for Key frame Masking Modeling. The KMM architecture enhances the Mamba model's ability to handle long motion sequences by addressing memory decay and improving focus on key actions. Additionally, it introduces a contrastive learning approach to better fuse multimodal data and align motion with textual queries, overcoming limitations seen in previous models. The authors demonstrate the effectiveness of their method through extensive experiments on the BABEL dataset, achieving significant performance improvements over existing techniques."}, 'zh': {'title': 'æå‡äººç±»åŠ¨ä½œç”Ÿæˆçš„å…³é”®æŠ€æœ¯', 'desc': 'äººç±»åŠ¨ä½œç”Ÿæˆæ˜¯ç”Ÿæˆè®¡ç®—æœºè§†è§‰ä¸­çš„å‰æ²¿ç ”ç©¶é¢†åŸŸï¼Œå…·æœ‰è§†é¢‘åˆ›ä½œã€æ¸¸æˆå¼€å‘å’Œæœºå™¨äººæ“ä½œç­‰åº”ç”¨å‰æ™¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„KMMæ¶æ„ï¼Œé€šè¿‡å…³é”®å¸§æ©è”½å»ºæ¨¡æ¥å¢å¼ºMambaåœ¨åŠ¨ä½œç‰‡æ®µä¸­çš„å…³é”®åŠ¨ä½œå…³æ³¨èƒ½åŠ›ï¼Œä»è€Œè§£å†³äº†å†…å­˜è¡°å‡é—®é¢˜ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ èŒƒå¼ï¼Œä»¥æ”¹å–„Mambaçš„å¤šæ¨¡æ€èåˆå’Œè¿åŠ¨-æ–‡æœ¬å¯¹é½é—®é¢˜ã€‚é€šè¿‡åœ¨BABELæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨FIDæŒ‡æ ‡ä¸Šå‡å°‘äº†è¶…è¿‡57%ï¼Œå¹¶ä¸”å‚æ•°é‡å‡å°‘äº†70%ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05990', 'title': 'Game-theoretic LLM: Agent Workflow for Negotiation Games', 'url': 'https://huggingface.co/papers/2411.05990', 'abstract': "This paper investigates the rationality of large language models (LLMs) in strategic decision-making contexts, specifically within the framework of game theory. We evaluate several state-of-the-art LLMs across a spectrum of complete-information and incomplete-information games. Our findings reveal that LLMs frequently deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees.   To address these limitations, we design multiple game-theoretic workflows that guide the reasoning and decision-making processes of LLMs. These workflows aim to enhance the models' ability to compute Nash Equilibria and make rational choices, even under conditions of uncertainty and incomplete information. Experimental results demonstrate that the adoption of these workflows significantly improves the rationality and robustness of LLMs in game-theoretic tasks. Specifically, with the workflow, LLMs exhibit marked improvements in identifying optimal strategies, achieving near-optimal allocations in negotiation scenarios, and reducing susceptibility to exploitation during negotiations. Furthermore, we explore the meta-strategic considerations of whether it is rational for agents to adopt such workflows, recognizing that the decision to use or forgo the workflow constitutes a game-theoretic issue in itself.   Our research contributes to a deeper understanding of LLMs' decision-making capabilities in strategic contexts and provides insights into enhancing their rationality through structured workflows. The findings have implications for the development of more robust and strategically sound AI agents capable of navigating complex interactive environments. Code and data supporting this study are available at https://github.com/Wenyueh/game_theory.", 'score': 3, 'issue_id': 522, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': 'aae23469f2886f4c', 'data': {'categories': ['#games', '#agents', '#rl', '#math', '#reasoning'], 'emoji': 'ğŸ²', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ³Ñ€', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ³Ñ€. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ÑÑÑ‚ÑÑ Ğ¾Ñ‚ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑ‚ÑŒ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ¸Ñ ĞÑÑˆĞ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'Enhancing LLM Rationality in Strategic Decision-Making', 'desc': "This paper examines how large language models (LLMs) make decisions in strategic situations using game theory. It finds that LLMs often do not follow rational strategies, especially in complex games with larger payoff matrices. To improve their decision-making, the authors propose game-theoretic workflows that help LLMs better compute Nash Equilibria and make rational choices under uncertainty. The results show that these workflows significantly enhance the models' ability to identify optimal strategies and perform better in negotiation scenarios."}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„åšå¼ˆç†æ€§', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æˆ˜ç•¥å†³ç­–ä¸­çš„ç†æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åšå¼ˆè®ºæ¡†æ¶ä¸‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§å…ˆè¿›çš„LLMsåœ¨å®Œå…¨ä¿¡æ¯å’Œä¸å®Œå…¨ä¿¡æ¯åšå¼ˆä¸­çš„è¡¨ç°ï¼Œå‘ç°éšç€åšå¼ˆå¤æ‚æ€§çš„å¢åŠ ï¼ŒLLMså¸¸å¸¸åç¦»ç†æ€§ç­–ç•¥ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¤šç§åšå¼ˆè®ºå·¥ä½œæµç¨‹ï¼Œä»¥æŒ‡å¯¼LLMsçš„æ¨ç†å’Œå†³ç­–è¿‡ç¨‹ï¼Œä»è€Œæé«˜å…¶è®¡ç®—çº³ä»€å‡è¡¡å’Œåœ¨ä¸ç¡®å®šæ¡ä»¶ä¸‹åšå‡ºç†æ€§é€‰æ‹©çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è¿™äº›å·¥ä½œæµç¨‹æ˜¾è‘—æé«˜äº†LLMsåœ¨åšå¼ˆä»»åŠ¡ä¸­çš„ç†æ€§å’Œç¨³å¥æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05902', 'title': 'Autoregressive Models in Vision: A Survey', 'url': 'https://huggingface.co/papers/2411.05902', 'abstract': 'Autoregressive modeling has been a huge success in the field of natural language processing (NLP). Recently, autoregressive models have emerged as a significant area of focus in computer vision, where they excel in producing high-quality visual content. Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, i.e., pixel-level, token-level, or scale-level, reflecting the diverse and hierarchical nature of visual data compared to the sequential structure of language. This survey comprehensively examines the literature on autoregressive models applied to vision. To improve readability for researchers from diverse research backgrounds, we start with preliminary sequence representation and modeling in vision. Next, we divide the fundamental frameworks of visual autoregressive models into three general sub-categories, including pixel-based, token-based, and scale-based models based on the strategy of representation. We then explore the interconnections between autoregressive models and other generative models. Furthermore, we present a multi-faceted categorization of autoregressive models in computer vision, including image generation, video generation, 3D generation, and multi-modal generation. We also elaborate on their applications in diverse domains, including emerging domains such as embodied AI and 3D medical AI, with about 250 related references. Finally, we highlight the current challenges to autoregressive models in vision with suggestions about potential research directions. We have also set up a Github repository to organize the papers included in this survey at: https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey.', 'score': 2, 'issue_id': 532, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': '49d4d6e55fb35ca2', 'data': {'categories': ['#healthcare', '#survey', '#video', '#3d', '#open_source', '#multimodal', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸: Ğ¾Ñ‚ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ´Ğ¾ 3D', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ². Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ¸ 3D Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¹ Ğ˜Ğ˜. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ÑÑ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Exploring Autoregressive Models: Bridging NLP and Computer Vision', 'desc': 'This paper surveys the application of autoregressive models in computer vision, highlighting their success in generating high-quality visual content. It categorizes these models into three main types: pixel-based, token-based, and scale-based, reflecting the hierarchical nature of visual data. The authors also discuss the relationship between autoregressive models and other generative models, as well as their applications in various fields such as embodied AI and 3D medical AI. Additionally, the paper addresses current challenges and suggests future research directions, providing a comprehensive resource for researchers in the area.'}, 'zh': {'title': 'è‡ªå›å½’æ¨¡å‹ï¼šè§†è§‰ç”Ÿæˆçš„æ–°å‰æ²¿', 'desc': 'è‡ªå›å½’å»ºæ¨¡åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œæœ€è¿‘åœ¨è®¡ç®—æœºè§†è§‰ä¸­ä¹Ÿæˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚è‡ªå›å½’æ¨¡å‹åœ¨è§†è§‰å†…å®¹ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒå’Œè§†é¢‘ã€‚æœ¬æ–‡ç»¼è¿°äº†è‡ªå›å½’æ¨¡å‹åœ¨è§†è§‰ä¸­çš„åº”ç”¨ï¼Œæ¢è®¨äº†åƒç´ çº§ã€æ ‡è®°çº§å’Œå°ºåº¦çº§ç­‰ä¸åŒçš„è¡¨ç¤ºç­–ç•¥ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†è‡ªå›å½’æ¨¡å‹ä¸å…¶ä»–ç”Ÿæˆæ¨¡å‹çš„å…³ç³»ï¼Œå¹¶æå‡ºäº†åœ¨æ–°å…´é¢†åŸŸä¸­çš„åº”ç”¨å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05945', 'title': 'NeKo: Toward Post Recognition Generative Correction Large Language Models with Task-Oriented Experts', 'url': 'https://huggingface.co/papers/2411.05945', 'abstract': "Construction of a general-purpose post-recognition error corrector poses a crucial question: how can we most effectively train a model on a large mixture of domain datasets? The answer would lie in learning dataset-specific features and digesting their knowledge in a single model. Previous methods achieve this by having separate correction language models, resulting in a significant increase in parameters. In this work, we present Mixture-of-Experts as a solution, highlighting that MoEs are much more than a scalability tool. We propose a Multi-Task Correction MoE, where we train the experts to become an ``expert'' of speech-to-text, language-to-text and vision-to-text datasets by learning to route each dataset's tokens to its mapped expert. Experiments on the Open ASR Leaderboard show that we explore a new state-of-the-art performance by achieving an average relative 5.0% WER reduction and substantial improvements in BLEU scores for speech and translation tasks. On zero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with 15.5% to 27.6% relative WER reduction in the Hyporadise benchmark. NeKo performs competitively on grammar and post-OCR correction as a multi-task model.", 'score': 2, 'issue_id': 528, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': '1306ade09b2fc5c5', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#machine_translation', '#transfer_learning', '#multimodal', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº: Ğ¾Ğ´Ğ¸Ğ½ MoE Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mixture-of-Experts (MoE) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ğ² MoE Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑÑ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ NeKo Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Harnessing Mixture-of-Experts for Superior Post-Recognition Error Correction', 'desc': "This paper introduces a novel approach to post-recognition error correction using a Mixture-of-Experts (MoE) model. The authors propose a Multi-Task Correction MoE that effectively learns from diverse datasets, such as speech-to-text and language-to-text, by routing tokens to specialized experts. This method reduces the need for separate correction models, leading to fewer parameters while maintaining high performance. Experimental results demonstrate significant improvements in word error rate (WER) and BLEU scores, showcasing the model's effectiveness across various tasks, including zero-shot evaluations against leading models."}, 'zh': {'title': 'æ··åˆä¸“å®¶æ¨¡å‹ï¼šæå‡å¤šä»»åŠ¡å­¦ä¹ çš„åˆ©å™¨', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•æœ‰æ•ˆåœ°è®­ç»ƒä¸€ä¸ªé€šç”¨çš„åè¯†åˆ«é”™è¯¯ä¿®æ­£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šç§é¢†åŸŸæ•°æ®é›†æ—¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMixture-of-Expertsï¼‰ï¼Œé€šè¿‡è®©ä¸“å®¶å­¦ä¹ ç‰¹å®šæ•°æ®é›†çš„ç‰¹å¾ï¼Œä»è€Œåœ¨ä¸€ä¸ªæ¨¡å‹ä¸­æ•´åˆè¿™äº›çŸ¥è¯†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨è¯­éŸ³è½¬æ–‡æœ¬ã€è¯­è¨€è½¬æ–‡æœ¬å’Œè§†è§‰è½¬æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—é™ä½äº†é”™è¯¯ç‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå±•ç¤ºäº†æ··åˆä¸“å®¶æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.06424', 'title': 'Ablation is Not Enough to Emulate DPO: How Neuron Dynamics Drive Toxicity Reduction', 'url': 'https://huggingface.co/papers/2411.06424', 'abstract': 'Safety fine-tuning algorithms are commonly used to fine-tune language models to reduce harmful outputs, but the exact internal mechanisms of how those models achieve this remain unclear. In studying direct preference optimisation (DPO) for toxicity reduction, current explanations claim that DPO works by dampening the most toxic MLP neurons to learn an offset to avert toxic regions in the residual stream. However, by ablating the most toxic neurons and applying activation patching, we find this explanation incomplete. By projecting neuron activation changes onto a toxicity probe, we find that only 31.8\\% of toxicity reduction comes from dampened toxic neurons. Instead, DPO reduces toxicity by accumulating effects across multiple neuron groups, both reducing writing in the toxic direction and promoting anti-toxicity in the residual stream. Moreover, DPO gives noisy adjustments to neuron activations, with many neurons actually increasing toxicity. This indicates that DPO is a balancing process between opposing neuron effects to achieve toxicity reduction.', 'score': 2, 'issue_id': 524, 'pub_date': '2024-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '045698e0e102aa83', 'data': {'categories': ['#ethics', '#training', '#hallucinations', '#rlhf', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'DPO: Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (DPO) Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ³Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼ DPO Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 31.8% ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ². DPO ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ², ĞºĞ°Ğº ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°Ñ Ğ°Ğ½Ñ‚Ğ¸-Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ğµ Ğ² Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ.'}, 'en': {'title': 'DPO: Balancing Neuron Effects for Safer Language Models', 'desc': 'This paper investigates how direct preference optimization (DPO) helps reduce harmful outputs in language models. It challenges the common belief that DPO primarily works by dampening the most toxic neurons. Instead, the study reveals that only a small portion of toxicity reduction is due to this dampening effect, with most of the reduction coming from a complex interplay of multiple neuron groups. The findings suggest that DPO functions as a balancing act, where both toxic and anti-toxic neuron activations are adjusted to achieve an overall decrease in toxicity.'}, 'zh': {'title': 'DPOï¼šåœ¨å¯¹ç«‹ç¥ç»å…ƒæ•ˆåº”ä¸­å®ç°æ¯’æ€§å‡å°‘çš„å¹³è¡¡è¿‡ç¨‹', 'desc': 'å®‰å…¨å¾®è°ƒç®—æ³•å¸¸ç”¨äºè°ƒæ•´è¯­è¨€æ¨¡å‹ï¼Œä»¥å‡å°‘æœ‰å®³è¾“å‡ºï¼Œä½†å…¶å†…éƒ¨æœºåˆ¶ä»ä¸æ¸…æ¥šã€‚æœ¬æ–‡ç ”ç©¶äº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åœ¨å‡å°‘æ¯’æ€§æ–¹é¢çš„ä½œç”¨ï¼Œå‘ç°ç°æœ‰è§£é‡Šä¸å¤Ÿå…¨é¢ã€‚é€šè¿‡å¯¹æœ€æ¯’ç¥ç»å…ƒçš„æ¶ˆèå’Œæ¿€æ´»ä¿®è¡¥ï¼Œæˆ‘ä»¬å‘ç°åªæœ‰31.8%çš„æ¯’æ€§å‡å°‘æ¥è‡ªäºæŠ‘åˆ¶æ¯’æ€§ç¥ç»å…ƒã€‚DPOé€šè¿‡å¤šä¸ªç¥ç»å…ƒç»„çš„ç´¯ç§¯æ•ˆåº”æ¥å‡å°‘æ¯’æ€§ï¼ŒåŒæ—¶åœ¨æ®‹å·®æµä¸­ä¿ƒè¿›åæ¯’æ€§ï¼Œè¡¨æ˜DPOæ˜¯ä¸€ä¸ªåœ¨å¯¹ç«‹ç¥ç»å…ƒæ•ˆåº”ä¹‹é—´å¹³è¡¡çš„è¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.06272', 'title': 'Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models', 'url': 'https://huggingface.co/papers/2411.06272', 'abstract': 'As large language models become increasingly prevalent in the financial sector, there is a pressing need for a standardized method to comprehensively assess their performance. However, existing finance benchmarks often suffer from limited language and task coverage, as well as challenges such as low-quality datasets and inadequate adaptability for LLM evaluation. To address these limitations, we propose "Golden Touchstone", the first comprehensive bilingual benchmark for financial LLMs, which incorporates representative datasets from both Chinese and English across eight core financial NLP tasks. Developed from extensive open source data collection and industry-specific demands, this benchmark includes a variety of financial tasks aimed at thoroughly assessing models\' language understanding and generation capabilities. Through comparative analysis of major models on the benchmark, such as GPT-4o Llama3, FinGPT and FinMA, we reveal their strengths and limitations in processing complex financial information. Additionally, we open-sourced Touchstone-GPT, a financial LLM trained through continual pre-training and financial instruction tuning, which demonstrates strong performance on the bilingual benchmark but still has limitations in specific tasks.This research not only provides the financial large language models with a practical evaluation tool but also guides the development and optimization of future research. The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at https://github.com/IDEA-FinAI/Golden-Touchstone, contributing to the ongoing evolution of FinLLMs and fostering further research in this critical area.', 'score': 2, 'issue_id': 520, 'pub_date': '2024-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': '2559c023f673c9b4', 'data': {'categories': ['#low_resource', '#optimization', '#open_source', '#multilingual', '#benchmark'], 'emoji': 'ğŸ’¹', 'ru': {'title': 'Ğ­Ñ‚Ğ°Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼ ÑĞµĞºÑ‚Ğ¾Ñ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ "Golden Touchstone", Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ NLP. Ğ­Ñ‚Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ĞºĞ¾Ğ´Ñƒ Ğ¸ Ğ²ĞµÑĞ°Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Touchstone-GPT, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… LLM.'}, 'en': {'title': 'Golden Touchstone: Elevating Financial LLM Evaluation', 'desc': "This paper introduces 'Golden Touchstone', a new bilingual benchmark designed to evaluate the performance of large language models (LLMs) in the financial sector. It addresses the shortcomings of existing benchmarks by providing a comprehensive assessment across eight key financial NLP tasks in both Chinese and English. The benchmark is built from high-quality datasets and reflects industry needs, allowing for a thorough evaluation of models like GPT-4o, Llama3, FinGPT, and FinMA. Additionally, the paper presents Touchstone-GPT, a financial LLM that has been fine-tuned for better performance on this benchmark, while also making the resources publicly available to support further research in financial LLMs."}, 'zh': {'title': 'é‡‘èé¢†åŸŸçš„æ ‡å‡†åŒ–è¯„ä¼°å·¥å…·â€”â€”é‡‘è‰²åŸºå‡†', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡‘èé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œè¯„ä¼°å…¶æ€§èƒ½çš„æ ‡å‡†åŒ–æ–¹æ³•å˜å¾—å°¤ä¸ºé‡è¦ã€‚ç°æœ‰çš„é‡‘èåŸºå‡†æµ‹è¯•å­˜åœ¨è¯­è¨€å’Œä»»åŠ¡è¦†ç›–é¢æœ‰é™ã€æ•°æ®é›†è´¨é‡ä½ä»¥åŠé€‚åº”æ€§ä¸è¶³ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œé‡‘è‰²åŸºå‡†â€ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢çš„åŒè¯­é‡‘èåŸºå‡†ï¼Œæ¶µç›–äº†ä¸­è‹±æ–‡çš„å…«ä¸ªæ ¸å¿ƒé‡‘èè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚é€šè¿‡å¯¹ä¸»è¦æ¨¡å‹çš„æ¯”è¾ƒåˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†å®ƒä»¬åœ¨å¤„ç†å¤æ‚é‡‘èä¿¡æ¯æ—¶çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å¼€æºäº†Touchstone-GPTæ¨¡å‹ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶å’Œä¼˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05966', 'title': 'Energy Efficient Protein Language Models: Leveraging Small Language Models with LoRA for Controllable Protein Generation', 'url': 'https://huggingface.co/papers/2411.05966', 'abstract': 'Large language models (LLMs) have demonstrated significant success in natural language processing (NLP) tasks and have shown promising results in other domains such as protein sequence generation. However, there remain salient differences between LLMs used for NLP, which effectively handle multiple tasks and are available in small sizes, and protein language models that are often specialized for specific tasks and only exist in larger sizes. In this work, we introduce two small protein language models, based on Llama-3-8B and Phi-3-mini, that are capable of both uncontrollable and controllable protein generation. For the uncontrollable generation task, our best model achieves an average pLDDT score of 69.75, demonstrating robust performance in generating viable protein structures. For the controllable generation task, in which the model generates proteins according to properties specified in the prompt, we achieve a remarkable average TM-Score of 0.84, indicating high structural similarity to target proteins. We chose 10 properties, including six classes of enzymes, to extend the capabilities of prior protein language models. Our approach utilizes the Low-Rank Adaptor (LoRA) technique, reducing trainable parameters to just 4% of the original model size, lowering computational requirements. By using a subset of the UniRef50 dataset and small models, we reduced the overall training time by 70% without compromising performance. Notably, Phi-3-mini reduced trainable parameters by 60%, decreasing training cost by 30% compared to Llama 3. Consequently, Phi-3 achieved a comparable TM-Score of 0.81, demonstrating that smaller models can match the performance of larger ones, like Llama 3. We also demonstrate the deployment of our models on the energy efficient ET-SoC-1 chip, significantly improving the TPS/W by a factor of 3.', 'score': 1, 'issue_id': 534, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': '7cf32991005092d0', 'data': {'categories': ['#low_resource', '#optimization', '#science', '#dataset', '#small_models', '#training', '#inference'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ»ĞºĞ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ±ĞµĞ»ĞºĞ°Ğ¼Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Llama-3-8B Ğ¸ Phi-3-mini. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ ĞºĞ°Ğº Ğº Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ»ĞºĞ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ pLDDT Ğ¸ TM-Score ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Low-Rank Adaptor (LoRA) Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': 'Small Models, Big Impact: Efficient Protein Generation', 'desc': 'This paper presents two small protein language models, Llama-3-8B and Phi-3-mini, which excel in both uncontrollable and controllable protein generation tasks. The models achieve impressive performance metrics, with an average pLDDT score of 69.75 for generating viable protein structures and a TM-Score of 0.84 for generating proteins based on specified properties. By employing the Low-Rank Adaptor (LoRA) technique, the models significantly reduce the number of trainable parameters, leading to a 70% decrease in training time and a 30% reduction in training costs. The results indicate that smaller models can achieve comparable performance to larger models while being more efficient, especially when deployed on energy-efficient hardware.'}, 'zh': {'title': 'å°å‹æ¨¡å‹ä¹Ÿèƒ½ç”Ÿæˆé«˜è´¨é‡è›‹ç™½è´¨', 'desc': 'æœ¬ç ”ç©¶ä»‹ç»äº†ä¸¤ç§å°å‹è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºLlama-3-8Bå’ŒPhi-3-miniï¼Œèƒ½å¤Ÿè¿›è¡Œä¸å¯æ§å’Œå¯æ§çš„è›‹ç™½è´¨ç”Ÿæˆã€‚æˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹åœ¨ä¸å¯æ§ç”Ÿæˆä»»åŠ¡ä¸­è¾¾åˆ°äº†69.75çš„å¹³å‡pLDDTåˆ†æ•°ï¼Œæ˜¾ç¤ºå‡ºç”Ÿæˆå¯è¡Œè›‹ç™½è´¨ç»“æ„çš„å¼ºå¤§æ€§èƒ½ã€‚åœ¨å¯æ§ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæ¨¡å‹æ ¹æ®æç¤ºç”Ÿæˆç‰¹å®šå±æ€§çš„è›‹ç™½è´¨ï¼Œå¹³å‡TM-Scoreè¾¾åˆ°äº†0.84ï¼Œè¡¨æ˜ä¸ç›®æ ‡è›‹ç™½è´¨çš„ç»“æ„ç›¸ä¼¼æ€§å¾ˆé«˜ã€‚é€šè¿‡ä½¿ç”¨ä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰æŠ€æœ¯ï¼Œæˆ‘ä»¬å°†å¯è®­ç»ƒå‚æ•°å‡å°‘åˆ°åŸæ¨¡å‹çš„4%ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (1)', '#agi', '#alignment (2)', '#architecture (2)', '#audio', '#benchmark (7)', '#cv (6)', '#data (2)', '#dataset (10)', '#diffusion (3)', '#ethics (1)', '#games (2)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (1)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource (3)', '#machine_translation (1)', '#math (2)', '#multilingual (2)', '#multimodal (4)', '#open_source (5)', '#optimization (7)', '#plp (1)', '#rag', '#reasoning (2)', '#rl (1)', '#rlhf (1)', '#robotics', '#science (1)', '#security', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic', '#training (7)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">ğŸ“ ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2024-11-12 21:08',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-11-12 21:08')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-11-12 21:08')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    