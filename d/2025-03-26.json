{
    "date": {
        "ru": "26 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 26",
        "zh": "3æœˆ26æ—¥"
    },
    "time_utc": "2025-03-26 13:21",
    "weekday": 2,
    "issue_id": 2907,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.19325",
            "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
            "url": "https://huggingface.co/papers/2503.19325",
            "abstract": "Long-context autoregressive modeling has significantly advanced language generation, but video generation still struggles to fully utilize extended temporal contexts. To investigate long-context video modeling, we introduce Frame AutoRegressive (FAR), a strong baseline for video autoregressive modeling. Just as language models learn causal dependencies between tokens (i.e., Token AR), FAR models temporal causal dependencies between continuous frames, achieving better convergence than Token AR and video diffusion transformers. Building on FAR, we observe that long-context vision modeling faces challenges due to visual redundancy. Existing RoPE lacks effective temporal decay for remote context and fails to extrapolate well to long video sequences. Additionally, training on long videos is computationally expensive, as vision tokens grow much faster than language tokens. To tackle these issues, we propose balancing locality and long-range dependency. We introduce FlexRoPE, an test-time technique that adds flexible temporal decay to RoPE, enabling extrapolation to 16x longer vision contexts. Furthermore, we propose long short-term context modeling, where a high-resolution short-term context window ensures fine-grained temporal consistency, while an unlimited long-term context window encodes long-range information using fewer tokens. With this approach, we can train on long video sequences with a manageable token context length. We demonstrate that FAR achieves state-of-the-art performance in both short- and long-video generation, providing a simple yet effective baseline for video autoregressive modeling.",
            "score": 52,
            "issue_id": 2896,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 25",
                "zh": "3æœˆ25æ—¥"
            },
            "hash": "543c7dbfad83ed73",
            "authors": [
                "Yuchao Gu",
                "Weijia Mao",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19325.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#long_context",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "FAR: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Frame AutoRegressive (FAR) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ FlexRoPE - Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ² 16 Ñ€Ğ°Ğ· Ğ´Ğ»Ğ¸Ğ½Ğ½ĞµĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. FAR Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Frame AutoRegressive Modeling",
                    "desc": "This paper presents Frame AutoRegressive (FAR), a new method for generating videos by modeling the temporal dependencies between frames. FAR improves upon existing models by addressing the challenges of visual redundancy and the limitations of current positional encoding techniques like RoPE. The authors introduce FlexRoPE, which allows for flexible temporal decay, enabling the model to handle longer video sequences effectively. By combining short-term and long-term context modeling, FAR achieves state-of-the-art results in video generation while maintaining computational efficiency."
                },
                "zh": {
                    "title": "é•¿æ—¶é—´ä¸Šä¸‹æ–‡è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘è‡ªå›å½’å»ºæ¨¡æ–¹æ³•ï¼Œç§°ä¸ºFrame AutoRegressive (FAR)ï¼Œæ—¨åœ¨è§£å†³é•¿æ—¶é—´ä¸Šä¸‹æ–‡è§†é¢‘ç”Ÿæˆä¸­çš„æŒ‘æˆ˜ã€‚FARé€šè¿‡å»ºæ¨¡è¿ç»­å¸§ä¹‹é—´çš„æ—¶é—´å› æœå…³ç³»ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹ï¼Œå–å¾—äº†æ›´å¥½çš„æ”¶æ•›æ•ˆæœã€‚ä¸ºäº†åº”å¯¹è§†è§‰å†—ä½™å’Œè®¡ç®—æˆæœ¬é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†FlexRoPEæŠ€æœ¯ï¼Œèƒ½å¤Ÿçµæ´»è°ƒæ•´è¿œç¨‹ä¸Šä¸‹æ–‡çš„æ—¶é—´è¡°å‡ï¼Œå¹¶å¼•å…¥äº†é•¿çŸ­æœŸä¸Šä¸‹æ–‡å»ºæ¨¡æ–¹æ³•ï¼Œä»¥ç¡®ä¿æ—¶é—´ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFARåœ¨çŸ­è§†é¢‘å’Œé•¿è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæˆä¸ºè§†é¢‘è‡ªå›å½’å»ºæ¨¡çš„æœ‰æ•ˆåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18931",
            "title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models",
            "url": "https://huggingface.co/papers/2503.18931",
            "abstract": "Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of varying sizes and produce visual representations that are more aligned with language representations, regardless of their original pre-training process. To this end, we introduce CoMP, a carefully designed multimodal pre-training pipeline. CoMP uses a Continual Rotary Position Embedding to support native resolution continual pre-training, and an Alignment Loss between visual and textual features through language prototypes to align multimodal representations. By three-stage training, our VFMs achieve remarkable improvements not only in multimodal understanding but also in other downstream tasks such as classification and segmentation. Remarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA with a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5 mIoU on ADE20K under frozen chunk evaluation.",
            "score": 21,
            "issue_id": 2897,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 24",
                "zh": "3æœˆ24æ—¥"
            },
            "hash": "59128d6a0bd3862c",
            "authors": [
                "Yitong Chen",
                "Lingchen Meng",
                "Wujian Peng",
                "Zuxuan Wu",
                "Yu-Gang Jiang"
            ],
            "affiliations": [
                "Shanghai Innovation Institute",
                "Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18931.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#multimodal",
                    "#optimization",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CoMP - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ (VFM). CoMP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ¢Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ĞºĞ°Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ CoMP-SigLIP Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ImageNet-1K Ğ¸ ADE20K."
                },
                "en": {
                    "title": "Enhancing Visual Models with Multimodal Pre-Training",
                    "desc": "This paper presents a method to enhance Vision Foundation Models (VFMs) by using a multimodal pre-training approach. The proposed method, called CoMP, allows VFMs to process visual inputs of different sizes and align visual representations with language representations. CoMP employs a Continual Rotary Position Embedding and an Alignment Loss to improve the integration of visual and textual features. The results show significant performance gains in multimodal tasks and other applications like classification and segmentation, demonstrating the effectiveness of the proposed training pipeline."
                },
                "zh": {
                    "title": "æå‡è§†è§‰æ¨¡å‹çš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ–¹æ³•CoMPï¼Œç”¨äºæå‡è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰çš„è¡¨ç°ã€‚é€šè¿‡æŒç»­çš„å¤šæ¨¡æ€é¢„è®­ç»ƒï¼Œæ¨¡å‹èƒ½å¤Ÿå¤„ç†ä¸åŒå¤§å°çš„è§†è§‰è¾“å…¥ï¼Œå¹¶ç”Ÿæˆä¸è¯­è¨€è¡¨ç¤ºæ›´ä¸€è‡´çš„è§†è§‰è¡¨ç¤ºã€‚CoMPé‡‡ç”¨äº†æŒç»­æ—‹è½¬ä½ç½®åµŒå…¥å’Œè§†è§‰ä¸æ–‡æœ¬ç‰¹å¾ä¹‹é—´çš„å¯¹é½æŸå¤±ï¼Œä»¥å®ç°å¤šæ¨¡æ€è¡¨ç¤ºçš„å¯¹é½ã€‚ç»è¿‡ä¸‰é˜¶æ®µè®­ç»ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£å’Œå…¶ä»–ä¸‹æ¸¸ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19385",
            "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing",
            "url": "https://huggingface.co/papers/2503.19385",
            "abstract": "We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion models--offering faster generation and high-quality outputs in state-of-the-art image and video generative models--efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches.",
            "score": 20,
            "issue_id": 2896,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 25",
                "zh": "3æœˆ25æ—¥"
            },
            "hash": "e0ead8fbe973f326",
            "authors": [
                "Jaihoon Kim",
                "Taehoon Yoon",
                "Jisung Hwang",
                "Minhyuk Sung"
            ],
            "affiliations": [
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19385.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#diffusion",
                    "#optimization",
                    "#video"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ´ĞµĞ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ (SDE), Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ SDE, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ† Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Enhancing Flow Models with Efficient Inference-Time Scaling",
                    "desc": "This paper introduces a new method for improving flow models during inference, which is the process of generating outputs from trained models. The authors focus on three innovative techniques: using stochastic differential equations (SDE) for particle sampling, converting interpolants to increase diversity in generated samples, and implementing Rollover Budget Forcing (RBF) to optimize computational resource allocation. These methods allow flow models to achieve better sample quality and efficiency, similar to advancements seen in diffusion models. The results show that their approach significantly enhances performance, making flow models more competitive in generating high-quality images and videos."
                },
                "zh": {
                    "title": "æµæ¨¡å‹çš„é«˜æ•ˆæ¨ç†æ—¶é—´ç¼©æ”¾æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹é¢„è®­ç»ƒæµæ¨¡å‹çš„æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚è¿‘å¹´æ¥ï¼Œæ¨ç†æ—¶é—´ç¼©æ”¾åœ¨å¤§è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ä¸­å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œé€šè¿‡åˆ©ç”¨é¢å¤–çš„è®¡ç®—æ¥æé«˜æ ·æœ¬è´¨é‡æˆ–æ›´å¥½åœ°ç¬¦åˆç”¨æˆ·åå¥½ã€‚å°½ç®¡æµæ¨¡å‹ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆè¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œä½†ç”±äºå…¶ç¡®å®šæ€§ç”Ÿæˆè¿‡ç¨‹ï¼Œç°æœ‰çš„æ‰©æ•£æ¨¡å‹æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•æ— æ³•ç›´æ¥åº”ç”¨äºæµæ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§å…³é”®æ€æƒ³ï¼Œä»¥å®ç°æµæ¨¡å‹çš„é«˜æ•ˆæ¨ç†æ—¶é—´ç¼©æ”¾ï¼šåŸºäºSDEçš„ç”Ÿæˆã€æ’å€¼è½¬æ¢å’Œè‡ªé€‚åº”è®¡ç®—èµ„æºåˆ†é…ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19622",
            "title": "Exploring Hallucination of Large Multimodal Models in Video\n  Understanding: Benchmark, Analysis and Mitigation",
            "url": "https://huggingface.co/papers/2503.19622",
            "abstract": "The hallucination of large multimodal models (LMMs), providing responses that appear correct but are actually incorrect, limits their reliability and applicability. This paper aims to study the hallucination problem of LMMs in video modality, which is dynamic and more challenging compared to static modalities like images and text. From this motivation, we first present a comprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in video understanding tasks. It is built upon three dimensions, i.e., hallucination causes, hallucination aspects, and question formats, resulting in 6K questions. Then, we quantitatively study 7 influential factors on hallucinations, e.g., duration time of videos, model sizes, and model reasoning, via experiments of 16 LMMs on the presented benchmark. In addition, inspired by recent thinking models like OpenAI o1, we propose a video-thinking model to mitigate the hallucinations of LMMs via supervised reasoning fine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT enhances reasoning capabilities while TDPO reduces hallucinations in the thinking process. Extensive experiments and analyses demonstrate the effectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on hallucination evaluation and reduces the bias score by 4.5%. The code and data are public at https://github.com/Hongcheng-Gao/HAVEN.",
            "score": 19,
            "issue_id": 2897,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 25",
                "zh": "3æœˆ25æ—¥"
            },
            "hash": "c727aefeccdca380",
            "authors": [
                "Hongcheng Gao",
                "Jiashu Qu",
                "Jingyi Tang",
                "Baolong Bi",
                "Yue Liu",
                "Hongyu Chen",
                "Li Liang",
                "Li Su",
                "Qingming Huang"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS",
                "National University of Singapore",
                "University of Chinese Academy of Sciences",
                "University of Cincinnati"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19622.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#hallucinations",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ Ğ¼Ñ‹ÑĞ»ÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LMM) Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº HAVEN Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ LMM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞµĞ¼Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ñ… Ğ½Ğ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ 16 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ LMM. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ 'video-thinking' Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² SRFT Ğ¸ TDPO, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ²ÑˆĞ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Mitigating Hallucinations in Video Understanding with HAVEN",
                    "desc": "This paper addresses the issue of hallucinations in large multimodal models (LMMs), particularly in video understanding tasks. It introduces a benchmark called HAVEN, which evaluates hallucinations based on various factors such as causes, aspects, and question formats, comprising 6,000 questions. The authors analyze seven influential factors affecting hallucinations and propose a novel video-thinking model that employs supervised reasoning fine-tuning and direct preference optimization to reduce these hallucinations. Experimental results show a significant improvement in accuracy and a reduction in bias, demonstrating the effectiveness of the proposed methods."
                },
                "zh": {
                    "title": "è§£å†³è§†é¢‘æ¨¡æ€ä¸­çš„å¹»è§‰é—®é¢˜",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œè¿™ç§é—®é¢˜ä½¿å¾—æ¨¡å‹çš„è¾“å‡ºçœ‹ä¼¼æ­£ç¡®ä½†å®é™…ä¸Šä¸å‡†ç¡®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºHAVENçš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°LMMsåœ¨è§†é¢‘æ¨¡æ€ä¸‹çš„å¹»è§‰ï¼Œæ¶µç›–äº†å¹»è§‰çš„åŸå› ã€æ–¹é¢å’Œé—®é¢˜æ ¼å¼ç­‰ä¸‰ä¸ªç»´åº¦ï¼Œå…±åŒ…å«6000ä¸ªé—®é¢˜ã€‚é€šè¿‡å¯¹16ä¸ªLMMsè¿›è¡Œå®éªŒï¼Œæˆ‘ä»¬å®šé‡åˆ†æäº†å½±å“å¹»è§‰çš„7ä¸ªå› ç´ ï¼Œå¦‚è§†é¢‘æ—¶é•¿ã€æ¨¡å‹è§„æ¨¡å’Œæ¨ç†èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è§†é¢‘æ€ç»´æ¨¡å‹ï¼Œé€šè¿‡ç›‘ç£æ¨ç†å¾®è°ƒï¼ˆSRFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆTDPOï¼‰æ¥å‡è½»å¹»è§‰ç°è±¡ï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§ä¸Šæé«˜äº†7.65%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14905",
            "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection\n  with Artifact Explanation",
            "url": "https://huggingface.co/papers/2503.14905",
            "abstract": "With the rapid advancement of Artificial Intelligence Generated Content (AIGC) technologies, synthetic images have become increasingly prevalent in everyday life, posing new challenges for authenticity assessment and detection. Despite the effectiveness of existing methods in evaluating image authenticity and locating forgeries, these approaches often lack human interpretability and do not fully address the growing complexity of synthetic data. To tackle these challenges, we introduce FakeVLM, a specialized large multimodal model designed for both general synthetic image and DeepFake detection tasks. FakeVLM not only excels in distinguishing real from fake images but also provides clear, natural language explanations for image artifacts, enhancing interpretability. Additionally, we present FakeClue, a comprehensive dataset containing over 100,000 images across seven categories, annotated with fine-grained artifact clues in natural language. FakeVLM demonstrates performance comparable to expert models while eliminating the need for additional classifiers, making it a robust solution for synthetic data detection. Extensive evaluations across multiple datasets confirm the superiority of FakeVLM in both authenticity classification and artifact explanation tasks, setting a new benchmark for synthetic image detection. The dataset and code will be released in: https://github.com/opendatalab/FakeVLM.",
            "score": 14,
            "issue_id": 2900,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 19",
                "zh": "3æœˆ19æ—¥"
            },
            "hash": "e8053458773c179b",
            "authors": [
                "Siwei Wen",
                "Junyan Ye",
                "Peilin Feng",
                "Hengrui Kang",
                "Zichen Wen",
                "Yize Chen",
                "Jiang Wu",
                "Wenjun Wu",
                "Conghui He",
                "Weijia Li"
            ],
            "affiliations": [
                "Beihang University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University",
                "Sun Yat-Sen University",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14905.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#interpretability",
                    "#benchmark",
                    "#dataset",
                    "#synthetic",
                    "#cv"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "FakeVLM: Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "FakeVLM - ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ DeepFake. ĞĞ½Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… FakeClue Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 100 000 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸ Ğ¾Ğ± Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ°Ñ…. FakeVLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "FakeVLM: Unmasking Synthetic Images with Clarity",
                    "desc": "This paper introduces FakeVLM, a large multimodal model designed to detect synthetic images and DeepFakes while providing human-readable explanations for its decisions. Unlike existing methods, FakeVLM enhances interpretability by explaining image artifacts in natural language, making it easier for users to understand the detection process. The model is evaluated on a new dataset called FakeClue, which contains over 100,000 annotated images, allowing for fine-grained analysis of image authenticity. Results show that FakeVLM performs comparably to expert models without needing extra classifiers, establishing a new standard in synthetic image detection."
                },
                "zh": {
                    "title": "FakeVLMï¼šåˆæˆå›¾åƒæ£€æµ‹çš„æ–°æ ‡æ†",
                    "desc": "éšç€äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œåˆæˆå›¾åƒåœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­å˜å¾—è¶Šæ¥è¶Šæ™®éï¼Œè¿™ç»™çœŸå®æ€§è¯„ä¼°å’Œæ£€æµ‹å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–¹æ³•è™½ç„¶åœ¨è¯„ä¼°å›¾åƒçœŸå®æ€§å’Œå®šä½ä¼ªé€ æ–¹é¢æœ‰æ•ˆï¼Œä½†å¾€å¾€ç¼ºä¹äººç±»å¯è§£é‡Šæ€§ï¼Œæ— æ³•å®Œå…¨åº”å¯¹åˆæˆæ•°æ®æ—¥ç›Šå¤æ‚çš„æƒ…å†µã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FakeVLMï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹åˆæˆå›¾åƒå’Œæ·±åº¦ä¼ªé€ æ£€æµ‹ä»»åŠ¡çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ã€‚FakeVLMä¸ä»…åœ¨åŒºåˆ†çœŸå®ä¸ä¼ªé€ å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¿˜èƒ½æä¾›æ¸…æ™°çš„è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œå¢å¼ºäº†å¯è§£é‡Šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19903",
            "title": "Scaling Vision Pre-Training to 4K Resolution",
            "url": "https://huggingface.co/papers/2503.19903",
            "abstract": "High-resolution perception of visual details is crucial for daily tasks. Current vision pre-training, however, is still limited to low resolutions (e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images. We introduce PS3 that scales CLIP-style vision pre-training to 4K resolution with a near-constant cost. Instead of contrastive learning on global image representation, PS3 is pre-trained by selectively processing local regions and contrasting them with local detailed captions, enabling high-resolution representation learning with greatly reduced computational overhead. The pre-trained PS3 is able to both encode the global image at low resolution and selectively process local high-resolution regions based on their saliency or relevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the resulting model, named VILA-HD, significantly improves high-resolution visual perception compared to baselines without high-resolution vision pre-training such as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks appealing scaling properties of VILA-HD, including scaling up resolution for free and scaling up test-time compute for better performance. Compared to state of the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL across multiple benchmarks and achieves better efficiency than latest token pruning approaches. Finally, we find current benchmarks do not require 4K-resolution perception, which motivates us to propose 4KPro, a new benchmark of image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs, including a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x speedup over Qwen2-VL.",
            "score": 11,
            "issue_id": 2898,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 25",
                "zh": "3æœˆ25æ—¥"
            },
            "hash": "7b81adffd1f39557",
            "authors": [
                "Baifeng Shi",
                "Boyi Li",
                "Han Cai",
                "Yao Lu",
                "Sifei Liu",
                "Marco Pavone",
                "Jan Kautz",
                "Song Han",
                "Trevor Darrell",
                "Pavlo Molchanov",
                "Hongxu Yin"
            ],
            "affiliations": [
                "NVIDIA",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19903.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#cv",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "PS3: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ",
                    "desc": "PS3 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ (4K) Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, PS3 Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ñ… Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ PS3 Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. PS3 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Scaling Vision Pre-Training to 4K with PS3",
                    "desc": "This paper presents PS3, a novel approach to vision pre-training that enables high-resolution image processing at 4K resolution while maintaining a near-constant computational cost. By focusing on local regions of images and contrasting them with detailed captions, PS3 enhances the learning of visual representations without the heavy resource demands typically associated with high-resolution data. The resulting model, VILA-HD, demonstrates significant improvements in visual perception tasks compared to existing models, achieving better efficiency and performance across various benchmarks. Additionally, the authors introduce 4KPro, a new benchmark for image question answering at 4K resolution, where VILA-HD shows superior results over previous models."
                },
                "zh": {
                    "title": "é«˜åˆ†è¾¨ç‡è§†è§‰æ„ŸçŸ¥çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPS3çš„è§†è§‰é¢„è®­ç»ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿä»¥æ¥è¿‘æ’å®šçš„æˆæœ¬å°†CLIPé£æ ¼çš„è§†è§‰é¢„è®­ç»ƒæ‰©å±•åˆ°4Kåˆ†è¾¨ç‡ã€‚PS3é€šè¿‡é€‰æ‹©æ€§å¤„ç†å±€éƒ¨åŒºåŸŸå¹¶ä¸å±€éƒ¨è¯¦ç»†æè¿°è¿›è¡Œå¯¹æ¯”ï¼Œæ¥å®ç°é«˜åˆ†è¾¨ç‡è¡¨ç¤ºå­¦ä¹ ï¼Œä»è€Œå¤§å¹…é™ä½è®¡ç®—å¼€é”€ã€‚é¢„è®­ç»ƒåçš„PS3èƒ½å¤Ÿåœ¨ä½åˆ†è¾¨ç‡ä¸‹ç¼–ç å…¨å±€å›¾åƒï¼Œå¹¶æ ¹æ®æ–‡æœ¬æç¤ºçš„æ˜¾è‘—æ€§æˆ–ç›¸å…³æ€§é€‰æ‹©æ€§å¤„ç†å±€éƒ¨é«˜åˆ†è¾¨ç‡åŒºåŸŸã€‚æœ€ç»ˆï¼ŒåŸºäºPS3çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹VILA-HDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†é«˜åˆ†è¾¨ç‡è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19855",
            "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking",
            "url": "https://huggingface.co/papers/2503.19855",
            "abstract": "Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite this, current models are constrained by limitations in handling long texts and reinforcement learning (RL) training efficiency. To address these issues, we propose a simple yet effective test-time scaling approach Multi-round Thinking. This method iteratively refines model reasoning by leveraging previous answers as prompts for subsequent rounds. Extensive experiments across multiple models, including QwQ-32B and DeepSeek-R1, consistently show performance improvements on various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and LiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round 1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a similar increase from 79.7% to 82.0%. These results confirm that Multi-round Thinking is a broadly applicable, straightforward approach to achieving stable enhancements in model performance, underscoring its potential for future developments in test-time scaling techniques. The key prompt: {Original question prompt} The assistant's previous answer is: <answer> {last round answer} </answer>, and please re-answer.",
            "score": 7,
            "issue_id": 2897,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 25",
                "zh": "3æœˆ25æ—¥"
            },
            "hash": "c9d16e0d2423104a",
            "authors": [
                "Xiaoyu Tian",
                "Sitong Zhao",
                "Haotian Wang",
                "Shuaiting Chen",
                "Yunjie Ji",
                "Yiping Peng",
                "Han Zhao",
                "Xiangang Li"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2503.19855.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ˜Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ˜Ğ˜: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ 'ĞœĞ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ'. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ QwQ-32B Ğ¸ DeepSeek-R1, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ 'ĞœĞ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ²Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ' - ÑÑ‚Ğ¾ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Model Performance with Multi-round Thinking",
                    "desc": "This paper introduces a new method called Multi-round Thinking to improve the performance of large language models (LLMs) during test-time scaling. The approach involves iteratively refining the model's reasoning by using previous answers as prompts for subsequent rounds of questioning. Experiments show that this method leads to significant accuracy improvements across various benchmarks, demonstrating its effectiveness in enhancing model performance. The results indicate that Multi-round Thinking is a simple yet powerful technique that can be widely applied to improve LLMs' reasoning capabilities."
                },
                "zh": {
                    "title": "å¤šè½®æ€è€ƒï¼šæå‡æ¨¡å‹æ¨ç†çš„æœ‰æ•ˆæ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºå¤šè½®æ€è€ƒçš„æµ‹è¯•æ—¶é—´æ‰©å±•æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ä¹‹å‰çš„ç­”æ¡ˆä½œä¸ºåç»­è½®æ¬¡çš„æç¤ºï¼Œè¿­ä»£åœ°ä¼˜åŒ–æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¤šè½®æ€è€ƒåï¼Œå¤šä¸ªæ¨¡å‹åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡æœ‰æ˜¾è‘—æå‡ã€‚æ¯”å¦‚ï¼ŒQwQ-32Båœ¨AIME 2024æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä»80.3%æå‡è‡³82.1%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19910",
            "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
            "url": "https://huggingface.co/papers/2503.19910",
            "abstract": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images based on a multimodal query. Typical training data consists of triplets containing a reference image, a textual description of desired modifications, and the target image, which are expensive and time-consuming to acquire. The scarcity of CIR datasets has led to zero-shot approaches utilizing synthetic triplets or leveraging vision-language models (VLMs) with ubiquitous web-crawled image-caption pairs. However, these methods have significant limitations: synthetic triplets suffer from limited scale, lack of diversity, and unnatural modification text, while image-caption pairs hinder joint embedding learning of the multimodal query due to the absence of triplet data. Moreover, existing approaches struggle with complex and nuanced modification texts that demand sophisticated fusion and understanding of vision and language modalities. We present CoLLM, a one-stop framework that effectively addresses these limitations. Our approach generates triplets on-the-fly from image-caption pairs, enabling supervised training without manual annotation. We leverage Large Language Models (LLMs) to generate joint embeddings of reference images and modification texts, facilitating deeper multimodal fusion. Additionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset comprising 3.4M samples, and refine existing CIR benchmarks (CIRR and Fashion-IQ) to enhance evaluation reliability. Experimental results demonstrate that CoLLM achieves state-of-the-art performance across multiple CIR benchmarks and settings. MTCIR yields competitive results, with up to 15% performance improvement. Our refined benchmarks provide more reliable evaluation metrics for CIR models, contributing to the advancement of this important field.",
            "score": 6,
            "issue_id": 2896,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 25",
                "zh": "3æœˆ25æ—¥"
            },
            "hash": "63f36082e6c27f3e",
            "authors": [
                "Chuong Huynh",
                "Jinyu Yang",
                "Ashish Tawari",
                "Mubarak Shah",
                "Son Tran",
                "Raffay Hamid",
                "Trishul Chilimbi",
                "Abhinav Shrivastava"
            ],
            "affiliations": [
                "Amazon",
                "Center for Research in Computer Vision, University of Central Florida",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19910.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#synthetic",
                    "#dataset",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "CoLLM: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CoLLM - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (CIR). CoLLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ Ğ¸Ğ· Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MTCIR Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ CIR."
                },
                "en": {
                    "title": "Revolutionizing Composed Image Retrieval with CoLLM",
                    "desc": "This paper introduces CoLLM, a novel framework for Composed Image Retrieval (CIR) that overcomes the challenges of limited training data. It generates triplets from existing image-caption pairs, allowing for supervised training without the need for manual annotations. By utilizing Large Language Models (LLMs), CoLLM creates joint embeddings that enhance the understanding of complex multimodal queries. The authors also present the Multi-Text CIR (MTCIR) dataset, which significantly improves evaluation metrics and demonstrates state-of-the-art performance in CIR tasks."
                },
                "zh": {
                    "title": "CoLLMï¼šå¤åˆå›¾åƒæ£€ç´¢çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCoLLMçš„æ¡†æ¶ï¼Œç”¨äºè§£å†³å¤åˆå›¾åƒæ£€ç´¢ï¼ˆCIRï¼‰ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»å›¾åƒ-æ–‡æœ¬å¯¹ä¸­åŠ¨æ€ç”Ÿæˆä¸‰å…ƒç»„ï¼Œé¿å…äº†æ‰‹åŠ¨æ ‡æ³¨çš„éœ€æ±‚ï¼Œä»è€Œå®ç°äº†ç›‘ç£å­¦ä¹ ã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆå‚è€ƒå›¾åƒå’Œä¿®æ”¹æ–‡æœ¬çš„è”åˆåµŒå…¥ï¼Œä¿ƒè¿›äº†å¤šæ¨¡æ€çš„æ·±åº¦èåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†ä¸€ä¸ªåŒ…å«340ä¸‡æ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®é›†MTCIRï¼Œå¹¶æ”¹è¿›äº†ç°æœ‰çš„CIRåŸºå‡†ï¼Œä»¥æé«˜è¯„ä¼°çš„å¯é æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13964",
            "title": "MDocAgent: A Multi-Modal Multi-Agent Framework for Document\n  Understanding",
            "url": "https://huggingface.co/papers/2503.13964",
            "abstract": "Document Question Answering (DocQA) is a very common task. Existing methods using Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and Retrieval Augmented Generation (RAG) often prioritize information from a single modal, failing to effectively integrate textual and visual cues. These approaches struggle with complex multi-modal reasoning, limiting their performance on real-world documents. We present MDocAgent (A Multi-Modal Multi-Agent Framework for Document Understanding), a novel RAG and multi-agent framework that leverages both text and image. Our system employs five specialized agents: a general agent, a critical agent, a text agent, an image agent and a summarizing agent. These agents engage in multi-modal context retrieval, combining their individual insights to achieve a more comprehensive understanding of the document's content. This collaborative approach enables the system to synthesize information from both textual and visual components, leading to improved accuracy in question answering. Preliminary experiments on five benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of our MDocAgent, achieve an average improvement of 12.1% compared to current state-of-the-art method. This work contributes to the development of more robust and comprehensive DocQA systems capable of handling the complexities of real-world documents containing rich textual and visual information. Our data and code are available at https://github.com/aiming-lab/MDocAgent.",
            "score": 5,
            "issue_id": 2900,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "ac3677766b7897a0",
            "authors": [
                "Siwei Han",
                "Peng Xia",
                "Ruiyi Zhang",
                "Tong Sun",
                "Yun Li",
                "Hongtu Zhu",
                "Huaxiu Yao"
            ],
            "affiliations": [
                "Adobe Research",
                "UNC-Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13964.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#benchmark",
                    "#optimization",
                    "#agents",
                    "#open_source",
                    "#rag"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MDocAgent - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ RAG, MDocAgent ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ÑÑ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 12.1% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "MDocAgent: Uniting Text and Images for Smarter Document Understanding",
                    "desc": "This paper introduces MDocAgent, a new framework designed for Document Question Answering (DocQA) that effectively combines text and image data. Unlike existing methods that often focus on a single type of information, MDocAgent utilizes a multi-agent system with five specialized agents to enhance multi-modal reasoning. These agents work together to retrieve and synthesize information from both textual and visual sources, improving the accuracy of answers to questions about documents. Preliminary tests show that MDocAgent outperforms current leading methods by an average of 12.1%, demonstrating its potential for better handling complex real-world documents."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ–‡æ¡£ç†è§£çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–‡æ¡£é—®ç­”ç³»ç»ŸMDocAgentï¼Œå®ƒç»“åˆäº†æ–‡æœ¬å’Œå›¾åƒä¿¡æ¯ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€æ–‡æ¡£ç†è§£çš„èƒ½åŠ›ã€‚ç°æœ‰çš„æ–¹æ³•å¾€å¾€åªå…³æ³¨å•ä¸€æ¨¡æ€ï¼Œå¯¼è‡´åœ¨å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†ä¸­è¡¨ç°ä¸ä½³ã€‚MDocAgenté‡‡ç”¨äº†äº”ä¸ªä¸“é—¨çš„ä»£ç†ï¼Œåˆ†åˆ«è´Ÿè´£ä¸åŒçš„ä»»åŠ¡ï¼Œé€šè¿‡åä½œæ£€ç´¢å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼Œä»è€Œæ›´å…¨é¢åœ°ç†è§£æ–‡æ¡£å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMDocAgentåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æé«˜äº†12.1%çš„å‡†ç¡®ç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†ç°å®ä¸–ç•Œæ–‡æ¡£ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18446",
            "title": "Latent Space Super-Resolution for Higher-Resolution Image Generation\n  with Diffusion Models",
            "url": "https://huggingface.co/papers/2503.18446",
            "abstract": "In this paper, we propose LSRNA, a novel framework for higher-resolution (exceeding 1K) image generation using diffusion models by leveraging super-resolution directly in the latent space. Existing diffusion models struggle with scaling beyond their training resolutions, often leading to structural distortions or content repetition. Reference-based methods address the issues by upsampling a low-resolution reference to guide higher-resolution generation. However, they face significant challenges: upsampling in latent space often causes manifold deviation, which degrades output quality. On the other hand, upsampling in RGB space tends to produce overly smoothed outputs. To overcome these limitations, LSRNA combines Latent space Super-Resolution (LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance high-frequency details. Our extensive experiments demonstrate that integrating LSRNA outperforms state-of-the-art reference-based methods across various resolutions and metrics, while showing the critical role of latent space upsampling in preserving detail and sharpness. The code is available at https://github.com/3587jjh/LSRNA.",
            "score": 4,
            "issue_id": 2897,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 24",
                "zh": "3æœˆ24æ—¥"
            },
            "hash": "d3e203fb399d6eee",
            "authors": [
                "Jinho Jeong",
                "Sangmin Han",
                "Jinwoo Kim",
                "Seon Joo Kim"
            ],
            "affiliations": [
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18446.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#3d",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "LSRNA: Ğ¡ÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "LSRNA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ (Ğ±Ğ¾Ğ»ĞµĞµ 1K) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. LSRNA ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ÑÑƒĞ¿ĞµÑ€Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ (LSR) Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ¿Ğ¾ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ¼ (RNA) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LSRNA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ² Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Enhancing High-Resolution Image Generation with LSRNA",
                    "desc": "This paper introduces LSRNA, a new framework designed to generate high-resolution images (over 1K) using diffusion models by applying super-resolution techniques in the latent space. Traditional diffusion models often struggle with generating images at higher resolutions, leading to issues like distortions and repeated content. The proposed method addresses these challenges by utilizing Latent space Super-Resolution (LSR) for better alignment and Region-wise Noise Addition (RNA) to improve detail in the generated images. Experimental results show that LSRNA significantly outperforms existing reference-based methods, highlighting the importance of latent space upsampling for maintaining image quality."
                },
                "zh": {
                    "title": "LSRNAï¼šè¶…åˆ†è¾¨ç‡ç”Ÿæˆçš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶LSRNAï¼Œç”¨äºç”Ÿæˆé«˜åˆ†è¾¨ç‡ï¼ˆè¶…è¿‡1Kï¼‰çš„å›¾åƒï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç›´æ¥åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ã€‚ç°æœ‰çš„æ‰©æ•£æ¨¡å‹åœ¨è¶…å‡ºè®­ç»ƒåˆ†è¾¨ç‡æ—¶å¸¸å¸¸å‡ºç°ç»“æ„å¤±çœŸæˆ–å†…å®¹é‡å¤çš„é—®é¢˜ã€‚å‚è€ƒåŸºç¡€çš„æ–¹æ³•é€šè¿‡å°†ä½åˆ†è¾¨ç‡å‚è€ƒå›¾åƒä¸Šé‡‡æ ·æ¥æŒ‡å¯¼é«˜åˆ†è¾¨ç‡ç”Ÿæˆï¼Œä½†åœ¨æ½œåœ¨ç©ºé—´ä¸­ä¸Šé‡‡æ ·ä¼šå¯¼è‡´æµå½¢åå·®ï¼Œä»è€Œé™ä½è¾“å‡ºè´¨é‡ã€‚LSRNAç»“åˆäº†æ½œåœ¨ç©ºé—´è¶…åˆ†è¾¨ç‡ï¼ˆLSRï¼‰å’ŒåŒºåŸŸå™ªå£°æ·»åŠ ï¼ˆRNAï¼‰ï¼Œæœ‰æ•ˆæå‡äº†é«˜é¢‘ç»†èŠ‚ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨å„ä¸ªåˆ†è¾¨ç‡å’ŒæŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„å‚è€ƒåŸºç¡€æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19470",
            "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2503.19470",
            "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning, exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating reasoning with external search processes remains challenging, especially for complex multi-hop questions requiring multiple retrieval steps. We propose ReSearch, a novel framework that trains LLMs to Reason with Search via reinforcement learning without using any supervised data on reasoning steps. Our approach treats search operations as integral components of the reasoning chain, where when and how to perform searches is guided by text-based thinking, and search results subsequently influence further reasoning. We train ReSearch on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct extensive experiments. Despite being trained on only one dataset, our models demonstrate strong generalizability across various benchmarks. Analysis reveals that ReSearch naturally elicits advanced reasoning capabilities such as reflection and self-correction during the reinforcement learning process.",
            "score": 3,
            "issue_id": 2897,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 25",
                "zh": "3æœˆ25æ—¥"
            },
            "hash": "3e50fa3f4f4a6c0c",
            "authors": [
                "Mingyang Chen",
                "Tianpeng Li",
                "Haoze Sun",
                "Yijie Zhou",
                "Chenzheng Zhu",
                "Fan Yang",
                "Zenan Zhou",
                "Weipeng Chen",
                "Haofen Wang",
                "Jeff Z. Pan",
                "Wen Zhang",
                "Huajun Chen"
            ],
            "affiliations": [
                "Baichuan Inc.",
                "The University of Edinburgh",
                "Tongji University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19470.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#rag",
                    "#training",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ReSearch, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¸ ĞºĞ°Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ ReSearch ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering LLMs: Reasoning Meets Search with ReSearch",
                    "desc": "This paper introduces ReSearch, a new framework that enhances Large Language Models (LLMs) by integrating reasoning with external search processes. It uses reinforcement learning to train LLMs to effectively decide when and how to perform searches, treating these operations as key parts of the reasoning process. The framework is tested on Qwen2.5 models and shows strong performance across different benchmarks, even though it was trained on a single dataset. Notably, ReSearch enables advanced reasoning skills like reflection and self-correction, showcasing its potential for complex question answering."
                },
                "zh": {
                    "title": "æ¨ç†ä¸æœç´¢çš„å®Œç¾ç»“åˆ",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å°†æ¨ç†ä¸å¤–éƒ¨æœç´¢è¿‡ç¨‹ç»“åˆä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯å¯¹äºå¤æ‚çš„å¤šè·³é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ReSearchï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒLLMsè¿›è¡Œæœç´¢æ¨ç†ï¼Œè€Œä¸ä½¿ç”¨ä»»ä½•ç›‘ç£æ•°æ®ã€‚è¯¥æ–¹æ³•å°†æœç´¢æ“ä½œè§†ä¸ºæ¨ç†é“¾çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œæœç´¢çš„æ—¶æœºå’Œæ–¹å¼ç”±åŸºäºæ–‡æœ¬çš„æ€ç»´æŒ‡å¯¼ï¼Œæœç´¢ç»“æœè¿›ä¸€æ­¥å½±å“æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå°½ç®¡åªåœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒï¼ŒReSearchæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19041",
            "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews",
            "url": "https://huggingface.co/papers/2503.19041",
            "abstract": "Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often undermines their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, which comprises two simple, low-resource, and effective data-driven methods that modify training data by previewing partial answer prefixes. Both methods aim to preserve the model's inherent safety mechanisms by minimizing perturbations to initial token distributions. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs. Code is released at https://github.com/zjunlp/LookAheadTuning.",
            "score": 3,
            "issue_id": 2896,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 24",
                "zh": "3æœˆ24æ—¥"
            },
            "hash": "8495b5a09ddf5611",
            "authors": [
                "Kangwei Liu",
                "Mengru Wang",
                "Yujie Luo",
                "Lin Yuan",
                "Mengshu Sun",
                "Ningyu Zhang",
                "Lei Liang",
                "Zhiqiang Zhang",
                "Jun Zhou",
                "Huajun Chen"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University",
                "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19041.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#low_resource"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LookAhead Tuning. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ñ… Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ¾Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LookAhead Tuning ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "LookAhead Tuning: Safeguarding LLMs During Fine-Tuning",
                    "desc": "This paper presents LookAhead Tuning, a novel approach to fine-tuning large language models (LLMs) while preserving their safety alignment. The method involves two data-driven techniques that adjust training data by examining partial answer prefixes, which helps maintain the model's safety mechanisms. By minimizing changes to the initial token distributions, LookAhead Tuning effectively prevents safety degradation during the adaptation process. Experimental results show that this approach not only safeguards model safety but also ensures strong performance on various downstream tasks."
                },
                "zh": {
                    "title": "LookAhead Tuningï¼šå®‰å…¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLookAhead Tuningçš„æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³åœ¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶å®‰å…¨æ€§ä¸‹é™çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡é¢„è§ˆéƒ¨åˆ†ç­”æ¡ˆå‰ç¼€ï¼Œé‡‡ç”¨ä¸¤ç§ç®€å•ä¸”ä½èµ„æºçš„æ•°æ®é©±åŠ¨æ–¹æ³•æ¥ä¿®æ”¹è®­ç»ƒæ•°æ®ã€‚å…¶ç›®æ ‡æ˜¯é€šè¿‡æœ€å°åŒ–åˆå§‹æ ‡è®°åˆ†å¸ƒçš„æ‰°åŠ¨ï¼Œä¿æŒæ¨¡å‹å›ºæœ‰çš„å®‰å…¨æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLookAhead Tuningèƒ½å¤Ÿæœ‰æ•ˆç»´æŠ¤æ¨¡å‹çš„å®‰å…¨æ€§ï¼ŒåŒæ—¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ä¿æŒå¼ºå¤§çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.18783",
            "title": "Frequency Dynamic Convolution for Dense Image Prediction",
            "url": "https://huggingface.co/papers/2503.18783",
            "abstract": "",
            "score": 2,
            "issue_id": 2897,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 25",
                "zh": "3æœˆ25æ—¥"
            },
            "hash": "0be08263b46c092e",
            "authors": [
                "Linwei Chen",
                "Lin Gu",
                "Liang Li",
                "Chenggang Yan",
                "Ying Fu"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "Chinese Academy of Sciences",
                "Hangzhou Dianzi University",
                "RIKEN",
                "The University of Tokyo",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.18783.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸ Ñ€ĞµÑÑƒÑ€ÑÑ‹, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¸Ñ… Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğº Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ LLM Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Hybrid Models: Bridging Spatial and Temporal Learning",
                    "desc": "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."
                },
                "zh": {
                    "title": "æå‡é¢„æµ‹å‡†ç¡®æ€§çš„åˆ›æ–°ç®—æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ”¹è¿›çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘æ•°æ®ç»´åº¦ï¼ŒåŒæ—¶ä¿ç•™é‡è¦ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚é€šè¿‡ä¼˜åŒ–æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼Œç ”ç©¶è€…å¸Œæœ›æ¨åŠ¨æœºå™¨å­¦ä¹ åœ¨å®é™…åº”ç”¨ä¸­çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19777",
            "title": "LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary\n  Semantic Segmentation",
            "url": "https://huggingface.co/papers/2503.19777",
            "abstract": "We propose a training-free method for open-vocabulary semantic segmentation using Vision-and-Language Models (VLMs). Our approach enhances the initial per-patch predictions of VLMs through label propagation, which jointly optimizes predictions by incorporating patch-to-patch relationships. Since VLMs are primarily optimized for cross-modal alignment and not for intra-modal similarity, we use a Vision Model (VM) that is observed to better capture these relationships. We address resolution limitations inherent to patch-based encoders by applying label propagation at the pixel level as a refinement step, significantly improving segmentation accuracy near class boundaries. Our method, called LPOSS+, performs inference over the entire image, avoiding window-based processing and thereby capturing contextual interactions across the full image. LPOSS+ achieves state-of-the-art performance among training-free methods, across a diverse set of datasets. Code: https://github.com/vladan-stojnic/LPOSS",
            "score": 1,
            "issue_id": 2905,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 25",
                "zh": "3æœˆ25æ—¥"
            },
            "hash": "adee7b684f0c25f1",
            "authors": [
                "Vladan StojniÄ‡",
                "Yannis Kalantidis",
                "JiÅ™Ã­ Matas",
                "Giorgos Tolias"
            ],
            "affiliations": [
                "NAVER LABS Europe",
                "VRG, FEE, Czech Technical University in Prague"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19777.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#inference",
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğº",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ VLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğº, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ñ‚Ñ‡Ğ°Ğ¼Ğ¸. Ğ”Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ (VM). ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ÑÑƒÑ‰Ğ¸Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing Semantic Segmentation with Training-Free Label Propagation",
                    "desc": "This paper introduces a novel method for open-vocabulary semantic segmentation that does not require training. The method leverages Vision-and-Language Models (VLMs) and enhances their predictions by using label propagation to optimize relationships between image patches. To improve accuracy, especially near class boundaries, the authors apply label propagation at the pixel level, addressing the limitations of patch-based encoders. The proposed method, LPOSS+, outperforms existing training-free approaches and effectively captures contextual interactions across the entire image."
                },
                "zh": {
                    "title": "æ— è®­ç»ƒçš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ–°æ–¹æ³•",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— è®­ç»ƒçš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰ä¸è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡æ ‡ç­¾ä¼ æ’­å¢å¼ºäº†VLMsçš„åˆå§‹æ¯ä¸ªè¡¥ä¸é¢„æµ‹ï¼Œä¼˜åŒ–äº†è¡¥ä¸ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬ä½¿ç”¨è§†è§‰æ¨¡å‹ï¼ˆVMï¼‰æ¥æ›´å¥½åœ°æ•æ‰è¿™äº›å…³ç³»ï¼Œå¹¶åœ¨åƒç´ çº§åˆ«åº”ç”¨æ ‡ç­¾ä¼ æ’­ï¼Œä»¥è§£å†³åŸºäºè¡¥ä¸ç¼–ç å™¨çš„åˆ†è¾¨ç‡é™åˆ¶ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†ç±»è¾¹ç•Œé™„è¿‘çš„åˆ†å‰²ç²¾åº¦ã€‚æˆ‘ä»¬çš„LPOSS+æ–¹æ³•åœ¨æ•´ä¸ªå›¾åƒä¸Šè¿›è¡Œæ¨ç†ï¼Œé¿å…äº†åŸºäºçª—å£çš„å¤„ç†ï¼Œèƒ½å¤Ÿæ•æ‰å…¨å›¾çš„ä¸Šä¸‹æ–‡äº¤äº’ï¼Œå¹¶åœ¨æ— è®­ç»ƒæ–¹æ³•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19123",
            "title": "Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided\n  Language Modeling",
            "url": "https://huggingface.co/papers/2503.19123",
            "abstract": "Using large teacher models to guide the training of smaller student models has become the prevailing paradigm for efficient and effective learning. However, vocabulary mismatches between teacher and student language models pose significant challenges in language modeling, resulting in divergent token sequences and output distributions. To overcome these limitations, we propose Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel approach that bridges the gap caused by vocabulary mismatch through two key methods: (1) Token-level Lexical Alignment, which aligns token sequences across mismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss of teacher model to guide effective student training. We demonstrate its effectiveness in language modeling with 1B student model using various 7B teacher models with different vocabularies. Notably, with Qwen2.5-Math-Instruct, a teacher model sharing only about 6% of its vocabulary with TinyLlama, VocAgnoLM achieves a 46% performance improvement compared to naive continual pretraining. Furthermore, we demonstrate that VocAgnoLM consistently benefits from stronger teacher models, providing a robust solution to vocabulary mismatches in language modeling.",
            "score": 1,
            "issue_id": 2907,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 24",
                "zh": "3æœˆ24æ—¥"
            },
            "hash": "b5efb0f50380f253",
            "authors": [
                "Haebin Shin",
                "Lei Ji",
                "Xiao Liu",
                "Yeyun Gong"
            ],
            "affiliations": [
                "KAIST AI",
                "Microsoft Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19123.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#optimization",
                    "#transfer_learning",
                    "#small_models",
                    "#training"
                ],
                "emoji": "ğŸ” ",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ° Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ VocAgnoLM. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¸ ÑƒÑ‡ĞµĞ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. VocAgnoLM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… ÑƒÑ‡ĞµĞ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging Vocabulary Gaps for Better Language Learning",
                    "desc": "This paper introduces a new method called Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM) to improve the training of smaller language models using larger teacher models. It addresses the problem of vocabulary mismatches that can lead to different token sequences and output distributions between the teacher and student models. VocAgnoLM employs two main techniques: Token-level Lexical Alignment to synchronize token sequences and Teacher Guided Loss to utilize the teacher's loss for better student training. The results show that this approach significantly enhances performance, achieving a 46% improvement in language modeling tasks, especially when using stronger teacher models."
                },
                "zh": {
                    "title": "æ‰“ç ´è¯æ±‡å£å’ï¼Œæå‡è¯­è¨€å»ºæ¨¡æ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼Œç§°ä¸ºVocabulary-agnostic Teacher Guided Language Modelingï¼ˆVocAgnoLMï¼‰ï¼Œæ—¨åœ¨è§£å†³æ•™å¸ˆæ¨¡å‹ä¸å­¦ç”Ÿæ¨¡å‹ä¹‹é—´çš„è¯æ±‡ä¸åŒ¹é…é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸¤ç§å…³é”®æŠ€æœ¯å®ç°ï¼šä¸€æ˜¯ä»¤ç‰Œçº§è¯æ±‡å¯¹é½ï¼ŒäºŒæ˜¯æ•™å¸ˆå¼•å¯¼æŸå¤±ï¼Œå¸®åŠ©å­¦ç”Ÿæ¨¡å‹æ›´æœ‰æ•ˆåœ°å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒVocAgnoLMåœ¨ä½¿ç”¨ä¸åŒè¯æ±‡çš„æ•™å¸ˆæ¨¡å‹æ—¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨è¯æ±‡é‡å è¾ƒå°‘çš„æƒ…å†µä¸‹ã€‚è¯¥æ–¹æ³•ä¸ºè¯­è¨€å»ºæ¨¡ä¸­çš„è¯æ±‡ä¸åŒ¹é…é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17361",
            "title": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation",
            "url": "https://huggingface.co/papers/2503.17361",
            "abstract": "Flow matching in the continuous simplex has emerged as a promising strategy for DNA sequence design, but struggles to scale to higher simplex dimensions required for peptide and protein generation. We introduce Gumbel-Softmax Flow and Score Matching, a generative framework on the simplex based on a novel Gumbel-Softmax interpolant with a time-dependent temperature. Using this interpolant, we introduce Gumbel-Softmax Flow Matching by deriving a parameterized velocity field that transports from smooth categorical distributions to distributions concentrated at a single vertex of the simplex. We alternatively present Gumbel-Softmax Score Matching which learns to regress the gradient of the probability density. Our framework enables high-quality, diverse generation and scales efficiently to higher-dimensional simplices. To enable training-free guidance, we propose Straight-Through Guided Flows (STGFlow), a classifier-based guidance method that leverages straight-through estimators to steer the unconditional velocity field toward optimal vertices of the simplex. STGFlow enables efficient inference-time guidance using classifiers pre-trained on clean sequences, and can be used with any discrete flow method. Together, these components form a robust framework for controllable de novo sequence generation. We demonstrate state-of-the-art performance in conditional DNA promoter design, sequence-only protein generation, and target-binding peptide design for rare disease treatment.",
            "score": 1,
            "issue_id": 2896,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 21",
                "zh": "3æœˆ21æ—¥"
            },
            "hash": "b5389a3e5ab241c3",
            "authors": [
                "Sophia Tang",
                "Yinuo Zhang",
                "Alexander Tong",
                "Pranam Chatterjee"
            ],
            "affiliations": [
                "Center of Computational Biology, Duke-NUS Medical School",
                "Department of Biomedical Engineering, Duke University",
                "Department of Biostatistics and Bioinformatics, Duke University",
                "Department of Computer Science, Duke University",
                "Management and Technology Program, University of Pennsylvania",
                "Mila, Quebec AI Institute",
                "UniversitÃ© de MontrÃ©al"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17361.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#data",
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#healthcare"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° ÑĞ¸Ğ¼Ğ¿Ğ»ĞµĞºÑĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑĞ¸Ğ¼Ğ¿Ğ»ĞµĞºÑĞµ, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Gumbel-Softmax Flow and Score Matching. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑĞ½Ñ‚ Gumbel-Softmax Ñ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ĞµĞ¹ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞµĞ³Ğ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° ÑĞ¸Ğ¼Ğ¿Ğ»ĞµĞºÑÑ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Straight-Through Guided Flows Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Sequence Generation with Gumbel-Softmax Flows",
                    "desc": "This paper presents a new method called Gumbel-Softmax Flow and Score Matching for generating DNA sequences, peptides, and proteins. It introduces a Gumbel-Softmax interpolant that helps in transitioning between smooth categorical distributions and specific points in a simplex, which is a mathematical space used for these types of data. The authors also propose a technique called Straight-Through Guided Flows (STGFlow) that uses classifiers to guide the generation process towards optimal outcomes without needing extensive training. Overall, this framework allows for efficient and high-quality generation of biological sequences, achieving impressive results in various applications."
                },
                "zh": {
                    "title": "é«˜æ•ˆç”Ÿæˆé«˜ç»´åºåˆ—çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆæ¡†æ¶ï¼Œç§°ä¸ºGumbel-Softmaxæµå’Œè¯„åˆ†åŒ¹é…ï¼Œæ—¨åœ¨è§£å†³DNAåºåˆ—è®¾è®¡ä¸­çš„é«˜ç»´ç®€å•å½¢é—®é¢˜ã€‚é€šè¿‡å¼•å…¥æ—¶é—´ä¾èµ–çš„Gumbel-Softmaxæ’å€¼ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨ç®€å•å½¢ä¸Šå®ç°é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„ç”Ÿæˆã€‚è¯¥æ¡†æ¶è¿˜åŒ…æ‹¬ä¸€ç§åä¸ºSTGFlowçš„åˆ†ç±»å™¨å¼•å¯¼æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†æ—¶æœ‰æ•ˆåœ°å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨æ¡ä»¶DNAå¯åŠ¨å­è®¾è®¡ã€åºåˆ—ç”Ÿæˆçš„è›‹ç™½è´¨å’Œé¶å‘ç»“åˆè‚½çš„è®¾è®¡ä¸­å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17237",
            "title": "Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID",
            "url": "https://huggingface.co/papers/2503.17237",
            "abstract": "Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal infrared video is inherently challenging due to low contrast, environmental noise, and small target sizes. This paper provides a straightforward approach to address multi-UAV tracking in thermal infrared video, leveraging recent advances in detection and tracking. Instead of relying on the YOLOv5 with the DeepSORT pipeline, we present a tracking framework built on YOLOv12 and BoT-SORT, enhanced with tailored training and inference strategies. We evaluate our approach following the metrics from the 4th Anti-UAV Challenge and demonstrate competitive performance. Notably, we achieve strong results without using contrast enhancement or temporal information fusion to enrich UAV features, highlighting our approach as a \"Strong Baseline\" for the multi-UAV tracking task. We provide implementation details, in-depth experimental analysis, and a discussion of potential improvements. The code is available at https://github.com/wish44165/YOLOv12-BoT-SORT-ReID .",
            "score": 1,
            "issue_id": 2900,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 21",
                "zh": "3æœˆ21æ—¥"
            },
            "hash": "03b184a7ba5377e7",
            "authors": [
                "Yu-Hsi Chen"
            ],
            "affiliations": [
                "The University of Melbourne, Parkville, Australia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17237.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸš",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ‘ĞŸĞ›Ğ Ğ½Ğ° Ñ‚ĞµĞ¿Ğ»Ğ¾Ğ²Ğ¸Ğ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ‘ĞŸĞ›Ğ Ğ½Ğ° Ğ¸Ğ½Ñ„Ñ€Ğ°ĞºÑ€Ğ°ÑĞ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ YOLOv12 Ğ¸ BoT-SORT. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ĞºÑƒ YOLOv5 Ğ¸ DeepSORT, Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ±ĞµĞ³Ğ°Ñ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ° Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ñ… 4-Ğ³Ğ¾ Anti-UAV Challenge. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing UAV Tracking with YOLOv12 and BoT-SORT",
                    "desc": "This paper addresses the challenge of detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal infrared video, which is difficult due to low visibility and small sizes of the targets. The authors propose a new tracking framework that utilizes YOLOv12 and BoT-SORT, moving away from the traditional YOLOv5 and DeepSORT methods. Their approach includes specific training and inference strategies that enhance performance without relying on contrast enhancement or temporal information. The results show that their method performs competitively in the 4th Anti-UAV Challenge, establishing it as a strong baseline for future multi-UAV tracking research."
                },
                "zh": {
                    "title": "çƒ­çº¢å¤–è§†é¢‘ä¸­çš„å¤šæ— äººæœºè·Ÿè¸ªæ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡é’ˆå¯¹çƒ­çº¢å¤–è§†é¢‘ä¸­å¤šæ— äººæœºï¼ˆUAVï¼‰çš„æ£€æµ‹ä¸è·Ÿè¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ã€‚æˆ‘ä»¬é‡‡ç”¨äº†YOLOv12å’ŒBoT-SORTæ„å»ºè·Ÿè¸ªæ¡†æ¶ï¼Œå¹¶é€šè¿‡å®šåˆ¶çš„è®­ç»ƒå’Œæ¨ç†ç­–ç•¥è¿›è¡Œå¢å¼ºã€‚æˆ‘ä»¬çš„è¯„ä¼°åŸºäºç¬¬å››å±Šåæ— äººæœºæŒ‘æˆ˜èµ›çš„æŒ‡æ ‡ï¼Œç»“æœæ˜¾ç¤ºå‡ºç«äº‰åŠ›çš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨ä¸ä½¿ç”¨å¯¹æ¯”åº¦å¢å¼ºæˆ–æ—¶é—´ä¿¡æ¯èåˆçš„æƒ…å†µä¸‹ï¼Œä¾ç„¶å–å¾—äº†è‰¯å¥½çš„ç»“æœï¼Œæ ‡å¿—ç€æˆ‘ä»¬çš„æ–¹æ³•æ˜¯å¤šæ— äººæœºè·Ÿè¸ªä»»åŠ¡çš„â€œå¼ºåŸºçº¿â€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16965",
            "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making",
            "url": "https://huggingface.co/papers/2503.16965",
            "abstract": "Embodied decision-making is fundamental for AI agents operating in real-world environments. While Visual Language Models (VLMs) have advanced this capability, they still struggle with complex decisions, particularly in human-centered situations that require deep reasoning about human needs and values. In this study, we systematically evaluate open-sourced VLMs on multimodal human-centered decision-making tasks. We find that LLMs receiving only textual descriptions unexpectedly outperform their VLM counterparts of similar scale that process actual images, suggesting that visual alignment may hinder VLM abilities. To address this challenge, we propose a novel text-only training approach with synthesized textual data. This method strengthens VLMs' language components and transfers the learned abilities to multimodal inference, eliminating the need for expensive image-text paired data. Furthermore, we show that VLMs can achieve substantial performance gains through self-improvement, using training data generated by their LLM counterparts rather than relying on larger teacher models like GPT-4. Our findings establish a more efficient and scalable approach to enhancing VLMs' human-centered decision-making capabilities, opening new avenues for optimizing VLMs through self-improvement mechanisms.",
            "score": 1,
            "issue_id": 2896,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 21",
                "zh": "3æœˆ21æ—¥"
            },
            "hash": "ca2e599ff0665dfe",
            "authors": [
                "Zhe Hu",
                "Jing Li",
                "Yu Yin"
            ],
            "affiliations": [
                "Department of Computer and Data Sciences, Case Western Reserve University",
                "Department of Computing, The Hong Kong Polytechnic University",
                "Research Centre for Data Science & Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16965.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#training",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ VLM Ñ‡ĞµÑ€ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ¾ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ VLM Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ VLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ²Ğ¾Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ñ… LLM-Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing VLMs through Text-Only Training and Self-Improvement",
                    "desc": "This paper explores the challenges faced by Visual Language Models (VLMs) in making complex decisions that involve understanding human needs and values. The authors find that VLMs that rely on visual data often perform worse than those using only text, indicating that visual information may complicate decision-making. To improve VLM performance, they propose a new training method that uses synthesized textual data, enhancing the language understanding of VLMs without needing paired image-text data. Additionally, the study demonstrates that VLMs can improve their decision-making abilities by learning from their own generated data, rather than depending on larger models, making the training process more efficient and scalable."
                },
                "zh": {
                    "title": "æå‡VLMäººç±»ä¸­å¿ƒå†³ç­–èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤æ‚äººç±»ä¸­å¿ƒå†³ç­–ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»…ä½¿ç”¨æ–‡æœ¬æè¿°çš„è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŸäº›ä»»åŠ¡ä¸Šæ„å¤–åœ°è¶…è¶Šäº†å¤„ç†å›¾åƒçš„VLMsï¼Œè¿™è¡¨æ˜è§†è§‰å¯¹é½å¯èƒ½ä¼šé™åˆ¶VLMçš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä»…åŸºäºæ–‡æœ¬çš„è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨åˆæˆæ–‡æœ¬æ•°æ®æ¥å¢å¼ºVLMçš„è¯­è¨€èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡è‡ªæˆ‘æ”¹è¿›ï¼ŒVLMså¯ä»¥æ˜¾è‘—æå‡å…¶äººç±»ä¸­å¿ƒå†³ç­–èƒ½åŠ›ï¼Œå¼€è¾Ÿäº†ä¼˜åŒ–VLMçš„æ–°é€”å¾„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19207",
            "title": "FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from\n  Few Images",
            "url": "https://huggingface.co/papers/2503.19207",
            "abstract": "We present a novel method for reconstructing personalized 3D human avatars with realistic animation from only a few images. Due to the large variations in body shapes, poses, and cloth types, existing methods mostly require hours of per-subject optimization during inference, which limits their practical applications. In contrast, we learn a universal prior from over a thousand clothed humans to achieve instant feedforward generation and zero-shot generalization. Specifically, instead of rigging the avatar with shared skinning weights, we jointly infer personalized avatar shape, skinning weights, and pose-dependent deformations, which effectively improves overall geometric fidelity and reduces deformation artifacts. Moreover, to normalize pose variations and resolve coupled ambiguity between canonical shapes and skinning weights, we design a 3D canonicalization process to produce pixel-aligned initial conditions, which helps to reconstruct fine-grained geometric details. We then propose a multi-frame feature aggregation to robustly reduce artifacts introduced in canonicalization and fuse a plausible avatar preserving person-specific identities. Finally, we train the model in an end-to-end framework on a large-scale capture dataset, which contains diverse human subjects paired with high-quality 3D scans. Extensive experiments show that our method generates more authentic reconstruction and animation than state-of-the-arts, and can be directly generalized to inputs from casually taken phone photos. Project page and code is available at https://github.com/rongakowang/FRESA.",
            "score": 0,
            "issue_id": 2907,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 24",
                "zh": "3æœˆ24æ—¥"
            },
            "hash": "051b6e95d11816f7",
            "authors": [
                "Rong Wang",
                "Fabian Prada",
                "Ziyan Wang",
                "Zhongshi Jiang",
                "Chengxiang Yin",
                "Junxuan Li",
                "Shunsuke Saito",
                "Igor Santesteban",
                "Javier Romero",
                "Rohan Joshi",
                "Hongdong Li",
                "Jason Saragih",
                "Yaser Sheikh"
            ],
            "affiliations": [
                "Australian National University",
                "Meta Reality Labs Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19207.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ»ÑĞ´ĞµĞ¹ Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ñ‚Ñ‹ÑÑÑ‡Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¾Ğ´ĞµÑ‚Ñ‹Ñ… Ğ»ÑĞ´ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñ‹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ°, Ğ²ĞµÑĞ° ÑĞºĞ¸Ğ½Ğ½Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ·Ñ‹ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ 3D-ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ»ĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Instant 3D Avatars from Few Images!",
                    "desc": "This paper introduces a new technique for creating personalized 3D human avatars with realistic animations using just a few images. Unlike previous methods that require extensive optimization for each individual, this approach leverages a universal model learned from a large dataset of clothed humans, allowing for quick and efficient avatar generation. The method improves the accuracy of the avatar's shape and movement by jointly inferring skinning weights and pose-dependent deformations, which minimizes visual artifacts. Additionally, a 3D canonicalization process is implemented to align poses and enhance geometric details, resulting in high-quality reconstructions that can be generated from casual photos."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–3Då¤´åƒé‡å»ºçš„æ–°æ–¹æ³•",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå¯ä»¥ä»…é€šè¿‡å‡ å¼ å›¾ç‰‡é‡å»ºä¸ªæ€§åŒ–çš„3Däººç±»å¤´åƒï¼Œå¹¶å®ç°é€¼çœŸçš„åŠ¨ç”»ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹æ¯ä¸ªå¯¹è±¡è¿›è¡Œæ•°å°æ—¶çš„ä¼˜åŒ–ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å®é™…åº”ç”¨ã€‚æˆ‘ä»¬çš„æŠ€æœ¯é€šè¿‡å­¦ä¹ æ¥è‡ªä¸€åƒå¤šåç©¿è¡£äººç±»çš„é€šç”¨å…ˆéªŒï¼Œå®ç°äº†å³æ—¶å‰é¦ˆç”Ÿæˆå’Œé›¶æ ·æœ¬æ³›åŒ–ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§3Dæ ‡å‡†åŒ–è¿‡ç¨‹ï¼Œä»¥è§£å†³å§¿åŠ¿å˜åŒ–å’Œå½¢çŠ¶ä¹‹é—´çš„æ¨¡ç³Šæ€§ï¼Œä»è€Œæé«˜å‡ ä½•ç²¾åº¦å¹¶å‡å°‘å˜å½¢ä¼ªå½±ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11849",
            "title": "Towards a Unified Copernicus Foundation Model for Earth Vision",
            "url": "https://huggingface.co/papers/2503.11849",
            "abstract": "Advances in Earth observation (EO) foundation models have unlocked the potential of big satellite data to learn generic representations from space, benefiting a wide range of downstream applications crucial to our planet. However, most existing efforts remain limited to fixed spectral sensors, focus solely on the Earth's surface, and overlook valuable metadata beyond imagery. In this work, we take a step towards next-generation EO foundation models with three key components: 1) Copernicus-Pretrain, a massive-scale pretraining dataset that integrates 18.7M aligned images from all major Copernicus Sentinel missions, spanning from the Earth's surface to its atmosphere; 2) Copernicus-FM, a unified foundation model capable of processing any spectral or non-spectral sensor modality using extended dynamic hypernetworks and flexible metadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark with 15 hierarchical downstream tasks ranging from preprocessing to specialized applications for each Sentinel mission. Our dataset, model, and benchmark greatly improve the scalability, versatility, and multimodal adaptability of EO foundation models, while also creating new opportunities to connect EO, weather, and climate research. Codes, datasets and models are available at https://github.com/zhu-xlab/Copernicus-FM.",
            "score": 0,
            "issue_id": 2904,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "73a149c0c20956cf",
            "authors": [
                "Yi Wang",
                "Zhitong Xiong",
                "Chenying Liu",
                "Adam J. Stewart",
                "Thomas Dujardin",
                "Nikolaos Ioannis Bountos",
                "Angelos Zavras",
                "Franziska Gerken",
                "Ioannis Papoutsis",
                "Laura Leal-TaixÃ©",
                "Xiao Xiang Zhu"
            ],
            "affiliations": [
                "Harokopio University of Athens",
                "Munich Center for Machine Learning",
                "NVIDIA",
                "National Technical University of Athens & National Observatory of Athens",
                "Technical University of Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11849.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "ğŸ›°ï¸",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ—ĞµĞ¼Ğ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Copernicus-FM - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚ĞµĞ¹. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Copernicus-Pretrain, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 18,7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¾ ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¼Ğ¸ÑÑĞ¸Ğ¸ Copernicus Sentinel. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Copernicus-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 15 Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Unlocking Earth Observation with Advanced Foundation Models",
                    "desc": "This paper presents advancements in Earth observation (EO) foundation models that utilize large satellite datasets to enhance learning from space imagery. The authors introduce Copernicus-Pretrain, a comprehensive dataset with 18.7 million aligned images from various Copernicus Sentinel missions, which includes data from both the Earth's surface and atmosphere. They also propose Copernicus-FM, a versatile foundation model that can handle different types of sensor data and incorporates metadata for improved analysis. Finally, the paper outlines Copernicus-Bench, a benchmark for evaluating the model's performance across 15 diverse tasks, thereby enhancing the scalability and adaptability of EO applications."
                },
                "zh": {
                    "title": "åœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹çš„æœªæ¥ï¼šå¤šæ¨¡æ€ä¸å¯æ‰©å±•æ€§",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†åœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹çš„è¿›å±•ï¼Œåˆ©ç”¨å¤§è§„æ¨¡å«æ˜Ÿæ•°æ®å­¦ä¹ é€šç”¨è¡¨ç¤ºï¼Œä¿ƒè¿›äº†å¤šç§é‡è¦åº”ç”¨çš„å‘å±•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šCopernicus-Pretrainï¼Œä¸€ä¸ªåŒ…å«1870ä¸‡å¯¹é½å›¾åƒçš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†ï¼Œæ¶µç›–åœ°çƒè¡¨é¢åˆ°å¤§æ°”å±‚çš„æ‰€æœ‰ä¸»è¦Copernicus Sentinelä»»åŠ¡ï¼›Copernicus-FMï¼Œä¸€ä¸ªç»Ÿä¸€çš„åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†ä»»ä½•å…‰è°±æˆ–éå…‰è°±ä¼ æ„Ÿå™¨çš„æ¨¡æ€ï¼Œå¹¶ä½¿ç”¨æ‰©å±•çš„åŠ¨æ€è¶…ç½‘ç»œå’Œçµæ´»çš„å…ƒæ•°æ®ç¼–ç ï¼›ä»¥åŠCopernicus-Benchï¼Œä¸€ä¸ªç³»ç»Ÿçš„è¯„ä¼°åŸºå‡†ï¼ŒåŒ…å«15ä¸ªå±‚æ¬¡çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œä»é¢„å¤„ç†åˆ°æ¯ä¸ªSentinelä»»åŠ¡çš„ä¸“ä¸šåº”ç”¨ã€‚æˆ‘ä»¬çš„å·¥ä½œæ˜¾è‘—æé«˜äº†åœ°çƒè§‚æµ‹åŸºç¡€æ¨¡å‹çš„å¯æ‰©å±•æ€§ã€å¤šåŠŸèƒ½æ€§å’Œå¤šæ¨¡æ€é€‚åº”æ€§ï¼ŒåŒæ—¶ä¸ºè¿æ¥åœ°çƒè§‚æµ‹ã€å¤©æ°”å’Œæ°”å€™ç ”ç©¶åˆ›é€ äº†æ–°çš„æœºä¼šã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-25.html",
    "link_next": "2025-03-27.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "25.03",
        "en": "03/25",
        "zh": "3æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "27.03",
        "en": "03/27",
        "zh": "3æœˆ27æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 9,
        "#agents": 2,
        "#cv": 7,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 2,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 9,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 12,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 10,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†é•¿ä¸Šä¸‹æ–‡è‡ªå›å½’æ¨¡å‹åœ¨è¯­è¨€ç”Ÿæˆä¸­çš„è¿›æ­¥ï¼Œä½†åœ¨è§†é¢‘ç”Ÿæˆä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä½œè€…æå‡ºäº†Frame AutoRegressive (FAR)ï¼Œä¸€ç§ç”¨äºè§†é¢‘è‡ªå›å½’å»ºæ¨¡çš„å¼ºå¤§åŸºå‡†ã€‚FARé€šè¿‡å»ºæ¨¡è¿ç»­å¸§ä¹‹é—´çš„æ—¶é—´å› æœä¾èµ–æ€§ï¼Œå®ç°äº†æ¯”Token ARå’Œè§†é¢‘æ‰©æ•£å˜å‹å™¨æ›´å¥½çš„æ”¶æ•›ã€‚ç„¶è€Œï¼Œé•¿ä¸Šä¸‹æ–‡è§†è§‰å»ºæ¨¡é¢ä¸´è§†è§‰å†—ä½™å’Œè®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†FlexRoPEå’Œé•¿çŸ­æœŸä¸Šä¸‹æ–‡å»ºæ¨¡æŠ€æœ¯ï¼Œä½¿å¾—åœ¨é•¿è§†é¢‘åºåˆ—ä¸Šçš„è®­ç»ƒæ›´åŠ é«˜æ•ˆï¼Œå¹¶åœ¨çŸ­é•¿è§†é¢‘ç”Ÿæˆä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚",
        "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹nle chÃ¡ng shÃ ngxiÃ wÃ©n zÃ¬huÃ­guÄ« mÃ³xÃ­ng zÃ i yÇ”yÃ¡n shÄ“ngchÃ©ng zhÅng de jÃ¬nbÃ¹, dÃ n zÃ i shÃ¬pÇn shÄ“ngchÃ©ng zhÅng rÃ©ng miÃ nliÃ o tiÇozhÃ n. ZuÃ²zhÄ› tÃ­chÅ«le Frame AutoRegressive (FAR), yÄ«zhÇ’ng yÃ²ngyÃº shÃ¬pÇn zÃ¬huÃ­guÄ« jiÃ nmÃ³ de qiÃ¡ngdÃ  jÄ«zhÇ”n. FAR tÅngguÃ² jiÃ nmÃ³ liÃ¡nxÃ¹ zhÄ“n jiÄn de shÃ­jiÄn yÄ«nguÇ’ yÄ«lÃ ixÃ¬ng, shÃ­xiÃ nle bÇ Token AR hÃ© shÃ¬pÇn kuÃ²sÃ n biÃ nshÅ«qÃ¬ gÃ¨ng hÇo de shÅulÇan. RÃ¡n'Ã©r, chÃ¡ng shÃ ngxiÃ wÃ©n shÃ¬jÃ¹e jiÃ nmÃ³ miÃ nliÃ o shÃ¬jÃ¹e rÇ’ngyÃº hÃ© jÃ¬suÃ n chÃ©ngbÄ›n gÄo de wÃ¨ntÃ­. WÃ¨i jiÄ›juÃ© zhÃ¨xiÄ“ wÃ¨ntÃ­, zuÃ²zhÄ› tÃ­chÅ«le FlexRoPE hÃ© chÃ¡ngduÇnqÄ« shÃ ngxiÃ wÃ©n jiÃ nmÃ³ jÃ¬shÃ¹, shÇdÃ© zÃ i chÃ¡ng shÃ¬pÇn xÃ¹liÃ¨ shÃ ng de xÃ¹nliÃ n gÃ¨ngjiÄ gÄoxiÃ o, bÃ¬ng zÃ i duÇn chÃ¡ng shÃ¬pÇn shÄ“ngchÃ©ng zhÅng qudÃ©le zuÃ¬jiÄ xÃ¬ngnÃ©ng.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"é•¿ä¸Šä¸‹æ–‡\", \"pinyin\": \"chÃ¡ng shÃ ng xiÃ  wÃ©n\", \"trans\": \"long context\"},\n    {\"word\": \"è‡ªå›å½’\", \"pinyin\": \"zÃ¬ huÃ­ guÄ«\", \"trans\": \"autoregressive\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³ xÃ­ng\", \"trans\": \"model\"},\n    {\"word\": \"è¿›æ­¥\", \"pinyin\": \"jÃ¬n bÃ¹\", \"trans\": \"progress\"},\n    {\"word\": \"é¢ä¸´\", \"pinyin\": \"miÃ n lÃ­n\", \"trans\": \"face\"},\n    {\"word\": \"æŒ‘æˆ˜\", \"pinyin\": \"tiÇo zhÃ n\", \"trans\": \"challenge\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"Frame\", \"pinyin\": \"Frame\", \"trans\": \"Frame\"},\n    {\"word\": \"AutoRegressive\", \"pinyin\": \"AutoRegressive\", \"trans\": \"AutoRegressive\"},\n    {\"word\": \"FAR\", \"pinyin\": \"FAR\", \"trans\": \"FAR\"},\n    {\"word\": \"ç”¨äº\", \"pinyin\": \"yÃ²ng yÃº\", \"trans\": \"used for\"},\n    {\"word\": \"è§†é¢‘\", \"pinyin\": \"shÃ¬ pÃ­n\", \"trans\": \"video\"},\n    {\"word\": \"è‡ªå›å½’å»ºæ¨¡\", \"pinyin\": \"zÃ¬ huÃ­ guÄ« jiÃ n mÃ³\", \"trans\": \"autoregressive modeling\"},\n    {\"word\": \"å¼ºå¤§\", \"pinyin\": \"qiÃ¡ng dÃ \", \"trans\": \"powerful\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"é€šè¿‡\", \"pinyin\": \"tÅng guÃ²\", \"trans\": \"through\"},\n    {\"word\": \"å»ºæ¨¡\", \"pinyin\": \"jiÃ n mÃ³\", \"trans\": \"modeling\"},\n    {\"word\": \"è¿ç»­\", \"pinyin\": \"liÃ¡n xÃ¹\", \"trans\": \"continuous\"},\n    {\"word\": \"å¸§\", \"pinyin\": \"zhÃ¨n\", \"trans\": \"frame\"},\n    {\"word\": \"ä¹‹é—´\", \"pinyin\": \"zhÄ« jiÄn\", \"trans\": \"between\"},\n    {\"word\": \"æ—¶é—´\", \"pinyin\": \"shÃ­ jiÄn\", \"trans\": \"time\"},\n    {\"word\": \"å› æœ\", \"pinyin\": \"yÄ«n guÇ’\", \"trans\": \"causal\"},\n    {\"word\": \"ä¾èµ–æ€§\", \"pinyin\": \"yÄ« lÃ i xÃ¬ng\", \"trans\": \"dependency\"},\n    {\"word\": \"å®ç°\", \"pinyin\": \"shÃ­ xiÃ n\", \"trans\": \"achieve\"},\n    {\"word\": \"æ”¶æ•›\", \"pinyin\": \"shÅu liÇn\", \"trans\": \"convergence\"},\n    {\"word\": \"Token\", \"pinyin\": \"Token\", \"trans\": \"Token\"},\n    {\"word\": \"AR\", \"pinyin\": \"AR\", \"trans\": \"AR\"},\n    {\"word\": \"è§†é¢‘æ‰©æ•£å˜å‹å™¨\", \"pinyin\": \"shÃ¬ pÃ­n kuÃ² sÃ n biÃ n yÄ qÃ¬\", \"trans\": \"video diffusion transformer\"},\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬ juÃ©\", \"trans\": \"visual\"},\n    {\"word\": \"å†—ä½™\", \"pinyin\": \"rÃ³ng yÃº\", \"trans\": \"redundancy\"},\n    {\"word\": \"è®¡ç®—\", \"pinyin\": \"jÃ¬ suÃ n\", \"trans\": \"computation\"},\n    {\"word\": \"æˆæœ¬\", \"pinyin\": \"chÃ©ng bÄ›n\", \"trans\": \"cost\"},\n    {\"word\": \"è§£å†³\", \"pinyin\": \"jiÄ› juÃ©\", \"trans\": \"solve\"},\n    {\"word\": \"FlexRoPE\", \"pinyin\": \"FlexRoPE\", \"trans\": \"FlexRoPE\"},\n    {\"word\": \"é•¿çŸ­æœŸ\", \"pinyin\": \"chÃ¡ng duÇn qÄ«\", \"trans\": \"long short-term\"},\n    {\"word\": \"ä¸Šä¸‹æ–‡å»ºæ¨¡æŠ€æœ¯\", \"pinyin\": \"shÃ ng xiÃ  wÃ©n jiÃ n mÃ³ jÃ¬ shÃ¹\", \"trans\": \"context modeling techniques\"},\n    {\"word\": \"ä½¿å¾—\", \"pinyin\": \"shÇ dÃ©\", \"trans\": \"make\"},\n    {\"word\": \"é«˜æ•ˆ\", \"pinyin\": \"gÄo xiÃ o\", \"trans\": \"efficient\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹n liÃ n\", \"trans\": \"training\"},\n    {\"word\": \"åºåˆ—\", \"pinyin\": \"xÃ¹ liÃ¨\", \"trans\": \"sequence\"},\n    {\"word\": \"å–å¾—\", \"pinyin\": \"qÇ” dÃ©\", \"trans\": \"achieve\"},\n    {\"word\": \"æœ€ä½³\", \"pinyin\": \"zuÃ¬ jiÄ\", \"trans\": \"optimal\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"}\n]",
        "trans": "This article discusses the advancements of long-context autoregressive models in language generation but highlights the challenges they still face in video generation. The authors introduce Frame AutoRegressive (FAR), a powerful benchmark for autoregressive modeling in video. FAR achieves better convergence than Token AR and Video Diffusion Transformers by modeling the temporal causal dependencies between consecutive frames. However, long-context visual modeling faces issues of visual redundancy and high computational costs. To address these problems, the authors propose FlexRoPE and long-short term context modeling techniques, making training on long video sequences more efficient and achieving state-of-the-art performance in both short and long video generation.",
        "update_ts": "2025-03-26 09:12"
    }
}