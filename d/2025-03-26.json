{
    "date": {
        "ru": "26 марта",
        "en": "March 26",
        "zh": "3月26日"
    },
    "time_utc": "2025-03-26 02:20",
    "weekday": 2,
    "issue_id": 2896,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.19385",
            "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing",
            "url": "https://huggingface.co/papers/2503.19385",
            "abstract": "We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion models--offering faster generation and high-quality outputs in state-of-the-art image and video generative models--efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches.",
            "score": 3,
            "issue_id": 2896,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 марта",
                "en": "March 25",
                "zh": "3月25日"
            },
            "hash": "e0ead8fbe973f326",
            "authors": [
                "Jaihoon Kim",
                "Taehoon Yoon",
                "Jisung Hwang",
                "Minhyuk Sung"
            ],
            "affiliations": [
                "KAIST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19385.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#diffusion",
                    "#optimization",
                    "#video"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "Эффективное масштабирование потоковых моделей при выводе",
                    "desc": "Статья предлагает метод масштабирования во время вывода для предобученных потоковых моделей. Авторы вводят три ключевые идеи: генерация на основе стохастических дифференциальных уравнений (SDE), преобразование интерполянтов и адаптивное распределение вычислительных ресурсов. Эксперименты показывают, что генерация на основе SDE, особенно с сохранением дисперсии, улучшает производительность методов выборки частиц для масштабирования во время вывода в потоковых моделях. Предложенный подход превосходит предыдущие методы масштабирования во время вывода."
                },
                "en": {
                    "title": "Enhancing Flow Models with Efficient Inference-Time Scaling",
                    "desc": "This paper introduces a new method for improving flow models during inference, which is the process of generating outputs from trained models. The authors focus on three innovative techniques: using stochastic differential equations (SDE) for particle sampling, converting interpolants to increase diversity in generated samples, and implementing Rollover Budget Forcing (RBF) to optimize computational resource allocation. These methods allow flow models to achieve better sample quality and efficiency, similar to advancements seen in diffusion models. The results show that their approach significantly enhances performance, making flow models more competitive in generating high-quality images and videos."
                },
                "zh": {
                    "title": "流模型的高效推理时间缩放新方法",
                    "desc": "本文提出了一种针对预训练流模型的推理时间缩放方法。近年来，推理时间缩放在大语言模型和扩散模型中受到广泛关注，通过利用额外的计算来提高样本质量或更好地符合用户偏好。尽管流模型作为扩散模型的替代方案越来越受欢迎，但由于其确定性生成过程，现有的扩散模型推理时间缩放方法无法直接应用于流模型。我们提出了三种关键思想，以实现流模型的高效推理时间缩放：基于SDE的生成、插值转换和自适应计算资源分配。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19325",
            "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
            "url": "https://huggingface.co/papers/2503.19325",
            "abstract": "Long-context autoregressive modeling has significantly advanced language generation, but video generation still struggles to fully utilize extended temporal contexts. To investigate long-context video modeling, we introduce Frame AutoRegressive (FAR), a strong baseline for video autoregressive modeling. Just as language models learn causal dependencies between tokens (i.e., Token AR), FAR models temporal causal dependencies between continuous frames, achieving better convergence than Token AR and video diffusion transformers. Building on FAR, we observe that long-context vision modeling faces challenges due to visual redundancy. Existing RoPE lacks effective temporal decay for remote context and fails to extrapolate well to long video sequences. Additionally, training on long videos is computationally expensive, as vision tokens grow much faster than language tokens. To tackle these issues, we propose balancing locality and long-range dependency. We introduce FlexRoPE, an test-time technique that adds flexible temporal decay to RoPE, enabling extrapolation to 16x longer vision contexts. Furthermore, we propose long short-term context modeling, where a high-resolution short-term context window ensures fine-grained temporal consistency, while an unlimited long-term context window encodes long-range information using fewer tokens. With this approach, we can train on long video sequences with a manageable token context length. We demonstrate that FAR achieves state-of-the-art performance in both short- and long-video generation, providing a simple yet effective baseline for video autoregressive modeling.",
            "score": 3,
            "issue_id": 2896,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 марта",
                "en": "March 25",
                "zh": "3月25日"
            },
            "hash": "543c7dbfad83ed73",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#long_context",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "FAR: эффективное моделирование длинных видеопоследовательностей",
                    "desc": "Статья представляет Frame AutoRegressive (FAR) - новый подход к авторегрессионному моделированию видео с длинным контекстом. Авторы вводят FlexRoPE - технику, позволяющую экстраполировать модель на контексты в 16 раз длиннее обучающих. Предлагается комбинировать моделирование краткосрочного и долгосрочного контекста для эффективной обработки длинных видеопоследовательностей. FAR демонстрирует наилучшие результаты в генерации как коротких, так и длинных видео."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Frame AutoRegressive Modeling",
                    "desc": "This paper presents Frame AutoRegressive (FAR), a new method for generating videos by modeling the temporal dependencies between frames. FAR improves upon existing models by addressing the challenges of visual redundancy and the limitations of current positional encoding techniques like RoPE. The authors introduce FlexRoPE, which allows for flexible temporal decay, enabling the model to handle longer video sequences effectively. By combining short-term and long-term context modeling, FAR achieves state-of-the-art results in video generation while maintaining computational efficiency."
                },
                "zh": {
                    "title": "长时间上下文视频生成的新突破",
                    "desc": "本文介绍了一种新的视频自回归建模方法，称为Frame AutoRegressive (FAR)，旨在解决长时间上下文视频生成中的挑战。FAR通过建模连续帧之间的时间因果关系，超越了传统的语言模型，取得了更好的收敛效果。为了应对视觉冗余和计算成本问题，本文提出了FlexRoPE技术，能够灵活调整远程上下文的时间衰减，并引入了长短期上下文建模方法，以确保时间一致性。实验结果表明，FAR在短视频和长视频生成任务中均达到了最先进的性能，成为视频自回归建模的有效基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19910",
            "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
            "url": "https://huggingface.co/papers/2503.19910",
            "abstract": "Composed Image Retrieval (CIR) is a complex task that aims to retrieve images based on a multimodal query. Typical training data consists of triplets containing a reference image, a textual description of desired modifications, and the target image, which are expensive and time-consuming to acquire. The scarcity of CIR datasets has led to zero-shot approaches utilizing synthetic triplets or leveraging vision-language models (VLMs) with ubiquitous web-crawled image-caption pairs. However, these methods have significant limitations: synthetic triplets suffer from limited scale, lack of diversity, and unnatural modification text, while image-caption pairs hinder joint embedding learning of the multimodal query due to the absence of triplet data. Moreover, existing approaches struggle with complex and nuanced modification texts that demand sophisticated fusion and understanding of vision and language modalities. We present CoLLM, a one-stop framework that effectively addresses these limitations. Our approach generates triplets on-the-fly from image-caption pairs, enabling supervised training without manual annotation. We leverage Large Language Models (LLMs) to generate joint embeddings of reference images and modification texts, facilitating deeper multimodal fusion. Additionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset comprising 3.4M samples, and refine existing CIR benchmarks (CIRR and Fashion-IQ) to enhance evaluation reliability. Experimental results demonstrate that CoLLM achieves state-of-the-art performance across multiple CIR benchmarks and settings. MTCIR yields competitive results, with up to 15% performance improvement. Our refined benchmarks provide more reliable evaluation metrics for CIR models, contributing to the advancement of this important field.",
            "score": 2,
            "issue_id": 2896,
            "pub_date": "2025-03-25",
            "pub_date_card": {
                "ru": "25 марта",
                "en": "March 25",
                "zh": "3月25日"
            },
            "hash": "63f36082e6c27f3e",
            "authors": [
                "Chuong Huynh",
                "Jinyu Yang",
                "Ashish Tawari",
                "Mubarak Shah",
                "Son Tran",
                "Raffay Hamid",
                "Trishul Chilimbi",
                "Abhinav Shrivastava"
            ],
            "affiliations": [
                "Amazon",
                "Center for Research in Computer Vision, University of Central Florida",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19910.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#synthetic",
                    "#dataset",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "CoLLM: Революция в композиционном поиске изображений",
                    "desc": "Статья представляет CoLLM - новый фреймворк для решения задачи композиционного поиска изображений (CIR). CoLLM генерирует обучающие триплеты на лету из пар изображение-подпись, что позволяет обучаться без ручной разметки. Авторы используют большие языковые модели для создания совместных эмбеддингов изображений и текстов модификации, улучшая мультимодальное слияние. Также представлен новый крупномасштабный датасет MTCIR и уточнены существующие бенчмарки для более надежной оценки моделей CIR."
                },
                "en": {
                    "title": "Revolutionizing Composed Image Retrieval with CoLLM",
                    "desc": "This paper introduces CoLLM, a novel framework for Composed Image Retrieval (CIR) that overcomes the challenges of limited training data. It generates triplets from existing image-caption pairs, allowing for supervised training without the need for manual annotations. By utilizing Large Language Models (LLMs), CoLLM creates joint embeddings that enhance the understanding of complex multimodal queries. The authors also present the Multi-Text CIR (MTCIR) dataset, which significantly improves evaluation metrics and demonstrates state-of-the-art performance in CIR tasks."
                },
                "zh": {
                    "title": "CoLLM：复合图像检索的新突破",
                    "desc": "本文介绍了一种名为CoLLM的框架，用于解决复合图像检索（CIR）中的数据稀缺问题。该框架通过从图像-文本对中动态生成三元组，避免了手动标注的需求，从而实现了监督学习。我们利用大型语言模型（LLMs）生成参考图像和修改文本的联合嵌入，促进了多模态的深度融合。此外，我们还推出了一个包含340万样本的大规模数据集MTCIR，并改进了现有的CIR基准，以提高评估的可靠性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.19041",
            "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews",
            "url": "https://huggingface.co/papers/2503.19041",
            "abstract": "Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often undermines their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, which comprises two simple, low-resource, and effective data-driven methods that modify training data by previewing partial answer prefixes. Both methods aim to preserve the model's inherent safety mechanisms by minimizing perturbations to initial token distributions. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs. Code is released at https://github.com/zjunlp/LookAheadTuning.",
            "score": 2,
            "issue_id": 2896,
            "pub_date": "2025-03-24",
            "pub_date_card": {
                "ru": "24 марта",
                "en": "March 24",
                "zh": "3月24日"
            },
            "hash": "8495b5a09ddf5611",
            "authors": [
                "Kangwei Liu",
                "Mengru Wang",
                "Yujie Luo",
                "Lin Yuan",
                "Mengshu Sun",
                "Ningyu Zhang",
                "Lei Liang",
                "Zhiqiang Zhang",
                "Jun Zhou",
                "Huajun Chen"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University",
                "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.19041.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#low_resource"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Безопасная адаптация языковых моделей с сохранением производительности",
                    "desc": "Статья представляет новый метод тонкой настройки больших языковых моделей под названием LookAhead Tuning. Этот подход позволяет адаптировать модели к конкретным доменам, сохраняя при этом их изначальную безопасность. Метод основан на модификации обучающих данных путем предварительного просмотра частичных префиксов ответов. Эксперименты показывают, что LookAhead Tuning эффективно поддерживает безопасность модели без ущерба для производительности на целевых задачах."
                },
                "en": {
                    "title": "LookAhead Tuning: Safeguarding LLMs During Fine-Tuning",
                    "desc": "This paper presents LookAhead Tuning, a novel approach to fine-tuning large language models (LLMs) while preserving their safety alignment. The method involves two data-driven techniques that adjust training data by examining partial answer prefixes, which helps maintain the model's safety mechanisms. By minimizing changes to the initial token distributions, LookAhead Tuning effectively prevents safety degradation during the adaptation process. Experimental results show that this approach not only safeguards model safety but also ensures strong performance on various downstream tasks."
                },
                "zh": {
                    "title": "LookAhead Tuning：安全微调大型语言模型的新方法",
                    "desc": "本论文介绍了一种名为LookAhead Tuning的技术，旨在解决在微调大型语言模型（LLMs）时安全性下降的问题。该方法通过预览部分答案前缀，采用两种简单且低资源的数据驱动方法来修改训练数据。其目标是通过最小化初始标记分布的扰动，保持模型固有的安全机制。实验结果表明，LookAhead Tuning能够有效维护模型的安全性，同时在下游任务中保持强大的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.17361",
            "title": "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation",
            "url": "https://huggingface.co/papers/2503.17361",
            "abstract": "Flow matching in the continuous simplex has emerged as a promising strategy for DNA sequence design, but struggles to scale to higher simplex dimensions required for peptide and protein generation. We introduce Gumbel-Softmax Flow and Score Matching, a generative framework on the simplex based on a novel Gumbel-Softmax interpolant with a time-dependent temperature. Using this interpolant, we introduce Gumbel-Softmax Flow Matching by deriving a parameterized velocity field that transports from smooth categorical distributions to distributions concentrated at a single vertex of the simplex. We alternatively present Gumbel-Softmax Score Matching which learns to regress the gradient of the probability density. Our framework enables high-quality, diverse generation and scales efficiently to higher-dimensional simplices. To enable training-free guidance, we propose Straight-Through Guided Flows (STGFlow), a classifier-based guidance method that leverages straight-through estimators to steer the unconditional velocity field toward optimal vertices of the simplex. STGFlow enables efficient inference-time guidance using classifiers pre-trained on clean sequences, and can be used with any discrete flow method. Together, these components form a robust framework for controllable de novo sequence generation. We demonstrate state-of-the-art performance in conditional DNA promoter design, sequence-only protein generation, and target-binding peptide design for rare disease treatment.",
            "score": 0,
            "issue_id": 2896,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "b5389a3e5ab241c3",
            "authors": [
                "Sophia Tang",
                "Yinuo Zhang",
                "Alexander Tong",
                "Pranam Chatterjee"
            ],
            "affiliations": [
                "Center of Computational Biology, Duke-NUS Medical School",
                "Department of Biomedical Engineering, Duke University",
                "Department of Biostatistics and Bioinformatics, Duke University",
                "Department of Computer Science, Duke University",
                "Management and Technology Program, University of Pennsylvania",
                "Mila, Quebec AI Institute",
                "Université de Montréal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.17361.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#data",
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#healthcare"
                ],
                "emoji": "🧬",
                "ru": {
                    "title": "Новый подход к генерации биологических последовательностей на симплексе",
                    "desc": "Статья представляет новый метод генеративного моделирования на симплексе, называемый Gumbel-Softmax Flow and Score Matching. Авторы вводят новый интерполянт Gumbel-Softmax с зависящей от времени температурой и используют его для создания параметризованного поля скоростей. Метод позволяет получать высококачественные и разнообразные результаты, эффективно масштабируясь на симплексы высокой размерности. Также предложен метод Straight-Through Guided Flows для управления генерацией без дополнительного обучения."
                },
                "en": {
                    "title": "Revolutionizing Sequence Generation with Gumbel-Softmax Flows",
                    "desc": "This paper presents a new method called Gumbel-Softmax Flow and Score Matching for generating DNA sequences, peptides, and proteins. It introduces a Gumbel-Softmax interpolant that helps in transitioning between smooth categorical distributions and specific points in a simplex, which is a mathematical space used for these types of data. The authors also propose a technique called Straight-Through Guided Flows (STGFlow) that uses classifiers to guide the generation process towards optimal outcomes without needing extensive training. Overall, this framework allows for efficient and high-quality generation of biological sequences, achieving impressive results in various applications."
                },
                "zh": {
                    "title": "高效生成高维序列的创新框架",
                    "desc": "本文提出了一种新的生成框架，称为Gumbel-Softmax流和评分匹配，旨在解决DNA序列设计中的高维简单形问题。通过引入时间依赖的Gumbel-Softmax插值，我们能够在简单形上实现高质量和多样化的生成。该框架还包括一种名为STGFlow的分类器引导方法，能够在推理时有效地引导生成过程。我们的研究在条件DNA启动子设计、序列生成的蛋白质和靶向结合肽的设计中展示了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16965",
            "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making",
            "url": "https://huggingface.co/papers/2503.16965",
            "abstract": "Embodied decision-making is fundamental for AI agents operating in real-world environments. While Visual Language Models (VLMs) have advanced this capability, they still struggle with complex decisions, particularly in human-centered situations that require deep reasoning about human needs and values. In this study, we systematically evaluate open-sourced VLMs on multimodal human-centered decision-making tasks. We find that LLMs receiving only textual descriptions unexpectedly outperform their VLM counterparts of similar scale that process actual images, suggesting that visual alignment may hinder VLM abilities. To address this challenge, we propose a novel text-only training approach with synthesized textual data. This method strengthens VLMs' language components and transfers the learned abilities to multimodal inference, eliminating the need for expensive image-text paired data. Furthermore, we show that VLMs can achieve substantial performance gains through self-improvement, using training data generated by their LLM counterparts rather than relying on larger teacher models like GPT-4. Our findings establish a more efficient and scalable approach to enhancing VLMs' human-centered decision-making capabilities, opening new avenues for optimizing VLMs through self-improvement mechanisms.",
            "score": 0,
            "issue_id": 2896,
            "pub_date": "2025-03-21",
            "pub_date_card": {
                "ru": "21 марта",
                "en": "March 21",
                "zh": "3月21日"
            },
            "hash": "ca2e599ff0665dfe",
            "authors": [
                "Zhe Hu",
                "Jing Li",
                "Yu Yin"
            ],
            "affiliations": [
                "Department of Computer and Data Sciences, Case Western Reserve University",
                "Department of Computing, The Hong Kong Polytechnic University",
                "Research Centre for Data Science & Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16965.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#transfer_learning",
                    "#training",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Самосовершенствование VLM через текстовое обучение",
                    "desc": "Это исследование сосредоточено на оценке визуальных языковых моделей (VLM) в задачах принятия решений, ориентированных на человека. Авторы обнаружили, что языковые модели, работающие только с текстом, превосходят VLM аналогичного масштаба, обрабатывающие изображения. Для решения этой проблемы предложен новый подход к обучению, использующий синтезированные текстовые данные. Исследование также демонстрирует, что VLM могут значительно улучшить свою производительность через самосовершенствование, используя данные, сгенерированные их LLM-аналогами."
                },
                "en": {
                    "title": "Enhancing VLMs through Text-Only Training and Self-Improvement",
                    "desc": "This paper explores the challenges faced by Visual Language Models (VLMs) in making complex decisions that involve understanding human needs and values. The authors find that VLMs that rely on visual data often perform worse than those using only text, indicating that visual information may complicate decision-making. To improve VLM performance, they propose a new training method that uses synthesized textual data, enhancing the language understanding of VLMs without needing paired image-text data. Additionally, the study demonstrates that VLMs can improve their decision-making abilities by learning from their own generated data, rather than depending on larger models, making the training process more efficient and scalable."
                },
                "zh": {
                    "title": "提升VLM人类中心决策能力的新方法",
                    "desc": "本研究探讨了视觉语言模型（VLMs）在复杂人类中心决策中的表现。我们发现，仅使用文本描述的语言模型（LLMs）在某些任务上意外地超越了处理图像的VLMs，这表明视觉对齐可能会限制VLM的能力。为了解决这个问题，我们提出了一种新的仅基于文本的训练方法，利用合成文本数据来增强VLM的语言能力。我们的研究结果表明，通过自我改进，VLMs可以显著提升其人类中心决策能力，开辟了优化VLM的新途径。"
                }
            }
        }
    ],
    "link_prev": "2025-03-25.html",
    "link_next": "2025-03-27.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "25.03",
        "en": "03/25",
        "zh": "3月25日"
    },
    "short_date_next": {
        "ru": "27.03",
        "en": "03/27",
        "zh": "3月27日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "现代游戏开发面临创意和成本挑战。新的视频生成模型可以创造逼真且互动的虚拟环境。我们提出互动生成视频（IGV）作为生成游戏引擎（GGE）的基础。GGE利用IGV的优势，如无限制的高质量内容生成和物理意识建模。我们展示了GGE的核心模块和分层成熟度路线图（L0-L4）。这项工作为AI时代的游戏开发开辟了新途径。",
        "title": "Position: Interactive Generative Video as Next-Generation Game Engine",
        "pinyin": "现代游戏开发面临创意和成本挑战。\nXiàndài yóuxì kāifā miànlín chuàngyì hé chéngběn tiǎozhàn.\n\n新的视频生成模型可以创造逼真且互动的虚拟环境。\nXīn de shìpín shēngchéng móxíng kěyǐ chuàngzào bīzhèn qiě hùdòng de xūnǐ huánjìng.\n\n我们提出互动生成视频（IGV）作为生成游戏引擎（GGE）的基础。\nWǒmen tíchū hùdòng shēngchéng shìpín (IGV) zuòwéi shēngchéng yóuxì yǐnqíng (GGE) de jīchǔ.\n\nGGE利用IGV的优势，如无限制的高质量内容生成和物理意识建模。\nGGE lìyòng IGV de yōushì, rú wúxiànzhì de gāo zhìliàng nèiróng shēngchéng hé wùlǐ yìshí jiànmó.\n\n我们展示了GGE的核心模块和分层成熟度路线图（L0-L4）。\nWǒmen zhǎnshìle GGE de héxīn mókuài hé fēncéng chéngshúdù lùxiàntú (L0-L4).\n\n这项工作为AI时代的游戏开发开辟了新途径。\nZhè xiàng gōngzuò wèi AI shídài de yóuxì kāifā kāipìle xīn tújìng.",
        "vocab": "[\n    {\"word\": \"现代\", \"pinyin\": \"xiàndài\", \"trans\": \"modern\"},\n    {\"word\": \"面临\", \"pinyin\": \"miànlín\", \"trans\": \"face\"},\n    {\"word\": \"创意\", \"pinyin\": \"chuàngyì\", \"trans\": \"creativity\"},\n    {\"word\": \"成本\", \"pinyin\": \"chéngběn\", \"trans\": \"cost\"},\n    {\"word\": \"挑战\", \"pinyin\": \"tiǎozhàn\", \"trans\": \"challenge\"},\n    {\"word\": \"视频\", \"pinyin\": \"shìpín\", \"trans\": \"video\"},\n    {\"word\": \"生成\", \"pinyin\": \"shēngchéng\", \"trans\": \"generate\"},\n    {\"word\": \"模型\", \"pinyin\": \"móxíng\", \"trans\": \"model\"},\n    {\"word\": \"逼真\", \"pinyin\": \"bīzhēn\", \"trans\": \"realistic\"},\n    {\"word\": \"互动\", \"pinyin\": \"hùdòng\", \"trans\": \"interactive\"},\n    {\"word\": \"虚拟\", \"pinyin\": \"xūnǐ\", \"trans\": \"virtual\"},\n    {\"word\": \"环境\", \"pinyin\": \"huánjìng\", \"trans\": \"environment\"},\n    {\"word\": \"提出\", \"pinyin\": \"tíchū\", \"trans\": \"propose\"},\n    {\"word\": \"引擎\", \"pinyin\": \"yǐnqíng\", \"trans\": \"engine\"},\n    {\"word\": \"基础\", \"pinyin\": \"jīchǔ\", \"trans\": \"foundation\"},\n    {\"word\": \"利用\", \"pinyin\": \"lìyòng\", \"trans\": \"utilize\"},\n    {\"word\": \"优势\", \"pinyin\": \"yōushì\", \"trans\": \"advantage\"},\n    {\"word\": \"无限制\", \"pinyin\": \"wúxiànzhì\", \"trans\": \"unlimited\"},\n    {\"word\": \"高质量\", \"pinyin\": \"gāo zhìliàng\", \"trans\": \"high quality\"},\n    {\"word\": \"内容\", \"pinyin\": \"nèiróng\", \"trans\": \"content\"},\n    {\"word\": \"物理\", \"pinyin\": \"wùlǐ\", \"trans\": \"physical\"},\n    {\"word\": \"意识\", \"pinyin\": \"yìshí\", \"trans\": \"awareness\"},\n    {\"word\": \"建模\", \"pinyin\": \"jiànmó\", \"trans\": \"modeling\"},\n    {\"word\": \"展示\", \"pinyin\": \"zhǎnshì\", \"trans\": \"demonstrate\"},\n    {\"word\": \"核心\", \"pinyin\": \"héxīn\", \"trans\": \"core\"},\n    {\"word\": \"模块\", \"pinyin\": \"mókuài\", \"trans\": \"module\"},\n    {\"word\": \"分层\", \"pinyin\": \"fēncéng\", \"trans\": \"layered\"},\n    {\"word\": \"成熟度\", \"pinyin\": \"chéngshúdù\", \"trans\": \"maturity\"},\n    {\"word\": \"路线图\", \"pinyin\": \"lùxiàntú\", \"trans\": \"roadmap\"},\n    {\"word\": \"开辟\", \"pinyin\": \"kāipì\", \"trans\": \"open up\"},\n    {\"word\": \"途径\", \"pinyin\": \"tújìng\", \"trans\": \"path\"},\n    {\"word\": \"时代\", \"pinyin\": \"shídài\", \"trans\": \"era\"}\n]",
        "trans": "Modern game development faces challenges in creativity and cost. New video generation models can create realistic and interactive virtual environments. We propose Interactive Generated Video (IGV) as the foundation for a Generative Game Engine (GGE). GGE leverages the advantages of IGV, such as unlimited high-quality content generation and physics-aware modeling. We demonstrate the core modules of GGE and a layered maturity roadmap (L0-L4). This work paves new ways for game development in the AI era.",
        "update_ts": "2025-03-25 09:11"
    }
}