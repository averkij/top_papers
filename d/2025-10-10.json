{
    "date": {
        "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 10",
        "zh": "10æœˆ10æ—¥"
    },
    "time_utc": "2025-10-10 03:28",
    "weekday": 4,
    "issue_id": 6345,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.08540",
            "title": "MM-HELIX: Boosting Multimodal Long-Chain Reflective Reasoning with\n  Holistic Platform and Adaptive Hybrid Policy Optimization",
            "url": "https://huggingface.co/papers/2510.08540",
            "abstract": "Existing Multimodal Large Language Models show performance deficits in long-chain reflective reasoning, which is addressed by developing MM-HELIX-100K and Adaptive Hybrid Policy Optimization, leading to improved accuracy and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t While current Multimodal Large Language Models (MLLMs) have demonstrated proficiency in reasoning tasks such as mathematics and logic, their capacity for long-chain reflective reasoning, a prerequisite for solving complex real-world problems, remains largely underexplored. In this work, we first conduct an extensive empirical investigation to evaluate this capability. Leveraging a carefully designed data synthesis engine, we construct MM-HELIX, a multimodal benchmark consisting 1,260 samples of 42 challenging synthetic tasks that require iterative thinking and backtracking. Empirical results on this benchmark reveal that existing MLLMs exhibit significant performance deficits in long-chain reflective reasoning. To address this limitation, we generate post-training data and further explore learning paradigms for exploiting such data. We first develop the Step-Elicited Response Generation pipeline to create MM-HELIX-100K, a large-scale dataset of 100k high-quality, reflective reasoning traces for instruction-tuning stage. Given that standard Reinforcement Learning fails on complex tasks due to sparse reward signals and catastrophic forgetting after Supervised Fine-Tuning, we propose Adaptive Hybrid Policy Optimization (AHPO), a novel training strategy that dynamically unifies offline supervision and online optimization into a single stage. This strategy enables the model to learn from expert data when rewards are sparse and conduct independent exploration once proficient. When applied to the Qwen2.5-VL-7B baseline, our method achieves a +18.6\\% accuracy improvement on MM-HELIX benchmark and demonstrates strong generalization with a +5.7\\% average performance gain on general mathematic and logic tasks. Our work demonstrate that reflective reasoning in MLLMs can be effectively learned and generalized, paving the way for developing more capable MLLMs.",
            "score": 58,
            "issue_id": 6345,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "62d107921c57dab0",
            "authors": [
                "Xiangyu Zhao",
                "Junming Lin",
                "Tianhao Liang",
                "Yifan Zhou",
                "Wenhao Chai",
                "Yuzhe Gu",
                "Weiyun Wang",
                "Kai Chen",
                "Gen Luo",
                "Wenwei Zhang",
                "Junchi Yan",
                "Hua Yang",
                "Haodong Duan",
                "Xue Yang"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Princeton University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08540.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#rl",
                    "#multimodal",
                    "#dataset",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ° Ğº Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼ ÑˆĞ°Ğ³Ğ°Ğ¼. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MM-HELIX Ñ 1260 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MM-HELIX-100K Ğ¸Ğ· 100 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Adaptive Hybrid Policy Optimization (AHPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ reinforcement learning, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ñ… Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-VL-7B Ğ´Ğ°Ğ»Ğ¾ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ +18.6% Ğ½Ğ° MM-HELIX Ğ¸ +5.7% Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Reflective Reasoning in Multimodal Models",
                    "desc": "This paper addresses the limitations of existing Multimodal Large Language Models (MLLMs) in performing long-chain reflective reasoning, which is essential for tackling complex problems. The authors introduce MM-HELIX-100K, a large dataset designed to enhance the instruction-tuning of MLLMs by providing high-quality reflective reasoning examples. They also propose Adaptive Hybrid Policy Optimization (AHPO), a novel training approach that combines offline supervision with online learning to improve model performance on challenging tasks. The results show significant accuracy improvements and better generalization in reasoning tasks, indicating that reflective reasoning can be effectively learned in MLLMs."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹çš„åæ€æ¨ç†èƒ½åŠ›",
                    "desc": "ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿é“¾åæ€æ¨ç†æ–¹é¢è¡¨ç°ä¸è¶³ï¼Œè¿™é¡¹ç ”ç©¶é€šè¿‡å¼€å‘MM-HELIX-100Kæ•°æ®é›†å’Œè‡ªé€‚åº”æ··åˆç­–ç•¥ä¼˜åŒ–æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåŒ…å«1260ä¸ªæ ·æœ¬çš„å¤šæ¨¡æ€åŸºå‡†ï¼Œè¯„ä¼°ç°æœ‰æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åæ€æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨é•¿é“¾åæ€æ¨ç†ä¸Šå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½ç¼ºé™·ã€‚é€šè¿‡å¼•å…¥æ–°çš„è®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨MM-HELIXåŸºå‡†ä¸Šå®ç°äº†18.6%çš„å‡†ç¡®ç‡æå‡ï¼Œå¹¶åœ¨ä¸€èˆ¬æ•°å­¦å’Œé€»è¾‘ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°å‡º5.7%çš„å¹³å‡æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08558",
            "title": "Agent Learning via Early Experience",
            "url": "https://huggingface.co/papers/2510.08558",
            "abstract": "Early experience, using agent-generated interaction data without reward signals, improves policy effectiveness and generalization, serving as a bridge between imitation learning and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t A long-term goal of language agents is to learn and improve through their own experience, ultimately outperforming humans in complex, real-world tasks. However, training agents from experience data with reinforcement learning remains difficult in many environments, which either lack verifiable rewards (e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn tool use). As a result, most current agents rely on supervised fine-tuning on expert data, which is challenging to scale and generalizes poorly. This limitation stems from the nature of expert demonstrations: they capture only a narrow range of scenarios and expose the agent to limited environment diversity. We address this limitation with a middle-ground paradigm we call early experience: interaction data generated by the agent's own actions, where the resulting future states serve as supervision without reward signals. Within this paradigm we study two strategies of using such data: (1) Implicit world modeling, which uses collected states to ground the policy in environment dynamics; and (2) Self-reflection, where the agent learns from its suboptimal actions to improve reasoning and decision-making. We evaluate across eight diverse environments and multiple model families. Our approaches consistently improve effectiveness and out-of-domain generalization, highlighting the value of early experience. Moreover, in environments with verifiable rewards, our results provide promising signals that early experience offers a strong foundation for subsequent reinforcement learning, positioning it as a practical bridge between imitation learning and fully experience-driven agents.",
            "score": 43,
            "issue_id": 6345,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "332f256ea51550f0",
            "authors": [
                "Kai Zhang",
                "Xiangchao Chen",
                "Bo Liu",
                "Tianci Xue",
                "Zeyi Liao",
                "Zhihan Liu",
                "Xiyao Wang",
                "Yuting Ning",
                "Zhaorun Chen",
                "Xiaohan Fu",
                "Jian Xie",
                "Yuxuan Sun",
                "Boyu Gou",
                "Qi Qi",
                "Zihang Meng",
                "Jianwei Yang",
                "Ning Zhang",
                "Xian Li",
                "Ashish Shah",
                "Dat Huynh",
                "Hengduo Li",
                "Zi Yang",
                "Sara Cao",
                "Lawrence Jang",
                "Shuyan Zhou",
                "Jiacheng Zhu",
                "Huan Sun",
                "Jason Weston",
                "Yu Su",
                "Yifan Wu"
            ],
            "affiliations": [
                "FAIR at Meta",
                "Meta Superintelligence Labs",
                "The Ohio State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08558.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#rl",
                    "#rlhf",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸŒ‰",
                "ru": {
                    "title": "Ğ Ğ°Ğ½Ğ½Ğ¸Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚: Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ language-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· \"Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚\" - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ°Ğ¼Ğ¸Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼, Ğ±ĞµĞ· ÑĞ²Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ´Ğ²Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸: Ğ¸Ğ¼Ğ¿Ğ»Ğ¸Ñ†Ğ¸Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ° Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ reasoning. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ğ¾ÑÑ‚Ğ¾Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ imitation learning Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ğ¼ reinforcement learning, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Harnessing Early Experience for Smarter Agents",
                    "desc": "This paper introduces a new approach called 'early experience' for training language agents, which uses data generated from the agent's own interactions without relying on reward signals. The authors highlight the challenges of traditional reinforcement learning, especially in environments lacking clear rewards or requiring complex decision-making. By employing strategies like implicit world modeling and self-reflection, agents can learn from their own actions and improve their performance and generalization capabilities. The results show that early experience not only enhances the effectiveness of agents but also serves as a valuable link between imitation learning and reinforcement learning."
                },
                "zh": {
                    "title": "æ—©æœŸç»éªŒï¼šè¿æ¥æ¨¡ä»¿å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ çš„æ¡¥æ¢",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä¸€ç§åä¸ºâ€œæ—©æœŸç»éªŒâ€çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ä»£ç†ç”Ÿæˆçš„äº¤äº’æ•°æ®æ¥æé«˜ç­–ç•¥çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œè€Œæ— éœ€ä¾èµ–å¥–åŠ±ä¿¡å·ã€‚è¿™ç§æ–¹æ³•ä¸ºæ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¹‹é—´æ¶èµ·äº†ä¸€åº§æ¡¥æ¢ï¼Œè§£å†³äº†å½“å‰ä»£ç†åœ¨ç¼ºä¹å¯éªŒè¯å¥–åŠ±çš„ç¯å¢ƒä¸­è®­ç»ƒçš„å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§åˆ©ç”¨æ—©æœŸç»éªŒæ•°æ®çš„ç­–ç•¥ï¼šéšå¼ä¸–ç•Œå»ºæ¨¡å’Œè‡ªæˆ‘åæ€ï¼Œå‰è€…é€šè¿‡æ”¶é›†çš„çŠ¶æ€æ¥å¢å¼ºç­–ç•¥ä¸ç¯å¢ƒåŠ¨æ€çš„è”ç³»ï¼Œåè€…åˆ™é€šè¿‡å­¦ä¹ æ¬¡ä¼˜è¡Œä¸ºæ¥æ”¹å–„æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨å¤šç§ç¯å¢ƒä¸­å‡èƒ½æœ‰æ•ˆæå‡ä»£ç†çš„è¡¨ç°ï¼Œæ˜¾ç¤ºå‡ºæ—©æœŸç»éªŒåœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æ½œåœ¨ä»·å€¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07242",
            "title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense",
            "url": "https://huggingface.co/papers/2510.07242",
            "abstract": "HERO, a reinforcement learning framework, combines verifier signals with reward-model scores to enhance reasoning in large language models, outperforming both RM-only and verifier-only methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training for reasoning of large language models (LLMs) increasingly relies on verifiable rewards: deterministic checkers that provide 0-1 correctness signals. While reliable, such binary feedback is brittle--many tasks admit partially correct or alternative answers that verifiers under-credit, and the resulting all-or-nothing supervision limits learning. Reward models offer richer, continuous feedback, which can serve as a complementary supervisory signal to verifiers. We introduce HERO (Hybrid Ensemble Reward Optimization), a reinforcement learning framework that integrates verifier signals with reward-model scores in a structured way. HERO employs stratified normalization to bound reward-model scores within verifier-defined groups, preserving correctness while refining quality distinctions, and variance-aware weighting to emphasize challenging prompts where dense signals matter most. Across diverse mathematical reasoning benchmarks, HERO consistently outperforms RM-only and verifier-only baselines, with strong gains on both verifiable and hard-to-verify tasks. Our results show that hybrid reward design retains the stability of verifiers while leveraging the nuance of reward models to advance reasoning.",
            "score": 17,
            "issue_id": 6345,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "4752a7b26fdfbe7b",
            "authors": [
                "Leitian Tao",
                "Ilia Kulikov",
                "Swarnadeep Saha",
                "Tianlu Wang",
                "Jing Xu",
                "Yixuan Li",
                "Jason E Weston",
                "Ping Yu"
            ],
            "affiliations": [
                "FAIR at Meta",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07242.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#rlhf",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ›ÑƒÑ‡ÑˆĞµĞµ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²: Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ HERO â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ reinforcement learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ñ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² (0 Ğ¸Ğ»Ğ¸ 1 Ğ·Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ) Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ¾Ñ‚ reward models. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ´Ğ°ÑÑ‚ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ³Ñ€ÑƒĞ±ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñƒ Â«Ğ²ÑÑ‘ Ğ¸Ğ»Ğ¸ Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾Â», Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹. HERO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ scores Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ¸ variance-aware Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ„Ğ¾ĞºÑƒÑĞ° Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ reward models Ğ¸Ğ»Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ reasoning."
                },
                "en": {
                    "title": "HERO: Enhancing Reasoning with Hybrid Rewards",
                    "desc": "HERO is a reinforcement learning framework that improves reasoning in large language models by combining verifier signals with reward-model scores. Verifiers provide binary feedback, which can be too strict and limit learning, while reward models offer richer, continuous feedback. HERO uses stratified normalization to ensure that reward-model scores align with verifier-defined correctness, enhancing the quality of learning. The framework demonstrates superior performance on various reasoning tasks compared to using either reward models or verifiers alone."
                },
                "zh": {
                    "title": "HEROï¼šæ··åˆå¥–åŠ±ä¼˜åŒ–ï¼Œæå‡æ¨ç†èƒ½åŠ›",
                    "desc": "HEROæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†éªŒè¯å™¨ä¿¡å·ä¸å¥–åŠ±æ¨¡å‹åˆ†æ•°ç»“åˆèµ·æ¥ï¼Œä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„éªŒè¯å™¨æä¾›çš„äºŒå…ƒåé¦ˆè™½ç„¶å¯é ï¼Œä½†åœ¨è®¸å¤šä»»åŠ¡ä¸­å¯èƒ½ä¼šä½ä¼°éƒ¨åˆ†æ­£ç¡®æˆ–æ›¿ä»£ç­”æ¡ˆã€‚HEROé€šè¿‡åˆ†å±‚å½’ä¸€åŒ–å’Œæ–¹å·®æ„ŸçŸ¥åŠ æƒï¼Œç¡®ä¿å¥–åŠ±æ¨¡å‹çš„åˆ†æ•°åœ¨éªŒè¯å™¨å®šä¹‰çš„ç»„å†…ä¿æŒç¨³å®šï¼ŒåŒæ—¶æé«˜äº†å¯¹å›°éš¾æç¤ºçš„é‡è§†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHEROåœ¨å¤šç§æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä»…ä½¿ç”¨å¥–åŠ±æ¨¡å‹æˆ–éªŒè¯å™¨çš„æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºæ··åˆå¥–åŠ±è®¾è®¡çš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08565",
            "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language\n  Models under Data Constraints",
            "url": "https://huggingface.co/papers/2510.08565",
            "abstract": "Native end-to-end training of Multimodal Large Language Models (MLLMs) achieves competitive performance with a balanced design and scaling relationship between visual encoders and LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Compositional training has been the de-facto paradigm in existing Multimodal Large Language Models (MLLMs), where pre-trained vision encoders are connected with pre-trained LLMs through continuous multimodal pre-training. However, the multimodal scaling property of this paradigm remains difficult to explore due to the separated training. In this paper, we focus on the native training of MLLMs in an end-to-end manner and systematically study its design space and scaling property under a practical setting, i.e., data constraint. Through careful study of various choices in MLLM, we obtain the optimal meta-architecture that best balances performance and training cost. After that, we further explore the scaling properties of the native MLLM and indicate the positively correlated scaling relationship between visual encoders and LLMs. Based on these findings, we propose a native MLLM called NaViL, combined with a simple and cost-effective recipe. Experimental results on 14 multimodal benchmarks confirm the competitive performance of NaViL against existing MLLMs. Besides that, our findings and results provide in-depth insights for the future study of native MLLMs.",
            "score": 14,
            "issue_id": 6344,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "39431998f40d1db5",
            "authors": [
                "Changyao Tian",
                "Hao Li",
                "Gen Luo",
                "Xizhou Zhu",
                "Weijie Su",
                "Hanming Deng",
                "Jinguo Zhu",
                "Jie Shao",
                "Ziran Zhu",
                "Yunpeng Liu",
                "Lewei Lu",
                "Wenhai Wang",
                "Hongsheng Li",
                "Jifeng Dai"
            ],
            "affiliations": [
                "Nanjing University",
                "Sensetime Research",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08565.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#architecture",
                    "#multimodal",
                    "#optimization",
                    "#agi"
                ],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "ĞĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½ÑƒĞ»Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ end-to-end Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Multimodal Large Language Models (MLLM) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ³Ğ´Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ ÑĞ¾ĞµĞ´Ğ¸Ğ½ÑÑÑ‚ÑÑ Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ LLM. ĞĞ½Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ°Ğ¹Ğ´Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ’Ğ°Ğ¶Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸ĞµĞ¼ ÑÑ‚Ğ°Ğ»Ğ° Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ LLM ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¾Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ NaViL, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ²ÑˆĞ°Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 14 Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing MLLMs with Native End-to-End Training",
                    "desc": "This paper introduces a new approach to training Multimodal Large Language Models (MLLMs) called native end-to-end training. Unlike traditional methods that use separate pre-trained vision and language models, this approach integrates both components in a single training process. The authors explore the design and scaling properties of this method, demonstrating that a balanced relationship between visual encoders and language models can enhance performance while managing training costs. Their proposed model, NaViL, shows competitive results across multiple benchmarks, paving the way for future research in native MLLMs."
                },
                "zh": {
                    "title": "åŸç”Ÿç«¯åˆ°ç«¯è®­ç»ƒï¼Œæå‡å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„åŸç”Ÿç«¯åˆ°ç«¯è®­ç»ƒæ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„ç»„åˆè®­ç»ƒæ–¹æ³•ä¸åŒï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®¾è®¡ç©ºé—´å’Œæ‰©å±•ç‰¹æ€§ï¼Œå¼ºè°ƒè§†è§‰ç¼–ç å™¨ä¸è¯­è¨€æ¨¡å‹ä¹‹é—´çš„å¹³è¡¡å…³ç³»ã€‚é€šè¿‡ç³»ç»Ÿç ”ç©¶ï¼Œä½œè€…æå‡ºäº†åä¸ºNaViLçš„åŸç”ŸMLLMï¼Œå±•ç¤ºäº†å…¶åœ¨14ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­çš„ç«äº‰æ€§èƒ½ã€‚ç ”ç©¶ç»“æœä¸ºæœªæ¥çš„åŸç”ŸMLLMç ”ç©¶æä¾›äº†æ·±å…¥çš„è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08483",
            "title": "DeepPrune: Parallel Scaling without Inter-trace Redundancy",
            "url": "https://huggingface.co/papers/2510.08483",
            "abstract": "DeepPrune, a novel framework using dynamic pruning and a specialized judge model, significantly reduces computational inefficiency in parallel scaling of large language models by pruning redundant reasoning traces.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel scaling has emerged as a powerful paradigm to enhance reasoning capabilities in large language models (LLMs) by generating multiple Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces significant computational inefficiency due to inter-trace redundancy -- our analysis reveals that over 80% of parallel reasoning traces yield identical final answers, representing substantial wasted computation. To address this critical efficiency bottleneck, we propose DeepPrune, a novel framework that enables efficient parallel scaling through dynamic pruning. Our method features a specialized judge model trained with focal loss and oversampling techniques to accurately predict answer equivalence from partial reasoning traces which realizes 0.87 AUROC on equivalence prediction, combined with an online greedy clustering algorithm that dynamically prunes redundant paths while preserving answer diversity. Comprehensive evaluations across three challenging benchmarks (AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that DeepPrune achieves remarkable token reduction by over 80% compared to conventional consensus sampling on most cases, while maintaining competitive accuracy within 3 percentage points. Our work establishes a new standard for efficient parallel reasoning, making high-performance reasoning more efficient. Our code and data are here: https://deepprune.github.io/",
            "score": 13,
            "issue_id": 6344,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "13be68ae86d17de1",
            "authors": [
                "Shangqing Tu",
                "Yaxuan Li",
                "Yushi Bai",
                "Lei Hou",
                "Juanzi Li"
            ],
            "affiliations": [
                "ShanghaiTech University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08483.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#inference",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… LLM",
                    "desc": "DeepPrune â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞºĞµĞ¹Ğ»Ğ¸Ğ½Ğ³Ğ° LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ 80% Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ judge-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. DeepPrune ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 80% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ñ… 3 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Efficient Reasoning with DeepPrune: Prune the Redundancy!",
                    "desc": "DeepPrune is a new framework designed to improve the efficiency of large language models by reducing unnecessary computations during parallel reasoning. It identifies and prunes redundant reasoning paths that often lead to the same answers, which can waste over 80% of computational resources. The framework employs a specialized judge model that predicts when reasoning traces are equivalent, allowing for dynamic pruning of these redundant paths. Evaluations show that DeepPrune can significantly reduce the number of tokens used while maintaining high accuracy, setting a new benchmark for efficient reasoning in AI models."
                },
                "zh": {
                    "title": "DeepPruneï¼šé«˜æ•ˆå¹¶è¡Œæ¨ç†çš„æ–°æ ‡å‡†",
                    "desc": "DeepPruneæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å‰ªæå’Œä¸“é—¨çš„åˆ¤æ–­æ¨¡å‹ï¼Œæ˜¾è‘—å‡å°‘äº†å¤§è¯­è¨€æ¨¡å‹åœ¨å¹¶è¡Œæ‰©å±•ä¸­çš„è®¡ç®—ä½æ•ˆã€‚è¯¥æ–¹æ³•è§£å†³äº†å¹¶è¡Œæ¨ç†ä¸­å­˜åœ¨çš„å†—ä½™é—®é¢˜ï¼Œåˆ†ææ˜¾ç¤ºè¶…è¿‡80%çš„æ¨ç†è½¨è¿¹äº§ç”Ÿç›¸åŒçš„æœ€ç»ˆç­”æ¡ˆï¼Œé€ æˆäº†å¤§é‡çš„è®¡ç®—æµªè´¹ã€‚DeepPruneé€šè¿‡è®­ç»ƒå…·æœ‰ç„¦ç‚¹æŸå¤±å’Œè¿‡é‡‡æ ·æŠ€æœ¯çš„åˆ¤æ–­æ¨¡å‹ï¼Œå‡†ç¡®é¢„æµ‹éƒ¨åˆ†æ¨ç†è½¨è¿¹çš„ç­”æ¡ˆç­‰ä»·æ€§ï¼Œå¹¶ç»“åˆåœ¨çº¿è´ªå©ªèšç±»ç®—æ³•åŠ¨æ€å‰ªé™¤å†—ä½™è·¯å¾„ã€‚ç»è¿‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å…¨é¢è¯„ä¼°ï¼ŒDeepPruneåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹å®ç°äº†è¶…è¿‡80%çš„ä»¤ç‰Œå‡å°‘ï¼ŒåŒæ—¶ä¿æŒäº†ä¸ä¼ ç»Ÿå…±è¯†é‡‡æ ·ç›¸è¿‘çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08377",
            "title": "UniVideo: Unified Understanding, Generation, and Editing for Videos",
            "url": "https://huggingface.co/papers/2510.08377",
            "abstract": "UniVideo, a dual-stream framework combining a Multimodal Large Language Model and a Multimodal DiT, extends unified modeling to video generation and editing, achieving state-of-the-art performance and supporting task composition and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models have shown promising results in multimodal content generation and editing but remain largely limited to the image domain. In this work, we present UniVideo, a versatile framework that extends unified modeling to the video domain. UniVideo adopts a dual-stream design, combining a Multimodal Large Language Model (MLLM) for instruction understanding with a Multimodal DiT (MMDiT) for video generation. This design enables accurate interpretation of complex multimodal instructions while preserving visual consistency. Built on this architecture, UniVideo unifies diverse video generation and editing tasks under a single multimodal instruction paradigm and is jointly trained across them. Extensive experiments demonstrate that UniVideo matches or surpasses state-of-the-art task-specific baselines in text/image-to-video generation, in-context video generation and in-context video editing. Notably, the unified design of UniVideo enables two forms of generalization. First, UniVideo supports task composition, such as combining editing with style transfer, by integrating multiple capabilities within a single instruction. Second, even without explicit training on free-form video editing, UniVideo transfers its editing capability from large-scale image editing data to this setting, handling unseen instructions such as green-screening characters or changing materials within a video. Beyond these core capabilities, UniVideo also supports visual-prompt-based video generation, where the MLLM interprets visual prompts and guides the MMDiT during synthesis. To foster future research, we will release our model and code.",
            "score": 11,
            "issue_id": 6345,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "5d36f7f0bde88332",
            "authors": [
                "Cong Wei",
                "Quande Liu",
                "Zixuan Ye",
                "Qiulin Wang",
                "Xintao Wang",
                "Pengfei Wan",
                "Kun Gai",
                "Wenhu Chen"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08377.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#architecture",
                    "#games",
                    "#multimodal",
                    "#video",
                    "#transfer_learning",
                    "#open_source"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼",
                    "desc": "UniVideo â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Multimodal LLM Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Multimodal DiT Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¸Ñ… ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾. UniVideo Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… text/image-to-video Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "UniVideo: Unifying Video Generation and Editing with Multimodal Intelligence",
                    "desc": "UniVideo is a dual-stream framework that integrates a Multimodal Large Language Model (MLLM) and a Multimodal DiT (MMDiT) to enhance video generation and editing. This innovative approach allows the model to understand complex multimodal instructions while ensuring visual consistency in the generated content. By unifying various video tasks under a single instruction paradigm, UniVideo demonstrates superior performance in text/image-to-video generation and editing compared to existing models. Additionally, it showcases the ability to generalize across tasks, enabling capabilities like style transfer and free-form video editing without specific training on those tasks."
                },
                "zh": {
                    "title": "UniVideoï¼šè§†é¢‘ç”Ÿæˆä¸ç¼–è¾‘çš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "UniVideoæ˜¯ä¸€ä¸ªåŒæµæ¡†æ¶ï¼Œç»“åˆäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€DiTï¼Œæ‰©å±•äº†è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘çš„ç»Ÿä¸€å»ºæ¨¡ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå‡†ç¡®ç†è§£å¤æ‚çš„å¤šæ¨¡æ€æŒ‡ä»¤ï¼ŒåŒæ—¶ä¿æŒè§†è§‰ä¸€è‡´æ€§ã€‚UniVideoå°†å¤šç§è§†é¢‘ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ç»Ÿä¸€åœ¨ä¸€ä¸ªå¤šæ¨¡æ€æŒ‡ä»¤èŒƒå¼ä¸‹ï¼Œå¹¶é€šè¿‡è”åˆè®­ç»ƒå®ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniVideoåœ¨æ–‡æœ¬/å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€ä¸Šä¸‹æ–‡è§†é¢‘ç”Ÿæˆå’Œä¸Šä¸‹æ–‡è§†é¢‘ç¼–è¾‘ç­‰ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æˆ–è¶…è¿‡äº†æœ€å…ˆè¿›çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07172",
            "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM\n  Agents",
            "url": "https://huggingface.co/papers/2510.07172",
            "abstract": "NewtonBench is a benchmark for scientific law discovery that addresses scalability, scientific relevance, and memorization resistance by using metaphysical shifts and interactive model discovery.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are emerging as powerful tools for scientific law discovery, a foundational challenge in AI-driven science. However, existing benchmarks for this task suffer from a fundamental methodological trilemma, forcing a trade-off between scientific relevance, scalability, and resistance to memorization. Furthermore, they oversimplify discovery as static function fitting, failing to capture the authentic scientific process of uncovering embedded laws through the interactive exploration of complex model systems. To address these critical gaps, we introduce NewtonBench, a benchmark comprising 324 scientific law discovery tasks across 12 physics domains. Our design mitigates the evaluation trilemma by using metaphysical shifts - systematic alterations of canonical laws - to generate a vast suite of problems that are scalable, scientifically relevant, and memorization-resistant. Moreover, we elevate the evaluation from static function fitting to interactive model discovery, requiring agents to experimentally probe simulated complex systems to uncover hidden principles. Our extensive experiment reveals a clear but fragile capability for discovery in frontier LLMs: this ability degrades precipitously with increasing system complexity and exhibits extreme sensitivity to observational noise. Notably, we uncover a paradoxical effect of tool assistance: providing a code interpreter can hinder more capable models by inducing a premature shift from exploration to exploitation, causing them to satisfice on suboptimal solutions. These results demonstrate that robust, generalizable discovery in complex, interactive environments remains the core challenge. By providing a scalable, robust, and scientifically authentic testbed, NewtonBench offers a crucial tool for measuring true progress and guiding the development of next-generation AI agents capable of genuine scientific discovery.",
            "score": 10,
            "issue_id": 6345,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "9ee73390b8a846fc",
            "authors": [
                "Tianshi Zheng",
                "Kelvin Kiu-Wai Tam",
                "Newt Hue-Nam K. Nguyen",
                "Baixuan Xu",
                "Zhaowei Wang",
                "Jiayang Cheng",
                "Hong Ting Tsang",
                "Weiqi Wang",
                "Jiaxin Bai",
                "Tianqing Fang",
                "Yangqiu Song",
                "Ginny Y. Wong",
                "Simon See"
            ],
            "affiliations": [
                "NVIDIA",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07172.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ°ÑƒÑ‡Ğ½Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "NewtonBench â€” ÑÑ‚Ğ¾ benchmark Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 324 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ· 12 Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ñ…Ñ€ÑƒĞ¿ĞºĞ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸ÑĞ¼: Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ĞºĞ¾ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ ÑƒÑĞ»Ğ¾Ğ¶Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸĞ°Ñ€Ğ°Ğ´Ğ¾ĞºÑĞ°Ğ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ€Ğ¾Ğ´Ğµ code interpreter Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½Ğ°Ğ²Ñ€ĞµĞ´Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ñ… ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ñ€Ğ°Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒÑÑ Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "NewtonBench: Advancing AI in Scientific Law Discovery",
                    "desc": "NewtonBench is a new benchmark designed to improve the process of discovering scientific laws using AI. It addresses key issues like scalability, scientific relevance, and the risk of models simply memorizing data instead of learning. By introducing metaphysical shifts, it creates a wide range of tasks that require interactive exploration rather than just fitting functions to data. The findings highlight the challenges faced by large language models in complex environments, emphasizing the need for better tools to support genuine scientific discovery."
                },
                "zh": {
                    "title": "NewtonBenchï¼šç§‘å­¦å®šå¾‹å‘ç°çš„æ–°åŸºå‡†",
                    "desc": "NewtonBenchæ˜¯ä¸€ä¸ªç”¨äºç§‘å­¦å®šå¾‹å‘ç°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è§£å†³å¯æ‰©å±•æ€§ã€ç§‘å­¦ç›¸å…³æ€§å’ŒæŠµæŠ—è®°å¿†åŒ–çš„é—®é¢˜ã€‚å®ƒé€šè¿‡ä½¿ç”¨å½¢è€Œä¸Šå­¦çš„è½¬å˜å’Œäº’åŠ¨æ¨¡å‹å‘ç°çš„æ–¹æ³•ï¼Œæä¾›äº†324ä¸ªç§‘å­¦å®šå¾‹å‘ç°ä»»åŠ¡ï¼Œæ¶µç›–12ä¸ªç‰©ç†é¢†åŸŸã€‚ä¸ç°æœ‰åŸºå‡†ä¸åŒï¼ŒNewtonBenchå¼ºè°ƒä»é™æ€å‡½æ•°æ‹Ÿåˆè½¬å‘äº’åŠ¨æ¨¡å‹å‘ç°ï¼Œè¦æ±‚æ™ºèƒ½ä½“é€šè¿‡å®éªŒæ¢æµ‹å¤æ‚ç³»ç»Ÿä»¥æ­ç¤ºéšè—çš„åŸåˆ™ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å‘ç°èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç³»ç»Ÿå¤æ‚æ€§å¢åŠ å’Œè§‚å¯Ÿå™ªå£°å½±å“ä¸‹ï¼Œå…¶èƒ½åŠ›ä¼šæ˜¾è‘—ä¸‹é™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03663",
            "title": "UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG",
            "url": "https://huggingface.co/papers/2510.03663",
            "abstract": "UniDoc-Bench is a large-scale benchmark for multimodal retrieval-augmented generation, evaluating systems across text, images, and their fusion in real-world document-centric scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal retrieval-augmented generation (MM-RAG) is a key approach for applying large language models (LLMs) and agents to real-world knowledge bases, yet current evaluations are fragmented, focusing on either text or images in isolation or on simplified multimodal setups that fail to capture document-centric multimodal use cases. In this paper, we introduce UniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from 70k real-world PDF pages across eight domains. Our pipeline extracts and links evidence from text, tables, and figures, then generates 1,600 multimodal QA pairs spanning factual retrieval, comparison, summarization, and logical reasoning queries. To ensure reliability, 20% of QA pairs are validated by multiple annotators and expert adjudication. UniDoc-Bench supports apples-to-apples comparison across four paradigms: (1) text-only, (2) image-only, (3) multimodal text-image fusion, and (4) multimodal joint retrieval -- under a unified protocol with standardized candidate pools, prompts, and evaluation metrics. Our experiments show that multimodal text-image fusion RAG systems consistently outperform both unimodal and jointly multimodal embedding-based retrieval, indicating that neither text nor images alone are sufficient and that current multimodal embeddings remain inadequate. Beyond benchmarking, our analysis reveals when and how visual context complements textual evidence, uncovers systematic failure modes, and offers actionable guidance for developing more robust MM-RAG pipelines.",
            "score": 9,
            "issue_id": 6344,
            "pub_date": "2025-10-04",
            "pub_date_card": {
                "ru": "4 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 4",
                "zh": "10æœˆ4æ—¥"
            },
            "hash": "25c006b7c90bf61e",
            "authors": [
                "Xiangyu Peng",
                "Can Qin",
                "Zeyuan Chen",
                "Ran Xu",
                "Caiming Xiong",
                "Chien-Sheng Wu"
            ],
            "affiliations": [
                "Salesforce AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03663.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#survey",
                    "#rag",
                    "#multimodal",
                    "#reasoning",
                    "#games"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ RAG: Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğµ ÑĞ¸Ğ»ÑŒĞ½ĞµĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UniDoc-Bench â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ retrieval-augmented generation (RAG), Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 70 Ñ‚Ñ‹ÑÑÑ‡ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… PDF-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¸Ğ· Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1600 Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ², ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ, ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… MM-RAG ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Unlocking the Power of Multimodal Retrieval with UniDoc-Bench",
                    "desc": "UniDoc-Bench is a comprehensive benchmark designed for evaluating multimodal retrieval-augmented generation (MM-RAG) systems that utilize both text and images. It addresses the limitations of existing evaluations by providing a realistic dataset derived from 70,000 real-world PDF pages across various domains. The benchmark includes 1,600 multimodal question-answer pairs that cover a range of tasks such as factual retrieval and logical reasoning, with a focus on ensuring quality through expert validation. Results indicate that systems leveraging multimodal text-image fusion significantly outperform those relying solely on text or images, highlighting the importance of integrating visual context with textual information."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆçš„åŸºå‡†æµ‹è¯•æ–°æ ‡å‡†",
                    "desc": "UniDoc-Benchæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºå¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMM-RAGï¼‰ï¼Œè¯„ä¼°æ–‡æœ¬ã€å›¾åƒåŠå…¶èåˆåœ¨çœŸå®æ–‡æ¡£åœºæ™¯ä¸­çš„è¡¨ç°ã€‚è¯¥åŸºå‡†ç”±70,000ä¸ªçœŸå®ä¸–ç•Œçš„PDFé¡µé¢æ„æˆï¼Œæ¶µç›–å…«ä¸ªé¢†åŸŸï¼Œæä¾›äº†1,600ä¸ªå¤šæ¨¡æ€é—®ç­”å¯¹ï¼Œæ¶‰åŠäº‹å®æ£€ç´¢ã€æ¯”è¾ƒã€æ‘˜è¦å’Œé€»è¾‘æ¨ç†ç­‰ä»»åŠ¡ã€‚é€šè¿‡ç»Ÿä¸€çš„åè®®å’Œæ ‡å‡†åŒ–çš„è¯„ä¼°æŒ‡æ ‡ï¼ŒUniDoc-Benchæ”¯æŒå››ç§æ¯”è¾ƒæ–¹å¼ï¼šä»…æ–‡æœ¬ã€ä»…å›¾åƒã€å¤šæ¨¡æ€æ–‡æœ¬-å›¾åƒèåˆå’Œå¤šæ¨¡æ€è”åˆæ£€ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ–‡æœ¬-å›¾åƒèåˆçš„RAGç³»ç»Ÿåœ¨æ€§èƒ½ä¸Šä¼˜äºå•æ¨¡æ€å’Œè”åˆå¤šæ¨¡æ€çš„æ£€ç´¢æ–¹æ³•ï¼Œå¼ºè°ƒäº†æ–‡æœ¬å’Œå›¾åƒçš„ç»“åˆåœ¨ä¿¡æ¯æ£€ç´¢ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08143",
            "title": "UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video\n  Super-Resolution",
            "url": "https://huggingface.co/papers/2510.08143",
            "abstract": "UniMMVSR is a unified generative video super-resolution framework that incorporates hybrid-modal conditions, including text, images, and videos, within a latent video diffusion model, achieving superior detail and conformity to multi-modal conditions.  \t\t\t\t\tAI-generated summary \t\t\t\t Cascaded video super-resolution has emerged as a promising technique for decoupling the computational burden associated with generating high-resolution videos using large foundation models. Existing studies, however, are largely confined to text-to-video tasks and fail to leverage additional generative conditions beyond text, which are crucial for ensuring fidelity in multi-modal video generation. We address this limitation by presenting UniMMVSR, the first unified generative video super-resolution framework to incorporate hybrid-modal conditions, including text, images, and videos. We conduct a comprehensive exploration of condition injection strategies, training schemes, and data mixture techniques within a latent video diffusion model. A key challenge was designing distinct data construction and condition utilization methods to enable the model to precisely utilize all condition types, given their varied correlations with the target video. Our experiments demonstrate that UniMMVSR significantly outperforms existing methods, producing videos with superior detail and a higher degree of conformity to multi-modal conditions. We also validate the feasibility of combining UniMMVSR with a base model to achieve multi-modal guided generation of 4K video, a feat previously unattainable with existing techniques.",
            "score": 8,
            "issue_id": 6345,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "141d62a733cd05ae",
            "authors": [
                "Shian Du",
                "Menghan Xia",
                "Chang Liu",
                "Quande Liu",
                "Xintao Wang",
                "Pengfei Wan",
                "Xiangyang Ji"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Kling Team, Kuaishou Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08143.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ¿ÑĞºĞµĞ¹Ğ»Ğ¸Ğ½Ğ³ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ¾ 4K Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ",
                    "desc": "UniMMVSR â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ¿ÑĞºĞµĞ¹Ğ»Ğ¸Ğ½Ğ³Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸: Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ latent diffusion Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹, ÑÑ…ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ UniMMVSR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 4K Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹."
                },
                "en": {
                    "title": "UniMMVSR: Elevating Video Quality with Multi-Modal Inputs",
                    "desc": "UniMMVSR is a novel framework for enhancing video quality by generating high-resolution videos from various input types, such as text, images, and videos. It utilizes a latent video diffusion model to effectively combine these different modalities, ensuring that the generated videos maintain high detail and fidelity. The framework addresses previous limitations by exploring various strategies for integrating multiple conditions during training, allowing for better utilization of diverse data types. Experiments show that UniMMVSR outperforms existing methods, achieving impressive results in multi-modal video generation, including the ability to produce 4K videos."
                },
                "zh": {
                    "title": "ç»Ÿä¸€ç”Ÿæˆè§†é¢‘è¶…åˆ†è¾¨ç‡ï¼Œæå‡å¤šæ¨¡æ€ä¸€è‡´æ€§",
                    "desc": "UniMMVSRæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç”Ÿæˆè§†é¢‘è¶…åˆ†è¾¨ç‡æ¡†æ¶ï¼Œèƒ½å¤Ÿç»“åˆæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç­‰å¤šç§æ¨¡æ€æ¡ä»¶ã€‚è¯¥æ¡†æ¶ä½¿ç”¨æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆè§†é¢‘çš„ç»†èŠ‚å’Œå¤šæ¨¡æ€æ¡ä»¶çš„ç¬¦åˆåº¦ã€‚æˆ‘ä»¬æ¢ç´¢äº†æ¡ä»¶æ³¨å…¥ç­–ç•¥ã€è®­ç»ƒæ–¹æ¡ˆå’Œæ•°æ®æ··åˆæŠ€æœ¯ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åˆ©ç”¨ä¸åŒç±»å‹çš„æ¡ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniMMVSRåœ¨ç”Ÿæˆè§†é¢‘çš„ç»†èŠ‚å’Œå¤šæ¨¡æ€ä¸€è‡´æ€§æ–¹é¢ï¼Œæ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.03222",
            "title": "Low-probability Tokens Sustain Exploration in Reinforcement Learning\n  with Verifiable Reward",
            "url": "https://huggingface.co/papers/2510.03222",
            "abstract": "Low-probability Regularization (Lp-Reg) enhances exploration in Reinforcement Learning with Verifiable Rewards (RLVR) by preserving valuable low-probability tokens, leading to improved performance in complex reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large Language Models in complex reasoning, yet its scalability is often hindered by a training bottleneck where performance plateaus as policy entropy collapses, signaling a loss of exploration. Previous methods typically address this by maintaining high policy entropy, yet the precise mechanisms that govern meaningful exploration have remained underexplored. Our analysis suggests that an unselective focus on entropy risks amplifying irrelevant tokens and destabilizing training. This paper investigates the exploration dynamics within RLVR and identifies a key issue: the gradual elimination of valuable low-probability exploratory tokens, which we term \\textit{reasoning sparks}. We find that while abundant in pre-trained models, these sparks are systematically extinguished during RLVR due to over-penalization, leading to a degeneracy in exploration. To address this, we introduce Low-probability Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a heuristic proxy distribution. This proxy is constructed by filtering out presumed noise tokens and re-normalizing the distribution over the remaining candidates. The result is a less-noisy proxy where the probability of reasoning sparks is amplified, which then serves as a soft regularization target to shield these valuable tokens from elimination via KL divergence. Experiments show that Lp-Reg enables stable on-policy training for around 1,000 steps, a regime where baseline entropy-control methods collapse. This sustained exploration leads to state-of-the-art performance, achieving a 60.17% average accuracy on five math benchmarks, an improvement of 2.66% over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.",
            "score": 7,
            "issue_id": 6345,
            "pub_date": "2025-10-03",
            "pub_date_card": {
                "ru": "3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 3",
                "zh": "10æœˆ3æ—¥"
            },
            "hash": "f829f1939095d3f9",
            "authors": [
                "Guanhua Huang",
                "Tingqiang Xu",
                "Mingze Wang",
                "Qi Yi",
                "Xue Gong",
                "Siheng Li",
                "Ruibin Xiong",
                "Kejiao Li",
                "Yuhao Jiang",
                "Bo Zhou"
            ],
            "affiliations": [
                "LLM Department, Tencent",
                "Peking University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.03222.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#optimization",
                    "#math",
                    "#reasoning"
                ],
                "emoji": "âœ¨",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ñ€ĞµĞ´ĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ LLM: Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ´ĞºĞ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Â«Ğ¸ÑĞºÑ€Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹Â», ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑÑ‡ĞµĞ·Ğ°ÑÑ‚ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹, Ñ‚Ğ°Ğº ĞºĞ°Ğº ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ´ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Low-probability Regularization Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ, Ğ³Ğ´Ğµ Ğ¸Ñ… Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² 1000 ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 60.17% Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ½Ğ° 2.66% Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Exploration with Low-Probability Regularization",
                    "desc": "This paper introduces Low-probability Regularization (Lp-Reg) to improve exploration in Reinforcement Learning with Verifiable Rewards (RLVR). It identifies that valuable low-probability tokens, or 'reasoning sparks', are often lost during training due to excessive penalties on policy entropy. Lp-Reg addresses this by creating a proxy distribution that emphasizes these low-probability tokens, allowing for better exploration and stability in training. The results demonstrate that Lp-Reg significantly enhances performance on complex reasoning tasks, achieving state-of-the-art accuracy on multiple benchmarks."
                },
                "zh": {
                    "title": "ä½æ¦‚ç‡æ­£åˆ™åŒ–ï¼šæå‡å¼ºåŒ–å­¦ä¹ çš„æ¢ç´¢èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºä½æ¦‚ç‡æ­£åˆ™åŒ–ï¼ˆLp-Regï¼‰çš„æ–¹æ³•ï¼Œä»¥å¢å¼ºå¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ¡†æ¶ä¸‹ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨RLVRè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½æ¦‚ç‡çš„æ¢ç´¢æ€§æ ‡è®°ï¼ˆç§°ä¸ºæ¨ç†ç«èŠ±ï¼‰ä¼šé€æ¸è¢«æ¶ˆé™¤ï¼Œå¯¼è‡´æ¢ç´¢èƒ½åŠ›ä¸‹é™ã€‚Lp-Regé€šè¿‡å¯¹ç­–ç•¥è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä¿ç•™è¿™äº›æœ‰ä»·å€¼çš„ä½æ¦‚ç‡æ ‡è®°ï¼Œä»è€Œæ”¹å–„å¤æ‚æ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨Lp-Regå¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒç¨³å®šçš„æ¢ç´¢ï¼Œæ˜¾è‘—æé«˜æ¨¡å‹åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08431",
            "title": "Large Scale Diffusion Distillation via Score-Regularized Continuous-Time\n  Consistency",
            "url": "https://huggingface.co/papers/2510.08431",
            "abstract": "Score-regularized continuous-time consistency model (rCM) improves large-scale diffusion distillation by addressing fine-detail generation and diversity issues, achieving high fidelity and accelerated sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t This work represents the first effort to scale up continuous-time consistency distillation to general application-level image and video diffusion models. Although continuous-time consistency model (sCM) is theoretically principled and empirically powerful for accelerating academic-scale diffusion, its applicability to large-scale text-to-image and video tasks remains unclear due to infrastructure challenges in Jacobian-vector product (JVP) computation and the limitations of standard evaluation benchmarks. We first develop a parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on models with over 10 billion parameters and high-dimensional video tasks. Our investigation reveals fundamental quality limitations of sCM in fine-detail generation, which we attribute to error accumulation and the \"mode-covering\" nature of its forward-divergence objective. To remedy this, we propose the score-regularized continuous-time consistency model (rCM), which incorporates score distillation as a long-skip regularizer. This integration complements sCM with the \"mode-seeking\" reverse divergence, effectively improving visual quality while maintaining high generation diversity. Validated on large-scale models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM matches or surpasses the state-of-the-art distillation method DMD2 on quality metrics while offering notable advantages in diversity, all without GAN tuning or extensive hyperparameter searches. The distilled models generate high-fidelity samples in only 1sim4 steps, accelerating diffusion sampling by 15timessim50times. These results position rCM as a practical and theoretically grounded framework for advancing large-scale diffusion distillation.",
            "score": 4,
            "issue_id": 6345,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "d389552144d07b65",
            "authors": [
                "Kaiwen Zheng",
                "Yuji Wang",
                "Qianli Ma",
                "Huayu Chen",
                "Jintao Zhang",
                "Yogesh Balaji",
                "Jianfei Chen",
                "Ming-Yu Liu",
                "Jun Zhu",
                "Qinsheng Zhang"
            ],
            "affiliations": [
                "NVIDIA",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08431.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#training",
                    "#cv",
                    "#benchmark",
                    "#diffusion",
                    "#video",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ² 50 Ñ€Ğ°Ğ· Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ rCM (score-regularized continuous-time consistency model) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ¸ Ñ€ĞµÑˆĞ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° sCM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ‚ĞµÑ€ÑĞ» Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸. Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ² score distillation ĞºĞ°Ğº Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ» ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ» Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ¾ 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 1-4 ÑˆĞ°Ğ³Ğ°, ÑƒÑĞºĞ¾Ñ€ÑÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ² 15-50 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Diffusion Distillation with rCM for High-Quality Outputs",
                    "desc": "The Score-regularized continuous-time consistency model (rCM) enhances large-scale diffusion distillation by improving the generation of fine details and diversity in images and videos. It addresses the limitations of the existing continuous-time consistency model (sCM) by introducing a new regularization technique that helps in better quality generation while maintaining variety. The rCM is designed to work efficiently with large models, enabling training on complex tasks without the need for extensive tuning. This approach significantly accelerates the sampling process, achieving high fidelity in generated outputs with fewer steps compared to previous methods."
                },
                "zh": {
                    "title": "å¾—åˆ†æ­£åˆ™åŒ–æ¨¡å‹ï¼šæå‡æ‰©æ•£è’¸é¦çš„è´¨é‡ä¸å¤šæ ·æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹ï¼Œç§°ä¸ºå¾—åˆ†æ­£åˆ™åŒ–è¿ç»­æ—¶é—´ä¸€è‡´æ€§æ¨¡å‹ï¼ˆrCMï¼‰ï¼Œæ—¨åœ¨æ”¹å–„å¤§è§„æ¨¡æ‰©æ•£è’¸é¦ä¸­çš„ç»†èŠ‚ç”Ÿæˆå’Œå¤šæ ·æ€§é—®é¢˜ã€‚rCMé€šè¿‡å¼•å…¥å¾—åˆ†è’¸é¦ä½œä¸ºé•¿è·³è·ƒæ­£åˆ™åŒ–å™¨ï¼Œå¢å¼ºäº†åŸæœ‰çš„è¿ç»­æ—¶é—´ä¸€è‡´æ€§æ¨¡å‹ï¼ˆsCMï¼‰ï¼Œä»è€Œæé«˜äº†è§†è§‰è´¨é‡å¹¶ä¿æŒäº†ç”Ÿæˆçš„å¤šæ ·æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒrCMåœ¨å¤„ç†è¶…è¿‡100äº¿å‚æ•°çš„å¤§è§„æ¨¡æ¨¡å‹å’Œé«˜ç»´è§†é¢‘ä»»åŠ¡æ—¶ï¼Œèƒ½å¤Ÿåœ¨è´¨é‡æŒ‡æ ‡ä¸Šä¸æœ€å…ˆè¿›çš„è’¸é¦æ–¹æ³•DMD2ç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚æœ€ç»ˆï¼ŒrCMæ˜¾è‘—åŠ å¿«äº†æ‰©æ•£é‡‡æ ·é€Ÿåº¦ï¼Œæå‡äº†ç”Ÿæˆæ ·æœ¬çš„ä¿çœŸåº¦ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤§è§„æ¨¡æ‰©æ•£è’¸é¦ä¸­çš„å®ç”¨æ€§å’Œç†è®ºåŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08308",
            "title": "First Try Matters: Revisiting the Role of Reflection in Reasoning Models",
            "url": "https://huggingface.co/papers/2510.08308",
            "abstract": "Analysis of reflective behaviors in reasoning models shows that reflections primarily confirm initial answers, and training with more reflections improves first-answer correctness; a question-aware early-stopping method reduces unnecessary reflections and tokens with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have recently demonstrated significant gains in reasoning ability, often attributed to their capacity to generate longer chains of thought and engage in reflective reasoning. However, the contribution of reflections to performance improvement remains unclear. In this paper, we systematically analyze the rollouts of eight reasoning models on five mathematical datasets. We focus on reflective behaviours where the model has already produced an answer but continues reflecting before finalizing its output. Our analysis reveals that reflections are predominantly confirmatory and rarely alter the model's initial answer, a pattern consistent across models and datasets. To understand the role of reflections in training, we construct supervised fine-tuning (SFT) datasets with varying amounts of reflection steps. We observe that training models on rollouts with more reflection steps primarily enhances first-answer correctness rather than the ability to correct initially wrong answers through reflections. This motivates us to propose a question-aware early-stopping method that enhances inference-time token efficiency by stopping the reasoning process once a few plausible candidate answers are generated, thereby reducing unnecessary reflection steps. Motivated by this, we further propose to dynamically truncate the reflections after a candidate answer has appeared during generation, which reduces reasoning tokens by 24.5% across five mathematical datasets, within a 2.9% drop in accuracy.",
            "score": 4,
            "issue_id": 6344,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "4a9143a437581621",
            "authors": [
                "Liwei Kang",
                "Yue Deng",
                "Yao Xiao",
                "Zhanfeng Mo",
                "Wee Sun Lee",
                "Lidong Bing"
            ],
            "affiliations": [
                "MiroMind AI",
                "National University of Singapore",
                "Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08308.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#inference",
                    "#data",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸª",
                "ru": {
                    "title": "Ğ ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ LLM Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ğ° Ğ½Ğµ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ½Ğ½ÑÑ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ² reasoning-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ° Ğ½Ğµ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¸Ñ…. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ğ½Ğ¾ Ğ½Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ½Ğ½ĞµĞ¹ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 24.5% Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 2.9%."
                },
                "en": {
                    "title": "Enhancing Reasoning Efficiency with Reflective Training",
                    "desc": "This paper investigates how reflective behaviors in reasoning models affect their performance, particularly in confirming initial answers. It finds that while reflections often do not change the first answer, training with more reflection steps improves the correctness of these initial answers. The authors introduce a question-aware early-stopping method to minimize unnecessary reflections and reduce token usage during inference. This method effectively decreases reasoning tokens by 24.5% with only a slight accuracy drop of 2.9%."
                },
                "zh": {
                    "title": "åæ€æå‡åˆå§‹ç­”æ¡ˆçš„æ­£ç¡®æ€§",
                    "desc": "æœ¬æ–‡åˆ†æäº†æ¨ç†æ¨¡å‹ä¸­çš„åæ€è¡Œä¸ºï¼Œå‘ç°åæ€ä¸»è¦æ˜¯ç¡®è®¤åˆå§‹ç­”æ¡ˆï¼Œè€Œä¸æ˜¯æ”¹å˜å®ƒã€‚é€šè¿‡å¯¹å…«ä¸ªæ¨ç†æ¨¡å‹åœ¨äº”ä¸ªæ•°å­¦æ•°æ®é›†ä¸Šçš„è¡¨ç°è¿›è¡Œç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬å‘ç°æ›´å¤šçš„åæ€æ­¥éª¤å¯ä»¥æé«˜åˆå§‹ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé—®é¢˜çš„æ—©åœæ–¹æ³•ï¼Œå¯ä»¥åœ¨ç”Ÿæˆå‡ ä¸ªåˆç†å€™é€‰ç­”æ¡ˆååœæ­¢æ¨ç†ï¼Œä»è€Œå‡å°‘ä¸å¿…è¦çš„åæ€æ­¥éª¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨å‡å°‘æ¨ç†ä»¤ç‰Œçš„åŒæ—¶ï¼Œä»…æœ‰è½»å¾®çš„å‡†ç¡®æ€§ä¸‹é™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08555",
            "title": "VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal\n  Patches via In-Context Conditioning",
            "url": "https://huggingface.co/papers/2510.08555",
            "abstract": "VideoCanvas addresses temporal ambiguity in latent video diffusion models to enable flexible spatio-temporal video completion using a hybrid conditioning strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce the task of arbitrary spatio-temporal video completion, where a video is generated from arbitrary, user-specified patches placed at any spatial location and timestamp, akin to painting on a video canvas. This flexible formulation naturally unifies many existing controllable video generation tasks--including first-frame image-to-video, inpainting, extension, and interpolation--under a single, cohesive paradigm. Realizing this vision, however, faces a fundamental obstacle in modern latent video diffusion models: the temporal ambiguity introduced by causal VAEs, where multiple pixel frames are compressed into a single latent representation, making precise frame-level conditioning structurally difficult. We address this challenge with VideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC) paradigm to this fine-grained control task with zero new parameters. We propose a hybrid conditioning strategy that decouples spatial and temporal control: spatial placement is handled via zero-padding, while temporal alignment is achieved through Temporal RoPE Interpolation, which assigns each condition a continuous fractional position within the latent sequence. This resolves the VAE's temporal ambiguity and enables pixel-frame-aware control on a frozen backbone. To evaluate this new capability, we develop VideoCanvasBench, the first benchmark for arbitrary spatio-temporal video completion, covering both intra-scene fidelity and inter-scene creativity. Experiments demonstrate that VideoCanvas significantly outperforms existing conditioning paradigms, establishing a new state of the art in flexible and unified video generation.",
            "score": 3,
            "issue_id": 6345,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "22786ba2a0b5f590",
            "authors": [
                "Minghong Cai",
                "Qiulin Wang",
                "Zongli Ye",
                "Wenze Liu",
                "Quande Liu",
                "Weicai Ye",
                "Xintao Wang",
                "Pengfei Wan",
                "Kun Gai",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "MMLab, The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08555.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#benchmark",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ñ…Ğ¾Ğ»ÑÑ‚: Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoCanvas â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ³Ğ´Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸ Ğ² Ğ»ÑĞ±Ñ‹Ñ… Ğ¼ĞµÑÑ‚Ğ°Ñ… ĞºĞ°Ğ´Ñ€Ğ° Ğ¸ Ğ½Ğ° Ğ»ÑĞ±Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ¼ĞµÑ‚ĞºĞ°Ñ…. Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ³Ğ´Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² ÑĞ¶Ğ¸Ğ¼Ğ°ÑÑ‚ÑÑ Ğ² Ğ¾Ğ´Ğ½Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ ÑÑ‚Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ conditioning: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· zero-padding Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Temporal RoPE Interpolation, Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ğ´Ñ€Ğ¾Ğ±Ğ½ÑƒÑ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡ controllable video generation (image-to-video, inpainting, extension, interpolation) Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VideoCanvasBench."
                },
                "en": {
                    "title": "Revolutionizing Video Generation with Flexible Spatio-Temporal Control",
                    "desc": "VideoCanvas is a framework designed to tackle the challenge of temporal ambiguity in latent video diffusion models, allowing for flexible video completion based on user-defined patches. It introduces a novel approach to spatio-temporal video generation, unifying various tasks like inpainting and interpolation under one system. The framework employs a hybrid conditioning strategy that separates spatial and temporal controls, using techniques like zero-padding for spatial placement and Temporal RoPE Interpolation for temporal alignment. Through the development of VideoCanvasBench, the framework is evaluated and shown to outperform existing methods, setting a new standard in video generation capabilities."
                },
                "zh": {
                    "title": "çµæ´»æ—¶ç©ºè§†é¢‘è¡¥å…¨çš„æ–°æ–¹æ³•",
                    "desc": "VideoCanvas è§£å†³äº†æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„æ—¶é—´æ¨¡ç³Šé—®é¢˜ï¼Œä»è€Œå®ç°çµæ´»çš„æ—¶ç©ºè§†é¢‘è¡¥å…¨ã€‚è¯¥æ–¹æ³•å…è®¸ç”¨æˆ·åœ¨ä»»æ„ç©ºé—´ä½ç½®å’Œæ—¶é—´æˆ³ç”Ÿæˆè§†é¢‘ï¼Œç±»ä¼¼äºåœ¨è§†é¢‘ç”»å¸ƒä¸Šç»˜ç”»ã€‚é€šè¿‡å¼•å…¥æ··åˆæ¡ä»¶ç­–ç•¥ï¼ŒVideoCanvas å°†ç©ºé—´å’Œæ—¶é—´æ§åˆ¶è§£è€¦ï¼Œå…‹æœäº†ç°ä»£æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„ç»“æ„æ€§æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoCanvas åœ¨çµæ´»å’Œç»Ÿä¸€çš„è§†é¢‘ç”Ÿæˆæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ¡ä»¶åŒ–èŒƒå¼ï¼Œå»ºç«‹äº†æ–°çš„æŠ€æœ¯é¢†å…ˆæ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08008",
            "title": "Recycling Pretrained Checkpoints: Orthogonal Growth of\n  Mixture-of-Experts for Efficient Large Language Model Pre-Training",
            "url": "https://huggingface.co/papers/2510.08008",
            "abstract": "Recycling pretrained checkpoints through orthogonal growth methods improves large language model performance with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapidly increasing computational cost of pretraining Large Language Models necessitates more efficient approaches. Numerous computational costs have been invested in existing well-trained checkpoints, but many of them remain underutilized due to engineering constraints or limited model capacity. To efficiently reuse this \"sunk\" cost, we propose to recycle pretrained checkpoints by expanding their parameter counts and continuing training. We propose orthogonal growth method well-suited for converged Mixture-of-Experts model: interpositional layer copying for depth growth and expert duplication with injected noise for width growth. To determine the optimal timing for such growth across checkpoints sequences, we perform comprehensive scaling experiments revealing that the final accuracy has a strong positive correlation with the amount of sunk cost, indicating that greater prior investment leads to better performance. We scale our approach to models with 70B parameters and over 1T training tokens, achieving 10.66% accuracy gain over training from scratch under the same additional compute budget. Our checkpoint recycling approach establishes a foundation for economically efficient large language model pretraining.",
            "score": 3,
            "issue_id": 6345,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "b56e81668ec4e66d",
            "authors": [
                "Ruizhe Wang",
                "Yucheng Ding",
                "Xiao Liu",
                "Yaoxiang Wang",
                "Peng Cheng",
                "Baining Guo",
                "Zhengjun Zha",
                "Yeyun Gong"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "Shanghai Jiao Tong University",
                "University of Science and Technology of China",
                "Xiamen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08008.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "â™»ï¸",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ¾Ğ²: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… LLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑƒĞ¶Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ¾ÑÑ‚Ğ° Ğ´Ğ»Ñ Mixture-of-Experts Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ñ‘Ğ² Ğ´Ğ»Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑˆÑƒĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ¾ 70B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‡ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² ÑƒĞ¶Ğµ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¾ Ğ² Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚, Ñ‚ĞµĞ¼ Ğ»ÑƒÑ‡ÑˆĞµ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ°Ñ‘Ñ‚ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 10.66% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ½ÑƒĞ»Ñ Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Recycle Checkpoints for Efficient LLM Growth!",
                    "desc": "This paper discusses a method to improve the performance of large language models (LLMs) while reducing the computational costs associated with their pretraining. The authors propose recycling pretrained checkpoints by expanding their parameters and continuing training, which allows for better utilization of previously invested resources. They introduce an orthogonal growth method that includes techniques for both depth and width expansion of models, specifically designed for Mixture-of-Experts architectures. Their experiments show that models with more prior investment yield higher accuracy, demonstrating that this approach can lead to significant performance gains without the need for extensive new training resources."
                },
                "zh": {
                    "title": "å›æ”¶é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ï¼Œæå‡æ¨¡å‹æ€§èƒ½ä¸æ•ˆç‡",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é€šè¿‡æ­£äº¤å¢é•¿æ–¹æ³•å›æ”¶é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒè®¡ç®—æˆæœ¬çš„è¿…é€Ÿå¢åŠ ï¼Œç°æœ‰çš„é¢„è®­ç»ƒæ£€æŸ¥ç‚¹å¾€å¾€ç”±äºå·¥ç¨‹é™åˆ¶æˆ–æ¨¡å‹å®¹é‡ä¸è¶³è€Œæœªè¢«å……åˆ†åˆ©ç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ‰©å±•å‚æ•°æ•°é‡å¹¶ç»§ç»­è®­ç»ƒï¼Œæ¥æœ‰æ•ˆåœ°é‡ç”¨è¿™äº›â€œæ²‰æ²¡â€æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¢åŠ çš„æŠ•èµ„ä¸æœ€ç»ˆå‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨å¼ºæ­£ç›¸å…³å…³ç³»ï¼Œä»è€Œä¸ºç»æµé«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒå¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07429",
            "title": "Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs",
            "url": "https://huggingface.co/papers/2510.07429",
            "abstract": "BaRP, a Bandit-feedback Routing with Preferences approach, optimizes large language model selection in an online setting with partial feedback, outperforming offline routers and large models.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficient use of large language models (LLMs) is critical for deployment at scale: without adaptive routing, systems either overpay for strong models or risk poor performance from weaker ones. Selecting the right LLM for each query is fundamentally an online decision problem: models differ in strengths, prices fluctuate, and users value accuracy and cost differently. Yet most routers are trained offline with labels for all candidate models, an assumption that breaks in deployment, where only the outcome of the chosen model is observed. We bridge this gap with BaRP, a Bandit-feedback Routing with Preferences approach that trains under the same partial-feedback restriction as deployment, while supporting preference-tunable inference: operators can dial the performance/cost trade-off at test time without retraining. Framed as a contextual bandit over prompt features and a user preference vector, our method simulates an online feedback setting during training and adapts its routing decisions to each new prompt, rather than depending on full-information offline supervision. Comprehensive experiments show that our method consistently outperforms strong offline routers by at least 12.46% and the largest LLM by at least 2.45%, and generalizes robustly for unseen tasks.",
            "score": 3,
            "issue_id": 6344,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "a22ba38f76ba84ee",
            "authors": [
                "Wang Wei",
                "Tiankai Yang",
                "Hongjie Chen",
                "Yue Zhao",
                "Franck Dernoncourt",
                "Ryan A. Rossi",
                "Hoda Eldardiry"
            ],
            "affiliations": [
                "Adobe Research",
                "Dolby Labs",
                "University of Southern California",
                "Virginia Tech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07429.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ°",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ…Ğ¾Ğ´Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BaRP â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ñ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ¾ÑƒÑ‚ĞµÑ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ğ±Ğ¾ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, BaRP Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº contextual bandit Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ñ… Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğº ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ½Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ñ€Ğ¾ÑƒÑ‚ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ½Ğ° 12.46% Ğ¸ Ğ½Ğ°Ğ´ ÑĞ°Ğ¼Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ LLM Ğ½Ğ° 2.45%."
                },
                "en": {
                    "title": "Optimize LLM Selection with BaRP: Smart, Adaptive, and Cost-Effective!",
                    "desc": "BaRP is a novel approach that optimizes the selection of large language models (LLMs) in real-time using a bandit-feedback mechanism. It addresses the challenge of choosing the right model based on partial feedback, which is common in practical applications. By allowing operators to adjust the balance between performance and cost dynamically, BaRP enhances decision-making without needing to retrain the models. Experimental results demonstrate that BaRP significantly outperforms traditional offline routers and even the largest LLMs, making it a robust solution for adaptive model selection."
                },
                "zh": {
                    "title": "æ™ºèƒ½é€‰æ‹©ï¼Œä¼˜åŒ–æ¨¡å‹æ€§èƒ½",
                    "desc": "BaRPæ˜¯ä¸€ç§åŸºäºåå¥½çš„å¸¦åé¦ˆè·¯ç”±æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„é€‰æ‹©ã€‚å®ƒåœ¨åœ¨çº¿ç¯å¢ƒä¸­å¤„ç†éƒ¨åˆ†åé¦ˆï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Œé¿å…äº†è¿‡åº¦æ”¯ä»˜æˆ–æ€§èƒ½ä¸ä½³çš„é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„ç¦»çº¿è·¯ç”±å™¨ä¸åŒï¼ŒBaRPåœ¨è®­ç»ƒæ—¶æ¨¡æ‹Ÿåœ¨çº¿åé¦ˆï¼Œæ”¯æŒåœ¨æµ‹è¯•æ—¶æ ¹æ®ç”¨æˆ·åå¥½è°ƒæ•´æ€§èƒ½å’Œæˆæœ¬çš„æƒè¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBaRPåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå‡ä¼˜äºå¼ºå¤§çš„ç¦»çº¿è·¯ç”±å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.06915",
            "title": "LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling",
            "url": "https://huggingface.co/papers/2510.06915",
            "abstract": "A benchmark and training strategy for reward models to improve long-context consistency and performance in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward model (RM) plays a pivotal role in aligning large language model (LLM) with human preferences. As real-world applications increasingly involve long history trajectories, e.g., LLM agent, it becomes indispensable to evaluate whether a model's responses are not only high-quality but also grounded in and consistent with the provided context. Yet, current RMs remain confined to short-context settings and primarily focus on response-level attributes (e.g., safety or helpfulness), while largely neglecting the critical dimension of long context-response consistency. In this work, we introduce Long-RewardBench, a benchmark specifically designed for long-context RM evaluation, featuring both Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that even state-of-the-art generative RMs exhibit significant fragility in long-context scenarios, failing to maintain context-aware preference judgments. Motivated by the analysis of failure patterns observed in model outputs, we propose a general multi-stage training strategy that effectively scales arbitrary models into robust Long-context RMs (LongRMs). Experiments show that our approach not only substantially improves performance on long-context evaluation but also preserves strong short-context capability. Notably, our 8B LongRM outperforms much larger 70B-scale baselines and matches the performance of the proprietary Gemini 2.5 Pro model.",
            "score": 3,
            "issue_id": 6344,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 8",
                "zh": "10æœˆ8æ—¥"
            },
            "hash": "f89f427922a20c18",
            "authors": [
                "Zecheng Tang",
                "Baibei Ji",
                "Quantong Qiu",
                "Haitian Wang",
                "Xiaobo Liang",
                "Juntao Li",
                "Min Zhang"
            ],
            "affiliations": [
                "LCM Laboratory",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.06915.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#alignment",
                    "#long_context"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Long-RewardBench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ reward models Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ reward models Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»ÑĞ±Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ñ…. Ğ˜Ñ… 8B Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ 70B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ° Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ğ¾Ğ¹ Gemini 2.5 Pro."
                },
                "en": {
                    "title": "Enhancing Long-Context Consistency in Reward Models",
                    "desc": "This paper addresses the limitations of reward models (RMs) in large language models (LLMs) when dealing with long-context scenarios. It introduces Long-RewardBench, a new benchmark for evaluating RMs specifically designed for long-context consistency and performance. The authors identify that existing RMs struggle with maintaining context-aware preferences in lengthy interactions. To overcome this, they propose a multi-stage training strategy that enhances the robustness of RMs for long contexts while retaining their effectiveness in short contexts, leading to improved performance even against larger models."
                },
                "zh": {
                    "title": "æå‡é•¿ä¸Šä¸‹æ–‡ä¸€è‡´æ€§çš„å¥–åŠ±æ¨¡å‹ç­–ç•¥",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†å’Œè®­ç»ƒç­–ç•¥ï¼Œç”¨äºå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­çš„ä¸€è‡´æ€§å’Œæ€§èƒ½ã€‚å½“å‰çš„å¥–åŠ±æ¨¡å‹ä¸»è¦é›†ä¸­åœ¨çŸ­ä¸Šä¸‹æ–‡è®¾ç½®ï¼Œå¿½è§†äº†é•¿ä¸Šä¸‹æ–‡ä¸å“åº”ä¸€è‡´æ€§çš„é‡è¦æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†Long-RewardBenchåŸºå‡†ï¼Œä¸“é—¨ç”¨äºé•¿ä¸Šä¸‹æ–‡çš„RMè¯„ä¼°ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥å¢å¼ºæ¨¡å‹åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­çš„é²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†é•¿ä¸Šä¸‹æ–‡è¯„ä¼°çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†çŸ­ä¸Šä¸‹æ–‡çš„å¼ºå¤§èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08425",
            "title": "Reinforcing Diffusion Models by Direct Group Preference Optimization",
            "url": "https://huggingface.co/papers/2510.08425",
            "abstract": "DGPO, a new online RL algorithm, enhances diffusion models by learning from group-level preferences, enabling the use of efficient deterministic ODE samplers and achieving faster training and superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t While reinforcement learning methods such as Group Relative Preference Optimization (GRPO) have significantly enhanced Large Language Models, adapting them to diffusion models remains challenging. In particular, GRPO demands a stochastic policy, yet the most cost-effective diffusion samplers are based on deterministic ODEs. Recent work addresses this issue by using inefficient SDE-based samplers to induce stochasticity, but this reliance on model-agnostic Gaussian noise leads to slow convergence. To resolve this conflict, we propose Direct Group Preference Optimization (DGPO), a new online RL algorithm that dispenses with the policy-gradient framework entirely. DGPO learns directly from group-level preferences, which utilize relative information of samples within groups. This design eliminates the need for inefficient stochastic policies, unlocking the use of efficient deterministic ODE samplers and faster training. Extensive results show that DGPO trains around 20 times faster than existing state-of-the-art methods and achieves superior performance on both in-domain and out-of-domain reward metrics. Code is available at https://github.com/Luo-Yihong/DGPO.",
            "score": 2,
            "issue_id": 6345,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "b99983698082fd03",
            "authors": [
                "Yihong Luo",
                "Tianyang Hu",
                "Jing Tang"
            ],
            "affiliations": [
                "CUHK (SZ)",
                "HKUST",
                "HKUST (GZ)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08425.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#rlhf",
                    "#games",
                    "#optimization"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ DGPO â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ…. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ‚Ğ¸Ğ¿Ğ° GRPO, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğµ SDE-ÑÑĞ¼Ğ¿Ğ»ĞµÑ€Ñ‹, DGPO Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ODE-ÑÑĞ¼Ğ¿Ğ»ĞµÑ€Ñ‹. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ policy-gradient Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ°Ğ¼Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ DGPO Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ² 20 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ñ… ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Diffusion Models with Direct Group Preference Optimization",
                    "desc": "DGPO is a novel online reinforcement learning algorithm that improves diffusion models by leveraging group-level preferences. Unlike traditional methods that require stochastic policies, DGPO operates without the policy-gradient framework, allowing it to utilize efficient deterministic ODE samplers. This approach significantly accelerates training, achieving speeds approximately 20 times faster than current leading methods. The results demonstrate that DGPO not only enhances training efficiency but also delivers superior performance across various reward metrics."
                },
                "zh": {
                    "title": "DGPOï¼šé«˜æ•ˆçš„ç¾¤ä½“åå¥½ä¼˜åŒ–ç®—æ³•",
                    "desc": "DGPOæ˜¯ä¸€ç§æ–°çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡å­¦ä¹ ç¾¤ä½“çº§åå¥½æ¥å¢å¼ºæ‰©æ•£æ¨¡å‹ã€‚å®ƒé¿å…äº†ä½¿ç”¨éšæœºç­–ç•¥ï¼Œä»è€Œèƒ½å¤Ÿä½¿ç”¨é«˜æ•ˆçš„ç¡®å®šæ€§å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰é‡‡æ ·å™¨ã€‚è¿™ç§è®¾è®¡ä½¿å¾—è®­ç»ƒé€Ÿåº¦æé«˜äº†çº¦20å€ï¼Œå¹¶åœ¨å„ç±»å¥–åŠ±æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚DGPOçš„æå‡ºè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­éšæœºæ€§ä¸æ•ˆç‡ä¹‹é—´çš„çŸ›ç›¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08276",
            "title": "Beyond Turn Limits: Training Deep Search Agents with Dynamic Context\n  Window",
            "url": "https://huggingface.co/papers/2510.08276",
            "abstract": "DeepMiner, a framework using high-difficulty training tasks and dynamic context management, enhances multi-turn reasoning agents through reinforcement learning, achieving significant performance improvements across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent advances in reasoning models have demonstrated cognitive behaviors through reinforcement learning, existing approaches struggle to invoke deep reasoning capabilities in multi-turn agents with long-horizon interactions. We propose DeepMiner, a novel framework that elicits such abilities by introducing high-difficulty training tasks and dynamic context window. DeepMiner presents a reverse construction method to generate complex but verifiable question-answer pairs from authentic web sources, which ensures the challenge and reliability of training data while injecting cognitive capabilities into multi-turn reasoning scenarios. We further design an elegant yet effective dynamic context management strategy for both training and inference, utilizing sliding window mechanisms while eliminating the dependency on external summarization models, thereby efficiently empowering the model to handle continuously expanding long-horizon contexts. Through reinforcement learning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial performance improvements across multiple search agent benchmarks. DeepMiner attains 33.5% accuracy on BrowseComp-en, surpassing the previous best open-source agent by almost 20 percentage points, and demonstrates consistent improvements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our dynamic context management enables sustained interactions of nearly 100 turns within standard 32k context length, effectively addressing the context limitations that constrain existing multi-turn interaction systems.",
            "score": 2,
            "issue_id": 6345,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "ec6091bbe962801d",
            "authors": [
                "Qiaoyu Tang",
                "Hao Xiang",
                "Le Yu",
                "Bowen Yu",
                "Yaojie Lu",
                "Xianpei Han",
                "Le Sun",
                "WenJuan Zhang",
                "Pengbo Wang",
                "Shixuan Liu",
                "Zhenru Zhang",
                "Jianhong Tu",
                "Hongyu Lin",
                "Junyang Lin"
            ],
            "affiliations": [
                "Alibaba Group",
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08276.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#rl",
                    "#benchmark",
                    "#optimization",
                    "#data",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "â›ï¸",
                "ru": {
                    "title": "Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸",
                    "desc": "DeepMiner â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ multi-turn reasoning Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ, Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµĞ±-Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¾ĞºĞ½Ğ¾Ğ¼ ÑĞ¾ sliding window Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ DeepMiner-32B Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Qwen3-32B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 33.5% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ BrowseComp-en, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ»Ğ¸Ğ´ĞµÑ€Ğ¾Ğ² Ğ½Ğ° 20 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾ 100 Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ 32k Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² multi-turn ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…."
                },
                "en": {
                    "title": "Empowering Multi-Turn Reasoning with DeepMiner",
                    "desc": "DeepMiner is a framework designed to improve multi-turn reasoning agents using reinforcement learning. It introduces high-difficulty training tasks and a dynamic context management strategy to enhance the agents' cognitive abilities during long interactions. The framework generates complex question-answer pairs from real web sources, ensuring the training data is both challenging and reliable. By utilizing a sliding window mechanism, DeepMiner allows agents to maintain effective interactions over extended contexts, achieving significant performance gains on various benchmarks."
                },
                "zh": {
                    "title": "DeepMinerï¼šæå‡å¤šè½®æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "DeepMineræ˜¯ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ å¢å¼ºå¤šè½®æ¨ç†ä»£ç†çš„æ¡†æ¶ã€‚å®ƒå¼•å…¥äº†é«˜éš¾åº¦çš„è®­ç»ƒä»»åŠ¡å’ŒåŠ¨æ€ä¸Šä¸‹æ–‡ç®¡ç†ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åå‘æ„å»ºæ–¹æ³•ï¼Œä»çœŸå®çš„ç½‘ç»œæ¥æºç”Ÿæˆå¤æ‚ä¸”å¯éªŒè¯çš„é—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œç¡®ä¿è®­ç»ƒæ•°æ®çš„æŒ‘æˆ˜æ€§å’Œå¯é æ€§ã€‚é€šè¿‡æ»‘åŠ¨çª—å£æœºåˆ¶ï¼ŒDeepMineræœ‰æ•ˆç®¡ç†ä¸Šä¸‹æ–‡ï¼Œæ”¯æŒè¿‘100è½®çš„æŒç»­äº¤äº’ï¼Œè§£å†³äº†ç°æœ‰å¤šè½®äº¤äº’ç³»ç»Ÿçš„ä¸Šä¸‹æ–‡é™åˆ¶é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08191",
            "title": "Training-Free Group Relative Policy Optimization",
            "url": "https://huggingface.co/papers/2510.08191",
            "abstract": "Training-Free GRPO enhances LLM agent performance in specialized domains by learning experiential knowledge as a token prior without parameter updates, improving out-of-domain tasks with minimal data.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Model (LLM) agents have demonstrated their promising general capabilities. However, their performance in specialized real-world domains often degrades due to challenges in effectively integrating external tools and specific prompting strategies. While methods like agentic reinforcement learning have been proposed to address this, they typically rely on costly parameter updates, for example, through a process that uses Supervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase with Group Relative Policy Optimization (GRPO) to alter the output distribution. However, we argue that LLMs can achieve a similar effect on the output distribution by learning experiential knowledge as a token prior, which is a far more lightweight approach that not only addresses practical data scarcity but also avoids the common issue of overfitting. To this end, we propose Training-Free Group Relative Policy Optimization (Training-Free GRPO), a cost-effective solution that enhances LLM agent performance without any parameter updates. Our method leverages the group relative semantic advantage instead of numerical ones within each group of rollouts, iteratively distilling high-quality experiential knowledge during multi-epoch learning on a minimal ground-truth data. Such knowledge serves as the learned token prior, which is seamlessly integrated during LLM API calls to guide model behavior. Experiments on mathematical reasoning and web searching tasks demonstrate that Training-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly improves out-of-domain performance. With just a few dozen training samples, Training-Free GRPO outperforms fine-tuned small LLMs with marginal training data and cost.",
            "score": 2,
            "issue_id": 6345,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "2727834df48a0b2b",
            "authors": [
                "Yuzheng Cai",
                "Siqi Cai",
                "Yuchen Shi",
                "Zihan Xu",
                "Lichao Chen",
                "Yulei Qin",
                "Xiaoyu Tan",
                "Gang Li",
                "Zongyi Li",
                "Haojia Lin",
                "Yong Mao",
                "Ke Li",
                "Xing Sun"
            ],
            "affiliations": [
                "Tencent",
                "Youtu-Agent Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08191.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#transfer_learning",
                    "#agents"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‹Ñ‚Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Training-Free GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ±ĞµĞ· Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ñ‹Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ Ñ‚Ğ¾ĞºĞµĞ½-Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ². Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ API-Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ»Ñ‹Ğµ LLM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ´ĞµÑÑÑ‚ĞºĞ¾Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Boosting LLMs with Lightweight Knowledge Learning",
                    "desc": "This paper introduces Training-Free Group Relative Policy Optimization (Training-Free GRPO), a novel approach to enhance the performance of Large Language Model (LLM) agents in specialized domains without requiring parameter updates. Instead of traditional methods that rely on costly fine-tuning and reinforcement learning, this method learns experiential knowledge as a token prior, which helps improve model behavior with minimal data. The approach focuses on leveraging group relative semantic advantages to distill high-quality knowledge iteratively, addressing issues like data scarcity and overfitting. Experiments show that Training-Free GRPO significantly boosts out-of-domain performance in tasks like mathematical reasoning and web searching, outperforming fine-tuned models with limited training samples."
                },
                "zh": {
                    "title": "æ— è®­ç»ƒä¼˜åŒ–ï¼Œæå‡LLMè¡¨ç°ï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ— è®­ç»ƒç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTraining-Free GRPOï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰¹å®šé¢†åŸŸçš„è¡¨ç°ã€‚è¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ ç»éªŒçŸ¥è¯†ä½œä¸ºä»¤ç‰Œå…ˆéªŒï¼Œè€Œæ— éœ€è¿›è¡Œå‚æ•°æ›´æ–°ï¼Œä»è€Œæœ‰æ•ˆåº”å¯¹æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒTraining-Free GRPOåœ¨å¤šè½®å­¦ä¹ ä¸­æç‚¼é«˜è´¨é‡çš„ç»éªŒçŸ¥è¯†ï¼Œé¿å…äº†è¿‡æ‹Ÿåˆçš„å¸¸è§é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°å­¦æ¨ç†å’Œç½‘ç»œæœç´¢ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†LLMçš„è·¨é¢†åŸŸæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07958",
            "title": "A^2Search: Ambiguity-Aware Question Answering with Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2510.07958",
            "abstract": "A$^2$Search is an annotation-free framework that handles ambiguity in open-domain QA by detecting ambiguous questions, gathering alternative answers, and optimizing with RL, achieving state-of-the-art performance across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) and Reinforcement Learning (RL) have led to strong performance in open-domain question answering (QA). However, existing models still struggle with questions that admit multiple valid answers. Standard QA benchmarks, which typically assume a single gold answer, overlook this reality and thus produce inappropriate training signals. Existing attempts to handle ambiguity often rely on costly manual annotation, which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue. In this paper, we present A^2Search, an annotation-free, end-to-end training framework to recognize and handle ambiguity. At its core is an automated pipeline that detects ambiguous questions and gathers alternative answers via trajectory sampling and evidence verification. The model is then optimized with RL using a carefully designed AnsF1 reward, which naturally accommodates multiple answers. Experiments on eight open-domain QA benchmarks demonstrate that A^2Search achieves new state-of-the-art performance. With only a single rollout, A^2Search-7B yields an average AnsF1@1 score of 48.4% across four multi-hop benchmarks, outperforming all strong baselines, including the substantially larger ReSearch-32B (46.2%). Extensive analyses further show that A^2Search resolves ambiguity and generalizes across benchmarks, highlighting that embracing ambiguity is essential for building more reliable QA systems. Our code, data, and model weights can be found at https://github.com/zfj1998/A2Search",
            "score": 2,
            "issue_id": 6345,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "a380e0c68e05f32c",
            "authors": [
                "Fengji Zhang",
                "Xinyao Niu",
                "Chengyang Ying",
                "Guancheng Lin",
                "Zhongkai Hao",
                "Zhou Fan",
                "Chengen Huang",
                "Jacky Keung",
                "Bei Chen",
                "Junyang Lin"
            ],
            "affiliations": [
                "Alibaba Group",
                "City University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07958.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#benchmark",
                    "#optimization",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ QA-ÑĞ¸ÑÑ‚ĞµĞ¼ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸",
                    "desc": "AÂ²Search â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒĞ¼ĞµĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ AnsF1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹. ĞĞ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ open-domain QA AÂ²Search Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ 7-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½ÑƒÑ 32-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½ÑƒÑ baseline Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ."
                },
                "en": {
                    "title": "Embracing Ambiguity for Superior Question Answering",
                    "desc": "A$^2$Search is a novel framework designed to improve open-domain question answering (QA) by addressing the challenge of ambiguous questions. It operates without the need for manual annotations, instead using an automated process to identify ambiguous queries and collect multiple valid answers. The framework employs reinforcement learning (RL) with a unique AnsF1 reward to optimize its performance, allowing it to effectively handle questions with several correct responses. Experiments show that A$^2$Search achieves state-of-the-art results on various benchmarks, demonstrating the importance of managing ambiguity in QA systems."
                },
                "zh": {
                    "title": "æ‹¥æŠ±æ­§ä¹‰ï¼Œæå‡é—®ç­”ç³»ç»Ÿçš„å¯é æ€§",
                    "desc": "A^2Searchæ˜¯ä¸€ä¸ªæ— æ³¨é‡Šçš„æ¡†æ¶ï¼Œæ—¨åœ¨å¤„ç†å¼€æ”¾é¢†åŸŸé—®ç­”ä¸­çš„æ­§ä¹‰æ€§ã€‚å®ƒé€šè¿‡æ£€æµ‹æ­§ä¹‰é—®é¢˜ã€æ”¶é›†æ›¿ä»£ç­”æ¡ˆï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ï¼Œè¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ã€‚è¯¥æ¨¡å‹é‡‡ç”¨è‡ªåŠ¨åŒ–æµç¨‹ï¼Œèƒ½å¤Ÿè¯†åˆ«æ­§ä¹‰é—®é¢˜å¹¶é€šè¿‡è½¨è¿¹é‡‡æ ·å’Œè¯æ®éªŒè¯æ”¶é›†ç­”æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒA^2Searchåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†å¤„ç†æ­§ä¹‰æ€§å¯¹äºæ„å»ºæ›´å¯é çš„é—®ç­”ç³»ç»Ÿçš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08559",
            "title": "SciVideoBench: Benchmarking Scientific Video Reasoning in Large\n  Multimodal Models",
            "url": "https://huggingface.co/papers/2510.08559",
            "abstract": "SciVideoBench is a benchmark designed to evaluate advanced video reasoning in scientific contexts, challenging models with sophisticated domain-specific knowledge and logical reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) have achieved remarkable progress across various capabilities; however, complex video reasoning in the scientific domain remains a significant and challenging frontier. Current video benchmarks predominantly target general scenarios where perception/recognition is heavily relied on, while with relatively simple reasoning tasks, leading to saturation and thus failing to effectively evaluate advanced multimodal cognitive skills. To address this critical gap, we introduce SciVideoBench, a rigorous benchmark specifically designed to assess advanced video reasoning in scientific contexts. SciVideoBench consists of 1,000 carefully crafted multiple-choice questions derived from cutting-edge scientific experimental videos spanning over 25 specialized academic subjects and verified by a semi-automatic system. Each question demands sophisticated domain-specific knowledge, precise spatiotemporal perception, and intricate logical reasoning, effectively challenging models' higher-order cognitive abilities. Our evaluation highlights significant performance deficits in state-of-the-art proprietary and open-source LMMs, including Gemini 2.5 Pro and Qwen2.5-VL, indicating substantial room for advancement in video reasoning capabilities. Detailed analyses of critical factors such as reasoning complexity and visual grounding provide valuable insights and clear direction for future developments in LMMs, driving the evolution of truly capable multimodal AI co-scientists. We hope SciVideoBench could fit the interests of the community and help to push the boundary of cutting-edge AI for border science.",
            "score": 1,
            "issue_id": 6345,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "0ebe2fdbf3a4a576",
            "authors": [
                "Andong Deng",
                "Taojiannan Yang",
                "Shoubin Yu",
                "Lincoln Spencer",
                "Mohit Bansal",
                "Chen Chen",
                "Serena Yeung-Levy",
                "Xiaohan Wang"
            ],
            "affiliations": [
                "Stanford University",
                "University of Central Florida",
                "University of North Carolina at Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08559.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#multimodal",
                    "#video",
                    "#reasoning"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ°ÑƒÑ‡Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "SciVideoBench â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1000 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 25 ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LMM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Gemini 2.5 Pro Ğ¸ Qwen2.5-VL, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ…, ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ AI."
                },
                "en": {
                    "title": "Pushing the Limits of Video Reasoning in Science",
                    "desc": "SciVideoBench is a new benchmark created to test advanced video reasoning specifically in scientific fields. It includes 1,000 multiple-choice questions based on complex scientific videos, requiring deep domain knowledge and logical reasoning. Current benchmarks focus on simpler tasks, which do not adequately challenge the capabilities of large multimodal models (LMMs). The results show that even the best LMMs struggle with these advanced reasoning tasks, highlighting the need for further development in this area."
                },
                "zh": {
                    "title": "æ¨åŠ¨ç§‘å­¦è§†é¢‘æ¨ç†çš„è¾¹ç•Œ",
                    "desc": "SciVideoBenchæ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°ç§‘å­¦é¢†åŸŸè§†é¢‘æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«1000ä¸ªç²¾å¿ƒè®¾è®¡çš„å¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¶µç›–25ä¸ªå­¦æœ¯é¢†åŸŸçš„å‰æ²¿ç§‘å­¦å®éªŒè§†é¢‘ã€‚æ¯ä¸ªé—®é¢˜éƒ½éœ€è¦å¤æ‚çš„é¢†åŸŸçŸ¥è¯†ã€ç²¾ç¡®çš„æ—¶ç©ºæ„ŸçŸ¥å’Œå¤æ‚çš„é€»è¾‘æ¨ç†ï¼Œæ—¨åœ¨æŒ‘æˆ˜æ¨¡å‹çš„é«˜çº§è®¤çŸ¥èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰çš„å…ˆè¿›å¤šæ¨¡æ€æ¨¡å‹åœ¨è§†é¢‘æ¨ç†èƒ½åŠ›ä¸Šå­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œè¡¨æ˜è¿™ä¸€é¢†åŸŸè¿˜æœ‰å¾ˆå¤§çš„å‘å±•ç©ºé—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08556",
            "title": "DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via\n  Joint-Wise Neural Dynamics Model",
            "url": "https://huggingface.co/papers/2510.08556",
            "abstract": "A novel framework enables a single simulation-trained policy to generalize to diverse real-world object rotations by learning joint-wise dynamics and autonomously collecting data.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving generalized in-hand object rotation remains a significant challenge in robotics, largely due to the difficulty of transferring policies from simulation to the real world. The complex, contact-rich dynamics of dexterous manipulation create a \"reality gap\" that has limited prior work to constrained scenarios involving simple geometries, limited object sizes and aspect ratios, constrained wrist poses, or customized hands. We address this sim-to-real challenge with a novel framework that enables a single policy, trained in simulation, to generalize to a wide variety of objects and conditions in the real world. The core of our method is a joint-wise dynamics model that learns to bridge the reality gap by effectively fitting limited amount of real-world collected data and then adapting the sim policy's actions accordingly. The model is highly data-efficient and generalizable across different whole-hand interaction distributions by factorizing dynamics across joints, compressing system-wide influences into low-dimensional variables, and learning each joint's evolution from its own dynamic profile, implicitly capturing these net effects. We pair this with a fully autonomous data collection strategy that gathers diverse, real-world interaction data with minimal human intervention. Our complete pipeline demonstrates unprecedented generality: a single policy successfully rotates challenging objects with complex shapes (e.g., animals), high aspect ratios (up to 5.33), and small sizes, all while handling diverse wrist orientations and rotation axes. Comprehensive real-world evaluations and a teleoperation application for complex tasks validate the effectiveness and robustness of our approach. Website: https://meowuu7.github.io/DexNDM/",
            "score": 1,
            "issue_id": 6345,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "1cc572ebd0cf9869",
            "authors": [
                "Xueyi Liu",
                "He Wang",
                "Li Yi"
            ],
            "affiliations": [
                "Galbot Project",
                "Peking University",
                "Shanghai Qi Zhi Institute",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08556.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#robotics",
                    "#optimization",
                    "#data",
                    "#transfer_learning",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ´Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ»ÑĞ±Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ€ÑƒĞºĞµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€ĞµÑˆĞ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ»Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ€ÑƒĞºĞµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ˜Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑÑƒÑÑ‚Ğ°Ğ²Ğ°, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑÑŒ Ğ½Ğ° Ğ¼Ğ°Ğ»Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ sim-Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼, Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±ĞµÑĞ¿Ñ€ĞµÑ†ĞµĞ´ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging the Reality Gap for Robotic Manipulation",
                    "desc": "This paper presents a new framework that allows a single policy, trained in simulation, to effectively handle various real-world object rotations. It addresses the challenge of transferring learned behaviors from simulated environments to real-world scenarios, overcoming the 'reality gap' that often limits robotic manipulation. The approach utilizes a joint-wise dynamics model to adapt the policy's actions based on limited real-world data, making it both data-efficient and generalizable. Additionally, an autonomous data collection strategy is employed to gather diverse interaction data, enabling the policy to successfully manipulate complex objects with different shapes and sizes."
                },
                "zh": {
                    "title": "å•ä¸€ç­–ç•¥å®ç°å¤šæ ·åŒ–ç‰©ä½“æ—‹è½¬çš„çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œä½¿å¾—å•ä¸€çš„æ¨¡æ‹Ÿè®­ç»ƒç­–ç•¥èƒ½å¤Ÿåœ¨å¤šæ ·çš„çœŸå®ç‰©ä½“æ—‹è½¬ä¸­å®ç°æ³›åŒ–ã€‚é€šè¿‡å­¦ä¹ å…³èŠ‚çº§çš„åŠ¨æ€æ¨¡å‹ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°ç¼©å°äº†æ¨¡æ‹Ÿä¸ç°å®ä¹‹é—´çš„å·®è·ï¼Œå¹¶èƒ½å¤Ÿè‡ªé€‚åº”åœ°è°ƒæ•´ç­–ç•¥çš„åŠ¨ä½œã€‚è¯¥æ¨¡å‹åœ¨æ•°æ®æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚å½¢çŠ¶å’Œé«˜çºµæ¨ªæ¯”çš„å°å‹ç‰©ä½“ã€‚æˆ‘ä»¬çš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œä¸­çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ï¼ŒæˆåŠŸå®ç°äº†å¯¹å¤šç§ç‰©ä½“çš„æ—‹è½¬æ“ä½œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08549",
            "title": "Entropy Regularizing Activation: Boosting Continuous Control, Large\n  Language Models, and Image Classification with Activation as Entropy\n  Constraints",
            "url": "https://huggingface.co/papers/2510.08549",
            "abstract": "ERA, a new paradigm using specially designed activations, enhances performance across LLMs, reinforcement learning, and image classification with minimal computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose ERA, a new paradigm that constrains the sampling entropy above given thresholds by applying specially designed activations to the outputs of models. Our approach demonstrates broad effectiveness across different domains: 1) for large language models(LLMs), boosting the AIME 2025 score for Qwen2.5-Math-7B by 37.4%; 2) for continuous control reinforcement learning agents, improving performance by more than 30% over strong baselines such as SAC on the challenging HumanoidBench; 3) for image classification, enhancing ImageNet top-1 accuracy by 0.69% for ResNet-50. These gains are achieved with a computational overhead of less than 7%. Our work validates output activation as a powerful tool for entropy control, opening a new direction for designing simpler and more robust algorithms.",
            "score": 1,
            "issue_id": 6345,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "8d7e757f9121f346",
            "authors": [
                "Zilin Kang",
                "Chonghua Liao",
                "Tingqiang Xu",
                "Huazhe Xu"
            ],
            "affiliations": [
                "Department of Computer Science and Technology, Tsinghua University",
                "Institute for Interdisciplinary Information Sciences, Tsinghua University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Qi Zhi Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08549.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#cv",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ERA: ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ERA â€” Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…: ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ» Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-Math-7B Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… AIME Ğ½Ğ° 37.4%, Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ» Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 30% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ SAC, Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ» Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ResNet-50 Ğ½Ğ° ImageNet Ğ½Ğ° 0.69%. Ğ’ÑĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ÑÑ Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¼ĞµĞ½ĞµĞµ 7%. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "ERA: Enhancing Model Performance with Controlled Activations",
                    "desc": "The paper introduces ERA, a novel approach that utilizes specially designed activation functions to improve model performance while maintaining low computational costs. By constraining sampling entropy, ERA enhances the effectiveness of large language models, reinforcement learning agents, and image classification tasks. Notably, it achieves significant performance boosts, such as a 37.4% increase in AIME 2025 scores for LLMs and over 30% improvement in reinforcement learning benchmarks. This method demonstrates that controlling output activations can lead to simpler and more robust machine learning algorithms."
                },
                "zh": {
                    "title": "ERAï¼šæå‡æ¨¡å‹æ€§èƒ½çš„æ–°èŒƒå¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°èŒƒå¼ERAï¼Œé€šè¿‡å¯¹æ¨¡å‹è¾“å‡ºåº”ç”¨ç‰¹åˆ«è®¾è®¡çš„æ¿€æ´»å‡½æ•°ï¼Œçº¦æŸé‡‡æ ·ç†µåœ¨ç»™å®šé˜ˆå€¼ä¹‹ä¸Šï¼Œä»è€Œæå‡æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºå¹¿æ³›çš„æœ‰æ•ˆæ€§ï¼šåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼ŒQwen2.5-Math-7Bçš„AIME 2025åˆ†æ•°æé«˜äº†37.4%ï¼›åœ¨è¿ç»­æ§åˆ¶å¼ºåŒ–å­¦ä¹ ä»£ç†ä¸­ï¼Œæ€§èƒ½æ¯”å¼ºåŸºçº¿SACåœ¨HumanoidBenchä¸Šæé«˜äº†30%ä»¥ä¸Šï¼›åœ¨å›¾åƒåˆ†ç±»ä¸­ï¼ŒResNet-50åœ¨ImageNetä¸Šçš„top-1å‡†ç¡®ç‡æé«˜äº†0.69%ã€‚è¿™äº›æå‡çš„è®¡ç®—å¼€é”€ä½äº7%ï¼ŒéªŒè¯äº†è¾“å‡ºæ¿€æ´»ä½œä¸ºç†µæ§åˆ¶çš„å¼ºå¤§å·¥å…·ï¼Œä¸ºè®¾è®¡æ›´ç®€å•å’Œæ›´ç¨³å¥çš„ç®—æ³•å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08547",
            "title": "R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized\n  Manipulation",
            "url": "https://huggingface.co/papers/2510.08547",
            "abstract": "A real-to-real 3D data generation framework enhances data efficiency for generalized robotic manipulation by augmenting pointcloud observations without simulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Towards the aim of generalized robotic manipulation, spatial generalization is the most fundamental capability that requires the policy to work robustly under different spatial distribution of objects, environment and agent itself. To achieve this, substantial human demonstrations need to be collected to cover different spatial configurations for training a generalized visuomotor policy via imitation learning. Prior works explore a promising direction that leverages data generation to acquire abundant spatially diverse data from minimal source demonstrations. However, most approaches face significant sim-to-real gap and are often limited to constrained settings, such as fixed-base scenarios and predefined camera viewpoints. In this paper, we propose a real-to-real 3D data generation framework (R2RGen) that directly augments the pointcloud observation-action pairs to generate real-world data. R2RGen is simulator- and rendering-free, thus being efficient and plug-and-play. Specifically, given a single source demonstration, we introduce an annotation mechanism for fine-grained parsing of scene and trajectory. A group-wise augmentation strategy is proposed to handle complex multi-object compositions and diverse task constraints. We further present camera-aware processing to align the distribution of generated data with real-world 3D sensor. Empirically, R2RGen substantially enhances data efficiency on extensive experiments and demonstrates strong potential for scaling and application on mobile manipulation.",
            "score": 1,
            "issue_id": 6344,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            },
            "hash": "a174dc6a582d8dff",
            "authors": [
                "Xiuwei Xu",
                "Angyuan Ma",
                "Hankun Li",
                "Bingyao Yu",
                "Zheng Zhu",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "affiliations": [
                "GigaAI",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08547.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#transfer_learning",
                    "#robotics",
                    "#data",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ R2RGen - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ñ‹ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ pointcloud Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ sim-to-real gap Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ±Ğ¾Ñ€Ğ° ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ visuomotor policy Ñ‡ĞµÑ€ĞµĞ· imitation learning, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Enhancing Robotic Manipulation with Real-to-Real Data Generation",
                    "desc": "This paper introduces a new framework called R2RGen for generating 3D data that improves the efficiency of training robotic manipulation systems. Unlike previous methods that rely on simulations, R2RGen works directly with real-world data by augmenting pointcloud observations from a single demonstration. It employs a unique annotation mechanism to analyze scenes and trajectories, along with a group-wise augmentation strategy to manage complex object interactions. The framework is designed to be efficient and adaptable, making it suitable for various mobile manipulation tasks while significantly reducing the need for extensive human demonstrations."
                },
                "zh": {
                    "title": "æå‡æœºå™¨äººæ“ä½œçš„æ•°æ®æ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§çœŸå®åˆ°çœŸå®çš„3Dæ•°æ®ç”Ÿæˆæ¡†æ¶ï¼ˆR2RGenï¼‰ï¼Œæ—¨åœ¨æé«˜æœºå™¨äººæ“ä½œçš„æ•°æ®ä¿¡æ¯æ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡å¢å¼ºç‚¹äº‘è§‚å¯Ÿ-åŠ¨ä½œå¯¹ï¼Œç›´æ¥ç”ŸæˆçœŸå®ä¸–ç•Œçš„æ•°æ®ï¼Œè€Œæ— éœ€ä½¿ç”¨æ¨¡æ‹Ÿå™¨æˆ–æ¸²æŸ“ã€‚R2RGenå¼•å…¥äº†ä¸€ç§æ³¨é‡Šæœºåˆ¶ï¼Œä»¥ä¾¿å¯¹åœºæ™¯å’Œè½¨è¿¹è¿›è¡Œç»†è‡´è§£æï¼Œå¹¶é‡‡ç”¨äº†åˆ†ç»„å¢å¼ºç­–ç•¥æ¥å¤„ç†å¤æ‚çš„å¤šç‰©ä½“ç»„åˆå’Œå¤šæ ·çš„ä»»åŠ¡çº¦æŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR2RGenåœ¨æ•°æ®æ•ˆç‡ä¸Šæ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†åœ¨ç§»åŠ¨æ“ä½œä¸­çš„å¼ºå¤§åº”ç”¨æ½œåŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-10-09.html",
    "link_next": "2025-10-13.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "09.10",
        "en": "10/09",
        "zh": "10æœˆ9æ—¥"
    },
    "short_date_next": {
        "ru": "13.10",
        "en": "10/13",
        "zh": "10æœˆ13æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 4,
        "#benchmark": 11,
        "#agents": 5,
        "#cv": 2,
        "#rl": 9,
        "#rlhf": 4,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 5,
        "#multimodal": 6,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 14,
        "#robotics": 2,
        "#agi": 2,
        "#games": 4,
        "#interpretability": 0,
        "#reasoning": 11,
        "#transfer_learning": 5,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 16,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0
    }
}