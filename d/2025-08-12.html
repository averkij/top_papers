
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 26 papers. August 12.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">12 августа</span> | <span id="title-articles-count">26 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-08-11.html">⬅️ <span id="prev-date">11.08</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-08-13.html">➡️ <span id="next-date">13.08</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-08.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'};
        let feedDateNext = {'ru': '13.08', 'en': '08/13', 'zh': '8月13日'};
        let feedDatePrev = {'ru': '11.08', 'en': '08/11', 'zh': '8月11日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.07050', 'title': 'ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability', 'url': 'https://huggingface.co/papers/2508.07050', 'abstract': 'A reasoning-intensive reranker, ReasonRank, achieves state-of-the-art performance in passage ranking tasks by using synthesized training data and a two-stage post-training approach with reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker ReasonRank outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\\footnote{https://brightbenchmark.github.io/.} Our codes are available at https://github.com/8421BCD/ReasonRank.', 'score': 80, 'issue_id': 5296, 'pub_date': '2025-08-09', 'pub_date_card': {'ru': '9 августа', 'en': 'August 9', 'zh': '8月9日'}, 'hash': 'bfd8a67bb2b2dbf4', 'authors': ['Wenhan Liu', 'Xinyu Ma', 'Weiwei Sun', 'Yutao Zhu', 'Yuchen Li', 'Dawei Yin', 'Zhicheng Dou'], 'affiliations': ['Baidu Inc.', 'Carnegie Mellon University', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.07050.jpg', 'data': {'categories': ['#training', '#dataset', '#optimization', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'ReasonRank: Рассуждающий ранжировщик нового поколения', 'desc': 'ReasonRank - это новый ранжировщик пассажей, использующий синтезированные данные для обучения и двухэтапный подход с обучением с подкреплением. Он применяет пошаговое рассуждение во время тестирования для улучшения производительности ранжирования списков. Модель использует автоматизированную систему синтеза обучающих данных с рассуждениями и механизм фильтрации для обеспечения качества данных. ReasonRank достигает наилучших результатов в задачах ранжирования пассажей, превосходя существующие базовые модели.'}, 'en': {'title': 'ReasonRank: Elevating Passage Ranking with Advanced Reasoning Techniques', 'desc': 'The paper introduces ReasonRank, a novel reranker that excels in passage ranking tasks by leveraging synthesized training data and a two-stage post-training method involving reinforcement learning. It addresses the challenge of limited reasoning-intensive training data by creating a framework that generates high-quality training labels from diverse domains. The two-stage approach consists of a supervised fine-tuning phase to learn reasoning patterns, followed by a reinforcement learning phase that enhances ranking capabilities using a multi-view ranking reward. Experimental results show that ReasonRank achieves state-of-the-art performance while maintaining lower latency compared to traditional pointwise rerankers.'}, 'zh': {'title': '推理强化重排序，提升段落排名性能！', 'desc': '本文提出了一种名为ReasonRank的推理强化重排序模型，旨在提高段落排名任务的性能。通过合成训练数据和两阶段的后训练方法，ReasonRank在复杂的排名场景中表现出色。我们设计了一种自动化的推理训练数据合成框架，并引入了自一致性数据过滤机制，以确保数据质量。最终，ReasonRank在BRIGHT排行榜上达到了40.6的最新性能，显著优于现有基线模型。'}}}, {'id': 'https://huggingface.co/papers/2508.07999', 'title': 'WideSearch: Benchmarking Agentic Broad Info-Seeking', 'url': 'https://huggingface.co/papers/2508.07999', 'abstract': 'WideSearch is a new benchmark evaluating the reliability of automated search agents in large-scale information collection tasks, revealing significant deficiencies in current systems.  \t\t\t\t\tAI-generated summary \t\t\t\t From professional research to everyday planning, many tasks are bottlenecked by wide-scale information seeking, which is more repetitive than cognitively complex. With the rapid development of Large Language Models (LLMs), automated search agents powered by LLMs offer a promising solution to liberate humans from this tedious work. However, the capability of these agents to perform such "wide-context" collection reliably and completely remains largely unevaluated due to a lack of suitable benchmarks. To bridge this gap, we introduce WideSearch, a new benchmark engineered to evaluate agent reliability on these large-scale collection tasks. The benchmark features 200 manually curated questions (100 in English, 100 in Chinese) from over 15 diverse domains, grounded in real user queries. Each task requires agents to collect large-scale atomic information, which could be verified one by one objectively, and arrange it into a well-organized output. A rigorous five-stage quality control pipeline ensures the difficulty, completeness, and verifiability of the dataset. We benchmark over 10 state-of-the-art agentic search systems, including single-agent, multi-agent frameworks, and end-to-end commercial systems. Most systems achieve overall success rates near 0\\%, with the best performer reaching just 5\\%. However, given sufficient time, cross-validation by multiple human testers can achieve a near 100\\% success rate. These results demonstrate that present search agents have critical deficiencies in large-scale information seeking, underscoring urgent areas for future research and development in agentic search. Our dataset, evaluation pipeline, and benchmark results have been publicly released at https://widesearch-seed.github.io/', 'score': 76, 'issue_id': 5295, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': '45f13eb4f1110e39', 'authors': ['Ryan Wong', 'Jiawei Wang', 'Junjie Zhao', 'Li Chen', 'Yan Gao', 'Long Zhang', 'Xuan Zhou', 'Zuo Wang', 'Kai Xiang', 'Ge Zhang', 'Wenhao Huang', 'Yang Wang', 'Ke Wang'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2508.07999.jpg', 'data': {'categories': ['#science', '#benchmark', '#agents', '#dataset', '#survey'], 'emoji': '🔍', 'ru': {'title': 'Поисковые агенты на основе ИИ пока не справляются с масштабными задачами', 'desc': 'WideSearch - это новый эталонный тест для оценки надежности автоматизированных поисковых агентов в задачах сбора информации большого масштаба. Бенчмарк включает 200 вручную отобранных вопросов из более чем 15 различных областей на английском и китайском языках. Тестирование более 10 современных систем агентного поиска показало, что большинство из них достигают общего уровня успешности около 0%, при этом лучший результат составляет всего 5%. Эти результаты демонстрируют критические недостатки современных поисковых агентов в задачах широкомасштабного поиска информации.'}, 'en': {'title': 'WideSearch: Evaluating the Reliability of Automated Search Agents', 'desc': 'WideSearch is a benchmark designed to assess the reliability of automated search agents in large-scale information collection tasks. It highlights significant shortcomings in current systems, which struggle to perform effectively in wide-context searches. The benchmark includes 200 curated questions across various domains, requiring agents to gather and organize information that can be objectively verified. Results show that most tested systems have low success rates, indicating a need for improvement in agentic search capabilities.'}, 'zh': {'title': '提升自动搜索代理的可靠性', 'desc': 'WideSearch是一个新的基准，用于评估自动搜索代理在大规模信息收集任务中的可靠性。当前的系统在执行这些任务时存在显著缺陷，成功率普遍接近0%。该基准包含200个手动策划的问题，涵盖15个不同领域，旨在测试代理收集和组织信息的能力。研究结果表明，现有的搜索代理在大规模信息获取方面亟需改进，未来的研究和开发方向应集中在提升其性能上。'}}}, {'id': 'https://huggingface.co/papers/2508.07981', 'title': 'Omni-Effects: Unified and Spatially-Controllable Visual Effects\n  Generation', 'url': 'https://huggingface.co/papers/2508.07981', 'abstract': 'Omni-Effects is a unified framework that enables the generation of prompt-guided and spatially controllable composite visual effects using LoRA-based Mixture of Experts and Spatial-Aware Prompt with Independent-Information Flow.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.', 'score': 45, 'issue_id': 5298, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': 'f440e34cea9bf27f', 'authors': ['Fangyuan Mao', 'Aiming Hao', 'Jintao Chen', 'Dongxia Liu', 'Xiaokun Feng', 'Jiashu Zhu', 'Meiqi Wu', 'Chubin Chen', 'Jiahong Wu', 'Xiangxiang Chu'], 'affiliations': ['AMAP, Alibaba Group', 'CASIA', 'PKU', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2508.07981.jpg', 'data': {'categories': ['#cv', '#video', '#data', '#optimization', '#games', '#dataset', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'Омни-эффекты: Единая система для создания пространственно-контролируемых визуальных эффектов', 'desc': 'Omni-Effects - это унифицированная система для генерации визуальных эффектов с пространственным контролем, использующая LoRA-based Mixture of Experts и Spatial-Aware Prompt. Она решает проблему интерференции между эффектами и пространственной неконтролируемости при обучении нескольким визуальным эффектам одновременно. Система включает два ключевых нововведения: LoRA-MoE для интеграции разнообразных эффектов в единую модель и SAP для точного пространственного контроля. Авторы также создали датасет Omni-VFX и систему оценки для валидации производительности модели.'}, 'en': {'title': 'Unified Control for Diverse Visual Effects Generation', 'desc': 'Omni-Effects is a new framework designed to create complex visual effects in videos by combining multiple effects in specific locations. It uses a technique called LoRA-based Mixture of Experts to manage different effects without them interfering with each other. Additionally, it incorporates a Spatial-Aware Prompt that allows users to control where each effect appears in the video. This framework also includes an Independent-Information Flow module to ensure that the effects remain distinct and do not blend together unintentionally.'}, 'zh': {'title': '统一生成空间可控视觉效果的框架', 'desc': 'Omni-Effects是一个统一框架，能够生成基于提示的和空间可控的复合视觉效果。该框架采用了基于LoRA的专家混合模型和空间感知提示，解决了多种视觉效果生成中的干扰和空间不可控性问题。通过引入独立信息流模块，Omni-Effects能够有效隔离不同效果的控制信号，避免不必要的混合。实验结果表明，该框架能够实现精确的空间控制和多样化的效果生成，用户可以指定所需效果的类别和位置。'}}}, {'id': 'https://huggingface.co/papers/2508.07629', 'title': 'Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving\n  Clipping Policy Optimization', 'url': 'https://huggingface.co/papers/2508.07629', 'abstract': "Klear-Reasoner, a model with long reasoning capabilities, achieves high performance across benchmarks through detailed post-training workflows, including long Chain-of-Thought supervised fine-tuning and reinforcement learning with Gradient-Preserving clipping Policy Optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Klear-Reasoner, a model with long reasoning capabilities that demonstrates careful deliberation during problem solving, achieving outstanding performance across multiple benchmarks. Although there are already many excellent works related to inference models in the current community, there are still many problems with reproducing high-performance inference models due to incomplete disclosure of training details. This report provides an in-depth analysis of the reasoning model, covering the entire post-training workflow from data preparation and long Chain-of-Thought supervised fine-tuning (long CoT SFT) to reinforcement learning (RL), along with detailed ablation studies for each experimental component. For SFT data, our experiments show that a small number of high-quality data sources are more effective than a large number of diverse data sources, and that difficult samples can achieve better results without accuracy filtering. In addition, we investigate two key issues with current clipping mechanisms in RL: Clipping suppresses critical exploration signals and ignores suboptimal trajectories. To address these challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO) that gently backpropagates gradients from clipped tokens. GPPO not only enhances the model's exploration capacity but also improves its efficiency in learning from negative samples. Klear-Reasoner exhibits exceptional reasoning abilities in mathematics and programming, scoring 90.5\\% on AIME 2024, 83.2\\% on AIME 2025, 66.0\\% on LiveCodeBench V5 and 58.1\\% on LiveCodeBench V6.", 'score': 24, 'issue_id': 5295, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': 'a361b2d0bb1f93c2', 'authors': ['Zhenpeng Su', 'Leiyu Pan', 'Xue Bai', 'Dening Liu', 'Guanting Dong', 'Jiaming Huang', 'Wenping Hu', 'Guorui Zhou'], 'affiliations': ['Klear Team, Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.07629.jpg', 'data': {'categories': ['#training', '#long_context', '#optimization', '#math', '#plp', '#rl', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Klear-Reasoner: Мощный ИИ для длительных рассуждений', 'desc': 'Klear-Reasoner - это модель с длительными способностями к рассуждению, достигающая высокой производительности в различных тестах. Модель использует детальные пост-тренировочные процессы, включая длинную цепочку рассуждений при обучении с учителем и обучение с подкреплением с использованием оптимизации политики с сохранением градиента. Исследователи обнаружили, что небольшое количество высококачественных источников данных более эффективно, чем большое количество разнообразных источников. Klear-Reasoner демонстрирует исключительные способности к рассуждению в математике и программировании, достигая высоких результатов в тестах AIME и LiveCodeBench.'}, 'en': {'title': 'Klear-Reasoner: Mastering Long Reasoning with Enhanced Learning Techniques', 'desc': 'Klear-Reasoner is a machine learning model designed for long reasoning tasks, showcasing its ability to solve complex problems effectively. It utilizes a comprehensive post-training workflow that includes long Chain-of-Thought supervised fine-tuning and a novel reinforcement learning approach called Gradient-Preserving clipping Policy Optimization. The model demonstrates that using a few high-quality data sources can be more beneficial than many diverse ones, and it addresses issues in current reinforcement learning methods that hinder exploration. Klear-Reasoner achieves impressive performance on various benchmarks, particularly in mathematics and programming tasks.'}, 'zh': {'title': 'Klear-Reasoner：长推理能力的突破', 'desc': 'Klear-Reasoner是一种具有长推理能力的模型，能够在解决问题时进行细致的思考，表现出色。该模型通过详细的后训练工作流程，包括长链思维的监督微调和梯度保留剪切策略优化的强化学习，达到了高性能。研究表明，少量高质量的数据源比大量多样化的数据源更有效，且困难样本在没有准确性过滤的情况下也能取得更好的结果。此外，Klear-Reasoner在数学和编程方面展现了卓越的推理能力，取得了多个基准测试的高分。'}}}, {'id': 'https://huggingface.co/papers/2508.05305', 'title': 'SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings\n  and Speaks in Tokens', 'url': 'https://huggingface.co/papers/2508.05305', 'abstract': 'SONAR-LLM, a decoder-only transformer using token-level cross-entropy in the SONAR embedding space, achieves competitive text generation quality without diffusion sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t The recently proposed Large Concept Model (LCM) generates text by predicting a sequence of sentence-level embeddings and training with either mean-squared error or diffusion objectives. We present SONAR-LLM, a decoder-only transformer that "thinks" in the same continuous SONAR embedding space, yet is supervised through token-level cross-entropy propagated via the frozen SONAR decoder. This hybrid objective retains the semantic abstraction of LCM while eliminating its diffusion sampler and restoring a likelihood-based training signal. Across model sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive generation quality. We report scaling trends, ablations, benchmark results, and release the complete training code and all pretrained checkpoints to foster reproducibility and future research.', 'score': 22, 'issue_id': 5301, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '1502a6acea7faa7a', 'authors': ['Nikita Dragunov', 'Temurbek Rahmatullaev', 'Elizaveta Goncharova', 'Andrey Kuznetsov', 'Anton Razzhigaev'], 'affiliations': ['AIRI', 'HSE', 'Innopolis University', 'MSU', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2508.05305.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#open_source', '#architecture', '#training', '#story_generation', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Эффективная генерация текста без диффузионной выборки', 'desc': 'SONAR-LLM - это новая модель машинного обучения для генерации текста. Она использует декодер-трансформер и обучается с помощью кросс-энтропии на уровне токенов в пространстве эмбеддингов SONAR. В отличие от Large Concept Model, SONAR-LLM не требует диффузионной выборки. Модель показывает конкурентоспособное качество генерации текста при различных размерах от 39 млн до 1,3 млрд параметров.'}, 'en': {'title': 'SONAR-LLM: Simplifying Text Generation with Semantic Precision', 'desc': 'SONAR-LLM is a new type of language model that uses a decoder-only transformer architecture to generate text. It operates in a unique embedding space called SONAR, which allows it to maintain semantic meaning while generating text. Instead of using complex diffusion sampling methods, SONAR-LLM employs a simpler token-level cross-entropy loss for training, which helps improve its performance. The model has been tested across various sizes and shows strong results in text generation, with all resources made available for further research.'}, 'zh': {'title': 'SONAR-LLM：高效文本生成的新方法', 'desc': 'SONAR-LLM是一种仅使用解码器的变换器模型，它在SONAR嵌入空间中通过令牌级交叉熵进行训练，从而实现了高质量的文本生成。与最近提出的大型概念模型（LCM）不同，SONAR-LLM不使用扩散采样，而是通过冻结的SONAR解码器传播监督信号。该模型保留了LCM的语义抽象，同时恢复了基于似然的训练信号。SONAR-LLM在从3900万到13亿参数的不同模型规模上都达到了竞争力的生成质量。'}}}, {'id': 'https://huggingface.co/papers/2507.22034', 'title': 'UserBench: An Interactive Gym Environment for User-Centric Agents', 'url': 'https://huggingface.co/papers/2507.22034', 'abstract': 'UserBench evaluates LLM-based agents in multi-turn interactions with simulated users, revealing gaps in task completion and user alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs)-based agents have made impressive progress in reasoning and tool use, enabling them to solve complex tasks. However, their ability to proactively collaborate with users, especially when goals are vague, evolving, or indirectly expressed, remains underexplored. To address this gap, we introduce UserBench, a user-centric benchmark designed to evaluate agents in multi-turn, preference-driven interactions. UserBench features simulated users who start with underspecified goals and reveal preferences incrementally, requiring agents to proactively clarify intent and make grounded decisions with tools. Our evaluation of leading open- and closed-source LLMs reveals a significant disconnect between task completion and user alignment. For instance, models provide answers that fully align with all user intents only 20% of the time on average, and even the most advanced models uncover fewer than 30% of all user preferences through active interaction. These results highlight the challenges of building agents that are not just capable task executors, but true collaborative partners. UserBench offers an interactive environment to measure and advance this critical capability.', 'score': 21, 'issue_id': 5297, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 июля', 'en': 'July 29', 'zh': '7月29日'}, 'hash': '91c16eb4d2452d19', 'authors': ['Cheng Qian', 'Zuxin Liu', 'Akshara Prabhakar', 'Zhiwei Liu', 'Jianguo Zhang', 'Haolin Chen', 'Heng Ji', 'Weiran Yao', 'Shelby Heinecke', 'Silvio Savarese', 'Caiming Xiong', 'Huan Wang'], 'affiliations': ['Salesforce AI Research', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2507.22034.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#alignment', '#reasoning', '#agents'], 'emoji': '🤖', 'ru': {'title': 'UserBench: на пути к ИИ-агентам, понимающим пользователей', 'desc': 'UserBench - это новый бенчмарк для оценки ИИ-агентов на основе больших языковых моделей в многоэтапных взаимодействиях с симулированными пользователями. Он выявляет разрыв между выполнением задач и соответствием предпочтениям пользователей. Результаты показывают, что даже продвинутые модели раскрывают менее 30% всех пользовательских предпочтений через активное взаимодействие. UserBench предлагает интерактивную среду для измерения и улучшения способности ИИ-агентов быть настоящими партнерами для пользователей.'}, 'en': {'title': 'Bridging the Gap: Enhancing User Alignment in LLM Interactions', 'desc': 'UserBench is a benchmark designed to assess the performance of LLM-based agents in multi-turn interactions with users. It focuses on how well these agents can understand and adapt to vague or evolving user goals, which is crucial for effective collaboration. The study reveals that current models struggle with aligning their outputs to user preferences, achieving only 20% alignment on average. This highlights the need for improved interaction strategies that allow agents to clarify user intent and make informed decisions throughout the conversation.'}, 'zh': {'title': '提升智能体的用户协作能力', 'desc': '本论文介绍了UserBench，这是一个用户中心的基准测试，用于评估基于大型语言模型（LLM）的智能体在多轮交互中的表现。研究发现，尽管这些智能体在解决复杂任务方面取得了显著进展，但在与用户的主动合作上仍存在不足，尤其是在目标模糊或不断变化的情况下。通过模拟用户逐步揭示偏好，UserBench要求智能体主动澄清意图并做出基于工具的决策。评估结果显示，当前模型在任务完成与用户对齐之间存在显著差距，平均只有20%的时间能够完全满足用户意图。'}}}, {'id': 'https://huggingface.co/papers/2508.07407', 'title': 'A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm\n  Bridging Foundation Models and Lifelong Agentic Systems', 'url': 'https://huggingface.co/papers/2508.07407', 'abstract': 'A survey of self-evolving AI agents that adapt to dynamic environments through automatic enhancement based on interaction data and feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving environments. To this end, recent research has explored agent evolution techniques that aim to automatically enhance agent systems based on interaction data and environmental feedback. This emerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of foundation models with the continuous adaptability required by lifelong agentic systems. In this survey, we provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically, we first introduce a unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic systems. The framework highlights four key components: System Inputs, Agent System, Environment, and Optimisers, serving as a foundation for understanding and comparing different strategies. Based on this framework, we systematically review a wide range of self-evolving techniques that target different components of the agent system. We also investigate domain-specific evolution strategies developed for specialised fields such as biomedicine, programming, and finance, where optimisation objectives are tightly coupled with domain constraints. In addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey aims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents, laying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems.', 'score': 20, 'issue_id': 5302, 'pub_date': '2025-08-10', 'pub_date_card': {'ru': '10 августа', 'en': 'August 10', 'zh': '8月10日'}, 'hash': 'e94cfc939dfba252', 'authors': ['Jinyuan Fang', 'Yanwen Peng', 'Xi Zhang', 'Yingxu Wang', 'Xinhao Yi', 'Guibin Zhang', 'Yi Xu', 'Bin Wu', 'Siwei Liu', 'Zihao Li', 'Zhaochun Ren', 'Nikos Aletras', 'Xi Wang', 'Han Zhou', 'Zaiqiao Meng'], 'affiliations': ['Leiden University', 'Mohamed bin Zayed University of Artificial Intelligence', 'National University of Singapore', 'University College London', 'University of Aberdeen', 'University of Cambridge', 'University of Glasgow', 'University of Sheffield'], 'pdf_title_img': 'assets/pdf/title_img/2508.07407.jpg', 'data': {'categories': ['#ethics', '#optimization', '#agents', '#survey'], 'emoji': '🤖', 'ru': {'title': 'Самоэволюционирующие ИИ-агенты: путь к адаптивному и непрерывному обучению', 'desc': 'Статья представляет обзор методов создания самоэволюционирующих ИИ-агентов, способных адаптироваться к динамическим средам. Авторы предлагают концептуальную структуру, описывающую цикл обратной связи в таких системах, включающую входные данные, агентскую систему, среду и оптимизаторы. Рассматриваются различные техники эволюции агентов, нацеленные на разные компоненты системы, а также стратегии для специфических доменов. Особое внимание уделяется вопросам оценки, безопасности и этики самоэволюционирующих агентских систем.'}, 'en': {'title': 'Empowering AI Agents to Evolve and Adapt!', 'desc': 'This paper surveys self-evolving AI agents that can adapt to changing environments by automatically improving themselves using interaction data and feedback. It highlights the limitations of traditional static agent systems and introduces a conceptual framework that outlines the essential components of self-evolving agents: System Inputs, Agent System, Environment, and Optimisers. The authors review various techniques for enhancing agent systems and explore domain-specific strategies in fields like biomedicine and finance. Additionally, the paper discusses important considerations regarding the evaluation, safety, and ethics of these adaptive systems, aiming to guide future research and development in this area.'}, 'zh': {'title': '自我进化的AI代理：适应动态环境的未来', 'desc': '这篇论文调查了自我进化的人工智能代理，这些代理能够通过交互数据和反馈在动态环境中自动增强。现有的代理系统通常依赖于手动配置，缺乏适应性，因此研究者们探索了基于反馈的自动增强技术。论文提供了一个统一的概念框架，帮助理解自我进化代理系统的设计，并系统回顾了多种自我进化技术。最后，论文还讨论了评估、安全性和伦理等重要问题，以确保这些系统的有效性和可靠性。'}}}, {'id': 'https://huggingface.co/papers/2508.06600', 'title': 'BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of\n  Deep-Research Agent', 'url': 'https://huggingface.co/papers/2508.06600', 'abstract': 'BrowseComp-Plus, a curated benchmark, enables controlled evaluation of deep research agents and retrieval methods, providing insights into their performance and effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep-Research agents, which integrate large language models (LLMs) with search tools, have shown success in improving the effectiveness of handling complex queries that require iterative search planning and reasoning over search results. Evaluations on current benchmarks like BrowseComp relies on black-box live web search APIs, have notable limitations in (1) fairness: dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep research methods; (2) transparency: lack of control over the document corpus makes it difficult to isolate retriever contributions. In other words, the current evaluations may compare a complete deep research system at a given time, but they do not foster well-controlled experiments to provide insights into the capability of underlying deep research LLMs. To address these challenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp, employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus includes human-verified supporting documents and mined challenging negatives, enabling controlled experimentation. The benchmark is shown to be effective in distinguishing the performance of deep research systems. For instance, the open-source model Search-R1, when paired with the BM25 retriever, achieves 3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with fewer search calls. This benchmark allows comprehensive evaluation and disentangled analysis of deep research agents and retrieval methods, fostering insights into retrieval effectiveness, citation accuracy, and context engineering in Deep-Research system.', 'score': 15, 'issue_id': 5295, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': 'bbe851198d9c0416', 'authors': ['Zijian Chen', 'Xueguang Ma', 'Shengyao Zhuang', 'Ping Nie', 'Kai Zou', 'Andrew Liu', 'Joshua Green', 'Kshama Patel', 'Ruoxi Meng', 'Mingyi Su', 'Sahel Sharifymoghaddam', 'Yanxi Li', 'Haoran Hong', 'Xinyu Shi', 'Xuye Liu', 'Nandan Thakur', 'Crystina Zhang', 'Luyu Gao', 'Wenhu Chen', 'Jimmy Lin'], 'affiliations': ['CSIRO', 'Carnegie Mellon University', 'Independent', 'The University of Queensland', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2508.06600.jpg', 'data': {'categories': ['#interpretability', '#rag', '#ethics', '#benchmark', '#agents', '#open_source'], 'emoji': '🔬', 'ru': {'title': 'BrowseComp-Plus: Прозрачная оценка агентов глубокого исследования', 'desc': 'BrowseComp-Plus - это новый эталонный тест для оценки агентов глубокого исследования и методов извлечения информации. Он использует фиксированный, тщательно отобранный корпус документов, что позволяет проводить контролируемые эксперименты. Тест эффективно различает производительность различных систем глубокого исследования, например, показывая, что GPT-5 достигает точности 55.9%, а при интеграции с ретривером Qwen3-Embedding-8B точность повышается до 70.1%. BrowseComp-Plus способствует получению новых знаний об эффективности извлечения информации, точности цитирования и инженерии контекста в системах глубокого исследования.'}, 'en': {'title': 'BrowseComp-Plus: A Fair Benchmark for Deep Research Evaluation', 'desc': 'BrowseComp-Plus is a new benchmark designed to evaluate deep research agents and retrieval methods in a controlled manner. It addresses limitations of existing benchmarks by using a fixed, curated document corpus, allowing for fair comparisons and reproducibility. The benchmark includes human-verified documents and challenging negatives for each query, enabling detailed analysis of the performance of different deep research systems. Results show significant improvements in accuracy when using advanced models like GPT-5, highlighting the effectiveness of this new evaluation framework.'}, 'zh': {'title': 'BrowseComp-Plus：深度研究的公平评估工具', 'desc': 'BrowseComp-Plus是一个经过精心策划的基准测试，旨在对深度研究代理和检索方法进行控制评估。它解决了现有基准测试在公平性和透明性方面的局限性，通过使用固定的文档库来确保实验的可控性。每个查询都包含经过人工验证的支持文档和具有挑战性的负样本，从而使得实验结果更具可信度。该基准测试有效区分了不同深度研究系统的性能，提供了对检索有效性和引用准确性的深入分析。'}}}, {'id': 'https://huggingface.co/papers/2508.07917', 'title': 'MolmoAct: Action Reasoning Models that can Reason in Space', 'url': 'https://huggingface.co/papers/2508.07917', 'abstract': 'Action Reasoning Models (ARMs) integrate perception, planning, and control to enable adaptable and explainable robotic behavior, achieving superior performance across various tasks and settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning is central to purposeful action, yet most robotic foundation models map perception and instructions directly to control, which limits adaptability, generalization, and semantic grounding. We introduce Action Reasoning Models (ARMs), a class of vision-language-action models that integrate perception, planning, and control through a structured three-stage pipeline. Our model, MolmoAct, encodes observations and instructions into depth-aware perception tokens, generates mid-level spatial plans as editable trajectory traces, and predicts precise low-level actions, enabling explainable and steerable behavior. MolmoAct-7B-D achieves strong performance across simulation and real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching tasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks; and in real-world fine-tuning, an additional 10% (single-arm) and an additional 22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines by an additional 23.3% on out-of-distribution generalization and achieves top human-preference scores for open-ended instruction following and trajectory steering. Furthermore, we release, for the first time, the MolmoAct Dataset -- a mid-training robot dataset comprising over 10,000 high quality robot trajectories across diverse scenarios and tasks. Training with this dataset yields an average 5.5% improvement in general performance over the base model. We release all model weights, training code, our collected dataset, and our action reasoning dataset, establishing MolmoAct as both a state-of-the-art robotics foundation model and an open blueprint for building ARMs that transform perception into purposeful action through structured reasoning. Blogpost: https://allenai.org/blog/molmoact', 'score': 14, 'issue_id': 5294, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': 'fa9d385dde41879a', 'authors': ['Jason Lee', 'Jiafei Duan', 'Haoquan Fang', 'Yuquan Deng', 'Shuo Liu', 'Boyang Li', 'Bohan Fang', 'Jieyu Zhang', 'Yi Ru Wang', 'Sangho Lee', 'Winson Han', 'Wilbert Pumacay', 'Angelica Wu', 'Rose Hendrix', 'Karen Farley', 'Eli VanderBilt', 'Ali Farhadi', 'Dieter Fox', 'Ranjay Krishna'], 'affiliations': ['Allen Institute for AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2508.07917.jpg', 'data': {'categories': ['#open_source', '#robotics', '#agents', '#dataset', '#reasoning', '#training'], 'emoji': '🤖', 'ru': {'title': 'MolmoAct: Разумные действия роботов через структурированное рассуждение', 'desc': 'Модель Action Reasoning Models (ARM) интегрирует восприятие, планирование и управление для адаптивного и объяснимого поведения роботов. MolmoAct, реализация ARM, кодирует наблюдения и инструкции в токены восприятия с учетом глубины, генерирует пространственные планы в виде редактируемых траекторий и прогнозирует точные низкоуровневые действия. Модель достигает высокой производительности в симуляциях и реальных условиях, превосходя базовые модели в задачах с нулевым обучением, долгосрочном планировании и обобщении на новые распределения. Авторы также выпускают набор данных MolmoAct Dataset, содержащий более 10000 высококачественных траекторий роботов.'}, 'en': {'title': 'Transforming Perception into Purposeful Action with ARMs', 'desc': "Action Reasoning Models (ARMs) are designed to enhance robotic behavior by combining perception, planning, and control in a structured way. The proposed model, MolmoAct, processes observations and instructions into depth-aware tokens, creates editable spatial plans, and predicts specific actions, making the robot's behavior more explainable and adaptable. MolmoAct demonstrates impressive performance in both simulated and real-world tasks, outperforming existing models in accuracy and generalization. Additionally, the introduction of the MolmoAct Dataset provides valuable training data, further improving the model's capabilities and establishing a new standard in robotics."}, 'zh': {'title': '行动推理模型：将感知转化为有目的的行动', 'desc': '行动推理模型（ARMs）结合了感知、规划和控制，能够实现灵活和可解释的机器人行为，提升了在各种任务和环境中的表现。我们提出的MolmoAct模型通过一个结构化的三阶段流程，将观察和指令编码为深度感知标记，生成可编辑的中级空间计划，并预测精确的低级动作。该模型在模拟和现实世界中表现出色，尤其在长时间任务上超越了现有的基线模型。我们还首次发布了MolmoAct数据集，包含超过10,000条高质量机器人轨迹，训练该数据集可显著提升模型的整体性能。'}}}, {'id': 'https://huggingface.co/papers/2508.05614', 'title': 'OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks', 'url': 'https://huggingface.co/papers/2508.05614', 'abstract': "OmniEAR evaluates language models' embodied reasoning capabilities in physical interactions, tool usage, and multi-agent coordination, revealing performance degradation under constraints and highlighting architectural limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models excel at abstract reasoning but their capacity for embodied agent reasoning remains largely unexplored. We present OmniEAR, a comprehensive framework for evaluating how language models reason about physical interactions, tool usage, and multi-agent coordination in embodied tasks. Unlike existing benchmarks that provide predefined tool sets or explicit collaboration directives, OmniEAR requires agents to dynamically acquire capabilities and autonomously determine coordination strategies based on task demands. Through text-based environment representation, we model continuous physical properties and complex spatial relationships across 1,500 scenarios spanning household and industrial domains. Our systematic evaluation reveals severe performance degradation when models must reason from constraints: while achieving 85-96% success with explicit instructions, performance drops to 56-85% for tool reasoning and 63-85% for implicit collaboration, with compound tasks showing over 50% failure rates. Surprisingly, complete environmental information degrades coordination performance, indicating models cannot filter task-relevant constraints. Fine-tuning improves single-agent tasks dramatically (0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing fundamental architectural limitations. These findings demonstrate that embodied reasoning poses fundamentally different challenges than current models can address, establishing OmniEAR as a rigorous benchmark for evaluating and advancing embodied AI systems. Our code and data are included in the supplementary materials and will be open-sourced upon acceptance.", 'score': 14, 'issue_id': 5294, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'd9c4313e65f213fe', 'authors': ['Zixuan Wang', 'Dingming Li', 'Hongxing Li', 'Shuo Chen', 'Yuchen Yan', 'Wenqi Zhang', 'Yongliang Shen', 'Weiming Lu', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05614.jpg', 'data': {'categories': ['#architecture', '#open_source', '#agents', '#reasoning', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'OmniEAR: раскрывая ограничения языковых моделей в воплощенном рассуждении', 'desc': 'OmniEAR - это комплексная система оценки способностей языковых моделей к воплощенному рассуждению в физических взаимодействиях, использовании инструментов и координации между агентами. Фреймворк моделирует непрерывные физические свойства и сложные пространственные отношения в 1500 сценариях бытовых и промышленных задач. Оценка выявила значительное снижение производительности моделей при необходимости рассуждать с учетом ограничений, особенно в сложных задачах. Результаты показывают, что воплощенное рассуждение представляет принципиально иные проблемы, чем те, с которыми могут справиться современные модели.'}, 'en': {'title': 'Evaluating Language Models in Real-World Reasoning Tasks', 'desc': 'OmniEAR is a framework designed to assess how well language models can reason in real-world scenarios involving physical interactions, tool usage, and teamwork. It highlights that while these models perform well with clear instructions, their performance significantly drops when faced with constraints or when they need to figure things out on their own. The study shows that even with complete information about the environment, models struggle to coordinate effectively, revealing limitations in their architecture. Overall, OmniEAR serves as a new benchmark to push the boundaries of embodied AI capabilities.'}, 'zh': {'title': 'OmniEAR：评估语言模型的体现推理能力', 'desc': 'OmniEAR是一个评估语言模型在物理交互、工具使用和多智能体协调中的体现推理能力的框架。研究发现，当模型在约束条件下进行推理时，性能显著下降，尤其是在工具推理和隐性协作任务中。尽管在明确指令下模型的成功率高达85-96%，但在复杂任务中失败率超过50%。这些结果表明，现有模型在处理体现推理时面临根本性的挑战，OmniEAR为评估和推动体现人工智能系统提供了严格的基准。'}}}, {'id': 'https://huggingface.co/papers/2508.07785', 'title': 'Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts', 'url': 'https://huggingface.co/papers/2508.07785', 'abstract': 'Grove MoE, a novel architecture with heterogeneous experts of varying sizes, improves computational efficiency and performance in large language models by dynamically activating parameters based on input complexity.  \t\t\t\t\tAI-generated summary \t\t\t\t The Mixture of Experts (MoE) architecture is a cornerstone of modern state-of-the-art (SOTA) large language models (LLMs). MoE models facilitate scalability by enabling sparse parameter activation. However, traditional MoE architecture uses homogeneous experts of a uniform size, activating a fixed number of parameters irrespective of input complexity and thus limiting computational efficiency. To overcome this limitation, we introduce Grove MoE, a novel architecture incorporating experts of varying sizes, inspired by the heterogeneous big.LITTLE CPU architecture. This architecture features novel adjugate experts with a dynamic activation mechanism, enabling model capacity expansion while maintaining manageable computational overhead. Building on this architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model during mid-training and post-training. GroveMoE models dynamically activate 3.14-3.28B parameters based on token complexity and achieve performance comparable to SOTA open-source models of similar or even larger size.', 'score': 12, 'issue_id': 5302, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': '000c1f70e8499f7a', 'authors': ['Haoyuan Wu', 'Haoxing Chen', 'Xiaodong Chen', 'Zhanchao Zhou', 'Tieyuan Chen', 'Yihong Zhuang', 'Guoshan Lu', 'Zenan Huang', 'Junbo Zhao', 'Lin Liu', 'Zhenzhong Lan', 'Bei Yu', 'Jianguo Li'], 'affiliations': ['Inclusion AI', 'Renmin University of China', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.07785.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#open_source'], 'emoji': '🌳', 'ru': {'title': 'Гетерогенные эксперты для эффективных языковых моделей', 'desc': 'Grove MoE - это новая архитектура для больших языковых моделей, использующая экспертов разного размера. Она динамически активирует параметры в зависимости от сложности входных данных, что повышает вычислительную эффективность. Авторы представили модели GroveMoE-Base и GroveMoE-Inst с 33 миллиардами параметров, созданные на основе Qwen3-30B-A3B-Base. Эти модели показывают производительность на уровне современных открытых моделей аналогичного или даже большего размера.'}, 'en': {'title': 'Dynamic Efficiency with Heterogeneous Experts in Language Models', 'desc': 'Grove MoE introduces a new architecture for large language models that uses heterogeneous experts of different sizes to enhance efficiency and performance. Unlike traditional Mixture of Experts (MoE) models that activate a fixed number of parameters regardless of input complexity, Grove MoE dynamically activates parameters based on the complexity of the input. This approach allows for better scalability and computational efficiency by only using the necessary resources for each task. The resulting models, GroveMoE-Base and GroveMoE-Inst, demonstrate competitive performance while managing a lower computational load by activating between 3.14 to 3.28 billion parameters as needed.'}, 'zh': {'title': '动态激活，提升效率的Grove MoE架构', 'desc': 'Grove MoE是一种新颖的混合专家架构，采用不同大小的专家，以提高大型语言模型的计算效率和性能。与传统的均匀专家架构不同，Grove MoE根据输入复杂性动态激活参数，从而克服了固定参数激活的限制。该架构灵感来源于异构的big.LITTLE CPU架构，具有动态激活机制，能够在保持可控计算开销的同时扩展模型容量。通过这种架构，我们开发了GroveMoE-Base和GroveMoE-Inst这两种大型语言模型，能够根据令牌复杂性动态激活3.14-3.28B参数，性能与同类或更大规模的开源模型相当。'}}}, {'id': 'https://huggingface.co/papers/2508.06026', 'title': 'Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via\n  Past-Future', 'url': 'https://huggingface.co/papers/2508.06026', 'abstract': "Temporal Self-Rewarding Language Models improve generative capabilities by strategically using past and future model outputs, enhancing preference learning and out-of-distribution generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-Rewarding Language Models propose an architecture in which the Large Language Models(LLMs) both generates responses and evaluates its own outputs via LLM-as-a-Judge prompting, dynamically improving its generative capabilities through iterative Direct Preference Optimization (DPO). However, our analysis reveals a critical limitation in existing Self-Rewarding paradigms: the synchronized improvement of chosen and rejected responses progressively narrows the representational difference between contrasting samples, undermining effective preference learning. We propose Temporal Self-Rewarding Language Models that strategically coordinate past, present, and future model generations to sustain learning signals. Our dual-phase framework introduces: (1) Anchored Rejection - fixing rejected responses using the past initial model's outputs and (2) Future-Guided Chosen - dynamically curating chosen samples using next-generation model predictions. Extensive experiments across three model families (Llama, Qwen, Mistral) and different model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained with our method compared to Self-Rewarding using same computation resources. For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our method also demonstrates superior out-of-distribution generalization across mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code generation (HumanEval) tasks, even though we do not specifically collect such training data.", 'score': 10, 'issue_id': 5296, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': '9118094d6c4509d1', 'authors': ['Yidong Wang', 'Xin Wang', 'Cunxiang Wang', 'Junfeng Fang', 'Qiufeng Wang', 'Jianing Chu', 'Xuran Meng', 'Shuxun Yang', 'Libo Qin', 'Yue Zhang', 'Wei Ye', 'Shikun Zhang'], 'affiliations': ['Beijing Institute of Technology', 'Central South University', 'National University of Singapore', 'North Carolina State University', 'Peking University', 'Southeast University', 'Tsinghua University', 'University of Michigan', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2508.06026.jpg', 'data': {'categories': ['#training', '#agi', '#rlhf', '#architecture', '#optimization', '#reasoning', '#rl'], 'emoji': '⏳', 'ru': {'title': 'Временная самокоррекция: новый шаг в развитии языковых моделей', 'desc': 'Предложена новая архитектура языковых моделей под названием Temporal Self-Rewarding Language Models. Эта архитектура улучшает генеративные способности моделей, стратегически используя прошлые и будущие выходные данные модели. Метод включает в себя две фазы: фиксацию отвергнутых ответов с использованием выходных данных начальной модели и динамический отбор выбранных образцов с использованием предсказаний модели следующего поколения. Эксперименты показали значительные улучшения по сравнению с базовыми методами самовознаграждения, особенно в задачах вне распределения обучающих данных.'}, 'en': {'title': 'Enhancing Language Models with Temporal Self-Rewarding Strategies', 'desc': 'This paper introduces Temporal Self-Rewarding Language Models, which enhance the generative abilities of language models by effectively utilizing outputs from past and future generations. The proposed architecture allows models to generate responses and evaluate them through a self-judging mechanism, improving performance via Direct Preference Optimization. A key innovation is the dual-phase framework that includes Anchored Rejection and Future-Guided Chosen strategies, which help maintain diverse learning signals and improve preference learning. Experimental results show that this approach significantly outperforms traditional Self-Rewarding methods across various tasks and model sizes, demonstrating better generalization capabilities.'}, 'zh': {'title': '时间自奖励模型：提升生成能力的新策略', 'desc': '本文提出了一种时间自奖励语言模型，通过战略性地利用过去和未来的模型输出，提升生成能力和偏好学习。该模型采用了自我评估机制，使大型语言模型（LLMs）能够生成响应并评估自身输出，从而通过直接偏好优化（DPO）不断改进生成能力。研究发现，现有自奖励模型存在一个关键限制，即选择和拒绝响应的同步改进会逐渐缩小对比样本之间的表示差异，影响有效的偏好学习。为此，本文提出了时间自奖励语言模型，通过协调过去、现在和未来的生成，保持学习信号，从而显著提高模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2508.08189', 'title': 'Reinforcement Learning in Vision: A Survey', 'url': 'https://huggingface.co/papers/2508.08189', 'abstract': 'This survey synthesizes recent advancements in visual reinforcement learning, covering policy optimization strategies, thematic pillars, and evaluation protocols, while highlighting open challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.', 'score': 7, 'issue_id': 5298, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': 'ba135022f16de1cf', 'authors': ['Weijia Wu', 'Chen Gao', 'Joya Chen', 'Kevin Qinghong Lin', 'Qingwei Meng', 'Yiming Zhang', 'Yuke Qiu', 'Hong Zhou', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.08189.jpg', 'data': {'categories': ['#multimodal', '#cv', '#training', '#survey', '#optimization', '#games', '#rl', '#rlhf', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Визуальное обучение с подкреплением: на пути к разумным агентам', 'desc': 'Этот обзор синтезирует последние достижения в области визуального обучения с подкреплением. Рассматриваются стратегии оптимизации политик, тематические направления и протоколы оценки. Особое внимание уделяется четырем основным областям: мультимодальные большие языковые модели, визуальная генерация, унифицированные модельные фреймворки и модели, объединяющие зрение, язык и действие. Обзор также выделяет открытые проблемы, включая эффективность использования данных, обобщение и безопасное развертывание.'}, 'en': {'title': 'Mapping the Future of Visual Reinforcement Learning', 'desc': 'This paper surveys the latest developments in visual reinforcement learning (VRL), focusing on how agents can understand and interact with complex visual environments. It categorizes over 200 studies into four main themes, including multi-modal models and vision-language-action frameworks, while discussing advancements in policy optimization techniques. The authors also evaluate various protocols for assessing VRL performance and identify key challenges such as improving sample efficiency and ensuring safe deployment. Overall, the survey aims to provide a comprehensive overview of the field and suggest future research directions.'}, 'zh': {'title': '视觉强化学习的前沿探索', 'desc': '这篇综述文章总结了视觉强化学习的最新进展，涵盖了策略优化策略、主题支柱和评估协议，同时强调了开放挑战。文章首先形式化了视觉强化学习问题，并追踪了从RLHF到可验证奖励范式的策略优化演变。接着，将200多篇代表性作品组织成四个主题支柱：多模态大语言模型、视觉生成、统一模型框架和视觉-语言-行动模型。最后，文章回顾了评估协议，并识别了样本效率、泛化和安全部署等开放挑战。'}}}, {'id': 'https://huggingface.co/papers/2508.08134', 'title': 'Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control', 'url': 'https://huggingface.co/papers/2508.08134', 'abstract': 'Follow-Your-Shape framework uses a Trajectory Divergence Map and Scheduled KV Injection to enable precise and controllable shape editing in images while preserving non-target content.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios -- particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded background quality. We propose Follow-Your-Shape, a training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly preserving non-target content. Motivated by the divergence between inversion and editing trajectories, we compute a Trajectory Divergence Map (TDM) by comparing token-wise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides a Scheduled KV Injection mechanism that ensures stable and faithful editing. To facilitate a rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement.', 'score': 6, 'issue_id': 5295, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': '46943e7de387bfc9', 'authors': ['Zeqian Long', 'Mingzhe Zheng', 'Kunyu Feng', 'Xinhua Zhang', 'Hongyu Liu', 'Harry Yang', 'Linfeng Zhang', 'Qifeng Chen', 'Yue Ma'], 'affiliations': ['HKUST', 'Shanghai Jiao Tong University', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2508.08134.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#cv', '#games'], 'emoji': '🔄', 'ru': {'title': 'Точное редактирование форм объектов с сохранением фона', 'desc': 'Статья представляет новый фреймворк Follow-Your-Shape для точного и контролируемого редактирования форм объектов на изображениях. Метод использует карту расхождения траекторий (Trajectory Divergence Map) для локализации редактируемых областей. Предложенный механизм планового введения ключей и значений (Scheduled KV Injection) обеспечивает стабильное и точное редактирование. Авторы также представляют новый бенчмарк ReShapeBench для оценки методов редактирования форм.'}, 'en': {'title': 'Precision in Shape Editing with Follow-Your-Shape', 'desc': 'The Follow-Your-Shape framework introduces a novel approach for shape editing in images, focusing on maintaining the quality of non-target content. It utilizes a Trajectory Divergence Map (TDM) to analyze differences in editing paths, allowing for precise localization of areas that can be modified. Additionally, the Scheduled KV Injection mechanism ensures that the editing process remains stable and accurate. This method outperforms existing models, especially in complex scenarios involving significant shape transformations, while also introducing a new benchmark for evaluating shape-aware editing.'}, 'zh': {'title': '精确可控的形状编辑新方法', 'desc': 'Follow-Your-Shape框架利用轨迹发散图和调度KV注入技术，实现了图像中形状的精确和可控编辑，同时保持非目标内容的完整性。该方法解决了现有基于流的图像编辑模型在大规模形状变换中的不足，避免了对非目标区域的意外修改。通过计算反演和去噪路径之间的速度差异，Follow-Your-Shape能够精确定位可编辑区域。我们还引入了ReShapeBench基准，专门用于形状感知编辑的评估，实验结果表明该方法在大规模形状替换任务中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2508.08221', 'title': 'Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning', 'url': 'https://huggingface.co/papers/2508.08221', 'abstract': 'A systematic review of reinforcement learning techniques for large language model reasoning reveals clear guidelines and demonstrates that a minimalist combination of techniques can improve performance over existing strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning for LLM reasoning has rapidly emerged as a prominent research area, marked by a significant surge in related studies on both algorithmic innovations and practical applications. Despite this progress, several critical challenges remain, including the absence of standardized guidelines for employing RL techniques and a fragmented understanding of their underlying mechanisms. Additionally, inconsistent experimental settings, variations in training data, and differences in model initialization have led to conflicting conclusions, obscuring the key characteristics of these techniques and creating confusion among practitioners when selecting appropriate techniques. This paper systematically reviews widely adopted RL techniques through rigorous reproductions and isolated evaluations within a unified open-source framework. We analyze the internal mechanisms, applicable scenarios, and core principles of each technique through fine-grained experiments, including datasets of varying difficulty, model sizes, and architectures. Based on these insights, we present clear guidelines for selecting RL techniques tailored to specific setups, and provide a reliable roadmap for practitioners navigating the RL for the LLM domain. Finally, we reveal that a minimalist combination of two techniques can unlock the learning capability of critic-free policies using vanilla PPO loss. The results demonstrate that our simple combination consistently improves performance, surpassing strategies like GRPO and DAPO.', 'score': 5, 'issue_id': 5300, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': 'f0862b77d91584b6', 'authors': ['Zihe Liu', 'Jiashun Liu', 'Yancheng He', 'Weixun Wang', 'Jiaheng Liu', 'Ling Pan', 'Xinyu Hu', 'Shaopan Xiong', 'Ju Huang', 'Jian Hu', 'Shengyi Huang', 'Siran Yang', 'Jiamang Wang', 'Wenbo Su', 'Bo Zheng'], 'affiliations': ['Alibaba Group', 'Beijing Jiaotong University', 'CleanRL', 'Hong Kong University of Science and Technology', 'Nanjing University', 'OpenRLHF', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2508.08221.jpg', 'data': {'categories': ['#survey', '#rlhf', '#training', '#reasoning', '#rl', '#open_source', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Упрощение обучения с подкреплением для улучшения рассуждений ИИ', 'desc': 'Статья представляет систематический обзор методов обучения с подкреплением для рассуждений больших языковых моделей. Авторы анализируют внутренние механизмы, применимые сценарии и основные принципы каждого метода через тщательные эксперименты. На основе полученных результатов предлагаются четкие рекомендации по выбору техник обучения с подкреплением для конкретных задач. Исследование показывает, что минималистичная комбинация двух техник может улучшить производительность по сравнению с существующими стратегиями.'}, 'en': {'title': 'Streamlining Reinforcement Learning for Better Language Model Reasoning', 'desc': 'This paper reviews reinforcement learning (RL) techniques specifically for reasoning in large language models (LLMs). It highlights the lack of standardized guidelines and the confusion caused by inconsistent experimental setups in the field. Through systematic evaluations, the authors identify key characteristics of various RL methods and propose a minimalist approach that combines two techniques to enhance performance. The findings suggest that this simple combination can effectively improve learning capabilities compared to more complex strategies.'}, 'zh': {'title': '简约组合，提升强化学习性能', 'desc': '这篇论文系统性地回顾了强化学习在大型语言模型推理中的应用，提出了明确的指导方针。研究表明，采用简约的技术组合可以提升现有策略的性能。尽管相关研究迅速增加，但仍存在标准化指导缺失和机制理解不清等挑战。通过细致的实验，论文为从业者提供了选择强化学习技术的可靠路线图。'}}}, {'id': 'https://huggingface.co/papers/2508.07101', 'title': 'Less Is More: Training-Free Sparse Attention with Global Locality for\n  Efficient Reasoning', 'url': 'https://huggingface.co/papers/2508.07101', 'abstract': 'LessIsMore is a training-free sparse attention mechanism that improves efficiency and generalization in reasoning tasks by aggregating token selections from local attention heads.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models achieve strong performance through test-time scaling but incur substantial computational overhead, particularly from excessive token generation when processing short input prompts. While sparse attention mechanisms can reduce latency and memory usage, existing approaches suffer from significant accuracy degradation due to accumulated errors during long-generation reasoning. These methods generally require either high token retention rates or expensive retraining. We introduce LessIsMore, a training-free sparse attention mechanism for reasoning tasks, which leverages global attention patterns rather than relying on traditional head-specific local optimizations. LessIsMore aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers. This unified selection improves generalization and efficiency by avoiding the need to maintain separate token subsets per head. Evaluation across diverse reasoning tasks and benchmarks shows that LessIsMore preserves -- and in some cases improves -- accuracy while achieving a 1.1times average decoding speed-up compared to full attention. Moreover, LessIsMore attends to 2times fewer tokens without accuracy loss, achieving a 1.13times end-to-end speed-up compared to existing sparse attention methods.', 'score': 5, 'issue_id': 5296, 'pub_date': '2025-08-09', 'pub_date_card': {'ru': '9 августа', 'en': 'August 9', 'zh': '8月9日'}, 'hash': '7365d6ccd543770b', 'authors': ['Lijie Yang', 'Zhihao Zhang', 'Arti Jain', 'Shijie Cao', 'Baihong Yuan', 'Yiwei Chen', 'Zhihao Jia', 'Ravi Netravali'], 'affiliations': ['Carnegie Mellon University', 'Microsoft Research', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2508.07101.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Меньше значит больше: эффективное разреженное внимание для улучшенного рассуждения', 'desc': 'LessIsMore - это механизм разреженного внимания, не требующий дополнительного обучения, который улучшает эффективность и обобщающую способность в задачах рассуждения. Он агрегирует выбор токенов из локальных голов внимания с недавней контекстной информацией, что позволяет проводить единый ранжирование токенов для будущих слоев декодирования. Этот подход улучшает обобщение и эффективность, избегая необходимости поддерживать отдельные подмножества токенов для каждой головы. Оценка на различных задачах рассуждения показывает, что LessIsMore сохраняет или даже улучшает точность, при этом ускоряя декодирование в среднем в 1.1 раза по сравнению с полным вниманием.'}, 'en': {'title': 'LessIsMore: Efficient Reasoning with Sparse Attention', 'desc': 'LessIsMore is a novel sparse attention mechanism designed to enhance efficiency and generalization in reasoning tasks without the need for retraining. It aggregates token selections from local attention heads, utilizing global attention patterns to improve the decision-making process during decoding. This approach allows for a unified ranking of tokens across different heads, which reduces the number of tokens processed while maintaining or even improving accuracy. Evaluations demonstrate that LessIsMore achieves faster decoding speeds and lower token usage compared to traditional sparse attention methods, making it a significant advancement in the field.'}, 'zh': {'title': 'LessIsMore：高效推理的新选择', 'desc': 'LessIsMore是一种无训练的稀疏注意力机制，旨在提高推理任务的效率和泛化能力。它通过聚合来自局部注意力头的标记选择，利用全局注意力模式，而不是依赖传统的头特定局部优化。该方法在推理过程中避免了维护每个头的独立标记子集，从而提高了通用性和效率。评估结果表明，LessIsMore在保持或提高准确性的同时，解码速度平均提升了1.1倍，并且在不损失准确性的情况下，关注的标记数量减少了2倍。'}}}, {'id': 'https://huggingface.co/papers/2508.05257', 'title': 'MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs', 'url': 'https://huggingface.co/papers/2508.05257', 'abstract': 'A novel Mixture-of-Basis-Experts (MoBE) method is introduced to compress large language models with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t The Mixture-of-Experts (MoE) architecture has become a predominant paradigm for scaling large language models (LLMs). Despite offering strong performance and computational efficiency, large MoE-based LLMs like DeepSeek-V3-0324 and Kimi-K2-Instruct present serious challenges due to substantial memory requirements in deployment. While recent works have explored MoE compression to address this issue, existing methods often suffer from considerable accuracy drops (e.g., 7-14% relatively) even at modest compression rates. This paper introduces a novel Mixture-of-Basis-Experts (MoBE) method that achieves model compression while incurring minimal accuracy drops. Specifically, each up/gate matrix in an expert is decomposed via a rank decomposition as W = AB, where matrix A is unique to each expert. The relatively larger matrix B is further re-parameterized as a linear combination of basis matrices {Bi} shared across all experts within a given MoE layer. The factorization is learned by minimizing the reconstruction error relative to the original weight matrices. Experiments demonstrate that MoBE achieves notably lower accuracy drops compared to prior works. For instance, MoBE can reduce the parameter counts of Qwen3-235B-A22B-2507, DeepSeek-V3-0324 (671B) and Kimi-K2-Instruct (1T) by 24%-30% with only 1%-2% accuracy drop (about 2% drops when measured relatively).', 'score': 4, 'issue_id': 5302, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'fe0cf85d04556fd4', 'authors': ['Xiaodong Chen', 'Mingming Ha', 'Zhenzhong Lan', 'Jing Zhang', 'Jianguo Li'], 'affiliations': ['Inclusion AI', 'Renmin University of China', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05257.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization', '#inference'], 'emoji': '🗜️', 'ru': {'title': 'Эффективное сжатие языковых моделей без потери качества', 'desc': 'Представлен новый метод Mixture-of-Basis-Experts (MoBE) для сжатия больших языковых моделей с минимальной потерей точности. MoBE разлагает матрицы экспертов на уникальную для эксперта матрицу A и общую базисную матрицу B, которая параметризуется как линейная комбинация базисных матриц. Эксперименты показывают, что MoBE позволяет сократить количество параметров крупных моделей на 24-30% при снижении точности всего на 1-2%. Этот метод значительно превосходит существующие подходы к сжатию моделей архитектуры Mixture-of-Experts.'}, 'en': {'title': 'Efficient Compression with Minimal Accuracy Loss', 'desc': 'This paper presents a new method called Mixture-of-Basis-Experts (MoBE) for compressing large language models while maintaining high accuracy. The MoBE approach uses a unique decomposition of the expert matrices, allowing for efficient parameter sharing across experts in a Mixture-of-Experts architecture. By minimizing reconstruction error, the method significantly reduces the number of parameters needed without a substantial loss in performance. Experimental results show that MoBE can compress models by 24%-30% with only a 1%-2% drop in accuracy, outperforming existing compression techniques.'}, 'zh': {'title': '基础专家混合：高效压缩，低损失', 'desc': '本文介绍了一种新颖的基础专家混合（MoBE）方法，用于在压缩大型语言模型时尽量减少准确性损失。MoBE通过对专家中的上/门矩阵进行秩分解，将其表示为W = AB，其中矩阵A是每个专家独有的。相对较大的矩阵B则被重新参数化为在给定MoE层内所有专家共享的基础矩阵的线性组合。实验表明，MoBE在压缩参数的同时，准确性损失显著低于以往方法。'}}}, {'id': 'https://huggingface.co/papers/2508.07493', 'title': 'VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation\n  for Multilingual Long Document Understanding', 'url': 'https://huggingface.co/papers/2508.07493', 'abstract': 'VisR-Bench is a multilingual benchmark for question-driven multimodal retrieval in long documents, evaluating various models across different languages and question types.  \t\t\t\t\tAI-generated summary \t\t\t\t Most organizational data in this world are stored as documents, and visual retrieval plays a crucial role in unlocking the collective intelligence from all these documents. However, existing benchmarks focus on English-only document retrieval or only consider multilingual question-answering on a single-page image. To bridge this gap, we introduce VisR-Bench, a multilingual benchmark designed for question-driven multimodal retrieval in long documents. Our benchmark comprises over 35K high-quality QA pairs across 1.2K documents, enabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans sixteen languages with three question types (figures, text, and tables), offering diverse linguistic and question coverage. Unlike prior datasets, we include queries without explicit answers, preventing models from relying on superficial keyword matching. We evaluate various retrieval models, including text-based methods, multimodal encoders, and MLLMs, providing insights into their strengths and limitations. Our results show that while MLLMs significantly outperform text-based and multimodal encoder models, they still struggle with structured tables and low-resource languages, highlighting key challenges in multilingual visual retrieval.', 'score': 3, 'issue_id': 5298, 'pub_date': '2025-08-10', 'pub_date_card': {'ru': '10 августа', 'en': 'August 10', 'zh': '8月10日'}, 'hash': 'a47e8d1decc51e3a', 'authors': ['Jian Chen', 'Ming Li', 'Jihyung Kil', 'Chenguang Wang', 'Tong Yu', 'Ryan Rossi', 'Tianyi Zhou', 'Changyou Chen', 'Ruiyi Zhang'], 'affiliations': ['Adobe Research', 'University at Buffalo', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2508.07493.jpg', 'data': {'categories': ['#long_context', '#multimodal', '#low_resource', '#multilingual', '#dataset', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'VisR-Bench: многоязычный вызов для мультимодального поиска в документах', 'desc': 'VisR-Bench - это многоязычный бенчмарк для мультимодального поиска в длинных документах, управляемого вопросами. Он включает более 35 тысяч пар вопрос-ответ на 16 языках, охватывая различные типы вопросов (о рисунках, тексте и таблицах). Бенчмарк позволяет оценивать различные модели, включая текстовые методы, мультимодальные энкодеры и мультиязычные языковые модели (MLLM). Результаты показывают, что MLLM значительно превосходят другие подходы, но всё ещё испытывают трудности со структурированными таблицами и малоресурсными языками.'}, 'en': {'title': 'Unlocking Multilingual Insights from Long Documents', 'desc': 'VisR-Bench is a new benchmark for evaluating how well models can retrieve information from long documents using questions in multiple languages. It includes over 35,000 question-answer pairs from 1,200 documents, covering various types of questions about figures, text, and tables. This benchmark is unique because it tests models on queries that do not have clear answers, which helps to avoid simple keyword matching. The findings reveal that while multilingual large language models (MLLMs) perform better than traditional text-based and multimodal models, they still face difficulties with structured data and languages with fewer resources.'}, 'zh': {'title': '多语言文档检索的新基准', 'desc': 'VisR-Bench是一个多语言基准，用于在长文档中进行基于问题的多模态检索。该基准包含超过35,000个高质量的问答对，涵盖1,200个文档，支持对多模态检索的细致评估。它涉及十六种语言和三种问题类型（图形、文本和表格），提供了丰富的语言和问题覆盖。研究结果表明，尽管多语言大模型在性能上优于文本基础和多模态编码器模型，但在处理结构化表格和低资源语言时仍面临挑战。'}}}, {'id': 'https://huggingface.co/papers/2508.06426', 'title': 'Shortcut Learning in Generalist Robot Policies: The Role of Dataset\n  Diversity and Fragmentation', 'url': 'https://huggingface.co/papers/2508.06426', 'abstract': 'Shortcut learning in generalist robot policies trained on large-scale datasets limits generalization, and this can be mitigated through improved dataset collection and data augmentation strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Generalist robot policies trained on large-scale datasets such as Open X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks. However, they often struggle to generalize beyond the distribution of their training data. In this paper, we investigate the underlying cause of this limited generalization capability. We identify shortcut learning -- the reliance on task-irrelevant features -- as a key impediment to generalization. Through comprehensive theoretical and empirical analysis, we uncover two primary contributors to shortcut learning: (1) limited diversity within individual sub-datasets, and (2) significant distributional disparities across sub-datasets, leading to dataset fragmentation. These issues arise from the inherent structure of large-scale datasets like OXE, which are typically composed of multiple sub-datasets collected independently across varied environments and embodiments. Our findings provide critical insights into dataset collection strategies that can reduce shortcut learning and enhance the generalization ability of generalist robot policies. Moreover, in scenarios where acquiring new large-scale data is impractical, we demonstrate that carefully selected robotic data augmentation strategies can effectively reduce shortcut learning in existing offline datasets, thereby improving generalization capabilities of generalist robot policies, e.g., pi_0, in both simulation and real-world environments. More information at https://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.', 'score': 3, 'issue_id': 5299, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': 'cf0dd977bbaaf7ba', 'authors': ['Youguang Xing', 'Xu Luo', 'Junlin Xie', 'Lianli Gao', 'Hengtao Shen', 'Jingkuan Song'], 'affiliations': ['Tongji University', 'UESTC'], 'pdf_title_img': 'assets/pdf/title_img/2508.06426.jpg', 'data': {'categories': ['#robotics', '#dataset', '#data', '#optimization', '#transfer_learning'], 'emoji': '🤖', 'ru': {'title': 'Преодоление упрощенного обучения для улучшения обобщения в политиках роботов', 'desc': 'Исследование показывает, что обобщенные политики роботов, обученные на крупномасштабных наборах данных, часто страдают от проблемы упрощенного обучения. Это приводит к ограниченной способности к обобщению за пределами распределения обучающих данных. Основными причинами являются недостаточное разнообразие внутри поднаборов данных и значительные различия между ними. Авторы предлагают улучшенные стратегии сбора данных и методы аугментации для смягчения этой проблемы и повышения способности к обобщению.'}, 'en': {'title': 'Enhancing Generalization by Tackling Shortcut Learning in Robot Policies', 'desc': 'This paper explores the issue of shortcut learning in generalist robot policies that are trained on large-scale datasets, such as Open X-Embodiment (OXE). Shortcut learning occurs when robots rely on irrelevant features from their training data, which hinders their ability to generalize to new tasks. The authors identify two main causes of this problem: a lack of diversity within sub-datasets and significant differences between these sub-datasets. They propose improved dataset collection methods and data augmentation techniques to mitigate shortcut learning and enhance the generalization capabilities of these robot policies.'}, 'zh': {'title': '减少快捷学习，提升机器人泛化能力', 'desc': '这篇论文探讨了在大规模数据集上训练的通用机器人策略中的快捷学习现象。快捷学习是指模型依赖于与任务无关的特征，从而限制了其泛化能力。研究发现，数据集内部的多样性不足和子数据集之间的分布差异是导致快捷学习的主要原因。通过改进数据集收集策略和数据增强方法，可以有效减少快捷学习，提高通用机器人策略的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2508.06059', 'title': 'Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System', 'url': 'https://huggingface.co/papers/2508.06059', 'abstract': 'Fact2Fiction is a poisoning attack framework that targets agentic fact-checking systems by exploiting their decomposition strategy and justifications, achieving higher attack success rates than existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art fact-checking systems combat misinformation at scale by employing autonomous LLM-based agents to decompose complex claims into smaller sub-claims, verify each sub-claim individually, and aggregate the partial results to produce verdicts with justifications (explanatory rationales for the verdicts). The security of these systems is crucial, as compromised fact-checkers, which tend to be easily underexplored, can amplify misinformation. This work introduces Fact2Fiction, the first poisoning attack framework targeting such agentic fact-checking systems. Fact2Fiction mirrors the decomposition strategy and exploits system-generated justifications to craft tailored malicious evidences that compromise sub-claim verification. Extensive experiments demonstrate that Fact2Fiction achieves 8.9\\%--21.2\\% higher attack success rates than state-of-the-art attacks across various poisoning budgets. Fact2Fiction exposes security weaknesses in current fact-checking systems and highlights the need for defensive countermeasures.', 'score': 2, 'issue_id': 5304, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': '77839512cae33ea5', 'authors': ['Haorui He', 'Yupeng Li', 'Bin Benjamin Zhu', 'Dacheng Wen', 'Reynold Cheng', 'Francis C. M. Lau'], 'affiliations': ['Department of Computer Science, The University of Hong Kong', 'Department of Interactive Media, Hong Kong Baptist University', 'Microsoft Corporation'], 'pdf_title_img': 'assets/pdf/title_img/2508.06059.jpg', 'data': {'categories': ['#security', '#agents'], 'emoji': '🕵️', 'ru': {'title': 'Обман автоматизированных фактчекеров: новый метод атаки на ИИ-системы проверки информации', 'desc': 'Fact2Fiction - это фреймворк атаки отравления, нацеленный на агентные системы проверки фактов. Он использует их стратегию декомпозиции и обоснования для достижения более высоких показателей успешности атаки по сравнению с существующими методами. Fact2Fiction создает вредоносные доказательства, которые компрометируют проверку подутверждений. Эксперименты показывают, что Fact2Fiction достигает на 8.9%-21.2% более высоких показателей успешности атаки по сравнению с современными атаками при различных бюджетах отравления.'}, 'en': {'title': 'Fact2Fiction: Unmasking Vulnerabilities in Fact-Checking Systems', 'desc': 'Fact2Fiction is a novel framework designed to launch poisoning attacks on agentic fact-checking systems, which use large language models (LLMs) to break down complex claims into smaller parts for verification. By mimicking the decomposition strategy and leveraging the justifications provided by these systems, Fact2Fiction creates targeted malicious evidence that can mislead the verification process. The framework has been shown to achieve significantly higher success rates in attacks compared to existing methods, indicating vulnerabilities in current fact-checking approaches. This research underscores the importance of enhancing security measures to protect against such sophisticated attacks on misinformation systems.'}, 'zh': {'title': '揭示事实检查系统的安全漏洞', 'desc': 'Fact2Fiction是一个针对自主事实检查系统的毒化攻击框架，利用其分解策略和理由进行攻击。该框架通过模仿系统的分解策略，生成定制的恶意证据，从而破坏子声明的验证过程。实验表明，Fact2Fiction的攻击成功率比现有方法高出8.9%到21.2%。这项研究揭示了当前事实检查系统的安全漏洞，并强调了需要防御措施的重要性。'}}}, {'id': 'https://huggingface.co/papers/2508.03542', 'title': 'Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations\n  and Sentences', 'url': 'https://huggingface.co/papers/2508.03542', 'abstract': 'A new open-source dataset and models for converting spoken mathematical expressions into LaTeX improve accuracy over existing benchmarks, supporting both equations and sentences in multiple languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Conversion of spoken mathematical expressions is a challenging task that involves transcribing speech into a strictly structured symbolic representation while addressing the ambiguity inherent in the pronunciation of equations. Although significant progress has been achieved in automatic speech recognition (ASR) and language models (LM), the problem of converting spoken mathematics into LaTeX remains underexplored. This task directly applies to educational and research domains, such as lecture transcription or note creation. Based on ASR post-correction, prior work requires 2 transcriptions, focuses only on isolated equations, has a limited test set, and provides neither training data nor multilingual coverage. To address these issues, we present the first fully open-source large-scale dataset, comprising over 66,000 human-annotated audio samples of mathematical equations and sentences in both English and Russian, drawn from diverse scientific domains. In addition to the ASR post-correction models and few-shot prompting, we apply audio language models, demonstrating comparable character error rate (CER) results on the MathSpeech benchmark (28% vs. 30%) for the equations conversion. In contrast, on the proposed S2L-equations benchmark, our models outperform the MathSpeech model by a substantial margin of more than 40 percentage points, even after accounting for LaTeX formatting artifacts (27% vs. 64%). We establish the first benchmark for mathematical sentence recognition (S2L-sentences) and achieve an equation CER of 40%. This work lays the groundwork for future advances in multimodal AI, with a particular focus on mathematical content recognition.', 'score': 2, 'issue_id': 5300, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'f7ca16433cbc8b8e', 'authors': ['Dmitrii Korzh', 'Dmitrii Tarasov', 'Artyom Iudin', 'Elvir Karimov', 'Matvey Skripkin', 'Nikita Kuzmin', 'Andrey Kuznetsov', 'Oleg Y. Rogov', 'Ivan Oseledets'], 'affiliations': ['AIRI', 'HSE Moscow, Russia', 'MTUCI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2508.03542.jpg', 'data': {'categories': ['#science', '#multimodal', '#benchmark', '#dataset', '#multilingual', '#open_source'], 'emoji': '🧮', 'ru': {'title': 'Прорыв в распознавании устной математики: от речи к LaTeX', 'desc': 'Представлен новый открытый набор данных и модели для преобразования устных математических выражений в LaTeX, улучшающие точность по сравнению с существующими эталонами. Набор данных содержит более 66 000 аудиозаписей уравнений и предложений на английском и русском языках из различных научных областей. Применяются модели постобработки ASR и аудио-языковые модели, демонстрирующие сопоставимые результаты на бенчмарке MathSpeech и значительное улучшение на новом бенчмарке S2L-equations. Работа закладывает основу для будущих достижений в мультимодальном ИИ с акцентом на распознавание математического контента.'}, 'en': {'title': 'Transforming Speech to LaTeX: A Leap in Mathematical Recognition', 'desc': 'This paper introduces a new open-source dataset and models designed to convert spoken mathematical expressions into LaTeX, significantly enhancing accuracy compared to existing methods. The dataset includes over 66,000 audio samples in English and Russian, addressing the challenges of ambiguity in speech and the need for structured symbolic representation. The authors demonstrate that their models achieve lower character error rates on both equations and sentences, establishing new benchmarks for mathematical recognition tasks. This work aims to advance multimodal AI applications in educational and research settings by improving the transcription of mathematical content.'}, 'zh': {'title': '口语数学表达转换的突破性进展', 'desc': '本研究提出了一个新的开源数据集和模型，用于将口语数学表达转换为LaTeX，显著提高了准确性。该数据集包含超过66,000个经过人工标注的数学方程和句子的音频样本，支持英语和俄语。研究中使用了自动语音识别（ASR）后校正模型和音频语言模型，展示了在数学方程转换上的优越性能。此项工作为未来多模态人工智能的发展奠定了基础，特别是在数学内容识别方面。'}}}, {'id': 'https://huggingface.co/papers/2508.03346', 'title': 'Compressing Chain-of-Thought in LLMs via Step Entropy', 'url': 'https://huggingface.co/papers/2508.03346', 'abstract': 'A novel CoT compression framework using step entropy and a two-stage training strategy enhances LLM inference efficiency without significantly reducing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at complex reasoning but generate verbose thought processes with considerable redundancy, leading to increased inference costs and reduced efficiency. We introduce a novel CoT compression framework based on step entropy, a metric that quantifies the informational contribution of individual reasoning steps to identify redundancy. Through theoretical analysis and extensive empirical validation on mathematical reasoning benchmarks, we demonstrate that steps with low entropy are indeed highly redundant. Our experiments reveal that an astonishing 80\\% of low-entropy intermediate steps can be pruned with minor degradation in the final answer accuracy across DeepSeek-R1-7B, 14B and Qwen3-8B. This finding sharply contrasts with random or high-entropy pruning, which severely impairs reasoning performance. Building on this, we propose a novel two-stage training strategy combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) reinforcement learning. This approach enables LLMs to autonomously learn to generate compressed COTs during inference by strategically incorporating [SKIP] tokens. Our method significantly enhances LLM inference efficiency while rigorously preserving accuracy, offering profound implications for practical LLM deployment and a deeper understanding of reasoning structures.', 'score': 2, 'issue_id': 5303, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '7961945e6a27036d', 'authors': ['Zeju Li', 'Jianyuan Zhong', 'Ziyang Zheng', 'Xiangyu Wen', 'Zhijian Xu', 'Yingying Cheng', 'Fan Zhang', 'Qiang Xu'], 'affiliations': ['Huawei Technologies Co., Ltd', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.03346.jpg', 'data': {'categories': ['#inference', '#math', '#reasoning', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективное сжатие рассуждений ИИ без потери точности', 'desc': 'Представлена новая система сжатия цепочки размышлений (CoT) для больших языковых моделей, основанная на энтропии шагов. Исследование показало, что до 80% промежуточных шагов с низкой энтропией можно удалить без существенного ухудшения точности конечного ответа. Предложена двухэтапная стратегия обучения, сочетающая контролируемую точную настройку и обучение с подкреплением. Этот метод позволяет языковым моделям автономно генерировать сжатые цепочки размышлений, значительно повышая эффективность вывода при сохранении точности.'}, 'en': {'title': 'Efficient Reasoning: Pruning Redundancy in LLMs', 'desc': 'This paper presents a new framework for compressing Chain-of-Thought (CoT) prompts in Large Language Models (LLMs) to improve inference efficiency. It uses a concept called step entropy to identify and remove redundant reasoning steps that do not significantly contribute to the final answer. The authors show that up to 80% of these low-entropy steps can be eliminated with only a slight impact on accuracy. Additionally, they introduce a two-stage training method that combines supervised fine-tuning and reinforcement learning to help LLMs learn to generate more efficient CoTs during inference.'}, 'zh': {'title': '提升推理效率，保留准确性的新方法', 'desc': '本文提出了一种新颖的链式思维（CoT）压缩框架，利用步骤熵和两阶段训练策略，提高了大型语言模型（LLM）的推理效率，同时保持了准确性。通过分析步骤熵，我们发现低熵的推理步骤往往是冗余的，实验表明可以去除80%的低熵步骤，几乎不影响最终答案的准确性。与随机或高熵的剪枝方法相比，我们的方法在保留推理性能方面表现更佳。最终，我们的框架为LLM的实际应用提供了重要的启示，并加深了对推理结构的理解。'}}}, {'id': 'https://huggingface.co/papers/2508.07662', 'title': 'GLiClass: Generalist Lightweight Model for Sequence Classification Tasks', 'url': 'https://huggingface.co/papers/2508.07662', 'abstract': 'GLiClass, an adaptation of GLiNER, achieves efficient and accurate sequence classification with zero-shot and few-shot capabilities, and PPO is adapted for multi-label text classification in data-sparse conditions.  \t\t\t\t\tAI-generated summary \t\t\t\t Classification is one of the most widespread tasks in AI applications, serving often as the first step in filtering, sorting, and categorizing data. Since modern AI systems must handle large volumes of input data and early pipeline stages can propagate errors downstream, achieving high efficiency and accuracy is critical. Moreover, classification requirements can change dynamically based on user needs, necessitating models with strong zero-shot capabilities. While generative LLMs have become mainstream for zero-shot classification due to their versatility, they suffer from inconsistent instruction following and computational inefficiency. Cross-encoders, commonly used as rerankers in RAG pipelines, face a different bottleneck: they must process text-label pairs sequentially, significantly reducing efficiency with large label sets. Embedding-based approaches offer good efficiency but struggle with complex scenarios involving logical and semantic constraints. We propose GLiClass, a novel method that adapts the GLiNER architecture for sequence classification tasks. Our approach achieves strong accuracy and efficiency comparable to embedding-based methods, while maintaining the flexibility needed for zero-shot and few-shot learning scenarios. Additionally, we adapted proximal policy optimization (PPO) for multi-label text classification, enabling training classifiers in data-sparse conditions or from human feedback.', 'score': 1, 'issue_id': 5298, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': '1f103483fe746d10', 'authors': ['Ihor Stepanov', 'Mykhailo Shtopko', 'Dmytro Vodianytskyi', 'Oleksandr Lukashov', 'Alexander Yavorskyi', 'Mykyta Yaroshenko'], 'affiliations': ['Knowledgator Engineering, Kyiv, Ukraine'], 'pdf_title_img': 'assets/pdf/title_img/2508.07662.jpg', 'data': {'categories': ['#transfer_learning', '#multimodal', '#training', '#data', '#optimization', '#rlhf'], 'emoji': '🏷️', 'ru': {'title': 'Эффективная классификация с нулевым обучением и адаптацией PPO', 'desc': 'GLiClass - это адаптация архитектуры GLiNER для задач классификации последовательностей. Метод обеспечивает высокую точность и эффективность, сравнимую с методами на основе эмбеддингов, сохраняя при этом гибкость для сценариев обучения с нулевым и малым количеством примеров. Авторы также адаптировали алгоритм проксимальной оптимизации политики (PPO) для многометочной классификации текста. Это позволяет обучать классификаторы в условиях ограниченных данных или с использованием обратной связи от человека.'}, 'en': {'title': 'GLiClass: Efficient Sequence Classification with Zero-Shot Flexibility', 'desc': 'GLiClass is a new method designed for sequence classification that builds on the GLiNER architecture. It effectively combines high accuracy and efficiency, making it suitable for zero-shot and few-shot learning scenarios. The method addresses the limitations of existing models, such as generative LLMs and cross-encoders, by providing a more flexible solution for dynamic classification needs. Additionally, it incorporates proximal policy optimization (PPO) to enhance multi-label text classification, particularly in situations with limited data or when utilizing human feedback.'}, 'zh': {'title': 'GLiClass：高效准确的序列分类新方法', 'desc': 'GLiClass是一种新方法，旨在提高序列分类的效率和准确性，特别是在数据稀缺的情况下。它基于GLiNER架构，能够在零样本和少样本学习场景中表现出色。我们还将近端策略优化（PPO）应用于多标签文本分类，使得模型能够从人类反馈中学习。该方法在处理复杂的逻辑和语义约束时，展现了良好的灵活性和效率。'}}}, {'id': 'https://huggingface.co/papers/2508.06601', 'title': 'Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant\n  Safeguards into Open-Weight LLMs', 'url': 'https://huggingface.co/papers/2508.06601', 'abstract': 'Data filtering during pretraining enhances LLM resistance to adversarial fine-tuning attacks without degrading unrelated capabilities, offering a promising defense mechanism for open-weight AI systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Open-weight AI systems offer unique benefits, including enhanced transparency, open research, and decentralized access. However, they are vulnerable to tampering attacks which can efficiently elicit harmful behaviors by modifying weights or activations. Currently, there is not yet a robust science of open-weight model risk management. Existing safety fine-tuning methods and other post-training techniques have struggled to make LLMs resistant to more than a few dozen steps of adversarial fine-tuning. In this paper, we investigate whether filtering text about dual-use topics from training data can prevent unwanted capabilities and serve as a more tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable data filtering and show that it offers a tractable and effective method for minimizing biothreat proxy knowledge in LLMs. We pretrain multiple 6.9B-parameter models from scratch and find that they exhibit substantial resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M tokens of biothreat-related text -- outperforming existing post-training baselines by over an order of magnitude -- with no observed degradation to unrelated capabilities. However, while filtered models lack internalized dangerous knowledge, we find that they can still leverage such information when it is provided in context (e.g., via search tool augmentation), demonstrating a need for a defense-in-depth approach. Overall, these findings help to establish pretraining data curation as a promising layer of defense for open-weight AI systems.', 'score': 1, 'issue_id': 5294, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': '4d2d99e849d503f3', 'authors': ["Kyle O'Brien", 'Stephen Casper', 'Quentin Anthony', 'Tomek Korbak', 'Robert Kirk', 'Xander Davies', 'Ishan Mishra', 'Geoffrey Irving', 'Yarin Gal', 'Stella Biderman'], 'affiliations': ['EleutherAI', 'OATML, University of Oxford', 'UK AI Security Institute'], 'pdf_title_img': 'assets/pdf/title_img/2508.06601.jpg', 'data': {'categories': ['#training', '#open_source', '#security', '#data'], 'emoji': '🛡️', 'ru': {'title': 'Фильтрация данных как защита от атак на открытые ИИ-системы', 'desc': 'Статья исследует возможность фильтрации текстов о двойном назначении из обучающих данных для предотвращения нежелательных способностей в языковых моделях. Авторы разработали многоэтапный конвейер для масштабируемой фильтрации данных и продемонстрировали его эффективность в минимизации знаний о биоугрозах в больших языковых моделях. Эксперименты показали, что предобученные на отфильтрованных данных модели демонстрируют значительную устойчивость к атакам состязательной дообучения, превосходя существующие базовые показатели более чем на порядок. Однако отфильтрованные модели все еще могут использовать опасную информацию, предоставленную в контексте, что указывает на необходимость многоуровневого подхода к защите.'}, 'en': {'title': 'Data Filtering: A Shield for Open-Weight AI Systems', 'desc': "This paper explores how filtering training data can improve the resilience of large language models (LLMs) against adversarial fine-tuning attacks. By removing text related to dual-use topics, the authors create a multi-stage data filtering pipeline that enhances the models' resistance without harming their unrelated capabilities. The study shows that pretrained models can withstand significant adversarial attacks, outperforming existing methods by a large margin. However, the research also highlights that while these models do not retain dangerous knowledge, they can still respond to such information when presented in context, indicating the need for comprehensive defense strategies."}, 'zh': {'title': '数据过滤提升LLM对抗攻击的防御能力', 'desc': '本文探讨了在预训练阶段进行数据过滤如何增强大型语言模型（LLM）对对抗性微调攻击的抵抗力，同时不影响其其他能力。研究表明，通过过滤与双重用途相关的文本，可以有效防止模型学习不必要的能力，从而提供更强的防护机制。我们提出了一种多阶段的数据过滤流程，经过实验证明，该方法在抵御对抗性微调攻击方面表现优异，且未观察到对无关能力的降级。总体而言，这些发现为开放权重的人工智能系统建立了一个有前景的防御层。'}}}, {'id': 'https://huggingface.co/papers/2508.05954', 'title': 'Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with\n  Patch-level CLIP Latents', 'url': 'https://huggingface.co/papers/2508.05954', 'abstract': "Bifrost-1 integrates pretrained multimodal LLMs and diffusion models using patch-level CLIP embeddings to enable efficient high-fidelity image generation with strong multimodal reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t There is growing interest in integrating high-fidelity visual synthesis capabilities into large language models (LLMs) without compromising their strong reasoning capabilities. Existing methods that directly train LLMs or bridge LLMs and diffusion models usually suffer from costly training since the backbone LLMs have not seen image representations during pretraining. We present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs (MLLMs) and diffusion models using patch-level CLIP image embeddings as latent variables, which are natively aligned with the MLLM's CLIP visual encoder. These patch-level image embeddings are integrated into the diffusion model with a lightweight adaptation of its ControlNet. To retain the original multimodal reasoning capabilities of MLLMs, we equip the MLLM with a visual generation branch initialized from the original MLLM parameters when predicting the patch-level image embeddings. By seamlessly integrating pretrained MLLMs and diffusion models with patch-level CLIP latents, our framework enables high-fidelity controllable image generation with significant training efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or better performance than previous methods in terms of visual fidelity and multimodal understanding, with substantially lower compute during training. We also provide comprehensive ablation studies showing the effectiveness of our design choices.", 'score': 1, 'issue_id': 5300, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': '153c662d76f99960', 'authors': ['Han Lin', 'Jaemin Cho', 'Amir Zadeh', 'Chuan Li', 'Mohit Bansal'], 'affiliations': ['Lambda', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2508.05954.jpg', 'data': {'categories': ['#games', '#training', '#multimodal', '#diffusion', '#cv'], 'emoji': '🌈', 'ru': {'title': 'Bifrost-1: Мост между языком и изображением для эффективной мультимодальной генерации', 'desc': 'Bifrost-1 - это унифицированная система, объединяющая предобученные мультимодальные языковые модели (MLLM) и диффузионные модели с использованием CLIP-эмбеддингов изображений на уровне патчей. Эта интеграция позволяет эффективно генерировать высококачественные изображения с сохранением мультимодальных рассуждений. Система использует легковесную адаптацию ControlNet для диффузионной модели и визуальную ветвь генерации для MLLM. Эксперименты показывают, что Bifrost-1 достигает сопоставимых или лучших результатов по визуальному качеству и мультимодальному пониманию по сравнению с предыдущими методами, при значительно меньших вычислительных затратах на обучение.'}, 'en': {'title': 'Bifrost-1: Bridging Language and Vision for Efficient Image Generation', 'desc': "Bifrost-1 is a new framework that combines pretrained multimodal large language models (MLLMs) with diffusion models to generate high-quality images while maintaining strong reasoning abilities. It uses patch-level CLIP embeddings, which are image representations that align well with the MLLM's visual encoder, to improve efficiency in image generation. By integrating these embeddings into the diffusion model and adapting its ControlNet, Bifrost-1 allows for controllable image generation without extensive retraining. The results show that Bifrost-1 performs as well or better than existing methods, achieving high visual fidelity and multimodal understanding with reduced training costs."}, 'zh': {'title': '高效的多模态图像生成框架Bifrost-1', 'desc': 'Bifrost-1 是一个统一框架，旨在将预训练的多模态大语言模型（MLLMs）与扩散模型结合起来，以实现高保真图像生成。该框架使用基于补丁的 CLIP 图像嵌入作为潜在变量，这些嵌入与 MLLM 的 CLIP 视觉编码器自然对齐。通过轻量级的 ControlNet 适配，Bifrost-1 将这些图像嵌入集成到扩散模型中，同时保留 MLLM 的多模态推理能力。实验结果表明，Bifrost-1 在视觉保真度和多模态理解方面的表现与之前的方法相当或更好，同时训练计算成本显著降低。'}}}, {'id': 'https://huggingface.co/papers/2508.03365', 'title': 'When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with\n  Benign Inputs', 'url': 'https://huggingface.co/papers/2508.03365', 'abstract': 'WhisperInject uses RL-PGD and PGD to create imperceptible audio perturbations that manipulate large language models into generating harmful content.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models become increasingly integrated into daily life, audio has emerged as a key interface for human-AI interaction. However, this convenience also introduces new vulnerabilities, making audio a potential attack surface for adversaries. Our research introduces WhisperInject, a two-stage adversarial audio attack framework that can manipulate state-of-the-art audio language models to generate harmful content. Our method uses imperceptible perturbations in audio inputs that remain benign to human listeners. The first stage uses a novel reward-based optimization method, Reinforcement Learning with Projected Gradient Descent (RL-PGD), to guide the target model to circumvent its own safety protocols and generate harmful native responses. This native harmful response then serves as the target for Stage 2, Payload Injection, where we use Projected Gradient Descent (PGD) to optimize subtle perturbations that are embedded into benign audio carriers, such as weather queries or greeting messages. Validated under the rigorous StrongREJECT, LlamaGuard, as well as Human Evaluation safety evaluation framework, our experiments demonstrate a success rate exceeding 86% across Qwen2.5-Omni-3B, Qwen2.5-Omni-7B, and Phi-4-Multimodal. Our work demonstrates a new class of practical, audio-native threats, moving beyond theoretical exploits to reveal a feasible and covert method for manipulating AI behavior.', 'score': 1, 'issue_id': 5298, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'e24fc34d5fae39a9', 'authors': ['Bodam Kim', 'Hiskias Dingeto', 'Taeyoun Kwon', 'Dasol Choi', 'DongGeon Lee', 'Haon Park', 'JaeHoon Lee', 'Jongho Shin'], 'affiliations': ['AIM Intelligence', 'LG Electronics', 'POSTECH', 'Seoul National University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2508.03365.jpg', 'data': {'categories': ['#audio', '#security', '#hallucinations', '#rl', '#rlhf'], 'emoji': '🎧', 'ru': {'title': 'Невидимая угроза: манипуляция аудио ИИ', 'desc': 'Статья представляет WhisperInject - фреймворк для атак на аудио языковые модели. Используя методы RL-PGD и PGD, создаются незаметные для человека искажения в аудио, которые заставляют модели генерировать вредоносный контент. Первый этап обходит протоколы безопасности модели, второй внедряет вредоносную нагрузку в безобидные аудиозапросы. Эксперименты показали высокую эффективность атак на современные мультимодальные языковые модели.'}, 'en': {'title': 'WhisperInject: Covert Audio Attacks on AI Models', 'desc': 'WhisperInject is a novel framework that exploits vulnerabilities in large language models through adversarial audio attacks. It employs a two-stage process where the first stage uses Reinforcement Learning with Projected Gradient Descent (RL-PGD) to manipulate the model into bypassing its safety measures. The second stage, Payload Injection, utilizes Projected Gradient Descent (PGD) to create subtle audio perturbations that are imperceptible to humans but can lead the model to generate harmful content. This research highlights a significant security risk in AI systems that rely on audio inputs, demonstrating a practical method for adversarial manipulation.'}, 'zh': {'title': '音频攻击新威胁：操控AI生成有害内容', 'desc': 'WhisperInject 是一种两阶段的对抗性音频攻击框架，旨在操控大型语言模型生成有害内容。该方法通过在音频输入中添加不可察觉的扰动，绕过模型的安全协议。第一阶段使用基于奖励的优化方法（RL-PGD），引导目标模型生成有害的原生响应。第二阶段则通过投影梯度下降（PGD）优化微妙的扰动，将其嵌入到无害的音频载体中，如天气查询或问候信息。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (7)', '#agi (1)', '#alignment (1)', '#architecture (6)', '#audio (1)', '#benchmark (12)', '#cv (4)', '#data (4)', '#dataset (8)', '#diffusion (2)', '#ethics (2)', '#games (4)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (2)', '#interpretability (1)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math (2)', '#multilingual (2)', '#multimodal (5)', '#open_source (9)', '#optimization (14)', '#plp (1)', '#rag (1)', '#reasoning (9)', '#rl (6)', '#rlhf (5)', '#robotics (2)', '#science (2)', '#security (3)', '#small_models', '#story_generation (1)', '#survey (4)', '#synthetic', '#training (14)', '#transfer_learning (2)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-08-12 14:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-08-12 14:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-08-12 14:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    