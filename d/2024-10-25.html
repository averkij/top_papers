
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF (29 статей)</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
                display: block;
                margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: inline-block;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        body.light-theme>div>main>article.x29bd02d560e09f69 { background: url("https://hfday.ru/img/20241022/29bd02d560e09f69.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x29bd02d560e09f69:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x29bd02d560e09f69 { background: url("https://hfday.ru/img/20241022/29bd02d560e09f69.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x29bd02d560e09f69:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xfa9696ca5a11c431 { background: url("https://hfday.ru/img/20241021/fa9696ca5a11c431.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xfa9696ca5a11c431:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xfa9696ca5a11c431 { background: url("https://hfday.ru/img/20241021/fa9696ca5a11c431.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xfa9696ca5a11c431:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x3094f5b6fc1c0168 { background: url("https://hfday.ru/img/20241024/3094f5b6fc1c0168.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x3094f5b6fc1c0168:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x3094f5b6fc1c0168 { background: url("https://hfday.ru/img/20241024/3094f5b6fc1c0168.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x3094f5b6fc1c0168:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xd898ba7b025b60e6 { background: url("https://hfday.ru/img/20241024/d898ba7b025b60e6.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xd898ba7b025b60e6:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xd898ba7b025b60e6 { background: url("https://hfday.ru/img/20241024/d898ba7b025b60e6.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xd898ba7b025b60e6:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.x2d7a29df8d1696c4 { background: url("https://hfday.ru/img/20241024/2d7a29df8d1696c4.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.x2d7a29df8d1696c4:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.x2d7a29df8d1696c4 { background: url("https://hfday.ru/img/20241024/2d7a29df8d1696c4.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.x2d7a29df8d1696c4:hover { background-color: rgba(60,60,60,0.92) !important;}
body.light-theme>div>main>article.xccd15b8785ec7920 { background: url("https://hfday.ru/img/20241024/ccd15b8785ec7920.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: lighten !important; background-color: rgba(255,255,255,0.91) !important;}
body.light-theme>div>main>article.xccd15b8785ec7920:hover { background-color: rgba(255,255,255,0.95) !important;}
body.dark-theme>div>main>article.xccd15b8785ec7920 { background: url("https://hfday.ru/img/20241024/ccd15b8785ec7920.jpg") !important; background-size: cover !important; background-position: center !important; background-blend-mode: hue !important; background-color: rgba(60,60,60,0.9) !important; }
body.dark-theme>div>main>article.xccd15b8785ec7920:hover { background-color: rgba(60,60,60,0.92) !important;}

        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">25 октября</span> | <span id="title-articles-count">29 статей</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-24.html">⬅️ <span id="prev-date">24.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-28.html">➡️ <span id="next-date">28.10</span></a></span>
            <!--<span class="nav-item" id="nav-weekly">Топ за неделю</span>
            <span class="nav-item" id="nav-weekly">Топ за месяц</span>-->
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">🏷️ Фильтр</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">градиент обреченный</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'ru';
        let feedDate = {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'};
        let feedDateNext = {'ru': '28.10', 'en': '10/28', 'zh': '10月28日'};
        let feedDatePrev = {'ru': '24.10', 'en': '10/24', 'zh': '10月24日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.17243', 'title': 'Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss', 'url': 'https://huggingface.co/papers/2410.17243', 'abstract': 'Contrastive loss is a powerful approach for representation learning, where larger batch sizes enhance performance by providing more negative samples to better distinguish between similar and dissimilar data. However, scaling batch sizes is constrained by the quadratic growth in GPU memory consumption, primarily due to the full instantiation of the similarity matrix. To address this, we propose a tile-based computation strategy that partitions the contrastive loss calculation into arbitrary small blocks, avoiding full materialization of the similarity matrix. Furthermore, we introduce a multi-level tiling strategy to leverage the hierarchical structure of distributed systems, employing ring-based communication at the GPU level to optimize synchronization and fused kernels at the CUDA core level to reduce I/O overhead. Experimental results show that the proposed method scales batch sizes to unprecedented levels. For instance, it enables contrastive training of a CLIP-ViT-L/14 model with a batch size of 4M or 12M using 8 or 32 A800 80GB without sacrificing any accuracy. Compared to SOTA memory-efficient solutions, it achieves a two-order-of-magnitude reduction in memory while maintaining comparable speed. The code will be made publicly available.', 'score': 66, 'issue_id': 257, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '29bd02d560e09f69', 'data': {'categories': ['#optimization', '#architecture'], 'emoji': '🧩', 'ru': {'title': 'Эффективное масштабирование контрастивного обучения с помощью тайлинга', 'desc': 'В статье представлен новый подход к вычислению контрастивной функции потерь, позволяющий значительно увеличить размер батча без роста потребления памяти GPU. Авторы предлагают стратегию вычислений на основе тайлинга, которая разбивает расчет контрастивной функции потерь на небольшие блоки. Также вводится многоуровневая стратегия тайлинга для оптимизации распределенных вычислений. Экспериментальные результаты показывают, что предложенный метод позволяет достичь беспрецедентных размеров батча при обучении моделей вроде CLIP-ViT-L/14.'}, 'en': {'title': 'Scaling Contrastive Learning: Breaking Memory Barriers with Tile-Based Strategies', 'desc': 'The paper introduces a novel tile-based computation strategy to efficiently calculate contrastive loss, which is crucial for representation learning. This method addresses the challenge of increased GPU memory consumption by partitioning the similarity matrix into smaller blocks, thus avoiding full instantiation. Additionally, a multi-level tiling strategy is employed to optimize communication and reduce I/O overhead in distributed systems. The approach allows for significantly larger batch sizes, achieving a two-order-of-magnitude reduction in memory usage without compromising speed or accuracy.'}, 'zh': {'title': '突破内存限制，实现超大批量对比学习', 'desc': '对比损失是一种强大的表示学习方法，通过提供更多的负样本来更好地区分相似和不相似的数据，从而提高性能。然而，批量大小的扩展受到GPU内存消耗的限制，因为相似性矩阵的完全实例化会导致内存消耗呈二次增长。为了解决这个问题，我们提出了一种基于块的计算策略，将对比损失的计算划分为任意小的块，避免了相似性矩阵的完全实例化。此外，我们引入了一种多级块策略，利用分布式系统的层次结构，在GPU级别采用环形通信优化同步，并在CUDA核心级别使用融合内核减少I/O开销。'}}}, {'id': 'https://huggingface.co/papers/2410.16251', 'title': 'Can Knowledge Editing Really Correct Hallucinations?', 'url': 'https://huggingface.co/papers/2410.16251', 'abstract': 'Large Language Models (LLMs) suffer from hallucinations, referring to the non-factual information in generated content, despite their superior capacities across tasks. Meanwhile, knowledge editing has been developed as a new popular paradigm to correct the erroneous factual knowledge encoded in LLMs with the advantage of avoiding retraining from scratch. However, one common issue of existing evaluation datasets for knowledge editing is that they do not ensure LLMs actually generate hallucinated answers to the evaluation questions before editing. When LLMs are evaluated on such datasets after being edited by different techniques, it is hard to directly adopt the performance to assess the effectiveness of different knowledge editing methods in correcting hallucinations. Thus, the fundamental question remains insufficiently validated: Can knowledge editing really correct hallucinations in LLMs? We proposed HalluEditBench to holistically benchmark knowledge editing methods in correcting real-world hallucinations. First, we rigorously construct a massive hallucination dataset with 9 domains, 26 topics and more than 6,000 hallucinations. Then, we assess the performance of knowledge editing methods in a holistic way on five dimensions including Efficacy, Generalization, Portability, Locality, and Robustness. Through HalluEditBench, we have provided new insights into the potentials and limitations of different knowledge editing methods in correcting hallucinations, which could inspire future improvements and facilitate the progress in the field of knowledge editing.', 'score': 42, 'issue_id': 259, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': 'fa9696ca5a11c431', 'data': {'categories': ['#hallucinations', '#benchmark', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Исправление галлюцинаций в LLM: новый бенчмарк для оценки методов редактирования знаний', 'desc': 'Данная статья посвящена проблеме галлюцинаций в больших языковых моделях (LLM) и методам редактирования знаний для их исправления. Авторы предлагают новый бенчмарк HalluEditBench для оценки эффективности методов редактирования знаний в исправлении реальных галлюцинаций. Бенчмарк включает массивный набор данных с более чем 6000 галлюцинаций из 9 доменов и 26 тем. HalluEditBench оценивает методы редактирования знаний по пяти измерениям: эффективность, обобщаемость, переносимость, локальность и устойчивость.'}, 'en': {'title': 'Benchmarking the Battle Against Hallucinations in LLMs', 'desc': 'The paper addresses the issue of hallucinations in Large Language Models (LLMs), where these models generate non-factual information. It introduces HalluEditBench, a comprehensive benchmark designed to evaluate the effectiveness of knowledge editing methods in correcting these hallucinations. The benchmark includes a large dataset with over 6,000 hallucinations across various domains and assesses methods on five key dimensions. This work provides new insights into the strengths and weaknesses of current knowledge editing techniques, aiming to inspire future advancements in the field.'}, 'zh': {'title': 'HalluEditBench：纠正幻觉的新希望', 'desc': '大型语言模型（LLMs）在生成内容时常常出现幻觉，即生成不真实的信息。知识编辑是一种新兴的方法，可以在不重新训练模型的情况下纠正这些错误信息。现有的评估数据集无法确保LLMs在编辑前生成幻觉答案，因此难以评估知识编辑方法的有效性。我们提出了HalluEditBench，通过构建大规模幻觉数据集，全面评估知识编辑方法在纠正幻觉方面的表现。'}}}, {'id': 'https://huggingface.co/papers/2410.18533', 'title': 'LOGO -- Long cOntext aliGnment via efficient preference Optimization', 'url': 'https://huggingface.co/papers/2410.18533', 'abstract': "Long-context models(LCMs) have shown great potential in processing long input sequences(even more than 100M tokens) conveniently and effectively. With significant progress, recent research has pointed out that LCMs can accurately locate token-level salient information within the context. Yet, the generation performance of these LCMs is far from satisfactory and might result in misaligned responses, such as hallucinations. To enhance the generation capability of LCMs, existing works have investigated the effects of data size and quality for both pre-training and instruction tuning. Though achieving meaningful improvement, previous methods fall short in either effectiveness or efficiency. In this paper, we introduce LOGO(Long cOntext aliGnment via efficient preference Optimization), a training strategy that first introduces preference optimization for long-context alignment. To overcome the GPU memory-bound issue caused by the long sequence, LOGO employs a reference-free preference optimization strategy and adopts a position synthesis method to construct the training data. By training with only 0.3B data on a single 8timesA800 GPU machine for 16 hours, LOGO allows the Llama-3-8B-Instruct-80K model to achieve comparable performance with GPT-4 in real-world long-context tasks while preserving the model's original capabilities on other tasks, e.g., language modeling and MMLU. Moreover, LOGO can extend the model's context window size while enhancing its generation performance.", 'score': 38, 'issue_id': 258, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '3094f5b6fc1c0168', 'data': {'categories': ['#alignment', '#hallucinations'], 'emoji': '🚀', 'ru': {'title': 'LOGO: Эффективное улучшение генерации для моделей с длинным контекстом', 'desc': 'Эта статья представляет новый метод обучения под названием LOGO для улучшения генеративных способностей моделей с длинным контекстом (LCM). LOGO использует оптимизацию предпочтений без эталона и метод синтеза позиций для создания обучающих данных. Авторы обучили модель Llama-3-8B-Instruct-80K всего на 0.3B данных за 16 часов, достигнув производительности, сравнимой с GPT-4 на задачах с длинным контекстом. LOGO также позволяет расширить размер контекстного окна модели, сохраняя при этом ее исходные возможности в других задачах.'}, 'en': {'title': 'LOGO: Aligning Long-Context Models for Better Generation', 'desc': "This paper introduces a new training strategy called LOGO, which improves the generation performance of long-context models (LCMs) by using preference optimization for better alignment. LOGO addresses the challenge of GPU memory limitations by employing a reference-free preference optimization strategy and a position synthesis method for training data construction. The approach allows significant performance improvements in long-context tasks, achieving results comparable to GPT-4, while maintaining the model's original capabilities. LOGO also extends the model's context window size, enhancing its ability to handle long sequences effectively."}, 'zh': {'title': 'LOGO：优化长上下文生成性能的新策略', 'desc': '这篇论文介绍了一种新的训练策略LOGO，用于提高长上下文模型的生成能力。LOGO通过引入偏好优化来实现长上下文对齐，并采用无参考的偏好优化策略来解决GPU内存限制问题。通过在单台8倍A800 GPU机器上训练16小时，LOGO使得Llama-3-8B-Instruct-80K模型在长上下文任务中表现与GPT-4相当。LOGO不仅提升了生成性能，还扩展了模型的上下文窗口大小，同时保留了模型在其他任务上的原有能力。'}}}, {'id': 'https://huggingface.co/papers/2410.18693', 'title': 'Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch', 'url': 'https://huggingface.co/papers/2410.18693', 'abstract': 'The availability of high-quality data is one of the most important factors in improving the reasoning capability of LLMs. Existing works have demonstrated the effectiveness of creating more instruction data from seed questions or knowledge bases. Recent research indicates that continually scaling up data synthesis from strong models (e.g., GPT-4) can further elicit reasoning performance. Though promising, the open-sourced community still lacks high-quality data at scale and scalable data synthesis methods with affordable costs. To address this, we introduce ScaleQuest, a scalable and novel data synthesis method that utilizes "small-size" (e.g., 7B) open-source models to generate questions from scratch without the need for seed data with complex augmentation constraints. With the efficient ScaleQuest, we automatically constructed a mathematical reasoning dataset consisting of 1 million problem-solution pairs, which are more effective than existing open-sourced datasets. It can universally increase the performance of mainstream open-source models (i.e., Mistral, Llama3, DeepSeekMath, and Qwen2-Math) by achieving 29.2% to 46.4% gains on MATH. Notably, simply fine-tuning the Qwen2-Math-7B-Base model with our dataset can even surpass Qwen2-Math-7B-Instruct, a strong and well-aligned model on closed-source data, and proprietary models such as GPT-4-Turbo and Claude-3.5 Sonnet.', 'score': 35, 'issue_id': 258, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'd898ba7b025b60e6', 'data': {'categories': ['#dataset', '#data', '#math'], 'emoji': '🧮', 'ru': {'title': 'ScaleQuest: Масштабируемый синтез данных для повышения математических способностей LLM', 'desc': 'Статья представляет ScaleQuest - новый метод синтеза данных для улучшения способностей LLM к рассуждениям. Используя небольшие модели с открытым исходным кодом, ScaleQuest генерирует вопросы с нуля без необходимости в исходных данных. С помощью этого метода был создан набор данных из 1 миллиона пар задача-решение для математических рассуждений. Применение этого набора данных значительно повысило производительность открытых моделей на тесте MATH, позволив некоторым из них превзойти даже проприетарные модели.'}, 'en': {'title': 'ScaleQuest: Elevating AI Reasoning with Scalable Data Synthesis', 'desc': 'The paper introduces ScaleQuest, a method for generating high-quality data using smaller open-source models without relying on seed data. This approach allows for the creation of a large mathematical reasoning dataset, significantly improving the performance of various open-source models. The dataset enhances model capabilities by up to 46.4% on mathematical tasks, outperforming some proprietary models. ScaleQuest offers a cost-effective solution for scalable data synthesis, addressing the need for high-quality data in the open-source community.'}, 'zh': {'title': 'ScaleQuest：小模型，大数据，强推理', 'desc': '高质量的数据是提升大型语言模型推理能力的关键因素之一。现有研究表明，从种子问题或知识库中生成更多指令数据是有效的。最近的研究显示，持续扩大数据合成规模可以进一步提高推理性能。为此，我们引入了ScaleQuest，一种利用小型开源模型生成问题的新方法，无需复杂的种子数据增强约束。'}}}, {'id': 'https://huggingface.co/papers/2410.18975', 'title': 'Unbounded: A Generative Infinite Game of Character Life Simulation', 'url': 'https://huggingface.co/papers/2410.18975', 'abstract': "We introduce the concept of a generative infinite game, a video game that transcends the traditional boundaries of finite, hard-coded systems by using generative models. Inspired by James P. Carse's distinction between finite and infinite games, we leverage recent advances in generative AI to create Unbounded: a game of character life simulation that is fully encapsulated in generative models. Specifically, Unbounded draws inspiration from sandbox life simulations and allows you to interact with your autonomous virtual character in a virtual world by feeding, playing with and guiding it - with open-ended mechanics generated by an LLM, some of which can be emergent. In order to develop Unbounded, we propose technical innovations in both the LLM and visual generation domains. Specifically, we present: (1) a specialized, distilled large language model (LLM) that dynamically generates game mechanics, narratives, and character interactions in real-time, and (2) a new dynamic regional image prompt Adapter (IP-Adapter) for vision models that ensures consistent yet flexible visual generation of a character across multiple environments. We evaluate our system through both qualitative and quantitative analysis, showing significant improvements in character life simulation, user instruction following, narrative coherence, and visual consistency for both characters and the environments compared to traditional related approaches.", 'score': 29, 'issue_id': 257, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '2d7a29df8d1696c4', 'data': {'categories': ['#games', '#multimodal', '#cv', '#rl'], 'emoji': '🎮', 'ru': {'title': 'Бесконечная игра: новый уровень генеративных видеоигр', 'desc': 'Статья представляет концепцию генеративной бесконечной игры, которая использует генеративные модели для создания видеоигры, выходящей за рамки традиционных конечных систем. Авторы разработали игру Unbounded - симулятор жизни персонажа, полностью основанный на генеративных моделях. В игре используется специализированная языковая модель для генерации игровых механик и нарратива, а также новый адаптер для визуальных моделей, обеспечивающий согласованную генерацию изображений персонажа. Результаты показывают значительные улучшения в симуляции жизни персонажа, следовании инструкциям пользователя и согласованности повествования по сравнению с традиционными подходами.'}, 'en': {'title': 'Unbounded: Redefining Gaming with Generative AI', 'desc': 'The paper introduces a new type of video game called a generative infinite game, which uses advanced generative models to create a limitless gaming experience. The game, named Unbounded, allows players to interact with virtual characters in a dynamic world, with game mechanics and narratives generated in real-time by a specialized large language model (LLM). Additionally, the game employs a novel image prompt Adapter (IP-Adapter) to maintain visual consistency across different environments. The authors demonstrate that their approach significantly enhances character simulation, narrative coherence, and visual consistency compared to traditional methods.'}, 'zh': {'title': '生成无限游戏：打破传统游戏界限的创新', 'desc': '这篇论文介绍了一种名为“生成无限游戏”的新概念，通过生成模型打破传统有限、硬编码系统的界限。受詹姆斯·P·卡尔斯关于有限游戏和无限游戏的区分启发，作者利用生成式人工智能的最新进展，创造了一个名为“Unbounded”的角色生活模拟游戏。该游戏允许玩家在虚拟世界中与自主虚拟角色互动，使用大型语言模型生成开放式的游戏机制。为了实现这一目标，作者在大型语言模型和视觉生成领域提出了技术创新，并通过定性和定量分析证明了其在角色生活模拟、用户指令遵循、叙事连贯性和视觉一致性方面的显著改进。'}}}, {'id': 'https://huggingface.co/papers/2410.18978', 'title': 'Framer: Interactive Frame Interpolation', 'url': 'https://huggingface.co/papers/2410.18978', 'abstract': 'We propose Framer for interactive frame interpolation, which targets producing smoothly transitioning frames between two images as per user creativity. Concretely, besides taking the start and end frames as inputs, our approach supports customizing the transition process by tailoring the trajectory of some selected keypoints. Such a design enjoys two clear benefits. First, incorporating human interaction mitigates the issue arising from numerous possibilities of transforming one image to another, and in turn enables finer control of local motions. Second, as the most basic form of interaction, keypoints help establish the correspondence across frames, enhancing the model to handle challenging cases (e.g., objects on the start and end frames are of different shapes and styles). It is noteworthy that our system also offers an "autopilot" mode, where we introduce a module to estimate the keypoints and refine the trajectory automatically, to simplify the usage in practice. Extensive experimental results demonstrate the appealing performance of Framer on various applications, such as image morphing, time-lapse video generation, cartoon interpolation, etc. The code, the model, and the interface will be released to facilitate further research.', 'score': 28, 'issue_id': 257, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'ccd15b8785ec7920', 'data': {'categories': ['#video', '#multimodal'], 'emoji': '🎞️', 'ru': {'title': 'Интерактивная интерполяция кадров с пользовательским контролем', 'desc': "Предложен метод Framer для интерактивной интерполяции кадров, позволяющий создавать плавные переходы между двумя изображениями с учетом пользовательских предпочтений. Система поддерживает настройку процесса перехода путем задания траектории выбранных ключевых точек, что обеспечивает более точный контроль локальных движений. Framer также включает 'автопилотный' режим с автоматическим определением ключевых точек и траекторий. Метод показал высокую эффективность в различных приложениях, таких как морфинг изображений, создание таймлапс-видео и интерполяция мультипликации."}, 'en': {'title': 'Framer: Crafting Seamless Image Transitions with User Control', 'desc': 'Framer is a tool for creating smooth transitions between two images by allowing users to customize the movement of keypoints, which are specific points on the images. This interactive approach helps users control how different parts of the images move, making it easier to handle complex transformations. The system also includes an automatic mode that estimates keypoints and their paths, simplifying the process for users. Framer has shown impressive results in applications like image morphing and cartoon interpolation, and its resources will be made available for further research.'}, 'zh': {'title': 'Framer：让图像过渡更顺滑的交互式帧插值', 'desc': '这篇论文介绍了一种名为Framer的交互式帧插值方法，旨在根据用户的创意在两张图像之间生成平滑过渡的帧。该方法允许用户通过调整关键点的轨迹来定制过渡过程，从而实现对局部运动的精细控制。通过这种人机交互，Framer能够更好地处理起始和结束帧中形状和风格不同的物体。此外，Framer还提供了自动模式，可以自动估计关键点并优化轨迹，简化实际使用。'}}}, {'id': 'https://huggingface.co/papers/2410.18798', 'title': 'Distill Visual Chart Reasoning Ability from LLMs to MLLMs', 'url': 'https://huggingface.co/papers/2410.18798', 'abstract': 'Solving complex chart Q&A tasks requires advanced visual reasoning abilities in multimodal large language models (MLLMs). Recent studies highlight that these abilities consist of two main parts: recognizing key information from visual inputs and conducting reasoning over it. Thus, a promising approach to enhance MLLMs is to construct relevant training data focusing on the two aspects. However, collecting and annotating complex charts and questions is costly and time-consuming, and ensuring the quality of annotated answers remains a challenge. In this paper, we propose Code-as-Intermediary Translation (CIT), a cost-effective, efficient and easily scalable data synthesis method for distilling visual reasoning abilities from LLMs to MLLMs. The code serves as an intermediary that translates visual chart representations into textual representations, enabling LLMs to understand cross-modal information. Specifically, we employ text-based synthesizing techniques to construct chart-plotting code and produce ReachQA, a dataset containing 3k reasoning-intensive charts and 20k Q&A pairs to enhance both recognition and reasoning abilities. Experiments show that when fine-tuned with our data, models not only perform well on chart-related benchmarks, but also demonstrate improved multimodal reasoning abilities on general mathematical benchmarks like MathVista. The code and dataset are publicly available at https://github.com/hewei2001/ReachQA.', 'score': 17, 'issue_id': 259, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '7d35c4638b6931ba', 'data': {'categories': ['#dataset', '#data', '#multimodal', '#cv'], 'emoji': '📊', 'ru': {'title': 'Код как переводчик: новый подход к визуальным рассуждениям в ИИ', 'desc': 'Статья представляет новый метод Code-as-Intermediary Translation (CIT) для улучшения визуальных рассуждений в мультимодальных больших языковых моделях (MLLM). Метод использует код как посредник для перевода визуальных представлений графиков в текстовые, что позволяет LLM понимать межмодальную информацию. Авторы создали датасет ReachQA с 3000 графиков и 20000 пар вопрос-ответ для улучшения способностей распознавания и рассуждения. Эксперименты показали, что модели, обученные на этих данных, улучшают свои способности мультимодальных рассуждений не только на задачах с графиками, но и на общих математических тестах.'}, 'en': {'title': 'Bridging Visual and Textual Worlds with Code', 'desc': 'The paper introduces a method called Code-as-Intermediary Translation (CIT) to improve visual reasoning in multimodal large language models (MLLMs). CIT uses code to convert visual chart data into text, making it easier for language models to process and understand. This approach helps create a dataset called ReachQA, which includes 3,000 charts and 20,000 Q&A pairs, enhancing both recognition and reasoning skills in MLLMs. Experiments show that models trained with this data perform better on both chart-related and general mathematical reasoning tasks.'}, 'zh': {'title': '代码中介翻译：提升多模态推理能力的新方法', 'desc': '这篇论文探讨了如何提升多模态大语言模型在复杂图表问答任务中的视觉推理能力。研究表明，这种能力主要包括从视觉输入中识别关键信息和进行推理。为此，作者提出了一种名为“代码作为中介翻译”的方法，通过生成图表绘制代码，将视觉信息转化为文本信息，从而提高模型的跨模态理解能力。实验结果显示，使用该方法训练的数据集可以显著提升模型在图表相关基准测试和一般数学推理任务中的表现。'}}}, {'id': 'https://huggingface.co/papers/2410.18745', 'title': 'Why Does the Effective Context Length of LLMs Fall Short?', 'url': 'https://huggingface.co/papers/2410.18745', 'abstract': 'Advancements in distributed training and efficient attention mechanisms have significantly expanded the context window sizes of large language models (LLMs). However, recent work reveals that the effective context lengths of open-source LLMs often fall short, typically not exceeding half of their training lengths. In this work, we attribute this limitation to the left-skewed frequency distribution of relative positions formed in LLMs pretraining and post-training stages, which impedes their ability to effectively gather distant information. To address this challenge, we introduce ShifTed Rotray position embeddING (STRING). STRING shifts well-trained positions to overwrite the original ineffective positions during inference, enhancing performance within their existing training lengths. Experimental results show that without additional training, STRING dramatically improves the performance of the latest large-scale models, such as Llama3.1 70B and Qwen2 72B, by over 10 points on popular long-context benchmarks RULER and InfiniteBench, establishing new state-of-the-art results for open-source LLMs. Compared to commercial models, Llama 3.1 70B with \\method even achieves better performance than GPT-4-128K and clearly surpasses Claude 2 and Kimi-chat.', 'score': 14, 'issue_id': 257, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '8694d1d100403e2f', 'data': {'categories': ['#architecture', '#benchmark'], 'emoji': '🧵', 'ru': {'title': 'STRING: Расширение горизонтов контекста для языковых моделей', 'desc': 'Статья представляет новый метод под названием STRING (ShifTed Rotray position embeddING) для улучшения эффективной длины контекста больших языковых моделей (LLM). Авторы обнаружили, что существующие LLM часто не могут эффективно использовать весь заявленный контекст из-за особенностей распределения относительных позиций при обучении. STRING решает эту проблему, смещая хорошо обученные позиции для перезаписи неэффективных позиций во время вывода. Эксперименты показывают значительное улучшение производительности современных моделей, таких как Llama 3.1 70B и Qwen2 72B, на бенчмарках длинного контекста без дополнительного обучения.'}, 'en': {'title': 'STRING: Unlocking Full Context Potential in LLMs', 'desc': 'The paper discusses how large language models (LLMs) often struggle to use their full context window effectively due to a skewed frequency distribution of relative positions during training. To solve this, the authors introduce a method called ShifTed Rotray position embeddING (STRING), which adjusts these positions to improve information gathering without additional training. STRING significantly boosts the performance of models like Llama3.1 70B and Qwen2 72B on long-context benchmarks, setting new records for open-source LLMs. This method even allows these models to outperform some commercial models, such as GPT-4-128K and Claude 2.'}, 'zh': {'title': 'STRING：提升大语言模型上下文理解的新方法', 'desc': '这篇论文讨论了大语言模型在分布式训练和高效注意力机制方面的进展，但指出开源模型的有效上下文长度往往不如预期。作者发现这种限制是由于模型训练过程中相对位置的频率分布不均衡，影响了模型获取远距离信息的能力。为了解决这个问题，作者提出了一种名为STRING的新方法，通过在推理时调整位置嵌入来提高模型性能。实验结果表明，STRING在不需要额外训练的情况下显著提升了最新大规模模型的表现，甚至超过了一些商业模型。'}}}, {'id': 'https://huggingface.co/papers/2410.18572', 'title': 'Taipan: Efficient and Expressive State Space Language Models with Selective Attention', 'url': 'https://huggingface.co/papers/2410.18572', 'abstract': "Efficient long-context language modeling remains a significant challenge in Natural Language Processing (NLP). While Transformers dominate language tasks, they struggle with long sequences due to quadratic computational complexity in training and linearly scaling memory costs during inference. Recent State Space Models (SSMs) such as Mamba offer alternatives with constant memory usage, but they underperform in tasks requiring extensive in-context retrieval. We introduce Taipan, a novel hybrid architecture that combines Mamba-2 with Selective Attention Layers (SALs). These SALs identify tokens requiring long-range interactions, remove less important features, and then augment their representations using the attention module. This approach balances Mamba's efficiency with Transformer-like performance in memory-intensive tasks. By constraining the attention budget, Taipan extends accurate predictions to context lengths of up to 1 million tokens while preserving computational efficiency. Our experiments demonstrate Taipan's superior performance across various scales and tasks, offering a promising solution for efficient long-context language modeling.", 'score': 12, 'issue_id': 262, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '406070252c5b43ea', 'data': {'categories': ['#architecture'], 'emoji': '🐍', 'ru': {'title': 'Taipan: Эффективное языковое моделирование с контекстом до миллиона токенов', 'desc': 'Исследователи представили Taipan - гибридную архитектуру, сочетающую Mamba-2 с селективными слоями внимания для эффективной обработки длинных последовательностей в языковом моделировании. Taipan идентифицирует важные токены, требующие взаимодействия на большом расстоянии, и усиливает их представления с помощью модуля внимания. Этот подход сочетает эффективность Mamba с производительностью трансформеров в задачах, требующих обширного извлечения информации из контекста. Эксперименты показывают превосходную производительность Taipan на различных масштабах и задачах, предлагая перспективное решение для эффективного языкового моделирования с длинным контекстом.'}, 'en': {'title': 'Taipan: Bridging Efficiency and Performance in Long-Context Language Modeling', 'desc': 'The paper introduces Taipan, a new model for handling long-context language tasks in NLP. Taipan combines the efficient memory usage of State Space Models like Mamba-2 with the performance benefits of Selective Attention Layers. These layers help the model focus on important tokens, allowing it to manage long sequences effectively. This hybrid approach enables Taipan to make accurate predictions with up to 1 million tokens while maintaining computational efficiency.'}, 'zh': {'title': 'Taipan：高效处理长文本的混合架构', 'desc': '在自然语言处理领域，处理长文本的语言建模一直是个难题。虽然Transformer在很多任务中表现出色，但在处理长序列时，它的计算复杂度和内存需求都很高。Taipan是一种新型混合架构，结合了Mamba-2和选择性注意层，能够在保持计算效率的同时，处理长达100万标记的上下文。实验表明，Taipan在多种任务中表现优异，是长文本语言建模的有效解决方案。'}}}, {'id': 'https://huggingface.co/papers/2410.18977', 'title': 'MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms', 'url': 'https://huggingface.co/papers/2410.18977', 'abstract': 'This research delves into the problem of interactive editing of human motion generation. Previous motion diffusion models lack explicit modeling of the word-level text-motion correspondence and good explainability, hence restricting their fine-grained editing ability. To address this issue, we propose an attention-based motion diffusion model, namely MotionCLR, with CLeaR modeling of attention mechanisms. Technically, MotionCLR models the in-modality and cross-modality interactions with self-attention and cross-attention, respectively. More specifically, the self-attention mechanism aims to measure the sequential similarity between frames and impacts the order of motion features. By contrast, the cross-attention mechanism works to find the fine-grained word-sequence correspondence and activate the corresponding timesteps in the motion sequence. Based on these key properties, we develop a versatile set of simple yet effective motion editing methods via manipulating attention maps, such as motion (de-)emphasizing, in-place motion replacement, and example-based motion generation, etc. For further verification of the explainability of the attention mechanism, we additionally explore the potential of action-counting and grounded motion generation ability via attention maps. Our experimental results show that our method enjoys good generation and editing ability with good explainability.', 'score': 12, 'issue_id': 259, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '094d940b9a0653c0', 'data': {'categories': ['#cv', '#multimodal', '#interpretability'], 'emoji': '🕺', 'ru': {'title': 'Точное редактирование движений с помощью внимания', 'desc': 'Статья представляет MotionCLR - модель диффузии движения на основе механизмов внимания для интерактивного редактирования генерации движений человека. Модель использует самовнимание для измерения последовательного сходства между кадрами и кросс-внимание для нахождения детальных соответствий между словами и последовательностью движений. На основе этих механизмов разработаны разнообразные методы редактирования движений путем манипуляции картами внимания. Эксперименты показывают, что метод обладает хорошими возможностями генерации и редактирования с высокой объяснимостью.'}, 'en': {'title': 'Unlocking Fine-Grained Motion Editing with Attention', 'desc': 'This paper introduces MotionCLR, an attention-based motion diffusion model designed to improve the fine-grained editing of human motion generation. By utilizing self-attention and cross-attention mechanisms, the model effectively captures both the sequential similarity between motion frames and the correspondence between text and motion sequences. This approach allows for versatile motion editing techniques, such as motion emphasizing and example-based generation, by manipulating attention maps. The experimental results demonstrate that MotionCLR provides enhanced generation and editing capabilities with improved explainability.'}, 'zh': {'title': 'MotionCLR：通过注意力机制实现动作生成的精细编辑', 'desc': '这项研究探讨了人类动作生成的交互式编辑问题。以往的动作扩散模型缺乏对文字与动作对应关系的明确建模，限制了其细粒度编辑能力。为解决此问题，我们提出了一种基于注意力机制的动作扩散模型MotionCLR，通过自注意力和交叉注意力分别建模模态内和跨模态的交互。实验结果表明，我们的方法在生成和编辑能力上表现出色，并具有良好的可解释性。'}}}, {'id': 'https://huggingface.co/papers/2410.18538', 'title': 'SMITE: Segment Me In TimE', 'url': 'https://huggingface.co/papers/2410.18538', 'abstract': 'Segmenting an object in a video presents significant challenges. Each pixel must be accurately labelled, and these labels must remain consistent across frames. The difficulty increases when the segmentation is with arbitrary granularity, meaning the number of segments can vary arbitrarily, and masks are defined based on only one or a few sample images. In this paper, we address this issue by employing a pre-trained text to image diffusion model supplemented with an additional tracking mechanism. We demonstrate that our approach can effectively manage various segmentation scenarios and outperforms state-of-the-art alternatives.', 'score': 11, 'issue_id': 265, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '39917935bf1d9f30', 'data': {'categories': ['#video', '#cv', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Гибкая сегментация видео с помощью диффузионных моделей', 'desc': 'Статья посвящена проблеме сегментации объектов в видео с произвольной гранулярностью. Авторы предлагают использовать предобученную диффузионную модель для перехода от текста к изображению, дополненную механизмом отслеживания. Этот подход позволяет эффективно справляться с различными сценариями сегментации. Результаты показывают, что предложенный метод превосходит современные альтернативы.'}, 'en': {'title': 'Mastering Video Segmentation with Diffusion and Tracking', 'desc': 'This paper tackles the complex task of segmenting objects in videos, where each pixel needs consistent labeling across frames. The challenge is heightened by arbitrary granularity, where the number of segments can vary, and segmentation masks are based on limited sample images. The authors propose a solution using a pre-trained text-to-image diffusion model enhanced with a tracking mechanism. Their approach shows superior performance in handling diverse segmentation scenarios compared to existing methods.'}, 'zh': {'title': '创新视频分割：文本到图像扩散模型的应用', 'desc': '这篇论文讨论了视频中物体分割的挑战，尤其是每个像素的准确标记和跨帧的一致性。作者提出了一种方法，利用预训练的文本到图像扩散模型，并结合额外的跟踪机制来解决这些问题。该方法能够处理不同的分割场景，并且在性能上优于现有的最先进方法。通过这种创新的方式，视频分割的精度和灵活性得到了显著提升。'}}}, {'id': 'https://huggingface.co/papers/2410.18451', 'title': 'Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs', 'url': 'https://huggingface.co/papers/2410.18451', 'abstract': 'In this report, we introduce a collection of methods to enhance reward modeling for LLMs, focusing specifically on data-centric techniques. We propose effective data selection and filtering strategies for curating high-quality open-source preference datasets, culminating in the Skywork-Reward data collection, which contains only 80K preference pairs -- significantly smaller than existing datasets. Using this curated dataset, we developed the Skywork-Reward model series -- Skywork-Reward-Gemma-27B and Skywork-Reward-Llama-3.1-8B -- with the former currently holding the top position on the RewardBench leaderboard. Notably, our techniques and datasets have directly enhanced the performance of many top-ranked models on RewardBench, highlighting the practical impact of our contributions in real-world preference learning applications.', 'score': 11, 'issue_id': 257, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '2ea2a42b9bcfc599', 'data': {'categories': ['#dataset', '#data', '#rlhf', '#benchmark'], 'emoji': '🚀', 'ru': {'title': 'Малые данные, большие результаты: революция в моделировании вознаграждений для LLM', 'desc': 'В этой работе представлен набор методов для улучшения моделирования вознаграждений для больших языковых моделей (LLM), с акцентом на техники, ориентированные на данные. Авторы предлагают эффективные стратегии отбора и фильтрации данных для курирования высококачественных открытых наборов данных предпочтений. Результатом стала коллекция данных Skywork-Reward, содержащая всего 80 тысяч пар предпочтений, что значительно меньше существующих наборов. Используя этот куриро  ванный набор данных, исследователи разработали серию моделей Skywork-Reward, которые показывают высокие результаты в бенчмарке RewardBench.'}, 'en': {'title': 'Smarter Data, Better Rewards: Revolutionizing LLMs with Skywork-Reward', 'desc': 'This paper presents new methods to improve reward modeling for large language models (LLMs) by focusing on data-centric approaches. The authors introduce strategies for selecting and filtering data to create high-quality preference datasets, resulting in the Skywork-Reward data collection with only 80,000 preference pairs. Using this dataset, they developed the Skywork-Reward model series, which includes models like Skywork-Reward-Gemma-27B, achieving top rankings on the RewardBench leaderboard. Their work demonstrates significant improvements in model performance, showcasing the practical benefits of their data-centric techniques in preference learning.'}, 'zh': {'title': '数据驱动的奖励建模新突破', 'desc': '这篇论文介绍了一系列方法来增强大型语言模型的奖励建模，特别是通过数据为中心的技术。作者提出了有效的数据选择和过滤策略，以便策划高质量的开源偏好数据集，最终形成了Skywork-Reward数据集，仅包含8万对偏好对，比现有数据集小得多。利用这个精心策划的数据集，开发了Skywork-Reward模型系列，其中Skywork-Reward-Gemma-27B在RewardBench排行榜上名列前茅。值得注意的是，这些技术和数据集直接提升了许多在RewardBench上排名靠前的模型的性能，突显了我们在实际偏好学习应用中的贡献。'}}}, {'id': 'https://huggingface.co/papers/2410.18362', 'title': 'WAFFLE: Multi-Modal Model for Automated Front-End Development', 'url': 'https://huggingface.co/papers/2410.18362', 'abstract': "Web development involves turning UI designs into functional webpages, which can be difficult for both beginners and experienced developers due to the complexity of HTML's hierarchical structures and styles. While Large Language Models (LLMs) have shown promise in generating source code, two major challenges persist in UI-to-HTML code generation: (1) effectively representing HTML's hierarchical structure for LLMs, and (2) bridging the gap between the visual nature of UI designs and the text-based format of HTML code. To tackle these challenges, we introduce Waffle, a new fine-tuning strategy that uses a structure-aware attention mechanism to improve LLMs' understanding of HTML's structure and a contrastive fine-tuning approach to align LLMs' understanding of UI images and HTML code. Models fine-tuned with Waffle show up to 9.00 pp (percentage point) higher HTML match, 0.0982 higher CW-SSIM, 32.99 higher CLIP, and 27.12 pp higher LLEM on our new benchmark WebSight-Test and an existing benchmark Design2Code, outperforming current fine-tuning methods.", 'score': 10, 'issue_id': 259, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '671e80fc5e01a1f7', 'data': {'categories': ['#plp', '#benchmark', '#multimodal'], 'emoji': '🧇', 'ru': {'title': 'Waffle: мост между UI-дизайном и HTML-кодом', 'desc': 'Статья представляет новую стратегию тонкой настройки моделей для генерации HTML-кода из UI-дизайнов. Метод Waffle использует механизм внимания, учитывающий структуру HTML, и контрастивное обучение для улучшения понимания моделями связи между изображениями UI и HTML-кодом. Эксперименты показывают значительное улучшение различных метрик по сравнению с существующими методами. Waffle решает проблемы эффективного представления иерархической структуры HTML для языковых моделей и преодоления разрыва между визуальной природой UI и текстовым форматом HTML.'}, 'en': {'title': 'Waffle: Bridging UI Design and HTML with Smarter LLMs', 'desc': "The paper introduces Waffle, a novel fine-tuning strategy for Large Language Models (LLMs) to improve UI-to-HTML code generation. Waffle addresses the challenges of representing HTML's hierarchical structure and aligning UI designs with HTML code by using a structure-aware attention mechanism and a contrastive fine-tuning approach. This method significantly enhances the model's performance, achieving higher scores in HTML match, CW-SSIM, CLIP, and LLEM on both new and existing benchmarks. The results demonstrate that Waffle outperforms current fine-tuning methods, making it a promising solution for web development tasks."}, 'zh': {'title': 'Waffle：提升UI到HTML代码生成的新策略', 'desc': '这篇论文介绍了一种名为Waffle的新方法，用于改进大型语言模型（LLMs）在UI到HTML代码生成中的表现。Waffle通过结构感知注意力机制来增强LLMs对HTML层次结构的理解，并使用对比微调方法来对齐LLMs对UI图像和HTML代码的理解。实验结果表明，使用Waffle微调的模型在HTML匹配度、CW-SSIM、CLIP和LLEM等指标上均优于现有方法。该方法有效解决了UI设计的视觉特性与HTML文本格式之间的差距。'}}}, {'id': 'https://huggingface.co/papers/2410.15999', 'title': 'Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering', 'url': 'https://huggingface.co/papers/2410.15999', 'abstract': 'Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context -- this phenomenon, known as context-memory knowledge conflicts, can lead to undesirable model behaviour, such as reliance on outdated or incorrect information. Analysing the internal activations of LLMs, we find that they can internally register the signals of knowledge conflict at mid-layers. Such signals allow us to detect whether a knowledge conflict occurs and use inference-time intervention strategies to resolve it. In this work, we propose SpARE, a training-free representation engineering method that uses pre-trained sparse auto-encoders (SAEs) to control the knowledge selection behaviour of LLMs. SpARE identifies the functional features that control the knowledge selection behaviours and applies them to edit the internal activations of LLMs at inference time. Our experimental results show that SpARE can effectively control the usage of either knowledge source to resolve knowledge conflict in open-domain question-answering tasks, surpassing existing representation engineering methods (+10%) as well as contrastive decoding methods (+15%).', 'score': 9, 'issue_id': 265, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '6667c4b8e3309bfb', 'data': {'categories': ['#inference', '#interpretability', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'SpARE: умное разрешение конфликтов знаний в языковых моделях', 'desc': 'Данная статья посвящена проблеме конфликтов знаний в больших языковых моделях (LLM). Авторы предлагают метод SpARE, использующий предобученные разреженные автоэнкодеры для управления выбором знаний в LLM. SpARE идентифицирует функциональные особенности, контролирующие выбор знаний, и применяет их для редактирования внутренних активаций LLM во время вывода. Эксперименты показывают, что SpARE эффективно разрешает конфликты знаний в задачах открытого вопросно-ответного анализа, превосходя существующие методы.'}, 'en': {'title': 'SpARE: Resolving Knowledge Conflicts in LLMs with Precision', 'desc': "The paper discusses how large language models (LLMs) can sometimes rely on outdated or incorrect information due to conflicts between their stored knowledge and new context. The authors introduce a method called SpARE, which uses pre-trained sparse auto-encoders to manage these conflicts without additional training. By analyzing the internal activations of LLMs, SpARE can detect and resolve knowledge conflicts by editing the model's internal processes during inference. The method shows significant improvements in open-domain question-answering tasks, outperforming other techniques by a notable margin."}, 'zh': {'title': 'SpARE：无训练知识冲突解决方案', 'desc': '大型语言模型（LLMs）可以在其参数中存储大量的事实知识，但这些知识可能与上下文提供的信息相冲突，导致模型行为不理想。通过分析LLMs的内部激活，我们发现它们可以在中间层内部记录知识冲突的信号。我们提出了一种名为SpARE的方法，利用预训练的稀疏自编码器（SAEs）来控制LLMs的知识选择行为。实验结果表明，SpARE在开放域问答任务中有效解决知识冲突，性能优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2410.18775', 'title': 'Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances', 'url': 'https://huggingface.co/papers/2410.18775', 'abstract': 'Current image watermarking methods are vulnerable to advanced image editing techniques enabled by large-scale text-to-image models. These models can distort embedded watermarks during editing, posing significant challenges to copyright protection. In this work, we introduce W-Bench, the first comprehensive benchmark designed to evaluate the robustness of watermarking methods against a wide range of image editing techniques, including image regeneration, global editing, local editing, and image-to-video generation. Through extensive evaluations of eleven representative watermarking methods against prevalent editing techniques, we demonstrate that most methods fail to detect watermarks after such edits. To address this limitation, we propose VINE, a watermarking method that significantly enhances robustness against various image editing techniques while maintaining high image quality. Our approach involves two key innovations: (1) we analyze the frequency characteristics of image editing and identify that blurring distortions exhibit similar frequency properties, which allows us to use them as surrogate attacks during training to bolster watermark robustness; (2) we leverage a large-scale pretrained diffusion model SDXL-Turbo, adapting it for the watermarking task to achieve more imperceptible and robust watermark embedding. Experimental results show that our method achieves outstanding watermarking performance under various image editing techniques, outperforming existing methods in both image quality and robustness. Code is available at https://github.com/Shilin-LU/VINE.', 'score': 9, 'issue_id': 258, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '1464291ab9f836a5', 'data': {'categories': ['#benchmark', '#cv', '#optimization'], 'emoji': '🔐', 'ru': {'title': 'VINE: Новый рубеж в защите авторских прав на изображения', 'desc': 'Статья представляет новый бенчмарк W-Bench для оценки устойчивости методов водяных знаков к современным техникам редактирования изображений. Авторы предлагают метод VINE, который значительно повышает робастность водяных знаков, сохраняя высокое качество изображений. VINE использует анализ частотных характеристик редактирования изображений и предобученную диффузионную модель SDXL-Turbo для улучшения встраивания водяных знаков. Экспериментальные результаты показывают превосходство VINE над существующими методами по качеству изображений и устойчивости водяных знаков.'}, 'en': {'title': 'VINE: Revolutionizing Watermark Robustness in the Age of Advanced Image Editing', 'desc': 'The paper addresses the vulnerability of current image watermarking methods to advanced editing techniques enabled by large-scale text-to-image models. It introduces W-Bench, a benchmark to evaluate the robustness of watermarking methods against various editing techniques, revealing that most methods fail to detect watermarks post-editing. To improve this, the authors propose VINE, a new watermarking method that enhances robustness by analyzing frequency characteristics of edits and using a pretrained diffusion model for better watermark embedding. Experimental results show that VINE outperforms existing methods in maintaining both image quality and watermark robustness.'}, 'zh': {'title': '提升水印鲁棒性的新方法：VINE', 'desc': '当前的图像水印方法容易受到由大规模文本到图像模型启用的高级图像编辑技术的攻击。这些模型在编辑过程中可能会扭曲嵌入的水印，给版权保护带来重大挑战。本文介绍了W-Bench，这是第一个全面的基准，用于评估水印方法在各种图像编辑技术下的鲁棒性。我们提出了一种名为VINE的水印方法，通过分析图像编辑的频率特性和利用大规模预训练扩散模型，显著提高了水印的鲁棒性和图像质量。'}}}, {'id': 'https://huggingface.co/papers/2410.18976', 'title': 'CAMEL-Bench: A Comprehensive Arabic LMM Benchmark', 'url': 'https://huggingface.co/papers/2410.18976', 'abstract': 'Recent years have witnessed a significant interest in developing large multimodal models (LMMs) capable of performing various visual reasoning and understanding tasks. This has led to the introduction of multiple LMM benchmarks to evaluate LMMs on different tasks. However, most existing LMM evaluation benchmarks are predominantly English-centric. In this work, we develop a comprehensive LMM evaluation benchmark for the Arabic language to represent a large population of over 400 million speakers. The proposed benchmark, named CAMEL-Bench, comprises eight diverse domains and 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding to evaluate broad scenario generalizability. Our CAMEL-Bench comprises around 29,036 questions that are filtered from a larger pool of samples, where the quality is manually verified by native speakers to ensure reliable model assessment. We conduct evaluations of both closed-source, including GPT-4 series, and open-source LMMs. Our analysis reveals the need for substantial improvement, especially among the best open-source models, with even the closed-source GPT-4o achieving an overall score of 62%. Our benchmark and evaluation scripts are open-sourced.', 'score': 8, 'issue_id': 260, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '932cc733871eb790', 'data': {'categories': ['#benchmark', '#multimodal', '#multilingual'], 'emoji': '🐪', 'ru': {'title': 'CAMEL-Bench: первый комплексный бенчмарк для мультимодальных моделей на арабском языке', 'desc': 'В статье представлен новый бенчмарк CAMEL-Bench для оценки мультимодальных языковых моделей (LMM) на арабском языке. Бенчмарк охватывает 8 разнообразных областей и 38 подобластей, включая понимание нескольких изображений, сложное визуальное восприятие и анализ видео. CAMEL-Bench содержит около 29,036 вопросов, качество которых проверено носителями языка. Оценка показала, что даже лучшие модели, такие как GPT-4, достигают лишь 62% общего балла, что указывает на необходимость дальнейшего улучшения LMM для арабского языка.'}, 'en': {'title': 'Bridging the Language Gap in Multimodal AI', 'desc': 'The paper introduces CAMEL-Bench, a new benchmark designed to evaluate large multimodal models (LMMs) in the Arabic language, addressing the lack of non-English-centric evaluation tools. CAMEL-Bench covers a wide range of tasks across eight domains, such as visual perception and medical imaging, to test the generalizability of LMMs. The benchmark includes over 29,000 questions, carefully curated and verified by native speakers to ensure accuracy. Evaluations show that even advanced models like GPT-4o have room for improvement, highlighting the need for better open-source LMMs.'}, 'zh': {'title': 'CAMEL-Bench：阿拉伯语多模态模型评估新基准', 'desc': '近年来，研究人员对开发能够执行各种视觉推理和理解任务的大型多模态模型（LMMs）表现出极大兴趣。现有的LMM评估基准大多以英语为中心，而本文提出了一个针对阿拉伯语的全面LMM评估基准，名为CAMEL-Bench。该基准涵盖八个不同领域和38个子领域，包括多图像理解、复杂视觉感知、手写文档理解等，以评估模型的广泛适应性。评估结果显示，即使是封闭源的GPT-4o模型也仅获得62%的总分，表明仍需大幅改进。'}}}, {'id': 'https://huggingface.co/papers/2410.18958', 'title': 'Stable Consistency Tuning: Understanding and Improving Consistency Models', 'url': 'https://huggingface.co/papers/2410.18958', 'abstract': 'Diffusion models achieve superior generation quality but suffer from slow generation speed due to the iterative nature of denoising. In contrast, consistency models, a new generative family, achieve competitive performance with significantly faster sampling. These models are trained either through consistency distillation, which leverages pretrained diffusion models, or consistency training/tuning directly from raw data. In this work, we propose a novel framework for understanding consistency models by modeling the denoising process of the diffusion model as a Markov Decision Process (MDP) and framing consistency model training as the value estimation through Temporal Difference~(TD) Learning. More importantly, this framework allows us to analyze the limitations of current consistency training/tuning strategies. Built upon Easy Consistency Tuning (ECT), we propose Stable Consistency Tuning (SCT), which incorporates variance-reduced learning using the score identity. SCT leads to significant performance improvements on benchmarks such as CIFAR-10 and ImageNet-64. On ImageNet-64, SCT achieves 1-step FID 2.42 and 2-step FID 1.55, a new SoTA for consistency models.', 'score': 8, 'issue_id': 257, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '1e70afdddccea2fd', 'data': {'categories': ['#diffusion', '#rl', '#benchmark'], 'emoji': '🚀', 'ru': {'title': 'Революция в генеративных моделях: SCT устанавливает новые стандарты', 'desc': 'Статья представляет новый подход к пониманию моделей согласованности, рассматривая процесс шумоподавления как марковский процесс принятия решений. Авторы предлагают метод Stable Consistency Tuning (SCT), который использует обучение с временной разницей и снижение дисперсии для улучшения производительности. SCT достигает значительных улучшений на таких бенчмарках, как CIFAR-10 и ImageNet-64, устанавливая новый рекорд для моделей согласованности. Этот метод позволяет преодолеть ограничения существующих стратегий обучения моделей согласованности.'}, 'en': {'title': 'Speed Meets Quality: Revolutionizing Generative Models with Consistency', 'desc': 'This paper introduces a new framework for understanding consistency models by viewing the denoising process of diffusion models as a Markov Decision Process (MDP). It uses Temporal Difference (TD) Learning to train consistency models, which are faster than traditional diffusion models. The authors propose Stable Consistency Tuning (SCT), an improved method that reduces variance in learning, leading to better performance. SCT achieves state-of-the-art results on benchmarks like CIFAR-10 and ImageNet-64, demonstrating its effectiveness.'}, 'zh': {'title': '一致性模型：快速生成的新纪元', 'desc': '扩散模型生成质量高，但速度慢。相反，一致性模型速度快，性能好。本文提出用马尔可夫决策过程理解一致性模型。我们的方法在CIFAR-10和ImageNet-64上表现优异。'}}}, {'id': 'https://huggingface.co/papers/2410.18860', 'title': 'DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations', 'url': 'https://huggingface.co/papers/2410.18860', 'abstract': 'Large Language Models (LLMs) often hallucinate, producing unfaithful or factually incorrect outputs by misrepresenting the provided context or incorrectly recalling internal knowledge. Recent studies have identified specific attention heads within the Transformer architecture, known as retrieval heads, responsible for extracting relevant contextual information. We hypothesise that masking these retrieval heads can induce hallucinations and that contrasting the outputs of the base LLM and the masked LLM can reduce hallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads (DeCoRe), a novel training-free decoding strategy that amplifies information found in the context and model parameters. DeCoRe mitigates potentially hallucinated responses by dynamically contrasting the outputs of the base LLM and the masked LLM, using conditional entropy as a guide. Our extensive experiments confirm that DeCoRe significantly improves performance on tasks requiring high contextual faithfulness, such as summarisation (XSum by 18.6%), instruction following (MemoTrap by 10.9%), and open-book question answering (NQ-Open by 2.4% and NQ-Swap by 5.5%).', 'score': 7, 'issue_id': 264, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'c82b0b4cc69a7fd9', 'data': {'categories': ['#hallucinations', '#interpretability'], 'emoji': '🎭', 'ru': {'title': 'Борьба с галлюцинациями в LLM путем контрастирования голов внимания', 'desc': 'Исследователи предложили новый метод декодирования под названием DeCoRe для уменьшения галлюцинаций в больших языковых моделях (LLM). Метод основан на маскировании определенных голов внимания в архитектуре Transformer, отвечающих за извлечение контекстной информации. DeCoRe сравнивает выходные данные базовой LLM и замаскированной LLM, используя условную энтропию. Эксперименты показали значительное улучшение производительности в задачах, требующих высокой контекстной точности, таких как суммаризация, следование инструкциям и ответы на вопросы.'}, 'en': {'title': 'DeCoRe: Enhancing LLM Accuracy by Contrasting Retrieval Heads', 'desc': 'The paper addresses the issue of hallucinations in Large Language Models (LLMs), where models produce incorrect outputs by misrepresenting context or recalling wrong information. It identifies specific attention heads, called retrieval heads, in the Transformer architecture that are crucial for extracting relevant information. The authors propose a method called Decoding by Contrasting Retrieval Heads (DeCoRe), which reduces hallucinations by comparing outputs from the original and masked LLMs. Experiments show that DeCoRe significantly improves the accuracy of tasks like summarization and question answering by enhancing contextual faithfulness.'}, 'zh': {'title': '对比检索头，减少模型幻觉', 'desc': '大型语言模型（LLMs）有时会产生不准确或不真实的输出，这被称为幻觉。研究发现，Transformer架构中的特定注意力头，称为检索头，负责提取相关的上下文信息。我们提出了一种新的解码策略，称为对比检索头解码（DeCoRe），通过对比基础模型和掩蔽模型的输出来减少幻觉。实验表明，DeCoRe在需要高上下文忠实度的任务中显著提高了性能。'}}}, {'id': 'https://huggingface.co/papers/2410.17779', 'title': 'ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning', 'url': 'https://huggingface.co/papers/2410.17779', 'abstract': 'Recent advancements in multimodal fusion have witnessed the remarkable success of vision-language (VL) models, which excel in various multimodal applications such as image captioning and visual question answering. However, building VL models requires substantial hardware resources, where efficiency is restricted by two key factors: the extended input sequence of the language model with vision features demands more computational operations, and a large number of additional learnable parameters increase memory complexity. These challenges significantly restrict the broader applicability of such models. To bridge this gap, we propose ADEM-VL, an efficient vision-language method that tunes VL models based on pretrained large language models (LLMs) by adopting a parameter-free cross-attention mechanism for similarity measurements in multimodal fusion. This approach only requires embedding vision features into the language space, significantly reducing the number of trainable parameters and accelerating both training and inference speeds. To enhance representation learning in fusion module, we introduce an efficient multiscale feature generation scheme that requires only a single forward pass through the vision encoder. Moreover, we propose an adaptive fusion scheme that dynamically discards less relevant visual information for each text token based on its attention score. This ensures that the fusion process prioritizes the most pertinent visual features. With experiments on various tasks including visual question answering, image captioning, and instruction-following, we demonstrate that our framework outperforms existing approaches. Specifically, our method surpasses existing methods by an average accuracy of 0.77% on ScienceQA dataset, with reduced training and inference latency, demonstrating the superiority of our framework. The code is available at https://github.com/Hao840/ADEM-VL.', 'score': 7, 'issue_id': 260, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'ce2779c63c3de4ec', 'data': {'categories': ['#multimodal', '#cv', '#inference'], 'emoji': '🔮', 'ru': {'title': 'ADEM-VL: Эффективное мультимодальное слияние для моделей зрения-языка', 'desc': 'Статья представляет ADEM-VL - эффективный метод для моделей зрения-языка, основанный на предобученных больших языковых моделях (LLM). Он использует механизм кросс-внимания без параметров для измерения сходства в мультимодальном слиянии, что значительно сокращает количество обучаемых параметров и ускоряет процессы обучения и вывода. Метод включает эффективную схему генерации многомасштабных признаков и адаптивное слияние для динамического отбрасывания менее релевантной визуальной информации. Эксперименты показывают превосходство ADEM-VL над существующими подходами в различных задачах, включая ответы на визуальные вопросы и генерацию подписей к изображениям.'}, 'en': {'title': 'Efficient Vision-Language Fusion: ADEM-VL Revolutionizes Multimodal Models', 'desc': 'The paper introduces ADEM-VL, a new method for improving vision-language models by using a parameter-free cross-attention mechanism. This approach reduces the number of trainable parameters and speeds up both training and inference by embedding vision features into the language space. It also includes a multiscale feature generation scheme and an adaptive fusion process to prioritize relevant visual information. Experiments show that ADEM-VL outperforms existing methods in tasks like visual question answering and image captioning, with better accuracy and efficiency.'}, 'zh': {'title': 'ADEM-VL：高效的视觉语言融合新方法', 'desc': '这篇论文介绍了一种名为ADEM-VL的高效视觉语言方法，通过无参数的交叉注意机制来优化预训练的大型语言模型，从而实现多模态融合。该方法通过将视觉特征嵌入到语言空间中，显著减少了可训练参数的数量，加快了训练和推理速度。为了提高融合模块的表示学习，论文引入了一种高效的多尺度特征生成方案，并提出了一种自适应融合方案，动态丢弃与文本不相关的视觉信息。实验结果表明，该方法在多个任务上表现优异，尤其是在ScienceQA数据集上平均准确率提高了0.77%。'}}}, {'id': 'https://huggingface.co/papers/2410.18505', 'title': 'CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models', 'url': 'https://huggingface.co/papers/2410.18505', 'abstract': 'We present CCI3.0-HQ (https://huggingface.co/datasets/BAAI/CCI3-HQ), a high-quality 500GB subset of the Chinese Corpora Internet 3.0 (CCI3.0)(https://huggingface.co/datasets/BAAI/CCI3-Data), developed using a novel two-stage hybrid filtering pipeline that significantly enhances data quality. To evaluate its effectiveness, we trained a 0.5B parameter model from scratch on 100B tokens across various datasets, achieving superior performance on 10 benchmarks in a zero-shot setting compared to CCI3.0, SkyPile, and WanjuanV1. The high-quality filtering process effectively distills the capabilities of the Qwen2-72B-instruct model into a compact 0.5B model, attaining optimal F1 scores for Chinese web data classification. We believe this open-access dataset will facilitate broader access to high-quality language models.', 'score': 7, 'issue_id': 257, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '4f06cfa8b602eba9', 'data': {'categories': ['#dataset', '#data', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Высококачественные данные для компактных и мощных языковых моделей', 'desc': 'Представлен CCI3.0-HQ - высококачественный набор данных объемом 500 ГБ, полученный из Chinese Corpora Internet 3.0. Для его создания использовался новый двухэтапный гибридный конвейер фильтрации, значительно улучшающий качество данных. Модель с 0.5 миллиардами параметров, обученная на этом наборе, превзошла аналоги на 10 бенчмарках в режиме zero-shot. Процесс фильтрации эффективно дистиллировал возможности модели Qwen2-72B-instruct в компактную модель размером 0.5B.'}, 'en': {'title': 'Unlocking High-Quality Language Models with CCI3.0-HQ', 'desc': 'The paper introduces CCI3.0-HQ, a refined 500GB dataset from the Chinese Corpora Internet 3.0, created using an innovative two-stage hybrid filtering process to improve data quality. A 0.5 billion parameter model was trained on 100 billion tokens from various datasets, outperforming other models in zero-shot settings on 10 benchmarks. This filtering method successfully compresses the capabilities of a larger model into a smaller one, achieving high F1 scores in Chinese web data classification. The open-access nature of this dataset aims to make high-quality language models more accessible to researchers and developers.'}, 'zh': {'title': '高质量中文语料库，提升语言模型表现', 'desc': 'CCI3.0-HQ 是一个高质量的中文语料库子集，使用创新的两阶段混合过滤流程来提升数据质量。研究人员从头开始训练了一个拥有0.5亿参数的模型，并在多种数据集上处理了1000亿个标记，结果在10个基准测试中表现优异。这个高质量的过滤过程有效地将Qwen2-72B-instruct模型的能力浓缩到一个紧凑的0.5亿模型中，实现了中文网络数据分类的最佳F1分数。我们相信这个开放访问的数据集将促进高质量语言模型的更广泛使用。'}}}, {'id': 'https://huggingface.co/papers/2410.17897', 'title': 'Value Residual Learning For Alleviating Attention Concentration In Transformers', 'url': 'https://huggingface.co/papers/2410.17897', 'abstract': 'Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earlier layers to be directly accessible to later layers. However, this approach is computationally expensive. To address this problem, we propose Transformer with residual value (ResFormer) which approximates cross-layer attention through adding a residual connection from the values of the the first layer to all subsequent layers. Based on this method, one variant is the Transformer with single layer value (SVFormer), where all layers share the same value embedding from first layer, reducing the KV cache by nearly 50%. Comprehensive empirical evidence demonstrates that ResFormer mitigates attention concentration problem in deeper layers and enhances representation across most layers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as downstream tasks. SVFormer trains significantly faster than the vanilla Transformer and performs better than other methods like GQA and CLA, with performance influenced by sequence length and cumulative learning rate.', 'score': 6, 'issue_id': 261, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'fe24d2d7be245ec3', 'data': {'categories': ['#architecture'], 'emoji': '🔄', 'ru': {'title': 'ResFormer: эффективное улучшение внимания в трансформерах', 'desc': 'Статья представляет новый подход к улучшению работы трансформеров под названием ResFormer. Он решает проблему концентрации внимания в глубоких слоях путем добавления остаточного соединения от значений первого слоя ко всем последующим. Предложенная модификация SVFormer использует одинаковые значения эмбеддингов из первого слоя для всех слоев, что сокращает KV-кэш почти на 50%. Эмпирические исследования показывают, что ResFormer превосходит стандартный трансформер и другие методы по ошибке обучения и в задачах обработки естественного языка.'}, 'en': {'title': 'ResFormer: Smarter, Faster Transformers with Less Concentration', 'desc': 'Transformers are powerful because they use self-attention to understand relationships between all parts of a sequence, but this can lead to too much focus on certain parts when many layers are stacked. To solve this, the paper introduces ResFormer, which uses a clever shortcut to let information from the first layer help later layers without needing lots of extra calculations. Another version, SVFormer, simplifies this even more by using the same information from the first layer across all layers, making it faster and more efficient. Tests show that these new methods not only reduce the problem of over-concentration but also improve learning and performance compared to traditional Transformers and other models.'}, 'zh': {'title': 'ResFormer：突破注意力集中瓶颈的新方法', 'desc': '这篇论文介绍了一种新的Transformer模型，称为ResFormer，通过在所有后续层中添加来自第一层的残差连接来近似跨层注意力，从而解决了注意力集中问题。ResFormer在更深层次上缓解了注意力集中问题，并在大多数层中增强了表示能力，优于传统的Transformer和其他模型。另一种变体SVFormer通过共享第一层的值嵌入，减少了近50%的KV缓存，显著加快了训练速度。实验结果表明，SVFormer在训练速度和性能上都优于其他方法，受序列长度和累积学习率的影响。'}}}, {'id': 'https://huggingface.co/papers/2410.15580', 'title': 'Language Models are Symbolic Learners in Arithmetic', 'url': 'https://huggingface.co/papers/2410.15580', 'abstract': 'Large Language Models (LLMs) are thought to struggle with arithmetic learning due to the inherent differences between language modeling and numerical computation, but concrete evidence has been lacking. This work responds to this claim through a two-side experiment. We first investigate whether LLMs leverage partial products during arithmetic learning. We find that although LLMs can identify some partial products after learning, they fail to leverage them for arithmetic tasks, conversely. We then explore how LLMs approach arithmetic symbolically by breaking tasks into subgroups, hypothesizing that difficulties arise from subgroup complexity and selection. Our results show that when subgroup complexity is fixed, LLMs treat a collection of different arithmetic operations similarly. By analyzing position-level accuracy across different training sizes, we further observe that it follows a U-shaped pattern: LLMs quickly learn the easiest patterns at the first and last positions, while progressively learning the more difficult patterns in the middle positions. This suggests that LLMs select subgroup following an easy-to-hard paradigm during learning. Our work confirms that LLMs are pure symbolic learners in arithmetic tasks and underscores the importance of understanding them deeply through subgroup-level quantification.', 'score': 5, 'issue_id': 259, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '8d361b61d5fc18c3', 'data': {'categories': ['#reasoning', '#interpretability'], 'emoji': '🧮', 'ru': {'title': 'LLM в арифметике: символическое обучение от простого к сложному', 'desc': "Это исследование посвящено изучению способностей больших языковых моделей (LLM) к обучению арифметике. Авторы обнаружили, что хотя LLM могут идентифицировать частичные произведения после обучения, они не способны использовать их для арифметических задач. Исследование также показало, что LLM подходят к арифметике символически, разбивая задачи на подгруппы, и выбирают подгруппы по принципу 'от простого к сложному'. Результаты подтверждают, что LLM являются чисто символическими учащимися в арифметических задачах."}, 'en': {'title': 'Unveiling the Symbolic Mind: How LLMs Tackle Arithmetic', 'desc': "This paper investigates how Large Language Models (LLMs) handle arithmetic tasks, revealing that they struggle to use partial products effectively. The study shows that LLMs approach arithmetic symbolically, breaking tasks into subgroups and learning them in an easy-to-hard sequence. The research finds that LLMs treat different arithmetic operations similarly when subgroup complexity is controlled. The findings highlight the importance of understanding LLMs' symbolic learning processes through subgroup-level analysis."}, 'zh': {'title': '揭示大型语言模型的算术学习奥秘', 'desc': '这篇论文研究了大型语言模型（LLMs）在算术学习中的表现。研究发现，虽然LLMs可以识别部分乘积，但在算术任务中并未有效利用这些信息。通过将任务分解为子组，研究表明LLMs在固定子组复杂度时，对不同算术操作的处理方式相似。结果显示，LLMs在学习过程中遵循从易到难的模式，强调了通过子组层次的量化来深入理解LLMs的重要性。'}}}, {'id': 'https://huggingface.co/papers/2410.18785', 'title': 'Should We Really Edit Language Models? On the Evaluation of Edited Language Models', 'url': 'https://huggingface.co/papers/2410.18785', 'abstract': 'Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. Current methods mainly focus on reliability, generalization, and locality, with many methods excelling across these criteria. Some recent works disclose the pitfalls of these editing methods such as knowledge distortion or conflict. However, the general abilities of post-edited language models remain unexplored. In this paper, we perform a comprehensive evaluation on various editing methods and different language models, and have following findings. (1) Existing editing methods lead to inevitable performance deterioration on general benchmarks, indicating that existing editing methods maintain the general abilities of the model within only a few dozen edits. When the number of edits is slightly large, the intrinsic knowledge structure of the model is disrupted or even completely damaged. (2) Instruction-tuned models are more robust to editing, showing less performance drop on general knowledge after editing. (3) Language model with large scale is more resistant to editing compared to small model. (4) The safety of the edited model, is significantly weakened, even for those safety-aligned models. Our findings indicate that current editing methods are only suitable for small-scale knowledge updates within language models, which motivates further research on more practical and reliable editing methods. The details of code and reproduction can be found in https://github.com/lqinfdim/EditingEvaluation.', 'score': 5, 'issue_id': 257, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '8ee1a0454341105c', 'data': {'categories': ['#interpretability', '#alignment'], 'emoji': '✏️', 'ru': {'title': 'Редактирование языковых моделей: компромисс между обновлением знаний и сохранением общих способностей', 'desc': 'В статье рассматривается влияние методов редактирования на общие способности языковых моделей. Исследование показывает, что существующие методы редактирования приводят к неизбежному ухудшению производительности на общих бенчмарках, особенно при большом количестве правок. Модели, настроенные на инструкции, и крупномасштабные языковые модели оказываются более устойчивыми к редактированию. Однако безопасность отредактированных моделей значительно снижается, даже для моделей, ориентированных на безопасность.'}, 'en': {'title': 'Balancing Act: Navigating the Challenges of Model Editing in Language Models', 'desc': 'The paper explores the impact of model editing on language models, revealing that current methods can lead to performance deterioration when too many edits are made. It finds that instruction-tuned models and larger models are more robust to these edits, maintaining better performance. However, even safety-aligned models experience weakened safety post-editing. The study suggests that existing editing methods are only effective for small-scale updates, highlighting the need for more reliable techniques.'}, 'zh': {'title': '语言模型编辑：小规模更新的挑战与机遇', 'desc': '这篇论文探讨了在语言模型中进行知识更新的编辑方法。研究发现，现有的编辑方法在进行大量编辑时，会导致模型性能下降，甚至破坏模型的内在知识结构。经过指令调优的模型在编辑后表现出更强的鲁棒性，而大规模的语言模型比小规模模型更能抵抗编辑的影响。此外，编辑后的模型安全性显著降低。'}}}, {'id': 'https://huggingface.co/papers/2410.18252', 'title': 'Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models', 'url': 'https://huggingface.co/papers/2410.18252', 'abstract': "The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this paradigm is computationally inefficient. Inspired by classical deep RL literature, we propose separating generation and learning in RLHF. This enables asynchronous generation of new samples while simultaneously training on old samples, leading to faster training and more compute-optimal scaling. However, asynchronous training relies on an underexplored regime, online but off-policy RLHF: learning on samples from previous iterations of our model. To understand the challenges in this regime, we investigate a fundamental question: how much off-policyness can we tolerate for asynchronous training to speed up learning but maintain performance? Among several RLHF algorithms we tested, we find that online DPO is most robust to off-policy data, and robustness increases with the scale of the policy model. We study further compute optimizations for asynchronous RLHF but find that they come at a performance cost, giving rise to a trade-off. Finally, we verify the scalability of asynchronous RLHF by training LLaMA 3.1 8B on an instruction-following task 40% faster than a synchronous run while matching final performance.", 'score': 4, 'issue_id': 265, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '9f92e038af1d74a2', 'data': {'categories': ['#rlhf', '#rl'], 'emoji': '⚡', 'ru': {'title': 'Ускорение RLHF через асинхронное обучение', 'desc': 'Статья предлагает новый подход к обучению с подкреплением по обратной связи человека (RLHF) для больших языковых моделей. Авторы предлагают разделить процессы генерации и обучения, что позволяет асинхронно генерировать новые образцы и одновременно обучаться на старых. Исследование показывает, что онлайн-алгоритм DPO наиболее устойчив к использованию офф-политики данных. Эксперименты подтверждают, что асинхронный RLHF позволяет обучать модели на 40% быстрее при сохранении производительности.'}, 'en': {'title': 'Speeding Up RLHF: Asynchronous Training for Faster Learning', 'desc': 'The paper explores a new approach to Reinforcement Learning with Human Feedback (RLHF) by separating the generation of data from the learning process, which allows for asynchronous training. This method, inspired by classical deep reinforcement learning, enables the model to generate new samples while training on old ones, improving computational efficiency. The study investigates the balance between using off-policy data and maintaining performance, finding that the online DPO algorithm is particularly robust in this setting. The authors demonstrate that this asynchronous approach can significantly speed up training times, as shown by their experiments with the LLaMA 3.1 8B model.'}, 'zh': {'title': '异步RLHF：加速训练的新范式', 'desc': '这篇论文探讨了在强化学习中使用人类反馈（RLHF）的新方法。传统方法效率低下，而作者提出将生成和学习过程分离，以提高训练速度。通过异步生成新样本并同时训练旧样本，研究表明这种方法可以加快训练速度。尽管如此，异步训练需要在在线但非策略的情况下进行，作者研究了这种情况下的挑战和权衡。'}}}, {'id': 'https://huggingface.co/papers/2410.18194', 'title': 'ZIP-FIT: Embedding-Free Data Selection via Compression-Based Alignment', 'url': 'https://huggingface.co/papers/2410.18194', 'abstract': 'Data selection is crucial for optimizing language model (LM) performance on specific tasks, yet most existing methods fail to effectively consider the target task distribution.   Current approaches either ignore task-specific requirements entirely or rely on approximations that fail to capture the nuanced patterns needed for tasks like Autoformalization or code generation.   Methods that do consider the target distribution often rely on simplistic, sometimes noisy, representations, like hashed n-gram features, which can lead to collisions and introduce noise.   We introduce ZIP-FIT, a data selection framework that uses gzip compression to directly measure alignment between potential training data and the target task distribution.   In extensive evaluations on Autoformalization and Python code generation, ZIP-FIT significantly outperforms leading baselines like DSIR and D4.   Models trained on ZIP-FIT-selected data achieve their lowest cross-entropy loss up to 85.1\\% faster than baselines, demonstrating that better task alignment leads to more efficient learning.   In addition, ZIP-FIT performs selection up to 65.8\\% faster than DSIR and two orders of magnitude faster than D4.   Notably, ZIP-FIT shows that smaller, well-aligned datasets often outperform larger but less targeted ones, demonstrating that a small amount of higher quality data is superior to a large amount of lower quality data.   Our results imply that task-aware data selection is crucial for efficient domain adaptation, and that compression offers a principled way to measure task alignment.   By showing that targeted data selection can dramatically improve task-specific performance, our work provides new insights into the relationship between data quality, task alignment, and model learning efficiency.', 'score': 3, 'issue_id': 268, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '36d3e8b24c9c11c2', 'data': {'categories': ['#data', '#transfer_learning'], 'emoji': '🎯', 'ru': {'title': 'Эффективный отбор данных для точной настройки языковых моделей', 'desc': 'ZIP-FIT - это новый метод отбора данных для обучения языковых моделей, использующий сжатие gzip для измерения соответствия между потенциальными обучающими данными и целевым распределением задачи. В сравнении с существующими подходами, ZIP-FIT значительно превосходит базовые методы в задачах автоформализации и генерации кода на Python. Модели, обученные на данных, отобранных ZIP-FIT, достигают минимальной кросс-энтропийной ошибки до 85.1% быстрее, чем базовые методы. Результаты показывают, что небольшие, но хорошо подобранные наборы данных часто превосходят большие, но менее целевые.'}, 'en': {'title': '"ZIP-FIT: Compressing Data, Expanding Performance"', 'desc': 'The paper introduces ZIP-FIT, a novel data selection framework that uses gzip compression to align training data with specific task distributions, improving language model performance. ZIP-FIT significantly outperforms existing methods like DSIR and D4 in tasks such as Autoformalization and Python code generation by achieving lower cross-entropy loss faster. The framework demonstrates that smaller, well-aligned datasets can be more effective than larger, less targeted ones, highlighting the importance of data quality over quantity. This approach underscores the value of task-aware data selection and suggests that compression can effectively measure task alignment, leading to more efficient domain adaptation.'}, 'zh': {'title': 'ZIP-FIT：通过压缩实现高效任务对齐的数据选择', 'desc': '这篇论文介绍了一种名为ZIP-FIT的数据选择框架，通过使用gzip压缩来直接测量潜在训练数据与目标任务分布的对齐程度。ZIP-FIT在自动形式化和Python代码生成任务中表现优异，比现有的基准方法如DSIR和D4更快、更有效。研究表明，较小但高质量的数据集往往比大而不精确的数据集表现更好，强调了数据质量和任务对齐的重要性。通过展示有针对性的数据选择如何显著提高任务特定性能，ZIP-FIT为数据质量、任务对齐和模型学习效率之间的关系提供了新的见解。'}}}, {'id': 'https://huggingface.co/papers/2410.18441', 'title': 'The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI', 'url': 'https://huggingface.co/papers/2410.18441', 'abstract': 'In this paper, we give an in-depth analysis on the mathematical problem formulations and the probabilistic optimization explorations for some of the key components in Transformer model [33] in the field of generative AI. We explore and discuss some potential further enhancement for current state of the art methods for some key underlying technologies of generative AI models from algorithmic and probabilistic optimization perspective. In particular, we present an optimal solution for sub-word encoding (SWE) based on similar initial settings as that of byte-pair encoding (BPE) algorithm in [9] with similar objectives as that of WordPiece approach in [28, 31] to maximize the likelihood of the training data. We also present cross entropy optimization method to optimize hyperparameters for word2vec model [17]. In addition, we propose a factored combination of rotary positional encoding (RoPE) [32] and attention with linear biases (ALiBi) [23] with a harmonic series. We also present a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a probability distribution over block distances in the matrix to decide which block is likely to participate in a given round of attention computation while maintaining the lower triangle shape of the tensor for autoregressive language models by re-shaping the tensors. Finally, we present staircase adaptive quantization (SAQ) of key-value (KV) cache for multi-query attention (MQA) based on the framework presented in [16] to have gradual quantization degradation while achieving reasonable model quality and cost savings.', 'score': 3, 'issue_id': 266, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '3e66fdb5b0da206e', 'data': {'categories': ['#math', '#optimization', '#architecture', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация ключевых компонентов Transformer для улучшения генеративного ИИ', 'desc': 'Статья представляет глубокий анализ математических формулировок и вероятностных оптимизаций ключевых компонентов модели Transformer в области генеративного ИИ. Авторы предлагают оптимальное решение для кодирования подслов (SWE), основанное на алгоритме byte-pair encoding (BPE) и подходе WordPiece. Представлен метод оптимизации перекрестной энтропии для настройки гиперпараметров модели word2vec. Также предложена комбинация позиционного кодирования RoPE и метода внимания ALiBi с гармоническим рядом, а также вероятностный метод FlashAttention (PrFlashAttention) для эффективного вычисления внимания.'}, 'en': {'title': 'Optimizing Transformers: Enhancing Generative AI with Mathematical Precision', 'desc': 'This paper delves into the mathematical and probabilistic optimization aspects of Transformer models, focusing on enhancing generative AI technologies. It introduces an optimal sub-word encoding solution inspired by byte-pair encoding and WordPiece to improve training data likelihood. The authors propose a novel combination of rotary positional encoding and attention with linear biases, alongside a probabilistic FlashAttention method to optimize attention computation. Additionally, they present a staircase adaptive quantization technique for multi-query attention to balance model quality and cost efficiency.'}, 'zh': {'title': '生成式AI的概率优化新突破', 'desc': '这篇论文深入分析了生成式AI中Transformer模型的一些关键组件的数学问题和概率优化探索。作者提出了一种基于子词编码的最优解决方案，类似于字节对编码和WordPiece方法，以最大化训练数据的可能性。此外，论文还介绍了一种交叉熵优化方法来优化word2vec模型的超参数，并提出了旋转位置编码和线性偏置注意力的组合方法。最后，作者提出了一种阶梯自适应量化方法，用于多查询注意力的键值缓存，以在保持模型质量的同时节省成本。'}}}, {'id': 'https://huggingface.co/papers/2410.16429', 'title': 'Pantograph: A Machine-to-Machine Interaction Interface for Advanced Theorem Proving, High Level Reasoning, and Data Extraction in Lean 4', 'url': 'https://huggingface.co/papers/2410.16429', 'abstract': "Machine-assisted theorem proving refers to the process of conducting structured reasoning to automatically generate proofs for mathematical theorems. Recently, there has been a surge of interest in using machine learning models in conjunction with proof assistants to perform this task. In this paper, we introduce Pantograph, a tool that provides a versatile interface to the Lean 4 proof assistant and enables efficient proof search via powerful search algorithms such as Monte Carlo Tree Search. In addition, Pantograph enables high-level reasoning by enabling a more robust handling of Lean 4's inference steps. We provide an overview of Pantograph's architecture and features. We also report on an illustrative use case: using machine learning models and proof sketches to prove Lean 4 theorems. Pantograph's innovative features pave the way for more advanced machine learning models to perform complex proof searches and high-level reasoning, equipping future researchers to design more versatile and powerful theorem provers.", 'score': 2, 'issue_id': 268, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '0dae164d1e8a7708', 'data': {'categories': ['#math', '#architecture', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Pantograph: мост между машинным обучением и автоматическим доказательством теорем', 'desc': 'Статья представляет инструмент Pantograph для автоматического доказательства теорем с помощью машинного обучения. Pantograph обеспечивает интерфейс к ассистенту доказательств Lean 4 и позволяет эффективно искать доказательства с помощью алгоритмов, таких как поиск методом Монте-Карло. Инструмент также поддерживает высокоуровневые рассуждения благодаря улучшенной обработке шагов вывода в Lean 4. Авторы описывают архитектуру Pantograph и демонстрируют его применение для доказательства теорем в Lean 4 с использованием моделей машинного обучения.'}, 'en': {'title': 'Pantograph: Elevating Theorem Proving with Machine Learning', 'desc': "The paper introduces Pantograph, a tool that integrates with the Lean 4 proof assistant to enhance theorem proving using machine learning. Pantograph employs advanced search algorithms like Monte Carlo Tree Search to efficiently explore proof possibilities. It also improves high-level reasoning by better managing Lean 4's inference steps, making the proof process more robust. The tool's architecture and features are designed to support the development of more sophisticated machine learning models for complex proof searches."}, 'zh': {'title': 'Pantograph：推动机器学习与定理证明的创新结合', 'desc': '这篇论文介绍了一个名为Pantograph的工具，它为Lean 4证明助手提供了一个多功能接口。Pantograph通过使用强大的搜索算法，如蒙特卡罗树搜索，实现了高效的证明搜索。该工具还增强了Lean 4推理步骤的处理能力，从而支持更高级别的推理。Pantograph的创新特性为未来的研究人员设计更强大和多功能的定理证明器铺平了道路。'}}}, {'id': 'https://huggingface.co/papers/2410.18647', 'title': 'Data Scaling Laws in Imitation Learning for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2410.18647', 'abstract': "Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment. To this end, we conduct a comprehensive empirical study on data scaling in imitation learning. By collecting data across numerous environments and objects, we study how a policy's generalization performance changes with the number of training environments, objects, and demonstrations. Throughout our research, we collect over 40,000 demonstrations and execute more than 15,000 real-world robot rollouts under a rigorous evaluation protocol. Our findings reveal several intriguing results: the generalization performance of the policy follows a roughly power-law relationship with the number of environments and objects. The diversity of environments and objects is far more important than the absolute number of demonstrations; once the number of demonstrations per environment or object reaches a certain threshold, additional demonstrations have minimal effect. Based on these insights, we propose an efficient data collection strategy. With four data collectors working for one afternoon, we collect sufficient data to enable the policies for two tasks to achieve approximately 90% success rates in novel environments with unseen objects.", 'score': 2, 'issue_id': 264, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'c54cdee62660d7f7', 'data': {'categories': ['#dataset', '#data', '#rl', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Масштабирование данных открывает путь к универсальным роботам-манипуляторам', 'desc': 'Исследователи изучают возможность применения принципов масштабирования данных в робототехнике, в частности, в манипуляциях роботов. Они провели масштабное эмпирическое исследование, собрав более 40 000 демонстраций и выполнив более 15 000 реальных запусков роботов. Результаты показали, что производительность обобщения политики следует степенному закону относительно количества сред и объектов, причем разнообразие важнее абсолютного числа демонстраций. На основе этих выводов была предложена эффективная стратегия сбора данных, позволяющая достичь 90% успеха в новых средах с незнакомыми объектами.'}, 'en': {'title': 'Scaling Data, Scaling Success: Revolutionizing Robotic Manipulation', 'desc': 'The paper explores the concept of data scaling in robotics, specifically focusing on robotic manipulation tasks. It examines whether scaling the amount of data can improve the generalization of robot policies, allowing them to perform tasks with new objects in different environments without additional training. Through extensive experiments, the study finds that the diversity of training environments and objects is more crucial than the sheer number of demonstrations. The research suggests an efficient data collection strategy that can significantly enhance policy performance with minimal effort.'}, 'zh': {'title': '数据扩展：机器人操作的泛化新路径', 'desc': '这篇论文研究了数据扩展在机器人领域，特别是机器人操作中的应用。研究发现，政策的泛化性能与环境和对象的数量呈现出大致的幂律关系。环境和对象的多样性比演示的绝对数量更为重要，一旦达到一定数量，额外的演示效果不大。基于这些发现，作者提出了一种高效的数据收集策略。'}}}, {'id': 'https://huggingface.co/papers/2410.18234', 'title': 'Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits', 'url': 'https://huggingface.co/papers/2410.18234', 'abstract': 'We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models. At each step, a token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target model. Previous works have demonstrated that the optimal scheme (which maximizes the probability of accepting one of the input tokens) can be cast as a solution to a linear program. In this work we show that the optimal scheme can be decomposed into a two-step solution: in the first step an importance sampling (IS) type scheme is used to select one intermediate token; in the second step (single-draft) speculative sampling is applied to generate the output token. For the case of two identical draft models we further 1) establish a necessary and sufficient condition on the distributions of the target and draft models for the acceptance probability to equal one and 2) provide an explicit expression for the optimal acceptance probability. Our theoretical analysis also motives a new class of token-level selection scheme based on weighted importance sampling. Our experimental results demonstrate consistent improvements in the achievable block efficiency and token rates over baseline schemes in a number of scenarios.', 'score': 2, 'issue_id': 258, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'e421e5808ba5b485', 'data': {'categories': ['#math', '#optimization'], 'emoji': '🎲', 'ru': {'title': 'Оптимизация выбора токенов при мультимодельном сэмплировании', 'desc': 'Статья рассматривает мультимодельное спекулятивное сэмплирование в контексте языковых моделей. Авторы предлагают оптимальную двухэтапную схему выбора токенов, использующую важностную выборку. Для случая двух идентичных черновых моделей установлены условия для 100% вероятности принятия и получено выражение для оптимальной вероятности. Экспериментальные результаты показывают улучшение эффективности блоков и скорости генерации токенов по сравнению с базовыми методами.'}, 'en': {'title': 'Optimizing Token Selection: A Two-Step Approach to Speculative Sampling', 'desc': 'The paper explores multi-draft speculative sampling, where different draft models independently generate proposal sequences. It introduces a two-step process for optimal token selection: first, an importance sampling scheme selects an intermediate token, followed by speculative sampling to produce the final output token. The authors establish conditions for achieving maximum acceptance probability with two identical draft models and propose a new token selection scheme using weighted importance sampling. Experimental results show that this approach improves block efficiency and token rates compared to traditional methods.'}, 'zh': {'title': '多草稿推测采样：提高效率的新方法', 'desc': '这篇论文研究了多草稿推测采样方法，其中提议序列是从不同的草稿模型中独立采样的。在每一步中，使用一个令牌级别的草稿选择方案来匹配目标模型的分布。作者提出了一种两步解决方案：首先使用重要性采样选择一个中间令牌，然后应用单草稿推测采样生成输出令牌。实验结果表明，这种方法在多个场景中提高了块效率和令牌速率。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents', '#agi', '#alignment (3)', '#architecture (6)', '#audio', '#benchmark (8)', '#cv (6)', '#data (6)', '#dataset (6)', '#diffusion (2)', '#edge_computing', '#ethics', '#games (1)', '#graphs', '#hallucinations (3)', '#inference (3)', '#interpretability (5)', '#long_context', '#math (4)', '#medicine', '#multilingual (1)', '#multimodal (7)', '#optimization (4)', '#plp (1)', '#quantum', '#rag', '#reasoning (2)', '#rl (4)', '#rlhf (2)', '#robotics (1)', '#security', '#story_generation', '#survey', '#training', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            //if (filteredArticles.length === 0) {
            //    selectedArticles = articlesData;
            //    selectedCategories = [];
            //    cleanCategorySelection();
            //} else {
            //    selectedArticles = filteredArticles;
            //}

            selectedArticles = filteredArticles;

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-10-27 16:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];       
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink() {
            if (isToday('2024-10-27 16:13')) {
                const element = document.getElementById('nav-next');
                if (element) {    
                    element.style.display = 'none';
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink(); 
        initializeLanguageFlags();
        updateLocalization();
    </script>
</body>
</html>
    