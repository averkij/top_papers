{
    "date": {
        "ru": "14 августа",
        "en": "August 14",
        "zh": "8月14日"
    },
    "time_utc": "2025-08-14 11:11",
    "weekday": 3,
    "issue_id": 5349,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.07901",
            "title": "Stand-In: A Lightweight and Plug-and-Play Identity Control for Video\n  Generation",
            "url": "https://huggingface.co/papers/2508.07901",
            "abstract": "A lightweight framework for identity preservation in video generation using conditional image branches and restricted self-attentions outperforms full-parameter methods with minimal additional parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating high-fidelity human videos that match user-specified identities is important yet challenging in the field of generative AI. Existing methods often rely on an excessive number of training parameters and lack compatibility with other AIGC tools. In this paper, we propose Stand-In, a lightweight and plug-and-play framework for identity preservation in video generation. Specifically, we introduce a conditional image branch into the pre-trained video generation model. Identity control is achieved through restricted self-attentions with conditional position mapping, and can be learned quickly with only 2000 pairs. Despite incorporating and training just sim1\\% additional parameters, our framework achieves excellent results in video quality and identity preservation, outperforming other full-parameter training methods. Moreover, our framework can be seamlessly integrated for other tasks, such as subject-driven video generation, pose-referenced video generation, stylization, and face swapping.",
            "score": 22,
            "issue_id": 5346,
            "pub_date": "2025-08-11",
            "pub_date_card": {
                "ru": "11 августа",
                "en": "August 11",
                "zh": "8月11日"
            },
            "hash": "1bba46c9fd359fad",
            "authors": [
                "Bowen Xue",
                "Qixin Yan",
                "Wenjing Wang",
                "Hao Liu",
                "Chen Li"
            ],
            "affiliations": [
                "WeChat Vision, Tencent Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.07901.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Эффективное сохранение идентичности в генерации видео",
                    "desc": "Статья представляет Stand-In - легковесный фреймворк для сохранения идентичности в генерации видео. Он использует условную ветвь изображений и ограниченное самовнимание в предобученной модели генерации видео. Несмотря на добавление всего около 1% дополнительных параметров, фреймворк превосходит методы с полным обучением параметров по качеству видео и сохранению идентичности. Stand-In также может быть легко интегрирован для других задач, таких как генерация видео по субъекту или позе."
                },
                "en": {
                    "title": "Efficient Identity Preservation in Video Generation",
                    "desc": "This paper presents a new framework called Stand-In for generating videos that maintain specific identities. It uses a lightweight approach by adding a conditional image branch to existing video generation models, which allows for effective identity control. The method employs restricted self-attention mechanisms with conditional position mapping, requiring only a small dataset of 2000 pairs for training. Stand-In achieves high video quality and identity preservation while using significantly fewer parameters than traditional methods, making it adaptable for various applications in generative AI."
                },
                "zh": {
                    "title": "轻量级框架，实现视频生成中的身份保留",
                    "desc": "本文提出了一种轻量级框架Stand-In，用于视频生成中的身份保留。该框架通过引入条件图像分支和限制自注意力机制，实现了对用户指定身份的控制。与传统方法相比，Stand-In仅需额外2000对样本和1%的参数，便能在视频质量和身份保留方面取得优异效果。该框架还具有良好的兼容性，可以与其他生成式AI工具无缝集成，适用于多种任务。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.08401",
            "title": "Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery",
            "url": "https://huggingface.co/papers/2508.08401",
            "abstract": "Mol-R1 framework enhances molecule discovery by improving reasoning performance and explainability through PRID and MoIA strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs), especially Explicit Long Chain-of-Thought (CoT) reasoning models like DeepSeek-R1 and QWQ, have demonstrated powerful reasoning capabilities, achieving impressive performance in commonsense reasoning and mathematical inference. Despite their effectiveness, Long-CoT reasoning models are often criticized for their limited ability and low efficiency in knowledge-intensive domains such as molecule discovery. Success in this field requires a precise understanding of domain knowledge, including molecular structures and chemical principles, which is challenging due to the inherent complexity of molecular data and the scarcity of high-quality expert annotations. To bridge this gap, we introduce Mol-R1, a novel framework designed to improve explainability and reasoning performance of R1-like Explicit Long-CoT reasoning LLMs in text-based molecule generation. Our approach begins with a high-quality reasoning dataset curated through Prior Regulation via In-context Distillation (PRID), a dedicated distillation strategy to effectively generate paired reasoning traces guided by prior regulations. Building upon this, we introduce MoIA, Molecular Iterative Adaptation, a sophisticated training strategy that iteratively combines Supervised Fine-tuning (SFT) with Reinforced Policy Optimization (RPO), tailored to boost the reasoning performance of R1-like reasoning models for molecule discovery. Finally, we examine the performance of Mol-R1 in the text-based molecule reasoning generation task, showing superior performance against existing baselines.",
            "score": 21,
            "issue_id": 5341,
            "pub_date": "2025-08-11",
            "pub_date_card": {
                "ru": "11 августа",
                "en": "August 11",
                "zh": "8月11日"
            },
            "hash": "5a63681c51881d66",
            "authors": [
                "Jiatong Li",
                "Weida Wang",
                "Qinggang Zhang",
                "Junxian Li",
                "Di Zhang",
                "Changmeng Zheng",
                "Shufei Zhang",
                "Xiaoyong Wei",
                "Qing Li"
            ],
            "affiliations": [
                "Fudan University",
                "Hong Kong Polytechnic University",
                "Shanghai AI Lab",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.08401.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#interpretability",
                    "#data",
                    "#rl",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "Mol-R1: Умное открытие молекул с помощью ИИ",
                    "desc": "Mol-R1 - это новая система для улучшения генерации молекул с помощью больших языковых моделей. Она использует стратегию PRID для создания качественного набора данных с рассуждениями, а также метод MoIA для итеративной адаптации модели. Mol-R1 сочетает обучение с учителем и обучение с подкреплением для повышения эффективности рассуждений. Эксперименты показали превосходство Mol-R1 над существующими базовыми моделями в задаче генерации молекул на основе текста."
                },
                "en": {
                    "title": "Revolutionizing Molecule Discovery with Enhanced Reasoning and Explainability",
                    "desc": "The Mol-R1 framework enhances the process of discovering new molecules by improving the reasoning abilities and explainability of large language models (LLMs). It utilizes a method called Prior Regulation via In-context Distillation (PRID) to create a high-quality dataset that helps the model learn effective reasoning strategies. Additionally, it incorporates Molecular Iterative Adaptation (MoIA), which combines supervised fine-tuning with reinforced policy optimization to refine the model's performance in generating molecular data. The results demonstrate that Mol-R1 outperforms existing models in text-based molecule reasoning tasks, making it a significant advancement in the field."
                },
                "zh": {
                    "title": "Mol-R1：提升分子发现的推理与可解释性",
                    "desc": "Mol-R1框架通过PRID和MoIA策略提升了分子发现的推理性能和可解释性。该框架针对长链思维推理模型的局限性，特别是在知识密集型领域如分子发现中的应用。我们首先通过先前调节和上下文蒸馏（PRID）构建高质量的推理数据集，然后采用分子迭代适应（MoIA）策略，结合监督微调和强化策略优化，提升推理能力。最终，Mol-R1在文本基础的分子推理生成任务中表现优于现有基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09889",
            "title": "AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust\n  GAIA Problem Solving",
            "url": "https://huggingface.co/papers/2508.09889",
            "abstract": "A dynamic Multi-Agent System (MAS) with Execution and Guard Agents improves the reliability and effectiveness of intelligent agents using external tools, outperforming single-agent systems on the GAIA leaderboard.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of large language models (LLMs) has empowered intelligent agents to leverage diverse external tools for solving complex real-world problems. However, as agents increasingly depend on multiple tools, they encounter new challenges: extended contexts from disparate sources and noisy or irrelevant tool outputs can undermine system reliability and accuracy. These challenges underscore the necessity for enhanced stability in agent-based systems. To address this, we introduce dynamic supervision and maneuvering mechanisms, constructing a robust and dynamic Multi-Agent System (MAS) architecture within the AWorld framework. In our approach, the Execution Agent invokes the Guard Agent at critical steps to verify and correct the reasoning process, effectively reducing errors arising from noise and bolstering problem-solving robustness. Extensive experiments on the GAIA test dataset reveal that our dynamic maneuvering mechanism significantly improves both the effectiveness and stability of solutions, outperforming single-agent system (SAS) and standard tool-augmented systems. As a result, our dynamic MAS system achieved first place among open-source projects on the prestigious GAIA leaderboard. These findings highlight the practical value of collaborative agent roles in developing more reliable and trustworthy intelligent systems.",
            "score": 19,
            "issue_id": 5340,
            "pub_date": "2025-08-13",
            "pub_date_card": {
                "ru": "13 августа",
                "en": "August 13",
                "zh": "8月13日"
            },
            "hash": "2c35df2a43900320",
            "authors": [
                "Zhitian Xie",
                "Qintong Wu",
                "Chengyue Yu",
                "Chenyi Zhuang",
                "Jinjie Gu"
            ],
            "affiliations": [
                "AWorld Team, Inclusion AI",
                "antgroup.com"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09889.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#open_source",
                    "#agi",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Мультиагентная система: надежность и эффективность в использовании ИИ-инструментов",
                    "desc": "Статья представляет динамическую мультиагентную систему (МАС) с агентами исполнения и контроля для повышения надежности и эффективности интеллектуальных агентов, использующих внешние инструменты. Система решает проблемы, связанные с расширенными контекстами из разных источников и шумными выходными данными инструментов. МАС превосходит одноагентные системы в рейтинге GAIA. Эксперименты показывают, что динамический механизм маневрирования значительно улучшает эффективность и стабильность решений."
                },
                "en": {
                    "title": "Dynamic Multi-Agent Systems: Enhancing Reliability through Collaboration",
                    "desc": "This paper presents a dynamic Multi-Agent System (MAS) that enhances the reliability and effectiveness of intelligent agents by utilizing Execution and Guard Agents. The system addresses challenges faced by agents when using multiple external tools, such as managing extended contexts and filtering out noisy outputs. By implementing dynamic supervision, the Execution Agent collaborates with the Guard Agent to verify and correct reasoning processes, which reduces errors and improves problem-solving capabilities. Experimental results demonstrate that this MAS architecture significantly outperforms single-agent systems on the GAIA leaderboard, showcasing the benefits of collaborative agent roles in creating more robust intelligent systems."
                },
                "zh": {
                    "title": "动态多智能体系统：提升智能体的可靠性与有效性",
                    "desc": "本文介绍了一种动态多智能体系统（MAS），通过执行代理和守护代理的协作，提高了智能体的可靠性和有效性。随着智能体使用多种外部工具解决复杂问题，系统面临新的挑战，如来自不同来源的上下文延长和工具输出的噪声。为了解决这些问题，本文提出了动态监督和操控机制，构建了一个稳健的MAS架构。实验结果表明，该系统在GAIA排行榜上表现优异，超越了单智能体系统和标准工具增强系统，展示了协作智能体角色在提升智能系统可靠性方面的实际价值。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09192",
            "title": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion\n  Forcing",
            "url": "https://huggingface.co/papers/2508.09192",
            "abstract": "Discrete diffusion forcing enhances diffusion large language models with block-wise autoregressive generation and inter-block parallel decoding, significantly improving inference speed while maintaining quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs for text generation, with the potential to decode multiple tokens in a single iteration. However, none of the existing open-source dLLMs have achieved superior inference speed over AR LLMs of similar size. This paper breaks this barrier based on a simple and effective strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key capabilities: (1) block-wise autoregressive generation to enable KV cache utilization; (2) prediction of following tokens without requiring completion of prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs are refurbished into an AR-diffusion hybrid paradigm for efficient inference. D2F can be implemented with an asymmetric distillation process based on pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm, which enables a trade-off between efficiency and efficacy. Empirically, D2F dLLMs achieve more than 2.5times inference speed than LLaMA3 and Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the acceleration can be more than 50times while maintaining comparable output quality. The code is available at https://github.com/zhijie-group/Discrete-Diffusion-Forcing.",
            "score": 17,
            "issue_id": 5342,
            "pub_date": "2025-08-08",
            "pub_date_card": {
                "ru": "8 августа",
                "en": "August 8",
                "zh": "8月8日"
            },
            "hash": "c7306295ccd097e9",
            "authors": [
                "Xu Wang",
                "Chenkai Xu",
                "Yijie Jin",
                "Jiachun Jin",
                "Hao Zhang",
                "Zhijie Deng"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Shanghai University",
                "University of California San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09192.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#diffusion",
                    "#inference",
                    "#architecture"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "D2F: Ускорение диффузионных языковых моделей без потери качества",
                    "desc": "Статья представляет новый метод под названием 'discrete diffusion forcing' (D2F) для улучшения диффузионных языковых моделей (dLLMs). D2F позволяет dLLMs использовать блочную авторегрессивную генерацию и параллельное декодирование между блоками. Это значительно увеличивает скорость вывода, сохраняя качество генерации текста. Эмпирические результаты показывают, что D2F dLLMs работают в 2.5 раза быстрее, чем LLaMA3 и Qwen2.5 на датасете GSM8K."
                },
                "en": {
                    "title": "Boosting Inference Speed in Language Models with Discrete Diffusion Forcing",
                    "desc": "This paper introduces a novel approach called discrete diffusion forcing (D2F) to enhance the performance of diffusion large language models (dLLMs) in text generation. D2F allows these models to generate text in blocks, utilizing key-value (KV) caching for improved efficiency, and enables parallel decoding of tokens across different blocks. This hybrid method combines the strengths of autoregressive (AR) models with diffusion techniques, resulting in significant speed improvements during inference without sacrificing output quality. The proposed pipelined parallel decoding algorithm further optimizes the trade-off between processing speed and the effectiveness of the generated text."
                },
                "zh": {
                    "title": "离散扩散强迫：提升语言模型推理速度的创新策略",
                    "desc": "本文提出了一种名为离散扩散强迫（D2F）的策略，旨在提高扩散大型语言模型（dLLMs）的推理速度，同时保持生成质量。D2F通过块级自回归生成和跨块并行解码，使得模型能够在单次迭代中解码多个标记。实验结果表明，D2F dLLMs在GSM8K数据集上的推理速度比LLaMA3和Qwen2.5快超过2.5倍，且与传统的dLLMs相比，速度提升可达50倍。该方法通过不对称蒸馏过程实现，提供了效率与效果之间的平衡。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09987",
            "title": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved\n  Image Generation",
            "url": "https://huggingface.co/papers/2508.09987",
            "abstract": "Echo-4o-Image, a synthetic dataset generated by GPT-4o, enhances image generation models by addressing rare scenarios and providing clean supervision, leading to improved performance and transferability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, GPT-4o has garnered significant attention for its strong performance in image generation, yet open-source models still lag behind. Several studies have explored distilling image data from GPT-4o to enhance open-source models, achieving notable progress. However, a key question remains: given that real-world image datasets already constitute a natural source of high-quality data, why should we use GPT-4o-generated synthetic data? In this work, we identify two key advantages of synthetic images. First, they can complement rare scenarios in real-world datasets, such as surreal fantasy or multi-reference image generation, which frequently occur in user queries. Second, they provide clean and controllable supervision. Real-world data often contains complex background noise and inherent misalignment between text descriptions and image content, whereas synthetic images offer pure backgrounds and long-tailed supervision signals, facilitating more accurate text-to-image alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Using this dataset, we fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o. In addition, we propose two new evaluation benchmarks for a more accurate and challenging assessment of image generation capabilities: GenEval++, which increases instruction complexity to mitigate score saturation, and Imagine-Bench, which focuses on evaluating both the understanding and generation of imaginative content. Echo-4o demonstrates strong performance across standard benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the datasets strong transferability.",
            "score": 14,
            "issue_id": 5342,
            "pub_date": "2025-08-13",
            "pub_date_card": {
                "ru": "13 августа",
                "en": "August 13",
                "zh": "8月13日"
            },
            "hash": "2b98a30444833d90",
            "authors": [
                "Junyan Ye",
                "Dongzhi Jiang",
                "Zihao Wang",
                "Leqi Zhu",
                "Zhenghao Hu",
                "Zilong Huang",
                "Jun He",
                "Zhiyuan Yan",
                "Jinghua Yu",
                "Hongsheng Li",
                "Conghui He",
                "Weijia Li"
            ],
            "affiliations": [
                "CUHK MMLab",
                "Peking University",
                "Shanghai Artificial Intelligence Laboratory",
                "Sun Yat-sen University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09987.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#transfer_learning",
                    "#cv",
                    "#dataset",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Синтетические данные - ключ к улучшению генерации изображений",
                    "desc": "Исследователи представили Echo-4o-Image - синтетический датасет, созданный с помощью GPT-4o для улучшения моделей генерации изображений. Этот датасет помогает решить проблему редких сценариев и предоставляет чистую разметку, что приводит к повышению производительности и переносимости моделей. Авторы использовали Echo-4o-Image для дообучения мультимодальной модели Bagel и создали модель Echo-4o, показавшую сильные результаты на стандартных бенчмарках. Кроме того, применение Echo-4o-Image к другим базовым моделям (например, OmniGen2, BLIP3-o) привело к стабильному улучшению показателей по нескольким метрикам."
                },
                "en": {
                    "title": "Enhancing Image Generation with Synthetic Data",
                    "desc": "The paper introduces Echo-4o-Image, a synthetic dataset created by GPT-4o to improve image generation models. It addresses the limitations of real-world datasets by providing clean supervision and covering rare scenarios that are often underrepresented. The dataset enhances the performance and transferability of various models, including Bagel, OmniGen2, and BLIP3-o. Additionally, the authors propose new evaluation benchmarks to better assess the capabilities of image generation systems."
                },
                "zh": {
                    "title": "合成数据集提升图像生成能力",
                    "desc": "本文介绍了Echo-4o-Image，这是一个由GPT-4o生成的合成数据集，旨在提升图像生成模型的性能。该数据集通过补充真实世界数据集中稀有场景，提供了干净且可控的监督信号，从而改善了文本与图像的对齐。研究表明，合成图像在处理复杂背景噪声和文本描述与图像内容不一致方面具有优势。通过使用Echo-4o-Image，研究人员在多个基础模型上实现了一致的性能提升，展示了该数据集的强大迁移能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09736",
            "title": "Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with\n  Long-Term Memory",
            "url": "https://huggingface.co/papers/2508.09736",
            "abstract": "M3-Agent, a multimodal agent with long-term memory, performs multi-turn reasoning and outperforms baselines on a new long-video question answering benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce M3-Agent, a novel multimodal agent framework equipped with long-term memory. Like humans, M3-Agent can process real-time visual and auditory inputs to build and update its long-term memory. Beyond episodic memory, it also develops semantic memory, enabling it to accumulate world knowledge over time. Its memory is organized in an entity-centric, multimodal format, allowing deeper and more consistent understanding of the environment. Given an instruction, M3-Agent autonomously performs multi-turn, iterative reasoning and retrieves relevant information from memory to accomplish the task. To evaluate memory effectiveness and memory-based reasoning in multimodal agents, we develop M3-Bench, a new long-video question answering benchmark. M3-Bench comprises 100 newly recorded real-world videos captured from a robot's perspective (M3-Bench-robot) and 929 web-sourced videos across diverse scenarios (M3-Bench-web). We annotate question-answer pairs designed to test key capabilities essential for agent applications, such as human understanding, general knowledge extraction, and cross-modal reasoning. Experimental results show that M3-Agent, trained via reinforcement learning, outperforms the strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o, achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web and VideoMME-long, respectively. Our work advances the multimodal agents toward more human-like long-term memory and provides insights into their practical design. Model, code and data are available at https://github.com/bytedance-seed/m3-agent",
            "score": 11,
            "issue_id": 5345,
            "pub_date": "2025-08-13",
            "pub_date_card": {
                "ru": "13 августа",
                "en": "August 13",
                "zh": "8月13日"
            },
            "hash": "8883a7582bf874bc",
            "authors": [
                "Lin Long",
                "Yichen He",
                "Wentao Ye",
                "Yiyuan Pan",
                "Yuan Lin",
                "Hang Li",
                "Junbo Zhao",
                "Wei Li"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Shanghai Jiao Tong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09736.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#agents",
                    "#reasoning",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "M3-Agent: мультимодальный ИИ-агент с человекоподобной долговременной памятью",
                    "desc": "M3-Agent - это новая мультимодальная агентная система с долговременной памятью, способная обрабатывать визуальные и аудио входные данные в реальном времени. Система формирует как эпизодическую, так и семантическую память, что позволяет ей накапливать знания об окружающем мире. M3-Agent использует многоэтапные рассуждения и извлечение релевантной информации из памяти для выполнения задач. Для оценки эффективности агента авторы разработали новый бенчмарк M3-Bench для ответов на вопросы по длинным видео."
                },
                "en": {
                    "title": "M3-Agent: Advancing Multimodal Reasoning with Human-like Memory",
                    "desc": "M3-Agent is a cutting-edge multimodal agent that integrates long-term memory to enhance its reasoning capabilities. It processes visual and auditory information in real-time, allowing it to build and update its memory similar to human cognition. The agent features both episodic and semantic memory, which helps it accumulate knowledge and understand its environment more deeply. Evaluated on the M3-Bench benchmark, M3-Agent demonstrates superior performance in multi-turn reasoning tasks compared to existing models, showcasing its potential for real-world applications."
                },
                "zh": {
                    "title": "M3-Agent：具备人类般长期记忆的多模态智能体",
                    "desc": "M3-Agent是一种新型的多模态智能体框架，具备长期记忆能力。它能够实时处理视觉和听觉输入，像人类一样构建和更新记忆。M3-Agent不仅拥有情节记忆，还发展了语义记忆，使其能够随着时间积累世界知识。通过自主进行多轮推理，M3-Agent能够从记忆中检索相关信息，完成任务，并在新的长视频问答基准上超越了现有的基线模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.07750",
            "title": "Learning to Align, Aligning to Learn: A Unified Approach for\n  Self-Optimized Alignment",
            "url": "https://huggingface.co/papers/2508.07750",
            "abstract": "GRAO, a unified framework combining SFT and RL, enhances language model alignment through multi-sample generation, Group Direct Alignment Loss, and reference-aware parameter updates, demonstrating superior performance across human alignment tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Alignment methodologies have emerged as a critical pathway for enhancing language model alignment capabilities. While SFT (supervised fine-tuning) accelerates convergence through direct token-level loss intervention, its efficacy is constrained by offline policy trajectory. In contrast, RL(reinforcement learning) facilitates exploratory policy optimization, but suffers from low sample efficiency and stringent dependency on high-quality base models. To address these dual challenges, we propose GRAO (Group Relative Alignment Optimization), a unified framework that synergizes the respective strengths of SFT and RL through three key innovations: 1) A multi-sample generation strategy enabling comparative quality assessment via reward feedback; 2) A novel Group Direct Alignment Loss formulation leveraging intra-group relative advantage weighting; 3) Reference-aware parameter updates guided by pairwise preference dynamics. Our theoretical analysis establishes GRAO's convergence guarantees and sample efficiency advantages over conventional approaches. Comprehensive evaluations across complex human alignment tasks demonstrate GRAO's superior performance, achieving 57.70\\%,17.65\\% 7.95\\% and 5.18\\% relative improvements over SFT, DPO, PPO and GRPO baselines respectively. This work provides both a theoretically grounded alignment framework and empirical evidence for efficient capability evolution in language models.",
            "score": 11,
            "issue_id": 5347,
            "pub_date": "2025-08-11",
            "pub_date_card": {
                "ru": "11 августа",
                "en": "August 11",
                "zh": "8月11日"
            },
            "hash": "adefb08b02280156",
            "authors": [
                "Haowen Wang",
                "Yun Yue",
                "Zhiling Ye",
                "Shuowen Zhang",
                "Lei Fan",
                "Jiaxin Liang",
                "Jiadi Jiang",
                "Cheng Wei",
                "Jingyuan Deng",
                "Xudong Han",
                "Ji Li",
                "Chunxiao Guo",
                "Peng Wei",
                "Jian Wang",
                "Jinjie Gu"
            ],
            "affiliations": [
                "Intelligence Healthcare Department, AntGroup Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.07750.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "GRAO: Объединение SFT и RL для эффективного выравнивания языковых моделей",
                    "desc": "GRAO - это новый фреймворк для улучшения выравнивания языковых моделей, объединяющий преимущества SFT и RL. Он использует генерацию нескольких образцов, групповую функцию потерь и обновление параметров с учетом эталонных образцов. GRAO демонстрирует превосходную производительность в задачах выравнивания по сравнению с базовыми методами. Теоретический анализ подтверждает гарантии сходимости и эффективность GRAO."
                },
                "en": {
                    "title": "GRAO: Uniting SFT and RL for Superior Language Model Alignment",
                    "desc": "The paper introduces GRAO, a new framework that combines supervised fine-tuning (SFT) and reinforcement learning (RL) to improve the alignment of language models. GRAO addresses the limitations of SFT's offline policy and RL's sample inefficiency by using multi-sample generation for better quality assessment and a novel Group Direct Alignment Loss for improved training. Additionally, it incorporates reference-aware parameter updates to enhance learning from pairwise preferences. The results show that GRAO significantly outperforms existing methods in various human alignment tasks, demonstrating its effectiveness and efficiency in evolving language model capabilities."
                },
                "zh": {
                    "title": "GRAO：提升语言模型对齐的新框架",
                    "desc": "GRAO是一种统一框架，结合了监督微调（SFT）和强化学习（RL），通过多样本生成、群体直接对齐损失和参考感知参数更新来增强语言模型的对齐能力。该方法通过三项创新，解决了SFT和RL各自的局限性，提升了样本效率和收敛性。理论分析表明，GRAO在收敛性和样本效率上优于传统方法。综合评估结果显示，GRAO在复杂的人类对齐任务中表现优异，相较于其他基线方法有显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.06009",
            "title": "MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math\n  Reasoning in Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2508.06009",
            "abstract": "MathReal, a dataset of real-world mathematical questions with images, evaluates the performance of multimodal large language models in authentic educational settings, highlighting challenges and providing insights for improvement.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in visual mathematical reasoning across various existing benchmarks. However, these benchmarks are predominantly based on clean or processed multimodal inputs, without incorporating the images provided by real-world Kindergarten through 12th grade (K-12) educational users. To address this gap, we introduce MathReal, a meticulously curated dataset comprising 2,000 mathematical questions with images captured by handheld mobile devices in authentic scenarios. Each question is an image, containing the question text and visual element. We systematically classify the real images into three primary categories: image quality degradation, perspective variation, and irrelevant content interference, which are further delineated into 14 subcategories. Additionally, MathReal spans five core knowledge and ability categories, which encompass three question types and are divided into three difficulty levels. To comprehensively evaluate the multimodal mathematical reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we design six experimental settings that enable a systematic analysis of their performance. Through extensive experimentation, we find that the problem-solving abilities of existing MLLMs are significantly challenged in realistic educational contexts. Based on this, we conduct a thorough analysis of their performance and error patterns, providing insights into their recognition, comprehension, and reasoning capabilities, and outlining directions for future improvements. Data and code: https://github.com/junfeng0288/MathReal.",
            "score": 10,
            "issue_id": 5341,
            "pub_date": "2025-08-08",
            "pub_date_card": {
                "ru": "8 августа",
                "en": "August 8",
                "zh": "8月8日"
            },
            "hash": "baeb9688cb58aad4",
            "authors": [
                "Jun Feng",
                "Zixin Wang",
                "Zhentao Zhang",
                "Yue Guo",
                "Zhihan Zhou",
                "Xiuyi Chen",
                "Zhenyang Li",
                "Dawei Yin"
            ],
            "affiliations": [
                "Baidu Inc., Beijing, China",
                "Beihang University, Beijing, China",
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "Nanyang Technological University, Singapore",
                "Xiaopeng Motors, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.06009.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#interpretability",
                    "#multimodal",
                    "#science",
                    "#dataset"
                ],
                "emoji": "📚",
                "ru": {
                    "title": "MathReal: реальный тест для ИИ в математическом образовании",
                    "desc": "Статья представляет MathReal - набор данных из 2000 математических задач с реальными изображениями для оценки мультимодальных больших языковых моделей (MLLM) в образовательном контексте. Изображения классифицированы по качеству, ракурсу и наличию посторонних элементов. Эксперименты показали, что существующие MLLM испытывают значительные трудности при решении задач в реалистичных условиях. Анализ результатов выявил проблемы в распознавании, понимании и рассуждении моделей, предоставляя направления для дальнейшего улучшения."
                },
                "en": {
                    "title": "Bridging the Gap: Real-World Math for MLLMs",
                    "desc": "The paper introduces MathReal, a new dataset designed to evaluate multimodal large language models (MLLMs) using real-world mathematical questions accompanied by images. Unlike previous benchmarks that used clean inputs, MathReal includes 2,000 questions sourced from actual K-12 educational settings, highlighting the challenges faced by MLLMs in processing real-world data. The dataset categorizes images based on quality issues, perspective variations, and irrelevant content, while also covering various knowledge categories and question types. Through rigorous testing, the study reveals significant performance gaps in MLLMs when applied to authentic educational scenarios, offering insights for future enhancements in their reasoning capabilities."
                },
                "zh": {
                    "title": "真实教育环境中的数学推理挑战",
                    "desc": "MathReal是一个包含真实世界数学问题和图像的数据集，旨在评估多模态大型语言模型在真实教育环境中的表现。该数据集包含2000个由手持移动设备拍摄的数学问题，涵盖了图像质量下降、视角变化和无关内容干扰等三大类问题。通过六种实验设置，我们系统地分析了现有多模态大型语言模型在真实教育场景中的数学推理能力。实验结果表明，现有模型在真实教育环境中面临显著挑战，并为未来的改进提供了见解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09983",
            "title": "Story2Board: A Training-Free Approach for Expressive Storyboard\n  Generation",
            "url": "https://huggingface.co/papers/2508.09983",
            "abstract": "Story2Board generates expressive storyboards from natural language using a consistency framework that enhances coherence and diversity without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Story2Board, a training-free framework for expressive storyboard generation from natural language. Existing methods narrowly focus on subject identity, overlooking key aspects of visual storytelling such as spatial composition, background evolution, and narrative pacing. To address this, we introduce a lightweight consistency framework composed of two components: Latent Panel Anchoring, which preserves a shared character reference across panels, and Reciprocal Attention Value Mixing, which softly blends visual features between token pairs with strong reciprocal attention. Together, these mechanisms enhance coherence without architectural changes or fine-tuning, enabling state-of-the-art diffusion models to generate visually diverse yet consistent storyboards. To structure generation, we use an off-the-shelf language model to convert free-form stories into grounded panel-level prompts. To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain narratives designed to assess layout diversity and background-grounded storytelling, in addition to consistency. We also introduce a new Scene Diversity metric that quantifies spatial and pose variation across storyboards. Our qualitative and quantitative results, as well as a user study, show that Story2Board produces more dynamic, coherent, and narratively engaging storyboards than existing baselines.",
            "score": 9,
            "issue_id": 5344,
            "pub_date": "2025-08-13",
            "pub_date_card": {
                "ru": "13 августа",
                "en": "August 13",
                "zh": "8月13日"
            },
            "hash": "8d5ac9177324ef60",
            "authors": [
                "David Dinkevich",
                "Matan Levy",
                "Omri Avrahami",
                "Dvir Samuel",
                "Dani Lischinski"
            ],
            "affiliations": [
                "Bar-Ilan University, Israel",
                "Hebrew University of Jerusalem, Israel",
                "OriginAI, Israel"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09983.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#story_generation",
                    "#benchmark",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Создание выразительных раскадровок с помощью ИИ без дополнительного обучения",
                    "desc": "Статья представляет Story2Board - фреймворк для генерации выразительных раскадровок из естественного языка без дополнительного обучения. Он использует механизмы сохранения согласованности персонажей и смешивания визуальных признаков для улучшения когерентности и разнообразия генерируемых изображений. Авторы предлагают новый набор данных Rich Storyboard Benchmark и метрику Scene Diversity для оценки качества раскадровок. Результаты показывают, что Story2Board создает более динамичные и увлекательные раскадровки по сравнению с существующими методами."
                },
                "en": {
                    "title": "Expressive Storyboards from Natural Language, No Fine-Tuning Needed!",
                    "desc": "Story2Board is a novel framework that generates expressive storyboards from natural language descriptions without the need for fine-tuning. It addresses limitations in existing methods by focusing on important visual storytelling elements like spatial composition and narrative pacing. The framework employs Latent Panel Anchoring to maintain character consistency across panels and Reciprocal Attention Value Mixing to enhance visual feature blending. This approach allows for the creation of coherent and diverse storyboards, evaluated through the Rich Storyboard Benchmark and a new Scene Diversity metric, demonstrating superior performance compared to traditional methods."
                },
                "zh": {
                    "title": "从自然语言生成一致且多样的故事板",
                    "desc": "Story2Board是一个无需训练的框架，可以从自然语言生成富有表现力的故事板。现有的方法主要关注角色身份，忽视了视觉叙事中的重要方面，如空间构图、背景演变和叙事节奏。为了解决这个问题，我们引入了一个轻量级的一致性框架，包括潜在面板锚定和互惠注意力值混合两个组件，以增强故事板的一致性和多样性。我们的实验结果表明，Story2Board生成的故事板在动态性、一致性和叙事吸引力方面优于现有的基线。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.05613",
            "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning\n  for Large Language Models",
            "url": "https://huggingface.co/papers/2508.05613",
            "abstract": "A reinforcement learning framework jointly optimizes policy and reward models to enhance robustness and mitigate reward hacking in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable performance in reasoning tasks, where reinforcement learning (RL) serves as a key algorithm for enhancing their reasoning capabilities. Currently, there are two mainstream reward paradigms: model-based rewards and rule-based rewards. However, both approaches suffer from limitations: rule-based rewards lack robustness, while model-based rewards are vulnerable to reward hacking. To address these issues, we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework that jointly optimizes both the policy model and the reward model. Cooper leverages the high precision of rule-based rewards when identifying correct responses, and dynamically constructs and selects positive-negative sample pairs for continued training the reward model. This design enhances robustness and mitigates the risk of reward hacking. To further support Cooper, we introduce a hybrid annotation strategy that efficiently and accurately generates training data for the reward model. We also propose a reference-based reward modeling paradigm, where the reward model takes a reference answer as input. Based on this design, we train a reward model named VerifyRM, which achieves higher accuracy on VerifyBench compared to other models of the same size. We conduct reinforcement learning using both VerifyRM and Cooper. Our experiments show that Cooper not only alleviates reward hacking but also improves end-to-end RL performance, for instance, achieving a 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that dynamically updating reward model is an effective way to combat reward hacking, providing a reference for better integrating reward models into RL.",
            "score": 7,
            "issue_id": 5341,
            "pub_date": "2025-08-07",
            "pub_date_card": {
                "ru": "7 августа",
                "en": "August 7",
                "zh": "8月7日"
            },
            "hash": "1682d3768247c2b0",
            "authors": [
                "Haitao Hong",
                "Yuchen Yan",
                "Xingyu Wu",
                "Guiyang Hou",
                "Wenqi Zhang",
                "Weiming Lu",
                "Yongliang Shen",
                "Jun Xiao"
            ],
            "affiliations": [
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.05613.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rlhf",
                    "#rl",
                    "#optimization",
                    "#training"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Cooper: умное обучение с подкреплением для больших языковых моделей",
                    "desc": "В этой статье представлена новая система обучения с подкреплением под названием Cooper, которая совместно оптимизирует модель политики и модель вознаграждения для больших языковых моделей. Cooper использует преимущества как правил, так и моделей для создания более надежной и устойчивой к обману системы вознаграждений. Авторы также предлагают гибридную стратегию аннотации и модель вознаграждения на основе эталонных ответов VerifyRM. Эксперименты показывают, что Cooper снижает риск обмана системы вознаграждений и улучшает общую производительность обучения с подкреплением."
                },
                "en": {
                    "title": "Cooper: Enhancing Robustness in RL with Joint Policy and Reward Optimization",
                    "desc": "This paper presents a reinforcement learning framework called Cooper, which aims to improve the robustness of large language models (LLMs) while reducing the risk of reward hacking. It jointly optimizes both the policy model and the reward model, addressing the weaknesses of existing reward paradigms: rule-based rewards are not robust, and model-based rewards can be exploited. Cooper utilizes the strengths of rule-based rewards for accurate response identification and employs a hybrid annotation strategy to generate effective training data. The results show that this approach enhances overall performance and accuracy in reinforcement learning tasks, demonstrating a significant improvement in model reliability."
                },
                "zh": {
                    "title": "共同优化策略与奖励模型，提升鲁棒性与安全性",
                    "desc": "本文提出了一种强化学习框架Cooper，旨在共同优化策略模型和奖励模型，以增强大型语言模型的鲁棒性并减少奖励黑客行为。当前的奖励机制主要分为基于模型的奖励和基于规则的奖励，但这两种方法各有局限性。Cooper利用基于规则的奖励在识别正确响应时的高精度，并动态构建和选择正负样本对来持续训练奖励模型，从而提高鲁棒性。实验结果表明，Cooper有效缓解了奖励黑客问题，并提升了强化学习的整体性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09456",
            "title": "IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding",
            "url": "https://huggingface.co/papers/2508.09456",
            "abstract": "A novel input-aware backdoor attack method, IAG, manipulates vision-language models to ground specific objects in images regardless of user queries, using a text-conditional U-Net and reconstruction loss to ensure stealthiness.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) have shown significant advancements in tasks such as visual grounding, where they localize specific objects in images based on natural language queries and images. However, security issues in visual grounding tasks for VLMs remain underexplored, especially in the context of backdoor attacks. In this paper, we introduce a novel input-aware backdoor attack method, IAG, designed to manipulate the grounding behavior of VLMs. This attack forces the model to ground a specific target object in the input image, regardless of the user's query. We propose an adaptive trigger generator that embeds the semantic information of the attack target's description into the original image using a text-conditional U-Net, thereby overcoming the open-vocabulary attack challenge. To ensure the attack's stealthiness, we utilize a reconstruction loss to minimize visual discrepancies between poisoned and clean images. Additionally, we introduce a unified method for generating attack data. IAG is evaluated theoretically and empirically, demonstrating its feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches over 65\\% on various testing sets. IAG also shows promising potential on manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on clean samples. Extensive specific experiments, such as ablation study and potential defense, also indicate the robustness and transferability of our attack.",
            "score": 6,
            "issue_id": 5341,
            "pub_date": "2025-08-13",
            "pub_date_card": {
                "ru": "13 августа",
                "en": "August 13",
                "zh": "8月13日"
            },
            "hash": "adbc75fda80ba5f2",
            "authors": [
                "Junxian Li",
                "Beining Xu",
                "Di Zhang"
            ],
            "affiliations": [
                "Fudan University, Shanghai, China",
                "Shanghai Jiao Tong University, Shanghai, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09456.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Скрытая атака на модели компьютерного зрения с помощью адаптивного генератора триггеров",
                    "desc": "Статья представляет новый метод атаки на модели компьютерного зрения и обработки естественного языка под названием IAG. Этот метод заставляет модель выделять определенный целевой объект на изображении независимо от запроса пользователя. IAG использует условный U-Net для внедрения семантической информации о цели атаки в исходное изображение. Для обеспечения скрытности атаки применяется функция потерь реконструкции, минимизирующая визуальные различия между отравленными и чистыми изображениями."
                },
                "en": {
                    "title": "Stealthy Backdoor Attacks on Vision-Language Models with IAG",
                    "desc": "This paper presents IAG, a new method for executing backdoor attacks on vision-language models (VLMs) that manipulate how these models identify objects in images. IAG uses a text-conditional U-Net to embed attack target descriptions into images, allowing the model to ground specific objects regardless of the user's input. The method ensures stealthiness by applying a reconstruction loss to minimize differences between altered and original images. The effectiveness of IAG is demonstrated through various experiments, achieving high attack success rates while maintaining accuracy on clean samples."
                },
                "zh": {
                    "title": "输入感知后门攻击：操控视觉语言模型的隐秘力量",
                    "desc": "本文提出了一种新颖的输入感知后门攻击方法IAG，旨在操控视觉语言模型（VLMs）在图像中定位特定对象，而不受用户查询的影响。该方法使用文本条件的U-Net和重建损失，确保攻击的隐蔽性。IAG通过将攻击目标的语义信息嵌入到原始图像中，克服了开放词汇攻击的挑战。实验结果表明，IAG在多个测试集上的攻击成功率超过65%，并且在干净样本上的准确率几乎没有下降，显示出其有效性和鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09968",
            "title": "Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models",
            "url": "https://huggingface.co/papers/2508.09968",
            "abstract": "A Noise Hypernetwork is introduced to integrate test-time scaling knowledge into diffusion models, reducing computational cost while maintaining quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The new paradigm of test-time scaling has yielded remarkable breakthroughs in Large Language Models (LLMs) (e.g. reasoning models) and in generative vision models, allowing models to allocate additional computation during inference to effectively tackle increasingly complex problems. Despite the improvements of this approach, an important limitation emerges: the substantial increase in computation time makes the process slow and impractical for many applications. Given the success of this paradigm and its growing usage, we seek to preserve its benefits while eschewing the inference overhead. In this work we propose one solution to the critical problem of integrating test-time scaling knowledge into a model during post-training. Specifically, we replace reward guided test-time noise optimization in diffusion models with a Noise Hypernetwork that modulates initial input noise. We propose a theoretically grounded framework for learning this reward-tilted distribution for distilled generators, through a tractable noise-space objective that maintains fidelity to the base model while optimizing for desired characteristics. We show that our approach recovers a substantial portion of the quality gains from explicit test-time optimization at a fraction of the computational cost. Code is available at https://github.com/ExplainableML/HyperNoise",
            "score": 4,
            "issue_id": 5342,
            "pub_date": "2025-08-13",
            "pub_date_card": {
                "ru": "13 августа",
                "en": "August 13",
                "zh": "8月13日"
            },
            "hash": "3d06b8d8d29135c3",
            "authors": [
                "Luca Eyring",
                "Shyamgopal Karthik",
                "Alexey Dosovitskiy",
                "Nataniel Ruiz",
                "Zeynep Akata"
            ],
            "affiliations": [
                "Google",
                "Helmholtz Munich",
                "Inceptive",
                "Munich Center of Machine Learning",
                "Technical University of Munich",
                "University of Tübingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09968.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#training",
                    "#inference",
                    "#architecture"
                ],
                "emoji": "🔊",
                "ru": {
                    "title": "Эффективное масштабирование диффузионных моделей с помощью шумовой гиперсети",
                    "desc": "В статье представлена концепция Шумовой гиперсети (Noise Hypernetwork) для интеграции знаний масштабирования во время тестирования в диффузионные модели. Этот подход позволяет сократить вычислительные затраты при сохранении качества генерации. Авторы предлагают теоретически обоснованную структуру для обучения распределения шума, оптимизированного под желаемые характеристики. Результаты показывают, что метод позволяет достичь значительного улучшения качества по сравнению с явной оптимизацией во время тестирования при существенно меньших вычислительных затратах."
                },
                "en": {
                    "title": "Efficient Quality Enhancement in Diffusion Models with Noise Hypernetworks",
                    "desc": "This paper presents a Noise Hypernetwork that enhances diffusion models by incorporating test-time scaling knowledge, which helps reduce computational costs while preserving output quality. The authors address the challenge of increased computation time associated with test-time scaling, which can hinder practical applications. By replacing traditional noise optimization methods with a Noise Hypernetwork, they effectively modulate the initial input noise to improve performance. Their framework allows for learning a reward-tilted distribution that maintains the fidelity of the base model while optimizing for specific characteristics, achieving significant quality improvements with lower computational demands."
                },
                "zh": {
                    "title": "降低计算成本，提升模型质量的创新方案",
                    "desc": "本文提出了一种噪声超网络，用于将测试时缩放知识整合到扩散模型中，从而降低计算成本，同时保持模型质量。测试时缩放的新范式在大型语言模型和生成视觉模型中取得了显著突破，但其计算时间的显著增加使得许多应用变得缓慢且不切实际。我们通过用噪声超网络替代扩散模型中的奖励引导测试时噪声优化，来解决这一问题。我们的框架通过可处理的噪声空间目标，优化所需特性，同时保持对基础模型的忠实度，显著减少了计算成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09945",
            "title": "VisCodex: Unified Multimodal Code Generation via Merging Vision and\n  Coding Models",
            "url": "https://huggingface.co/papers/2508.09945",
            "abstract": "VisCodex integrates vision and coding models to enhance multimodal code generation, achieving top performance using a novel dataset and benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have significantly advanced the integration of visual and textual understanding. However, their ability to generate code from multimodal inputs remains limited. In this work, we introduce VisCodex, a unified framework that seamlessly merges vision and coding language models to empower MLLMs with strong multimodal code generation abilities. Leveraging a task vector-based model merging technique, we integrate a state-of-the-art coding LLM into a strong vision-language backbone, while preserving both visual comprehension and advanced coding skills. To support training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a large-scale and diverse collection of 598k samples, including high-quality HTML code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic problems. Furthermore, we propose InfiBench-V, a novel and challenging benchmark specifically designed to assess models on visually-rich, real-world programming questions that demand a nuanced understanding of both textual and visual contexts. Extensive experiments show that VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4o, highlighting the effectiveness of our model merging strategy and new datasets.",
            "score": 3,
            "issue_id": 5347,
            "pub_date": "2025-08-13",
            "pub_date_card": {
                "ru": "13 августа",
                "en": "August 13",
                "zh": "8月13日"
            },
            "hash": "dfb473d7795699b3",
            "authors": [
                "Lingjie Jiang",
                "Shaohan Huang",
                "Xun Wu",
                "Yixia Li",
                "Dongdong Zhang",
                "Furu Wei"
            ],
            "affiliations": [
                "Microsoft Research",
                "Peking University",
                "Southern University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09945.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#games",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🔮",
                "ru": {
                    "title": "VisCodex: слияние зрения и кода для прорыва в мультимодальной генерации программ",
                    "desc": "VisCodex - это инновационная система, объединяющая модели компьютерного зрения и генерации кода для улучшения мультимодальной генерации программного кода. Авторы представили новый набор данных Multimodal Coding Dataset (MCD) и бенчмарк InfiBench-V для оценки моделей на реальных задачах программирования с визуальным контекстом. VisCodex использует технику слияния моделей на основе векторов задач, интегрируя передовую языковую модель для кодирования в сильную основу для работы с изображениями и текстом. Эксперименты показывают, что VisCodex достигает наилучших результатов среди открытых мультимодальных языковых моделей и приближается к проприетарным решениям вроде GPT-4."
                },
                "en": {
                    "title": "Empowering Code Generation through Vision and Language Integration",
                    "desc": "VisCodex is a new framework that combines vision and coding models to improve the generation of code from visual and textual inputs. It uses a unique method to merge a powerful coding language model with a strong vision-language model, enhancing the model's ability to understand and generate code. To train and evaluate this framework, the authors created the Multimodal Coding Dataset (MCD), which includes a wide variety of coding examples and visual data. The results show that VisCodex performs exceptionally well, even rivaling advanced proprietary models, demonstrating the success of its innovative approach."
                },
                "zh": {
                    "title": "VisCodex：视觉与编码的完美结合",
                    "desc": "VisCodex 是一个将视觉和编码模型整合在一起的框架，旨在提升多模态代码生成的能力。通过任务向量模型合并技术，VisCodex 将先进的编码大语言模型与强大的视觉-语言基础模型结合，保持了视觉理解和编码技能。我们还引入了多模态编码数据集（MCD），包含598,000个样本，支持训练和评估。实验结果表明，VisCodex 在开源多模态大语言模型中表现出色，接近于像 GPT-4o 这样的专有模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.06944",
            "title": "AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal\n  Imitation-Exploration Balance",
            "url": "https://huggingface.co/papers/2508.06944",
            "abstract": "Adaptive Meta Fine-Tuning (AMFT) dynamically balances Supervised Fine-Tuning and Reinforcement Learning using implicit rewards to improve LLM performance and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are typically fine-tuned for reasoning tasks through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL), a process fraught with catastrophic forgetting and suboptimal trade-offs between imitation and exploration. Recent single-stage methods attempt to unify SFT and RL using heuristics, but lack a principled mechanism for dynamically balancing the two paradigms. In this paper, we reframe this challenge through the theoretical lens of implicit rewards, viewing SFT and RL not as distinct methods but as complementary reward signals. We introduce Adaptive Meta Fine-Tuning (AMFT), a novel single-stage algorithm that learns the optimal balance between SFT's implicit, path-level reward and RL's explicit, outcome-based reward. The core of AMFT is a meta-gradient adaptive weight controller that treats the SFT-RL balance as a learnable parameter, dynamically optimizing it to maximize long-term task performance. This forward-looking approach, regularized by policy entropy for stability, autonomously discovers an effective training curriculum. We conduct a comprehensive evaluation on challenging benchmarks spanning mathematical reasoning, abstract visual reasoning (General Points), and vision-language navigation (V-IRL). AMFT consistently establishes a new state-of-the-art and demonstrats superior generalization on out-of-distribution (OOD) tasks. Ablation studies and training dynamic analysis confirm that the meta-learning controller is crucial for AMFT's stability, sample efficiency, and performance, offering a more principled and effective paradigm for LLM alignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.",
            "score": 1,
            "issue_id": 5348,
            "pub_date": "2025-08-09",
            "pub_date_card": {
                "ru": "9 августа",
                "en": "August 9",
                "zh": "8月9日"
            },
            "hash": "3c5ce2e46bfb3d7b",
            "authors": [
                "Lixuan He",
                "Jie Feng",
                "Yong Li"
            ],
            "affiliations": [
                "Department of Electronic Engineering, Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.06944.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#agi",
                    "#rl",
                    "#benchmark",
                    "#reasoning",
                    "#multimodal",
                    "#optimization",
                    "#open_source",
                    "#training"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Балансирование обучения языковых моделей: новый подход к тонкой настройке",
                    "desc": "Статья представляет новый метод обучения больших языковых моделей - Адаптивную Мета-Тонкую Настройку (AMFT). AMFT динамически балансирует между контролируемой тонкой настройкой и обучением с подкреплением, используя неявные вознаграждения. Этот подход позволяет улучшить производительность и обобщающую способность языковых моделей. AMFT демонстрирует превосходные результаты на различных сложных задачах, включая математические рассуждения и навигацию с использованием зрения и языка."
                },
                "en": {
                    "title": "Balancing Fine-Tuning and Learning for Better AI Performance",
                    "desc": "The paper introduces Adaptive Meta Fine-Tuning (AMFT), a new approach that combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to enhance the performance of Large Language Models (LLMs). AMFT treats SFT and RL as complementary reward signals rather than separate methods, allowing for a dynamic balance between them. It employs a meta-gradient adaptive weight controller to optimize this balance, which helps in maximizing long-term task performance while maintaining stability. The results show that AMFT achieves state-of-the-art performance on various reasoning tasks and demonstrates improved generalization on out-of-distribution challenges."
                },
                "zh": {
                    "title": "自适应元微调：平衡学习与探索的创新方法",
                    "desc": "自适应元微调（AMFT）通过动态平衡监督微调（SFT）和强化学习（RL），利用隐式奖励来提升大型语言模型（LLM）的性能和泛化能力。该方法将SFT和RL视为互补的奖励信号，而非独立的方法，从而引入了一种新的单阶段算法。AMFT的核心是一个元梯度自适应权重控制器，它将SFT-RL的平衡视为可学习的参数，动态优化以最大化长期任务表现。通过在多个具有挑战性的基准上进行评估，AMFT在超出分布（OOD）任务上展现了卓越的泛化能力，确立了新的最先进水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.09776",
            "title": "Can LLM-Generated Textual Explanations Enhance Model Classification\n  Performance? An Empirical Study",
            "url": "https://huggingface.co/papers/2508.09776",
            "abstract": "Automated generation of textual explanations using large language models improves model performance in natural language inference tasks, offering a scalable alternative to human annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t In the rapidly evolving field of Explainable Natural Language Processing (NLP), textual explanations, i.e., human-like rationales, are pivotal for explaining model predictions and enriching datasets with interpretable labels. Traditional approaches rely on human annotation, which is costly, labor-intensive, and impedes scalability. In this work, we present an automated framework that leverages multiple state-of-the-art large language models (LLMs) to generate high-quality textual explanations. We rigorously assess the quality of these LLM-generated explanations using a comprehensive suite of Natural Language Generation (NLG) metrics. Furthermore, we investigate the downstream impact of these explanations on the performance of pre-trained language models (PLMs) and LLMs across natural language inference tasks on two diverse benchmark datasets. Our experiments demonstrate that automated explanations exhibit highly competitive effectiveness compared to human-annotated explanations in improving model performance. Our findings underscore a promising avenue for scalable, automated LLM-based textual explanation generation for extending NLP datasets and enhancing model performance.",
            "score": 0,
            "issue_id": 5346,
            "pub_date": "2025-08-13",
            "pub_date_card": {
                "ru": "13 августа",
                "en": "August 13",
                "zh": "8月13日"
            },
            "hash": "581a97483aba2e0a",
            "authors": [
                "Mahdi Dhaini",
                "Juraj Vladika",
                "Ege Erdogan",
                "Zineb Attaoui",
                "Gjergji Kasneci"
            ],
            "affiliations": [
                "Technical University of Munich, School of Computation, Information and Technology, Department of Computer Science, Munich, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.09776.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#optimization",
                    "#dataset",
                    "#benchmark",
                    "#data"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Автоматизация объяснений в NLP: LLM как альтернатива человеку",
                    "desc": "Исследователи разработали автоматизированный метод генерации текстовых объяснений с использованием больших языковых моделей (LLM) для задач естественного языкового вывода. Этот подход предлагает масштабируемую альтернативу трудоемкой ручной аннотации данных. Эксперименты показали, что автоматически сгенерированные объяснения сравнимы по эффективности с объяснениями, созданными людьми, для улучшения производительности моделей. Результаты открывают перспективный путь для автоматизированного расширения наборов данных NLP и повышения эффективности моделей."
                },
                "en": {
                    "title": "Automated Explanations: Boosting NLP Performance with LLMs",
                    "desc": "This paper discusses a new method for generating textual explanations using large language models (LLMs) to improve natural language inference tasks. Instead of relying on costly human annotations, the authors propose an automated framework that creates high-quality explanations, making the process more scalable. They evaluate the generated explanations using various Natural Language Generation (NLG) metrics and analyze their impact on the performance of pre-trained language models (PLMs). The results show that these automated explanations can significantly enhance model performance, rivaling traditional human-generated explanations."
                },
                "zh": {
                    "title": "自动化文本解释生成，提升模型性能的未来",
                    "desc": "本文提出了一种自动生成文本解释的框架，利用多种先进的大型语言模型（LLMs）来生成高质量的文本解释。这种方法为自然语言推理任务提供了一种可扩展的替代方案，减少了对人工标注的依赖。通过全面的自然语言生成（NLG）指标评估生成解释的质量，研究表明自动生成的解释在提升模型性能方面与人工标注的解释具有高度竞争力。我们的研究为基于LLM的文本解释生成提供了一个有前景的方向，能够扩展NLP数据集并增强模型性能。"
                }
            }
        }
    ],
    "link_prev": "2025-08-13.html",
    "link_next": "2025-08-15.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "13.08",
        "en": "08/13",
        "zh": "8月13日"
    },
    "short_date_next": {
        "ru": "15.08",
        "en": "08/15",
        "zh": "8月15日"
    },
    "categories": {
        "#dataset": 6,
        "#data": 2,
        "#benchmark": 6,
        "#agents": 2,
        "#cv": 3,
        "#rl": 5,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 9,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 2,
        "#games": 1,
        "#interpretability": 3,
        "#reasoning": 6,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 2,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}