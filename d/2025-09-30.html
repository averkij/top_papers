
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 46 papers. September 30.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">30 сентября</span> | <span id="title-articles-count">46 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-09-29.html">⬅️ <span id="prev-date">29.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-01.html">➡️ <span id="next-date">01.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-09.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'};
        let feedDateNext = {'ru': '01.10', 'en': '10/01', 'zh': '10月1日'};
        let feedDatePrev = {'ru': '29.09', 'en': '09/29', 'zh': '9月29日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.24006', 'title': 'SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable\n  Sparse-Linear Attention', 'url': 'https://huggingface.co/papers/2509.24006', 'abstract': 'SLA, a trainable attention method combining sparse and linear attention, accelerates Diffusion Transformer models for video generation with minimal quality loss.  \t\t\t\t\tAI-generated summary \t\t\t\t In Diffusion Transformer (DiT) models, particularly for video generation, attention latency is a major bottleneck due to the long sequence length and the quadratic complexity. We find that attention weights can be separated into two parts: a small fraction of large weights with high rank and the remaining weights with very low rank. This naturally suggests applying sparse acceleration to the first part and low-rank acceleration to the second. Based on this finding, we propose SLA (Sparse-Linear Attention), a trainable attention method that fuses sparse and linear attention to accelerate diffusion models. SLA classifies attention weights into critical, marginal, and negligible categories, applying O(N^2) attention to critical weights, O(N) attention to marginal weights, and skipping negligible ones. SLA combines these computations into a single GPU kernel and supports both forward and backward passes. With only a few fine-tuning steps using SLA, DiT models achieve a 20x reduction in attention computation, resulting in significant acceleration without loss of generation quality. Experiments show that SLA reduces attention computation by 95% without degrading end-to-end generation quality, outperforming baseline methods. In addition, we implement an efficient GPU kernel for SLA, which yields a 13.7x speedup in attention computation and a 2.2x end-to-end speedup in video generation on Wan2.1-1.3B.', 'score': 72, 'issue_id': 6153, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 сентября', 'en': 'September 28', 'zh': '9月28日'}, 'hash': 'a49453eb43711107', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#video', '#training', '#optimization', '#architecture', '#diffusion'], 'emoji': '⚡', 'ru': {'title': 'Ускорение видео-генерации через умное разделение внимания', 'desc': 'В статье представлен метод SLA (Sparse-Linear Attention), который ускоряет Diffusion Transformer модели для генерации видео путем комбинирования разреженного и линейного внимания. Авторы обнаружили, что веса внимания можно разделить на две части: небольшую долю больших весов с высоким рангом и остальные веса с очень низким рангом. SLA классифицирует веса внимания на критические, маргинальные и незначительные категории, применяя соответствующие вычислительные стратегии для каждой. Метод достигает 20-кратного сокращения вычислений внимания и 2.2-кратного ускорения генерации видео при сохранении качества.'}, 'en': {'title': 'Accelerating Video Generation with Sparse-Linear Attention', 'desc': 'The paper introduces SLA, a novel attention mechanism that enhances the efficiency of Diffusion Transformer models used for video generation. By analyzing attention weights, the authors categorize them into critical, marginal, and negligible, allowing for a tailored application of sparse and linear attention techniques. This approach significantly reduces the computational burden of attention mechanisms, achieving a 20x reduction in computation while maintaining high-quality output. The implementation of SLA on GPU demonstrates impressive speed improvements, making it a valuable advancement in the field of machine learning for video generation.'}, 'zh': {'title': '稀疏-线性注意力：加速视频生成的创新方法', 'desc': '本论文提出了一种名为SLA（稀疏-线性注意力）的可训练注意力方法，旨在加速扩散变换器模型在视频生成中的应用。通过将注意力权重分为关键、边际和可忽略三类，SLA对关键权重使用O(N^2)的注意力，对边际权重使用O(N)的注意力，而跳过可忽略的权重，从而显著减少计算量。实验表明，SLA在不降低生成质量的情况下，能够将注意力计算减少95%，并实现20倍的加速。该方法还实现了高效的GPU内核，使得注意力计算速度提升了13.7倍，视频生成的端到端速度提升了2.2倍。'}}}, {'id': 'https://huggingface.co/papers/2509.23102', 'title': 'Multiplayer Nash Preference Optimization', 'url': 'https://huggingface.co/papers/2509.23102', 'abstract': 'Multiplayer Nash Preference Optimization (MNPO) extends Nash learning from human feedback to handle complex, non-transitive human preferences by formulating alignment as an n-player game.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models (LLMs) with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. In this work, we introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an n-player game, where each policy competes against a population of opponents while being regularized toward a reference model. Our framework establishes well-defined Nash equilibria in multiplayer settings and extends the concept of duality gap to quantify approximation quality. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Through comprehensive empirical evaluation, we show that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at https://github.com/smiles724/MNPO.', 'score': 42, 'issue_id': 6153, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': 'f2df8e1f219cb611', 'authors': ['Fang Wu', 'Xu Huang', 'Weihao Xuan', 'Zhiwei Zhang', 'Yijia Xiao', 'Guancheng Wan', 'Xiaomin Li', 'Bing Hu', 'Peng Xia', 'Jure Leskovec', 'Yejin Choi'], 'affiliations': ['Georgia Institute of Technology', 'Harvard University', 'Independent Researcher', 'Pennsylvania State University', 'RIKEN AIP', 'Stanford University', 'The University of Tokyo', 'UNC Chapel Hill', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2509.23102.jpg', 'data': {'categories': ['#rl', '#training', '#alignment', '#optimization', '#rlhf'], 'emoji': '🎮', 'ru': {'title': 'Многопользовательская игра для лучшего понимания предпочтений человека', 'desc': 'Исследователи предложили новый метод обучения языковых моделей на основе предпочтений людей, расширив подход Nash learning до многопользовательской игры. Традиционные методы RLHF с предположением Bradley-Terry не могут справиться с нетранзитивными и разнородными предпочтениями реального мира. Multiplayer Nash Preference Optimization (MNPO) формулирует задачу выравнивания как n-игровую игру, где каждая модель конкурирует с популяцией противников. Экспериментальные результаты показывают, что MNPO превосходит существующие методы на бенчмарках следования инструкциям и лучше справляется с разнородными аннотаторами.'}, 'en': {'title': 'Elevating AI Alignment: Multiplayer Nash Preference Optimization', 'desc': 'Multiplayer Nash Preference Optimization (MNPO) is a new framework that enhances Nash learning from human feedback by addressing complex human preferences in a multiplayer setting. Unlike traditional methods that focus on two-player interactions, MNPO formulates alignment as an n-player game, allowing multiple policies to compete against a diverse set of opponents. This approach not only captures the non-transitive nature of real-world preferences but also establishes well-defined Nash equilibria, improving the robustness of the learning process. Empirical results show that MNPO outperforms existing methods, providing better alignment quality in scenarios with varied human feedback.'}, 'zh': {'title': '多玩家博弈：优化人类偏好对齐', 'desc': '多玩家纳什偏好优化（MNPO）是一个新框架，它将人类反馈的纳什学习扩展到多玩家环境中。该方法将对齐问题视为一个n人博弈，每个策略与一组对手竞争，同时向参考模型进行正则化。MNPO在多玩家设置中建立了明确的纳什均衡，并扩展了对偶间隙的概念，以量化近似质量。通过实证评估，MNPO在遵循指令的基准测试中表现优于现有的两玩家方法，能够更好地处理复杂和非传递的人类偏好。'}}}, {'id': 'https://huggingface.co/papers/2509.25190', 'title': 'Visual Jigsaw Post-Training Improves MLLMs', 'url': 'https://huggingface.co/papers/2509.25190', 'abstract': "Visual Jigsaw, a self-supervised reinforcement learning framework, enhances multimodal large language models' visual understanding through a permutation task without additional annotations or generative components.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning based post-training has recently emerged as a powerful paradigm for enhancing the alignment and reasoning capabilities of multimodal large language models (MLLMs). While vision-centric post-training is crucial for enhancing MLLMs' intrinsic understanding of visual signals, current post-training paradigms are predominantly text-centric, where dense visual inputs are only leveraged to extract sparse cues for text-based reasoning. There exist a few approaches in this direction, however, they often still rely on text as an intermediate mediator or introduce additional visual generative designs. In this work, we introduce Visual Jigsaw, a generic self-supervised post-training framework designed to strengthen visual understanding in MLLMs. Visual Jigsaw is formulated as a general ordering task: visual inputs are partitioned, shuffled, and the model must reconstruct the visual information by producing the correct permutation in natural language. This naturally aligns with reinforcement learning from verifiable rewards (RLVR), requires no additional visual generative components, and derives its supervisory signal automatically without any annotations. We instantiate Visual Jigsaw across three visual modalities, including images, videos, and 3D data. Extensive experiments demonstrate substantial improvements in fine-grained perception, temporal reasoning, and 3D spatial understanding. Our findings highlight the potential of self-supervised vision-centric tasks in post-training MLLMs and aim to inspire further research on vision-centric pretext designs. Project Page: https://penghao-wu.github.io/visual_jigsaw/", 'score': 31, 'issue_id': 6153, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '0ffce000f93cc7e9', 'authors': ['Penghao Wu', 'Yushan Zhang', 'Haiwen Diao', 'Bo Li', 'Lewei Lu', 'Ziwei Liu'], 'affiliations': ['Linkoping University', 'S-Lab, Nanyang Technological University', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.25190.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#alignment', '#multimodal', '#3d', '#cv'], 'emoji': '🧩', 'ru': {'title': 'Собираем пазл из визуальных данных для лучшего понимания', 'desc': 'Visual Jigsaw - это фреймворк самообучающегося reinforcement learning для улучшения визуального понимания у мультимодальных больших языковых моделей. Метод работает как задача упорядочивания: визуальные входы разбиваются на части, перемешиваются, и модель должна восстановить правильную последовательность на естественном языке. Подход не требует дополнительных аннотаций или генеративных компонентов, получая сигнал для обучения автоматически. Эксперименты показывают значительные улучшения в детальном восприятии, временном рассуждении и понимании 3D пространства.'}, 'en': {'title': 'Enhancing Visual Understanding in MLLMs with Visual Jigsaw', 'desc': 'Visual Jigsaw is a self-supervised reinforcement learning framework that improves the visual understanding of multimodal large language models (MLLMs) by using a permutation task. Instead of relying on text or additional generative components, it focuses on visual inputs by shuffling and requiring the model to reconstruct the correct order. This approach aligns with reinforcement learning from verifiable rewards, allowing the model to learn without needing extra annotations. The framework has been tested on images, videos, and 3D data, showing significant enhancements in perception, reasoning, and spatial understanding.'}, 'zh': {'title': '视觉拼图：提升多模态模型的视觉理解', 'desc': 'Visual Jigsaw是一种自监督的强化学习框架，旨在通过排列任务增强多模态大语言模型的视觉理解能力。该方法不需要额外的注释或生成组件，而是通过将视觉输入分割和打乱，要求模型在自然语言中重建正确的排列。通过这种方式，Visual Jigsaw能够自动生成监督信号，并与可验证奖励的强化学习相结合。实验结果表明，该框架在细粒度感知、时间推理和三维空间理解方面显著提升了模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2509.24897', 'title': 'RealUnify: Do Unified Models Truly Benefit from Unification? A\n  Comprehensive Benchmark', 'url': 'https://huggingface.co/papers/2509.24897', 'abstract': 'RealUnify evaluates the bidirectional synergy between understanding and generation in unified multimodal models, revealing that current models lack effective integration despite architectural unification.  \t\t\t\t\tAI-generated summary \t\t\t\t The integration of visual understanding and generation into unified multimodal models represents a significant stride toward general-purpose AI. However, a fundamental question remains unanswered by existing benchmarks: does this architectural unification actually enable synergetic interaction between the constituent capabilities? Existing evaluation paradigms, which primarily assess understanding and generation in isolation, are insufficient for determining whether a unified model can leverage its understanding to enhance its generation, or use generative simulation to facilitate deeper comprehension. To address this critical gap, we introduce RealUnify, a benchmark specifically designed to evaluate bidirectional capability synergy. RealUnify comprises 1,000 meticulously human-annotated instances spanning 10 categories and 32 subtasks. It is structured around two core axes: 1) Understanding Enhances Generation, which requires reasoning (e.g., commonsense, logic) to guide image generation, and 2) Generation Enhances Understanding, which necessitates mental simulation or reconstruction (e.g., of transformed or disordered visual inputs) to solve reasoning tasks. A key contribution is our dual-evaluation protocol, which combines direct end-to-end assessment with a diagnostic stepwise evaluation that decomposes tasks into distinct understanding and generation phases. This protocol allows us to precisely discern whether performance bottlenecks stem from deficiencies in core abilities or from a failure to integrate them. Through large-scale evaluations of 12 leading unified models and 6 specialized baselines, we find that current unified models still struggle to achieve effective synergy, indicating that architectural unification alone is insufficient. These results highlight the need for new training strategies and inductive biases to fully unlock the potential of unified modeling.', 'score': 27, 'issue_id': 6153, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': 'b10849bf0e51cb05', 'authors': ['Yang Shi', 'Yuhao Dong', 'Yue Ding', 'Yuran Wang', 'Xuanyu Zhu', 'Sheng Zhou', 'Wenting Liu', 'Haochen Tian', 'Rundong Wang', 'Huanqian Wang', 'Zuyan Liu', 'Bohan Zeng', 'Ruizhe Chen', 'Qixun Wang', 'Zhuoran Zhang', 'Xinlong Chen', 'Chengzhuo Tong', 'Bozhou Li', 'Chaoyou Fu', 'Qiang Liu', 'Haotian Wang', 'Wenjing Yang', 'Yuanxing Zhang', 'Pengfei Wan', 'Yi-Fan Zhang', 'Ziwei Liu'], 'affiliations': ['CASIA', 'Kling Team', 'NJU', 'NTU', 'NUS', 'PKU', 'THU', 'USTC', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2509.24897.jpg', 'data': {'categories': ['#training', '#reasoning', '#agi', '#multimodal', '#benchmark', '#survey', '#architecture'], 'emoji': '🔄', 'ru': {'title': 'Унификация без синергии: почему объединение понимания и генерации пока не работает', 'desc': 'Исследователи представляют RealUnify - новый бенчмарк для оценки синергии между пониманием и генерацией в унифицированных мультимодальных моделях. Бенчмарк включает 1000 задач, разделенных на два направления: использование понимания для улучшения генерации и использование генерации для углубления понимания. Тестирование 12 ведущих унифицированных моделей показало, что текущие архитектуры не достигают эффективной синергии между компонентами. Результаты указывают на необходимость новых стратегий обучения для полного раскрытия потенциала унифицированного моделирования.'}, 'en': {'title': 'Unlocking Synergy in Unified Multimodal Models', 'desc': 'RealUnify is a benchmark that assesses how well unified multimodal models can integrate visual understanding and generation. It addresses the gap in existing evaluations that only test these capabilities separately, rather than their interaction. The benchmark includes 1,000 human-annotated examples across various tasks, focusing on how understanding can improve generation and vice versa. Findings show that current models do not effectively leverage their integrated capabilities, suggesting a need for improved training methods to enhance synergy between understanding and generation.'}, 'zh': {'title': '评估理解与生成的双向协同作用', 'desc': 'RealUnify 是一个评估统一多模态模型中理解与生成之间双向协同作用的基准。尽管现有模型在架构上实现了统一，但它们在有效整合理解与生成能力方面仍然存在不足。RealUnify 包含1000个经过人工标注的实例，旨在评估理解如何增强生成，以及生成如何促进理解。我们的研究表明，当前的统一模型在实现有效协同方面仍面临挑战，强调了需要新的训练策略来充分发挥统一建模的潜力。'}}}, {'id': 'https://huggingface.co/papers/2509.23426', 'title': 'Democratizing AI scientists using ToolUniverse', 'url': 'https://huggingface.co/papers/2509.23426', 'abstract': 'ToolUniverse is an ecosystem that standardizes and integrates tools, models, and data for AI scientists, enabling automated refinement, creation, and composition of workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t AI scientists are emerging computational systems that serve as collaborative partners in discovery. These systems remain difficult to build because they are bespoke, tied to rigid workflows, and lack shared environments that unify tools, data, and analyses into a common ecosystem. In omics, unified ecosystems have transformed research by enabling interoperability, reuse, and community-driven development; AI scientists require comparable infrastructure. We present ToolUniverse, an ecosystem for building AI scientists from any language or reasoning model, whether open or closed. TOOLUNIVERSE standardizes how AI scientists identify and call tools, integrating more than 600 machine learning models, datasets, APIs, and scientific packages for data analysis, knowledge retrieval, and experimental design. It automatically refines tool interfaces for correct use by AI scientists, creates new tools from natural language descriptions, iteratively optimizes tool specifications, and composes tools into agentic workflows. In a case study of hypercholesterolemia, ToolUniverse was used to create an AI scientist to identify a potent analog of a drug with favorable predicted properties. The open-source ToolUniverse is available at https://aiscientist.tools.', 'score': 26, 'issue_id': 6153, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': 'dfc18b241e0932cd', 'authors': ['Shanghua Gao', 'Richard Zhu', 'Pengwei Sui', 'Zhenglun Kong', 'Sufian Aldogom', 'Yepeng Huang', 'Ayush Noori', 'Reza Shamji', 'Krishna Parvataneni', 'Theodoros Tsiligkaridis', 'Marinka Zitnik'], 'affiliations': ['Broad Institute of MIT and Harvard, Cambridge, MA', 'Department of Biomedical Informatics, Harvard Medical School, Boston, MA', 'Harvard College, Harvard University, Cambridge, MA', 'Harvard Data Science Initiative, Cambridge, MA', 'Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Cambridge, MA', 'MIT Lincoln Laboratory, Lexington, MA', 'Massachusetts Institute of Technology, Cambridge, MA'], 'pdf_title_img': 'assets/pdf/title_img/2509.23426.jpg', 'data': {'categories': ['#data', '#dataset', '#science', '#open_source', '#multimodal', '#agents'], 'emoji': '🔬', 'ru': {'title': 'Универсальная экосистема для создания AI-ученых', 'desc': 'ToolUniverse - это экосистема, которая стандартизирует и интегрирует инструменты, модели и данные для AI-ученых, обеспечивая автоматическое улучшение, создание и компоновку рабочих процессов. Система объединяет более 600 моделей машинного обучения, датасетов, API и научных пакетов для анализа данных, извлечения знаний и планирования экспериментов. ToolUniverse автоматически улучшает интерфейсы инструментов для корректного использования AI-системами, создает новые инструменты из описаний на естественном языке и итеративно оптимизирует их спецификации. В практическом исследовании гиперхолестеринемии система была использована для создания AI-ученого, который идентифицировал мощный аналог лекарства с благоприятными предсказанными свойствами.'}, 'en': {'title': 'Empowering AI Scientists with ToolUniverse: A Unified Ecosystem for Discovery', 'desc': 'ToolUniverse is a standardized ecosystem designed for AI scientists, facilitating the integration of various tools, models, and datasets. It allows for the automated creation and refinement of workflows, making it easier for AI systems to collaborate in scientific discovery. By supporting over 600 machine learning models and APIs, ToolUniverse enhances interoperability and promotes community-driven development in research. A practical application demonstrated its capability by creating an AI scientist that identified a promising drug analog for hypercholesterolemia.'}, 'zh': {'title': 'ToolUniverse：构建AI科学家的统一生态系统', 'desc': 'ToolUniverse是一个为AI科学家提供标准化和集成工具、模型和数据的生态系统，旨在实现工作流程的自动化优化和创建。该系统解决了构建AI科学家时面临的挑战，如定制化、僵化的工作流程和缺乏统一环境的问题。ToolUniverse集成了超过600个机器学习模型、数据集、API和科学包，支持数据分析、知识检索和实验设计。通过案例研究，ToolUniverse成功创建了一个AI科学家，帮助识别具有良好预测特性的药物类比。'}}}, {'id': 'https://huggingface.co/papers/2509.24900', 'title': 'OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation\n  and Editing', 'url': 'https://huggingface.co/papers/2509.24900', 'abstract': 'OpenGPT-4o-Image, a large-scale dataset with hierarchical task taxonomy and automated generation, significantly improves performance in image generation and editing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The performance of unified multimodal models for image generation and editing is fundamentally constrained by the quality and comprehensiveness of their training data. While existing datasets have covered basic tasks like style transfer and simple object manipulation, they often lack the systematic structure and challenging scenarios required for real-world applications. To address this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset constructed using a novel methodology that combines hierarchical task taxonomy with automated data generation. Our taxonomy not only includes fundamental capabilities such as text rendering and style control but also introduces highly practical yet challenging categories like scientific imagery for chemistry illustrations and complex instruction editing requiring simultaneous execution of multiple operations. Through an automated pipeline leveraging structured resource pools and GPT-4o, we generate 80k high-quality instruction-image pairs with controlled diversity, covering 11 major domains and 51 subtasks. Extensive experiments show that fine-tuning leading models on our dataset achieves significant performance gains across multiple benchmarks, with improvements of up to 18\\% on editing tasks (UniWorld-V1 on ImgEdit-Bench) and 13% on generation tasks (Harmon on GenEval). Our work demonstrates that systematic data construction is key to advancing multimodal AI capabilities.', 'score': 24, 'issue_id': 6154, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '0b2c3d817fd472dc', 'authors': ['Zhihong Chen', 'Xuehai Bai', 'Yang Shi', 'Chaoyou Fu', 'Huanyu Zhang', 'Haotian Wang', 'Xiaoyan Sun', 'Zhang Zhang', 'Liang Wang', 'Yuanxing Zhang', 'Pengfei Wan', 'Yi-Fan Zhang'], 'affiliations': ['CASIA', 'HDU', 'Kling Team', 'NJU', 'PKU', 'THU', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2509.24900.jpg', 'data': {'categories': ['#data', '#optimization', '#benchmark', '#synthetic', '#multimodal', '#dataset'], 'emoji': '🎨', 'ru': {'title': 'Систематический подход к созданию данных — ключ к прорыву в мультимодальном AI', 'desc': 'Исследователи создали OpenGPT-4o-Image — крупномасштабный датасет для обучения мультимодальных моделей генерации и редактирования изображений. Датасет содержит 80 тысяч пар инструкция-изображение, охватывающих 11 основных доменов и 51 подзадачу, включая сложные сценарии вроде научной визуализации. Для создания данных использовалась иерархическая таксономия задач и автоматизированная генерация с помощью GPT-4o. Дообучение ведущих моделей на этом датасете показало значительный прирост производительности — до 18% на задачах редактирования и до 13% на задачах генерации изображений.'}, 'en': {'title': 'Unlocking Image Generation with Structured Data', 'desc': 'OpenGPT-4o-Image is a large-scale dataset designed to enhance image generation and editing tasks by providing a structured and comprehensive training resource. It utilizes a hierarchical task taxonomy to categorize tasks, including both basic operations and complex scenarios like scientific imagery. The dataset consists of 80,000 high-quality instruction-image pairs generated through an automated pipeline, ensuring diversity across 11 domains and 51 subtasks. Experiments show that models fine-tuned on this dataset achieve significant performance improvements, highlighting the importance of systematic data construction in advancing multimodal AI capabilities.'}, 'zh': {'title': '系统化数据构建推动多模态AI能力提升', 'desc': 'OpenGPT-4o-Image是一个大规模的数据集，采用分层任务分类法和自动生成方法，显著提升了图像生成和编辑任务的性能。现有的数据集虽然涵盖了基本任务，但往往缺乏系统结构和真实场景所需的挑战性。我们通过自动化流程生成了8万对高质量的指令-图像配对，覆盖了11个主要领域和51个子任务。实验结果表明，在我们的数据集上微调领先模型可以在多个基准测试中实现显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2509.25175', 'title': 'EasySteer: A Unified Framework for High-Performance and Extensible LLM\n  Steering', 'url': 'https://huggingface.co/papers/2509.25175', 'abstract': "EasySteer is a unified framework for efficient and extensible steering of large language models, offering significant speedups and improved functionality over existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM) steering has emerged as a promising paradigm for controlling model behavior at inference time through targeted manipulation of hidden states, offering a lightweight alternative to expensive retraining. However, existing steering frameworks suffer from critical limitations: computational inefficiency, limited extensibility, and restricted functionality that hinder both research progress and practical deployment. We present EasySteer, a unified framework for high-performance, extensible LLM steering built on vLLM. Our system features modular architecture with pluggable interfaces for both analysis-based and learning-based methods, fine-grained parameter control, pre-computed steering vectors for eight application domains, and an interactive demonstration system. Through deep integration with vLLM's optimized inference engine, EasySteer achieves 5.5-11.4times speedup over existing frameworks. Extensive experiments demonstrate its effectiveness in overthinking mitigation, hallucination reduction, and other key applications. EasySteer transforms steering from research technique to production-ready capability, establishing critical infrastructure for deployable, controllable language models.", 'score': 20, 'issue_id': 6153, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '213f3b6e7bcca3af', 'authors': ['Haolei Xu', 'Xinyu Mei', 'Yuchen Yan', 'Rui Zhou', 'Wenqi Zhang', 'Weiming Lu', 'Yueting Zhuang', 'Yongliang Shen'], 'affiliations': ['Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.25175.jpg', 'data': {'categories': ['#training', '#alignment', '#inference', '#optimization', '#hallucinations', '#architecture'], 'emoji': '🎛️', 'ru': {'title': 'Высокоскоростное управление LLM без переобучения', 'desc': 'EasySteer — это унифицированная система для эффективного управления поведением больших языковых моделей во время инференса через манипуляцию скрытыми состояниями. Система построена на основе vLLM и обеспечивает ускорение в 5.5-11.4 раза по сравнению с существующими методами. Фреймворк имеет модульную архитектуру с возможностью подключения различных методов анализа и обучения, а также предварительно вычисленные векторы управления для восьми областей применения. EasySteer решает проблемы переосмысления, галлюцинаций и другие задачи, превращая steering из исследовательской техники в готовое к производству решение.'}, 'en': {'title': 'EasySteer: Fast and Flexible Steering for Language Models', 'desc': "EasySteer is a new framework designed to improve the steering of large language models (LLMs) during inference, allowing for better control over their behavior without the need for costly retraining. It addresses the inefficiencies and limitations of previous steering methods by providing a modular architecture that supports both analysis-based and learning-based approaches. The framework includes pre-computed steering vectors for various applications and offers fine-grained control over model parameters, resulting in significant speed improvements. With its integration into vLLM's optimized inference engine, EasySteer not only enhances performance but also makes steering techniques practical for real-world applications."}, 'zh': {'title': 'EasySteer：高效可扩展的语言模型引导框架', 'desc': 'EasySteer是一个统一的框架，用于高效和可扩展的大型语言模型（LLM）引导。它通过对隐藏状态的有针对性操作，在推理时控制模型行为，提供了一种比昂贵的再训练更轻量的替代方案。与现有方法相比，EasySteer在计算效率、可扩展性和功能性方面有显著提升，能够支持多种应用领域。通过与vLLM的深度集成，EasySteer实现了5.5到11.4倍的速度提升，成为可部署的可控语言模型的重要基础设施。'}}}, {'id': 'https://huggingface.co/papers/2509.24695', 'title': 'SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer', 'url': 'https://huggingface.co/papers/2509.24695', 'abstract': 'SANA-Video, a small diffusion model, efficiently generates high-resolution, high-quality videos with strong text-video alignment using linear attention and a constant-memory KV cache, achieving competitive performance at a lower cost and faster speed.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation.', 'score': 18, 'issue_id': 6153, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '95921e0ae91378d2', 'authors': ['Junsong Chen', 'Yuyang Zhao', 'Jincheng Yu', 'Ruihang Chu', 'Junyu Chen', 'Shuai Yang', 'Xianbang Wang', 'Yicheng Pan', 'Daquan Zhou', 'Huan Ling', 'Haozhe Liu', 'Hongwei Yi', 'Hao Zhang', 'Muyang Li', 'Yukang Chen', 'Han Cai', 'Sanja Fidler', 'Ping Luo', 'Song Han', 'Enze Xie'], 'affiliations': ['HKU', 'KAUST', 'MIT', 'NVIDIA', 'PKU', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2509.24695.jpg', 'data': {'categories': ['#video', '#training', '#inference', '#small_models', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Быстрая и экономичная генерация длинных видео высокого качества', 'desc': 'SANA-Video - это компактная диффузионная модель для генерации видео высокого разрешения до 720x1280 пикселей длительностью до минуты. Модель использует линейное внимание (linear attention) вместо обычного механизма внимания для более эффективной обработки большого количества токенов в видеогенерации. Ключевой особенностью является константный KV-кэш с блочным автопрогрессивным подходом, который обеспечивает глобальный контекст при фиксированных затратах памяти. Модель работает в 16 раз быстрее аналогов и может быть развернута на RTX 5090 GPU с ускорением генерации 5-секундного 720p видео в 2.4 раза.'}, 'en': {'title': 'Efficient High-Quality Video Generation with SANA-Video', 'desc': 'SANA-Video is a small diffusion model designed to generate high-resolution videos efficiently. It utilizes linear attention and a constant-memory KV cache to improve speed and reduce costs while maintaining strong text-video alignment. The model can produce videos up to 720x1280 resolution and minute-long duration, achieving competitive performance compared to larger models. With a significant reduction in training time and cost, SANA-Video is optimized for deployment on modern GPUs, making it a practical choice for video generation tasks.'}, 'zh': {'title': '低成本高质量视频生成的革命性模型', 'desc': 'SANA-Video是一种小型扩散模型，能够高效生成高分辨率、高质量的视频，且与文本视频的对齐性强。该模型采用线性注意力和常量内存KV缓存，显著降低了生成视频的成本和时间。通过线性DiT和块线性注意力的设计，SANA-Video在生成长视频时保持了高效性和有效性。与现代小型扩散模型相比，SANA-Video在性能上具有竞争力，同时在生成速度上快了16倍。'}}}, {'id': 'https://huggingface.co/papers/2509.23909', 'title': 'EditScore: Unlocking Online RL for Image Editing via High-Fidelity\n  Reward Modeling', 'url': 'https://huggingface.co/papers/2509.23909', 'abstract': 'A specialized reward model, EditScore, enables effective reinforcement learning for instruction-guided image editing by providing a high-fidelity reward signal.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-guided image editing has achieved remarkable progress, yet current models still face challenges with complex instructions and often require multiple samples to produce a desired result. Reinforcement Learning (RL) offers a promising solution, but its adoption in image editing has been severely hindered by the lack of a high-fidelity, efficient reward signal. In this work, we present a comprehensive methodology to overcome this barrier, centered on the development of a state-of-the-art, specialized reward model. We first introduce EditReward-Bench, a comprehensive benchmark to systematically evaluate reward models on editing quality. Building on this benchmark, we develop EditScore, a series of reward models (7B-72B) for evaluating the quality of instruction-guided image editing. Through meticulous data curation and filtering, EditScore effectively matches the performance of learning proprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy tailored for the generative nature of EditScore, our largest variant even surpasses GPT-5 in the benchmark. We then demonstrate that a high-fidelity reward model is the key to unlocking online RL for image editing. Our experiments show that, while even the largest open-source VLMs fail to provide an effective learning signal, EditScore enables efficient and robust policy optimization. Applying our framework to a strong base model, OmniGen2, results in a final model that shows a substantial and consistent performance uplift. Overall, this work provides the first systematic path from benchmarking to reward modeling to RL training in image editing, showing that a high-fidelity, domain-specialized reward model is the key to unlocking the full potential of RL in this domain.', 'score': 18, 'issue_id': 6153, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 сентября', 'en': 'September 28', 'zh': '9月28日'}, 'hash': 'a3543c4787af3b5e', 'authors': ['Xin Luo', 'Jiahao Wang', 'Chenyuan Wu', 'Shitao Xiao', 'Xiyan Jiang', 'Defu Lian', 'Jiajun Zhang', 'Dong Liu', 'Zheng liu'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Institute of Automation, Chinese Academy of Sciences', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.23909.jpg', 'data': {'categories': ['#rl', '#data', '#training', '#optimization', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'Высококачественная reward модель - ключ к RL в редактировании изображений', 'desc': 'Исследователи создали специализированную reward модель EditScore для обучения с подкреплением в задаче редактирования изображений по текстовым инструкциям. Они разработали бенчмарк EditReward-Bench для систематической оценки качества reward моделей и показали, что их модель превосходит даже GPT-5. С помощью высококачественной reward модели они успешно применили reinforcement learning к редактированию изображений, что ранее было затруднительно. Результаты показывают значительное улучшение производительности базовой модели OmniGen2 после применения их подхода.'}, 'en': {'title': 'Unlocking Image Editing with High-Fidelity Rewards', 'desc': 'This paper introduces EditScore, a specialized reward model designed to enhance reinforcement learning (RL) for instruction-guided image editing. The authors highlight the challenges faced by current models in interpreting complex instructions and generating desired outputs efficiently. By developing EditReward-Bench, a benchmark for evaluating reward models, they demonstrate that EditScore significantly improves the quality of image editing by providing a high-fidelity reward signal. The results show that EditScore not only matches but can also surpass existing models, enabling effective policy optimization and substantial performance improvements in image editing tasks.'}, 'zh': {'title': '高保真奖励模型，解锁图像编辑的潜力', 'desc': '本论文提出了一种专门的奖励模型EditScore，旨在提升基于指令的图像编辑中的强化学习效果。当前的图像编辑模型在处理复杂指令时仍面临挑战，且通常需要多个样本才能达到预期效果。EditScore通过提供高保真度的奖励信号，克服了这一障碍，并在基准测试中表现出色。研究表明，高保真度的奖励模型是实现图像编辑在线强化学习的关键。'}}}, {'id': 'https://huggingface.co/papers/2509.25160', 'title': 'GSM8K-V: Can Vision Language Models Solve Grade School Math Word\n  Problems in Visual Contexts', 'url': 'https://huggingface.co/papers/2509.25160', 'abstract': 'GSM8K-V is a new visual multi-image mathematical reasoning benchmark that highlights the limitations of current vision language models in handling visual mathematical problems.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision language models (VLMs) achieve unified modeling of images and text, enabling them to accomplish complex real-world tasks through perception, planning, and reasoning. Among these tasks, reasoning is particularly representative, with mathematical reasoning serving as a prominent example. It highlights the high-level capability of VLMs to comprehend mathematical information in images and to perform sophisticated reasoning. Recently, numerous visual mathematical reasoning benchmarks have been proposed, but they are often restricted to geometry, lack coverage of math word problems, and rarely assess reasoning across multiple images. To address these gaps, we introduce GSM8K-V, a purely visual multi-image mathematical reasoning benchmark. GSM8K-V is built by systematically mapping each sample from the widely used text-based GSM8K into visual form. Through a carefully designed automated image-generation pipeline combined with meticulous human annotation, we curate 1,319 high-quality samples. We evaluate a wide range of open-source and closed-source models on GSM8K-V. Results show that although existing VLMs have nearly saturated performance on text-based GSM8K, there remains substantial room for improvement on GSM8K-V. For example, the best-performing model, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on GSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the limitations of current models as well as potential directions for improvement. GSM8K-V offers a new perspective on visual mathematical reasoning and establishes a benchmark to guide the development of more robust and generalizable VLMs.', 'score': 17, 'issue_id': 6154, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '763f54eeeeb77cf4', 'authors': ['Fan Yuan', 'Yuchen Yan', 'Yifan Jiang', 'Haoran Zhao', 'Tao Feng', 'Jinyan Chen', 'Yanwei Lou', 'Wenqi Zhang', 'Yongliang Shen', 'Weiming Lu', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.25160.jpg', 'data': {'categories': ['#survey', '#benchmark', '#reasoning', '#cv', '#dataset'], 'emoji': '🧮', 'ru': {'title': 'Когда картинки ставят AI в тупик: визуальная математика как новый вызов для умных систем', 'desc': 'Исследователи создали новый бенчмарк GSM8K-V для оценки математического мышления vision-language моделей при работе с несколькими изображениями. Они преобразовали 1,319 текстовых математических задач из популярного датасета GSM8K в визуальный формат с помощью автоматической генерации изображений и человеческой разметки. Тестирование показало, что даже лучшие модели, которые показывают почти идеальные результаты на текстовых задачах (95% точности), сильно проседают на визуальных версиях (всего 47% точности). Этот бенчмарк выявляет серьезные ограничения современных VLM в визуальном математическом рассуждении и задает направления для их улучшения.'}, 'en': {'title': 'GSM8K-V: Bridging the Gap in Visual Mathematical Reasoning', 'desc': 'GSM8K-V is a new benchmark designed to test visual multi-image mathematical reasoning, revealing the shortcomings of current vision language models (VLMs) in solving visual math problems. Unlike previous benchmarks that focus mainly on geometry or single-image tasks, GSM8K-V includes a diverse range of math word problems and requires reasoning across multiple images. The benchmark consists of 1,319 high-quality visual samples created through an automated image-generation process and human annotation. Evaluation results indicate that while VLMs perform well on text-based tasks, they struggle significantly with the visual aspects of mathematical reasoning, highlighting the need for further advancements in this area.'}, 'zh': {'title': '视觉数学推理的新基准：GSM8K-V', 'desc': 'GSM8K-V是一个新的视觉多图像数学推理基准，突显了当前视觉语言模型在处理视觉数学问题时的局限性。该基准通过将广泛使用的文本基础GSM8K样本系统性地映射为视觉形式而构建，包含1319个高质量样本。尽管现有的视觉语言模型在文本基础GSM8K上表现接近饱和，但在GSM8K-V上仍有显著的改进空间。GSM8K-V为视觉数学推理提供了新的视角，并为开发更强大和更具通用性的视觉语言模型奠定了基准。'}}}, {'id': 'https://huggingface.co/papers/2509.24014', 'title': 'SparseD: Sparse Attention for Diffusion Language Models', 'url': 'https://huggingface.co/papers/2509.24014', 'abstract': "SparseD is a novel sparse attention method for diffusion language models that addresses the high inference latency by pre-computing head-specific sparse patterns and switching to sparse attention in later denoising steps.  \t\t\t\t\tAI-generated summary \t\t\t\t While diffusion language models (DLMs) offer a promising alternative to autoregressive models (ARs), existing open-source DLMs suffer from high inference latency. This bottleneck is mainly due to the attention's quadratic complexity with respect to context length in computing all query-key pairs. Intuitively, to reduce this complexity, a natural strategy is to restrict attention to sparse patterns that retain only the most relevant connections. Such approaches are well-established in ARs, where attention follows fixed and clearly defined sparse patterns. However, in DLMs, we observe distinct sparsity behaviors: (1) attention patterns vary across heads, (2) attention patterns in each head remain highly similar across denoising steps, and (3) early denoising steps are critical for generation. These findings render sparse attention methods designed for ARs largely incompatible with DLMs, as they fail to capture head-specific structures and risk degrading generation when applied in early denoising steps. To address these challenges, we propose SparseD, a novel sparse attention method for DLMs. Leveraging the observations, SparseD only requires pre-computing head-specific sparse patterns one time, and reuses them across all steps. This prevents recomputing sparse patterns at each denoising step. Meanwhile, SparseD uses full attention in the early steps, then switches to sparse attention later to maintain generation quality. Together, these establish SparseD as a practical and efficient solution for deploying DLMs in long-context applications. Experimental results demonstrate that SparseD achieves lossless acceleration, delivering up to 1.50times speedup over FlashAttention at a 64k context length with 1,024 denoising steps.", 'score': 16, 'issue_id': 6153, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 сентября', 'en': 'September 28', 'zh': '9月28日'}, 'hash': 'de0de0ca5cd9c93d', 'authors': ['Zeqing Wang', 'Gongfan Fang', 'Xinyin Ma', 'Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2509.24014.jpg', 'data': {'categories': ['#inference', '#long_context', '#optimization', '#architecture', '#diffusion'], 'emoji': '⚡', 'ru': {'title': 'Ускорение диффузионных моделей через умное разреженное внимание', 'desc': 'SparseD - это новый метод разреженного внимания для диффузионных языковых моделей, который решает проблему высокой задержки вывода. Метод предварительно вычисляет специфичные для каждой головы разреженные паттерны и переключается на разреженное внимание на поздних этапах денойзинга. В отличие от авторегрессионных моделей, диффузионные модели имеют различные паттерны внимания для разных голов, которые остаются стабильными между шагами денойзинга. SparseD достигает ускорения до 1.5 раза по сравнению с FlashAttention при длине контекста 64k без потери качества генерации.'}, 'en': {'title': 'SparseD: Speeding Up Diffusion Language Models with Smart Sparse Attention', 'desc': 'SparseD is a new method designed to improve the efficiency of diffusion language models (DLMs) by implementing sparse attention techniques. It addresses the problem of high inference latency caused by the quadratic complexity of traditional attention mechanisms. By pre-computing specific sparse patterns for each attention head, SparseD allows for faster processing during later denoising steps while maintaining high generation quality. This approach not only speeds up the model significantly but also ensures that the unique sparsity behaviors of DLMs are effectively utilized, resulting in a practical solution for long-context applications.'}, 'zh': {'title': 'SparseD：高效的扩散语言模型稀疏注意力方法', 'desc': 'SparseD是一种新颖的稀疏注意力方法，专为扩散语言模型设计，旨在通过预计算特定头的稀疏模式来解决高推理延迟问题。该方法在后续的去噪步骤中切换到稀疏注意力，从而提高效率。SparseD的关键在于它只需一次性预计算头特定的稀疏模式，并在所有步骤中重复使用，避免了每个去噪步骤都重新计算稀疏模式。实验结果表明，SparseD在64k上下文长度和1024个去噪步骤下，能够实现高达1.50倍的加速。'}}}, {'id': 'https://huggingface.co/papers/2509.25123', 'title': 'From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL by\n  Composing Old Ones', 'url': 'https://huggingface.co/papers/2509.25123', 'abstract': "Reinforcement learning enables large language models to acquire new compositional skills by combining existing ones, which transfer to different tasks and improve reasoning behaviors.  \t\t\t\t\tAI-generated summary \t\t\t\t Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.", 'score': 11, 'issue_id': 6155, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '6c3606d0185935f9', 'authors': ['Lifan Yuan', 'Weize Chen', 'Yuchen Zhang', 'Ganqu Cui', 'Hanbin Wang', 'Ziming You', 'Ning Ding', 'Zhiyuan Liu', 'Maosong Sun', 'Hao Peng'], 'affiliations': ['Peking University', 'Shanghai AI Laboratory', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2509.25123.jpg', 'data': {'categories': ['#transfer_learning', '#training', '#reasoning', '#rl', '#synthetic', '#rlhf'], 'emoji': '🧩', 'ru': {'title': 'RL учит LLM комбинировать навыки как конструктор', 'desc': 'Исследование показывает, что reinforcement learning позволяет большим языковым моделям приобретать новые навыки путем композиции уже существующих способностей. Авторы доказали, что LLM могут изучать комбинации функций h(x)=g(f(x)) во время RL-обучения, даже если функции f и g были изучены ранее. Эти композиционные навыки обобщаются на более сложные задачи и переносятся между различными доменами. В отличие от обучения next-token prediction, RL фундаментально меняет рассуждения модели и способствует развитию продвинутых навыков решения сложных проблем.'}, 'en': {'title': 'Reinforcement Learning: Unlocking New Skills in Language Models', 'desc': 'This paper explores how reinforcement learning (RL) can help large language models (LLMs) develop new skills by combining existing ones, enhancing their reasoning capabilities. The authors investigate whether RL genuinely teaches LLMs new skills or simply reactivates learned strategies. Through a synthetic framework, they demonstrate that LLMs can learn to compose functions during RL training, allowing them to solve more complex tasks. The findings suggest that RL significantly alters the reasoning behaviors of LLMs, leading to improved performance on tasks without prior specific training.'}, 'zh': {'title': '强化学习助力语言模型获得新技能', 'desc': '强化学习（RL）使大型语言模型（LLM）能够通过组合现有技能来获得新的组合技能，这些技能可以转移到不同的任务中并改善推理行为。研究表明，LLM在强化学习过程中确实可以获得真正的新技能，而不仅仅是激活已有的推理策略。我们开发了一个合成框架来控制任务复杂性，并发现LLM能够学习未见的函数组合，并且这种组合能力可以推广到更复杂的问题上。实验结果显示，LLM在源任务上获得的组合技能可以转移到不同的目标任务，即使在目标任务上没有进行组合训练。'}}}, {'id': 'https://huggingface.co/papers/2509.24663', 'title': 'InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long\n  Adaptation', 'url': 'https://huggingface.co/papers/2509.24663', 'abstract': 'A dense-sparse switchable attention framework, InfLLM-V2, enhances long-sequence processing in large language models by efficiently adapting between dense and sparse attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-sequence processing is a critical capability for modern large language models. However, the self-attention mechanism in the standard Transformer architecture faces severe computational and memory bottlenecks when processing long sequences. While trainable sparse attention methods offer a promising solution, existing approaches such as NSA introduce excessive extra parameters and disrupt the conventional pretrain-on-short, finetune-on-long workflow, resulting in slow convergence and difficulty in acceleration. To overcome these limitations, we introduce dense-sparse switchable attention framework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that seamlessly adapts models from short to long sequences. Specifically, InfLLM-V2 reuses dense attention parameters through parameter-free architecture modification, maintaining consistency between short and long sequence processing. Additionally, InfLLM-V2 ensures computational efficiency across all sequence lengths, by using dense attention for short inputs and smoothly transitioning to sparse attention for long sequences. To achieve practical acceleration, we further introduce an efficient implementation of InfLLM-V2 that significantly reduces the computational overhead. Our experiments on long-context understanding and chain-of-thought reasoning demonstrate that InfLLM-V2 is 4times faster than dense attention while retaining 98.1% and 99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we have trained and open-sourced MiniCPM4.1 (https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model, providing a reproducible implementation for the research community.', 'score': 11, 'issue_id': 6154, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': 'bc083d082afe02e9', 'authors': ['Weilin Zhao', 'Zihan Zhou', 'Zhou Su', 'Chaojun Xiao', 'Yuxuan Li', 'Yanghao Li', 'Yudi Zhang', 'Weilun Zhao', 'Zhen Li', 'Yuxiang Huang', 'Ao Sun', 'Xu Han', 'Zhiyuan Liu'], 'affiliations': ['Harbin Institute of Technology', 'OpenBMB', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.24663.jpg', 'data': {'categories': ['#architecture', '#open_source', '#reasoning', '#training', '#long_context'], 'emoji': '⚡', 'ru': {'title': 'Умное переключение внимания для эффективной работы с длинными текстами', 'desc': 'Исследователи представили InfLLM-V2 — новый подход к обработке длинных последовательностей в больших языковых моделях. Система умело переключается между плотным и разреженным вниманием (dense и sparse attention) в зависимости от длины входной последовательности. Для коротких текстов используется обычное плотное внимание, а для длинных автоматически активируется разреженное, что значительно снижает вычислительную нагрузку. Эксперименты показали, что метод работает в 4 раза быстрее стандартного подхода, сохраняя при этом 98-99% качества модели.'}, 'en': {'title': 'Efficient Long-Sequence Processing with InfLLM-V2', 'desc': 'The paper presents InfLLM-V2, a novel framework that improves the processing of long sequences in large language models by utilizing a dense-sparse switchable attention mechanism. This approach allows the model to efficiently transition between dense attention for short sequences and sparse attention for longer ones, addressing the computational and memory challenges of traditional self-attention methods. InfLLM-V2 maintains parameter efficiency by reusing dense attention parameters, which helps preserve performance while speeding up processing times. Experimental results show that InfLLM-V2 is significantly faster than standard dense attention, achieving high performance retention in long-context tasks.'}, 'zh': {'title': 'InfLLM-V2：高效处理长序列的注意力框架', 'desc': 'InfLLM-V2是一种密集-稀疏可切换注意力框架，旨在提高大型语言模型对长序列的处理能力。该框架通过在短序列和长序列之间高效切换密集和稀疏注意力机制，解决了传统Transformer在处理长序列时的计算和内存瓶颈。InfLLM-V2通过参数无关的架构修改重用密集注意力参数，确保短序列和长序列处理的一致性，同时在所有序列长度上保持计算效率。实验表明，InfLLM-V2在长上下文理解和推理任务中比密集注意力快4倍，同时保持了高达98.1%和99.7%的性能。'}}}, {'id': 'https://huggingface.co/papers/2509.24007', 'title': 'Sequential Diffusion Language Models', 'url': 'https://huggingface.co/papers/2509.24007', 'abstract': 'Sequential Diffusion Language Model (SDLM) enhances pre-trained autoregressive language models by adaptively determining generation length and maintaining KV-cache compatibility, achieving high efficiency and throughput.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion language models (DLMs) have strong theoretical efficiency but are limited by fixed-length decoding and incompatibility with key-value (KV) caches. Block diffusion mitigates these issues, yet still enforces a fixed block size and requires expensive training. We introduce Next Sequence Prediction (NSP), which unifies next-token and next-block prediction, enabling the model to adaptively determine the generation length at each step. When the length is fixed to 1, NSP reduces to standard next-token prediction. Building on NSP, we propose Sequential Diffusion Language Model (SDLM), which can retrofit pre-trained autoregressive language models (ALMs) at minimal cost. Specifically, SDLM performs diffusion inference within fixed-size mask blocks, but dynamically decodes consecutive subsequences based on model confidence, thereby preserving KV-cache compatibility and improving robustness to varying uncertainty and semantics across the sequence. Experiments show that SDLM matches or surpasses strong autoregressive baselines using only 3.5M training samples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the SDLM-32B model delivers even more pronounced efficiency gains, demonstrating the strong scalability potential of our modeling paradigm. Project page and codes: https://github.com/OpenGVLab/SDLM', 'score': 11, 'issue_id': 6153, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 сентября', 'en': 'September 28', 'zh': '9月28日'}, 'hash': 'd49df754c8588e2c', 'authors': ['Yangzhou Liu', 'Yue Cao', 'Hao Li', 'Gen Luo', 'Zhe Chen', 'Weiyun Wang', 'Xiaobo Liang', 'Biqing Qi', 'Lijun Wu', 'Changyao Tian', 'Yanting Zhang', 'Yuqiang Li', 'Tong Lu', 'Yu Qiao', 'Jifeng Dai', 'Wenhai Wang'], 'affiliations': ['Donghua University', 'Fudan University', 'Nanjing University', 'Shanghai AI Laboratory', 'Soochow University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.24007.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#training', '#optimization'], 'emoji': '🔀', 'ru': {'title': 'Адаптивная генерация с переменной длиной блоков', 'desc': 'В работе представлена Sequential Diffusion Language Model (SDLM) - новая архитектура, которая улучшает предобученные авторегрессионные языковые модели. Модель использует метод Next Sequence Prediction (NSP), который объединяет предсказание следующего токена и следующего блока токенов, позволяя адаптивно определять длину генерации на каждом шаге. SDLM выполняет диффузионный вывод внутри блоков фиксированного размера, но динамически декодирует последовательности на основе уверенности модели, сохраняя совместимость с KV-cache. Эксперименты показывают, что модель достигает производительности сильных авторегрессионных базовых моделей, используя всего 3.5M обучающих примеров, при этом обеспечивая в 2.1 раза более высокую пропускную способность.'}, 'en': {'title': 'Adaptive Text Generation with SDLM: Efficiency Meets Flexibility', 'desc': 'The Sequential Diffusion Language Model (SDLM) improves pre-trained autoregressive language models by allowing them to adaptively decide how long to generate text while ensuring compatibility with key-value (KV) caches. Traditional diffusion language models face challenges with fixed-length decoding, but SDLM introduces Next Sequence Prediction (NSP) to unify next-token and next-block predictions, enhancing flexibility. This approach enables the model to dynamically decode subsequences based on its confidence, which helps it handle varying levels of uncertainty and semantics. Experiments demonstrate that SDLM achieves high efficiency and throughput, outperforming strong baselines with significantly fewer training samples.'}, 'zh': {'title': '顺序扩散语言模型：高效生成的未来', 'desc': '顺序扩散语言模型（SDLM）通过自适应确定生成长度和保持KV缓存兼容性，增强了预训练的自回归语言模型，达到了高效性和吞吐量。扩散语言模型在理论上具有强大的效率，但受到固定长度解码和与KV缓存不兼容的限制。我们提出的下一序列预测（NSP）方法统一了下一个标记和下一个块的预测，使模型能够在每一步自适应地确定生成长度。实验表明，SDLM在仅使用350万训练样本的情况下，能够匹配或超越强大的自回归基线，同时实现比Qwen-2.5高出2.1的吞吐量。'}}}, {'id': 'https://huggingface.co/papers/2509.22799', 'title': 'VideoScore2: Think before You Score in Generative Video Evaluation', 'url': 'https://huggingface.co/papers/2509.22799', 'abstract': 'VideoScore2 is a multi-dimensional, interpretable framework for evaluating text-to-video generation, assessing visual quality, alignment, and consistency with detailed rationales.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in text-to-video generation have produced increasingly realistic and diverse content, yet evaluating such videos remains a fundamental challenge due to their multi-faceted nature encompassing visual quality, semantic alignment, and physical consistency. Existing evaluators and reward models are limited to single opaque scores, lack interpretability, or provide only coarse analysis, making them insufficient for capturing the comprehensive nature of video quality assessment. We present VideoScore2, a multi-dimensional, interpretable, and human-aligned framework that explicitly evaluates visual quality, text-to-video alignment, and physical/common-sense consistency while producing detailed chain-of-thought rationales. Our model is trained on a large-scale dataset VideoFeedback2 containing 27,168 human-annotated videos with both scores and reasoning traces across three dimensions, using a two-stage pipeline of supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to enhance analytical robustness. Extensive experiments demonstrate that VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc), while providing interpretable assessments that bridge the gap between evaluation and controllable generation through effective reward modeling for Best-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/', 'score': 11, 'issue_id': 6154, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': 'f70244e03286d72f', 'authors': ['Xuan He', 'Dongfu Jiang', 'Ping Nie', 'Minghao Liu', 'Zhengxuan Jiang', 'Mingyi Su', 'Wentao Ma', 'Junru Lin', 'Chun Ye', 'Yi Lu', 'Keming Wu', 'Benjamin Schneider', 'Quy Duc Do', 'Zhuofeng Li', 'Yiming Jia', 'Yuxuan Zhang', 'Guo Cheng', 'Haozhe Wang', 'Wangchunshu Zhou', 'Qunshu Lin', 'Yuanxing Zhang', 'Ge Zhang', 'Wenhao Huang', 'Wenhu Chen'], 'affiliations': ['AI', 'Abaka AI', 'Independent', 'M-A-P', 'Netmind.AI', 'University of Illinois Urbana-Champaign', 'University of Toronto', 'University of Waterloo', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.22799.jpg', 'data': {'categories': ['#benchmark', '#video', '#rlhf', '#interpretability', '#alignment'], 'emoji': '🎬', 'ru': {'title': 'Умная оценка AI-видео с объяснениями', 'desc': 'В статье представлена VideoScore2 — многомерная и интерпретируемая система для оценки качества видео, сгенерированных из текста. Модель оценивает визуальное качество, соответствие тексту и физическую согласованность видео, предоставляя детальные объяснения для каждой оценки. Система обучена на большом датасете VideoFeedback2 с 27,168 размеченными человеком видео и использует двухэтапный подход с обучением с подкреплением. Эксперименты показывают превосходную производительность модели на различных бенчмарках с возможностью интерпретации результатов.'}, 'en': {'title': 'VideoScore2: A Clearer Lens for Evaluating Text-to-Video Generation', 'desc': 'VideoScore2 is a novel framework designed to evaluate text-to-video generation by focusing on three key aspects: visual quality, alignment with the input text, and physical consistency. Unlike previous models that provide single scores without interpretability, VideoScore2 offers detailed rationales for its evaluations, making it easier to understand the reasoning behind the scores. The framework is trained on a large dataset of human-annotated videos, utilizing a two-stage training process that combines supervised fine-tuning and reinforcement learning to improve its analytical capabilities. Experimental results show that VideoScore2 outperforms existing benchmarks, providing a more comprehensive and interpretable assessment of generated videos.'}, 'zh': {'title': 'VideoScore2：多维度视频生成评估新标准', 'desc': 'VideoScore2是一个多维度、可解释的框架，用于评估文本到视频生成的质量。它不仅评估视觉质量，还考虑文本与视频的对齐性和物理一致性，并提供详细的推理过程。现有的评估模型通常只能给出单一的分数，缺乏可解释性，无法全面捕捉视频质量的复杂性。通过在大型数据集VideoFeedback2上训练，VideoScore2展示了在多个基准测试中优越的性能，同时提供了可解释的评估结果。'}}}, {'id': 'https://huggingface.co/papers/2509.22824', 'title': 'Critique-Coder: Enhancing Coder Models by Critique Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2509.22824', 'abstract': 'Critique Reinforcement Learning (CRL) enhances LLMs by teaching them to generate critiques, leading to improved performance on code generation and logic reasoning tasks compared to standard RL.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning (RL) has emerged as a popular training paradigm, particularly when paired with reasoning models. While effective, it primarily focuses on generating responses and lacks mechanisms to explicitly foster critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT) and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly teaching LLMs how to critique. Motivated by them, we propose Critique Reinforcement Learning (CRL), where the model is tasked with generating a critique for a given (question, solution) pair. The reward is determined solely by whether the final judgment label c in {True, False} of the generated critique aligns with the ground-truth judgment c^*. Building on this point, we introduce Critique-Coder, which is trained on a hybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL data. We fine-tune multiple models (Critique-Coder) and evaluate them on different benchmarks to show their advantages over RL-only models. We show that Critique-Coder consistently outperforms RL-only baselines on all the evaluated benchmarks. Notably, our Critique-Coder-8B can reach over 60\\% on LiveCodeBench (v5), outperforming other reasoning models like DeepCoder-14B and GPT-o1. Beyond code generation, Critique-Coder also demonstrates enhanced general reasoning abilities, as evidenced by its better performance on logic reasoning tasks from the BBEH dataset. This indicates that the application of CRL on coding datasets enhances general reasoning and critique abilities, which are transferable across a broad range of tasks. Hence, we believe that CRL works as a great complement to standard RL for LLM reasoning.', 'score': 10, 'issue_id': 6154, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '18f3e09fc9d2fcc9', 'authors': ['Chi Ruan', 'Dongfu Jiang', 'Yubo Wang', 'Wenhu Chen'], 'affiliations': ['University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2509.22824.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#benchmark', '#rl', '#rlhf', '#reasoning', '#training'], 'emoji': '🔍', 'ru': {'title': 'Критическое мышление делает AI умнее', 'desc': 'В статье представлен метод Critique Reinforcement Learning (CRL), который обучает LLM генерировать критические оценки решений задач. Авторы создали модель Critique-Coder, которая комбинирует стандартное обучение с подкреплением и CRL в пропорции 80/20. Модель показала превосходные результаты в генерации кода и логических рассуждениях по сравнению с обычным RL. Исследование демонстрирует, что способность к критическому анализу улучшает общие рассуждения модели.'}, 'en': {'title': 'Enhancing LLMs with Critique Reinforcement Learning', 'desc': 'Critique Reinforcement Learning (CRL) improves large language models (LLMs) by training them to generate critiques for given question-solution pairs. This method enhances their performance in tasks like code generation and logic reasoning, surpassing traditional reinforcement learning (RL) approaches. The model receives rewards based on the accuracy of its critique compared to a ground-truth judgment, fostering better reasoning skills. By integrating CRL with standard RL, the proposed Critique-Coder model shows significant improvements across various benchmarks, demonstrating its effectiveness in both coding and general reasoning tasks.'}, 'zh': {'title': '批评强化学习：提升模型推理能力的关键', 'desc': '批评强化学习（CRL）通过教导大型语言模型（LLM）生成批评，提升了它们在代码生成和逻辑推理任务上的表现。与传统的强化学习（RL）相比，CRL专注于生成批评，从而促进模型的反思能力。我们提出的批评编码器（Critique-Coder）结合了RL和CRL的优点，经过训练后在多个基准测试中表现优于仅使用RL的模型。研究表明，CRL不仅提高了代码生成能力，还增强了模型在逻辑推理任务上的表现，显示出其广泛的适用性。'}}}, {'id': 'https://huggingface.co/papers/2509.22572', 'title': 'Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs\n  at Test Time', 'url': 'https://huggingface.co/papers/2509.22572', 'abstract': 'Dynamic Experts Search (DES) enhances large language models by controlling expert activation during inference, improving accuracy and stability without additional cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocating additional computation during inference. However, existing approaches primarily rely on output-level sampling while overlooking the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we observe that varying the number of activated experts yields complementary solution sets with stable accuracy, revealing a new and underexplored source of diversity. Motivated by this observation, we propose Dynamic Experts Search (DES), a TTS strategy that elevates expert activation into a controllable dimension of the search space. DES integrates two key components: (1) Dynamic MoE, which enables direct control of expert counts during inference to generate diverse reasoning trajectories without additional cost; and (2) Expert Configuration Inheritance, which preserves consistent expert counts within a reasoning path while varying them across runs, thereby balancing stability and diversity throughout the search. Extensive experiments across MoE architectures, verifiers and reasoning benchmarks (i.e., math, code and knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional cost. These results highlight DES as a practical and scalable form of architecture-aware TTS, illustrating how structural flexibility in modern LLMs can advance reasoning.', 'score': 10, 'issue_id': 6154, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '9bf9aff7575fc967', 'authors': ['Yixuan Han', 'Fan Ma', 'Ruijie Quan', 'Yi Yang'], 'affiliations': ['Nanyang Technological University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.22572.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#training', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'Динамический поиск экспертов: новое измерение для улучшения рассуждений LLM', 'desc': 'Исследование предлагает метод Dynamic Experts Search (DES), который улучшает рассуждения больших языковых моделей путем контроля активации экспертов во время инференса. В моделях типа Mixture-of-Experts авторы обнаружили, что изменение количества активных экспертов создает дополнительное разнообразие решений при сохранении точности. DES включает два компонента: Dynamic MoE для прямого управления числом экспертов и Expert Configuration Inheritance для баланса стабильности и разнообразия. Эксперименты показывают, что метод превосходит базовые подходы Test-Time Scaling без дополнительных вычислительных затрат.'}, 'en': {'title': 'Unlocking Diverse Reasoning with Dynamic Experts Search', 'desc': "Dynamic Experts Search (DES) is a method that improves large language models by managing which experts are activated during their operation, leading to better accuracy and stability without extra costs. It builds on the idea of Test-Time Scaling (TTS) but focuses on the model's architecture rather than just output sampling. By allowing control over the number of active experts, DES creates diverse reasoning paths while maintaining consistent expert usage within each path. Experiments show that DES outperforms traditional TTS methods, demonstrating its effectiveness in enhancing reasoning capabilities in various tasks."}, 'zh': {'title': '动态专家搜索：提升推理的准确性与稳定性', 'desc': '动态专家搜索（DES）通过在推理过程中控制专家的激活，增强了大型语言模型的性能，提高了准确性和稳定性，而无需额外成本。该方法利用混合专家（MoE）架构的灵活性，发现激活专家数量的变化可以产生互补的解决方案集。DES的两个关键组成部分是动态MoE和专家配置继承，前者允许在推理时直接控制专家数量，后者则在推理路径中保持一致的专家数量。通过广泛的实验，DES在多个基准测试中表现优于现有的测试时扩展方法，展示了其作为一种实用且可扩展的架构感知策略的潜力。'}}}, {'id': 'https://huggingface.co/papers/2509.25176', 'title': 'SIRI: Scaling Iterative Reinforcement Learning with Interleaved\n  Compression', 'url': 'https://huggingface.co/papers/2509.25176', 'abstract': 'SIRI, a reinforcement learning approach with interleaved compression and expansion, enhances the efficiency and accuracy of large reasoning models by dynamically adjusting the reasoning budget.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved Compression, a simple yet effective RL approach for Large Reasoning Models (LRMs) that enables more efficient and accurate reasoning. Existing studies have observed repetitive thinking patterns in LRMs, and attempts to reduce them often come at the cost of performance. In this paper, we show that this trade-off can be overcome through a training regime that iteratively alternates between compressing and expanding the reasoning budget, by dynamically adjusting the maximum rollout length during training. The compression phase cuts the rollout length, forcing the model to make precise and valuable decisions within a limited context, which effectively reduces redundant tokens and increases reasoning density. The expansion phase then relaxes the length limit, providing space for the model to explore and plan in long-horizon settings. Remarkably, we find that after each compression-expansion cycle, the model\'s performance improves even as its output length decreases, steadily pushing it closer to the Pareto frontier in the performance-efficiency trade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves performance on AIME24 by 43.2% while reducing token usage by 46.9% after three iterations, and SIRI-high achieves the highest accuracy compared to all other methods (Figure 1). Our findings shed light on the potential of periodically oscillating the LRM\'s output truncation length during training to dynamically balance exploration and efficiency in reasoning, converging towards an optimal "sweet spot" between the two. Our models are publicly available.', 'score': 9, 'issue_id': 6153, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '4aad706cb9d9d40b', 'authors': ['Haoming Wen', 'Yushi Bai', 'Juanzi Li', 'Jie Tang'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.25176.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#open_source', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'Умные рассуждения через сжатие и расширение контекста', 'desc': 'В статье представлен метод SIRI для обучения больших языковых моделей рассуждения с использованием reinforcement learning. Подход чередует фазы сжатия и расширения контекста рассуждений, динамически регулируя максимальную длину генерируемого текста во время тренировки. В фазе сжатия модель учится принимать точные решения в ограниченном контексте, убирая избыточные токены, а в фазе расширения получает возможность для долгосрочного планирования. Результаты показывают улучшение точности на 43.2% при одновременном снижении использования токенов на 46.9%.'}, 'en': {'title': 'SIRI: Balancing Efficiency and Accuracy in Large Reasoning Models', 'desc': 'The paper presents SIRI, a novel reinforcement learning method designed to enhance the efficiency and accuracy of large reasoning models (LRMs). It addresses the issue of repetitive thinking patterns by implementing a training strategy that alternates between compressing and expanding the reasoning budget. During the compression phase, the model focuses on making precise decisions with a limited context, which reduces redundancy and increases reasoning density. The expansion phase allows for longer planning horizons, leading to improved performance while decreasing output length, ultimately achieving a better balance between exploration and efficiency in reasoning.'}, 'zh': {'title': '动态平衡推理效率与准确性', 'desc': '本文介绍了一种名为SIRI的强化学习方法，它通过交替压缩和扩展推理预算，提高了大型推理模型的效率和准确性。SIRI通过动态调整训练过程中的最大展开长度，克服了现有研究中观察到的重复思维模式所带来的性能损失。压缩阶段缩短了展开长度，迫使模型在有限的上下文中做出精确决策，从而减少冗余标记并提高推理密度。扩展阶段则放宽了长度限制，为模型在长时间范围内探索和规划提供了空间，最终实现了性能和效率的最佳平衡。'}}}, {'id': 'https://huggingface.co/papers/2509.25161', 'title': 'Rolling Forcing: Autoregressive Long Video Diffusion in Real Time', 'url': 'https://huggingface.co/papers/2509.25161', 'abstract': 'Rolling Forcing is a novel video generation technique that reduces error accumulation in long video streams by using joint denoising, attention sink mechanism, and efficient training with non-overlapping windows.  \t\t\t\t\tAI-generated summary \t\t\t\t Streaming video generation, as one fundamental component in interactive world models and neural game engines, aims to generate high-quality, low-latency, and temporally coherent long video streams. However, most existing work suffers from severe error accumulation that often significantly degrades the generated stream videos over long horizons. We design Rolling Forcing, a novel video generation technique that enables streaming long videos with minimal error accumulation. Rolling Forcing comes with three novel designs. First, instead of iteratively sampling individual frames, which accelerates error propagation, we design a joint denoising scheme that simultaneously denoises multiple frames with progressively increasing noise levels. This design relaxes the strict causality across adjacent frames, effectively suppressing error growth. Second, we introduce the attention sink mechanism into the long-horizon stream video generation task, which allows the model to keep key value states of initial frames as a global context anchor and thereby enhances long-term global consistency. Third, we design an efficient training algorithm that enables few-step distillation over largely extended denoising windows. This algorithm operates on non-overlapping windows and mitigates exposure bias conditioned on self-generated histories. Extensive experiments show that Rolling Forcing enables real-time streaming generation of multi-minute videos on a single GPU, with substantially reduced error accumulation.', 'score': 9, 'issue_id': 6154, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': 'a2272753ec4619e6', 'authors': ['Kunhao Liu', 'Wenbo Hu', 'Jiale Xu', 'Ying Shan', 'Shijian Lu'], 'affiliations': ['ARC Lab, Tencent PCG', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2509.25161.jpg', 'data': {'categories': ['#long_context', '#optimization', '#games', '#video'], 'emoji': '🎬', 'ru': {'title': 'Стриминг длинных видео без накопления ошибок', 'desc': 'Rolling Forcing - это новая техника генерации видео, которая решает проблему накопления ошибок при создании длинных видеопотоков. Метод использует совместное шумоподавление нескольких кадров одновременно, что ослабляет строгую причинность между соседними кадрами. Механизм attention sink сохраняет ключевую информацию из начальных кадров как глобальный контекст для поддержания долгосрочной согласованности. Алгоритм позволяет генерировать многоминутные видео в реальном времени на одной GPU с минимальным накоплением ошибок.'}, 'en': {'title': 'Minimizing Errors in Long Video Generation with Rolling Forcing', 'desc': 'Rolling Forcing is a new technique for generating long video streams that minimizes error accumulation, which is a common problem in video generation. It uses a joint denoising approach to process multiple frames at once, reducing the propagation of errors that can occur when frames are generated one by one. The method also incorporates an attention sink mechanism to maintain important information from earlier frames, ensuring consistency throughout the video. Additionally, it features an efficient training process that allows for quick learning over extended periods, making it possible to generate high-quality videos in real-time.'}, 'zh': {'title': '减少视频生成中的错误累积', 'desc': 'Rolling Forcing是一种新的视频生成技术，旨在减少长视频流中的错误累积。它通过联合去噪、注意力汇聚机制和高效的非重叠窗口训练来实现这一目标。该方法同时去噪多个帧，抑制错误传播，并保持初始帧的关键状态以增强长期一致性。实验表明，Rolling Forcing能够在单个GPU上实时生成多分钟的视频，显著降低错误累积。'}}}, {'id': 'https://huggingface.co/papers/2509.24473', 'title': "Euclid's Gift: Enhancing Spatial Perception and Reasoning in\n  Vision-Language Models via Geometric Surrogate Tasks", 'url': 'https://huggingface.co/papers/2509.24473', 'abstract': 'Geometry-centric fine-tuning using the Euclid30K dataset significantly improves spatial reasoning abilities in multimodal large language models across multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs).To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. To enable the model to acquire and apply Euclidean principles from these geometry problems, we employed Group Relative Policy Optimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family, inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models rose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them, RoboBrain2.0-Euclid-7B achieves 49.6\\% accuracy, surpassing the previous state-of-the-art model, Spatial-MLLM.To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift.', 'score': 9, 'issue_id': 6153, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '1cf3d7b11e8af378', 'authors': ['Shijie Lian', 'Changti Wu', 'Laurence Tianruo Yang', 'Hang Yuan', 'Bin Yu', 'Lei Zhang', 'Kai Chen'], 'affiliations': ['East China Normal University', 'Huazhong University of Science and Technology', 'Zhengzhou University', 'Zhongguancun Academy', 'Zhongguancun Institute of Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2509.24473.jpg', 'data': {'categories': ['#training', '#dataset', '#reasoning', '#multimodal', '#benchmark', '#transfer_learning'], 'emoji': '📐', 'ru': {'title': 'Геометрия как ключ к пространственному интеллекту AI', 'desc': 'Исследователи создали датасет Euclid30K с 30 тысячами задач по планиметрии и стереометрии для обучения мультимодальных LLM пространственному мышлению. Используя метод Group Relative Policy Optimization, они дообучили модели семейств Qwen2.5VL и RoboBrain2.0 на решении геометрических задач. Эксперименты показали, что такой подход значительно улучшает способности моделей к пространственному рассуждению без дополнительной адаптации под конкретные задачи. Модель RoboBrain2.0-Euclid-7B достигла точности 49.6% на бенчмарке VSI-Bench, превзойдя предыдущие результаты.'}, 'en': {'title': 'Boosting Spatial Reasoning in MLLMs with Geometry Fine-Tuning', 'desc': 'This paper presents a method to enhance the spatial reasoning capabilities of Multimodal Large Language Models (MLLMs) by using a specially designed dataset called Euclid30K, which contains geometry problems. The authors employed Group Relative Policy Optimization (GRPO) to fine-tune models like Qwen2.5VL and RoboBrain2.0, enabling them to better understand and apply Euclidean geometry principles. The results showed significant improvements in spatial reasoning performance across multiple benchmarks, with the RoboBrain2.0-Euclid-7B model achieving a new state-of-the-art accuracy. This research highlights the effectiveness of geometry-centric fine-tuning in developing transferable spatial skills in vision-language models.'}, 'zh': {'title': '几何微调提升空间推理能力', 'desc': '本研究提出了一种几何中心的微调方法，利用Euclid30K数据集显著提升了多模态大型语言模型的空间推理能力。我们构建了一个包含约3万道平面和立体几何问题的多模态数据集，以帮助模型学习和应用欧几里得几何原理。通过使用群体相对策略优化（GRPO）对模型进行微调，模型能够识别形状、计数、关联实体，并进行多步推理。实验结果表明，经过训练后，模型在多个空间推理基准测试中的表现显著提升，尤其是RoboBrain2.0-Euclid-7B模型的准确率达到了49.6%，超越了之前的最佳模型。'}}}, {'id': 'https://huggingface.co/papers/2509.23285', 'title': 'Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference\n  Learning', 'url': 'https://huggingface.co/papers/2509.23285', 'abstract': "Tool-Light framework improves large language models' tool-integrated reasoning efficiency and accuracy by leveraging information entropy and a two-stage fine-tuning process.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to improve their internal reasoning ability by integrating external tools. However, models employing TIR often display suboptimal behaviors, such as insufficient or excessive tool usage and overthinking after tool calls. The challenge of incentivizing LLMs to perform TIR efficiently and accurately, while stabilizing the reasoning process, remains an open question. In this paper, we start by exploring the impact of tool calls on model reasoning from the perspective of information entropy. Our findings indicate that tool call results lead to a distinct change in the information entropy of subsequent reasoning, with the overall entropy of the reasoning chain varying based on the number of tool calls. Building on these insights, we propose Tool-Light, a framework designed to encourage LLMs to perform TIR efficiently and accurately. Our framework includes dataset construction and multi-stage fine-tuning. For dataset construction, we employ continuous self-evolved sampling using the fine-tuned model, integrating both vanilla sampling and entropy-guided sampling. Besides, we establish strict criteria for selecting positive-negative pairs during sampling. The training process involves a two-stage approach, comprising Supervised Fine-Tuning (SFT) and Self-Evolved Direct Preference Optimization (DPO). Experimental results on 10 datasets demonstrate the effectiveness of Tool-Light, significantly improving the model's efficiency in executing TIR tasks.", 'score': 8, 'issue_id': 6152, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': 'a2cf471319463564', 'authors': ['Yifei Chen', 'Guanting Dong', 'Zhicheng Dou'], 'affiliations': ['Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2509.23285.jpg', 'data': {'categories': ['#dataset', '#optimization', '#reasoning', '#training', '#rlhf'], 'emoji': '🔧', 'ru': {'title': 'Умное использование инструментов через энтропию рассуждений', 'desc': 'Исследователи предлагают фреймворк Tool-Light для улучшения интеграции внешних инструментов в рассуждения больших языковых моделей. Анализируя информационную энтропию, они обнаружили, что вызовы инструментов влияют на качество последующих рассуждений модели. Фреймворк включает двухэтапное обучение с использованием supervised fine-tuning и direct preference optimization с самоэволюционной выборкой данных. Эксперименты на 10 датасетах показали значительное улучшение эффективности и точности использования инструментов в задачах рассуждения.'}, 'en': {'title': 'Enhancing Tool-Integrated Reasoning with Tool-Light Framework', 'desc': 'The Tool-Light framework enhances the efficiency and accuracy of large language models (LLMs) in tool-integrated reasoning (TIR) by utilizing information entropy and a two-stage fine-tuning process. It addresses common issues in TIR, such as improper tool usage and overthinking, by analyzing how tool calls affect the reasoning process. The framework includes a novel dataset construction method that combines vanilla and entropy-guided sampling, along with strict criteria for selecting training pairs. Experimental results show that Tool-Light significantly improves LLM performance across multiple datasets, making TIR more effective.'}, 'zh': {'title': 'Tool-Light框架：提升推理效率与准确性', 'desc': '本文提出了一种名为Tool-Light的框架，旨在提高大型语言模型在工具集成推理（TIR）中的效率和准确性。通过信息熵的视角，研究了工具调用对模型推理的影响，发现工具调用结果会显著改变后续推理的信息熵。Tool-Light框架包括数据集构建和多阶段微调，采用自我演化采样和严格的正负样本选择标准。实验结果表明，该框架在10个数据集上显著提升了模型执行TIR任务的效率。'}}}, {'id': 'https://huggingface.co/papers/2509.25191', 'title': 'VGGT-X: When VGGT Meets Dense Novel View Synthesis', 'url': 'https://huggingface.co/papers/2509.25191', 'abstract': 'VGGT-X addresses VRAM and output quality issues in scaling 3D Foundation Models for dense Novel View Synthesis without relying on COLMAP.  \t\t\t\t\tAI-generated summary \t\t\t\t We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS). Despite significant progress in Novel View Synthesis powered by NeRF and 3DGS, current approaches remain reliant on accurate 3D attributes (e.g., camera poses and point clouds) acquired from Structure-from-Motion (SfM), which is often slow and fragile in low-texture or low-overlap captures. Recent 3DFMs showcase orders of magnitude speedup over the traditional pipeline and great potential for online NVS. But most of the validation and conclusions are confined to sparse-view settings. Our study reveals that naively scaling 3DFMs to dense views encounters two fundamental barriers: dramatically increasing VRAM burden and imperfect outputs that degrade initialization-sensitive 3D training. To address these barriers, we introduce VGGT-X, incorporating a memory-efficient VGGT implementation that scales to 1,000+ images, an adaptive global alignment for VGGT output enhancement, and robust 3DGS training practices. Extensive experiments show that these measures substantially close the fidelity gap with COLMAP-initialized pipelines, achieving state-of-the-art results in dense COLMAP-free NVS and pose estimation. Additionally, we analyze the causes of remaining gaps with COLMAP-initialized rendering, providing insights for the future development of 3D foundation models and dense NVS. Our project page is available at https://dekuliutesla.github.io/vggt-x.github.io/', 'score': 7, 'issue_id': 6155, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '3c76ba3e06f19be4', 'authors': ['Yang Liu', 'Chuanchen Luo', 'Zimo Tang', 'Junran Peng', 'Zhaoxiang Zhang'], 'affiliations': ['Huazhong University of Science and Technology', 'NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences', 'Shandong University', 'University of Chinese Academy of Sciences', 'University of Science and Technology Beijing'], 'pdf_title_img': 'assets/pdf/title_img/2509.25191.jpg', 'data': {'categories': ['#games', '#3d', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'Плотный синтез видов без COLMAP с помощью 3D Foundation Models', 'desc': 'Исследователи изучают применение 3D Foundation Models для плотного синтеза новых видов (Novel View Synthesis). Традиционные методы зависят от медленных и ненадежных алгоритмов Structure-from-Motion для получения камерных поз и облаков точек. Масштабирование 3DFM на плотные виды сталкивается с проблемами нехватки видеопамяти и низкого качества результатов. Предложенный метод VGGT-X решает эти проблемы через эффективную реализацию памяти, адаптивное выравнивание и улучшенные практики обучения 3DGS.'}, 'en': {'title': 'VGGT-X: Scaling 3D Models for High-Quality View Synthesis Without COLMAP', 'desc': 'VGGT-X tackles the challenges of VRAM usage and output quality when scaling 3D Foundation Models (3DFMs) for dense Novel View Synthesis (NVS) without using COLMAP. Traditional methods rely on accurate 3D attributes from Structure-from-Motion, which can be slow and unreliable in certain conditions. The paper introduces a memory-efficient implementation and adaptive techniques to enhance output quality, allowing for the processing of over 1,000 images. Experimental results demonstrate that VGGT-X significantly improves fidelity in dense NVS and pose estimation compared to existing methods, while also providing insights for future advancements in 3D modeling.'}, 'zh': {'title': 'VGGT-X：高效解决密集新视图合成中的挑战', 'desc': 'VGGT-X 研究了在不依赖 COLMAP 的情况下，如何解决在密集新视图合成中扩展 3D 基础模型时的 VRAM 和输出质量问题。尽管现有的新视图合成方法取得了显著进展，但仍然依赖于从运动结构中获取的准确 3D 属性，这在低纹理或低重叠的场景中往往表现不佳。我们的研究表明，简单地将 3D 基础模型扩展到密集视图会面临 VRAM 负担增加和输出质量下降的两个主要障碍。为了解决这些问题，我们提出了 VGGT-X，采用了内存高效的实现和自适应全局对齐技术，显著提高了密集新视图合成的效果。'}}}, {'id': 'https://huggingface.co/papers/2509.23196', 'title': 'From Harm to Help: Turning Reasoning In-Context Demos into Assets for\n  Reasoning LMs', 'url': 'https://huggingface.co/papers/2509.23196', 'abstract': "Insight-to-Solve (I2S) and its refined version (I2S+) improve few-shot chain-of-thought performance by converting demonstrations into reusable insights, outperforming direct answering and scaling methods across various models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reasoning LLMs (RLMs), especially those trained with verifier-based reinforcement learning, often perform worse with few-shot CoT than with direct answering. We revisit this paradox using high-quality reasoning traces from DeepSeek-R1 as demonstrations and find that adding more exemplars consistently degrades accuracy, even when demonstrations are optimal. A detailed analysis reveals two mechanisms behind this decline: (i) semantic misguidance, where high textual similarity leads the model to treat the target as the same as the exemplar and to copy intermediate steps verbatim; and (ii) strategy transfer failure, where the model struggles to extract useful reasoning strategies and apply them to target questions. Guided by these, we introduce Insight-to-Solve (I2S), a sequential test-time procedure that turns demonstrations into explicit, reusable insights and derives a target-specific reasoning trace; optionally, the reasoning is self-refined for coherence and correctness (I2S+). Extensive experiments on diverse benchmarks show that I2S and I2S+ consistently outperform both direct answering and test-time scaling baselines across open- and closed-source models. Even for GPT models, our method helps: on AIME'25, GPT-4.1 rises by +14.0%, and o1-mini improves by +2.7% on AIME and +1.7% on GPQA, indicating that in-context demonstrations can be harnessed effectively via insight-refine-solve framework.", 'score': 7, 'issue_id': 6153, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': '05ad7f2b555ea8c1', 'authors': ['Haonan Wang', 'Weida Liang', 'Zihang Fu', 'Nie Zheng', 'Yifan Zhang', 'Yao Tong', 'Tongyao Zhu', 'Hao Jiang', 'Chuang Li', 'Jiaying Wu', 'Kenji Kawaguchi'], 'affiliations': ['MiroMind AI', 'National University of Singapore', 'University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2509.23196.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#optimization', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'От примеров к инсайтам: новый подход к few-shot рассуждениям', 'desc': 'Исследователи обнаружили, что современные reasoning LLM часто показывают худшие результаты при использовании few-shot chain-of-thought по сравнению с прямыми ответами. Анализ выявил две основные проблемы: семантическое искажение, когда модель копирует промежуточные шаги из-за текстового сходства, и неудачный перенос стратегий рассуждения. Для решения этих проблем предложен метод Insight-to-Solve (I2S), который преобразует демонстрации в явные переиспользуемые инсайты и создает специфичные для задачи цепочки рассуждений. Эксперименты показали, что I2S превосходит как прямое отвечание, так и методы масштабирования на различных бенчмарках.'}, 'en': {'title': 'Transforming Demonstrations into Reusable Insights for Better Reasoning', 'desc': 'The paper introduces Insight-to-Solve (I2S) and its enhanced version I2S+, which improve few-shot chain-of-thought (CoT) performance in reasoning language models (RLMs). It identifies issues with traditional few-shot learning, such as semantic misguidance and strategy transfer failure, which can lead to decreased accuracy when using multiple demonstrations. I2S transforms these demonstrations into reusable insights, allowing models to generate more accurate and coherent reasoning traces for specific tasks. Experimental results show that I2S and I2S+ significantly outperform direct answering methods and scaling techniques across various models and benchmarks.'}, 'zh': {'title': '洞察解决：提升少样本推理的有效方法', 'desc': '本文提出了Insight-to-Solve (I2S) 及其改进版I2S+，旨在提升少样本推理链的表现。通过将示例转化为可重用的洞察，I2S和I2S+在多种模型上超越了直接回答和扩展方法。研究发现，增加示例数量会导致准确性下降，主要原因包括语义误导和策略转移失败。通过引导模型提取有效的推理策略，I2S能够生成针对特定问题的推理轨迹，从而提高推理的连贯性和正确性。'}}}, {'id': 'https://huggingface.co/papers/2509.24193', 'title': 'AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced\n  Self-Play', 'url': 'https://huggingface.co/papers/2509.24193', 'abstract': "AceSearcher, a cooperative self-play framework, enhances a large language model's reasoning ability by alternating between decomposing queries and solving them, outperforming state-of-the-art models with fewer parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9x more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks. Our code will be published at https://github.com/ritaranx/AceSearcher and https://huggingface.co/AceSearcher.", 'score': 6, 'issue_id': 6153, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': 'f817dc5569e19eb9', 'authors': ['Ran Xu', 'Yuchen Zhuang', 'Zihan Dong', 'Jonathan Wang', 'Yue Yu', 'Joyce C. Ho', 'Linjun Zhang', 'Haoyu Wang', 'Wenqi Shi', 'Carl Yang'], 'affiliations': ['Emory University', 'Georgia Institute of Technology', 'Rutgers University', 'SUNY Albany', 'UT Southwestern Medical Center'], 'pdf_title_img': 'assets/pdf/title_img/2509.24193.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#optimization', '#small_models'], 'emoji': '🔍', 'ru': {'title': 'Кооперативная самоигра для эффективного поиска и рассуждений', 'desc': 'В статье представлена AceSearcher — framework для кооперативной самоигры, который улучшает способности LLM к рассуждениям. Модель попеременно выполняет две роли: декомпозитор разбивает сложные запросы на части, а решатель интегрирует найденную информацию для генерации ответов. Система использует supervised fine-tuning на смеси задач поиска и рассуждений, а затем reinforcement learning для оптимизации точности финальных ответов. AceSearcher показывает улучшение на 7.6% по сравнению с современными методами и демонстрирует исключительную эффективность — модель в 32B параметров соперничает с DeepSeek-V3, используя менее 5% его параметров.'}, 'en': {'title': 'AceSearcher: Efficient Reasoning with Less Complexity', 'desc': 'AceSearcher is a novel framework that improves the reasoning capabilities of large language models (LLMs) by using a cooperative self-play approach. It alternates between two roles: a decomposer that simplifies complex queries and a solver that uses retrieved information to generate answers. This method combines supervised fine-tuning with reinforcement learning to enhance accuracy without needing extra annotations. The results show that AceSearcher outperforms existing models, achieving better performance with significantly fewer parameters, demonstrating its efficiency in handling complex reasoning tasks.'}, 'zh': {'title': 'AceSearcher：高效推理的新方法', 'desc': 'AceSearcher是一种合作自我对弈框架，通过交替分解查询和解决问题，增强了大型语言模型的推理能力。该框架训练一个大型语言模型（LLM）在分解者和求解者之间切换，分解者负责将复杂查询拆解，而求解者则整合检索到的上下文生成答案。AceSearcher结合了多种搜索、推理和分解任务的监督微调与针对最终答案准确性的强化微调，消除了对中间注释的需求。实验结果表明，AceSearcher在三个推理密集型任务上超越了最先进的基线，展现出卓越的效率和效果。'}}}, {'id': 'https://huggingface.co/papers/2509.23951', 'title': 'HunyuanImage 3.0 Technical Report', 'url': 'https://huggingface.co/papers/2509.23951', 'abstract': 'HunyuanImage 3.0, a multimodal model with an autoregressive framework, achieves state-of-the-art performance in image generation and text-image alignment using a Mixture-of-Experts architecture with over 80 billion parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t We present HunyuanImage 3.0, a native multimodal model that unifies multimodal understanding and generation within an autoregressive framework, with its image generation module publicly available. The achievement of HunyuanImage 3.0 relies on several key components, including meticulous data curation, advanced architecture design, a native Chain-of-Thoughts schema, progressive model pre-training, aggressive model post-training, and an efficient infrastructure that enables large-scale training and inference. With these advancements, we successfully trained a Mixture-of-Experts (MoE) model comprising over 80 billion parameters in total, with 13 billion parameters activated per token during inference, making it the largest and most powerful open-source image generative model to date. We conducted extensive experiments and the results of automatic and human evaluation of text-image alignment and visual quality demonstrate that HunyuanImage 3.0 rivals previous state-of-the-art models. By releasing the code and weights of HunyuanImage 3.0, we aim to enable the community to explore new ideas with a state-of-the-art foundation model, fostering a dynamic and vibrant multimodal ecosystem. All open source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanImage-3.0', 'score': 6, 'issue_id': 6154, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 сентября', 'en': 'September 28', 'zh': '9月28日'}, 'hash': 'ad3b7b8b923097f2', 'authors': ['Siyu Cao', 'Hangting Chen', 'Peng Chen', 'Yiji Cheng', 'Yutao Cui', 'Xinchi Deng', 'Ying Dong', 'Kipper Gong', 'Tianpeng Gu', 'Xiusen Gu', 'Tiankai Hang', 'Duojun Huang', 'Jie Jiang', 'Zhengkai Jiang', 'Weijie Kong', 'Changlin Li', 'Donghao Li', 'Junzhe Li', 'Xin Li', 'Yang Li', 'Zhenxi Li', 'Zhimin Li', 'Jiaxin Lin', 'Linus', 'Lucaz Liu', 'Shu Liu', 'Songtao Liu', 'Yu Liu', 'Yuhong Liu', 'Yanxin Long', 'Fanbin Lu', 'Qinglin Lu', 'Yuyang Peng', 'Yuanbo Peng', 'Xiangwei Shen', 'Yixuan Shi', 'Jiale Tao', 'Yangyu Tao', 'Qi Tian', 'Pengfei Wan', 'Chunyu Wang', 'Kai Wang', 'Lei Wang', 'Linqing Wang', 'Lucas Wang', 'Qixun Wang', 'Weiyan Wang', 'Hao Wen', 'Bing Wu', 'Jianbing Wu', 'Yue Wu', 'Senhao Xie', 'Fang Yang', 'Miles Yang', 'Xiaofeng Yang', 'Xuan Yang', 'Zhantao Yang', 'Jingmiao Yu', 'Zheng Yuan', 'Chao Zhang', 'Jian-Wei Zhang', 'Peizhen Zhang', 'Shi-Xue Zhang', 'Tao Zhang', 'Weigang Zhang', 'Yepeng Zhang', 'Yingfang Zhang', 'Zihao Zhang', 'Zijian Zhang', 'Penghao Zhao', 'Zhiyuan Zhao', 'Xuefei Zhe', 'Jianchen Zhu', 'Zhao Zhong'], 'affiliations': ['Tencent Hunyuan Foundation Model Team'], 'pdf_title_img': 'assets/pdf/title_img/2509.23951.jpg', 'data': {'categories': ['#architecture', '#data', '#open_source', '#diffusion', '#multimodal', '#training'], 'emoji': '🎨', 'ru': {'title': 'Гигантская мультимодальная модель для генерации изображений с 80 миллиардами параметров', 'desc': 'HunyuanImage 3.0 - это мультимодальная модель с авторегрессивной архитектурой, которая объединяет понимание и генерацию изображений в одной системе. Модель использует архитектуру Mixture-of-Experts с более чем 80 миллиардами параметров, из которых 13 миллиардов активируются при обработке каждого токена. Система достигает state-of-the-art результатов в генерации изображений и выравнивании текста с изображениями благодаря тщательной курации данных, продвинутому дизайну архитектуры и нативной схеме Chain-of-Thoughts. Это самая большая и мощная open-source модель для генерации изображений на сегодняшний день, код и веса которой доступны для сообщества.'}, 'en': {'title': 'Unleashing the Power of Multimodal AI with HunyuanImage 3.0', 'desc': "HunyuanImage 3.0 is a cutting-edge multimodal model designed for image generation and text-image alignment, utilizing an autoregressive framework. It features a Mixture-of-Experts architecture with over 80 billion parameters, allowing for efficient processing and high-quality outputs. The model's success is attributed to careful data curation, innovative architecture, and a robust training process that includes both pre-training and post-training phases. By making this model open-source, the authors encourage further exploration and development within the multimodal AI community."}, 'zh': {'title': 'HunyuanImage 3.0：开创多模态生成的新纪元', 'desc': 'HunyuanImage 3.0 是一个多模态模型，采用自回归框架，能够在图像生成和文本-图像对齐方面达到最先进的性能。该模型使用了超过800亿个参数的专家混合架构，具有强大的生成能力。通过精心的数据整理、先进的架构设计和有效的训练基础设施，HunyuanImage 3.0 实现了大规模的训练和推理。我们希望通过开源代码和模型权重，促进社区探索新的多模态应用。'}}}, {'id': 'https://huggingface.co/papers/2509.23866', 'title': 'Efficient Multi-turn RL for GUI Agents via Decoupled Training and\n  Adaptive Data Curation', 'url': 'https://huggingface.co/papers/2509.23866', 'abstract': 'DART, a decoupled reinforcement learning framework for GUI agents, improves efficiency and learning effectiveness through asynchronous modules and adaptive data curation, achieving high task success rates on the OSWorld benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language model (VLM) based GUI agents show promise for automating complex desktop and mobile tasks, but face significant challenges in applying reinforcement learning (RL): (1) slow multi-turn interactions with GUI environments for policy rollout, and (2) insufficient high-quality agent-environment interactions for policy learning. To address these challenges, we propose DART, a Decoupled Agentic RL Training framework for GUI agents, which coordinates heterogeneous modules in a highly decoupled manner. DART separates the training system into four asynchronous modules: environment cluster, rollout service, data manager, and trainer. This design enables non-blocking communication, asynchronous training, rollout-wise trajectory sampling, and per-worker model synchronization, significantly improving the system efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput, and 5.5* environment utilization. To facilitate effective learning from abundant samples, we introduce an adaptive data curation scheme: (1) pre-collecting successful trajectories for challenging tasks to supplement sparse success in online sampling; (2) dynamically adjusting rollout numbers and trajectory lengths based on task difficulty; (3) training selectively on high-entropy steps to prioritize critical decisions; (4) stabilizing learning via truncated importance sampling for policy mismatch between policy rollout and updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task success rate, a 14.61% absolute gain over the base model, and 7.34% higher than open-source SOTA. We will fully open-source our training framework, data, and model checkpoints via computer-use-agents.github.io/dart-gui, which we believe is a timely contribution to the open-source community of agentic RL training.', 'score': 5, 'issue_id': 6154, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 сентября', 'en': 'September 28', 'zh': '9月28日'}, 'hash': '5427fac94e859a8b', 'authors': ['Pengxiang Li', 'Zechen Hu', 'Zirui Shang', 'Jingrong Wu', 'Yang Liu', 'Hui Liu', 'Zhi Gao', 'Chenrui Shi', 'Bofei Zhang', 'Zihao Zhang', 'Xiaochuan Shi', 'Zedong YU', 'Yuwei Wu', 'Xinxiao Wu', 'Yunde Jia', 'Liuyu Xiang', 'Zhaofeng He', 'Qing Li'], 'affiliations': ['Beijing Institute of Technology', 'Beijing University of Posts and Telecommunications', 'DataCanvas', 'Shenzhen MSU-BIT University', 'State Key Laboratory of General Artificial Intelligence, BIGAI'], 'pdf_title_img': 'assets/pdf/title_img/2509.23866.jpg', 'data': {'categories': ['#data', '#optimization', '#benchmark', '#rl', '#open_source', '#training', '#games', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Асинхронное обучение GUI агентов через децентрализованную архитектуру', 'desc': 'DART представляет собой децентрализованную архитектуру для обучения с подкреплением GUI агентов, которая разделяет систему на четыре асинхронных модуля для повышения эффективности. Фреймворк использует адаптивную схему курирования данных, включая предварительный сбор успешных траекторий и селективное обучение на шагах с высокой энтропией. Система значительно улучшает утилизацию GPU и пропускную способность обучения по сравнению с традиционными подходами. На бенчмарке OSWorld модель DART-GUI-7B достигает 42.13% успешности выполнения задач, превосходя базовую модель на 14.61%.'}, 'en': {'title': 'DART: Decoupling Reinforcement Learning for Efficient GUI Agents', 'desc': 'DART is a new framework designed to improve reinforcement learning for GUI agents by using a decoupled approach. It organizes the training process into four asynchronous modules, allowing for more efficient communication and training without delays. The framework also includes an adaptive data curation method that enhances learning by focusing on successful interactions and adjusting to task difficulty. As a result, DART significantly boosts task success rates and overall system performance compared to previous models.'}, 'zh': {'title': 'DART：提升GUI代理学习效率的解耦强化学习框架', 'desc': 'DART是一种为图形用户界面（GUI）代理设计的解耦强化学习框架。它通过异步模块和自适应数据管理，提高了学习效率和效果。DART将训练系统分为四个异步模块，允许非阻塞通信和高效的模型同步，从而显著提升了系统的利用率。通过引入自适应数据策划方案，DART在OSWorld基准测试中实现了42.13%的任务成功率，超越了基础模型和开源最优模型。'}}}, {'id': 'https://huggingface.co/papers/2509.23808', 'title': 'Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach\n  for LLM Reasoning in RLVR', 'url': 'https://huggingface.co/papers/2509.23808', 'abstract': 'Re-examining the exploration-exploitation trade-off in Reinforcement Learning for Verifiable Rewards through hidden-state analysis reveals opportunities for simultaneous enhancement using Effective Rank and its derivatives, leading to improved performance in diverse benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR) interprets recent progress through the lens of an exploration-exploitation trade-off, a perspective largely shaped by token-level metrics. We re-examine this perspective, proposing that this perceived trade-off may not be a fundamental constraint but rather an artifact of the measurement level. To investigate this, we shift the analysis to the semantically rich hidden-state space, adopting Effective Rank (ER) to quantify exploration and proposing its novel first- and second-order derivatives, named Effective Rank Velocity (ERV) and Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our analysis reveals that at the hidden-state level, exploration and exploitation could be decoupled (Sec. 4). This finding reveals an opportunity to enhance both capacities simultaneously. This insight motivates our method, Velocity-Exploiting Rank-Learning (VERL), the first to operationalize the principle of synergistic exploration-exploitation enhancement by directly shaping the RL advantage function. The key innovation is leveraging the theoretically stable ERA as a predictive meta-controller to create a synergistic, dual-channel incentive structure. Instead of forcing a trade-off, VERL prospectively amplifies rewards for exploration to preempt overconfidence and reinforces exploitative gains to consolidate reasoning. Experiments across diverse LLMs and reasoning benchmarks show consistent gains, including up to 21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.', 'score': 5, 'issue_id': 6156, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 сентября', 'en': 'September 28', 'zh': '9月28日'}, 'hash': '4e729698740f3ee5', 'authors': ['Fanding Huang', 'Guanbo Huang', 'Xiao Fan', 'Yi He', 'Xiao Liang', 'Xiao Chen', 'Qinting Jiang', 'Faisal Nadeem Khan', 'Jingyan Jiang', 'Zhi Wang'], 'affiliations': ['Shenzhen Technology University', 'Tsinghua Shenzhen International Graduate School, Tsinghua University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2509.23808.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#benchmark', '#optimization'], 'emoji': '⚖️', 'ru': {'title': 'Разделяй и властвуй: одновременное усиление исследования и эксплуатации в RL', 'desc': 'Исследователи переосмыслили компромисс между исследованием и эксплуатацией в обучении с подкреплением для верифицируемых наград (RLVR). Вместо анализа на уровне токенов, они предложили изучать скрытые состояния модели, используя метрику Effective Rank и её производные для измерения динамики исследования и эксплуатации. Оказалось, что на уровне скрытых состояний эти процессы можно разделить и улучшать одновременно. Их новый метод VERL показал улучшения до 21.4% точности на сложных задачах рассуждения.'}, 'en': {'title': 'Enhancing Exploration and Exploitation in RL with VERL', 'desc': 'This paper investigates the exploration-exploitation trade-off in Reinforcement Learning for Verifiable Rewards (RLVR) by analyzing hidden states instead of traditional token-level metrics. The authors propose that the trade-off is not a fundamental limitation but a result of how performance is measured. They introduce Effective Rank (ER) and its derivatives, Effective Rank Velocity (ERV) and Effective Rank Acceleration (ERA), to better understand and enhance both exploration and exploitation simultaneously. Their method, Velocity-Exploiting Rank-Learning (VERL), uses these insights to improve reward structures, leading to significant performance gains in various benchmarks, including a notable accuracy increase on the Gaokao 2024 dataset.'}, 'zh': {'title': '协同增强探索与利用的创新方法', 'desc': '本文重新审视了强化学习中可验证奖励的探索与利用权衡，提出这一权衡可能并非根本限制，而是测量层面的伪影。我们通过分析隐藏状态空间，采用有效秩（Effective Rank）来量化探索，并提出其一阶和二阶导数，分别称为有效秩速度（Effective Rank Velocity）和有效秩加速度（Effective Rank Acceleration），以捕捉利用动态。研究表明，在隐藏状态层面，探索与利用可以解耦，这为同时增强两者提供了机会。基于此，我们提出了速度利用秩学习（VERL）方法，通过直接塑造强化学习的优势函数，实现协同增强探索与利用的原则。'}}}, {'id': 'https://huggingface.co/papers/2509.21953', 'title': 'MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially\n  Disentangled Attention and Identity-Aware Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.21953', 'abstract': "MultiCrafter framework improves multi-subject image generation by addressing attribute leakage through explicit positional supervision, utilizing a Mixture-of-Experts architecture, and aligning with human preferences via online reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-subject image generation aims to synthesize user-provided subjects in a single image while preserving subject fidelity, ensuring prompt consistency, and aligning with human aesthetic preferences. However, existing methods, particularly those built on the In-Context-Learning paradigm, are limited by their reliance on simple reconstruction-based objectives, leading to both severe attribute leakage that compromises subject fidelity and failing to align with nuanced human preferences. To address this, we propose MultiCrafter, a framework that ensures high-fidelity, preference-aligned generation. First, we find that the root cause of attribute leakage is a significant entanglement of attention between different subjects during the generation process. Therefore, we introduce explicit positional supervision to explicitly separate attention regions for each subject, effectively mitigating attribute leakage. To enable the model to accurately plan the attention region of different subjects in diverse scenarios, we employ a Mixture-of-Experts architecture to enhance the model's capacity, allowing different experts to focus on different scenarios. Finally, we design a novel online reinforcement learning framework to align the model with human preferences, featuring a scoring mechanism to accurately assess multi-subject fidelity and a more stable training strategy tailored for the MoE architecture. Experiments validate that our framework significantly improves subject fidelity while aligning with human preferences better.", 'score': 5, 'issue_id': 6152, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '0c3656cedf425566', 'authors': ['Tao Wu', 'Yibo Jiang', 'Yehao Lu', 'Zhizhong Wang', 'Zeyi Huang', 'Zequn Qin', 'Xi Li'], 'affiliations': ['College of Computer Science and Technology, Zhejiang University', 'Huawei Technologies Ltd', 'School of Software Technology, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.21953.jpg', 'data': {'categories': ['#leakage', '#alignment', '#multimodal', '#training', '#rl', '#architecture', '#synthetic'], 'emoji': '🎨', 'ru': {'title': 'Точная генерация изображений с множественными объектами через разделение внимания', 'desc': 'Исследователи представили MultiCrafter - фреймворк для генерации изображений с несколькими объектами, который решает проблему смешивания атрибутов разных объектов. Основная проблема заключается в том, что механизм внимания (attention) запутывается между различными объектами в процессе генерации. Для решения этого авторы вводят явный позиционный контроль, используют архитектуру Mixture-of-Experts для обработки разных сценариев, и применяют онлайн обучение с подкреплением для лучшего соответствия человеческим предпочтениям. Эксперименты показывают значительное улучшение точности воспроизведения объектов и соответствие эстетическим предпочтениям людей.'}, 'en': {'title': 'MultiCrafter: Enhancing Multi-Subject Image Generation with Precision and Preference', 'desc': 'The MultiCrafter framework enhances the generation of images containing multiple subjects by tackling the issue of attribute leakage through the use of explicit positional supervision. This approach helps to clearly define attention regions for each subject, preventing confusion during the image creation process. Additionally, the framework employs a Mixture-of-Experts architecture, which allows different model components to specialize in various scenarios, improving overall performance. Finally, an online reinforcement learning strategy is implemented to ensure that the generated images align closely with human aesthetic preferences, resulting in higher fidelity and satisfaction.'}, 'zh': {'title': 'MultiCrafter：提升多主体图像生成的保真度与人类偏好对齐', 'desc': 'MultiCrafter框架通过显式的位置信息监督来解决多主体图像生成中的属性泄漏问题，从而提高生成的图像质量。该框架采用混合专家架构，使不同的专家能够专注于不同的场景，增强模型的能力。为了更好地符合人类的审美偏好，我们设计了一种新的在线强化学习机制，能够准确评估多主体的保真度。实验结果表明，MultiCrafter显著提高了生成图像的主体保真度，并更好地与人类偏好对齐。'}}}, {'id': 'https://huggingface.co/papers/2509.25185', 'title': 'PixelCraft: A Multi-Agent System for High-Fidelity Visual Reasoning on\n  Structured Images', 'url': 'https://huggingface.co/papers/2509.25185', 'abstract': 'PixelCraft, a multi-agent system, enhances visual reasoning in multimodal large language models by integrating high-fidelity image processing and flexible reasoning through a dynamic workflow and image memory.  \t\t\t\t\tAI-generated summary \t\t\t\t Structured images (e.g., charts and geometric diagrams) remain challenging for multimodal large language models (MLLMs), as perceptual slips can cascade into erroneous conclusions. Intermediate visual cues can steer reasoning; however, existing cue-based methods are constrained with low-fidelity image processing and linear, rigid reasoning patterns, limiting their effectiveness on complex structured-image tasks. In this paper, we propose PixelCraft, a novel multi-agent system for high-fidelity image processing and flexible visual reasoning on structured images. The system comprises a dispatcher, a planner, a reasoner, critics, and a set of visual tool agents. To achieve high-fidelity processing, we construct a high-quality corpus and fine-tune an MLLM into a grounding model, whose pixel-level localizations are integrated with traditional computer vision (CV) algorithms in tool agents. Building on this foundation, PixelCraft facilitates flexible visual reasoning through a dynamic three-stage workflow of tool selection, agent discussion, and self-criticism. Moreover, unlike prior linear reasoning patterns that simply append historical images, PixelCraft maintains an image memory to allow the planner to adaptively revisit earlier visual steps, explore alternative reasoning branches, and dynamically adjust the reasoning trajectory during discussion. Extensive experiments on challenging chart and geometry benchmarks demonstrate that PixelCraft significantly improves visual reasoning performance for advanced MLLMs, setting a new standard for structured image reasoning. Our code will be available at https://github.com/microsoft/PixelCraft.', 'score': 4, 'issue_id': 6154, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '9cba7a1f097e46cc', 'authors': ['Shuoshuo Zhang', 'Zijian Li', 'Yizhen Zhang', 'Jingjing Fu', 'Lei Song', 'Jiang Bian', 'Jun Zhang', 'Yujiu Yang', 'Rui Wang'], 'affiliations': ['Hong Kong University of Science and Technology', 'Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.25185.jpg', 'data': {'categories': ['#benchmark', '#cv', '#reasoning', '#interpretability', '#multimodal', '#agents'], 'emoji': '🔍', 'ru': {'title': 'Мультиагентное визуальное рассуждение высокой точности', 'desc': 'PixelCraft — это мультиагентная система, которая улучшает визуальное рассуждение в мультимодальных LLM при работе со структурированными изображениями, такими как графики и диаграммы. Система включает диспетчер, планировщик, анализатор, критиков и набор визуальных инструментов, объединяя точную локализацию на уровне пикселей с традиционными алгоритмами компьютерного зрения. В отличие от линейных методов рассуждения, PixelCraft использует динамический трёхэтапный процесс и поддерживает память изображений для адаптивного пересмотра предыдущих визуальных шагов. Эксперименты на сложных бенчмарках с графиками и геометрическими задачами показывают значительное улучшение производительности визуального рассуждения.'}, 'en': {'title': 'PixelCraft: Revolutionizing Visual Reasoning in MLLMs', 'desc': 'PixelCraft is a multi-agent system designed to improve visual reasoning in multimodal large language models (MLLMs) by combining high-quality image processing with flexible reasoning capabilities. It addresses the challenges posed by structured images, such as charts and diagrams, which often lead to errors due to perceptual slips. The system features a dynamic workflow that includes tool selection, agent discussions, and self-criticism, allowing for adaptive reasoning. By integrating pixel-level localizations with traditional computer vision techniques and maintaining an image memory, PixelCraft enhances the reasoning process, leading to significant performance improvements on complex visual tasks.'}, 'zh': {'title': 'PixelCraft：提升视觉推理的新标准', 'desc': 'PixelCraft 是一个多智能体系统，旨在通过动态工作流程和图像记忆来增强多模态大语言模型的视觉推理能力。该系统结合了高保真图像处理和灵活的推理机制，解决了现有方法在处理结构化图像时的局限性。通过构建高质量语料库并微调模型，PixelCraft 实现了像素级的本地化，并与传统计算机视觉算法相结合。实验结果表明，PixelCraft 在复杂的图表和几何任务上显著提升了视觉推理性能，树立了结构化图像推理的新标准。'}}}, {'id': 'https://huggingface.co/papers/2509.25131', 'title': 'MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech', 'url': 'https://huggingface.co/papers/2509.25131', 'abstract': 'MGM-Omni is a unified multimodal language model for speech generation and understanding, featuring a dual-track architecture for efficient cross-modal interaction and data-efficient training.  \t\t\t\t\tAI-generated summary \t\t\t\t We present MGM-Omni, a unified Omni LLM for omni-modal understanding and expressive, long-horizon speech generation. Unlike cascaded pipelines that isolate speech synthesis, MGM-Omni adopts a "brain-mouth" design with a dual-track, token-based architecture that cleanly decouples multimodal reasoning from real-time speech generation. This design enables efficient cross-modal interaction and low-latency, streaming speech generation. For understanding, a unified training strategy coupled with a dual audio encoder design enables long-form audio perception across diverse acoustic conditions. For generation, a chunk-based parallel decoding scheme narrows the text speech token-rate gap, accelerating inference and supporting streaming zero-shot voice cloning with stable timbre over extended durations. Compared to concurrent work, MGM-Omni achieves these capabilities with markedly data-efficient training. Extensive experiments demonstrate that MGM-Omni outperforms existing open source models in preserving timbre identity across extended sequences, producing natural and context-aware speech, and achieving superior long-form audio and omnimodal understanding. MGM-Omni establishes an efficient, end-to-end paradigm for omnimodal understanding and controllable, personalised long-horizon speech generation.', 'score': 4, 'issue_id': 6153, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': 'e5f4eb55f7f45ab2', 'authors': ['Chengyao Wang', 'Zhisheng Zhong', 'Bohao Peng', 'Senqiao Yang', 'Yuqi Liu', 'Haokun Gui', 'Bin Xia', 'Jingyao Li', 'Bei Yu', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HKUST', 'SmartMore'], 'pdf_title_img': 'assets/pdf/title_img/2509.25131.jpg', 'data': {'categories': ['#training', '#open_source', '#games', '#agi', '#long_context', '#multimodal', '#interpretability', '#audio', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Единая модель для понимания и генерации речи с архитектурой мозг-рот', 'desc': 'MGM-Omni представляет собой унифицированную мультимодальную языковую модель с архитектурой "мозг-рот", которая объединяет понимание и генерацию речи в единой системе. Модель использует двухканальную токенную архитектуру, которая разделяет мультимодальное рассуждение и генерацию речи в реальном времени. Для понимания применяется унифицированная стратегия обучения с двойным аудио энкодером, обеспечивающим восприятие длинных аудиозаписей в различных акустических условиях. Для генерации используется схема параллельного декодирования по частям, которая ускоряет инференс и поддерживает потоковое клонирование голоса с сохранением тембра на протяжении длительных последовательностей.'}, 'en': {'title': 'MGM-Omni: Revolutionizing Speech Generation and Understanding', 'desc': 'MGM-Omni is a cutting-edge multimodal language model designed for both understanding and generating speech. It features a unique dual-track architecture that separates the processes of multimodal reasoning and real-time speech generation, allowing for efficient interaction between different types of data. The model employs a unified training strategy and advanced audio encoding to enhance its ability to perceive and generate long-form audio across various conditions. With its innovative design, MGM-Omni achieves high-quality, context-aware speech generation while being data-efficient, outperforming existing models in maintaining voice identity and producing natural speech.'}, 'zh': {'title': 'MGM-Omni：高效的多模态语音生成与理解', 'desc': 'MGM-Omni是一种统一的多模态语言模型，专注于语音生成和理解。它采用双轨架构，能够高效地进行跨模态交互，并实现数据高效训练。该模型通过“脑-口”设计，将多模态推理与实时语音生成解耦，支持低延迟的流式语音生成。实验表明，MGM-Omni在保持音色一致性和生成自然语音方面优于现有的开源模型。'}}}, {'id': 'https://huggingface.co/papers/2509.24786', 'title': 'LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in\n  Mechanism via Multi-Step Reasoning', 'url': 'https://huggingface.co/papers/2509.24786', 'abstract': 'LOVE-R1, a model with adaptive frame sampling, enhances long video understanding by balancing temporal and spatial details through multi-step reasoning and decoupled reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video understanding is still challenging for recent Large Video-Language Models (LVLMs) due to the conflict between long-form temporal understanding and detailed spatial perception. LVLMs with a uniform frame sampling mechanism, which samples frames with an equal frame size and fixed sampling rate, inevitably sacrifice either temporal clues or spatial details, resulting in suboptimal solutions. To mitigate this dilemma, we propose LOVE-R1, a model that can adaptively zoom in on a video clip. The model is first provided with densely sampled frames but in a small resolution. If some spatial details are needed, the model can zoom in on a clip of interest with a large frame resolution based on its reasoning until key visual information is obtained. The whole process is implemented as a multi-step reasoning process. To train the reasoning ability, we first finetune the model on our collected 38k high-quality CoT data and enhance it with decoupled reinforcement finetuning. As outcome rewards can not provide fine-grained process supervision, we decouple multi-step reasoning into multiple single-step reasoning and optimize the internal zoom-in ability explicitly. Experiments on long video understanding benchmarks show that our model with the slow-fast adaptive frame sampling mechanism achieves a great trade-off between sampling density and frame resolutions, and LOVE-R1 outperforms our baseline Qwen2.5-VL by an average of 3.1% points across 4 common long video understanding benchmarks.', 'score': 4, 'issue_id': 6153, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '60657ccb584048d7', 'authors': ['Shenghao Fu', 'Qize Yang', 'Yuan-Ming Li', 'Xihan Wei', 'Xiaohua Xie', 'Wei-Shi Zheng'], 'affiliations': ['Guangdong Province Key Laboratory of Information Security Technology, China', 'Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China', 'Pazhou Laboratory (Huangpu), China', 'Peng Cheng Laboratory, China', 'School of Computer Science and Engineering, Sun Yat-sen University, China', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2509.24786.jpg', 'data': {'categories': ['#video', '#rl', '#training', '#reasoning', '#long_context', '#optimization', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Адаптивное масштабирование видео для лучшего понимания длинных роликов', 'desc': 'Исследователи представили LOVE-R1 - модель для понимания длинных видео, которая решает проблему баланса между временной и пространственной информацией. Модель использует адаптивную выборку кадров: сначала анализирует видео с низким разрешением, а затем увеличивает разрешение на важных участках через многошаговое рассуждение. Для обучения способности к рассуждению используется файнтюнинг на 38k высококачественных данных Chain-of-Thought и декомпозированное обучение с подкреплением. Модель превосходит базовую Qwen2.5-VL на 3.1% по четырем бенчмаркам понимания длинных видео.'}, 'en': {'title': 'LOVE-R1: Adaptive Sampling for Enhanced Long Video Understanding', 'desc': 'The paper introduces LOVE-R1, a novel model designed to improve long video understanding by using adaptive frame sampling. This model addresses the challenge faced by Large Video-Language Models (LVLMs) in balancing temporal and spatial information. LOVE-R1 employs a multi-step reasoning process that allows it to dynamically adjust frame resolution based on the importance of spatial details. Through decoupled reinforcement learning and extensive training on high-quality data, LOVE-R1 demonstrates superior performance on long video understanding tasks compared to existing models.'}, 'zh': {'title': 'LOVE-R1：自适应帧采样提升长视频理解', 'desc': 'LOVE-R1是一种具有自适应帧采样的模型，旨在提高长视频理解能力。该模型通过多步推理和平衡时间和空间细节，解决了长视频语言模型在处理长时间序列时面临的挑战。LOVE-R1首先以低分辨率密集采样帧，然后根据需要对感兴趣的片段进行放大，以获取关键视觉信息。实验结果表明，LOVE-R1在长视频理解基准测试中表现优于基线模型，显示出采样密度和帧分辨率之间的良好平衡。'}}}, {'id': 'https://huggingface.co/papers/2509.24335', 'title': 'Hyperspherical Latents Improve Continuous-Token Autoregressive\n  Generation', 'url': 'https://huggingface.co/papers/2509.24335', 'abstract': 'SphereAR, an autoregressive model with hyperspherical constraints, achieves state-of-the-art performance in image generation, surpassing diffusion and masked-generation models at similar parameter scales.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) models are promising for image generation, yet continuous-token AR variants often trail latent diffusion and masked-generation models. The core issue is heterogeneous variance in VAE latents, which is amplified during AR decoding, especially under classifier-free guidance (CFG), and can cause variance collapse. We propose SphereAR to address this issue. Its core design is to constrain all AR inputs and outputs -- including after CFG -- to lie on a fixed-radius hypersphere (constant ell_2 norm), leveraging hyperspherical VAEs. Our theoretical analysis shows that hyperspherical constraint removes the scale component (the primary cause of variance collapse), thereby stabilizing AR decoding. Empirically, on ImageNet generation, SphereAR-H (943M) sets a new state of the art for AR models, achieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54 and SphereAR-B (208M) reaches 1.92, matching or surpassing much larger baselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge, this is the first time a pure next-token AR image generator with raster order surpasses diffusion and masked-generation models at comparable parameter scales.', 'score': 4, 'issue_id': 6153, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '4f3275712f51af77', 'authors': ['Guolin Ke', 'Hui Xue'], 'affiliations': ['DP Technology', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2509.24335.jpg', 'data': {'categories': ['#training', '#optimization', '#cv', '#architecture', '#diffusion'], 'emoji': '🌐', 'ru': {'title': 'Гиперсферические ограничения для стабильной авторегрессионной генерации изображений', 'desc': 'В статье представлена SphereAR - авторегрессионная модель для генерации изображений, которая ограничивает все входы и выходы на гиперсфере фиксированного радиуса. Основная проблема традиционных AR моделей заключается в неоднородной дисперсии в латентных представлениях VAE, что усиливается при декодировании и может привести к коллапсу дисперсии. Гиперсферические ограничения устраняют масштабную компоненту, которая является основной причиной коллапса дисперсии, тем самым стабилизируя процесс декодирования. SphereAR достигает нового состояния искусства среди AR моделей на ImageNet с FID 1.34, превосходя диффузионные модели и модели с маскированной генерацией при сопоставимом количестве параметров.'}, 'en': {'title': 'SphereAR: Revolutionizing Image Generation with Hyperspherical Constraints', 'desc': 'SphereAR is an autoregressive model designed for image generation that incorporates hyperspherical constraints to improve performance. By constraining inputs and outputs to a fixed-radius hypersphere, it effectively addresses the issue of variance collapse that often occurs in traditional autoregressive models. This innovative approach allows SphereAR to achieve state-of-the-art results on image generation tasks, outperforming both diffusion and masked-generation models at similar parameter sizes. The model demonstrates significant improvements in FID scores, showcasing its effectiveness in generating high-quality images.'}, 'zh': {'title': 'SphereAR：超球面约束下的自回归图像生成新突破', 'desc': 'SphereAR是一种自回归模型，采用超球面约束，能够在图像生成任务中实现最先进的性能，超越了同等参数规模的扩散模型和掩蔽生成模型。该模型解决了在自回归解码过程中，由于变分自编码器（VAE）潜变量的异质方差导致的方差崩溃问题。SphereAR通过将所有自回归输入和输出限制在固定半径的超球面上，稳定了自回归解码过程。实验结果表明，SphereAR在ImageNet生成任务中取得了新的最佳结果，展示了其在图像生成领域的强大能力。'}}}, {'id': 'https://huggingface.co/papers/2509.23371', 'title': 'Alignment through Meta-Weighted Online Sampling: Bridging the Gap\n  between Data Generation and Preference Optimization', 'url': 'https://huggingface.co/papers/2509.23371', 'abstract': 'Meta-Weighted Adaptive Preference Optimization (MetaAPO) dynamically balances online and offline data to align large language models with human preferences, outperforming existing methods and reducing annotation costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Preference optimization is crucial for aligning large language models (LLMs) with human values and intentions. A significant challenge in this process is the distribution mismatch between pre-collected offline preference data and the evolving model policy. Existing methods attempt to reduce this gap using static heuristics or decoupled online sampling strategies, but they often fail to adapt to the model\'s dynamic learning state. To bridge this gap, we propose Meta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework that dynamically couples data generation with model training. MetaAPO employs a lightweight meta-learner, as an "alignment gap estimator", to evaluate the potential benefits of on-policy sampling in relation to offline data. This guides targeted online generation and assigns sample-wise meta-weights to the optimization objective, dynamically balancing the quality and distribution of online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench demonstrate that MetaAPO consistently outperforms existing preference optimization approaches across various settings, while reducing 42% in online annotation costs.', 'score': 4, 'issue_id': 6152, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': 'e5432419a1266b60', 'authors': ['Junming Yang', 'Ning Xu', 'Biao Liu', 'Shiqi Qiao', 'Xin Geng'], 'affiliations': ['School of Computer Science and Engineering Southeast University Nanjing, China'], 'pdf_title_img': 'assets/pdf/title_img/2509.23371.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#alignment'], 'emoji': '⚖️', 'ru': {'title': 'Умная балансировка данных для выравнивания LLM с человеческими предпочтениями', 'desc': 'В статье представлен MetaAPO - новый подход для выравнивания больших языковых моделей с человеческими предпочтениями. Метод динамически балансирует онлайн и офлайн данные обучения, используя легковесную мета-модель для оценки потенциальной пользы от генерации новых примеров. Система назначает веса каждому образцу в зависимости от его качества и релевантности для текущего состояния модели. Эксперименты показывают превосходство над существующими методами при сокращении затрат на аннотацию на 42%.'}, 'en': {'title': 'Dynamic Data Balancing for Better AI Alignment', 'desc': "MetaAPO is a new method designed to improve how large language models (LLMs) align with human preferences by effectively managing both online and offline data. It addresses the issue of distribution mismatch between previously collected offline preference data and the model's current learning state. By using a meta-learner to assess the value of on-policy sampling, MetaAPO dynamically adjusts the optimization process, ensuring that the model learns from the most relevant data. This approach not only enhances performance compared to existing methods but also significantly cuts down on the costs associated with online data annotation."}, 'zh': {'title': '动态平衡在线与离线数据的偏好优化', 'desc': 'Meta加权自适应偏好优化（MetaAPO）是一种新颖的框架，旨在动态平衡在线和离线数据，以使大型语言模型与人类偏好对齐。该方法通过使用轻量级的元学习器来评估在线采样的潜在好处，从而解决了预先收集的离线偏好数据与模型政策之间的分布不匹配问题。MetaAPO通过动态调整在线和离线数据的质量和分布，优化了样本的加权目标。实验结果表明，MetaAPO在多个设置中均优于现有的偏好优化方法，并且在线标注成本降低了42%。'}}}, {'id': 'https://huggingface.co/papers/2509.22570', 'title': 'UniMIC: Token-Based Multimodal Interactive Coding for Human-AI\n  Collaboration', 'url': 'https://huggingface.co/papers/2509.22570', 'abstract': 'UniMIC, a unified token-based framework, enhances multimodal communication by using compact tokenized representations and lightweight Transformer-based entropy models, achieving significant bitrate savings without compromising performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI agents is transforming human-AI collaboration into bidirectional, multimodal interaction. However, existing codecs remain optimized for unimodal, one-way communication, resulting in repeated degradation under conventional compress-transmit-reconstruct pipelines. To address this limitation, we propose UniMIC, a Unified token-based Multimodal Interactive Coding framework that bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or plain text, UniMIC employs compact tokenized representations as the communication medium, enabling efficient low-bitrate transmission while maintaining compatibility with LMMs. To further enhance compression, lightweight Transformer-based entropy models with scenario-specific designs-generic, masked, and text-conditioned-effectively minimize inter-token redundancy. Extensive experiments on text-to-image generation, text-guided inpainting, outpainting, and visual question answering show that UniMIC achieves substantial bitrate savings and remains robust even at ultra-low bitrates (<0.05bpp), without compromising downstream task performance. These results establish UniMIC as a practical and forward-looking paradigm for next-generation multimodal interactive communication.', 'score': 3, 'issue_id': 6153, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '0d8cb97b2040b91e', 'authors': ['Qi Mao', 'Tinghan Yang', 'Jiahao Li', 'Bin Li', 'Libiao Jin', 'Yan Lu'], 'affiliations': ['Microsoft Research Asia, Beijing 10080, China', 'State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing 100024, China'], 'pdf_title_img': 'assets/pdf/title_img/2509.22570.jpg', 'data': {'categories': ['#data', '#games', '#optimization', '#multimodal', '#cv'], 'emoji': '📡', 'ru': {'title': 'Токенизированное сжатие для эффективной мультимодальной коммуникации', 'desc': 'UniMIC представляет новый подход к сжатию мультимодальных данных для взаимодействия между устройствами и облачными AI-агентами. Вместо передачи сырых пикселей или текста, система использует компактные токенизированные представления как средство коммуникации. Легковесные Transformer-модели с энтропийным кодированием минимизируют избыточность между токенами для трех сценариев: универсального, маскированного и текстово-обусловленного сжатия. Эксперименты показали значительную экономию битрейта при сохранении качества выполнения задач генерации изображений, инпейнтинга и визуального вопрос-ответа.'}, 'en': {'title': 'UniMIC: Efficient Multimodal Communication with Tokenization', 'desc': 'UniMIC is a new framework designed to improve communication between different types of data, like text and images, using a token-based approach. It replaces traditional methods that send raw data with compact tokenized representations, which allows for more efficient data transmission. By using lightweight Transformer models to reduce redundancy, UniMIC can save significant amounts of data while still performing well in various tasks. This makes it a promising solution for future interactions between edge devices and cloud-based AI systems.'}, 'zh': {'title': 'UniMIC：高效的多模态交互编码框架', 'desc': 'UniMIC是一个统一的基于令牌的框架，旨在增强多模态通信。它通过使用紧凑的令牌化表示和轻量级的基于Transformer的熵模型，实现了显著的比特率节省，同时不影响性能。与传统的单一模态通信不同，UniMIC能够在边缘设备和云AI代理之间高效传输信息。实验结果表明，UniMIC在超低比特率下仍然保持强大的性能，适用于下一代多模态交互通信。'}}}, {'id': 'https://huggingface.co/papers/2509.25149', 'title': 'Pretraining Large Language Models with NVFP4', 'url': 'https://huggingface.co/papers/2509.25149', 'abstract': 'A novel training approach using NVFP4 format with Random Hadamard transforms, two-dimensional quantization, stochastic rounding, and selective high-precision layers enables stable and accurate training of large language models in 4-bit precision.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) today are powerful problem solvers across many domains, and they continue to get stronger as they scale in model size, training set size, and training set quality, as shown by extensive research and experimentation across the industry. Training a frontier model today requires on the order of tens to hundreds of yottaflops, which is a massive investment of time, compute, and energy. Improving pretraining efficiency is therefore essential to enable the next generation of even more capable LLMs. While 8-bit floating point (FP8) training is now widely adopted, transitioning to even narrower precision, such as 4-bit floating point (FP4), could unlock additional improvements in computational speed and resource utilization. However, quantization at this level poses challenges to training stability, convergence, and implementation, notably for large-scale models trained on long token horizons.   In this study, we introduce a novel approach for stable and accurate training of large language models (LLMs) using the NVFP4 format. Our method integrates Random Hadamard transforms (RHT) to bound block-level outliers, employs a two-dimensional quantization scheme for consistent representations across both the forward and backward passes, utilizes stochastic rounding for unbiased gradient estimation, and incorporates selective high-precision layers. We validate our approach by training a 12-billion-parameter model on 10 trillion tokens -- the longest publicly documented training run in 4-bit precision to date. Our results show that the model trained with our NVFP4-based pretraining technique achieves training loss and downstream task accuracies comparable to an FP8 baseline. These findings highlight that NVFP4, when combined with our training approach, represents a major step forward in narrow-precision LLM training algorithms.', 'score': 2, 'issue_id': 6154, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': 'b4b8ecd5db151816', 'authors': ['NVIDIA', 'Felix Abecassis', 'Anjulie Agrusa', 'Dong Ahn', 'Jonah Alben', 'Stefania Alborghetti', 'Michael Andersch', 'Sivakumar Arayandi', 'Alexis Bjorlin', 'Aaron Blakeman', 'Evan Briones', 'Ian Buck', 'Bryan Catanzaro', 'Jinhang Choi', 'Mike Chrzanowski', 'Eric Chung', 'Victor Cui', 'Steve Dai', 'Bita Darvish Rouhani', 'Carlo del Mundo', 'Deena Donia', 'Burc Eryilmaz', 'Henry Estela', 'Abhinav Goel', 'Oleg Goncharov', 'Yugi Guvvala', 'Robert Hesse', 'Russell Hewett', 'Herbert Hum', 'Ujval Kapasi', 'Brucek Khailany', 'Mikail Khona', 'Nick Knight', 'Alex Kondratenko', 'Ronny Krashinsky', 'Ben Lanir', 'Simon Layton', 'Michael Lightstone', 'Daniel Lo', 'Paulius Micikevicius', 'Asit Mishra', 'Tim Moon', 'Deepak Narayanan', 'Chao Ni', 'Abhijit Paithankar', 'Satish Pasumarthi', 'Ankit Patel', 'Mostofa Patwary', 'Ashwin Poojary', 'Gargi Prasad', 'Sweta Priyadarshi', 'Yigong Qin', 'Xiaowei Ren', 'Oleg Rybakov', 'Charbel Sakr', 'Sanjeev Satheesh', 'Stas Sergienko', 'Pasha Shamis', 'Kirthi Shankar', 'Nishant Sharma', 'Mohammad Shoeybi', 'Michael Siu', 'Misha Smelyanskiy', 'Darko Stosic', 'Dusan Stosic', 'Bor-Yiing Su', 'Frank Sun', 'Nima Tajbakhsh', 'Shelby Thomas', 'Przemek Tredak', 'Evgeny Tsykunov', 'Gandhi Vaithilingam', 'Aditya Vavre', 'Rangharajan Venkatesan', 'Roger Waleffe', 'Qiyu Wan', 'Hexin Wang', 'Mengdi Wang', 'Lizzie Wei', 'Hao Wu', 'Evan Wu', 'Keith Wyss', 'Ning Xu', 'Jinze Xue', 'Charlene Yang', 'Yujia Zhai', 'Ruoxi Zhang', 'Jingyang Zhu', 'Zhongbo Zhu'], 'affiliations': ['DeepSeek-AI', 'NVIDIA', 'Open-Compute-Project'], 'pdf_title_img': 'assets/pdf/title_img/2509.25149.jpg', 'data': {'categories': ['#inference', '#optimization', '#training'], 'emoji': '⚡', 'ru': {'title': 'Революция в обучении LLM: стабильная 4-битная точность', 'desc': 'Исследователи разработали новый метод обучения больших языковых моделей с использованием 4-битной точности NVFP4 вместо традиционной 8-битной FP8. Метод включает Random Hadamard transforms для ограничения выбросов, двумерную квантизацию, стохастическое округление и селективные слои высокой точности. Они успешно обучили модель на 12 миллиардов параметров на 10 триллионах токенов, показав результаты сравнимые с FP8 baseline. Это достижение открывает путь к значительному ускорению обучения LLM и снижению энергопотребления.'}, 'en': {'title': 'Unlocking 4-bit Precision for Powerful Language Models', 'desc': 'This paper presents a new training method for large language models (LLMs) using a 4-bit floating point format called NVFP4. The approach incorporates Random Hadamard transforms to manage outliers, a two-dimensional quantization technique for consistent data representation, and stochastic rounding to improve gradient accuracy. By also using selective high-precision layers, the method ensures stable training even with reduced precision. The results demonstrate that models trained with NVFP4 can achieve performance similar to those trained with higher precision formats, marking a significant advancement in efficient LLM training.'}, 'zh': {'title': 'NVFP4：大语言模型训练的新突破', 'desc': '本文提出了一种新颖的训练方法，使用NVFP4格式结合随机哈达玛变换、二维量化、随机舍入和选择性高精度层，以实现大语言模型在4位精度下的稳定和准确训练。当前的大语言模型在多个领域表现出强大的问题解决能力，但训练这些模型需要巨大的计算资源和时间。通过采用4位浮点数训练，本文的方法在计算速度和资源利用率上提供了额外的改进，同时解决了低精度训练带来的稳定性和收敛性挑战。实验结果表明，使用NVFP4的预训练技术在训练损失和下游任务准确性上与FP8基线相当，标志着窄精度大语言模型训练算法的重大进展。'}}}, {'id': 'https://huggingface.co/papers/2509.25077', 'title': 'BRIDGE - Building Reinforcement-Learning Depth-to-Image Data Generation\n  Engine for Monocular Depth Estimation', 'url': 'https://huggingface.co/papers/2509.25077', 'abstract': 'BRIDGE uses RL-optimized depth-to-image generation to create a large, diverse dataset, enhancing monocular depth estimation robustness and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Monocular Depth Estimation (MDE) is a foundational task for computer vision. Traditional methods are limited by data scarcity and quality, hindering their robustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image (D2I) generation framework that synthesizes over 20M realistic and geometrically accurate RGB images, each intrinsically paired with its ground truth depth, from diverse source depth maps. Then we train our depth estimation model on this dataset, employing a hybrid supervision strategy that integrates teacher pseudo-labels with ground truth depth for comprehensive and robust training. This innovative data generation and training paradigm enables BRIDGE to achieve breakthroughs in scale and domain diversity, consistently outperforming existing state-of-the-art approaches quantitatively and in complex scene detail capture, thereby fostering general and robust depth features. Code and models are available at https://dingning-liu.github.io/bridge.github.io/.', 'score': 2, 'issue_id': 6155, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': 'f1a1fef4161fc727', 'authors': ['Dingning Liu', 'Haoyu Guo', 'Jingyi Zhou', 'Tong He'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2509.25077.jpg', 'data': {'categories': ['#training', '#data', '#optimization', '#rl', '#dataset', '#synthetic', '#cv'], 'emoji': '🌉', 'ru': {'title': 'Мостик к точной оценке глубины через генерацию данных', 'desc': 'В статье представлен метод BRIDGE для улучшения моnocular depth estimation - задачи определения глубины сцены по одному изображению. Авторы используют reinforcement learning для генерации более 20 миллионов реалистичных RGB изображений с точными картами глубины из разнообразных исходных данных. Для обучения модели применяется гибридная стратегия, которая комбинирует псевдо-метки от учителя с истинными значениями глубины. Такой подход позволяет BRIDGE превосходить существующие state-of-the-art методы и лучше справляться со сложными деталями сцен.'}, 'en': {'title': 'BRIDGE: Revolutionizing Depth Estimation with RL-Optimized Data Generation', 'desc': 'BRIDGE is a novel framework that utilizes reinforcement learning (RL) to optimize the generation of depth-to-image (D2I) data, creating a vast dataset of over 20 million realistic RGB images paired with their corresponding ground truth depth maps. This approach addresses the limitations of traditional monocular depth estimation (MDE) methods, which often suffer from insufficient and low-quality data. By employing a hybrid supervision strategy that combines teacher pseudo-labels with actual depth data, BRIDGE enhances the training process for depth estimation models. As a result, BRIDGE significantly improves performance and robustness in depth estimation tasks, outperforming existing methods in both quantitative metrics and the ability to capture complex scene details.'}, 'zh': {'title': 'BRIDGE：深度估计的新突破', 'desc': 'BRIDGE是一个基于强化学习优化的深度到图像生成框架，旨在创建一个大型多样化的数据集，以增强单目深度估计的鲁棒性和性能。该框架合成了超过2000万张真实且几何准确的RGB图像，并与其真实深度一一对应，解决了传统方法在数据稀缺和质量上的限制。通过采用混合监督策略，将教师伪标签与真实深度结合，BRIDGE的深度估计模型在这个数据集上进行训练，取得了显著的突破。最终，BRIDGE在规模和领域多样性上表现优异，超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2509.25052', 'title': 'Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and\n  Planning', 'url': 'https://huggingface.co/papers/2509.25052', 'abstract': "CEL, a novel agent architecture using a Large Language Model, learns to master complex environments through explicit reasoning and planning, achieving success in diverse grid-world tasks with sparse rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of artificial agents that can learn to master complex environments has led to remarkable successes, yet prevailing deep reinforcement learning methods often rely on immense experience, encoding their knowledge opaquely within neural network weights. We propose a different paradigm, one in which an agent learns to play by reasoning and planning. We introduce Cogito, ergo ludo (CEL), a novel agent architecture that leverages a Large Language Model (LLM) to build an explicit, language-based understanding of its environment's mechanics and its own strategy. Starting from a tabula rasa state with no prior knowledge (except action set), CEL operates on a cycle of interaction and reflection. After each episode, the agent analyzes its complete trajectory to perform two concurrent learning processes: Rule Induction, where it refines its explicit model of the environment's dynamics, and Strategy and Playbook Summarization, where it distills experiences into an actionable strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e., Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent successfully learns to master these games by autonomously discovering their rules and developing effective policies from sparse rewards. Ablation studies confirm that the iterative process is critical for sustained learning. Our work demonstrates a path toward more general and interpretable agents that not only act effectively but also build a transparent and improving model of their world through explicit reasoning on raw experience.", 'score': 2, 'issue_id': 6154, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': 'ca5fb75d67ceeb90', 'authors': ['Sai Wang', 'Yu Wu', 'Zhongwen Xu'], 'affiliations': ['Tencent', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2509.25052.jpg', 'data': {'categories': ['#rl', '#interpretability', '#reasoning', '#games', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Агент, который учится мыслить: явное рассуждение вместо скрытых весов', 'desc': 'В статье представлена архитектура агента CEL (Cogito, ergo ludo), которая использует большую языковую модель для обучения в сложных средах через явное рассуждение и планирование. Агент начинает с нулевого знания и работает в цикле взаимодействия и рефлексии, анализируя каждый эпизод для улучшения понимания динамики среды и стратегии. В отличие от классического глубокого обучения с подкреплением, CEL строит явную языковую модель правил среды и создает стратегический справочник действий. Эксперименты на задачах типа grid-world показали, что агент успешно обучается автономно открывать правила игр и разрабатывать эффективные политики при разреженных наградах.'}, 'en': {'title': 'Reasoning and Planning for Mastery in Complex Environments', 'desc': "The paper introduces CEL, a new agent architecture that utilizes a Large Language Model to enhance learning in complex environments. Unlike traditional deep reinforcement learning methods that depend heavily on vast amounts of experience, CEL focuses on reasoning and planning to understand its surroundings. The agent learns by analyzing its actions and refining its knowledge of the environment's rules while creating a strategic playbook for decision-making. This approach allows CEL to effectively master various grid-world tasks by autonomously discovering rules and developing policies from limited rewards."}, 'zh': {'title': '通过推理与规划掌握复杂环境的智能体', 'desc': 'CEL是一种新型的智能体架构，利用大型语言模型（LLM）来学习复杂环境中的推理和规划。它通过明确的语言理解环境的机制和自身策略，从而在稀疏奖励的情况下成功掌握多种网格世界任务。CEL在每个回合后分析其完整轨迹，进行规则归纳和策略总结，逐步优化其环境模型和行动策略。我们的研究表明，这种迭代学习过程对于持续学习至关重要，展示了构建更通用和可解释智能体的可能性。'}}}, {'id': 'https://huggingface.co/papers/2509.24910', 'title': 'Learning Goal-Oriented Language-Guided Navigation with Self-Improving\n  Demonstrations at Scale', 'url': 'https://huggingface.co/papers/2509.24910', 'abstract': 'SID, a self-improving demonstration approach, enhances exploration and generalization in goal-oriented language-guided navigation tasks, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Goal-oriented language-guided navigation requires robust exploration capabilities for agents to navigate to specified goals in unknown environments without step-by-step instructions. Existing methods tend to exclusively utilize shortest-path trajectories, lacking effective exploration priors for training navigation agents. To address the above challenges, we present SID, a goal-oriented language-guided navigation learning approach with Self-Improving Demonstrations. Specifically, SID learns an initial agent on the shortest-path data sampled from environments and then leverages this agent to generate novel exploration trajectories. The novel rollouts provide demonstrations with stronger exploration strategies to train a better agent, which in turn produces higher-quality agent demonstrations for the next round of training. We show that this iterative self-improving pipeline readily scales to new environments, and the resulting demonstrations can be transferred across a variety of language-guided navigation tasks, elevating the performance ceiling in diverse goal-oriented navigation tasks. Extensive experiments demonstrate that SID significantly boosts the exploration capabilities and generalization of navigation agents. The resulting agent achieves new state-of-the-art performance on goal-oriented language-guided navigation tasks, including REVERIE, SOON, notably achieving a 50.9% success rate on the unseen validation splits of SOON, surpassing the prior leading approaches by a margin of 13.9%.', 'score': 2, 'issue_id': 6153, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '6aa993d12be845fa', 'authors': ['Songze Li', 'Zun Wang', 'Gengze Zhou', 'Jialu Li', 'Xiangyu Zeng', 'Limin Wang', 'Yu Qiao', 'Qi Wu', 'Mohit Bansal', 'Yi Wang'], 'affiliations': ['Fudan University', 'Nanjing University', 'Shanghai AI Laboratory', 'The University of Adelaide', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2509.24910.jpg', 'data': {'categories': ['#rl', '#agi', '#optimization', '#transfer_learning', '#agents'], 'emoji': '🗺️', 'ru': {'title': 'Агенты учатся навигации, улучшая собственные демонстрации', 'desc': "Статья представляет SID - подход для обучения навигационных агентов в задачах целевой навигации с языковым управлением. Метод начинает с обучения агента на кратчайших путях, затем использует его для генерации новых траекторий исследования среды. Эти новые траектории становятся демонстрациями для обучения улучшенного агента, создавая итеративный процесс самосовершенствования. Подход показал значительное улучшение способностей к исследованию и обобщению, достигнув новых рекордных результатов на benchmark'ах REVERIE и SOON."}, 'en': {'title': 'SID: Self-Improving Demonstrations for Enhanced Navigation', 'desc': "The paper introduces SID, a novel approach for enhancing exploration and generalization in goal-oriented language-guided navigation tasks. It addresses the limitations of existing methods that rely solely on shortest-path trajectories by implementing a self-improving demonstration framework. SID begins with an initial agent trained on basic data and iteratively generates new exploration trajectories that improve the agent's performance. This method not only scales to new environments but also allows for the transfer of learned strategies across various navigation tasks, achieving state-of-the-art results in multiple benchmarks."}, 'zh': {'title': '自我改进演示，提升导航能力！', 'desc': 'SID是一种自我改进的演示方法，旨在增强目标导向的语言引导导航任务中的探索和泛化能力。该方法首先在最短路径数据上训练初始代理，然后利用该代理生成新的探索轨迹。通过这种迭代的自我改进流程，SID能够在新的环境中扩展，并在多种语言引导的导航任务中实现演示的迁移。实验结果表明，SID显著提高了导航代理的探索能力和泛化能力，达到了新的最先进性能。'}}}, {'id': 'https://huggingface.co/papers/2509.24269', 'title': 'AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety\n  Alignment of Large Reasoning Models', 'url': 'https://huggingface.co/papers/2509.24269', 'abstract': 'AdvChain enhances the safety and reliability of large reasoning models by teaching them dynamic self-correction through adversarial chain-of-thought tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in complex problem-solving through Chain-of-Thought (CoT) reasoning. However, the multi-step nature of CoT introduces new safety challenges that extend beyond conventional language model alignment. We identify a failure mode in current safety CoT tuning methods: the snowball effect, where minor reasoning deviations progressively amplify throughout the thought process, leading to either harmful compliance or excessive refusal. This effect stems from models being trained to imitate perfect reasoning scripts without learning to self-correct. To address this limitation, we propose AdvChain, an alignment paradigm that teaches models dynamic self-correction through adversarial CoT tuning. Our method involves constructing a dataset containing Temptation-Correction and Hesitation-Correction samples, where models learn to recover from harmful reasoning drifts and unnecessary cautions. Extensive experiments show that AdvChain significantly enhances robustness against jailbreak attacks and CoT hijacking while substantially reducing over-refusal on benign prompts, achieving a superior safety-utility balance without compromising reasoning capabilities. Our work establishes a new direction for building more robust and reliable reasoning models.', 'score': 2, 'issue_id': 6153, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': 'dd7bc5e3eeee681d', 'authors': ['Zihao Zhu', 'Xinyu Wu', 'Gehan Hu', 'Siwei Lyu', 'Ke Xu', 'Baoyuan Wu'], 'affiliations': ['Huawei International, Singapore', 'State University of New York at Buffalo', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2509.24269.jpg', 'data': {'categories': ['#training', '#dataset', '#reasoning', '#security', '#alignment', '#rlhf'], 'emoji': '⚖️', 'ru': {'title': 'Обучение AI самокоррекции через adversarial цепочки рассуждений', 'desc': 'Исследователи выявили проблему «эффекта снежного кома» в больших рассуждающих моделях (LRM), где небольшие отклонения в цепочке рассуждений постепенно усиливаются и приводят к опасным ответам или чрезмерным отказам. Для решения этой проблемы предложен метод AdvChain, который обучает модели динамической самокоррекции через adversarial настройку Chain-of-Thought. Метод использует специальный датасет с примерами «Искушение-Исправление» и «Колебание-Исправление», где модели учатся восстанавливаться от вредных отклонений в рассуждениях. Эксперименты показали, что AdvChain значительно повышает устойчивость к jailbreak атакам и снижает излишние отказы на безвредных запросах, улучшая баланс между безопасностью и полезностью.'}, 'en': {'title': 'Empowering Models with Dynamic Self-Correction for Safer Reasoning', 'desc': "AdvChain is a novel approach designed to improve the safety and reliability of Large Reasoning Models (LRMs) by enabling them to self-correct during complex reasoning tasks. It addresses a critical issue known as the snowball effect, where small errors in reasoning can escalate, leading to harmful outcomes or excessive caution. By using adversarial chain-of-thought tuning, AdvChain trains models with specific datasets that help them recover from these reasoning drifts. The results demonstrate that this method enhances the models' robustness against attacks while maintaining their reasoning capabilities, thus achieving a better balance between safety and utility."}, 'zh': {'title': 'AdvChain：提升推理模型的安全性与可靠性', 'desc': 'AdvChain通过对抗性思维链调优，增强了大型推理模型的安全性和可靠性。该方法解决了当前思维链调优中的雪崩效应问题，避免了小的推理偏差在思维过程中逐渐放大的情况。通过构建包含诱惑-纠正和犹豫-纠正样本的数据集，模型学习如何从有害的推理偏差中恢复。实验表明，AdvChain显著提高了对越狱攻击和思维链劫持的鲁棒性，同时减少了对良性提示的过度拒绝，达到了安全性和效用的优良平衡。'}}}, {'id': 'https://huggingface.co/papers/2509.23143', 'title': 'MathBode: Frequency-Domain Fingerprints of LLM Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2509.23143', 'abstract': 'MathBode provides a diagnostic for mathematical reasoning in LLMs by analyzing frequency-resolved metrics of model outputs compared to exact solutions, revealing systematic low-pass behavior and phase lag.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents MathBode, a dynamic diagnostic for mathematical reasoning in large language models (LLMs). Instead of one-shot accuracy, MathBode treats each parametric problem as a system: we drive a single parameter sinusoidally and fit first-harmonic responses of model outputs and exact solutions. This yields interpretable, frequency-resolved metrics -- gain (amplitude tracking) and phase (lag) -- that form Bode-style fingerprints. Across five closed-form families (linear solve, ratio/saturation, compound interest, 2x2 linear systems, similar triangles), the diagnostic surfaces systematic low-pass behavior and growing phase lag that accuracy alone obscures. We compare several models against a symbolic baseline that calibrates the instrument (G approx 1, phi approx 0). Results separate frontier from mid-tier models on dynamics, providing a compact, reproducible protocol that complements standard benchmarks with actionable measurements of reasoning fidelity and consistency. We open-source the dataset and code to enable further research and adoption.', 'score': 2, 'issue_id': 6154, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': 'c2f1e8211a73ca99', 'authors': ['Charles L. Wang'], 'affiliations': ['Department of Computer Science, Columbia University'], 'pdf_title_img': 'assets/pdf/title_img/2509.23143.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#reasoning', '#math', '#interpretability', '#dataset'], 'emoji': '📊', 'ru': {'title': 'Частотный анализ математического мышления в LLM', 'desc': 'В статье представлен MathBode - новый диагностический инструмент для анализа математических способностей больших языковых моделей. Вместо простой оценки точности авторы предлагают анализировать ответы моделей на параметрические задачи через частотные характеристики, получая метрики усиления и фазового сдвига. Исследование показало, что LLM демонстрируют систематическое low-pass поведение и растущую фазовую задержку при решении математических задач. Этот подход позволяет лучше различать модели разного уровня и дает более глубокое понимание качества математического мышления AI.'}, 'en': {'title': 'MathBode: Unveiling the Dynamics of Mathematical Reasoning in LLMs', 'desc': 'This paper introduces MathBode, a diagnostic tool designed to evaluate mathematical reasoning in large language models (LLMs) by analyzing their outputs against exact solutions. Instead of focusing solely on accuracy, MathBode examines how models respond to changes in parameters, using frequency-resolved metrics like gain and phase to create Bode-style fingerprints. The findings reveal that many models exhibit systematic low-pass behavior and increasing phase lag, which are not apparent when only considering accuracy. By comparing various models to a symbolic baseline, MathBode provides a new way to assess reasoning fidelity and consistency, and the authors have made the dataset and code available for further research.'}, 'zh': {'title': 'MathBode：揭示LLMs数学推理的新工具', 'desc': '本文介绍了MathBode，这是一种用于大型语言模型（LLMs）数学推理的动态诊断工具。MathBode通过分析模型输出与精确解的频率分辨度量，揭示了系统性的低通行为和相位滞后。与单次准确性不同，MathBode将每个参数问题视为一个系统，通过驱动单个参数的正弦波并拟合模型输出和精确解的第一谐波响应，生成可解释的频率分辨度量。研究结果显示，MathBode能够有效区分不同模型的推理能力，并提供可重复的协议，以补充标准基准测试。'}}}, {'id': 'https://huggingface.co/papers/2509.22518', 'title': 'REMA: A Unified Reasoning Manifold Framework for Interpreting Large\n  Language Model', 'url': 'https://huggingface.co/papers/2509.22518', 'abstract': "The Reasoning Manifold framework quantifies and localizes reasoning failures in Large Language Models by analyzing geometric deviations in internal representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding how Large Language Models (LLMs) perform complex reasoning and their failure mechanisms is a challenge in interpretability research. To provide a measurable geometric analysis perspective, we define the concept of the Reasoning Manifold, a latent low-dimensional geometric structure formed by the internal representations corresponding to all correctly reasoned generations. This structure can be conceptualized as the embodiment of the effective thinking paths that the model has learned to successfully solve a given task. Based on this concept, we build REMA, a framework that explains the origins of failures by quantitatively comparing the spatial relationships of internal model representations corresponding to both erroneous and correct reasoning samples. Specifically, REMA first quantifies the geometric deviation of each erroneous representation by calculating its k-nearest neighbors distance to the approximated manifold formed by correct representations, thereby providing a unified failure signal. It then localizes the divergence points where these deviations first become significant by tracking this deviation metric across the model's layers and comparing it against a baseline of internal fluctuations from correct representations, thus identifying where the reasoning chain begins to go off-track. Our extensive experiments on diverse language and multimodal models and tasks demonstrate the low-dimensional nature of the reasoning manifold and the high separability between erroneous and correct reasoning representations. The results also validate the effectiveness of the REMA framework in analyzing the origins of reasoning failures. This research connects abstract reasoning failures to measurable geometric deviations in representations, providing new avenues for in-depth understanding and diagnosis of the internal computational processes of black-box models.", 'score': 2, 'issue_id': 6154, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '22e35d1e36530f86', 'authors': ['Bo Li', 'Guanzhi Deng', 'Ronghao Chen', 'Junrong Yue', 'Shuo Zhang', 'Qinghua Zhao', 'Linqi Song', 'Lijie Wen'], 'affiliations': ['Baidu Inc.', 'Beihang University', 'Beijing University of Posts and Telecommunications', 'City University of Hong Kong', 'Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.22518.jpg', 'data': {'categories': ['#architecture', '#data', '#interpretability', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Геометрия мышления: как найти сбои в рассуждениях ИИ', 'desc': "Исследователи предложили фреймворк Reasoning Manifold для анализа ошибок рассуждений в больших языковых моделях через геометрические отклонения во внутренних представлениях. Они определили концепцию 'многообразия рассуждений' как низкоразмерную геометрическую структуру, образованную внутренними представлениями правильных рассуждений модели. Фреймворк REMA количественно измеряет геометрические отклонения ошибочных представлений от этого многообразия и локализует точки расхождения по слоям модели. Эксперименты показали высокую разделимость между правильными и ошибочными представлениями рассуждений, что открывает новые возможности для диагностики внутренних вычислительных процессов LLM."}, 'en': {'title': 'Unveiling Reasoning Failures through Geometric Analysis', 'desc': "The paper introduces the Reasoning Manifold framework, which helps to identify and measure reasoning failures in Large Language Models (LLMs) by examining geometric changes in their internal representations. It defines a low-dimensional geometric structure, called the Reasoning Manifold, that represents successful reasoning paths learned by the model. The framework, REMA, quantifies how far erroneous representations deviate from this manifold by analyzing their spatial relationships with correct representations. Through experiments, the study shows that these geometric deviations can effectively indicate where reasoning errors occur, enhancing our understanding of LLMs' internal processes."}, 'zh': {'title': '推理流形：揭示语言模型推理失败的几何分析', 'desc': '本文提出了一个名为推理流形的框架，用于量化和定位大型语言模型中的推理失败。通过分析内部表示的几何偏差，研究者能够理解模型在复杂推理中的表现及其失败机制。推理流形是由正确推理生成的内部表示形成的低维几何结构，代表了模型成功解决任务的有效思维路径。REMA框架通过比较错误和正确推理样本的空间关系，定量分析推理失败的来源，帮助我们更深入地理解黑箱模型的内部计算过程。'}}}, {'id': 'https://huggingface.co/papers/2509.24981', 'title': 'Random Policy Valuation is Enough for LLM Reasoning with Verifiable\n  Rewards', 'url': 'https://huggingface.co/papers/2509.24981', 'abstract': "ROVER, a minimalist RL method, achieves superior performance and diversity in LLM math reasoning by leveraging Q-values from a fixed random policy, bypassing complex policy iteration.  \t\t\t\t\tAI-generated summary \t\t\t\t RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for improving the reasoning abilities of large language models (LLMs). Current methods rely primarily on policy optimization frameworks like PPO and GRPO, which follow generalized policy iteration that alternates between evaluating the current policy's value and improving the policy based on evaluation. While effective, they often suffer from training instability and diversity collapse, requiring complex heuristic tricks and careful tuning. We observe that standard RLVR in math reasoning can be formalized as a specialized finite-horizon Markov Decision Process with deterministic state transitions, tree-structured dynamics, and binary terminal rewards. Though large in scale, the underlying structure is simpler than general-purpose control settings for which popular RL algorithms (e.g., PPO) were developed, suggesting that several sophisticated techniques in existing methods may be reduced or even omitted. Based on this insight, we prove a surprising result: the optimal action can be recovered from the Q-function of a fixed uniformly random policy, thereby bypassing the generalized policy iteration loop and its associated heuristics. We introduce Random Policy Valuation for Diverse Reasoning (ROVER) to translate this principle into a practical and scalable algorithm for LLM math reasoning, a minimalist yet highly effective RL method that samples actions from a softmax over these uniform-policy Q-values. ROVER preserves diversity throughout training, allowing sustained exploration of multiple valid pathways. Across multiple base models and standard math reasoning benchmarks, ROVER demonstrates superior performance in both quality (+8.2 on pass@1, +16.8 on pass@256) and diversity (+17.6\\%), despite its radical simplification compared to strong, complicated existing methods.", 'score': 1, 'issue_id': 6156, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '9271e4a6f3547297', 'authors': ['Haoran He', 'Yuxiao Ye', 'Qingpeng Cai', 'Chen Hu', 'Binxing Jiao', 'Daxin Jiang', 'Ling Pan'], 'affiliations': ['Hong Kong University of Science and Technology', 'Kuaishou Technology', 'StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2509.24981.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#rlhf', '#optimization'], 'emoji': '🎲', 'ru': {'title': 'Случайная политика превосходит сложные алгоритмы в математических рассуждениях', 'desc': 'В статье представлен ROVER - минималистичный метод обучения с подкреплением для улучшения математических рассуждений LLM. Авторы доказывают, что оптимальные действия можно восстановить из Q-функции фиксированной случайной политики, избегая сложной итерации политик. Метод сэмплирует действия через softmax над Q-значениями случайной политики, сохраняя разнообразие решений на протяжении всего обучения. ROVER показывает превосходную производительность (+8.2 на pass@1, +16.8 на pass@256) и разнообразие (+17.6%) по сравнению с существующими сложными методами.'}, 'en': {'title': 'Simplifying RL for Superior Math Reasoning in LLMs', 'desc': 'ROVER is a new reinforcement learning (RL) method designed to enhance the math reasoning capabilities of large language models (LLMs) by simplifying the training process. Instead of using complex policy optimization techniques like PPO, ROVER utilizes Q-values derived from a fixed random policy, which allows it to avoid the instability and diversity issues common in traditional methods. This approach enables ROVER to maintain a diverse set of reasoning pathways while achieving better performance on math reasoning tasks. The results show that ROVER outperforms existing methods in both quality and diversity, proving that simpler techniques can be highly effective in RL applications.'}, 'zh': {'title': 'ROVER：简化的强化学习，提升数学推理能力', 'desc': 'ROVER是一种简化的强化学习方法，专注于大语言模型的数学推理。它通过利用固定随机策略的Q值，避免了复杂的策略迭代过程，从而提高了性能和多样性。与传统的策略优化方法相比，ROVER在训练过程中保持了多样性，能够持续探索多条有效路径。实验结果表明，ROVER在多个基准测试中表现出色，质量和多样性均有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2509.24709', 'title': 'IWR-Bench: Can LVLMs reconstruct interactive webpage from a user\n  interaction video?', 'url': 'https://huggingface.co/papers/2509.24709', 'abstract': "IWR-Bench evaluates Large Vision-Language Models in reconstructing interactive webpages from video, highlighting challenges in multi-modal reasoning and code generation.  \t\t\t\t\tAI-generated summary \t\t\t\t The webpage-to-code task requires models to understand visual representations of webpages and generate corresponding code. However, existing benchmarks primarily focus on static screenshot-to-code tasks, thereby overlooking the dynamic interactions fundamental to real-world web applications. To address this limitation, this paper introduces IWR-Bench, a novel benchmark for evaluating the capabilities of Large Vision-Language Models (LVLMs) in interactive webpage reconstruction from video. IWR-Bench comprises 113 meticulously curated tasks from 100 real-world websites, with 1,001 actions and featuring diverse interaction complexities (e.g., web games), visual styles, and domains. Aligning with standard web development practices, each task includes not only user interaction videos but also all crawled static assets (e.g., images, videos). This benchmark evaluates models on two fundamental challenges: comprehensive multi-modal reasoning to infer interaction logic from video and assets, and advanced code generation to translate this logic into functional code. An agent-as-a-judge framework with a comprehensive metric system automatically assesses the functional correctness and visual fidelity of generated webpages. Extensive experiments on 28 LVLMs reveal a significant challenge: the best model achieves an overall score of only 36.35%, as functional correctness (24.39% IFS) lags significantly behind visual fidelity (64.25% VFS). These results highlight critical limitations in current models' ability to reason about temporal dynamics and synthesize event-driven logic, establishing IWR-Bench as a challenging frontier for vision-language research. The benchmark and evaluation code will be made publicly available. Code is available at https://github.com/L-O-I/IWR-Bench.", 'score': 1, 'issue_id': 6154, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '66300a4a6e51add9', 'authors': ['Yang Chen', 'Minghao Liu', 'Yufan Shen', 'Yunwen Li', 'Tianyuan Huang', 'Xinyu Fang', 'Tianyu Zheng', 'Wenxuan Huang', 'Cheng Yang', 'Daocheng Fu', 'Jianbiao Mei', 'Rong Wu', 'Licheng Wen', 'Xuemeng Yang', 'Song Mao', 'Qunshu Lin', 'Zhi Yu', 'Yongliang Shen', 'Yu Qiao', 'Botian Shi'], 'affiliations': ['Bai et al.', 'Caron et al.', 'Comanici et al.', 'Deka et al.', 'Gui et al.', 'Gupta & Kembhavi', 'IWR-Bench Team', 'Jiang et al.', 'Jimenez et al.', 'Laurencon et al.', 'Lee et al.', 'Li et al.', 'Luo et al.', 'OpenAI', 'Radford et al.', 'Si et al.', 'Song et al.', 'Xiao et al.', 'Yun et al.', 'Zhang et al.', 'Zhuge et al.'], 'pdf_title_img': 'assets/pdf/title_img/2509.24709.jpg', 'data': {'categories': ['#open_source', '#optimization', '#benchmark', '#reasoning', '#cv', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'От видео к коду: новый вызов для AI в создании интерактивных веб-страниц', 'desc': 'В статье представлен IWR-Bench — новый бенчмарк для оценки способности больших vision-language моделей восстанавливать интерактивные веб-страницы из видео. Бенчмарк включает 113 задач с реальных сайтов, содержащих 1001 действие и разнообразные типы взаимодействий. Модели должны анализировать видео пользовательских действий и генерировать функциональный код веб-страниц. Лучшая модель достигла лишь 36.35% общего результата, что показывает серьезные ограничения современных LLM в понимании временной динамики и логики взаимодействий.'}, 'en': {'title': 'IWR-Bench: Advancing Interactive Webpage Reconstruction with LVLMs', 'desc': 'The paper introduces IWR-Bench, a new benchmark designed to evaluate Large Vision-Language Models (LVLMs) on their ability to reconstruct interactive webpages from video inputs. Unlike previous benchmarks that focus on static screenshots, IWR-Bench emphasizes the dynamic interactions that are essential for real-world web applications. It includes 113 tasks from 100 websites, featuring a variety of interaction complexities and visual styles, and assesses models on their multi-modal reasoning and code generation capabilities. The findings reveal that current models struggle significantly with functional correctness, achieving only 24.39% in this area, while visual fidelity is comparatively higher at 64.25%, indicating a need for improved reasoning about temporal dynamics and event-driven logic.'}, 'zh': {'title': 'IWR-Bench：重建交互网页的新挑战', 'desc': 'IWR-Bench是一个新的基准，用于评估大型视觉-语言模型在从视频重建交互网页方面的能力。该基准包含来自100个真实网站的113个精心策划的任务，涵盖了多种交互复杂性和视觉风格。研究发现，现有模型在理解视频中的交互逻辑和生成功能代码方面存在显著挑战，最佳模型的整体得分仅为36.35%。这表明当前模型在处理时间动态和合成事件驱动逻辑方面存在关键限制，IWR-Bench为视觉-语言研究设定了新的挑战。'}}}, {'id': 'https://huggingface.co/papers/2509.23115', 'title': 'RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human\n  Mobility', 'url': 'https://huggingface.co/papers/2509.23115', 'abstract': "RHYTHM uses hierarchical temporal tokenization and large language models to predict human mobility, capturing long-range dependencies and multi-scale periodic behaviors efficiently.  \t\t\t\t\tAI-generated summary \t\t\t\t Predicting human mobility is inherently challenging due to complex long-range dependencies and multi-scale periodic behaviors. To address this, we introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), a unified framework that leverages large language models (LLMs) as general-purpose spatio-temporal predictors and trajectory reasoners. Methodologically, RHYTHM employs temporal tokenization to partition each trajectory into daily segments and encode them as discrete tokens with hierarchical attention that captures both daily and weekly dependencies, thereby significantly reducing the sequence length while preserving cyclical information. Additionally, we enrich token representations by adding pre-computed prompt embeddings for trajectory segments and prediction targets via a frozen LLM, and feeding these combined embeddings back into the LLM backbone to capture complex interdependencies. Computationally, RHYTHM freezes the pretrained LLM's backbone to reduce attention complexity and memory cost. We evaluate our model against state-of-the-art methods using three real-world datasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a 5.0% increase on weekends, and a 24.6% reduction in training time. Code is publicly available at https://github.com/he-h/rhythm.", 'score': 1, 'issue_id': 6154, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': 'c32db800b68d7fd5', 'authors': ['Haoyu He', 'Haozheng Luo', 'Yan Chen', 'Qi R. Wang'], 'affiliations': ['Northeastern University', 'Northwestern University'], 'pdf_title_img': 'assets/pdf/title_img/2509.23115.jpg', 'data': {'categories': ['#architecture', '#data', '#optimization', '#benchmark', '#open_source', '#reasoning', '#training', '#long_context', '#dataset'], 'emoji': '🏃', 'ru': {'title': 'Ритмы движения: предсказание мобильности через иерархическую токенизацию', 'desc': 'В статье представлена модель RHYTHM для предсказания человеческой мобильности, которая использует иерархическую токенизацию временных данных и большие языковые модели. Модель разбивает траектории на ежедневные сегменты и кодирует их как дискретные токены с иерархическим вниманием для захвата зависимостей на уровне дней и недель. Архитектура включает предварительно вычисленные эмбеддинги подсказок и замороженный backbone LLM для снижения вычислительной сложности. Экспериментальные результаты показывают улучшение точности на 2.4% и сокращение времени обучения на 24.6% по сравнению с современными методами.'}, 'en': {'title': 'RHYTHM: Smart Predictions for Human Mobility', 'desc': 'RHYTHM is a novel framework designed to predict human mobility by effectively managing complex long-range dependencies and periodic behaviors. It utilizes hierarchical temporal tokenization to break down mobility trajectories into daily segments, which are then encoded as discrete tokens, allowing the model to focus on both daily and weekly patterns. By incorporating large language models (LLMs) and enriching token representations with prompt embeddings, RHYTHM captures intricate interdependencies while minimizing computational costs. The model demonstrates significant improvements in accuracy and efficiency compared to existing methods, particularly during weekends, showcasing its effectiveness in real-world applications.'}, 'zh': {'title': 'RHYTHM：高效预测人类移动的智能框架', 'desc': 'RHYTHM是一种用于预测人类移动的框架，利用层次时间标记化和大型语言模型来捕捉长距离依赖和多尺度周期性行为。该方法通过将每个轨迹分割为日常片段，并使用层次注意力编码这些片段，从而显著减少序列长度，同时保留周期性信息。RHYTHM还通过添加预计算的提示嵌入来丰富标记表示，进一步捕捉复杂的相互依赖关系。经过评估，RHYTHM在准确性和训练时间上均表现出显著的提升。'}}}, {'id': 'https://huggingface.co/papers/2509.22830', 'title': 'ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents', 'url': 'https://huggingface.co/papers/2509.22830', 'abstract': "ChatInject, a novel attack exploiting structured chat templates and persuasive multi-turn dialogues, significantly enhances attack success rates on large language model-based agents compared to traditional methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The growing deployment of large language model (LLM) based agents that interact with external environments has created new attack surfaces for adversarial manipulation. One major threat is indirect prompt injection, where attackers embed malicious instructions in external environment output, causing agents to interpret and execute them as if they were legitimate prompts. While previous research has focused primarily on plain-text injection attacks, we find a significant yet underexplored vulnerability: LLMs' dependence on structured chat templates and their susceptibility to contextual manipulation through persuasive multi-turn dialogues. To this end, we introduce ChatInject, an attack that formats malicious payloads to mimic native chat templates, thereby exploiting the model's inherent instruction-following tendencies. Building on this foundation, we develop a persuasion-driven Multi-turn variant that primes the agent across conversational turns to accept and execute otherwise suspicious actions. Through comprehensive experiments across frontier LLMs, we demonstrate three critical findings: (1) ChatInject achieves significantly higher average attack success rates than traditional prompt injection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13% to 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong performance at average 52.33% success rate on InjecAgent, (2) chat-template-based payloads demonstrate strong transferability across models and remain effective even against closed-source LLMs, despite their unknown template structures, and (3) existing prompt-based defenses are largely ineffective against this attack approach, especially against Multi-turn variants. These findings highlight vulnerabilities in current agent systems.", 'score': 1, 'issue_id': 6153, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '5a9ccef5c0547575', 'authors': ['Hwan Chang', 'Yonghyun Jun', 'Hwanhee Lee'], 'affiliations': ['Department of Artificial Intelligence, Chung-Ang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.22830.jpg', 'data': {'categories': ['#agents', '#rlhf', '#security'], 'emoji': '💬', 'ru': {'title': 'Обман через чат-шаблоны: новая угроза для AI-агентов', 'desc': 'Исследователи представили ChatInject - новый тип атаки на LLM-агентов, который использует структурированные chat-шаблоны для внедрения вредоносных инструкций. Атака форматирует злонамеренные команды так, чтобы они выглядели как обычные сообщения в чате, эксплуатируя склонность модели следовать инструкциям. Многоходовая версия атаки использует убедительный диалог для постепенной подготовки агента к выполнению подозрительных действий. Эксперименты показали, что ChatInject достигает значительно более высоких показателей успешности по сравнению с традиционными методами prompt injection, при этом существующие защитные механизмы оказываются малоэффективными.'}, 'en': {'title': 'ChatInject: Elevating Attack Success with Persuasive Dialogues', 'desc': 'The paper introduces ChatInject, a new type of attack that targets large language model (LLM) agents by using structured chat templates and persuasive multi-turn dialogues. This method significantly increases the success rate of attacks compared to traditional prompt injection techniques. The research reveals that LLMs are particularly vulnerable to indirect prompt injection, where malicious instructions are hidden in seemingly legitimate outputs. Through experiments, the authors demonstrate that ChatInject not only achieves higher success rates but also shows strong transferability across different models, exposing critical weaknesses in current defenses against such attacks.'}, 'zh': {'title': '利用聊天模板的攻击新方式：ChatInject', 'desc': 'ChatInject是一种新型攻击方法，利用结构化聊天模板和说服性的多轮对话，显著提高了对基于大型语言模型的代理的攻击成功率。该方法通过将恶意负载格式化为类似于原生聊天模板的形式，利用了模型固有的指令遵循倾向。研究表明，ChatInject在多个前沿大型语言模型上表现出更高的攻击成功率，尤其是在多轮对话中，成功率可达52.33%。此外，现有的基于提示的防御措施对这种攻击方法的有效性较低，尤其是针对多轮变体。'}}}, {'id': 'https://huggingface.co/papers/2509.16538', 'title': 'Advancing Reference-free Evaluation of Video Captions with Factual\n  Analysis', 'url': 'https://huggingface.co/papers/2509.16538', 'abstract': 'VC-Inspector, a reference-free and factually grounded caption quality evaluator, uses large language models to generate pseudo captions and train a multimodal model, demonstrating superior performance in evaluating video captions across diverse domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Video captions offer concise snapshots of actors, objects, and actions within a video, serving as valuable assets for applications such as question answering and event localization. However, acquiring human annotations for video captions is costly or even impractical, especially when dealing with diverse video domains. Existing models trained on supervised datasets face challenges in evaluating performance across different domains due to the reliance on reference-based evaluation protocols, which necessitate ground truth captions. This assumption is unrealistic for evaluating videos in the wild. To address these limitations, we propose a reference-free evaluation framework that does not require ground truth captions, focusing on factual grounding to ensure accurate assessment of caption quality. We introduce VC-Inspector, a novel caption quality evaluator that is both reference-free and factually grounded. Utilizing large language models, we generate pseudo captions of varying quality based on supervised data, which are subsequently used to train a multimodal model (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior alignment with human judgments on the VATEX-Eval dataset, outperforming existing methods. The performance also generalizes to image caption datasets, Flickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos. Overall, VC-Inspector offers a scalable and generalizable solution for evaluating the factual accuracy of video captions, paving the way for more effective and objective assessment methodologies in diverse video domains.', 'score': 0, 'issue_id': 6156, 'pub_date': '2025-09-20', 'pub_date_card': {'ru': '20 сентября', 'en': 'September 20', 'zh': '9月20日'}, 'hash': '9a484f605f3326dc', 'authors': ['Shubhashis Roy Dipta', 'Tz-Ying Wu', 'Subarna Tripathi'], 'affiliations': ['Intel Labs', 'University of Maryland, Baltimore County'], 'pdf_title_img': 'assets/pdf/title_img/2509.16538.jpg', 'data': {'categories': ['#interpretability', '#video', '#multimodal', '#benchmark', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'Оценка видео субтитров без эталонов через фактическую обоснованность', 'desc': 'Исследователи представили VC-Inspector - новую систему оценки качества видео субтитров, которая работает без эталонных подписей. Система использует большие языковые модели для генерации псевдо-подписей разного качества, на которых затем обучается мультимодальная модель-оценщик. Подход фокусируется на фактической обоснованности подписей, что позволяет точно оценивать их качество в различных доменах. Метод показал превосходную производительность на датасете VATEX-Eval и успешно обобщается на изображения, рассматривая их как одно-кадровые видео.'}, 'en': {'title': 'Revolutionizing Video Caption Evaluation with VC-Inspector', 'desc': 'VC-Inspector is a novel tool designed to evaluate the quality of video captions without needing reference captions. It uses large language models to create pseudo captions, which helps train a multimodal model for better assessment. This approach addresses the challenges of traditional methods that rely on human-annotated ground truth, making it more practical for diverse video content. The results show that VC-Inspector aligns well with human evaluations and performs effectively across various datasets, enhancing the evaluation of video captions.'}, 'zh': {'title': '无参考的字幕质量评估新方法', 'desc': 'VC-Inspector是一种无参考且基于事实的字幕质量评估工具，旨在解决视频字幕评估中的挑战。它利用大型语言模型生成伪字幕，并训练一个多模态模型，从而在不同领域的视频字幕评估中表现出色。与传统依赖参考的评估方法不同，VC-Inspector不需要真实的字幕，专注于确保字幕质量的准确评估。该方法在VATEX-Eval数据集上与人类判断高度一致，并且在图像字幕数据集上也表现良好，展示了其可扩展性和通用性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (6)', '#agi (3)', '#alignment (7)', '#architecture (13)', '#audio (1)', '#benchmark (16)', '#cv (7)', '#data (9)', '#dataset (9)', '#diffusion (6)', '#ethics', '#games (6)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (4)', '#interpretability (7)', '#leakage (1)', '#long_context (6)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (13)', '#open_source (9)', '#optimization (28)', '#plp', '#rag', '#reasoning (22)', '#rl (16)', '#rlhf (9)', '#robotics', '#science (1)', '#security (2)', '#small_models (2)', '#story_generation', '#survey (2)', '#synthetic (4)', '#training (30)', '#transfer_learning (4)', '#video (6)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-09-30 06:17',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-30 06:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-30 06:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    