{
    "date": {
        "ru": "13 октября",
        "en": "October 13",
        "zh": "10月13日"
    },
    "time_utc": "2025-10-13 02:26",
    "weekday": 0,
    "issue_id": 6375,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2510.04533",
            "title": "TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion\n  Sampling",
            "url": "https://huggingface.co/papers/2510.04533",
            "abstract": "Tangential Amplifying Guidance (TAG) improves diffusion model sample quality by directly amplifying tangential components of estimated scores without modifying the model architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent diffusion models achieve the state-of-the-art performance in image generation, but often suffer from semantic inconsistencies or hallucinations. While various inference-time guidance methods can enhance generation, they often operate indirectly by relying on external signals or architectural modifications, which introduces additional computational overhead. In this paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and direct guidance method that operates solely on trajectory signals without modifying the underlying diffusion model. TAG leverages an intermediate sample as a projection basis and amplifies the tangential components of the estimated scores with respect to this basis to correct the sampling trajectory. We formalize this guidance process by leveraging a first-order Taylor expansion, which demonstrates that amplifying the tangential component steers the state toward higher-probability regions, thereby reducing inconsistencies and enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module that improves diffusion sampling fidelity with minimal computational addition, offering a new perspective on diffusion guidance.",
            "score": 16,
            "issue_id": 6375,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 октября",
                "en": "October 6",
                "zh": "10月6日"
            },
            "hash": "0b9aca186679e05a",
            "authors": [
                "Hyunmin Cho",
                "Donghoon Ahn",
                "Susung Hong",
                "Jee Eun Kim",
                "Seungryong Kim",
                "Kyong Hwan Jin"
            ],
            "affiliations": [
                "KAIST AI",
                "Korea University",
                "University of California, Berkeley",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04533.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#inference",
                    "#optimization",
                    "#hallucinations"
                ],
                "emoji": "📐",
                "ru": {
                    "title": "Улучшение генерации через усиление касательных компонент",
                    "desc": "Статья предлагает метод Tangential Amplifying Guidance (TAG) для улучшения качества генерации диффузионных моделей. Вместо использования внешних сигналов или изменения архитектуры, TAG работает напрямую с траекторией сэмплирования, усиливая касательные компоненты оценочных скоров. Метод направляет процесс генерации в области с более высокой вероятностью, что уменьшает семантические несоответствия и галлюцинации. TAG является plug-and-play модулем, который добавляет минимальные вычислительные затраты и работает с любой архитектурой диффузионной модели."
                },
                "en": {
                    "title": "Enhancing Diffusion Models with Direct Tangential Guidance",
                    "desc": "Tangential Amplifying Guidance (TAG) is a novel method designed to enhance the quality of samples generated by diffusion models. Unlike traditional guidance methods that modify the model architecture or rely on external signals, TAG directly amplifies the tangential components of estimated scores during the sampling process. This approach uses an intermediate sample as a basis for projection, allowing for a more efficient correction of the sampling trajectory. By applying a first-order Taylor expansion, TAG effectively steers the sampling towards higher-probability regions, thereby reducing semantic inconsistencies and improving overall sample fidelity."
                },
                "zh": {
                    "title": "切向放大引导：提升扩散模型生成质量的新方法",
                    "desc": "本文提出了一种新的引导方法，称为切向放大引导（TAG），旨在提高扩散模型生成图像的质量。TAG 通过直接放大估计分数的切向分量来修正采样轨迹，而不需要修改模型架构。该方法利用中间样本作为投影基础，采用一阶泰勒展开形式化引导过程，从而将状态引导至更高概率区域，减少语义不一致性。TAG 是一个即插即用的模块，能够在不增加计算负担的情况下提升扩散采样的保真度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08673",
            "title": "Thinking with Camera: A Unified Multimodal Model for Camera-Centric\n  Understanding and Generation",
            "url": "https://huggingface.co/papers/2510.08673",
            "abstract": "Puffin, a unified multimodal model, integrates language regression and diffusion-based generation to enhance camera-centric spatial understanding and generation by treating camera parameters as language.  \t\t\t\t\tAI-generated summary \t\t\t\t Camera-centric understanding and generation are two cornerstones of spatial intelligence, yet they are typically studied in isolation. We present Puffin, a unified camera-centric multimodal model that extends spatial awareness along the camera dimension. Puffin integrates language regression and diffusion-based generation to interpret and create scenes from arbitrary viewpoints. To bridge the modality gap between cameras and vision-language, we introduce a novel paradigm that treats camera as language, enabling thinking with camera. This guides the model to align spatially grounded visual cues with photographic terminology while reasoning across geometric context. Puffin is trained on Puffin-4M, a large-scale dataset of 4 million vision-language-camera triplets. We incorporate both global camera parameters and pixel-wise camera maps, yielding flexible and reliable spatial generation. Experiments demonstrate Puffin superior performance over specialized models for camera-centric generation and understanding. With instruction tuning, Puffin generalizes to diverse cross-view tasks such as spatial imagination, world exploration, and photography guidance. We will release the code, models, dataset pipeline, and benchmark to advance multimodal spatial intelligence research.",
            "score": 7,
            "issue_id": 6375,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 октября",
                "en": "October 9",
                "zh": "10月9日"
            },
            "hash": "2b4c322ac948b29e",
            "authors": [
                "Kang Liao",
                "Size Wu",
                "Zhonghua Wu",
                "Linyi Jin",
                "Chao Wang",
                "Yikai Wang",
                "Fei Wang",
                "Wei Li",
                "Chen Change Loy"
            ],
            "affiliations": [
                "Max-Planck Institute for Informatics",
                "S-Lab, Nanyang Technological University",
                "SenseTime Research",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08673.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#dataset",
                    "#multimodal",
                    "#benchmark",
                    "#alignment",
                    "#open_source"
                ],
                "emoji": "📸",
                "ru": {
                    "title": "Камера как язык: единая модель для понимания и генерации сцен",
                    "desc": "Puffin — это мультимодальная модель, которая объединяет понимание и генерацию изображений с учётом параметров камеры. Авторы предлагают новый подход «камера как язык», при котором параметры камеры обрабатываются как текстовые данные, что позволяет модели рассуждать о геометрическом контексте сцены. Модель обучена на датасете Puffin-4M из 4 миллионов триплетов изображение-текст-камера и использует как глобальные параметры камеры, так и попиксельные карты. Puffin демонстрирует превосходные результаты в задачах генерации изображений с заданной точки обзора и понимания пространственных характеристик сцены."
                },
                "en": {
                    "title": "Puffin: Bridging Language and Camera for Enhanced Spatial Intelligence",
                    "desc": "Puffin is a new multimodal model designed to improve how machines understand and generate images based on camera perspectives. It combines language regression and diffusion-based generation techniques to interpret scenes from different viewpoints. By treating camera parameters as a form of language, Puffin aligns visual information with photographic terms, enhancing its spatial reasoning capabilities. Trained on a large dataset of vision-language-camera triplets, Puffin outperforms existing models in tasks related to camera-centric understanding and generation."
                },
                "zh": {
                    "title": "Puffin：相机视角下的空间智能新突破",
                    "desc": "Puffin是一种统一的多模态模型，旨在增强基于相机的空间理解和生成。它通过将相机参数视为语言，结合语言回归和基于扩散的生成方法，来处理和创建不同视角的场景。Puffin在一个包含400万对视觉-语言-相机三元组的大规模数据集上进行训练，能够灵活可靠地进行空间生成。实验表明，Puffin在相机中心生成和理解方面的表现优于专门模型，并且通过指令调优，能够适应多样的跨视角任务。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.04759",
            "title": "Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open\n  Vocabulary Occupancy Prediction",
            "url": "https://huggingface.co/papers/2510.04759",
            "abstract": "PG-Occ, a Progressive Gaussian Transformer Framework, enhances 3D occupancy prediction with progressive densification and anisotropy-aware sampling, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The 3D occupancy prediction task has witnessed remarkable progress in recent years, playing a crucial role in vision-based autonomous driving systems. While traditional methods are limited to fixed semantic categories, recent approaches have moved towards predicting text-aligned features to enable open-vocabulary text queries in real-world scenes. However, there exists a trade-off in text-aligned scene modeling: sparse Gaussian representation struggles to capture small objects in the scene, while dense representation incurs significant computational overhead. To address these limitations, we present PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables open-vocabulary 3D occupancy prediction. Our framework employs progressive online densification, a feed-forward strategy that gradually enhances the 3D Gaussian representation to capture fine-grained scene details. By iteratively enhancing the representation, the framework achieves increasingly precise and detailed scene understanding. Another key contribution is the introduction of an anisotropy-aware sampling strategy with spatio-temporal fusion, which adaptively assigns receptive fields to Gaussians at different scales and stages, enabling more effective feature aggregation and richer scene information capture. Through extensive evaluations, we demonstrate that PG-Occ achieves state-of-the-art performance with a relative 14.3% mIoU improvement over the previous best performing method. Code and pretrained models will be released upon publication on our project page: https://yanchi-3dv.github.io/PG-Occ",
            "score": 4,
            "issue_id": 6375,
            "pub_date": "2025-10-06",
            "pub_date_card": {
                "ru": "6 октября",
                "en": "October 6",
                "zh": "10月6日"
            },
            "hash": "681d2ff717aff51e",
            "authors": [
                "Chi Yan",
                "Dan Xu"
            ],
            "affiliations": [
                "The Hong Kong University of Science and Technology (HKUST)",
                "ZEEKR Automobile R&D Co., Ltd"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.04759.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Прогрессивное уплотнение гауссиан для понимания 3D-сцен",
                    "desc": "Статья представляет PG-Occ — фреймворк на основе Gaussian Transformer для предсказания 3D occupancy с поддержкой open-vocabulary запросов. Ключевая идея — прогрессивное уплотнение 3D-гауссиан, которое постепенно улучшает детализацию представления сцены без огромных вычислительных затрат. Метод использует анизотропную стратегию сэмплирования с пространственно-временным слиянием для адаптивного назначения рецептивных полей гауссианам разных масштабов. Фреймворк достигает state-of-the-art результатов с улучшением mIoU на 14.3% по сравнению с предыдущими методами для автономного вождения."
                },
                "en": {
                    "title": "Revolutionizing 3D Occupancy Prediction with PG-Occ",
                    "desc": "The paper introduces PG-Occ, a Progressive Gaussian Transformer Framework designed to improve 3D occupancy prediction, which is essential for autonomous driving systems. It addresses the limitations of traditional methods by using progressive online densification to enhance Gaussian representations, allowing for better detail capture in complex scenes. Additionally, the framework incorporates an anisotropy-aware sampling strategy that optimizes feature aggregation across different scales, leading to richer scene understanding. The results show that PG-Occ outperforms previous methods, achieving a 14.3% improvement in mean Intersection over Union (mIoU)."
                },
                "zh": {
                    "title": "PG-Occ：提升3D占用预测的创新框架",
                    "desc": "PG-Occ是一种进步的高斯变换框架，旨在提升3D占用预测的精度。该框架通过逐步在线稠密化，逐渐增强3D高斯表示，以捕捉场景中的细节。它还引入了一个考虑各向异性的采样策略，能够在不同尺度和阶段自适应地分配感受野，从而更有效地聚合特征。通过广泛的评估，PG-Occ在性能上超越了之前的最佳方法，取得了14.3%的mIoU提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08867",
            "title": "ReviewerToo: Should AI Join The Program Committee? A Look At The Future\n  of Peer Review",
            "url": "https://huggingface.co/papers/2510.08867",
            "abstract": "ReviewerToo, a modular AI-assisted peer review framework, complements human judgment with systematic assessments, achieving high accuracy and quality in specific domains while highlighting areas where human expertise remains essential.  \t\t\t\t\tAI-generated summary \t\t\t\t Peer review is the cornerstone of scientific publishing, yet it suffers from inconsistencies, reviewer subjectivity, and scalability challenges. We introduce ReviewerToo, a modular framework for studying and deploying AI-assisted peer review to complement human judgment with systematic and consistent assessments. ReviewerToo supports systematic experiments with specialized reviewer personas and structured evaluation criteria, and can be partially or fully integrated into real conference workflows. We validate ReviewerToo on a carefully curated dataset of 1,963 paper submissions from ICLR 2025, where our experiments with the gpt-oss-120b model achieves 81.8% accuracy for the task of categorizing a paper as accept/reject compared to 83.9% for the average human reviewer. Additionally, ReviewerToo-generated reviews are rated as higher quality than the human average by an LLM judge, though still trailing the strongest expert contributions. Our analysis highlights domains where AI reviewers excel (e.g., fact-checking, literature coverage) and where they struggle (e.g., assessing methodological novelty and theoretical contributions), underscoring the continued need for human expertise. Based on these findings, we propose guidelines for integrating AI into peer-review pipelines, showing how AI can enhance consistency, coverage, and fairness while leaving complex evaluative judgments to domain experts. Our work provides a foundation for systematic, hybrid peer-review systems that scale with the growth of scientific publishing.",
            "score": 3,
            "issue_id": 6375,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 октября",
                "en": "October 9",
                "zh": "10月9日"
            },
            "hash": "2d81b8196c727d2e",
            "authors": [
                "Gaurav Sahu",
                "Hugo Larochelle",
                "Laurent Charlin",
                "Christopher Pal"
            ],
            "affiliations": [
                "Canada CIFAR Chair",
                "HEC Montreal",
                "Mila Quebec AI Institute",
                "Polytechnique Montreal",
                "ServiceNow Research",
                "Universite de Montreal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08867.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#multimodal",
                    "#ethics",
                    "#benchmark",
                    "#science"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "AI-рецензент: систематичность машины плюс экспертиза человека",
                    "desc": "В статье представлен ReviewerToo — модульный фреймворк для AI-ассистированного научного рецензирования, который дополняет человеческую экспертизу систематической оценкой. На датасете из 1963 статей с конференции ICLR 2025 система достигла 81.8% точности в классификации accept/reject (против 83.9% у среднего человека-рецензента), а качество AI-генерируемых рецензий оценивается выше человеческого среднего. Анализ показал, что AI хорошо справляется с проверкой фактов и охватом литературы, но испытывает трудности с оценкой методологической новизны и теоретического вклада. Работа предлагает рекомендации по внедрению гибридных систем рецензирования, где AI повышает консистентность и справедливость, а сложные оценочные суждения остаются за экспертами."
                },
                "en": {
                    "title": "Enhancing Peer Review with AI: A Hybrid Approach",
                    "desc": "ReviewerToo is a modular framework designed to enhance the peer review process in scientific publishing by integrating AI with human judgment. It systematically evaluates paper submissions, achieving high accuracy in categorizing them as accept or reject, while also providing consistent assessments. The framework has been validated using a dataset from ICLR 2025, showing that AI-generated reviews can match human accuracy and are often rated higher in quality. However, it also identifies areas where human expertise is crucial, particularly in evaluating methodological novelty and theoretical contributions, suggesting a hybrid approach for future peer review systems."
                },
                "zh": {
                    "title": "AI辅助同行评审，提升科学出版质量",
                    "desc": "ReviewerToo是一个模块化的AI辅助同行评审框架，旨在通过系统化评估来补充人类判断，从而在特定领域实现高准确性和质量。该框架支持使用专业评审角色和结构化评估标准进行系统实验，可以部分或完全融入实际会议工作流程。我们的实验表明，ReviewerToo在对论文进行接受/拒绝分类时，准确率达到81.8%，接近人类评审者的83.9%。分析结果显示，AI评审在事实核查和文献覆盖等领域表现优异，但在评估方法新颖性和理论贡献方面仍需依赖人类专家。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08696",
            "title": "Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence\n  Reweighting",
            "url": "https://huggingface.co/papers/2510.08696",
            "abstract": "LENS modifies GRPO by assigning confidence-dependent rewards to incorrect responses, improving efficiency and performance in reinforcement learning with verifiable rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has become a standard recipe for improving large language models (LLMs) on reasoning tasks, with Group Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO wastes substantial compute on negative groups: groups in which no sampled response is correct yield zero advantage and thus no gradient. We ask whether negative groups can be leveraged without extra supervision. Starting from a maximum-likelihood (MLE) objective in reward modeling, we show that the MLE gradient is equivalent to a policy gradient for a modified value function. This value function adds a confidence-weighted penalty on incorrect responses, imposing larger penalties on more confident mistakes. We refer to this as Likelihood Estimation with Negative Samples (LENS). LENS modifies GRPO to assign non-zero, confidence-dependent rewards to incorrect generations, making negative groups informative and converting previously wasted samples into useful gradient updates. On the MATH benchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently outperforms GRPO baseline, with significant gains on harder items. These results demonstrate a principled and practical way to \"rescue\" negative groups, improving efficiency and performance in RLVR.",
            "score": 3,
            "issue_id": 6375,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 октября",
                "en": "October 9",
                "zh": "10月9日"
            },
            "hash": "8ebc9455daa7299d",
            "authors": [
                "Yunzhen Feng",
                "Parag Jain",
                "Anthony Hartshorn",
                "Yaqi Duan",
                "Julia Kempe"
            ],
            "affiliations": [
                "Meta Superintelligence Labs",
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08696.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#benchmark",
                    "#rl",
                    "#optimization",
                    "#rlhf"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Превращая ошибки в уроки: как извлечь пользу из неправильных ответов LLM",
                    "desc": "Статья представляет метод LENS, который улучшает алгоритм GRPO для обучения языковых моделей на задачах рассуждения. Проблема GRPO в том, что он тратит вычислительные ресурсы впустую на «негативные группы» — случаи, где все сгенерированные ответы неверные и не дают обучающего сигнала. LENS решает это, назначая неправильным ответам штрафы, зависящие от уверенности модели: чем увереннее модель в неправильном ответе, тем больше штраф. Эксперименты на бенчмарке MATH показывают, что LENS превосходит базовый GRPO, особенно на сложных задачах, превращая ранее бесполезные примеры в полезные обновления градиентов."
                },
                "en": {
                    "title": "Transforming Mistakes into Learning: LENS in Reinforcement Learning",
                    "desc": "This paper introduces LENS, a method that enhances Group Relative Policy Optimization (GRPO) by incorporating confidence-dependent rewards for incorrect responses in reinforcement learning. By leveraging negative groups, which traditionally provide no gradient information, LENS assigns non-zero penalties based on the confidence of mistakes, thus transforming wasted computational resources into valuable learning signals. The approach is grounded in a maximum-likelihood objective, showing that the modified value function can effectively guide policy updates. Experimental results on the MATH benchmark demonstrate that LENS significantly outperforms the GRPO baseline, particularly on challenging tasks, highlighting its efficiency and effectiveness in reinforcement learning with verifiable rewards."
                },
                "zh": {
                    "title": "利用置信度提升强化学习效率",
                    "desc": "LENS是一种改进的强化学习方法，通过对错误响应分配与置信度相关的奖励，提升了效率和性能。它在奖励建模中采用最大似然估计（MLE）目标，展示了MLE梯度与修改后的价值函数的策略梯度等价。该价值函数对错误响应施加置信度加权的惩罚，对更自信的错误施加更大的惩罚。通过这种方式，LENS使得负样本组变得有用，从而提高了强化学习的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09606",
            "title": "SpaceVista: All-Scale Visual Spatial Reasoning from mm to km",
            "url": "https://huggingface.co/papers/2510.09606",
            "abstract": "A spatial reasoning model using scale-aware experts and progressive rewards demonstrates competitive performance across diverse tasks and scales using a large, curated dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t With the current surge in spatial reasoning explorations, researchers have made significant progress in understanding indoor scenes, but still struggle with diverse applications such as robotics and autonomous driving. This paper aims to advance all-scale spatial reasoning across diverse scenarios by tackling two key challenges: 1) the heavy reliance on indoor 3D scans and labor-intensive manual annotations for dataset curation; 2) the absence of effective all-scale scene modeling, which often leads to overfitting to individual scenes. In this paper, we introduce a holistic solution that integrates a structured spatial reasoning knowledge system, scale-aware modeling, and a progressive training paradigm, as the first attempt to broaden the all-scale spatial intelligence of MLLMs to the best of our knowledge. Using a task-specific, specialist-driven automated pipeline, we curate over 38K video scenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising approximately 1M spatial QA pairs spanning 19 diverse task types. While specialist models can inject useful domain knowledge, they are not reliable for evaluation. We then build an all-scale benchmark with precise annotations by manually recording, retrieving, and assembling video-based data. However, naive training with SpaceVista-1M often yields suboptimal results due to the potential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a spatial reasoning model that accepts dense inputs beyond semantics and uses scale as an anchor for scale-aware experts and progressive rewards. Finally, extensive evaluations across 5 benchmarks, including our SpaceVista-Bench, demonstrate competitive performance, showcasing strong generalization across all scales and scenarios. Our dataset, model, and benchmark will be released on https://peiwensun2000.github.io/mm2km .",
            "score": 2,
            "issue_id": 6375,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 октября",
                "en": "October 10",
                "zh": "10月10日"
            },
            "hash": "892c43f9f20768e2",
            "authors": [
                "Peiwen Sun",
                "Shiqiang Lang",
                "Dongming Wu",
                "Yi Ding",
                "Kaituo Feng",
                "Huadai Liu",
                "Zhen Ye",
                "Rui Liu",
                "Yun-Hui Liu",
                "Jianan Wang",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "Astribot, Beijing University of Posts and Telecommunications",
                "Hong Kong University of Science and Technology",
                "Multimedia Lab, Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09606.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#cv",
                    "#training",
                    "#dataset",
                    "#reasoning",
                    "#survey",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🔭",
                "ru": {
                    "title": "Пространственное мышление на всех масштабах: от объектов до городских сцен",
                    "desc": "Исследователи представили SpaceVista-7B — модель для пространственного рассуждения, которая работает на всех масштабах от маленьких объектов до больших сцен. Они создали датасет SpaceVista-1M с примерно 1 миллионом пар вопросов-ответов, используя автоматизированный pipeline на основе специализированных моделей для обработки 38 тысяч видео сцен. Модель использует scale-aware экспертов и прогрессивное обучение с наградами, чтобы избежать конфликта знаний между разными масштабами. Эксперименты показали конкурентоспособную производительность на 5 бенчмарках, демонстрируя хорошую генерализацию для робототехники, автономного вождения и других задач."
                },
                "en": {
                    "title": "Advancing Spatial Reasoning with Scale-Aware Models",
                    "desc": "This paper presents a novel spatial reasoning model that effectively handles various tasks and scales by utilizing scale-aware experts and a progressive reward system. It addresses challenges in spatial reasoning, particularly the dependence on indoor 3D scans and the need for effective all-scale scene modeling. The authors introduce SpaceVista-1M, a large dataset with over 1 million spatial question-answer pairs, curated from diverse video scenes across multiple scales. The proposed SpaceVista-7B model demonstrates strong generalization capabilities, achieving competitive performance on several benchmarks, thus advancing the field of spatial reasoning in machine learning."
                },
                "zh": {
                    "title": "全尺度空间推理的创新解决方案",
                    "desc": "本论文提出了一种空间推理模型，利用规模感知专家和渐进奖励，在多种任务和尺度上表现出色。研究者们面临的主要挑战包括对室内3D扫描和人工标注的依赖，以及缺乏有效的全尺度场景建模。为了解决这些问题，论文介绍了一种整合结构化空间推理知识系统、规模感知建模和渐进训练范式的整体解决方案。通过创建一个包含38,000个视频场景的SpaceVista-1M数据集，模型在多个基准测试中展示了强大的泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08189",
            "title": "R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth\n  and Depth?",
            "url": "https://huggingface.co/papers/2510.08189",
            "abstract": "R-HORIZON, a method using query composition, improves long-horizon reasoning in Large Reasoning Models through a benchmark of complex multi-step tasks, enhancing performance and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought (CoT). However, existing benchmarks mainly focus on immediate, single-horizon tasks, failing to adequately evaluate models' ability to understand and respond to complex, long-horizon scenarios. To address this incomplete evaluation of Large Reasoning Models (LRMs), we propose R-HORIZON, a method designed to stimulate long-horizon reasoning behaviors in LRMs through query composition. Based on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising complex multi-step reasoning tasks with interdependent problems that span long reasoning horizons. Through comprehensive evaluation of LRMs using the R-HORIZON benchmark, we find that even the most advanced LRMs suffer significant performance degradation. Our analysis reveals that LRMs exhibit limited effective reasoning length and struggle to allocate thinking budget across multiple problems appropriately. Recognizing these limitations, we use R-HORIZON to construct long-horizon reasoning data for reinforcement learning with verified rewards (RLVR). Compared to training with single-horizon data, RLVR with R-HORIZON not only substantially improves performance on the multi-horizon reasoning tasks, but also promotes accuracy on standard reasoning tasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as a scalable, controllable, and low-cost paradigm for enhancing and evaluating the long-horizon reasoning capabilities of LRMs.",
            "score": 2,
            "issue_id": 6375,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 октября",
                "en": "October 9",
                "zh": "10月9日"
            },
            "hash": "4d2818b42a388048",
            "authors": [
                "Yi Lu",
                "Jianing Wang",
                "Linsen Guo",
                "Wei He",
                "Hongyin Tang",
                "Tao Gui",
                "Xuanjing Huang",
                "Xuezhi Cao",
                "Wei Wang",
                "Xunliang Cai"
            ],
            "affiliations": [
                "Fudan University",
                "Meituan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08189.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#training",
                    "#reasoning",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "🔭",
                "ru": {
                    "title": "R-HORIZON: Обучение AI мыслить на дальние дистанции",
                    "desc": "Статья представляет R-HORIZON - метод для оценки и улучшения способности больших языковых моделей к долгосрочному многоэтапному рассуждению. Авторы создали бенчмарк из сложных взаимосвязанных задач и обнаружили, что даже продвинутые модели показывают значительное снижение качества на длинных цепочках рассуждений. Модели имеют ограниченную эффективную длину рассуждений и плохо распределяют вычислительный бюджет между несколькими подзадачами. Использование R-HORIZON для обучения с подкреплением улучшает производительность на многошаговых задачах и повышает точность на стандартных задачах на 7.5 пунктов."
                },
                "en": {
                    "title": "R-HORIZON: Elevating Long-Horizon Reasoning in AI Models",
                    "desc": "The paper introduces R-HORIZON, a novel method that enhances long-horizon reasoning in Large Reasoning Models (LRMs) through query composition. It identifies a gap in existing benchmarks that primarily assess immediate tasks, which do not challenge models' abilities to handle complex, multi-step reasoning scenarios. By creating a benchmark that includes interdependent problems requiring extended reasoning, R-HORIZON reveals significant performance limitations in current LRMs. The study demonstrates that using R-HORIZON for reinforcement learning with verified rewards leads to substantial improvements in both multi-horizon and standard reasoning tasks."
                },
                "zh": {
                    "title": "R-HORIZON：提升长时间推理能力的创新方法",
                    "desc": "R-HORIZON是一种通过查询组合的方法，旨在提高大型推理模型在长时间推理任务中的表现。该方法构建了一个包含复杂多步骤推理任务的基准，能够更好地评估模型在长时间推理场景中的能力。研究发现，现有的推理模型在处理长时间推理时表现不佳，尤其是在多个问题之间分配思考资源方面存在困难。通过使用R-HORIZON进行强化学习训练，模型在多时间推理任务上的表现显著提升，准确率也有所提高。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08047",
            "title": "Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic\n  Speech Recognition",
            "url": "https://huggingface.co/papers/2510.08047",
            "abstract": "A parameter-space correction method reduces Word Error Rate in ASR systems by addressing pseudo-label biases without target ground truth.  \t\t\t\t\tAI-generated summary \t\t\t\t Robust ASR under domain shift is crucial because real-world systems encounter unseen accents and domains with limited labeled data. Although pseudo-labeling offers a practical workaround, it often introduces systematic, accent-specific errors that filtering fails to fix. We ask: How can we correct these recurring biases without target ground truth? We propose a simple parameter-space correction: in a source domain containing both real and pseudo-labeled data, two ASR models are fine-tuned from the same initialization, one on ground-truth labels and the other on pseudo-labels, and their weight difference forms a correction vector that captures pseudo-label biases. When applied to a pseudo-labeled target model, this vector enhances recognition, achieving up to a 35% relative Word Error Rate (WER) reduction on AfriSpeech-200 across ten African accents with the Whisper tiny model.",
            "score": 2,
            "issue_id": 6375,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 октября",
                "en": "October 9",
                "zh": "10月9日"
            },
            "hash": "abba5710f84f98b1",
            "authors": [
                "Yi-Cheng Lin",
                "Yu-Hsuan Li Liang",
                "Hsuan Su",
                "Tzu-Quan Lin",
                "Shang-Tse Chen",
                "Yun-Nung Chen",
                "Hung-yi Lee"
            ],
            "affiliations": [
                "National Taiwan University, Taipei, Taiwan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08047.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#low_resource",
                    "#optimization",
                    "#audio"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Коррекция параметров для улучшения распознавания речи",
                    "desc": "В статье рассматривается метод коррекции параметров для снижения уровня ошибок в системах распознавания речи (ASR) без использования истинных меток. Проблема заключается в том, что псевдо-метки часто вносят систематические ошибки, связанные с акцентами, которые сложно исправить. Предложенный метод использует разницу в весах двух моделей, обученных на реальных и псевдо-метках, для создания вектора коррекции. Этот вектор позволяет улучшить распознавание речи, снижая уровень ошибок до 35% на различных африканских акцентах."
                },
                "en": {
                    "title": "Correcting Pseudo-Label Biases for Better ASR Performance",
                    "desc": "This paper presents a method to improve Automatic Speech Recognition (ASR) systems by correcting biases introduced by pseudo-labels, which are labels generated without ground truth. The authors highlight that ASR systems often struggle with unseen accents and limited labeled data, leading to increased errors. They propose a parameter-space correction technique that involves fine-tuning two ASR models: one on real labels and the other on pseudo-labels, to create a correction vector. This vector is then used to adjust a target model, resulting in a significant reduction in Word Error Rate (WER), demonstrating the effectiveness of the approach across various African accents."
                },
                "zh": {
                    "title": "修正伪标签偏差，提升语音识别准确率",
                    "desc": "本文提出了一种参数空间修正方法，用于减少自动语音识别（ASR）系统中的词错误率（WER）。该方法解决了伪标签偏差的问题，而无需目标真实标签。通过在源域中对两个ASR模型进行微调，一个使用真实标签，另一个使用伪标签，形成的权重差异构成了捕捉伪标签偏差的修正向量。应用该向量后，在AfriSpeech-200数据集上，识别率提高，词错误率相对降低了35%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09608",
            "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
            "url": "https://huggingface.co/papers/2510.09608",
            "abstract": "StreamingVLM is a real-time vision-language model that efficiently processes infinite video streams using a compact KV cache and supervised fine-tuning, achieving high performance on long videos and diverse benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) could power real-time assistants and autonomous agents, but they face a critical challenge: understanding near-infinite video streams without escalating latency and memory usage. Processing entire videos with full attention leads to quadratic computational costs and poor performance on long videos. Meanwhile, simple sliding window methods are also flawed, as they either break coherence or suffer from high latency due to redundant recomputation. In this paper, we introduce StreamingVLM, a model designed for real-time, stable understanding of infinite visual input. Our approach is a unified framework that aligns training with streaming inference. During inference, we maintain a compact KV cache by reusing states of attention sinks, a short window of recent vision tokens, and a long window of recent text tokens. This streaming ability is instilled via a simple supervised fine-tuning (SFT) strategy that applies full attention on short, overlapped video chunks, which effectively mimics the inference-time attention pattern without training on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a new benchmark with videos averaging over two hours that requires dense, per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy also enhances general VQA abilities without any VQA-specific fine-tuning, improving performance on LongVideoBench by +4.30 and OVOBench Realtime by +5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
            "score": 1,
            "issue_id": 6375,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 октября",
                "en": "October 10",
                "zh": "10月10日"
            },
            "hash": "ef99bf0338c5c2c5",
            "authors": [
                "Ruyi Xu",
                "Guangxuan Xiao",
                "Yukang Chen",
                "Liuning He",
                "Kelly Peng",
                "Yao Lu",
                "Song Han"
            ],
            "affiliations": [
                "First Intelligence",
                "MIT",
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09608.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#long_context",
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#optimization",
                    "#agents"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Бесконечные видеопотоки без перегрузки памяти",
                    "desc": "StreamingVLM - это модель для обработки бесконечных видеопотоков в реальном времени, которая решает проблему растущих затрат памяти и вычислений. Модель использует компактный KV-кеш, сохраняя только ключевые токены (attention sinks, недавние визуальные и текстовые токены) вместо всей истории. Обучение происходит через supervised fine-tuning на коротких перекрывающихся фрагментах видео, что имитирует паттерн внимания при инференсе. На новом бенчмарке с двухчасовыми видео модель достигает 66.18% win rate против GPT-4O mini и работает в реальном времени на одной H100."
                },
                "en": {
                    "title": "Real-Time Understanding of Infinite Video Streams",
                    "desc": "StreamingVLM is a vision-language model designed to process continuous video streams in real-time while minimizing latency and memory usage. It utilizes a compact key-value (KV) cache and a supervised fine-tuning (SFT) strategy to maintain coherence and efficiency during inference. By applying full attention on short, overlapping video segments, it effectively simulates the attention patterns needed for long videos without the computational burden of processing entire videos at once. The model demonstrates superior performance on the new Inf-Streams-Eval benchmark, achieving high win rates against existing models and improving visual question answering capabilities without specific fine-tuning."
                },
                "zh": {
                    "title": "实时处理无限视频流的智能模型",
                    "desc": "StreamingVLM是一种实时视觉-语言模型，能够高效处理无限的视频流。它通过紧凑的KV缓存和监督微调，解决了长视频处理中的延迟和内存使用问题。该模型在推理时重用注意力状态，结合短窗口和长窗口的视觉和文本标记，确保了稳定的理解能力。评估结果显示，StreamingVLM在新的基准测试中表现优异，具有实时性能和增强的视觉问答能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09592",
            "title": "Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in\n  Spoken Language Models",
            "url": "https://huggingface.co/papers/2510.09592",
            "abstract": "Mind-Paced Speaking (MPS) is a brain-inspired framework that enables real-time reasoning and fluent speech generation by dividing the process into a \"Formulation Brain\" for reasoning and an \"Articulation Brain\" for speech, achieving high accuracy with low latency.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-time Spoken Language Models (SLMs) struggle to leverage Chain-of-Thought (CoT) reasoning due to the prohibitive latency of generating the entire thought process sequentially. Enabling SLMs to think while speaking, similar to humans, is attracting increasing attention. We present, for the first time, Mind-Paced Speaking (MPS), a brain-inspired framework that enables high-fidelity, real-time reasoning. Similar to how humans utilize distinct brain regions for thinking and responding, we propose a novel dual-brain approach, employing a \"Formulation Brain\" for high-level reasoning to pace and guide a separate \"Articulation Brain\" for fluent speech generation. This division of labor eliminates mode-switching, preserving the integrity of the reasoning process. Experiments show that MPS significantly outperforms existing think-while-speaking methods and achieves reasoning performance comparable to models that pre-compute the full CoT before speaking, while drastically reducing latency. Under a zero-latency configuration, the proposed method achieves an accuracy of 92.8% on the mathematical reasoning task Spoken-MQA and attains a score of 82.5 on the speech conversation task URO-Bench. Our work effectively bridges the gap between high-quality reasoning and real-time interaction.",
            "score": 1,
            "issue_id": 6375,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 октября",
                "en": "October 10",
                "zh": "10月10日"
            },
            "hash": "d8062cbce503ccc1",
            "authors": [
                "Donghang Wu",
                "Haoyang Zhang",
                "Jun Chen",
                "Xiangyu",
                "Zhang",
                "Hexin Liu",
                "Eng Siong Chng",
                "Fei Tian",
                "Xuerui Yang",
                "Xiangyu Zhang",
                "Daxin Jiang",
                "Gang Yu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "StepFun",
                "University of New South Wales"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09592.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#reasoning",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Думай и говори одновременно: архитектура двух мозгов для разговорного AI",
                    "desc": "Исследователи представили Mind-Paced Speaking (MPS) — фреймворк, вдохновлённый работой человеческого мозга, который позволяет AI-моделям рассуждать и генерировать речь одновременно в режиме реального времени. Система использует архитектуру «двух мозгов»: «Мозг Формулирования» отвечает за высокоуровневое рассуждение по методу Chain-of-Thought, а «Мозг Артикуляции» генерирует плавную речь. Такое разделение задач устраняет необходимость переключения между режимами и сохраняет целостность процесса рассуждения. MPS достигает точности 92.8% на математических задачах и 82.5 баллов на разговорных бенчмарках при практически нулевой задержке, что значительно превосходит существующие методы."
                },
                "en": {
                    "title": "Think and Speak Like a Human with MPS!",
                    "desc": "Mind-Paced Speaking (MPS) is a novel framework designed to enhance real-time reasoning and speech generation by mimicking human brain functions. It separates the reasoning process into two components: a 'Formulation Brain' for high-level reasoning and an 'Articulation Brain' for fluent speech output. This dual-brain approach allows for simultaneous thinking and speaking, reducing latency and improving accuracy. Experimental results demonstrate that MPS outperforms existing methods, achieving high accuracy in reasoning tasks while maintaining real-time interaction capabilities."
                },
                "zh": {
                    "title": "实时推理与流畅表达的完美结合",
                    "desc": "Mind-Paced Speaking (MPS) 是一种受大脑启发的框架，能够实现实时推理和流畅的语言生成。它将过程分为“推理大脑”和“表达大脑”，前者负责高层次的推理，后者负责流畅的语言表达。通过这种分工，MPS 消除了模式切换，保持了推理过程的完整性。实验表明，MPS 在推理性能和实时交互方面显著优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09577",
            "title": "Dyna-Mind: Learning to Simulate from Experience for Better AI Agents",
            "url": "https://huggingface.co/papers/2510.09577",
            "abstract": "Introducing Dyna-Mind, a two-stage training framework that enhances AI agents' reasoning and planning abilities through simulation, leading to improved performance in complex interactive environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning models have recently shown remarkable progress in domains such as math and coding. However, their expert-level abilities in math and coding contrast sharply with their performance in long-horizon, interactive tasks such as web navigation and computer/phone-use. Inspired by literature on human cognition, we argue that current AI agents need ''vicarious trial and error'' - the capacity to mentally simulate alternative futures before acting - in order to enhance their understanding and performance in complex interactive environments. We introduce Dyna-Mind, a two-stage training framework that explicitly teaches (V)LM agents to integrate such simulation into their reasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which trains the agent to generate structured reasoning traces from expanded search trees built from real experience gathered through environment interactions. ReSim thus grounds the agent's reasoning in faithful world dynamics and equips it with the ability to anticipate future states in its reasoning. In stage 2, we propose Dyna-GRPO, an online reinforcement learning method to further strengthen the agent's simulation and decision-making ability by using both outcome rewards and intermediate states as feedback from real rollouts. Experiments on two synthetic benchmarks (Sokoban and ALFWorld) and one realistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively infuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome and interaction-level signals to learn better policies for long-horizon, planning-intensive tasks. Together, these results highlight the central role of simulation in enabling AI agents to reason, plan, and act more effectively in the ever more challenging environments.",
            "score": 1,
            "issue_id": 6375,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 октября",
                "en": "October 10",
                "zh": "10月10日"
            },
            "hash": "c25b935d58908751",
            "authors": [
                "Xiao Yu",
                "Baolin Peng",
                "Michel Galley",
                "Hao Cheng",
                "Qianhui Wu",
                "Janardhan Kulkarni",
                "Suman Nath",
                "Zhou Yu",
                "Jianfeng Gao"
            ],
            "affiliations": [
                "Columbia University, NY",
                "Microsoft Research, Redmond"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09577.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#optimization",
                    "#agents"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "Учим AI-агентов думать перед действием через симуляцию",
                    "desc": "Dyna-Mind — это двухэтапный фреймворк для обучения AI-агентов, который улучшает их способность к рассуждению и планированию через ментальную симуляцию возможных сценариев. На первом этапе метод ReSim учит агента генерировать структурированные цепочки рассуждений на основе деревьев поиска, построенных из реального опыта взаимодействия со средой. На втором этапе Dyna-GRPO использует онлайн reinforcement learning с учётом как финальных наград, так и промежуточных состояний для улучшения навыков симуляции и принятия решений. Эксперименты на синтетических бенчмарках (Sokoban, ALFWorld) и реалистичном AndroidWorld показали, что симуляция будущих состояний существенно повышает эффективность агентов в долгосрочных интерактивных задачах."
                },
                "en": {
                    "title": "Empowering AI with Simulation for Smarter Decision-Making",
                    "desc": "Dyna-Mind is a two-stage training framework designed to improve AI agents' reasoning and planning skills through simulation. The first stage, Reasoning with Simulations (ReSim), helps agents create structured reasoning paths by simulating potential future scenarios based on real experiences. The second stage, Dyna-GRPO, employs online reinforcement learning to enhance decision-making by utilizing both rewards and intermediate feedback from real interactions. This approach demonstrates that incorporating simulation significantly boosts AI agents' performance in complex tasks that require long-term planning and reasoning."
                },
                "zh": {
                    "title": "Dyna-Mind：通过模拟提升 AI 代理的推理与规划能力",
                    "desc": "Dyna-Mind 是一个两阶段的训练框架，旨在通过模拟增强 AI 代理的推理和规划能力，从而提高其在复杂交互环境中的表现。第一阶段是模拟推理（ReSim），它训练代理从真实环境交互中生成结构化的推理轨迹，帮助代理理解世界动态并预测未来状态。第二阶段是 Dyna-GRPO，这是一种在线强化学习方法，通过使用结果奖励和中间状态反馈，进一步增强代理的模拟和决策能力。实验结果表明，Dyna-Mind 有效地提升了 AI 代理在长时间规划任务中的表现，强调了模拟在推理和决策中的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.09561",
            "title": "TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion\n  Control",
            "url": "https://huggingface.co/papers/2510.09561",
            "abstract": "TC-LoRA enhances generative fidelity and adherence to spatial conditions by dynamically conditioning model weights through a hypernetwork, improving upon static activation-based methods in diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Current controllable diffusion models typically rely on fixed architectures that modify intermediate activations to inject guidance conditioned on a new modality. This approach uses a static conditioning strategy for a dynamic, multi-stage denoising process, limiting the model's ability to adapt its response as the generation evolves from coarse structure to fine detail. We introduce TC-LoRA (Temporally Modulated Conditional LoRA), a new paradigm that enables dynamic, context-aware control by conditioning the model's weights directly. Our framework uses a hypernetwork to generate LoRA adapters on-the-fly, tailoring weight modifications for the frozen backbone at each diffusion step based on time and the user's condition. This mechanism enables the model to learn and execute an explicit, adaptive strategy for applying conditional guidance throughout the entire generation process. Through experiments on various data domains, we demonstrate that this dynamic, parametric control significantly enhances generative fidelity and adherence to spatial conditions compared to static, activation-based methods. TC-LoRA establishes an alternative approach in which the model's conditioning strategy is modified through a deeper functional adaptation of its weights, allowing control to align with the dynamic demands of the task and generative stage.",
            "score": 1,
            "issue_id": 6375,
            "pub_date": "2025-10-10",
            "pub_date_card": {
                "ru": "10 октября",
                "en": "October 10",
                "zh": "10月10日"
            },
            "hash": "88da4f7bc3556972",
            "authors": [
                "Minkyoung Cho",
                "Ruben Ohana",
                "Christian Jacobsen",
                "Adityan Jothi",
                "Min-Hung Chen",
                "Z. Morley Mao",
                "Ethem Can"
            ],
            "affiliations": [
                "NVIDIA",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.09561.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#diffusion",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "🎛️",
                "ru": {
                    "title": "Динамическая настройка весов для адаптивного контроля диффузионных моделей",
                    "desc": "TC-LoRA представляет новый подход к управлению диффузионными моделями через динамическое изменение весов модели, а не активаций. Гиперсеть генерирует LoRA-адаптеры на лету для каждого шага диффузии, учитывая временной этап и условия пользователя. Это позволяет модели адаптивно применять управляющие сигналы на протяжении всего процесса генерации - от грубой структуры до мелких деталей. Эксперименты показывают, что такой параметрический контроль значительно улучшает качество генерации и соответствие пространственным условиям по сравнению со статическими методами."
                },
                "en": {
                    "title": "Dynamic Control for Enhanced Generative Fidelity",
                    "desc": "TC-LoRA introduces a novel method for enhancing generative models by dynamically adjusting model weights using a hypernetwork. Unlike traditional methods that rely on fixed architectures and static activations, TC-LoRA allows for context-aware control throughout the denoising process. This approach enables the model to adapt its responses as it transitions from coarse to fine details, improving the overall quality of generated outputs. Experiments show that TC-LoRA significantly outperforms static methods in terms of generative fidelity and adherence to spatial conditions."
                },
                "zh": {
                    "title": "动态控制，提升生成质量",
                    "desc": "TC-LoRA是一种新方法，通过超网络动态调整模型权重，从而增强生成的真实性和空间条件的遵循。与传统的静态激活方法不同，TC-LoRA允许模型在生成过程中根据时间和用户条件实时调整权重。该框架通过生成LoRA适配器，使得模型能够在每个去噪步骤中灵活应对变化。实验表明，TC-LoRA在多个数据领域中显著提高了生成的质量和对空间条件的遵循能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.08697",
            "title": "BigCodeArena: Unveiling More Reliable Human Preferences in Code\n  Generation via Execution",
            "url": "https://huggingface.co/papers/2510.08697",
            "abstract": "BigCodeArena is an open human evaluation platform for code generation that enables real-time execution and interaction, revealing preferences and capabilities of LLMs in coding tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Crowdsourced model evaluation platforms, such as Chatbot Arena, enable real-time evaluation from human perspectives to assess the quality of model responses. In the coding domain, manually examining the quality of LLM-generated content is extremely challenging, as it requires understanding long chunks of raw code and deliberately simulating code execution. To this end, we introduce BigCodeArena, an open human evaluation platform for code generation backed by a comprehensive and on-the-fly execution environment. Built on top of Chatbot Arena, BigCodeArena enables the execution of LLM-generated code and allows humans to interact with the execution process and outcomes. We collected over 14,000 raw code-centric conversation sessions across 10 widely used LLMs, spanning 10 languages and 8 types of execution environments. Among these conversations, we identified more than 4,700 multi-turn samples with pairwise human preferences. Further analysis uncovers underexplored preferences of LLMs in fine-grained domains characterized by tasks, languages, and frameworks. To systematically examine code understanding and generation capabilities of frontier LLMs, we curated two benchmarks based on the collected data, namely BigCodeReward and AutoCodeArena. For BigCodeReward, we post-processed the 4,700 conversations and evaluated the consistency between reward models and human preferences. The evaluation shows that most LLMs have superior performance in judging coding preferences when the execution results are available. Inspired by these findings, we propose AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding quality of LLMs without human involvement. We find that proprietary LLMs like GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation performance among recent emerging models.",
            "score": 1,
            "issue_id": 6375,
            "pub_date": "2025-10-09",
            "pub_date_card": {
                "ru": "9 октября",
                "en": "October 9",
                "zh": "10月9日"
            },
            "hash": "ad4cd46a5b83fe05",
            "authors": [
                "Terry Yue Zhuo",
                "Xiaolong Jin",
                "Hange Liu",
                "Juyong Jiang",
                "Tianyang Liu",
                "Chen Gong",
                "Bhupesh Bishnoi",
                "Vaisakhi Mishra",
                "Marek Suppa",
                "Noah Ziems",
                "Saiteja Utpala",
                "Ming Xu",
                "Guangyu Song",
                "Kaixin Li",
                "Yuhan Cao",
                "Bo Liu",
                "Zheng Liu",
                "Sabina Abdurakhmanova",
                "Wenhao Yu",
                "Mengzhao Jia",
                "Jihan Yao",
                "Kenneth Hamilton",
                "Kumar Shridhar",
                "Minh Chien Vu",
                "Dingmin Wang",
                "Jiawei Liu",
                "Zijian Wang",
                "Qian Liu",
                "Binyuan Hui",
                "Meg Risdal",
                "Ahsen Khaliq",
                "Atin Sood",
                "Zhenchang Xing",
                "Wasi Uddin Ahmad",
                "John Grundy",
                "David Lo",
                "Banghua Zhu",
                "Xiaoning Du",
                "Torsten Scholak",
                "Leandro von Werra"
            ],
            "affiliations": [
                "CNRS, France",
                "CSIROs Data61",
                "Cisco",
                "Comenius University in Bratislava",
                "Detomo Inc",
                "ETH Zurich",
                "Google",
                "HKUST (Guangzhou)",
                "Hugging Face",
                "IBM",
                "Independent",
                "Institute of Automation, CAS",
                "Monash University",
                "NUS",
                "NVIDIA",
                "Nevsky Collective",
                "Purdue University",
                "ServiceNow Research",
                "Singapore Management University",
                "Tano Labs",
                "Tencent AI Lab",
                "UCSD",
                "UIUC",
                "UVA",
                "Uber",
                "University of Notre Dame",
                "University of Oxford",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.08697.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#dataset",
                    "#multilingual",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "⚔️",
                "ru": {
                    "title": "Арена для кода: краудсорсинговая оценка способностей LLM в программировании",
                    "desc": "BigCodeArena — это открытая платформа для оценки генерации кода с помощью людей, которая позволяет выполнять код в реальном времени и взаимодействовать с процессом выполнения. Исследователи собрали более 14,000 диалогов с участием 10 популярных LLM и выделили 4,700 примеров с предпочтениями пользователей. На основе собранных данных созданы два бенчмарка: BigCodeReward для оценки reward-моделей и AutoCodeArena для автоматического ранжирования качества кода. Результаты показывают, что проприетарные модели вроде GPT-5, Claude-Sonnet-4 и Claude-Opus-4 по-прежнему лидируют в генерации кода среди современных моделей."
                },
                "en": {
                    "title": "Revolutionizing Code Evaluation with BigCodeArena",
                    "desc": "BigCodeArena is a platform designed for evaluating code generation by large language models (LLMs) through real-time human interaction and code execution. It allows users to execute LLM-generated code and observe the outcomes, making it easier to assess the quality of the generated content. The platform has gathered extensive data from over 14,000 coding sessions across various LLMs and programming languages, revealing insights into human preferences and model capabilities. Additionally, it introduces benchmarks like BigCodeReward and AutoCodeArena to systematically evaluate LLM performance in coding tasks, highlighting the strengths of leading models in this domain."
                },
                "zh": {
                    "title": "BigCodeArena：实时评估代码生成的开放平台",
                    "desc": "BigCodeArena是一个开放的人类评估平台，专注于代码生成，能够实时执行和互动，揭示大型语言模型（LLMs）在编码任务中的偏好和能力。该平台基于Chatbot Arena，支持对LLM生成的代码进行执行，并允许人类与执行过程和结果进行互动。我们收集了超过14,000个与代码相关的对话会话，涵盖10种流行的LLM和8种执行环境，识别出超过4,700个多轮样本及其人类偏好。通过分析，我们发现LLMs在细分领域的偏好尚未被充分探索，并提出了BigCodeReward和AutoCodeArena两个基准，以系统性地评估前沿LLMs的代码理解和生成能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.07319",
            "title": "Temporal Prompting Matters: Rethinking Referring Video Object\n  Segmentation",
            "url": "https://huggingface.co/papers/2510.07319",
            "abstract": "The Tenet framework decomposes the RVOS task into referring, video, and segmentation factors, using temporal prompts and prompt preference learning to adapt image-based foundation segmentation models for efficient RVOS.  \t\t\t\t\tAI-generated summary \t\t\t\t Referring Video Object Segmentation (RVOS) aims to segment the object referred to by the query sentence in the video. Most existing methods require end-to-end training with dense mask annotations, which could be computation-consuming and less scalable. In this work, we rethink the RVOS problem and aim to investigate the key to this task. Based on existing foundation segmentation models, we decompose the RVOS task into referring, video, and segmentation factors, and propose a Temporal Prompt Generation and Selection (Tenet) framework to address the referring and video factors while leaving the segmentation problem to foundation models. To efficiently adapt image-based foundation segmentation models to referring video object segmentation, we leverage off-the-shelf object detectors and trackers to produce temporal prompts associated with the referring sentence. While high-quality temporal prompts could be produced, they can not be easily identified from confidence scores. To tackle this issue, we propose Prompt Preference Learning to evaluate the quality of the produced temporal prompts. By taking such prompts to instruct image-based foundation segmentation models, we would be able to produce high-quality masks for the referred object, enabling efficient model adaptation to referring video object segmentation. Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenet framework.",
            "score": 1,
            "issue_id": 6375,
            "pub_date": "2025-10-08",
            "pub_date_card": {
                "ru": "8 октября",
                "en": "October 8",
                "zh": "10月8日"
            },
            "hash": "574ecd29563b6aa8",
            "authors": [
                "Ci-Siang Lin",
                "Min-Hung Chen",
                "I-Jieh Liu",
                "Chien-Yi Wang",
                "Sifei Liu",
                "Yu-Chiang Frank Wang"
            ],
            "affiliations": [
                "Graduate Institute of Communication Engineering, National Taiwan University, Taiwan",
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.07319.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#video",
                    "#optimization"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Декомпозиция задачи видеосегментации через временные промпты",
                    "desc": "Статья представляет фреймворк Tenet для задачи сегментации объектов в видео по текстовому описанию (RVOS). Авторы декомпозируют задачу на три фактора: понимание текстового запроса, обработку видео и саму сегментацию, используя готовые foundation модели для сегментации. Ключевая идея — генерация временных промптов с помощью детекторов и трекеров объектов, качество которых оценивается через специальный механизм Prompt Preference Learning. Подход позволяет эффективно адаптировать модели сегментации изображений для работы с видео без затратного end-to-end обучения на размеченных масках."
                },
                "en": {
                    "title": "Efficient RVOS through Temporal Prompting and Preference Learning",
                    "desc": "The paper introduces the Tenet framework, which breaks down the Referring Video Object Segmentation (RVOS) task into three main components: referring, video, and segmentation factors. It utilizes temporal prompts generated from object detectors and trackers to enhance the performance of existing image-based segmentation models without requiring extensive training on dense mask annotations. To ensure the quality of these prompts, the authors implement Prompt Preference Learning, which assesses the effectiveness of the generated prompts. The results show that the Tenet framework significantly improves the efficiency and accuracy of RVOS tasks by leveraging pre-trained segmentation models."
                },
                "zh": {
                    "title": "高效的参考视频物体分割新方法",
                    "desc": "本文提出了Tenet框架，将参考视频物体分割（RVOS）任务分解为参考、视频和分割三个因素。通过使用时间提示和提示偏好学习，Tenet框架能够有效地将基于图像的基础分割模型适应于RVOS任务。我们利用现成的物体检测器和跟踪器生成与查询句子相关的时间提示，并通过提示偏好学习评估这些提示的质量。实验结果表明，Tenet框架在RVOS基准测试中表现出色，能够生成高质量的分割掩码。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2510.01119",
            "title": "Instant4D: 4D Gaussian Splatting in Minutes",
            "url": "https://huggingface.co/papers/2510.01119",
            "abstract": "Instant4D uses deep visual SLAM and a 4D Gaussian representation to efficiently reconstruct scenes from uncalibrated video sequences in minutes.  \t\t\t\t\tAI-generated summary \t\t\t\t Dynamic view synthesis has seen significant advances, yet reconstructing scenes from uncalibrated, casual video remains challenging due to slow optimization and complex parameter estimation. In this work, we present Instant4D, a monocular reconstruction system that leverages native 4D representation to efficiently process casual video sequences within minutes, without calibrated cameras or depth sensors. Our method begins with geometric recovery through deep visual SLAM, followed by grid pruning to optimize scene representation. Our design significantly reduces redundancy while maintaining geometric integrity, cutting model size to under 10% of its original footprint. To handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian representation, achieving a 30x speed-up and reducing training time to within two minutes, while maintaining competitive performance across several benchmarks. Our method reconstruct a single video within 10 minutes on the Dycheck dataset or for a typical 200-frame video. We further apply our model to in-the-wild videos, showcasing its generalizability. Our project website is published at https://instant4d.github.io/.",
            "score": 0,
            "issue_id": 6375,
            "pub_date": "2025-10-01",
            "pub_date_card": {
                "ru": "1 октября",
                "en": "October 1",
                "zh": "10月1日"
            },
            "hash": "9c375f4bf8782c80",
            "authors": [
                "Zhanpeng Luo",
                "Haoxi Ran",
                "Li Lu"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Sichuan University",
                "University of Pittsburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2510.01119.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#cv",
                    "#video",
                    "#dataset",
                    "#inference",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "⚡",
                "ru": {
                    "title": "Мгновенная 4D реконструкция сцен из обычного видео за минуты",
                    "desc": "Instant4D — это система для реконструкции динамических сцен из некалиброванного видео, которая работает за считанные минуты. Метод использует deep visual SLAM для восстановления геометрии и компактное 4D представление на основе гауссианов. Авторы сократили размер модели до 10% от исходного и ускорили обучение в 30 раз — теперь оно занимает всего две минуты. Система обрабатывает типичное 200-кадровое видео за 10 минут и работает даже на произвольных роликах из интернета."
                },
                "en": {
                    "title": "Revolutionizing Scene Reconstruction in Minutes with Instant4D",
                    "desc": "Instant4D is a novel system that utilizes deep visual SLAM and a 4D Gaussian representation to reconstruct scenes from uncalibrated video sequences quickly. It addresses the challenges of slow optimization and complex parameter estimation by processing casual videos in minutes without the need for calibrated cameras or depth sensors. The method employs geometric recovery and grid pruning to optimize the scene representation, significantly reducing redundancy while preserving geometric integrity. With a 30x speed-up in processing time, Instant4D can reconstruct a typical 200-frame video in just 10 minutes, demonstrating its efficiency and effectiveness across various benchmarks."
                },
                "zh": {
                    "title": "快速重建，瞬间呈现",
                    "desc": "Instant4D 是一个单目重建系统，利用深度视觉 SLAM 和 4D 高斯表示法，能够在几分钟内高效地从未校准的视频序列中重建场景。该方法通过几何恢复和网格修剪来优化场景表示，显著减少冗余，同时保持几何完整性。Instant4D 的设计使得模型大小减少到原始的 10% 以下，并且在处理时间动态方面实现了 30 倍的加速。该系统在 Dycheck 数据集上能够在 10 分钟内重建单个视频，展示了其在实际视频中的广泛适用性。"
                }
            }
        }
    ],
    "link_prev": "2025-10-10.html",
    "link_next": "2025-10-14.html",
    "link_month": "2025-10.html",
    "short_date_prev": {
        "ru": "10.10",
        "en": "10/10",
        "zh": "10月10日"
    },
    "short_date_next": {
        "ru": "14.10",
        "en": "10/14",
        "zh": "10月14日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 3,
        "#benchmark": 9,
        "#agents": 2,
        "#cv": 8,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 2,
        "#video": 3,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 10,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    }
}