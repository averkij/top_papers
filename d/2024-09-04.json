{
    "date": {
        "ru": "4 сентября",
        "en": "September 4",
        "zh": "9月4日"
    },
    "time_utc": "2024-09-04 09:00",
    "weekday": 2,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-04",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.01704",
            "title": "General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model",
            "url": "https://huggingface.co/papers/2409.01704",
            "abstract": "Traditional OCR systems (OCR-1.0) are increasingly unable to meet people's usage due to the growing demand for intelligent processing of man-made optical characters. In this paper, we collectively refer to all artificial optical signals (e.g., plain texts, math/molecular formulas, tables, charts, sheet music, and even geometric shapes) as \"characters\" and propose the General OCR Theory along with an excellent model, namely GOT, to promote the arrival of OCR-2.0. The GOT, with 580M parameters, is a unified, elegant, and end-to-end model, consisting of a high-compression encoder and a long-contexts decoder. As an OCR-2.0 model, GOT can handle all the above \"characters\" under various OCR tasks. On the input side, the model supports commonly used scene- and document-style images in slice and whole-page styles. On the output side, GOT can generate plain or formatted results (markdown/tikz/smiles/kern) via an easy prompt. Besides, the model enjoys interactive OCR features, i.e., region-level recognition guided by coordinates or colors. Furthermore, we also adapt dynamic resolution and multi-page OCR technologies to GOT for better practicality. In experiments, we provide sufficient results to prove the superiority of our model.",
            "score": 80,
            "issue_id": 1,
            "pub_date": "2024-09-03",
            "pub_date_card": {
                "ru": "3 сентября",
                "en": "September 3",
                "zh": "9月3日"
            },
            "hash": "cc052755a384b1f8",
            "data": {
                "categories": [
                    "#cv",
                    "#long_context",
                    "#optimization"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "GOT: Универсальное оптическое распознавание для эры OCR 2.0",
                    "desc": "Статья представляет новую модель под названием GOT для оптического распознавания символов (OCR). Эта модель способна обрабатывать различные типы 'символов', включая обычный текст, формулы, таблицы и даже геометрические фигуры. GOT использует архитектуру кодировщик-декодировщик и поддерживает интерактивное распознавание на уровне регионов. Модель демонстрирует превосходные результаты в экспериментах и обещает стать шагом к OCR 2.0."
                },
                "en": {
                    "title": "Revolutionizing OCR with GOT: The Future of Intelligent Character Recognition",
                    "desc": "This paper introduces the General OCR Theory and a new model called GOT, designed to enhance optical character recognition (OCR) capabilities. GOT is an end-to-end model with 580 million parameters that can process a wide range of artificial optical signals, including text, formulas, and charts. It features a high-compression encoder and a long-context decoder, allowing it to handle various OCR tasks effectively. The model supports both scene and document images and can produce formatted outputs while offering interactive features for improved user experience."
                },
                "zh": {
                    "title": "推动OCR-2.0的通用OCR理论与GOT模型",
                    "desc": "传统的光学字符识别系统（OCR-1.0）已无法满足人们对智能处理人造光学字符的需求。本文提出了一种通用OCR理论及其优秀模型GOT，旨在推动OCR-2.0的到来。GOT模型具有580M参数，是一个统一、优雅的端到端模型，能够处理各种光学字符，包括文本、公式、表格等。通过动态分辨率和多页OCR技术，GOT在实验中展示了其卓越的性能和实用性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02060",
            "title": "OLMoE: Open Mixture-of-Experts Language Models",
            "url": "https://huggingface.co/papers/2409.02060",
            "abstract": "We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.",
            "score": 77,
            "issue_id": 1,
            "pub_date": "2024-09-03",
            "pub_date_card": {
                "ru": "3 сентября",
                "en": "September 3",
                "zh": "9月3日"
            },
            "hash": "fd0ab48efdc585ed",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "OLMoE: Эффективная языковая модель на основе экспертов",
                    "desc": "OLMoE - это новая языковая модель, использующая метод Mixture-of-Experts (MoE). Модель имеет 7 миллиардов параметров, но активирует только 1 миллиард для каждого токена ввода. OLMoE превосходит по производительности существующие модели с аналогичным количеством активных параметров, включая более крупные модели. Авторы проводят различные эксперименты по обучению MoE и анализируют маршрутизацию в своей модели, демонстрируя высокую специализацию."
                },
                "en": {
                    "title": "Unlocking Efficiency with OLMoE: A New Era in Language Models",
                    "desc": "OLMoE is a cutting-edge language model that utilizes a sparse Mixture-of-Experts (MoE) architecture, allowing it to efficiently manage a large number of parameters. With 7 billion parameters, OLMoE-1B-7B only activates 1 billion parameters for each input, optimizing resource use. It has been pretrained on an extensive dataset of 5 trillion tokens and fine-tuned for instruction-based tasks, demonstrating superior performance compared to other models with similar active parameters. The research includes detailed experiments on MoE training and routing analysis, showcasing the model's specialization, and all components of the project are open-sourced for public access."
                },
                "zh": {
                    "title": "OLMoE：高效的稀疏专家混合语言模型",
                    "desc": "我们介绍了OLMoE，这是一种完全开放的最先进语言模型，利用了稀疏的专家混合（MoE）技术。OLMoE-1B-7B拥有70亿个参数，但每个输入令牌仅使用10亿个参数。我们在5万亿个令牌上进行了预训练，并进一步调整以创建OLMoE-1B-7B-Instruct。我们的模型在活跃参数相似的情况下超越了所有可用模型，甚至超过了更大的模型，如Llama2-13B-Chat和DeepSeekMoE-16B。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.01437",
            "title": "Kvasir-VQA: A Text-Image Pair GI Tract Dataset",
            "url": "https://huggingface.co/papers/2409.01437",
            "abstract": "We introduce Kvasir-VQA, an extended dataset derived from the HyperKvasir and Kvasir-Instrument datasets, augmented with question-and-answer annotations to facilitate advanced machine learning tasks in Gastrointestinal (GI) diagnostics. This dataset comprises 6,500 annotated images spanning various GI tract conditions and surgical instruments, and it supports multiple question types including yes/no, choice, location, and numerical count. The dataset is intended for applications such as image captioning, Visual Question Answering (VQA), text-based generation of synthetic medical images, object detection, and classification. Our experiments demonstrate the dataset's effectiveness in training models for three selected tasks, showcasing significant applications in medical image analysis and diagnostics. We also present evaluation metrics for each task, highlighting the usability and versatility of our dataset. The dataset and supporting artifacts are available at https://datasets.simula.no/kvasir-vqa.",
            "score": 70,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "a914aa77110d40cd",
            "data": {
                "categories": [
                    "#dataset",
                    "#medicine",
                    "#cv"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Новый набор данных для ИИ-анализа медицинских изображений ЖКТ",
                    "desc": "Представлен набор данных Kvasir-VQA для задач машинного обучения в области гастроэнтерологической диагностики. Он содержит 6500 аннотированных изображений ЖКТ с вопросами и ответами различных типов. Набор данных предназначен для решения задач генерации подписей к изображениям, визуального ответа на вопросы, синтеза медицинских изображений и других. Эксперименты показали эффективность набора данных для обучения моделей по трем выбранным задачам."
                },
                "en": {
                    "title": "Kvasir-VQA: Advancing GI Diagnostics with Visual Question Answering",
                    "desc": "Kvasir-VQA is a new dataset designed to enhance machine learning applications in Gastrointestinal diagnostics. It includes 6,500 annotated images of various GI conditions and surgical tools, with questions that can be answered in different formats like yes/no or numerical counts. This dataset is useful for tasks such as image captioning, Visual Question Answering (VQA), and object detection. Our experiments show that Kvasir-VQA effectively trains models for these tasks, proving its value in medical image analysis."
                },
                "zh": {
                    "title": "Kvasir-VQA：推动医学图像分析的新数据集",
                    "desc": "Kvasir-VQA是一个扩展的数据集，源自HyperKvasir和Kvasir-Instrument数据集，增加了问答注释，以促进胃肠道（GI）诊断中的高级机器学习任务。该数据集包含6500张标注图像，涵盖各种GI道疾病和手术工具，支持多种问题类型，包括是/否、选择、位置和数字计数。该数据集适用于图像描述、视觉问答（VQA）、基于文本的合成医学图像生成、物体检测和分类等应用。我们的实验表明，该数据集在训练模型方面的有效性，展示了在医学图像分析和诊断中的重要应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00509",
            "title": "LongRecipe: Recipe for Efficient Long Context Generalization in Large Languge Models",
            "url": "https://huggingface.co/papers/2409.00509",
            "abstract": "Large language models (LLMs) face significant challenges in handling long-context tasks because of their limited effective context window size during pretraining, which restricts their ability to generalize over extended sequences. Meanwhile, extending the context window in LLMs through post-pretraining is highly resource-intensive. To address this, we introduce **LongRecipe**, an efficient training strategy for extending the context window of LLMs, including impactful token analysis, position index transformation, and training optimization strategies. It simulates long-sequence inputs while maintaining training efficiency and significantly improves the model's understanding of long-range dependencies. Experiments on three types of LLMs show that LongRecipe can utilize long sequences while requiring only 30% of the target context window size, and reduces computational training resource over 85% compared to full sequence training. Furthermore, LongRecipe also preserves the original LLM's capabilities in general tasks. Ultimately, *we can extend the effective context window of open-source LLMs from 8k to 128k, achieving performance close to GPT-4 with just one day of dedicated training using a single GPU with 80G memory.* Our code is released at the [link](https://github.com/zhiyuanhubj/LongRecipe).",
            "score": 38,
            "issue_id": 1,
            "pub_date": "2024-08-31",
            "pub_date_card": {
                "ru": "31 августа",
                "en": "August 31",
                "zh": "8月31日"
            },
            "hash": "7c96239ce2e612ce",
            "data": {
                "categories": [
                    "#long_context",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Эффективное расширение контекста LLM: больше возможностей, меньше ресурсов",
                    "desc": "Статья предлагает новый метод LongRecipe для эффективного расширения контекстного окна больших языковых моделей (LLM). Этот подход позволяет моделям обрабатывать длинные последовательности, используя лишь 30% от целевого размера контекстного окна и сокращая вычислительные ресурсы на 85% по сравнению с полным обучением. LongRecipe включает в себя анализ значимых токенов, преобразование позиционных индексов и оптимизацию стратегий обучения. В результате авторам удалось расширить эффективное контекстное окно открытых LLM с 8 тыс. до 128 тыс. токенов за один день обучения на одном GPU."
                },
                "en": {
                    "title": "Unlocking Long Contexts Efficiently with LongRecipe",
                    "desc": "This paper presents LongRecipe, a novel training strategy designed to enhance the context window of large language models (LLMs) without the extensive resource demands typically associated with such tasks. By employing techniques like impactful token analysis and position index transformation, LongRecipe efficiently simulates long-sequence inputs, allowing models to better understand long-range dependencies. The approach enables LLMs to utilize long sequences while only requiring 30% of the target context window size, leading to over 85% reduction in computational resources compared to traditional full sequence training. Ultimately, LongRecipe allows for the effective context window of open-source LLMs to be extended from 8k to 128k, achieving performance levels comparable to GPT-4 with minimal training time on a single GPU."
                },
                "zh": {
                    "title": "高效扩展大型语言模型的上下文窗口",
                    "desc": "大型语言模型（LLMs）在处理长上下文任务时面临挑战，因为它们在预训练期间的有效上下文窗口大小有限，限制了对长序列的泛化能力。为了解决这个问题，我们提出了**LongRecipe**，这是一种高效的训练策略，可以扩展LLMs的上下文窗口。该方法通过影响力的标记分析、位置索引转换和训练优化策略来模拟长序列输入，同时保持训练效率。实验表明，LongRecipe能够在仅需目标上下文窗口大小的30%的情况下利用长序列，并且相比于完整序列训练，计算资源减少超过85%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02095",
            "title": "DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos",
            "url": "https://huggingface.co/papers/2409.02095",
            "abstract": "Despite significant advancements in monocular depth estimation for static images, estimating video depth in the open world remains challenging, since open-world videos are extremely diverse in content, motion, camera movement, and length. We present DepthCrafter, an innovative method for generating temporally consistent long depth sequences with intricate details for open-world videos, without requiring any supplementary information such as camera poses or optical flow. DepthCrafter achieves generalization ability to open-world videos by training a video-to-depth model from a pre-trained image-to-video diffusion model, through our meticulously designed three-stage training strategy with the compiled paired video-depth datasets. Our training approach enables the model to generate depth sequences with variable lengths at one time, up to 110 frames, and harvest both precise depth details and rich content diversity from realistic and synthetic datasets. We also propose an inference strategy that processes extremely long videos through segment-wise estimation and seamless stitching. Comprehensive evaluations on multiple datasets reveal that DepthCrafter achieves state-of-the-art performance in open-world video depth estimation under zero-shot settings. Furthermore, DepthCrafter facilitates various downstream applications, including depth-based visual effects and conditional video generation.",
            "score": 35,
            "issue_id": 1,
            "pub_date": "2024-09-03",
            "pub_date_card": {
                "ru": "3 сентября",
                "en": "September 3",
                "zh": "9月3日"
            },
            "hash": "803f31c2683713de",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#dataset",
                    "#training",
                    "#inference",
                    "#synthetic"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "DepthCrafter: Революция в оценке глубины видео реального мира",
                    "desc": "DepthCrafter - это инновационный метод для генерации темпорально согласованных длинных последовательностей глубины с детальной проработкой для видео из реального мира. Модель обучается на основе предварительно обученной диффузионной модели преобразования изображений в видео, используя трехэтапную стратегию обучения на составленных парных наборах данных видео-глубина. DepthCrafter способен генерировать последовательности глубины переменной длины до 110 кадров за один раз, сочетая точные детали глубины и разнообразие контента из реалистичных и синтетических датасетов. Метод достигает высокой производительности в оценке глубины видео реального мира в условиях нулевого обучения и может применяться для различных задач, включая создание визуальных эффектов на основе глубины и условную генерацию видео."
                },
                "en": {
                    "title": "DepthCrafter: Revolutionizing Depth Estimation in Open-World Videos",
                    "desc": "DepthCrafter is a novel method designed to estimate depth in open-world videos, addressing the challenges posed by diverse content and motion. It utilizes a three-stage training strategy that leverages a pre-trained image-to-video diffusion model, allowing it to generate long depth sequences without needing additional information like camera poses. The model can produce depth sequences of varying lengths, capturing detailed depth information and content diversity from both realistic and synthetic datasets. Additionally, DepthCrafter includes an inference strategy for processing long videos by segmenting them and seamlessly stitching the results together, achieving state-of-the-art performance in zero-shot depth estimation."
                },
                "zh": {
                    "title": "DepthCrafter：开放世界视频深度估计的新突破",
                    "desc": "尽管在静态图像的单目深度估计方面取得了显著进展，但在开放世界中估计视频深度仍然具有挑战性。我们提出了DepthCrafter，这是一种创新的方法，可以为开放世界视频生成时间一致的长深度序列，且无需额外的信息，如相机姿态或光流。DepthCrafter通过从预训练的图像到视频扩散模型中训练视频到深度模型，展现了对开放世界视频的泛化能力。我们的训练方法使模型能够一次生成可变长度的深度序列，并从真实和合成数据集中提取精确的深度细节和丰富的内容多样性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00587",
            "title": "FLUX that Plays Music",
            "url": "https://huggingface.co/papers/2409.00587",
            "abstract": "This paper explores a simple extension of diffusion-based rectified flow Transformers for text-to-music generation, termed as FluxMusic. Generally, along with design in advanced Fluxhttps://github.com/black-forest-labs/flux model, we transfers it into a latent VAE space of mel-spectrum. It involves first applying a sequence of independent attention to the double text-music stream, followed by a stacked single music stream for denoised patch prediction. We employ multiple pre-trained text encoders to sufficiently capture caption semantic information as well as inference flexibility. In between, coarse textual information, in conjunction with time step embeddings, is utilized in a modulation mechanism, while fine-grained textual details are concatenated with the music patch sequence as inputs. Through an in-depth study, we demonstrate that rectified flow training with an optimized architecture significantly outperforms established diffusion methods for the text-to-music task, as evidenced by various automatic metrics and human preference evaluations. Our experimental data, code, and model weights are made publicly available at: https://github.com/feizc/FluxMusic.",
            "score": 31,
            "issue_id": 1,
            "pub_date": "2024-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "8e90043f3c31a7df",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "FluxMusic: Улучшенная генерация музыки по текстовому описанию",
                    "desc": "В этой статье представлен FluxMusic - расширение диффузионных Transformer-моделей для генерации музыки по текстовому описанию. Модель работает в латентном VAE-пространстве мел-спектрограмм, используя независимое внимание для текста и музыки, а затем стекированное внимание для предсказания музыкальных фрагментов. Применяются предобученные текстовые энкодеры и механизм модуляции для интеграции текстовой информации. Эксперименты показывают превосходство этого подхода над стандартными диффузионными методами для задачи текст-в-музыку."
                },
                "en": {
                    "title": "Transforming Text to Music with FluxMusic: A New Era in Generation!",
                    "desc": "This paper introduces FluxMusic, an innovative approach that enhances diffusion-based rectified flow Transformers for generating music from text. It utilizes a latent Variational Autoencoder (VAE) space focused on the mel-spectrum, allowing for effective representation of audio features. The model employs independent attention mechanisms to process text and music streams, improving the prediction of music patches. The results show that this optimized architecture outperforms traditional diffusion methods in generating music from textual descriptions, supported by both quantitative metrics and qualitative assessments."
                },
                "zh": {
                    "title": "FluxMusic：文本到音乐生成的新突破",
                    "desc": "本文提出了一种名为FluxMusic的扩展方法，用于基于扩散的文本到音乐生成。该方法将模型转移到梅尔频谱的潜在变分自编码器空间中，首先对文本和音乐流进行独立注意力处理，然后进行去噪音的音乐片段预测。我们使用多个预训练的文本编码器来捕捉语义信息，并结合时间步嵌入进行调制机制。通过深入研究，我们证明了优化架构的修正流训练在文本到音乐任务中显著优于传统的扩散方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02097",
            "title": "LinFusion: 1 GPU, 1 Minute, 16K Image",
            "url": "https://huggingface.co/papers/2409.02097",
            "abstract": "Modern diffusion models, particularly those utilizing a Transformer-based UNet for denoising, rely heavily on self-attention operations to manage complex spatial relationships, thus achieving impressive generation performance. However, this existing paradigm faces significant challenges in generating high-resolution visual content due to its quadratic time and memory complexity with respect to the number of spatial tokens. To address this limitation, we aim at a novel linear attention mechanism as an alternative in this paper. Specifically, we begin our exploration from recently introduced models with linear complexity, e.g., Mamba, Mamba2, and Gated Linear Attention, and identify two key features-attention normalization and non-causal inference-that enhance high-resolution visual generation performance. Building on these insights, we introduce a generalized linear attention paradigm, which serves as a low-rank approximation of a wide spectrum of popular linear token mixers. To save the training cost and better leverage pre-trained models, we initialize our models and distill the knowledge from pre-trained StableDiffusion (SD). We find that the distilled model, termed LinFusion, achieves performance on par with or superior to the original SD after only modest training, while significantly reducing time and memory complexity. Extensive experiments on SD-v1.5, SD-v2.1, and SD-XL demonstrate that LinFusion delivers satisfactory zero-shot cross-resolution generation performance, generating high-resolution images like 16K resolution. Moreover, it is highly compatible with pre-trained SD components, such as ControlNet and IP-Adapter, requiring no adaptation efforts. Codes are available at https://github.com/Huage001/LinFusion.",
            "score": 31,
            "issue_id": 1,
            "pub_date": "2024-09-03",
            "pub_date_card": {
                "ru": "3 сентября",
                "en": "September 3",
                "zh": "9月3日"
            },
            "hash": "704dccb6db5f3e9d",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#architecture",
                    "#training"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "LinFusion: Линейное внимание для сверхвысокого разрешения",
                    "desc": "Статья представляет новый подход к генерации изображений высокого разрешения с использованием линейного механизма внимания. Авторы предлагают обобщенную парадигму линейного внимания, которая служит аппроксимацией низкого ранга для широкого спектра популярных линейных токен-миксеров. Модель, названная LinFusion, инициализируется и обучается на основе предобученной StableDiffusion, достигая сопоставимой или превосходящей производительности при значительном снижении временной и пространственной сложности. Эксперименты показывают, что LinFusion способна генерировать изображения сверхвысокого разрешения до 16K без дополнительного обучения."
                },
                "en": {
                    "title": "Revolutionizing High-Resolution Image Generation with Linear Attention",
                    "desc": "This paper presents a new approach to improve the performance of diffusion models in generating high-resolution images by introducing a linear attention mechanism. Traditional models struggle with high memory and time complexity due to self-attention operations, especially as the number of spatial tokens increases. The authors explore existing linear complexity models and propose a generalized linear attention paradigm that enhances visual generation while reducing resource demands. Their model, LinFusion, shows competitive performance with pre-trained models like StableDiffusion, achieving impressive results in generating images up to 16K resolution with minimal training effort."
                },
                "zh": {
                    "title": "线性注意力，提升高分辨率生成性能！",
                    "desc": "现代扩散模型，特别是使用基于Transformer的UNet进行去噪的模型，依赖自注意力操作来处理复杂的空间关系，从而实现出色的生成性能。然而，由于其在空间标记数量上的二次时间和内存复杂性，现有范式在生成高分辨率视觉内容时面临重大挑战。为了解决这一限制，本文提出了一种新的线性注意力机制，旨在提高高分辨率视觉生成性能。我们通过引入注意力归一化和非因果推理等关键特征，开发了一种通用的线性注意力范式，显著降低了训练成本并提高了生成效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.01071",
            "title": "VideoLLaMB: Long-context Video Understanding with Recurrent Memory Bridges",
            "url": "https://huggingface.co/papers/2409.01071",
            "abstract": "Recent advancements in large-scale video-language models have shown significant potential for real-time planning and detailed interactions. However, their high computational demands and the scarcity of annotated datasets limit their practicality for academic researchers. In this work, we introduce VideoLLaMB, a novel framework that utilizes temporal memory tokens within bridge layers to allow for the encoding of entire video sequences alongside historical visual data, effectively preserving semantic continuity and enhancing model performance across various tasks. This approach includes recurrent memory tokens and a SceneTilling algorithm, which segments videos into independent semantic units to preserve semantic integrity. Empirically, VideoLLaMB significantly outstrips existing video-language models, demonstrating a 5.5 points improvement over its competitors across three VideoQA benchmarks, and 2.06 points on egocentric planning. Comprehensive results on the MVBench show that VideoLLaMB-7B achieves markedly better results than previous 7B models of same LLM. Remarkably, it maintains robust performance as PLLaVA even as video length increases up to 8 times. Besides, the frame retrieval results on our specialized Needle in a Video Haystack (NIAVH) benchmark, further validate VideoLLaMB's prowess in accurately identifying specific frames within lengthy videos. Our SceneTilling algorithm also enables the generation of streaming video captions directly, without necessitating additional training. In terms of efficiency, VideoLLaMB, trained on 16 frames, supports up to 320 frames on a single Nvidia A100 GPU with linear GPU memory scaling, ensuring both high performance and cost-effectiveness, thereby setting a new foundation for long-form video-language models in both academic and practical applications.",
            "score": 26,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "3a4932f3d059c107",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "VideoLLaMB: Прорыв в обработке длинных видео с сохранением семантической целостности",
                    "desc": "VideoLLaMB - это новая модель для обработки видео и языка, использующая временные токены памяти в мостовых слоях для кодирования целых видеопоследовательностей. Модель включает рекуррентные токены памяти и алгоритм SceneTilling для сегментации видео на семантические единицы. VideoLLaMB значительно превосходит существующие модели на нескольких бенчмарках, демонстрируя улучшение на 5.5 пунктов в задачах VideoQA. Модель эффективно обрабатывает длинные видео и поддерживает до 320 кадров на одном GPU Nvidia A100."
                },
                "en": {
                    "title": "VideoLLaMB: Revolutionizing Video-Language Models with Efficiency and Performance",
                    "desc": "This paper presents VideoLLaMB, a new framework designed to improve the performance of video-language models by using temporal memory tokens and a SceneTilling algorithm. These innovations allow the model to encode entire video sequences while maintaining semantic continuity, which is crucial for tasks like video question answering and planning. VideoLLaMB outperforms existing models, showing significant improvements in benchmarks and maintaining efficiency even with longer videos. The framework is also cost-effective, enabling high performance on a single GPU, making it accessible for both researchers and practical applications."
                },
                "zh": {
                    "title": "VideoLLaMB：高效的视频语言模型新基石",
                    "desc": "本文介绍了一种新颖的视频语言模型框架VideoLLaMB，利用时间记忆标记在桥接层中编码整个视频序列和历史视觉数据，从而增强模型在各种任务中的表现。该方法通过引入递归记忆标记和场景切分算法，将视频分割为独立的语义单元，以保持语义完整性。实验结果表明，VideoLLaMB在三个视频问答基准上比现有模型提高了5.5分，在自我中心规划上提高了2.06分，显示出其优越的性能。VideoLLaMB在效率上也表现出色，能够在单个Nvidia A100 GPU上支持高达320帧的处理，确保高性能和成本效益。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00588",
            "title": "Diffusion Policy Policy Optimization",
            "url": "https://huggingface.co/papers/2409.00588",
            "abstract": "We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic framework including best practices for fine-tuning diffusion-based policies (e.g. Diffusion Policy) in continuous control and robot learning tasks using the policy gradient (PG) method from reinforcement learning (RL). PG methods are ubiquitous in training RL policies with other policy parameterizations; nevertheless, they had been conjectured to be less efficient for diffusion-based policies. Surprisingly, we show that DPPO achieves the strongest overall performance and efficiency for fine-tuning in common benchmarks compared to other RL methods for diffusion-based policies and also compared to PG fine-tuning of other policy parameterizations. Through experimental investigation, we find that DPPO takes advantage of unique synergies between RL fine-tuning and the diffusion parameterization, leading to structured and on-manifold exploration, stable training, and strong policy robustness. We further demonstrate the strengths of DPPO in a range of realistic settings, including simulated robotic tasks with pixel observations, and via zero-shot deployment of simulation-trained policies on robot hardware in a long-horizon, multi-stage manipulation task. Website with code: diffusion-ppo.github.io",
            "score": 19,
            "issue_id": 1,
            "pub_date": "2024-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "a92679b4e7f59810",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#robots",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "DPPO: Мощный метод настройки диффузионных политик для робототехники",
                    "desc": "Статья представляет DPPO (Diffusion Policy Policy Optimization) - алгоритмическую структуру для настройки политик на основе диффузии в задачах непрерывного управления и обучения роботов. Авторы показывают, что DPPO достигает наилучших результатов по сравнению с другими методами обучения с подкреплением для политик на основе диффузии. Исследование выявляет уникальную синергию между настройкой с помощью обучения с подкреплением и параметризацией диффузии, что приводит к структурированному исследованию, стабильному обучению и устойчивости политики. Эффективность DPPO демонстрируется на реалистичных задачах, включая симулированные робототехнические задачи с пиксельными наблюдениями и развертывание обученных в симуляции политик на реальном роботизированном оборудовании."
                },
                "en": {
                    "title": "Unlocking Efficiency in Robot Learning with DPPO",
                    "desc": "The paper presents Diffusion Policy Policy Optimization (DPPO), a new framework designed to enhance the fine-tuning of diffusion-based policies in continuous control and robot learning tasks. It utilizes the policy gradient method from reinforcement learning, which is commonly used for training various policy types. The authors demonstrate that DPPO outperforms other reinforcement learning methods and traditional policy gradient fine-tuning, achieving superior performance and efficiency on standard benchmarks. Additionally, DPPO benefits from the unique characteristics of diffusion parameterization, enabling effective exploration, stable training, and robust policy performance in complex robotic tasks."
                },
                "zh": {
                    "title": "扩散策略优化：强化学习的新突破",
                    "desc": "我们提出了一种名为扩散策略优化（DPPO）的算法框架，旨在通过强化学习中的策略梯度方法对基于扩散的策略进行微调。尽管策略梯度方法在训练强化学习策略中广泛应用，但它们在扩散策略上的效率曾被认为较低。令人惊讶的是，DPPO在常见基准测试中表现出色，显示出其在微调方面的强大性能和效率。通过实验，我们发现DPPO利用了强化学习微调与扩散参数化之间的独特协同作用，从而实现了结构化的探索、稳定的训练和强大的策略鲁棒性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00558",
            "title": "Compositional 3D-aware Video Generation with LLM Director",
            "url": "https://huggingface.co/papers/2409.00558",
            "abstract": "Significant progress has been made in text-to-video generation through the use of powerful generative models and large-scale internet data. However, substantial challenges remain in precisely controlling individual concepts within the generated video, such as the motion and appearance of specific characters and the movement of viewpoints. In this work, we propose a novel paradigm that generates each concept in 3D representation separately and then composes them with priors from Large Language Models (LLM) and 2D diffusion models. Specifically, given an input textual prompt, our scheme consists of three stages: 1) We leverage LLM as the director to first decompose the complex query into several sub-prompts that indicate individual concepts within the video~(e.g., scene, objects, motions), then we let LLM to invoke pre-trained expert models to obtain corresponding 3D representations of concepts. 2) To compose these representations, we prompt multi-modal LLM to produce coarse guidance on the scales and coordinates of trajectories for the objects. 3) To make the generated frames adhere to natural image distribution, we further leverage 2D diffusion priors and use Score Distillation Sampling to refine the composition. Extensive experiments demonstrate that our method can generate high-fidelity videos from text with diverse motion and flexible control over each concept. Project page: https://aka.ms/c3v.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2024-08-31",
            "pub_date_card": {
                "ru": "31 августа",
                "en": "August 31",
                "zh": "8月31日"
            },
            "hash": "dd559e15ff6e9a56",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "3D-композиция для управляемой генерации видео из текста",
                    "desc": "В статье представлен новый подход к генерации видео на основе текста с использованием 3D-представлений отдельных концепций. Авторы предлагают трехэтапный процесс, включающий декомпозицию запроса с помощью большой языковой модели (LLM), создание 3D-представлений и их композицию. Метод использует предобученные экспертные модели и диффузионные модели для улучшения качества генерации. Эксперименты показывают, что данный подход позволяет создавать высококачественные видео с разнообразными движениями и гибким контролем над отдельными элементами."
                },
                "en": {
                    "title": "Mastering Video Generation: Control Every Concept!",
                    "desc": "This paper presents a new approach to generating videos from text prompts by separately creating 3D representations of individual concepts. It utilizes Large Language Models (LLMs) to break down complex queries into simpler sub-prompts, which helps in controlling specific elements like motion and appearance. The method involves a three-stage process: decomposing the input, guiding the composition of 3D representations, and refining the output using 2D diffusion models. The results show that this approach allows for high-quality video generation with precise control over various aspects of the content."
                },
                "zh": {
                    "title": "精确控制视频生成中的概念",
                    "desc": "本文提出了一种新颖的文本到视频生成方法，旨在更精确地控制生成视频中的各个概念。我们首先利用大型语言模型（LLM）将复杂的文本提示分解为多个子提示，以获取每个概念的3D表示。接着，通过多模态LLM生成对象的运动轨迹的粗略指导，最后结合2D扩散模型进行细化，确保生成的帧符合自然图像分布。实验结果表明，该方法能够从文本生成高保真度的视频，并对每个概念进行灵活控制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00729",
            "title": "ContextCite: Attributing Model Generation to Context",
            "url": "https://huggingface.co/papers/2409.00729",
            "abstract": "How do language models use information provided as context when generating a response? Can we infer whether a particular generated statement is actually grounded in the context, a misinterpretation, or fabricated? To help answer these questions, we introduce the problem of context attribution: pinpointing the parts of the context (if any) that led a model to generate a particular statement. We then present ContextCite, a simple and scalable method for context attribution that can be applied on top of any existing language model. Finally, we showcase the utility of ContextCite through three applications: (1) helping verify generated statements (2) improving response quality by pruning the context and (3) detecting poisoning attacks. We provide code for ContextCite at https://github.com/MadryLab/context-cite.",
            "score": 13,
            "issue_id": 1,
            "pub_date": "2024-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "69f8f12112b36fb2",
            "data": {
                "categories": [
                    "#interpretability",
                    "#alignment",
                    "#hallucinations"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "ContextCite: прозрачность генерации текста языковыми моделями",
                    "desc": "Статья представляет метод ContextCite для определения, какие части контекста использовались языковой моделью при генерации ответа. Авторы вводят понятие 'context attribution' - привязки генерируемых утверждений к конкретным частям контекста. ContextCite можно применять поверх существующих языковых моделей для проверки сгенерированных утверждений, улучшения качества ответов и обнаружения атак. Метод позволяет понять, основано ли утверждение на контексте, является ли оно неверной интерпретацией или выдумкой."
                },
                "en": {
                    "title": "ContextCite: Uncovering the Roots of Language Model Responses",
                    "desc": "This paper addresses how language models utilize context when generating responses and whether generated statements are based on that context. It introduces the concept of context attribution, which identifies specific parts of the context that influence the model's output. The authors present ContextCite, a method that can be easily integrated with any language model to perform context attribution. They demonstrate its effectiveness in verifying statements, enhancing response quality, and detecting potential poisoning attacks."
                },
                "zh": {
                    "title": "揭示语言模型生成背后的上下文",
                    "desc": "本文探讨了语言模型在生成响应时如何利用上下文信息。我们提出了上下文归因的问题，即确定哪些上下文部分导致模型生成特定语句。为此，我们介绍了ContextCite，这是一种简单且可扩展的上下文归因方法，可以应用于任何现有的语言模型。通过三个应用案例，我们展示了ContextCite的实用性，包括验证生成语句、提高响应质量和检测攻击。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.01199",
            "title": "OD-VAE: An Omni-dimensional Video Compressor for Improving Latent Video Diffusion Model",
            "url": "https://huggingface.co/papers/2409.01199",
            "abstract": "Variational Autoencoder (VAE), compressing videos into latent representations, is a crucial preceding component of Latent Video Diffusion Models (LVDMs). With the same reconstruction quality, the more sufficient the VAE's compression for videos is, the more efficient the LVDMs are. However, most LVDMs utilize 2D image VAE, whose compression for videos is only in the spatial dimension and often ignored in the temporal dimension. How to conduct temporal compression for videos in a VAE to obtain more concise latent representations while promising accurate reconstruction is seldom explored. To fill this gap, we propose an omni-dimension compression VAE, named OD-VAE, which can temporally and spatially compress videos. Although OD-VAE's more sufficient compression brings a great challenge to video reconstruction, it can still achieve high reconstructed accuracy by our fine design. To obtain a better trade-off between video reconstruction quality and compression speed, four variants of OD-VAE are introduced and analyzed. In addition, a novel tail initialization is designed to train OD-VAE more efficiently, and a novel inference strategy is proposed to enable OD-VAE to handle videos of arbitrary length with limited GPU memory. Comprehensive experiments on video reconstruction and LVDM-based video generation demonstrate the effectiveness and efficiency of our proposed methods.",
            "score": 12,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "b85158ae49081549",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#training",
                    "#inference"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "OD-VAE: Революция в сжатии видео для эффективных латентных видеодиффузионных моделей",
                    "desc": "Статья представляет OD-VAE - вариационный автоэнкодер для сжатия видео как в пространственном, так и во временном измерении. Авторы предлагают несколько вариантов OD-VAE для оптимального баланса между качеством реконструкции и скоростью сжатия. Разработаны новые методы инициализации и стратегия вывода для эффективного обучения и работы с видео произвольной длины при ограниченной памяти GPU. Эксперименты показывают эффективность предложенного подхода для реконструкции видео и генерации видео на основе латентных видеодиффузионных моделей."
                },
                "en": {
                    "title": "Revolutionizing Video Compression with OD-VAE",
                    "desc": "This paper introduces the Omni-Dimension Variational Autoencoder (OD-VAE), which enhances video compression by addressing both spatial and temporal dimensions. Traditional VAEs typically focus on spatial compression, neglecting the temporal aspect, which limits their efficiency in video applications. OD-VAE achieves a more concise latent representation while maintaining high reconstruction accuracy through innovative design strategies. The authors also present four variants of OD-VAE and novel techniques for efficient training and inference, demonstrating significant improvements in video reconstruction and generation tasks."
                },
                "zh": {
                    "title": "全维压缩，提升视频重建效率",
                    "desc": "变分自编码器（VAE）在将视频压缩为潜在表示方面起着重要作用，是潜在视频扩散模型（LVDM）的关键组成部分。我们提出了一种全维压缩的变分自编码器（OD-VAE），能够同时进行时间和空间上的视频压缩，从而获得更简洁的潜在表示。尽管OD-VAE的压缩带来了视频重建的挑战，但通过精心设计，它仍能实现高重建精度。我们还引入了四种OD-VAE的变体，并设计了一种新颖的尾部初始化方法，以提高训练效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00492",
            "title": "Accurate Compression of Text-to-Image Diffusion Models via Vector Quantization",
            "url": "https://huggingface.co/papers/2409.00492",
            "abstract": "Text-to-image diffusion models have emerged as a powerful framework for high-quality image generation given textual prompts. Their success has driven the rapid development of production-grade diffusion models that consistently increase in size and already contain billions of parameters. As a result, state-of-the-art text-to-image models are becoming less accessible in practice, especially in resource-limited environments. Post-training quantization (PTQ) tackles this issue by compressing the pretrained model weights into lower-bit representations. Recent diffusion quantization techniques primarily rely on uniform scalar quantization, providing decent performance for the models compressed to 4 bits. This work demonstrates that more versatile vector quantization (VQ) may achieve higher compression rates for large-scale text-to-image diffusion models. Specifically, we tailor vector-based PTQ methods to recent billion-scale text-to-image models (SDXL and SDXL-Turbo), and show that the diffusion models of 2B+ parameters compressed to around 3 bits using VQ exhibit the similar image quality and textual alignment as previous 4-bit compression techniques.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-08-31",
            "pub_date_card": {
                "ru": "31 августа",
                "en": "August 31",
                "zh": "8月31日"
            },
            "hash": "caecd9179e740837",
            "data": {
                "categories": [
                    "#inference",
                    "#diffusion"
                ],
                "emoji": "🗜️",
                "ru": {
                    "title": "Векторное квантование сжимает гигантские модели без потери качества",
                    "desc": "Эта статья посвящена применению векторного квантования (VQ) для сжатия крупномасштабных диффузионных моделей генерации изображений по текстовому описанию. Авторы адаптировали методы векторного квантования для моделей SDXL и SDXL-Turbo, содержащих более 2 миллиардов параметров. Результаты показывают, что сжатие до 3 бит с помощью VQ позволяет сохранить качество изображений и соответствие текстовым описаниям на уровне предыдущих методов 4-битного сжатия. Это исследование открывает путь к более эффективному использованию ресурсов при работе с крупными диффузионными моделями."
                },
                "en": {
                    "title": "Enhancing Accessibility of Text-to-Image Models through Vector Quantization",
                    "desc": "This paper discusses the advancements in text-to-image diffusion models, which are used for generating high-quality images from text prompts. It highlights the challenge of accessibility due to the increasing size of these models, which can have billions of parameters. To address this, the authors propose post-training quantization (PTQ) as a method to compress model weights into lower-bit formats. They specifically focus on vector quantization (VQ) techniques, demonstrating that they can achieve better compression rates while maintaining image quality and alignment with text prompts compared to traditional 4-bit methods."
                },
                "zh": {
                    "title": "向量量化提升文本到图像模型的压缩效率",
                    "desc": "文本到图像的扩散模型是一种强大的图像生成框架，可以根据文本提示生成高质量图像。随着模型规模的迅速扩大，现有的最先进模型已经包含数十亿个参数，这使得在资源有限的环境中使用这些模型变得更加困难。为了解决这个问题，后训练量化（PTQ）通过将预训练模型的权重压缩为低位表示来降低模型的复杂性。本文展示了向量量化（VQ）在大规模文本到图像扩散模型中的应用，证明了使用VQ进行压缩可以在保持图像质量和文本对齐的同时，达到更高的压缩率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.01392",
            "title": "GenAgent: Build Collaborative AI Systems with Automated Workflow Generation -- Case Studies on ComfyUI",
            "url": "https://huggingface.co/papers/2409.01392",
            "abstract": "Much previous AI research has focused on developing monolithic models to maximize their intelligence and capability, with the primary goal of enhancing performance on specific tasks. In contrast, this paper explores an alternative approach: collaborative AI systems that use workflows to integrate models, data sources, and pipelines to solve complex and diverse tasks. We introduce GenAgent, an LLM-based framework that automatically generates complex workflows, offering greater flexibility and scalability compared to monolithic models. The core innovation of GenAgent lies in representing workflows with code, alongside constructing workflows with collaborative agents in a step-by-step manner. We implement GenAgent on the ComfyUI platform and propose a new benchmark, OpenComfy. The results demonstrate that GenAgent outperforms baseline approaches in both run-level and task-level evaluations, showing its capability to generate complex workflows with superior effectiveness and stability.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "ccb90d08b0c22618",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Генерация сложных ИИ-процессов с помощью коллаборативных агентов",
                    "desc": "Эта статья представляет GenAgent - фреймворк на основе больших языковых моделей для автоматической генерации сложных рабочих процессов в ИИ-системах. В отличие от монолитных моделей, GenAgent использует совместную работу агентов для пошагового создания рабочих процессов, представленных в виде кода. Фреймворк реализован на платформе ComfyUI и протестирован на новом бенчмарке OpenComfy. Результаты показывают, что GenAgent превосходит базовые подходы по эффективности и стабильности при создании сложных рабочих процессов."
                },
                "en": {
                    "title": "Empowering AI Collaboration with GenAgent Workflows",
                    "desc": "This paper presents GenAgent, a framework that utilizes large language models (LLMs) to create collaborative AI systems. Unlike traditional monolithic models that focus on single tasks, GenAgent generates complex workflows that integrate various models and data sources. The innovative aspect of GenAgent is its ability to represent workflows as code, allowing for step-by-step construction with collaborative agents. The implementation on the ComfyUI platform and the introduction of the OpenComfy benchmark show that GenAgent significantly outperforms existing methods in generating effective and stable workflows."
                },
                "zh": {
                    "title": "协作AI：灵活高效的工作流生成",
                    "desc": "这篇论文探讨了一种新的人工智能系统，称为协作AI系统，旨在通过工作流整合模型、数据源和管道来解决复杂任务。我们介绍了GenAgent，这是一个基于大型语言模型的框架，能够自动生成复杂的工作流，提供比单一模型更大的灵活性和可扩展性。GenAgent的核心创新在于用代码表示工作流，并通过协作代理逐步构建工作流。实验结果表明，GenAgent在运行级别和任务级别的评估中均优于基线方法，展示了其生成复杂工作流的卓越效果和稳定性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.01055",
            "title": "Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation",
            "url": "https://huggingface.co/papers/2409.01055",
            "abstract": "This paper explores higher-resolution video outpainting with extensive content generation. We point out common issues faced by existing methods when attempting to largely outpaint videos: the generation of low-quality content and limitations imposed by GPU memory. To address these challenges, we propose a diffusion-based method called Follow-Your-Canvas. It builds upon two core designs. First, instead of employing the common practice of \"single-shot\" outpainting, we distribute the task across spatial windows and seamlessly merge them. It allows us to outpaint videos of any size and resolution without being constrained by GPU memory. Second, the source video and its relative positional relation are injected into the generation process of each window. It makes the generated spatial layout within each window harmonize with the source video. Coupling with these two designs enables us to generate higher-resolution outpainting videos with rich content while keeping spatial and temporal consistency. Follow-Your-Canvas excels in large-scale video outpainting, e.g., from 512X512 to 1152X2048 (9X), while producing high-quality and aesthetically pleasing results. It achieves the best quantitative results across various resolution and scale setups. The code is released on https://github.com/mayuelala/FollowYourCanvas",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "f683bbfc6edc815b",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Расширение видео без границ: Follow-Your-Canvas покоряет новые горизонты",
                    "desc": "Эта статья представляет новый метод для расширения видео с высоким разрешением, называемый Follow-Your-Canvas. Метод основан на диффузионной модели и решает проблемы низкого качества контента и ограничений памяти GPU, с которыми сталкиваются существующие подходы. Follow-Your-Canvas использует распределенную генерацию по пространственным окнам и внедряет информацию об исходном видео в процесс генерации каждого окна. Это позволяет создавать высококачественные расширенные видео с богатым содержанием, сохраняя пространственную и временную согласованность."
                },
                "en": {
                    "title": "Revolutionizing Video Outpainting with Follow-Your-Canvas",
                    "desc": "This paper presents a novel approach for higher-resolution video outpainting using a method called Follow-Your-Canvas. It addresses common challenges in video outpainting, such as low-quality content generation and GPU memory limitations, by distributing the outpainting task across spatial windows. The method incorporates the source video's positional information to ensure that the generated content aligns well with the original video, maintaining both spatial and temporal consistency. As a result, Follow-Your-Canvas can effectively generate high-quality, large-scale outpainted videos, significantly improving upon existing techniques."
                },
                "zh": {
                    "title": "高分辨率视频外延的新方法",
                    "desc": "本文探讨了高分辨率视频的外延生成，提出了一种名为Follow-Your-Canvas的扩散方法。我们指出现有方法在大规模视频外延时常遇到的低质量内容生成和GPU内存限制等问题。该方法通过将任务分布到空间窗口并无缝合并，解决了内存限制，支持任意大小和分辨率的视频外延。同时，源视频及其相对位置关系被注入到每个窗口的生成过程中，确保生成内容与源视频的空间布局和谐一致。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00391",
            "title": "Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders",
            "url": "https://huggingface.co/papers/2409.00391",
            "abstract": "Speech-based depression detection poses significant challenges for automated detection due to its unique manifestation across individuals and data scarcity. Addressing these challenges, we introduce DAAMAudioCNNLSTM and DAAMAudioTransformer, two parameter efficient and explainable models for audio feature extraction and depression detection. DAAMAudioCNNLSTM features a novel CNN-LSTM framework with multi-head Density Adaptive Attention Mechanism (DAAM), focusing dynamically on informative speech segments. DAAMAudioTransformer, leveraging a transformer encoder in place of the CNN-LSTM architecture, incorporates the same DAAM module for enhanced attention and interpretability. These approaches not only enhance detection robustness and interpretability but also achieve state-of-the-art performance: DAAMAudioCNNLSTM with an F1 macro score of 0.702 and DAAMAudioTransformer with an F1 macro score of 0.72 on the DAIC-WOZ dataset, without reliance on supplementary information such as vowel positions and speaker information during training/validation as in previous approaches. Both models' significant explainability and efficiency in leveraging speech signals for depression detection represent a leap towards more reliable, clinically useful diagnostic tools, promising advancements in speech and mental health care. To foster further research in this domain, we make our code publicly available.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2024-08-31",
            "pub_date_card": {
                "ru": "31 августа",
                "en": "August 31",
                "zh": "8月31日"
            },
            "hash": "76a69d5f1be00050",
            "data": {
                "categories": [
                    "#audio",
                    "#interpretability",
                    "#medicine"
                ],
                "emoji": "🎙️",
                "ru": {
                    "title": "Инновационные модели машинного обучения для выявления депрессии по голосу",
                    "desc": "Статья представляет два новых метода для обнаружения депрессии по речи: DAAMAudioCNNLSTM и DAAMAudioTransformer. Обе модели используют механизм адаптивного внимания к плотности (DAAM) для фокусировки на информативных сегментах речи. Модели демонстрируют высокую производительность на датасете DAIC-WOZ, достигая F1-macro score 0.702 и 0.72 соответственно. Предложенные подходы обеспечивают интерпретируемость результатов и эффективность в использовании речевых сигналов для диагностики депрессии."
                },
                "en": {
                    "title": "Revolutionizing Depression Detection with Speech Analysis",
                    "desc": "This paper presents two innovative models, DAAMAudioCNNLSTM and DAAMAudioTransformer, designed for detecting depression through speech analysis. Both models utilize a Density Adaptive Attention Mechanism (DAAM) to focus on the most relevant parts of speech data, improving the accuracy of detection. The DAAMAudioCNNLSTM combines Convolutional Neural Networks (CNN) with Long Short-Term Memory (LSTM) networks, while the DAAMAudioTransformer employs a transformer architecture for enhanced interpretability. Achieving state-of-the-art F1 macro scores on the DAIC-WOZ dataset, these models demonstrate significant advancements in automated depression detection without needing additional speaker information."
                },
                "zh": {
                    "title": "基于语音的抑郁检测新突破",
                    "desc": "本论文提出了两种新的模型，DAAMAudioCNNLSTM和DAAMAudioTransformer，用于基于语音的抑郁检测。这些模型通过引入多头密度自适应注意机制（DAAM），有效提取音频特征并动态关注重要的语音片段。与以往方法不同，这些模型在训练和验证过程中不依赖于额外的信息，如元音位置和说话者信息。实验结果表明，这两种模型在DAIC-WOZ数据集上达到了最先进的性能，展示了在抑郁检测领域的显著进展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.01357",
            "title": "Know When to Fuse: Investigating Non-English Hybrid Retrieval in the Legal Domain",
            "url": "https://huggingface.co/papers/2409.01357",
            "abstract": "Hybrid search has emerged as an effective strategy to offset the limitations of different matching paradigms, especially in out-of-domain contexts where notable improvements in retrieval quality have been observed. However, existing research predominantly focuses on a limited set of retrieval methods, evaluated in pairs on domain-general datasets exclusively in English. In this work, we study the efficacy of hybrid search across a variety of prominent retrieval models within the unexplored field of law in the French language, assessing both zero-shot and in-domain scenarios. Our findings reveal that in a zero-shot context, fusing different domain-general models consistently enhances performance compared to using a standalone model, regardless of the fusion method. Surprisingly, when models are trained in-domain, we find that fusion generally diminishes performance relative to using the best single system, unless fusing scores with carefully tuned weights. These novel insights, among others, expand the applicability of prior findings across a new field and language, and contribute to a deeper understanding of hybrid search in non-English specialized domains.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "8cc67a6869d64f74",
            "data": {
                "categories": [
                    "#rag",
                    "#multilingual"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Гибридный поиск в юриспруденции: неожиданные результаты для французского языка",
                    "desc": "В статье исследуется эффективность гибридного поиска в юридической области на французском языке. Авторы оценивают различные модели извлечения информации в сценариях zero-shot и обучения на предметной области. Результаты показывают, что в zero-shot режиме объединение разных моделей улучшает производительность. Однако при обучении на предметной области слияние моделей обычно снижает эффективность, если не использовать тщательно подобранные веса."
                },
                "en": {
                    "title": "Enhancing Legal Search: The Power of Hybrid Models in French",
                    "desc": "This paper explores hybrid search, which combines different retrieval methods to improve search results, particularly in the legal domain using the French language. The authors find that using a mix of models in a zero-shot scenario leads to better performance than any single model. However, when models are trained specifically for the legal domain, combining them often results in worse performance unless the fusion weights are finely adjusted. These results highlight the complexities of hybrid search in specialized fields and suggest that previous findings may not directly apply to non-English contexts."
                },
                "zh": {
                    "title": "混合搜索：提升检索质量的新策略",
                    "desc": "混合搜索是一种有效的策略，可以弥补不同匹配范式的局限性，特别是在领域外的情况下，检索质量显著提高。现有研究主要集中在有限的检索方法上，且仅在英语的领域通用数据集上进行评估。本文研究了混合搜索在法语法律领域的有效性，评估了零样本和领域内场景。我们的发现表明，在零样本情况下，融合不同的领域通用模型始终能提高性能，而在领域内训练的模型中，除非使用精心调整的权重进行融合，否则融合通常会降低性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00447",
            "title": "The MERIT Dataset: Modelling and Efficiently Rendering Interpretable Transcripts",
            "url": "https://huggingface.co/papers/2409.00447",
            "abstract": "This paper introduces the MERIT Dataset, a multimodal (text + image + layout) fully labeled dataset within the context of school reports. Comprising over 400 labels and 33k samples, the MERIT Dataset is a valuable resource for training models in demanding Visually-rich Document Understanding (VrDU) tasks. By its nature (student grade reports), the MERIT Dataset can potentially include biases in a controlled way, making it a valuable tool to benchmark biases induced in Language Models (LLMs). The paper outlines the dataset's generation pipeline and highlights its main features in the textual, visual, layout, and bias domains. To demonstrate the dataset's utility, we present a benchmark with token classification models, showing that the dataset poses a significant challenge even for SOTA models and that these would greatly benefit from including samples from the MERIT Dataset in their pretraining phase.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2024-08-31",
            "pub_date_card": {
                "ru": "31 августа",
                "en": "August 31",
                "zh": "8月31日"
            },
            "hash": "7d3ef5f0783e4fb6",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "MERIT: мультимодальный датасет для анализа документов и оценки предвзятости ИИ",
                    "desc": "Статья представляет набор данных MERIT - мультимодальный датасет для понимания визуально насыщенных документов. Он содержит более 400 меток и 33 тысячи образцов школьных отчетов, включая текст, изображения и разметку. MERIT может использоваться для оценки предвзятости языковых моделей, так как содержит потенциальные смещения в контролируемом виде. В работе описывается процесс создания датасета и проводится бенчмарк моделей классификации токенов, демонстрирующий сложность задачи даже для современных алгоритмов."
                },
                "en": {
                    "title": "Unlocking Visually-rich Document Understanding with MERIT",
                    "desc": "The MERIT Dataset is a new resource designed for training machine learning models on school reports, combining text, images, and layout information. It contains over 33,000 samples and 400 labels, making it suitable for complex tasks in Visually-rich Document Understanding (VrDU). The dataset also allows researchers to study biases in Language Models (LLMs) due to its specific context. The paper demonstrates that even state-of-the-art models struggle with this dataset, indicating its potential to improve model performance when included in pretraining."
                },
                "zh": {
                    "title": "MERIT数据集：推动视觉文档理解的利器",
                    "desc": "本文介绍了MERIT数据集，这是一个包含文本、图像和布局的多模态完全标注数据集，专注于学校报告。该数据集包含超过400个标签和33,000个样本，是训练视觉丰富文档理解（VrDU）任务模型的重要资源。由于其特性（学生成绩报告），MERIT数据集可能以受控方式包含偏见，成为评估语言模型（LLMs）偏见的重要工具。文章还描述了数据集的生成流程，并强调了其在文本、视觉、布局和偏见领域的主要特征。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00138",
            "title": "PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action",
            "url": "https://huggingface.co/papers/2409.00138",
            "abstract": "As language models (LMs) are widely utilized in personalized communication scenarios (e.g., sending emails, writing social media posts) and endowed with a certain level of agency, ensuring they act in accordance with the contextual privacy norms becomes increasingly critical. However, quantifying the privacy norm awareness of LMs and the emerging privacy risk in LM-mediated communication is challenging due to (1) the contextual and long-tailed nature of privacy-sensitive cases, and (2) the lack of evaluation approaches that capture realistic application scenarios. To address these challenges, we propose PrivacyLens, a novel framework designed to extend privacy-sensitive seeds into expressive vignettes and further into agent trajectories, enabling multi-level evaluation of privacy leakage in LM agents' actions. We instantiate PrivacyLens with a collection of privacy norms grounded in privacy literature and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM performance in answering probing questions and their actual behavior when executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4 and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even when prompted with privacy-enhancing instructions. We also demonstrate the dynamic nature of PrivacyLens by extending each seed into multiple trajectories to red-team LM privacy leakage risk. Dataset and code are available at https://github.com/SALT-NLP/PrivacyLens.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2024-08-29",
            "pub_date_card": {
                "ru": "29 августа",
                "en": "August 29",
                "zh": "8月29日"
            },
            "hash": "b5d2f82929b24c1c",
            "data": {
                "categories": [
                    "#dataset",
                    "#agents",
                    "#ethics"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "PrivacyLens: защита приватности в эпоху языковых моделей",
                    "desc": "Авторы представляют PrivacyLens - новую систему для оценки осведомленности языковых моделей о нормах конфиденциальности и рисках утечки данных в коммуникациях с их участием. Система генерирует сценарии и траектории агентов на основе исходных примеров, связанных с приватностью. Эксперименты показали, что современные языковые модели, такие как GPT-4 и Llama-3-70B, допускают утечку конфиденциальной информации в 25-39% случаев даже при наличии инструкций по защите приватности. PrivacyLens позволяет проводить многоуровневую оценку рисков и тестировать модели на устойчивость к утечкам данных."
                },
                "en": {
                    "title": "Enhancing Privacy Awareness in Language Models with PrivacyLens",
                    "desc": "This paper introduces PrivacyLens, a framework aimed at evaluating the privacy awareness of language models (LMs) in communication tasks. It addresses the challenges of quantifying privacy norms due to the complex and varied nature of privacy-sensitive situations. The framework allows for a multi-level assessment of privacy leakage by transforming privacy-sensitive seeds into detailed scenarios and agent actions. The study reveals significant privacy risks, showing that advanced LMs like GPT-4 and Llama-3-70B can leak sensitive information in a substantial percentage of cases, even when given privacy-focused instructions."
                },
                "zh": {
                    "title": "确保语言模型遵循隐私规范的关键",
                    "desc": "本文提出了一种名为PrivacyLens的新框架，旨在评估语言模型在个性化沟通中对隐私规范的遵循情况。该框架通过将隐私敏感的种子扩展为生动的场景和代理轨迹，实现对隐私泄露的多层次评估。研究发现，尽管先进的语言模型在回答隐私相关问题时表现良好，但在实际执行用户指令时，仍有相当比例的案例泄露敏感信息。通过动态扩展种子，PrivacyLens能够有效识别和评估语言模型的隐私泄露风险。"
                }
            }
        }
    ],
    "link_prev": "2024-09-03.html",
    "link_next": "2024-09-05.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "03.09",
        "en": "09/03",
        "zh": "9月3日"
    },
    "short_date_next": {
        "ru": "05.09",
        "en": "09/05",
        "zh": "9月5日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 4,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 3,
        "#3d": 1,
        "#audio": 1,
        "#video": 5,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 5,
        "#medicine": 2,
        "#training": 8,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 6,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 1,
        "#translation": 0,
        "#robots": 0
    }
}