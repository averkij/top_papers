{
    "date": {
        "ru": "4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 4",
        "zh": "9æœˆ4æ—¥"
    },
    "time_utc": "2024-09-04 09:00",
    "weekday": 2,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-04",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.01704",
            "title": "General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model",
            "url": "https://huggingface.co/papers/2409.01704",
            "abstract": "Traditional OCR systems (OCR-1.0) are increasingly unable to meet people's usage due to the growing demand for intelligent processing of man-made optical characters. In this paper, we collectively refer to all artificial optical signals (e.g., plain texts, math/molecular formulas, tables, charts, sheet music, and even geometric shapes) as \"characters\" and propose the General OCR Theory along with an excellent model, namely GOT, to promote the arrival of OCR-2.0. The GOT, with 580M parameters, is a unified, elegant, and end-to-end model, consisting of a high-compression encoder and a long-contexts decoder. As an OCR-2.0 model, GOT can handle all the above \"characters\" under various OCR tasks. On the input side, the model supports commonly used scene- and document-style images in slice and whole-page styles. On the output side, GOT can generate plain or formatted results (markdown/tikz/smiles/kern) via an easy prompt. Besides, the model enjoys interactive OCR features, i.e., region-level recognition guided by coordinates or colors. Furthermore, we also adapt dynamic resolution and multi-page OCR technologies to GOT for better practicality. In experiments, we provide sufficient results to prove the superiority of our model.",
            "score": 80,
            "issue_id": 1,
            "pub_date": "2024-09-03",
            "pub_date_card": {
                "ru": "3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 3",
                "zh": "9æœˆ3æ—¥"
            },
            "hash": "cc052755a384b1f8",
            "data": {
                "categories": [
                    "#cv",
                    "#long_context",
                    "#optimization"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "GOT: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ€Ñ‹ OCR 2.0",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GOT Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² (OCR). Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ 'ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²', Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚, Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹, Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ„Ğ¸Ğ³ÑƒÑ€Ñ‹. GOT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº-Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ¸ Ğ¾Ğ±ĞµÑ‰Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒ ÑˆĞ°Ğ³Ğ¾Ğ¼ Ğº OCR 2.0."
                },
                "en": {
                    "title": "Revolutionizing OCR with GOT: The Future of Intelligent Character Recognition",
                    "desc": "This paper introduces the General OCR Theory and a new model called GOT, designed to enhance optical character recognition (OCR) capabilities. GOT is an end-to-end model with 580 million parameters that can process a wide range of artificial optical signals, including text, formulas, and charts. It features a high-compression encoder and a long-context decoder, allowing it to handle various OCR tasks effectively. The model supports both scene and document images and can produce formatted outputs while offering interactive features for improved user experience."
                },
                "zh": {
                    "title": "æ¨åŠ¨OCR-2.0çš„é€šç”¨OCRç†è®ºä¸GOTæ¨¡å‹",
                    "desc": "ä¼ ç»Ÿçš„å…‰å­¦å­—ç¬¦è¯†åˆ«ç³»ç»Ÿï¼ˆOCR-1.0ï¼‰å·²æ— æ³•æ»¡è¶³äººä»¬å¯¹æ™ºèƒ½å¤„ç†äººé€ å…‰å­¦å­—ç¬¦çš„éœ€æ±‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨OCRç†è®ºåŠå…¶ä¼˜ç§€æ¨¡å‹GOTï¼Œæ—¨åœ¨æ¨åŠ¨OCR-2.0çš„åˆ°æ¥ã€‚GOTæ¨¡å‹å…·æœ‰580Må‚æ•°ï¼Œæ˜¯ä¸€ä¸ªç»Ÿä¸€ã€ä¼˜é›…çš„ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å„ç§å…‰å­¦å­—ç¬¦ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å…¬å¼ã€è¡¨æ ¼ç­‰ã€‚é€šè¿‡åŠ¨æ€åˆ†è¾¨ç‡å’Œå¤šé¡µOCRæŠ€æœ¯ï¼ŒGOTåœ¨å®éªŒä¸­å±•ç¤ºäº†å…¶å“è¶Šçš„æ€§èƒ½å’Œå®ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02060",
            "title": "OLMoE: Open Mixture-of-Experts Language Models",
            "url": "https://huggingface.co/papers/2409.02060",
            "abstract": "We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.",
            "score": 77,
            "issue_id": 1,
            "pub_date": "2024-09-03",
            "pub_date_card": {
                "ru": "3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 3",
                "zh": "9æœˆ3æ—¥"
            },
            "hash": "fd0ab48efdc585ed",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "OLMoE: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²",
                    "desc": "OLMoE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´ Mixture-of-Experts (MoE). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ½Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ°. OLMoE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ MoE Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² ÑĞ²Ğ¾ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Efficiency with OLMoE: A New Era in Language Models",
                    "desc": "OLMoE is a cutting-edge language model that utilizes a sparse Mixture-of-Experts (MoE) architecture, allowing it to efficiently manage a large number of parameters. With 7 billion parameters, OLMoE-1B-7B only activates 1 billion parameters for each input, optimizing resource use. It has been pretrained on an extensive dataset of 5 trillion tokens and fine-tuned for instruction-based tasks, demonstrating superior performance compared to other models with similar active parameters. The research includes detailed experiments on MoE training and routing analysis, showcasing the model's specialization, and all components of the project are open-sourced for public access."
                },
                "zh": {
                    "title": "OLMoEï¼šé«˜æ•ˆçš„ç¨€ç–ä¸“å®¶æ··åˆè¯­è¨€æ¨¡å‹",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†OLMoEï¼Œè¿™æ˜¯ä¸€ç§å®Œå…¨å¼€æ”¾çš„æœ€å…ˆè¿›è¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨äº†ç¨€ç–çš„ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æŠ€æœ¯ã€‚OLMoE-1B-7Bæ‹¥æœ‰70äº¿ä¸ªå‚æ•°ï¼Œä½†æ¯ä¸ªè¾“å…¥ä»¤ç‰Œä»…ä½¿ç”¨10äº¿ä¸ªå‚æ•°ã€‚æˆ‘ä»¬åœ¨5ä¸‡äº¿ä¸ªä»¤ç‰Œä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¹¶è¿›ä¸€æ­¥è°ƒæ•´ä»¥åˆ›å»ºOLMoE-1B-7B-Instructã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ´»è·ƒå‚æ•°ç›¸ä¼¼çš„æƒ…å†µä¸‹è¶…è¶Šäº†æ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼Œç”šè‡³è¶…è¿‡äº†æ›´å¤§çš„æ¨¡å‹ï¼Œå¦‚Llama2-13B-Chatå’ŒDeepSeekMoE-16Bã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.01437",
            "title": "Kvasir-VQA: A Text-Image Pair GI Tract Dataset",
            "url": "https://huggingface.co/papers/2409.01437",
            "abstract": "We introduce Kvasir-VQA, an extended dataset derived from the HyperKvasir and Kvasir-Instrument datasets, augmented with question-and-answer annotations to facilitate advanced machine learning tasks in Gastrointestinal (GI) diagnostics. This dataset comprises 6,500 annotated images spanning various GI tract conditions and surgical instruments, and it supports multiple question types including yes/no, choice, location, and numerical count. The dataset is intended for applications such as image captioning, Visual Question Answering (VQA), text-based generation of synthetic medical images, object detection, and classification. Our experiments demonstrate the dataset's effectiveness in training models for three selected tasks, showcasing significant applications in medical image analysis and diagnostics. We also present evaluation metrics for each task, highlighting the usability and versatility of our dataset. The dataset and supporting artifacts are available at https://datasets.simula.no/kvasir-vqa.",
            "score": 70,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "a914aa77110d40cd",
            "data": {
                "categories": [
                    "#dataset",
                    "#medicine",
                    "#cv"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ˜Ğ˜-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ–ĞšĞ¢",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Kvasir-VQA Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³Ğ°ÑÑ‚Ñ€Ğ¾ÑĞ½Ñ‚ĞµÑ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 6500 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ–ĞšĞ¢ Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ². ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹, ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼."
                },
                "en": {
                    "title": "Kvasir-VQA: Advancing GI Diagnostics with Visual Question Answering",
                    "desc": "Kvasir-VQA is a new dataset designed to enhance machine learning applications in Gastrointestinal diagnostics. It includes 6,500 annotated images of various GI conditions and surgical tools, with questions that can be answered in different formats like yes/no or numerical counts. This dataset is useful for tasks such as image captioning, Visual Question Answering (VQA), and object detection. Our experiments show that Kvasir-VQA effectively trains models for these tasks, proving its value in medical image analysis."
                },
                "zh": {
                    "title": "Kvasir-VQAï¼šæ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†æçš„æ–°æ•°æ®é›†",
                    "desc": "Kvasir-VQAæ˜¯ä¸€ä¸ªæ‰©å±•çš„æ•°æ®é›†ï¼Œæºè‡ªHyperKvasirå’ŒKvasir-Instrumentæ•°æ®é›†ï¼Œå¢åŠ äº†é—®ç­”æ³¨é‡Šï¼Œä»¥ä¿ƒè¿›èƒƒè‚ é“ï¼ˆGIï¼‰è¯Šæ–­ä¸­çš„é«˜çº§æœºå™¨å­¦ä¹ ä»»åŠ¡ã€‚è¯¥æ•°æ®é›†åŒ…å«6500å¼ æ ‡æ³¨å›¾åƒï¼Œæ¶µç›–å„ç§GIé“ç–¾ç—…å’Œæ‰‹æœ¯å·¥å…·ï¼Œæ”¯æŒå¤šç§é—®é¢˜ç±»å‹ï¼ŒåŒ…æ‹¬æ˜¯/å¦ã€é€‰æ‹©ã€ä½ç½®å’Œæ•°å­—è®¡æ•°ã€‚è¯¥æ•°æ®é›†é€‚ç”¨äºå›¾åƒæè¿°ã€è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ã€åŸºäºæ–‡æœ¬çš„åˆæˆåŒ»å­¦å›¾åƒç”Ÿæˆã€ç‰©ä½“æ£€æµ‹å’Œåˆ†ç±»ç­‰åº”ç”¨ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ•°æ®é›†åœ¨è®­ç»ƒæ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†åœ¨åŒ»å­¦å›¾åƒåˆ†æå’Œè¯Šæ–­ä¸­çš„é‡è¦åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00509",
            "title": "LongRecipe: Recipe for Efficient Long Context Generalization in Large Languge Models",
            "url": "https://huggingface.co/papers/2409.00509",
            "abstract": "Large language models (LLMs) face significant challenges in handling long-context tasks because of their limited effective context window size during pretraining, which restricts their ability to generalize over extended sequences. Meanwhile, extending the context window in LLMs through post-pretraining is highly resource-intensive. To address this, we introduce **LongRecipe**, an efficient training strategy for extending the context window of LLMs, including impactful token analysis, position index transformation, and training optimization strategies. It simulates long-sequence inputs while maintaining training efficiency and significantly improves the model's understanding of long-range dependencies. Experiments on three types of LLMs show that LongRecipe can utilize long sequences while requiring only 30% of the target context window size, and reduces computational training resource over 85% compared to full sequence training. Furthermore, LongRecipe also preserves the original LLM's capabilities in general tasks. Ultimately, *we can extend the effective context window of open-source LLMs from 8k to 128k, achieving performance close to GPT-4 with just one day of dedicated training using a single GPU with 80G memory.* Our code is released at the [link](https://github.com/zhiyuanhubj/LongRecipe).",
            "score": 38,
            "issue_id": 1,
            "pub_date": "2024-08-31",
            "pub_date_card": {
                "ru": "31 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 31",
                "zh": "8æœˆ31æ—¥"
            },
            "hash": "7c96239ce2e612ce",
            "data": {
                "categories": [
                    "#long_context",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° LLM: Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ LongRecipe Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ğ¸ÑˆÑŒ 30% Ğ¾Ñ‚ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ½Ğ° 85% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼. LongRecipe Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½Ğ´ĞµĞºÑĞ¾Ğ² Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM Ñ 8 Ñ‚Ñ‹Ñ. Ğ´Ğ¾ 128 Ñ‚Ñ‹Ñ. Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ´ĞµĞ½ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU."
                },
                "en": {
                    "title": "Unlocking Long Contexts Efficiently with LongRecipe",
                    "desc": "This paper presents LongRecipe, a novel training strategy designed to enhance the context window of large language models (LLMs) without the extensive resource demands typically associated with such tasks. By employing techniques like impactful token analysis and position index transformation, LongRecipe efficiently simulates long-sequence inputs, allowing models to better understand long-range dependencies. The approach enables LLMs to utilize long sequences while only requiring 30% of the target context window size, leading to over 85% reduction in computational resources compared to traditional full sequence training. Ultimately, LongRecipe allows for the effective context window of open-source LLMs to be extended from 8k to 128k, achieving performance levels comparable to GPT-4 with minimal training time on a single GPU."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬åœ¨é¢„è®­ç»ƒæœŸé—´çš„æœ‰æ•ˆä¸Šä¸‹æ–‡çª—å£å¤§å°æœ‰é™ï¼Œé™åˆ¶äº†å¯¹é•¿åºåˆ—çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†**LongRecipe**ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼Œå¯ä»¥æ‰©å±•LLMsçš„ä¸Šä¸‹æ–‡çª—å£ã€‚è¯¥æ–¹æ³•é€šè¿‡å½±å“åŠ›çš„æ ‡è®°åˆ†æã€ä½ç½®ç´¢å¼•è½¬æ¢å’Œè®­ç»ƒä¼˜åŒ–ç­–ç•¥æ¥æ¨¡æ‹Ÿé•¿åºåˆ—è¾“å…¥ï¼ŒåŒæ—¶ä¿æŒè®­ç»ƒæ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒLongRecipeèƒ½å¤Ÿåœ¨ä»…éœ€ç›®æ ‡ä¸Šä¸‹æ–‡çª—å£å¤§å°çš„30%çš„æƒ…å†µä¸‹åˆ©ç”¨é•¿åºåˆ—ï¼Œå¹¶ä¸”ç›¸æ¯”äºå®Œæ•´åºåˆ—è®­ç»ƒï¼Œè®¡ç®—èµ„æºå‡å°‘è¶…è¿‡85%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02095",
            "title": "DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos",
            "url": "https://huggingface.co/papers/2409.02095",
            "abstract": "Despite significant advancements in monocular depth estimation for static images, estimating video depth in the open world remains challenging, since open-world videos are extremely diverse in content, motion, camera movement, and length. We present DepthCrafter, an innovative method for generating temporally consistent long depth sequences with intricate details for open-world videos, without requiring any supplementary information such as camera poses or optical flow. DepthCrafter achieves generalization ability to open-world videos by training a video-to-depth model from a pre-trained image-to-video diffusion model, through our meticulously designed three-stage training strategy with the compiled paired video-depth datasets. Our training approach enables the model to generate depth sequences with variable lengths at one time, up to 110 frames, and harvest both precise depth details and rich content diversity from realistic and synthetic datasets. We also propose an inference strategy that processes extremely long videos through segment-wise estimation and seamless stitching. Comprehensive evaluations on multiple datasets reveal that DepthCrafter achieves state-of-the-art performance in open-world video depth estimation under zero-shot settings. Furthermore, DepthCrafter facilitates various downstream applications, including depth-based visual effects and conditional video generation.",
            "score": 35,
            "issue_id": 1,
            "pub_date": "2024-09-03",
            "pub_date_card": {
                "ru": "3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 3",
                "zh": "9æœˆ3æ—¥"
            },
            "hash": "803f31c2683713de",
            "data": {
                "categories": [
                    "#cv",
                    "#video",
                    "#dataset",
                    "#training",
                    "#inference",
                    "#synthetic"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "DepthCrafter: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "DepthCrafter - ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ°. DepthCrafter ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ´Ğ¾ 110 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ·, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "DepthCrafter: Revolutionizing Depth Estimation in Open-World Videos",
                    "desc": "DepthCrafter is a novel method designed to estimate depth in open-world videos, addressing the challenges posed by diverse content and motion. It utilizes a three-stage training strategy that leverages a pre-trained image-to-video diffusion model, allowing it to generate long depth sequences without needing additional information like camera poses. The model can produce depth sequences of varying lengths, capturing detailed depth information and content diversity from both realistic and synthetic datasets. Additionally, DepthCrafter includes an inference strategy for processing long videos by segmenting them and seamlessly stitching the results together, achieving state-of-the-art performance in zero-shot depth estimation."
                },
                "zh": {
                    "title": "DepthCrafterï¼šå¼€æ”¾ä¸–ç•Œè§†é¢‘æ·±åº¦ä¼°è®¡çš„æ–°çªç ´",
                    "desc": "å°½ç®¡åœ¨é™æ€å›¾åƒçš„å•ç›®æ·±åº¦ä¼°è®¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¼€æ”¾ä¸–ç•Œä¸­ä¼°è®¡è§†é¢‘æ·±åº¦ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†DepthCrafterï¼Œè¿™æ˜¯ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥ä¸ºå¼€æ”¾ä¸–ç•Œè§†é¢‘ç”Ÿæˆæ—¶é—´ä¸€è‡´çš„é•¿æ·±åº¦åºåˆ—ï¼Œä¸”æ— éœ€é¢å¤–çš„ä¿¡æ¯ï¼Œå¦‚ç›¸æœºå§¿æ€æˆ–å…‰æµã€‚DepthCrafteré€šè¿‡ä»é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­è®­ç»ƒè§†é¢‘åˆ°æ·±åº¦æ¨¡å‹ï¼Œå±•ç°äº†å¯¹å¼€æ”¾ä¸–ç•Œè§†é¢‘çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è®­ç»ƒæ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿä¸€æ¬¡ç”Ÿæˆå¯å˜é•¿åº¦çš„æ·±åº¦åºåˆ—ï¼Œå¹¶ä»çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸­æå–ç²¾ç¡®çš„æ·±åº¦ç»†èŠ‚å’Œä¸°å¯Œçš„å†…å®¹å¤šæ ·æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00587",
            "title": "FLUX that Plays Music",
            "url": "https://huggingface.co/papers/2409.00587",
            "abstract": "This paper explores a simple extension of diffusion-based rectified flow Transformers for text-to-music generation, termed as FluxMusic. Generally, along with design in advanced Fluxhttps://github.com/black-forest-labs/flux model, we transfers it into a latent VAE space of mel-spectrum. It involves first applying a sequence of independent attention to the double text-music stream, followed by a stacked single music stream for denoised patch prediction. We employ multiple pre-trained text encoders to sufficiently capture caption semantic information as well as inference flexibility. In between, coarse textual information, in conjunction with time step embeddings, is utilized in a modulation mechanism, while fine-grained textual details are concatenated with the music patch sequence as inputs. Through an in-depth study, we demonstrate that rectified flow training with an optimized architecture significantly outperforms established diffusion methods for the text-to-music task, as evidenced by various automatic metrics and human preference evaluations. Our experimental data, code, and model weights are made publicly available at: https://github.com/feizc/FluxMusic.",
            "score": 31,
            "issue_id": 1,
            "pub_date": "2024-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "8e90043f3c31a7df",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "FluxMusic: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ FluxMusic - Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Transformer-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ VAE-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¼ĞµĞ»-ÑĞ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑÑ‚ĞµĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ."
                },
                "en": {
                    "title": "Transforming Text to Music with FluxMusic: A New Era in Generation!",
                    "desc": "This paper introduces FluxMusic, an innovative approach that enhances diffusion-based rectified flow Transformers for generating music from text. It utilizes a latent Variational Autoencoder (VAE) space focused on the mel-spectrum, allowing for effective representation of audio features. The model employs independent attention mechanisms to process text and music streams, improving the prediction of music patches. The results show that this optimized architecture outperforms traditional diffusion methods in generating music from textual descriptions, supported by both quantitative metrics and qualitative assessments."
                },
                "zh": {
                    "title": "FluxMusicï¼šæ–‡æœ¬åˆ°éŸ³ä¹ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFluxMusicçš„æ‰©å±•æ–¹æ³•ï¼Œç”¨äºåŸºäºæ‰©æ•£çš„æ–‡æœ¬åˆ°éŸ³ä¹ç”Ÿæˆã€‚è¯¥æ–¹æ³•å°†æ¨¡å‹è½¬ç§»åˆ°æ¢…å°”é¢‘è°±çš„æ½œåœ¨å˜åˆ†è‡ªç¼–ç å™¨ç©ºé—´ä¸­ï¼Œé¦–å…ˆå¯¹æ–‡æœ¬å’ŒéŸ³ä¹æµè¿›è¡Œç‹¬ç«‹æ³¨æ„åŠ›å¤„ç†ï¼Œç„¶åè¿›è¡Œå»å™ªéŸ³çš„éŸ³ä¹ç‰‡æ®µé¢„æµ‹ã€‚æˆ‘ä»¬ä½¿ç”¨å¤šä¸ªé¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨æ¥æ•æ‰è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶ç»“åˆæ—¶é—´æ­¥åµŒå…¥è¿›è¡Œè°ƒåˆ¶æœºåˆ¶ã€‚é€šè¿‡æ·±å…¥ç ”ç©¶ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¼˜åŒ–æ¶æ„çš„ä¿®æ­£æµè®­ç»ƒåœ¨æ–‡æœ¬åˆ°éŸ³ä¹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æ‰©æ•£æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02097",
            "title": "LinFusion: 1 GPU, 1 Minute, 16K Image",
            "url": "https://huggingface.co/papers/2409.02097",
            "abstract": "Modern diffusion models, particularly those utilizing a Transformer-based UNet for denoising, rely heavily on self-attention operations to manage complex spatial relationships, thus achieving impressive generation performance. However, this existing paradigm faces significant challenges in generating high-resolution visual content due to its quadratic time and memory complexity with respect to the number of spatial tokens. To address this limitation, we aim at a novel linear attention mechanism as an alternative in this paper. Specifically, we begin our exploration from recently introduced models with linear complexity, e.g., Mamba, Mamba2, and Gated Linear Attention, and identify two key features-attention normalization and non-causal inference-that enhance high-resolution visual generation performance. Building on these insights, we introduce a generalized linear attention paradigm, which serves as a low-rank approximation of a wide spectrum of popular linear token mixers. To save the training cost and better leverage pre-trained models, we initialize our models and distill the knowledge from pre-trained StableDiffusion (SD). We find that the distilled model, termed LinFusion, achieves performance on par with or superior to the original SD after only modest training, while significantly reducing time and memory complexity. Extensive experiments on SD-v1.5, SD-v2.1, and SD-XL demonstrate that LinFusion delivers satisfactory zero-shot cross-resolution generation performance, generating high-resolution images like 16K resolution. Moreover, it is highly compatible with pre-trained SD components, such as ControlNet and IP-Adapter, requiring no adaptation efforts. Codes are available at https://github.com/Huage001/LinFusion.",
            "score": 31,
            "issue_id": 1,
            "pub_date": "2024-09-03",
            "pub_date_card": {
                "ru": "3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 3",
                "zh": "9æœˆ3æ—¥"
            },
            "hash": "704dccb6db5f3e9d",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "LinFusion: Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½-Ğ¼Ğ¸ĞºÑĞµÑ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ğ°Ñ LinFusion, Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ StableDiffusion, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LinFusion ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 16K Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing High-Resolution Image Generation with Linear Attention",
                    "desc": "This paper presents a new approach to improve the performance of diffusion models in generating high-resolution images by introducing a linear attention mechanism. Traditional models struggle with high memory and time complexity due to self-attention operations, especially as the number of spatial tokens increases. The authors explore existing linear complexity models and propose a generalized linear attention paradigm that enhances visual generation while reducing resource demands. Their model, LinFusion, shows competitive performance with pre-trained models like StableDiffusion, achieving impressive results in generating images up to 16K resolution with minimal training effort."
                },
                "zh": {
                    "title": "çº¿æ€§æ³¨æ„åŠ›ï¼Œæå‡é«˜åˆ†è¾¨ç‡ç”Ÿæˆæ€§èƒ½ï¼",
                    "desc": "ç°ä»£æ‰©æ•£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨åŸºäºTransformerçš„UNetè¿›è¡Œå»å™ªçš„æ¨¡å‹ï¼Œä¾èµ–è‡ªæ³¨æ„åŠ›æ“ä½œæ¥å¤„ç†å¤æ‚çš„ç©ºé—´å…³ç³»ï¼Œä»è€Œå®ç°å‡ºè‰²çš„ç”Ÿæˆæ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºå…¶åœ¨ç©ºé—´æ ‡è®°æ•°é‡ä¸Šçš„äºŒæ¬¡æ—¶é—´å’Œå†…å­˜å¤æ‚æ€§ï¼Œç°æœ‰èŒƒå¼åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡è§†è§‰å†…å®¹æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„çº¿æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨æé«˜é«˜åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆæ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥æ³¨æ„åŠ›å½’ä¸€åŒ–å’Œéå› æœæ¨ç†ç­‰å…³é”®ç‰¹å¾ï¼Œå¼€å‘äº†ä¸€ç§é€šç”¨çš„çº¿æ€§æ³¨æ„åŠ›èŒƒå¼ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬å¹¶æé«˜äº†ç”Ÿæˆæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.01071",
            "title": "VideoLLaMB: Long-context Video Understanding with Recurrent Memory Bridges",
            "url": "https://huggingface.co/papers/2409.01071",
            "abstract": "Recent advancements in large-scale video-language models have shown significant potential for real-time planning and detailed interactions. However, their high computational demands and the scarcity of annotated datasets limit their practicality for academic researchers. In this work, we introduce VideoLLaMB, a novel framework that utilizes temporal memory tokens within bridge layers to allow for the encoding of entire video sequences alongside historical visual data, effectively preserving semantic continuity and enhancing model performance across various tasks. This approach includes recurrent memory tokens and a SceneTilling algorithm, which segments videos into independent semantic units to preserve semantic integrity. Empirically, VideoLLaMB significantly outstrips existing video-language models, demonstrating a 5.5 points improvement over its competitors across three VideoQA benchmarks, and 2.06 points on egocentric planning. Comprehensive results on the MVBench show that VideoLLaMB-7B achieves markedly better results than previous 7B models of same LLM. Remarkably, it maintains robust performance as PLLaVA even as video length increases up to 8 times. Besides, the frame retrieval results on our specialized Needle in a Video Haystack (NIAVH) benchmark, further validate VideoLLaMB's prowess in accurately identifying specific frames within lengthy videos. Our SceneTilling algorithm also enables the generation of streaming video captions directly, without necessitating additional training. In terms of efficiency, VideoLLaMB, trained on 16 frames, supports up to 320 frames on a single Nvidia A100 GPU with linear GPU memory scaling, ensuring both high performance and cost-effectiveness, thereby setting a new foundation for long-form video-language models in both academic and practical applications.",
            "score": 26,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "3a4932f3d059c107",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "VideoLLaMB: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "VideoLLaMB - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ·Ñ‹ĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ¾ÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ… Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ†ĞµĞ»Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ SceneTilling Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ‹. VideoLLaMB Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 5.5 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… VideoQA. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾ 320 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU Nvidia A100."
                },
                "en": {
                    "title": "VideoLLaMB: Revolutionizing Video-Language Models with Efficiency and Performance",
                    "desc": "This paper presents VideoLLaMB, a new framework designed to improve the performance of video-language models by using temporal memory tokens and a SceneTilling algorithm. These innovations allow the model to encode entire video sequences while maintaining semantic continuity, which is crucial for tasks like video question answering and planning. VideoLLaMB outperforms existing models, showing significant improvements in benchmarks and maintaining efficiency even with longer videos. The framework is also cost-effective, enabling high performance on a single GPU, making it accessible for both researchers and practical applications."
                },
                "zh": {
                    "title": "VideoLLaMBï¼šé«˜æ•ˆçš„è§†é¢‘è¯­è¨€æ¨¡å‹æ–°åŸºçŸ³",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„è§†é¢‘è¯­è¨€æ¨¡å‹æ¡†æ¶VideoLLaMBï¼Œåˆ©ç”¨æ—¶é—´è®°å¿†æ ‡è®°åœ¨æ¡¥æ¥å±‚ä¸­ç¼–ç æ•´ä¸ªè§†é¢‘åºåˆ—å’Œå†å²è§†è§‰æ•°æ®ï¼Œä»è€Œå¢å¼ºæ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥é€’å½’è®°å¿†æ ‡è®°å’Œåœºæ™¯åˆ‡åˆ†ç®—æ³•ï¼Œå°†è§†é¢‘åˆ†å‰²ä¸ºç‹¬ç«‹çš„è¯­ä¹‰å•å…ƒï¼Œä»¥ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoLLaMBåœ¨ä¸‰ä¸ªè§†é¢‘é—®ç­”åŸºå‡†ä¸Šæ¯”ç°æœ‰æ¨¡å‹æé«˜äº†5.5åˆ†ï¼Œåœ¨è‡ªæˆ‘ä¸­å¿ƒè§„åˆ’ä¸Šæé«˜äº†2.06åˆ†ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚VideoLLaMBåœ¨æ•ˆç‡ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªNvidia A100 GPUä¸Šæ”¯æŒé«˜è¾¾320å¸§çš„å¤„ç†ï¼Œç¡®ä¿é«˜æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00588",
            "title": "Diffusion Policy Policy Optimization",
            "url": "https://huggingface.co/papers/2409.00588",
            "abstract": "We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic framework including best practices for fine-tuning diffusion-based policies (e.g. Diffusion Policy) in continuous control and robot learning tasks using the policy gradient (PG) method from reinforcement learning (RL). PG methods are ubiquitous in training RL policies with other policy parameterizations; nevertheless, they had been conjectured to be less efficient for diffusion-based policies. Surprisingly, we show that DPPO achieves the strongest overall performance and efficiency for fine-tuning in common benchmarks compared to other RL methods for diffusion-based policies and also compared to PG fine-tuning of other policy parameterizations. Through experimental investigation, we find that DPPO takes advantage of unique synergies between RL fine-tuning and the diffusion parameterization, leading to structured and on-manifold exploration, stable training, and strong policy robustness. We further demonstrate the strengths of DPPO in a range of realistic settings, including simulated robotic tasks with pixel observations, and via zero-shot deployment of simulation-trained policies on robot hardware in a long-horizon, multi-stage manipulation task. Website with code: diffusion-ppo.github.io",
            "score": 19,
            "issue_id": 1,
            "pub_date": "2024-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "a92679b4e7f59810",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf",
                    "#robots",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "DPPO: ĞœĞ¾Ñ‰Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DPPO (Diffusion Policy Policy Optimization) - Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DPPO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ DPPO Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Unlocking Efficiency in Robot Learning with DPPO",
                    "desc": "The paper presents Diffusion Policy Policy Optimization (DPPO), a new framework designed to enhance the fine-tuning of diffusion-based policies in continuous control and robot learning tasks. It utilizes the policy gradient method from reinforcement learning, which is commonly used for training various policy types. The authors demonstrate that DPPO outperforms other reinforcement learning methods and traditional policy gradient fine-tuning, achieving superior performance and efficiency on standard benchmarks. Additionally, DPPO benefits from the unique characteristics of diffusion parameterization, enabling effective exploration, stable training, and robust policy performance in complex robotic tasks."
                },
                "zh": {
                    "title": "æ‰©æ•£ç­–ç•¥ä¼˜åŒ–ï¼šå¼ºåŒ–å­¦ä¹ çš„æ–°çªç ´",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºæ‰©æ•£ç­–ç•¥ä¼˜åŒ–ï¼ˆDPPOï¼‰çš„ç®—æ³•æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸­çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•å¯¹åŸºäºæ‰©æ•£çš„ç­–ç•¥è¿›è¡Œå¾®è°ƒã€‚å°½ç®¡ç­–ç•¥æ¢¯åº¦æ–¹æ³•åœ¨è®­ç»ƒå¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†å®ƒä»¬åœ¨æ‰©æ•£ç­–ç•¥ä¸Šçš„æ•ˆç‡æ›¾è¢«è®¤ä¸ºè¾ƒä½ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒDPPOåœ¨å¸¸è§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¾®è°ƒæ–¹é¢çš„å¼ºå¤§æ€§èƒ½å’Œæ•ˆç‡ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°DPPOåˆ©ç”¨äº†å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸æ‰©æ•£å‚æ•°åŒ–ä¹‹é—´çš„ç‹¬ç‰¹ååŒä½œç”¨ï¼Œä»è€Œå®ç°äº†ç»“æ„åŒ–çš„æ¢ç´¢ã€ç¨³å®šçš„è®­ç»ƒå’Œå¼ºå¤§çš„ç­–ç•¥é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00558",
            "title": "Compositional 3D-aware Video Generation with LLM Director",
            "url": "https://huggingface.co/papers/2409.00558",
            "abstract": "Significant progress has been made in text-to-video generation through the use of powerful generative models and large-scale internet data. However, substantial challenges remain in precisely controlling individual concepts within the generated video, such as the motion and appearance of specific characters and the movement of viewpoints. In this work, we propose a novel paradigm that generates each concept in 3D representation separately and then composes them with priors from Large Language Models (LLM) and 2D diffusion models. Specifically, given an input textual prompt, our scheme consists of three stages: 1) We leverage LLM as the director to first decompose the complex query into several sub-prompts that indicate individual concepts within the video~(e.g., scene, objects, motions), then we let LLM to invoke pre-trained expert models to obtain corresponding 3D representations of concepts. 2) To compose these representations, we prompt multi-modal LLM to produce coarse guidance on the scales and coordinates of trajectories for the objects. 3) To make the generated frames adhere to natural image distribution, we further leverage 2D diffusion priors and use Score Distillation Sampling to refine the composition. Extensive experiments demonstrate that our method can generate high-fidelity videos from text with diverse motion and flexible control over each concept. Project page: https://aka.ms/c3v.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2024-08-31",
            "pub_date_card": {
                "ru": "31 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 31",
                "zh": "8æœˆ31æ—¥"
            },
            "hash": "dd559e15ff6e9a56",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "3D-ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM), ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ½Ğ°Ğ´ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Mastering Video Generation: Control Every Concept!",
                    "desc": "This paper presents a new approach to generating videos from text prompts by separately creating 3D representations of individual concepts. It utilizes Large Language Models (LLMs) to break down complex queries into simpler sub-prompts, which helps in controlling specific elements like motion and appearance. The method involves a three-stage process: decomposing the input, guiding the composition of 3D representations, and refining the output using 2D diffusion models. The results show that this approach allows for high-quality video generation with precise control over various aspects of the content."
                },
                "zh": {
                    "title": "ç²¾ç¡®æ§åˆ¶è§†é¢‘ç”Ÿæˆä¸­çš„æ¦‚å¿µ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨æ›´ç²¾ç¡®åœ°æ§åˆ¶ç”Ÿæˆè§†é¢‘ä¸­çš„å„ä¸ªæ¦‚å¿µã€‚æˆ‘ä»¬é¦–å…ˆåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†å¤æ‚çš„æ–‡æœ¬æç¤ºåˆ†è§£ä¸ºå¤šä¸ªå­æç¤ºï¼Œä»¥è·å–æ¯ä¸ªæ¦‚å¿µçš„3Dè¡¨ç¤ºã€‚æ¥ç€ï¼Œé€šè¿‡å¤šæ¨¡æ€LLMç”Ÿæˆå¯¹è±¡çš„è¿åŠ¨è½¨è¿¹çš„ç²—ç•¥æŒ‡å¯¼ï¼Œæœ€åç»“åˆ2Dæ‰©æ•£æ¨¡å‹è¿›è¡Œç»†åŒ–ï¼Œç¡®ä¿ç”Ÿæˆçš„å¸§ç¬¦åˆè‡ªç„¶å›¾åƒåˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä»æ–‡æœ¬ç”Ÿæˆé«˜ä¿çœŸåº¦çš„è§†é¢‘ï¼Œå¹¶å¯¹æ¯ä¸ªæ¦‚å¿µè¿›è¡Œçµæ´»æ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00729",
            "title": "ContextCite: Attributing Model Generation to Context",
            "url": "https://huggingface.co/papers/2409.00729",
            "abstract": "How do language models use information provided as context when generating a response? Can we infer whether a particular generated statement is actually grounded in the context, a misinterpretation, or fabricated? To help answer these questions, we introduce the problem of context attribution: pinpointing the parts of the context (if any) that led a model to generate a particular statement. We then present ContextCite, a simple and scalable method for context attribution that can be applied on top of any existing language model. Finally, we showcase the utility of ContextCite through three applications: (1) helping verify generated statements (2) improving response quality by pruning the context and (3) detecting poisoning attacks. We provide code for ContextCite at https://github.com/MadryLab/context-cite.",
            "score": 13,
            "issue_id": 1,
            "pub_date": "2024-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "69f8f12112b36fb2",
            "data": {
                "categories": [
                    "#interpretability",
                    "#alignment",
                    "#hallucinations"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ContextCite: Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ContextCite Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, ĞºĞ°ĞºĞ¸Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ 'context attribution' - Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ñ‡Ğ°ÑÑ‚ÑĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ContextCite Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ… ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ğ°Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ»Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»Ğ¸ Ğ¾Ğ½Ğ¾ Ğ½ĞµĞ²ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ»Ğ¸ Ğ²Ñ‹Ğ´ÑƒĞ¼ĞºĞ¾Ğ¹."
                },
                "en": {
                    "title": "ContextCite: Uncovering the Roots of Language Model Responses",
                    "desc": "This paper addresses how language models utilize context when generating responses and whether generated statements are based on that context. It introduces the concept of context attribution, which identifies specific parts of the context that influence the model's output. The authors present ContextCite, a method that can be easily integrated with any language model to perform context attribution. They demonstrate its effectiveness in verifying statements, enhancing response quality, and detecting potential poisoning attacks."
                },
                "zh": {
                    "title": "æ­ç¤ºè¯­è¨€æ¨¡å‹ç”ŸæˆèƒŒåçš„ä¸Šä¸‹æ–‡",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå“åº”æ—¶å¦‚ä½•åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡å½’å› çš„é—®é¢˜ï¼Œå³ç¡®å®šå“ªäº›ä¸Šä¸‹æ–‡éƒ¨åˆ†å¯¼è‡´æ¨¡å‹ç”Ÿæˆç‰¹å®šè¯­å¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»‹ç»äº†ContextCiteï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„ä¸Šä¸‹æ–‡å½’å› æ–¹æ³•ï¼Œå¯ä»¥åº”ç”¨äºä»»ä½•ç°æœ‰çš„è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡ä¸‰ä¸ªåº”ç”¨æ¡ˆä¾‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ContextCiteçš„å®ç”¨æ€§ï¼ŒåŒ…æ‹¬éªŒè¯ç”Ÿæˆè¯­å¥ã€æé«˜å“åº”è´¨é‡å’Œæ£€æµ‹æ”»å‡»ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.01199",
            "title": "OD-VAE: An Omni-dimensional Video Compressor for Improving Latent Video Diffusion Model",
            "url": "https://huggingface.co/papers/2409.01199",
            "abstract": "Variational Autoencoder (VAE), compressing videos into latent representations, is a crucial preceding component of Latent Video Diffusion Models (LVDMs). With the same reconstruction quality, the more sufficient the VAE's compression for videos is, the more efficient the LVDMs are. However, most LVDMs utilize 2D image VAE, whose compression for videos is only in the spatial dimension and often ignored in the temporal dimension. How to conduct temporal compression for videos in a VAE to obtain more concise latent representations while promising accurate reconstruction is seldom explored. To fill this gap, we propose an omni-dimension compression VAE, named OD-VAE, which can temporally and spatially compress videos. Although OD-VAE's more sufficient compression brings a great challenge to video reconstruction, it can still achieve high reconstructed accuracy by our fine design. To obtain a better trade-off between video reconstruction quality and compression speed, four variants of OD-VAE are introduced and analyzed. In addition, a novel tail initialization is designed to train OD-VAE more efficiently, and a novel inference strategy is proposed to enable OD-VAE to handle videos of arbitrary length with limited GPU memory. Comprehensive experiments on video reconstruction and LVDM-based video generation demonstrate the effectiveness and efficiency of our proposed methods.",
            "score": 12,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "b85158ae49081549",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#training",
                    "#inference"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "OD-VAE: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OD-VAE - Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² OD-VAE Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing Video Compression with OD-VAE",
                    "desc": "This paper introduces the Omni-Dimension Variational Autoencoder (OD-VAE), which enhances video compression by addressing both spatial and temporal dimensions. Traditional VAEs typically focus on spatial compression, neglecting the temporal aspect, which limits their efficiency in video applications. OD-VAE achieves a more concise latent representation while maintaining high reconstruction accuracy through innovative design strategies. The authors also present four variants of OD-VAE and novel techniques for efficient training and inference, demonstrating significant improvements in video reconstruction and generation tasks."
                },
                "zh": {
                    "title": "å…¨ç»´å‹ç¼©ï¼Œæå‡è§†é¢‘é‡å»ºæ•ˆç‡",
                    "desc": "å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰åœ¨å°†è§†é¢‘å‹ç¼©ä¸ºæ½œåœ¨è¡¨ç¤ºæ–¹é¢èµ·ç€é‡è¦ä½œç”¨ï¼Œæ˜¯æ½œåœ¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆLVDMï¼‰çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨ç»´å‹ç¼©çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆOD-VAEï¼‰ï¼Œèƒ½å¤ŸåŒæ—¶è¿›è¡Œæ—¶é—´å’Œç©ºé—´ä¸Šçš„è§†é¢‘å‹ç¼©ï¼Œä»è€Œè·å¾—æ›´ç®€æ´çš„æ½œåœ¨è¡¨ç¤ºã€‚å°½ç®¡OD-VAEçš„å‹ç¼©å¸¦æ¥äº†è§†é¢‘é‡å»ºçš„æŒ‘æˆ˜ï¼Œä½†é€šè¿‡ç²¾å¿ƒè®¾è®¡ï¼Œå®ƒä»èƒ½å®ç°é«˜é‡å»ºç²¾åº¦ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å››ç§OD-VAEçš„å˜ä½“ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„å°¾éƒ¨åˆå§‹åŒ–æ–¹æ³•ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00492",
            "title": "Accurate Compression of Text-to-Image Diffusion Models via Vector Quantization",
            "url": "https://huggingface.co/papers/2409.00492",
            "abstract": "Text-to-image diffusion models have emerged as a powerful framework for high-quality image generation given textual prompts. Their success has driven the rapid development of production-grade diffusion models that consistently increase in size and already contain billions of parameters. As a result, state-of-the-art text-to-image models are becoming less accessible in practice, especially in resource-limited environments. Post-training quantization (PTQ) tackles this issue by compressing the pretrained model weights into lower-bit representations. Recent diffusion quantization techniques primarily rely on uniform scalar quantization, providing decent performance for the models compressed to 4 bits. This work demonstrates that more versatile vector quantization (VQ) may achieve higher compression rates for large-scale text-to-image diffusion models. Specifically, we tailor vector-based PTQ methods to recent billion-scale text-to-image models (SDXL and SDXL-Turbo), and show that the diffusion models of 2B+ parameters compressed to around 3 bits using VQ exhibit the similar image quality and textual alignment as previous 4-bit compression techniques.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-08-31",
            "pub_date_card": {
                "ru": "31 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 31",
                "zh": "8æœˆ31æ—¥"
            },
            "hash": "caecd9179e740837",
            "data": {
                "categories": [
                    "#inference",
                    "#diffusion"
                ],
                "emoji": "ğŸ—œï¸",
                "ru": {
                    "title": "Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚ÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (VQ) Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ SDXL Ğ¸ SDXL-Turbo, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ĞµĞµ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ¾ 3 Ğ±Ğ¸Ñ‚ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ VQ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² 4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Accessibility of Text-to-Image Models through Vector Quantization",
                    "desc": "This paper discusses the advancements in text-to-image diffusion models, which are used for generating high-quality images from text prompts. It highlights the challenge of accessibility due to the increasing size of these models, which can have billions of parameters. To address this, the authors propose post-training quantization (PTQ) as a method to compress model weights into lower-bit formats. They specifically focus on vector quantization (VQ) techniques, demonstrating that they can achieve better compression rates while maintaining image quality and alignment with text prompts compared to traditional 4-bit methods."
                },
                "zh": {
                    "title": "å‘é‡é‡åŒ–æå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å‹ç¼©æ•ˆç‡",
                    "desc": "æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§å¼ºå¤§çš„å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œå¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚éšç€æ¨¡å‹è§„æ¨¡çš„è¿…é€Ÿæ‰©å¤§ï¼Œç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹å·²ç»åŒ…å«æ•°åäº¿ä¸ªå‚æ•°ï¼Œè¿™ä½¿å¾—åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ä½¿ç”¨è¿™äº›æ¨¡å‹å˜å¾—æ›´åŠ å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰é€šè¿‡å°†é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡å‹ç¼©ä¸ºä½ä½è¡¨ç¤ºæ¥é™ä½æ¨¡å‹çš„å¤æ‚æ€§ã€‚æœ¬æ–‡å±•ç¤ºäº†å‘é‡é‡åŒ–ï¼ˆVQï¼‰åœ¨å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œè¯æ˜äº†ä½¿ç”¨VQè¿›è¡Œå‹ç¼©å¯ä»¥åœ¨ä¿æŒå›¾åƒè´¨é‡å’Œæ–‡æœ¬å¯¹é½çš„åŒæ—¶ï¼Œè¾¾åˆ°æ›´é«˜çš„å‹ç¼©ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.01392",
            "title": "GenAgent: Build Collaborative AI Systems with Automated Workflow Generation -- Case Studies on ComfyUI",
            "url": "https://huggingface.co/papers/2409.01392",
            "abstract": "Much previous AI research has focused on developing monolithic models to maximize their intelligence and capability, with the primary goal of enhancing performance on specific tasks. In contrast, this paper explores an alternative approach: collaborative AI systems that use workflows to integrate models, data sources, and pipelines to solve complex and diverse tasks. We introduce GenAgent, an LLM-based framework that automatically generates complex workflows, offering greater flexibility and scalability compared to monolithic models. The core innovation of GenAgent lies in representing workflows with code, alongside constructing workflows with collaborative agents in a step-by-step manner. We implement GenAgent on the ComfyUI platform and propose a new benchmark, OpenComfy. The results demonstrate that GenAgent outperforms baseline approaches in both run-level and task-level evaluations, showing its capability to generate complex workflows with superior effectiveness and stability.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "ccb90d08b0c22618",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GenAgent - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ² Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, GenAgent Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ², Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ² Ğ²Ğ¸Ğ´Ğµ ĞºĞ¾Ğ´Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ ComfyUI Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OpenComfy. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GenAgent Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Empowering AI Collaboration with GenAgent Workflows",
                    "desc": "This paper presents GenAgent, a framework that utilizes large language models (LLMs) to create collaborative AI systems. Unlike traditional monolithic models that focus on single tasks, GenAgent generates complex workflows that integrate various models and data sources. The innovative aspect of GenAgent is its ability to represent workflows as code, allowing for step-by-step construction with collaborative agents. The implementation on the ComfyUI platform and the introduction of the OpenComfy benchmark show that GenAgent significantly outperforms existing methods in generating effective and stable workflows."
                },
                "zh": {
                    "title": "åä½œAIï¼šçµæ´»é«˜æ•ˆçš„å·¥ä½œæµç”Ÿæˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§æ–°çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œç§°ä¸ºåä½œAIç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡å·¥ä½œæµæ•´åˆæ¨¡å‹ã€æ•°æ®æºå’Œç®¡é“æ¥è§£å†³å¤æ‚ä»»åŠ¡ã€‚æˆ‘ä»¬ä»‹ç»äº†GenAgentï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå¤æ‚çš„å·¥ä½œæµï¼Œæä¾›æ¯”å•ä¸€æ¨¡å‹æ›´å¤§çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ã€‚GenAgentçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºç”¨ä»£ç è¡¨ç¤ºå·¥ä½œæµï¼Œå¹¶é€šè¿‡åä½œä»£ç†é€æ­¥æ„å»ºå·¥ä½œæµã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGenAgentåœ¨è¿è¡Œçº§åˆ«å’Œä»»åŠ¡çº§åˆ«çš„è¯„ä¼°ä¸­å‡ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶ç”Ÿæˆå¤æ‚å·¥ä½œæµçš„å“è¶Šæ•ˆæœå’Œç¨³å®šæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.01055",
            "title": "Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation",
            "url": "https://huggingface.co/papers/2409.01055",
            "abstract": "This paper explores higher-resolution video outpainting with extensive content generation. We point out common issues faced by existing methods when attempting to largely outpaint videos: the generation of low-quality content and limitations imposed by GPU memory. To address these challenges, we propose a diffusion-based method called Follow-Your-Canvas. It builds upon two core designs. First, instead of employing the common practice of \"single-shot\" outpainting, we distribute the task across spatial windows and seamlessly merge them. It allows us to outpaint videos of any size and resolution without being constrained by GPU memory. Second, the source video and its relative positional relation are injected into the generation process of each window. It makes the generated spatial layout within each window harmonize with the source video. Coupling with these two designs enables us to generate higher-resolution outpainting videos with rich content while keeping spatial and temporal consistency. Follow-Your-Canvas excels in large-scale video outpainting, e.g., from 512X512 to 1152X2048 (9X), while producing high-quality and aesthetically pleasing results. It achieves the best quantitative results across various resolution and scale setups. The code is released on https://github.com/mayuelala/FollowYourCanvas",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "f683bbfc6edc815b",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†: Follow-Your-Canvas Ğ¿Ğ¾ĞºĞ¾Ñ€ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Follow-Your-Canvas. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU, Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹. Follow-Your-Canvas Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾ĞºĞ½Ğ°Ğ¼ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Revolutionizing Video Outpainting with Follow-Your-Canvas",
                    "desc": "This paper presents a novel approach for higher-resolution video outpainting using a method called Follow-Your-Canvas. It addresses common challenges in video outpainting, such as low-quality content generation and GPU memory limitations, by distributing the outpainting task across spatial windows. The method incorporates the source video's positional information to ensure that the generated content aligns well with the original video, maintaining both spatial and temporal consistency. As a result, Follow-Your-Canvas can effectively generate high-quality, large-scale outpainted videos, significantly improving upon existing techniques."
                },
                "zh": {
                    "title": "é«˜åˆ†è¾¨ç‡è§†é¢‘å¤–å»¶çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†é«˜åˆ†è¾¨ç‡è§†é¢‘çš„å¤–å»¶ç”Ÿæˆï¼Œæå‡ºäº†ä¸€ç§åä¸ºFollow-Your-Canvasçš„æ‰©æ•£æ–¹æ³•ã€‚æˆ‘ä»¬æŒ‡å‡ºç°æœ‰æ–¹æ³•åœ¨å¤§è§„æ¨¡è§†é¢‘å¤–å»¶æ—¶å¸¸é‡åˆ°çš„ä½è´¨é‡å†…å®¹ç”Ÿæˆå’ŒGPUå†…å­˜é™åˆ¶ç­‰é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ä»»åŠ¡åˆ†å¸ƒåˆ°ç©ºé—´çª—å£å¹¶æ— ç¼åˆå¹¶ï¼Œè§£å†³äº†å†…å­˜é™åˆ¶ï¼Œæ”¯æŒä»»æ„å¤§å°å’Œåˆ†è¾¨ç‡çš„è§†é¢‘å¤–å»¶ã€‚åŒæ—¶ï¼Œæºè§†é¢‘åŠå…¶ç›¸å¯¹ä½ç½®å…³ç³»è¢«æ³¨å…¥åˆ°æ¯ä¸ªçª—å£çš„ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œç¡®ä¿ç”Ÿæˆå†…å®¹ä¸æºè§†é¢‘çš„ç©ºé—´å¸ƒå±€å’Œè°ä¸€è‡´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00391",
            "title": "Density Adaptive Attention-based Speech Network: Enhancing Feature Understanding for Mental Health Disorders",
            "url": "https://huggingface.co/papers/2409.00391",
            "abstract": "Speech-based depression detection poses significant challenges for automated detection due to its unique manifestation across individuals and data scarcity. Addressing these challenges, we introduce DAAMAudioCNNLSTM and DAAMAudioTransformer, two parameter efficient and explainable models for audio feature extraction and depression detection. DAAMAudioCNNLSTM features a novel CNN-LSTM framework with multi-head Density Adaptive Attention Mechanism (DAAM), focusing dynamically on informative speech segments. DAAMAudioTransformer, leveraging a transformer encoder in place of the CNN-LSTM architecture, incorporates the same DAAM module for enhanced attention and interpretability. These approaches not only enhance detection robustness and interpretability but also achieve state-of-the-art performance: DAAMAudioCNNLSTM with an F1 macro score of 0.702 and DAAMAudioTransformer with an F1 macro score of 0.72 on the DAIC-WOZ dataset, without reliance on supplementary information such as vowel positions and speaker information during training/validation as in previous approaches. Both models' significant explainability and efficiency in leveraging speech signals for depression detection represent a leap towards more reliable, clinically useful diagnostic tools, promising advancements in speech and mental health care. To foster further research in this domain, we make our code publicly available.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2024-08-31",
            "pub_date_card": {
                "ru": "31 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 31",
                "zh": "8æœˆ31æ—¥"
            },
            "hash": "76a69d5f1be00050",
            "data": {
                "categories": [
                    "#audio",
                    "#interpretability",
                    "#medicine"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "Ğ˜Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¿Ñ€ĞµÑÑĞ¸Ğ¸ Ğ¿Ğ¾ Ğ³Ğ¾Ğ»Ğ¾ÑÑƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¿Ñ€ĞµÑÑĞ¸Ğ¸ Ğ¿Ğ¾ Ñ€ĞµÑ‡Ğ¸: DAAMAudioCNNLSTM Ğ¸ DAAMAudioTransformer. ĞĞ±Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğº Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ (DAAM) Ğ´Ğ»Ñ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ñ€ĞµÑ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ DAIC-WOZ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ F1-macro score 0.702 Ğ¸ 0.72 ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ´ĞµĞ¿Ñ€ĞµÑÑĞ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Depression Detection with Speech Analysis",
                    "desc": "This paper presents two innovative models, DAAMAudioCNNLSTM and DAAMAudioTransformer, designed for detecting depression through speech analysis. Both models utilize a Density Adaptive Attention Mechanism (DAAM) to focus on the most relevant parts of speech data, improving the accuracy of detection. The DAAMAudioCNNLSTM combines Convolutional Neural Networks (CNN) with Long Short-Term Memory (LSTM) networks, while the DAAMAudioTransformer employs a transformer architecture for enhanced interpretability. Achieving state-of-the-art F1 macro scores on the DAIC-WOZ dataset, these models demonstrate significant advancements in automated depression detection without needing additional speaker information."
                },
                "zh": {
                    "title": "åŸºäºè¯­éŸ³çš„æŠ‘éƒæ£€æµ‹æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸¤ç§æ–°çš„æ¨¡å‹ï¼ŒDAAMAudioCNNLSTMå’ŒDAAMAudioTransformerï¼Œç”¨äºåŸºäºè¯­éŸ³çš„æŠ‘éƒæ£€æµ‹ã€‚è¿™äº›æ¨¡å‹é€šè¿‡å¼•å…¥å¤šå¤´å¯†åº¦è‡ªé€‚åº”æ³¨æ„æœºåˆ¶ï¼ˆDAAMï¼‰ï¼Œæœ‰æ•ˆæå–éŸ³é¢‘ç‰¹å¾å¹¶åŠ¨æ€å…³æ³¨é‡è¦çš„è¯­éŸ³ç‰‡æ®µã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼Œè¿™äº›æ¨¡å‹åœ¨è®­ç»ƒå’ŒéªŒè¯è¿‡ç¨‹ä¸­ä¸ä¾èµ–äºé¢å¤–çš„ä¿¡æ¯ï¼Œå¦‚å…ƒéŸ³ä½ç½®å’Œè¯´è¯è€…ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ¨¡å‹åœ¨DAIC-WOZæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†åœ¨æŠ‘éƒæ£€æµ‹é¢†åŸŸçš„æ˜¾è‘—è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.01357",
            "title": "Know When to Fuse: Investigating Non-English Hybrid Retrieval in the Legal Domain",
            "url": "https://huggingface.co/papers/2409.01357",
            "abstract": "Hybrid search has emerged as an effective strategy to offset the limitations of different matching paradigms, especially in out-of-domain contexts where notable improvements in retrieval quality have been observed. However, existing research predominantly focuses on a limited set of retrieval methods, evaluated in pairs on domain-general datasets exclusively in English. In this work, we study the efficacy of hybrid search across a variety of prominent retrieval models within the unexplored field of law in the French language, assessing both zero-shot and in-domain scenarios. Our findings reveal that in a zero-shot context, fusing different domain-general models consistently enhances performance compared to using a standalone model, regardless of the fusion method. Surprisingly, when models are trained in-domain, we find that fusion generally diminishes performance relative to using the best single system, unless fusing scores with carefully tuned weights. These novel insights, among others, expand the applicability of prior findings across a new field and language, and contribute to a deeper understanding of hybrid search in non-English specialized domains.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "8cc67a6869d64f74",
            "data": {
                "categories": [
                    "#rag",
                    "#multilingual"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ² ÑÑ€Ğ¸ÑĞ¿Ñ€ÑƒĞ´ĞµĞ½Ñ†Ğ¸Ğ¸: Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ° Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… zero-shot Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ² zero-shot Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, ĞµÑĞ»Ğ¸ Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ°."
                },
                "en": {
                    "title": "Enhancing Legal Search: The Power of Hybrid Models in French",
                    "desc": "This paper explores hybrid search, which combines different retrieval methods to improve search results, particularly in the legal domain using the French language. The authors find that using a mix of models in a zero-shot scenario leads to better performance than any single model. However, when models are trained specifically for the legal domain, combining them often results in worse performance unless the fusion weights are finely adjusted. These results highlight the complexities of hybrid search in specialized fields and suggest that previous findings may not directly apply to non-English contexts."
                },
                "zh": {
                    "title": "æ··åˆæœç´¢ï¼šæå‡æ£€ç´¢è´¨é‡çš„æ–°ç­–ç•¥",
                    "desc": "æ··åˆæœç´¢æ˜¯ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ï¼Œå¯ä»¥å¼¥è¡¥ä¸åŒåŒ¹é…èŒƒå¼çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢†åŸŸå¤–çš„æƒ…å†µä¸‹ï¼Œæ£€ç´¢è´¨é‡æ˜¾è‘—æé«˜ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æœ‰é™çš„æ£€ç´¢æ–¹æ³•ä¸Šï¼Œä¸”ä»…åœ¨è‹±è¯­çš„é¢†åŸŸé€šç”¨æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚æœ¬æ–‡ç ”ç©¶äº†æ··åˆæœç´¢åœ¨æ³•è¯­æ³•å¾‹é¢†åŸŸçš„æœ‰æ•ˆæ€§ï¼Œè¯„ä¼°äº†é›¶æ ·æœ¬å’Œé¢†åŸŸå†…åœºæ™¯ã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œåœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹ï¼Œèåˆä¸åŒçš„é¢†åŸŸé€šç”¨æ¨¡å‹å§‹ç»ˆèƒ½æé«˜æ€§èƒ½ï¼Œè€Œåœ¨é¢†åŸŸå†…è®­ç»ƒçš„æ¨¡å‹ä¸­ï¼Œé™¤éä½¿ç”¨ç²¾å¿ƒè°ƒæ•´çš„æƒé‡è¿›è¡Œèåˆï¼Œå¦åˆ™èåˆé€šå¸¸ä¼šé™ä½æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00447",
            "title": "The MERIT Dataset: Modelling and Efficiently Rendering Interpretable Transcripts",
            "url": "https://huggingface.co/papers/2409.00447",
            "abstract": "This paper introduces the MERIT Dataset, a multimodal (text + image + layout) fully labeled dataset within the context of school reports. Comprising over 400 labels and 33k samples, the MERIT Dataset is a valuable resource for training models in demanding Visually-rich Document Understanding (VrDU) tasks. By its nature (student grade reports), the MERIT Dataset can potentially include biases in a controlled way, making it a valuable tool to benchmark biases induced in Language Models (LLMs). The paper outlines the dataset's generation pipeline and highlights its main features in the textual, visual, layout, and bias domains. To demonstrate the dataset's utility, we present a benchmark with token classification models, showing that the dataset poses a significant challenge even for SOTA models and that these would greatly benefit from including samples from the MERIT Dataset in their pretraining phase.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2024-08-31",
            "pub_date_card": {
                "ru": "31 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 31",
                "zh": "8æœˆ31æ—¥"
            },
            "hash": "7d3ef5f0783e4fb6",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "MERIT: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MERIT - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 400 Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¸ 33 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² ÑˆĞºĞ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ. MERIT Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°Ğº ĞºĞ°Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ²Ğ¸Ğ´Ğµ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking Visually-rich Document Understanding with MERIT",
                    "desc": "The MERIT Dataset is a new resource designed for training machine learning models on school reports, combining text, images, and layout information. It contains over 33,000 samples and 400 labels, making it suitable for complex tasks in Visually-rich Document Understanding (VrDU). The dataset also allows researchers to study biases in Language Models (LLMs) due to its specific context. The paper demonstrates that even state-of-the-art models struggle with this dataset, indicating its potential to improve model performance when included in pretraining."
                },
                "zh": {
                    "title": "MERITæ•°æ®é›†ï¼šæ¨åŠ¨è§†è§‰æ–‡æ¡£ç†è§£çš„åˆ©å™¨",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†MERITæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«æ–‡æœ¬ã€å›¾åƒå’Œå¸ƒå±€çš„å¤šæ¨¡æ€å®Œå…¨æ ‡æ³¨æ•°æ®é›†ï¼Œä¸“æ³¨äºå­¦æ ¡æŠ¥å‘Šã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡400ä¸ªæ ‡ç­¾å’Œ33,000ä¸ªæ ·æœ¬ï¼Œæ˜¯è®­ç»ƒè§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£ï¼ˆVrDUï¼‰ä»»åŠ¡æ¨¡å‹çš„é‡è¦èµ„æºã€‚ç”±äºå…¶ç‰¹æ€§ï¼ˆå­¦ç”Ÿæˆç»©æŠ¥å‘Šï¼‰ï¼ŒMERITæ•°æ®é›†å¯èƒ½ä»¥å—æ§æ–¹å¼åŒ…å«åè§ï¼Œæˆä¸ºè¯„ä¼°è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åè§çš„é‡è¦å·¥å…·ã€‚æ–‡ç« è¿˜æè¿°äº†æ•°æ®é›†çš„ç”Ÿæˆæµç¨‹ï¼Œå¹¶å¼ºè°ƒäº†å…¶åœ¨æ–‡æœ¬ã€è§†è§‰ã€å¸ƒå±€å’Œåè§é¢†åŸŸçš„ä¸»è¦ç‰¹å¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00138",
            "title": "PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action",
            "url": "https://huggingface.co/papers/2409.00138",
            "abstract": "As language models (LMs) are widely utilized in personalized communication scenarios (e.g., sending emails, writing social media posts) and endowed with a certain level of agency, ensuring they act in accordance with the contextual privacy norms becomes increasingly critical. However, quantifying the privacy norm awareness of LMs and the emerging privacy risk in LM-mediated communication is challenging due to (1) the contextual and long-tailed nature of privacy-sensitive cases, and (2) the lack of evaluation approaches that capture realistic application scenarios. To address these challenges, we propose PrivacyLens, a novel framework designed to extend privacy-sensitive seeds into expressive vignettes and further into agent trajectories, enabling multi-level evaluation of privacy leakage in LM agents' actions. We instantiate PrivacyLens with a collection of privacy norms grounded in privacy literature and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM performance in answering probing questions and their actual behavior when executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4 and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even when prompted with privacy-enhancing instructions. We also demonstrate the dynamic nature of PrivacyLens by extending each seed into multiple trajectories to red-team LM privacy leakage risk. Dataset and code are available at https://github.com/SALT-NLP/PrivacyLens.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2024-08-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 29",
                "zh": "8æœˆ29æ—¥"
            },
            "hash": "b5d2f82929b24c1c",
            "data": {
                "categories": [
                    "#dataset",
                    "#agents",
                    "#ethics"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "PrivacyLens: Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ PrivacyLens - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ¸ÑĞºĞ°Ñ… ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸ÑÑ… Ñ Ğ¸Ñ… ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4 Ğ¸ Llama-3-70B, Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°ÑÑ‚ ÑƒÑ‚ĞµÑ‡ĞºÑƒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² 25-39% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸. PrivacyLens Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑƒÑ‚ĞµÑ‡ĞºĞ°Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Privacy Awareness in Language Models with PrivacyLens",
                    "desc": "This paper introduces PrivacyLens, a framework aimed at evaluating the privacy awareness of language models (LMs) in communication tasks. It addresses the challenges of quantifying privacy norms due to the complex and varied nature of privacy-sensitive situations. The framework allows for a multi-level assessment of privacy leakage by transforming privacy-sensitive seeds into detailed scenarios and agent actions. The study reveals significant privacy risks, showing that advanced LMs like GPT-4 and Llama-3-70B can leak sensitive information in a substantial percentage of cases, even when given privacy-focused instructions."
                },
                "zh": {
                    "title": "ç¡®ä¿è¯­è¨€æ¨¡å‹éµå¾ªéšç§è§„èŒƒçš„å…³é”®",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPrivacyLensçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ä¸ªæ€§åŒ–æ²Ÿé€šä¸­å¯¹éšç§è§„èŒƒçš„éµå¾ªæƒ…å†µã€‚è¯¥æ¡†æ¶é€šè¿‡å°†éšç§æ•æ„Ÿçš„ç§å­æ‰©å±•ä¸ºç”ŸåŠ¨çš„åœºæ™¯å’Œä»£ç†è½¨è¿¹ï¼Œå®ç°å¯¹éšç§æ³„éœ²çš„å¤šå±‚æ¬¡è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡å…ˆè¿›çš„è¯­è¨€æ¨¡å‹åœ¨å›ç­”éšç§ç›¸å…³é—®é¢˜æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å®é™…æ‰§è¡Œç”¨æˆ·æŒ‡ä»¤æ—¶ï¼Œä»æœ‰ç›¸å½“æ¯”ä¾‹çš„æ¡ˆä¾‹æ³„éœ²æ•æ„Ÿä¿¡æ¯ã€‚é€šè¿‡åŠ¨æ€æ‰©å±•ç§å­ï¼ŒPrivacyLensèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å’Œè¯„ä¼°è¯­è¨€æ¨¡å‹çš„éšç§æ³„éœ²é£é™©ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-09-03.html",
    "link_next": "2024-09-05.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "03.09",
        "en": "09/03",
        "zh": "9æœˆ3æ—¥"
    },
    "short_date_next": {
        "ru": "05.09",
        "en": "09/05",
        "zh": "9æœˆ5æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 4,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 3,
        "#3d": 1,
        "#audio": 1,
        "#video": 5,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 5,
        "#medicine": 2,
        "#training": 8,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 6,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 1,
        "#translation": 0,
        "#robots": 0
    }
}