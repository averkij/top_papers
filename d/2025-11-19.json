{
    "date": {
        "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 19",
        "zh": "11æœˆ19æ—¥"
    },
    "time_utc": "2025-11-19 09:00",
    "weekday": 2,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2025-11-19",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2511.11113",
            "title": "VIDEOP2R: Video Understanding from Perception to Reasoning",
            "url": "https://huggingface.co/papers/2511.11113",
            "abstract": "VideoP2R, a process-aware reinforcement fine-tuning framework, improves video reasoning and understanding by modeling perception and reasoning separately, achieving state-of-the-art results on multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement fine-tuning (RFT), a two-stage framework consisting of supervised fine-tuning (SFT) and reinforcement learning (RL) has shown promising results on improving reasoning ability of large language models (LLMs). Yet extending RFT to large video language models (LVLMs) remains challenging. We propose VideoP2R, a novel process-aware video RFT framework that enhances video reasoning by modeling perception and reasoning as distinct processes. In the SFT stage, we develop a three-step pipeline to generate VideoP2R-CoT-162K, a high-quality, process-aware chain-of-thought (CoT) dataset for perception and reasoning. In the RL stage, we introduce a novel process-aware group relative policy optimization (PA-GRPO) algorithm that supplies separate rewards for perception and reasoning. Extensive experiments show that VideoP2R achieves state-of-the-art (SotA) performance on six out of seven video reasoning and understanding benchmarks. Ablation studies further confirm the effectiveness of our process-aware modeling and PA-GRPO and demonstrate that model's perception output is information-sufficient for downstream reasoning.",
            "score": 111,
            "issue_id": 1,
            "pub_date": "2025-11-14",
            "pub_date_card": {
                "ru": "14 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 14",
                "zh": "11æœˆ14æ—¥"
            },
            "hash": "c356d5771463f428",
            "authors": [
                "Yifan Jiang",
                "Yueying Wang",
                "Rui Zhao",
                "Toufiq Parag",
                "Zhimin Chen",
                "Zhenyu Liao",
                "Jayakrishnan Unnikrishnan"
            ],
            "affiliations": [
                "Amazon",
                "USC"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.11113.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#rl",
                    "#dataset",
                    "#rlhf",
                    "#reasoning",
                    "#synthetic"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "VideoP2R â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ğ¾ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¿Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing Video Reasoning with Process-Aware Fine-Tuning",
                    "desc": "VideoP2R is a new framework designed to enhance video reasoning and understanding by treating perception and reasoning as separate processes. It utilizes a two-stage approach called reinforcement fine-tuning (RFT), which includes supervised fine-tuning (SFT) followed by reinforcement learning (RL). In the SFT stage, a specialized dataset called VideoP2R-CoT-162K is created to improve the model's ability to think through video content. The RL stage employs a unique algorithm, PA-GRPO, that provides distinct rewards for perception and reasoning, leading to state-of-the-art performance on various benchmarks."
                },
                "zh": {
                    "title": "è§†é¢‘æ¨ç†çš„æ–°çªç ´ï¼šVideoP2Ræ¡†æ¶",
                    "desc": "VideoP2Ræ˜¯ä¸€ä¸ªè¿‡ç¨‹æ„ŸçŸ¥çš„å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†æ„ŸçŸ¥å’Œæ¨ç†å»ºæ¨¡ä¸ºç‹¬ç«‹çš„è¿‡ç¨‹æ¥æé«˜è§†é¢‘æ¨ç†å’Œç†è§£èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œå¹¶åœ¨SFTé˜¶æ®µç”Ÿæˆé«˜è´¨é‡çš„è¿‡ç¨‹æ„ŸçŸ¥é“¾å¼æ€ç»´æ•°æ®é›†ã€‚RLé˜¶æ®µå¼•å…¥äº†ä¸€ç§æ–°çš„è¿‡ç¨‹æ„ŸçŸ¥ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œä¸ºæ„ŸçŸ¥å’Œæ¨ç†æä¾›ç‹¬ç«‹çš„å¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoP2Råœ¨å¤šä¸ªè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.08577",
            "title": "Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models",
            "url": "https://huggingface.co/papers/2511.08577",
            "abstract": "Think-at-Hard (TaH) dynamically refines only hard tokens in LLMs using a neural decider and LoRA, improving reasoning performance with minimal additional parameters or iterations.  \t\t\t\t\tAI-generated summary \t\t\t\t Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.",
            "score": 104,
            "issue_id": 1,
            "pub_date": "2025-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "b4583d31245848fa",
            "authors": [
                "Tianyu Fu",
                "Yichen You",
                "Zekai Chen",
                "Guohao Dai",
                "Huazhong Yang",
                "Yu Wang"
            ],
            "affiliations": [
                "Infinigence AI",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.08577.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#open_source",
                    "#reasoning",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Think-at-Hard Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Â«Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸ÑÂ» - ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸ÑÑ…. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¸Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğº Ğ½Ğ¸Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 8-12% Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ 94% Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ÑÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Refining Hard Tokens for Smarter LLMs",
                    "desc": "The paper introduces Think-at-Hard (TaH), a method that enhances the reasoning abilities of Large Language Models (LLMs) by focusing on difficult tokens during the prediction process. Instead of refining all tokens, TaH uses a neural decider to identify and iterate only on those tokens that are likely to be incorrect after the initial pass. This approach minimizes unnecessary revisions on easy tokens, which can lead to errors, and employs Low-Rank Adaptation (LoRA) to adjust the model's focus during these iterations. The results demonstrate significant accuracy improvements on various benchmarks while keeping the number of parameters constant, showcasing TaH's efficiency in enhancing LLM performance."
                },
                "zh": {
                    "title": "åŠ¨æ€ä¼˜åŒ–éš¾è¯å…ƒï¼Œæé«˜æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºThink-at-Hard (TaH) çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸€ä¸ªè½»é‡çº§çš„ç¥ç»å†³ç­–å™¨ï¼Œä»…å¯¹éš¾ä»¥é¢„æµ‹çš„è¯å…ƒè¿›è¡ŒåŠ¨æ€è¿­ä»£ï¼Œä»è€Œé¿å…äº†å¯¹ç®€å•è¯å…ƒçš„è¿‡åº¦æ€è€ƒã€‚ä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ¨¡å—ï¼ŒTaHå°†æ¨¡å‹çš„ç›®æ ‡ä»ä¸€èˆ¬çš„ä¸‹ä¸€ä¸ªè¯é¢„æµ‹è½¬å‘ä¸“æ³¨äºéš¾è¯å…ƒçš„ç²¾ç»†åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTaHåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨ç†æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å‚æ•°æ•°é‡ä¸å˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.14295",
            "title": "AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models",
            "url": "https://huggingface.co/papers/2511.14295",
            "abstract": "AraLingBench evaluates Arabic and bilingual LLMs' linguistic competence using a benchmark with expert-designed questions across grammar, morphology, spelling, reading comprehension, and syntax, revealing gaps between surface proficiency and true comprehension.  \t\t\t\t\tAI-generated summary \t\t\t\t We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.",
            "score": 71,
            "issue_id": 1,
            "pub_date": "2025-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "37807f1afeb1fa25",
            "authors": [
                "Mohammad Zbeeb",
                "Hasan Abed Al Kader Hammoud",
                "Sina Mukalled",
                "Nadine Rizk",
                "Fatima Karnib",
                "Issam Lakkis",
                "Ammar Mohanna",
                "Bernard Ghanem"
            ],
            "affiliations": [
                "American University of Beirut (AUB)",
                "King Abdullah University of Science and Technology (KAUST)"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.14295.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#reasoning",
                    "#low_resource",
                    "#multilingual"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ° Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ¸Ğ»Ğ»ÑĞ·Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ AraLingBench â€” Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹: Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸ĞºÑƒ, Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ, Ğ¾Ñ€Ñ„Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ Ñ‡ĞµÑ€ĞµĞ· 150 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼. ĞÑ†ĞµĞ½ĞºĞ° 35 Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ñ… Ğ¸ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… LLM Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğµ Ğ²Ğ»Ğ°Ğ´ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ¼, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼. AraLingBench Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ»Ğ»Ğ°Ğ¼Ğ¸ Ğ² Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ½Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ²Ğ»Ğ°Ğ´ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ², Ğ° Ğ½Ğµ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Bridging the Gap in Arabic Linguistic Mastery",
                    "desc": "AraLingBench is a benchmark designed to assess the linguistic competence of Arabic and bilingual large language models (LLMs). It includes 150 expert-crafted multiple choice questions that evaluate key areas such as grammar, morphology, spelling, reading comprehension, and syntax. The evaluation of 35 LLMs shows that while these models perform well on surface-level tasks, they often lack deeper understanding of grammatical and syntactic structures. This highlights a significant gap between high performance on standard tests and true linguistic mastery, indicating that many models rely on memorization rather than genuine comprehension."
                },
                "zh": {
                    "title": "è¯„ä¼°é˜¿æ‹‰ä¼¯è¯­èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•",
                    "desc": "AraLingBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°é˜¿æ‹‰ä¼¯è¯­å’ŒåŒè¯­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¯­è¨€èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†è¯­æ³•ã€å½¢æ€å­¦ã€æ‹¼å†™ã€é˜…è¯»ç†è§£å’Œå¥æ³•äº”ä¸ªæ ¸å¿ƒé¢†åŸŸï¼Œé€šè¿‡150ä¸ªä¸“å®¶è®¾è®¡çš„å¤šé¡¹é€‰æ‹©é¢˜ç›´æ¥è¯„ä¼°è¯­è¨€ç†è§£çš„ç»“æ„æ€§ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨è¡¨é¢æ°´å¹³ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ›´æ·±å±‚æ¬¡çš„è¯­æ³•å’Œå¥æ³•æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚AraLingBenchæ­ç¤ºäº†çŸ¥è¯†æ€§åŸºå‡†æµ‹è¯•é«˜åˆ†ä¸çœŸæ­£è¯­è¨€æŒæ¡ä¹‹é—´çš„å·®è·ï¼Œå¼ºè°ƒäº†è®¸å¤šæ¨¡å‹ä¾èµ–è®°å¿†æˆ–æ¨¡å¼è¯†åˆ«è€ŒéçœŸå®ç†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.10555",
            "title": "A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space",
            "url": "https://huggingface.co/papers/2511.10555",
            "abstract": "A novel method, CoTyle, generates images in consistent visual styles using unique numerical style codes, filling an academic gap in code-to-style image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.",
            "score": 60,
            "issue_id": 1,
            "pub_date": "2025-11-13",
            "pub_date_card": {
                "ru": "13 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 13",
                "zh": "11æœˆ13æ—¥"
            },
            "hash": "7c29f7b93138de5e",
            "authors": [
                "Huijie Liu",
                "Shuhao Cui",
                "Haoxiang Cao",
                "Shuai Ma",
                "Kai Wu",
                "Guoliang Kang"
            ],
            "affiliations": [
                "Beihang University",
                "Kolors Team, Kuaishou Technology",
                "South China Normal University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.10555.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞĞ´Ğ¸Ğ½ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ â€” Ğ¾Ğ´Ğ¸Ğ½ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‚Ğ¸Ğ»ÑŒ",
                    "desc": "CoTyle â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ÑÑ…, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ´Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ±ÑƒĞº ÑÑ‚Ğ¸Ğ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ ÑÑ‚Ğ¸Ğ»ĞµĞ²Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ ĞºĞ°Ğº ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ½Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ²Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ñ… Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ¹. ĞŸÑ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ ÑÑ‚Ğ¸Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ÑÑ Ğ² ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‚Ğ¸Ğ»ĞµĞ²Ğ¾Ğ¹ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¼ ÑÑ‚Ğ¸Ğ»Ğµ."
                },
                "en": {
                    "title": "Transforming Numbers into Unique Visual Styles",
                    "desc": "The paper introduces CoTyle, a novel method for generating images with consistent visual styles using unique numerical style codes. This approach addresses the challenges of existing generative methods that rely on complex prompts and often fail to maintain style consistency. CoTyle utilizes a discrete style codebook to extract style embeddings, which are then used to condition a text-to-image diffusion model for image generation. The method simplifies the process of style generation, allowing for a diverse range of reproducible styles from minimal input, thus filling a significant gap in the academic research of code-to-style image generation."
                },
                "zh": {
                    "title": "ç”¨ä¸€ä¸ªä»£ç å®šä¹‰é£æ ¼çš„é©å‘½æ€§æ–¹æ³•",
                    "desc": "CoTyleæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œé€šè¿‡ç‹¬ç‰¹çš„æ•°å€¼é£æ ¼ä»£ç ç”Ÿæˆä¸€è‡´çš„è§†è§‰é£æ ¼å›¾åƒï¼Œå¡«è¡¥äº†ä»£ç åˆ°é£æ ¼å›¾åƒç”Ÿæˆçš„å­¦æœ¯ç©ºç™½ã€‚è¯¥æ–¹æ³•ä»…ä¾èµ–æ•°å€¼é£æ ¼ä»£ç ï¼Œèƒ½å¤Ÿç”Ÿæˆæ–°é¢–ä¸”ä¸€è‡´çš„è§†è§‰é£æ ¼ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨é£æ ¼ä¸€è‡´æ€§å’Œåˆ›é€ åŠ›æ–¹é¢çš„å±€é™ã€‚æˆ‘ä»¬é¦–å…ˆä»å›¾åƒé›†åˆä¸­è®­ç»ƒç¦»æ•£é£æ ¼ä»£ç æœ¬ï¼Œä»¥æå–é£æ ¼åµŒå…¥ï¼Œç„¶ååˆ©ç”¨è¿™äº›åµŒå…¥æŒ‡å¯¼æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆé£æ ¼å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoTyleæœ‰æ•ˆåœ°å°†æ•°å€¼ä»£ç è½¬åŒ–ä¸ºé£æ ¼æ§åˆ¶å™¨ï¼Œè¯æ˜äº†é£æ ¼çš„ç¡®å¯ä»¥ç”¨ä¸€ä¸ªä»£ç æ¥è¡¨ç¤ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.13189",
            "title": "Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework",
            "url": "https://huggingface.co/papers/2511.13189",
            "abstract": "Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.",
            "score": 38,
            "issue_id": 1,
            "pub_date": "2025-11-17",
            "pub_date_card": {
                "ru": "17 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 17",
                "zh": "11æœˆ17æ—¥"
            },
            "hash": "332f65259461354a",
            "authors": [
                "Diego Ortego",
                "Marlon RodrÃ­guez",
                "Mario Almagro",
                "Kunal Dahiya",
                "David JimÃ©nez",
                "Juan C. SanMiguel"
            ],
            "affiliations": [
                "IIT Delhi",
                "NielsenIQ",
                "Universidad AutÃ³noma de Madrid (UAM)"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.13189.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ”¤",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… foundation models Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ (XMC), Ğ³Ğ´Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸Ğ· Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Only Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° (Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ViXML â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· foundation vision Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ´Ğ½Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking the Power of Vision in Extreme Multi-label Classification",
                    "desc": "This paper explores the use of foundation models in Extreme Multi-label Classification (XMC), which involves associating queries with relevant labels from very large label sets. The authors propose a new framework called Vision-enhanced eXtreme Multi-label Learning (ViXML) that combines the strengths of larger decoder-only models and visual information to improve classification performance. They demonstrate that using a few billion parameters in the decoder can significantly enhance results while keeping computational costs low. Their experiments show that ViXML outperforms traditional text-only models, highlighting the importance of integrating visual data in multi-label classification tasks."
                },
                "zh": {
                    "title": "è§†è§‰å¢å¼ºæç«¯å¤šæ ‡ç­¾å­¦ä¹ çš„çªç ´",
                    "desc": "åŸºç¡€æ¨¡å‹åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œä½†åœ¨æç«¯å¤šæ ‡ç­¾åˆ†ç±»ï¼ˆXMCï¼‰ä¸­çš„æ½œåŠ›å°šæœªè¢«å……åˆ†åˆ©ç”¨ã€‚XMCä¸­çš„æŸ¥è¯¢ä¸æ¥è‡ªæå¤§æ ‡ç­¾ç©ºé—´çš„ç›¸å…³æ ‡ç­¾ç›¸å…³è”ï¼Œå› æ­¤åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´æ‰¾åˆ°å¹³è¡¡è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æ›´å¤§çš„è§£ç å™¨æ¨¡å‹ä»¥åŠå¦‚ä½•åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶åˆ©ç”¨è§†è§‰ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºçš„è§†è§‰å¢å¼ºæç«¯å¤šæ ‡ç­¾å­¦ä¹ æ¡†æ¶ï¼ˆViXMLï¼‰é€šè¿‡å¯¹æ¯ä¸ªå›¾åƒè¿›è¡Œå•ä¸€åµŒå…¥æ± åŒ–ï¼ŒæˆåŠŸæ•´åˆäº†åŸºç¡€è§†è§‰æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.13853",
            "title": "Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark",
            "url": "https://huggingface.co/papers/2511.13853",
            "abstract": "Gen-ViRe benchmarks video models on reasoning abilities using a framework that decomposes Chain-of-Frames reasoning into cognitive dimensions and subtasks.  \t\t\t\t\tAI-generated summary \t\t\t\t While Chain-of-Thought (CoT) prompting enables sophisticated symbolic reasoning in LLMs, it remains confined to discrete text and cannot simulate the continuous, physics-governed dynamics of the real world. Recent video generation models have emerged as potential world simulators through Chain-of-Frames (CoF) reasoning -- materializing thought as frame-by-frame visual sequences, with each frame representing a physically-grounded reasoning step. Despite compelling demonstrations, a challenge persists: existing benchmarks, focusing on fidelity or alignment, do not assess CoF reasoning and thus cannot measure core cognitive abilities in multi-step planning, algorithmic logic, or abstract pattern extrapolation. This evaluation void prevents systematic understanding of model capabilities and principled guidance for improvement. We introduce Gen-ViRe (Generative Visual Reasoning Benchmark), a framework grounded in cognitive science and real-world AI applications, which decomposes CoF reasoning into six cognitive dimensions -- from perceptual logic to abstract planning -- and 24 subtasks. Through multi-source data curation, minimal prompting protocols, and hybrid VLM-assisted evaluation with detailed criteria, Gen-ViRe delivers the first quantitative assessment of video models as reasoners. Our experiments on SOTA systems reveal substantial discrepancies between impressive visual quality and actual reasoning depth, establishing baselines and diagnostic tools to advance genuine world simulators.",
            "score": 34,
            "issue_id": 1,
            "pub_date": "2025-11-17",
            "pub_date_card": {
                "ru": "17 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 17",
                "zh": "11æœˆ17æ—¥"
            },
            "hash": "538a88e47dec5dca",
            "authors": [
                "Xinxin Liu",
                "Zhaopan Xu",
                "Ming Li",
                "Kai Wang",
                "Yong Jae Lee",
                "Yuzhang Shang"
            ],
            "affiliations": [
                "National University of Singapore",
                "UW-Madison",
                "University of Central Florida"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.13853.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Gen-ViRe â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Chain-of-Frames, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑˆĞµÑÑ‚ÑŒ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¸ 24 Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ´Ğ¾ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ VLM Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñƒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Gen-ViRe: Benchmarking Video Models for Real-World Reasoning",
                    "desc": "The paper introduces Gen-ViRe, a benchmark designed to evaluate video models based on their reasoning abilities through a framework that breaks down Chain-of-Frames (CoF) reasoning into cognitive dimensions and subtasks. Unlike traditional benchmarks that focus on visual fidelity, Gen-ViRe assesses core cognitive skills such as multi-step planning and abstract reasoning. This framework is informed by cognitive science and aims to provide a systematic understanding of model capabilities, highlighting the gap between visual quality and reasoning depth. The results from testing state-of-the-art systems reveal significant differences in reasoning performance, paving the way for improved world simulation models."
                },
                "zh": {
                    "title": "Gen-ViReï¼šè§†é¢‘æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°åŸºå‡†",
                    "desc": "Gen-ViReæ˜¯ä¸€ä¸ªè¯„ä¼°è§†é¢‘æ¨¡å‹æ¨ç†èƒ½åŠ›çš„åŸºå‡†æ¡†æ¶ï¼Œå®ƒå°†é“¾å¸§æ¨ç†åˆ†è§£ä¸ºè®¤çŸ¥ç»´åº¦å’Œå­ä»»åŠ¡ã€‚è¯¥æ¡†æ¶æ—¨åœ¨å¡«è¡¥ç°æœ‰åŸºå‡†åœ¨å¤šæ­¥è§„åˆ’ã€ç®—æ³•é€»è¾‘å’ŒæŠ½è±¡æ¨¡å¼æ¨æ–­ç­‰æ ¸å¿ƒè®¤çŸ¥èƒ½åŠ›è¯„ä¼°æ–¹é¢çš„ç©ºç™½ã€‚é€šè¿‡å¤šæºæ•°æ®æ•´ç†å’Œæ··åˆè§†è§‰è¯­è¨€æ¨¡å‹è¾…åŠ©è¯„ä¼°ï¼ŒGen-ViReæä¾›äº†è§†é¢‘æ¨¡å‹ä½œä¸ºæ¨ç†è€…çš„é¦–æ¬¡å®šé‡è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡è§†è§‰è´¨é‡ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†å®é™…æ¨ç†æ·±åº¦å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸ºçœŸæ­£çš„ä¸–ç•Œæ¨¡æ‹Ÿå™¨çš„è¿›æ­¥å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.13026",
            "title": "REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding",
            "url": "https://huggingface.co/papers/2511.13026",
            "abstract": "Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.",
            "score": 25,
            "issue_id": 1,
            "pub_date": "2025-11-17",
            "pub_date_card": {
                "ru": "17 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 17",
                "zh": "11æœˆ17æ—¥"
            },
            "hash": "7891552879d1cdc9",
            "authors": [
                "Jiaze Li",
                "Hao Yin",
                "Wenhui Tan",
                "Jingyang Chen",
                "Boshen Xu",
                "Yuxun Qu",
                "Yijing Chen",
                "Jianzhong Ju",
                "Zhenbo Luo",
                "Jian Luan"
            ],
            "affiliations": [
                "Independent Researcher",
                "MiLM Plus, Xiaomi Inc.",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.13026.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞšÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ REVISOR â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (MLLM) Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ½Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Dual Attribution Decoupled Reward (DADR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (GRPO) Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Video Understanding with Multimodal Reflection",
                    "desc": "This paper introduces REVISOR, a new framework designed to improve long-form video understanding by integrating visual and textual reflection processes. Traditional text-based self-reflection methods struggle with the dynamic nature of video content, as they do not adequately process visual information. REVISOR addresses this by enabling multimodal large language models (MLLMs) to collaboratively reflect on both text and video, enhancing their reasoning capabilities. The framework employs a unique Dual Attribution Decoupled Reward (DADR) mechanism to ensure that the model's reasoning aligns with relevant video evidence, achieving strong performance across multiple benchmarks without needing additional supervised training."
                },
                "zh": {
                    "title": "REVISORï¼šæå‡é•¿è§†é¢‘ç†è§£çš„æ–°æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºREVISORçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€åæ€èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ã€‚ä¼ ç»Ÿçš„æ–‡æœ¬åæ€æœºåˆ¶åœ¨å¤„ç†åŠ¨æ€è§†è§‰ä¿¡æ¯æ—¶å­˜åœ¨æ˜æ˜¾å±€é™ï¼Œå› æ­¤REVISORå¼•å…¥äº†è·¨æ¨¡æ€çš„åæ€è¿‡ç¨‹ï¼Œç»“åˆæ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯è¿›è¡Œæ›´æ·±å…¥çš„æ¨ç†ã€‚é€šè¿‡è®¾è®¡åŒé‡å½’å› è§£è€¦å¥–åŠ±æœºåˆ¶ï¼ˆDADRï¼‰ï¼Œè¯¥æ¡†æ¶ç¡®ä¿æ¨¡å‹çš„æ¨ç†ä¸æ‰€é€‰è§†é¢‘è¯æ®ä¹‹é—´çš„å› æœå¯¹é½ã€‚REVISORåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æå‡äº†é•¿è§†é¢‘ç†è§£çš„èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„ç›‘ç£å¾®è°ƒæˆ–å¤–éƒ¨æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.14159",
            "title": "MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs",
            "url": "https://huggingface.co/papers/2511.14159",
            "abstract": "MVI-Bench evaluates the robustness of Large Vision-Language Models against misleading visual inputs using a hierarchical taxonomy and a novel sensitivity metric, revealing significant vulnerabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating the robustness of Large Vision-Language Models (LVLMs) is essential for their continued development and responsible deployment in real-world applications. However, existing robustness benchmarks typically focus on hallucination or misleading textual inputs, while largely overlooking the equally critical challenge posed by misleading visual inputs in assessing visual understanding. To fill this important gap, we introduce MVI-Bench, the first comprehensive benchmark specially designed for evaluating how Misleading Visual Inputs undermine the robustness of LVLMs. Grounded in fundamental visual primitives, the design of MVI-Bench centers on three hierarchical levels of misleading visual inputs: Visual Concept, Visual Attribute, and Visual Relationship. Using this taxonomy, we curate six representative categories and compile 1,248 expertly annotated VQA instances. To facilitate fine-grained robustness evaluation, we further introduce MVI-Sensitivity, a novel metric that characterizes LVLM robustness at a granular level. Empirical results across 18 state-of-the-art LVLMs uncover pronounced vulnerabilities to misleading visual inputs, and our in-depth analyses on MVI-Bench provide actionable insights that can guide the development of more reliable and robust LVLMs. The benchmark and codebase can be accessed at https://github.com/chenyil6/MVI-Bench.",
            "score": 24,
            "issue_id": 1,
            "pub_date": "2025-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "be7a75bd7662179a",
            "authors": [
                "Huiyi Chen",
                "Jiawei Peng",
                "Dehai Min",
                "Changchang Sun",
                "Kaijie Chen",
                "Yan Yan",
                "Xu Yang",
                "Lu Cheng"
            ],
            "affiliations": [
                "Department of Computer Science, University of Illinois at Chicago",
                "Guohao School, Tongji University",
                "School of Computer Science & Engineering, Southeast University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.14159.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#benchmark",
                    "#hallucinations",
                    "#dataset",
                    "#cv",
                    "#security"
                ],
                "emoji": "ğŸš¨",
                "ru": {
                    "title": "Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ñ‡Ğ¸Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MVI-Bench â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ğ²Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¼ Ğ² Ğ·Ğ°Ğ±Ğ»ÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ğ¿Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ñ‚Ñ€Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ: Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ, Ñ 1248 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ MVI-Sensitivity Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 18 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Strengthening LVLMs Against Visual Deception",
                    "desc": "MVI-Bench is a new benchmark designed to test how well Large Vision-Language Models (LVLMs) handle misleading visual inputs. It introduces a hierarchical taxonomy that categorizes misleading visuals into three levels: Visual Concept, Visual Attribute, and Visual Relationship. The benchmark includes 1,248 annotated visual question-answering instances to assess the models' robustness. A new metric called MVI-Sensitivity is also introduced to evaluate the models' vulnerabilities in detail, revealing significant weaknesses in current LVLMs when faced with misleading visuals."
                },
                "zh": {
                    "title": "è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§æ–°åŸºå‡†",
                    "desc": "MVI-Benchæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰å¯¹è¯¯å¯¼æ€§è§†è§‰è¾“å…¥é²æ£’æ€§çš„æ–°åŸºå‡†ã€‚å®ƒé€šè¿‡å»ºç«‹ä¸€ä¸ªåˆ†å±‚åˆ†ç±»æ³•ï¼Œå…³æ³¨è§†è§‰æ¦‚å¿µã€è§†è§‰å±æ€§å’Œè§†è§‰å…³ç³»ä¸‰ä¸ªå±‚æ¬¡ï¼Œæ­ç¤ºäº†LVLMåœ¨é¢å¯¹è¯¯å¯¼æ€§è§†è§‰è¾“å…¥æ—¶çš„æ˜¾è‘—è„†å¼±æ€§ã€‚è¯¥åŸºå‡†åŒ…å«1248ä¸ªç»è¿‡ä¸“å®¶æ³¨é‡Šçš„è§†è§‰é—®ç­”å®ä¾‹ï¼Œå¹¶å¼•å…¥äº†MVI-Sensitivityè¿™ä¸€æ–°é¢–çš„é²æ£’æ€§è¯„ä¼°æŒ‡æ ‡ã€‚é€šè¿‡å¯¹18ä¸ªæœ€å…ˆè¿›çš„LVLMè¿›è¡Œå®è¯åˆ†æï¼Œå‘ç°å®ƒä»¬åœ¨å¤„ç†è¯¯å¯¼æ€§è§†è§‰è¾“å…¥æ—¶å­˜åœ¨æ˜æ˜¾çš„è„†å¼±æ€§ï¼Œä¸ºæœªæ¥çš„æ¨¡å‹æ”¹è¿›æä¾›äº†é‡è¦çš„æŒ‡å¯¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.14210",
            "title": "Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution",
            "url": "https://huggingface.co/papers/2511.14210",
            "abstract": "Orion, a visual agent framework, uses a suite of specialized computer vision tools to execute complex visual workflows, achieving competitive performance on multiple benchmarks and enabling autonomous visual reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Orion, a visual agent framework that can take in any modality and generate any modality. Using an agentic framework with multiple tool-calling capabilities, Orion is designed for visual AI tasks and achieves state-of-the-art results. Unlike traditional vision-language models that produce descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition, and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance on MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic vision-language models to production-grade visual intelligence. By combining neural perception with symbolic execution, Orion enables autonomous visual reasoning, marking a transition from passive visual understanding to active, tool-driven visual intelligence.",
            "score": 19,
            "issue_id": 1,
            "pub_date": "2025-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "f0bf227b9c4ab079",
            "authors": [
                "N Dinesh Reddy",
                "Dylan Snyder",
                "Lona Kiragu",
                "Mirajul Mohin",
                "Shahrear Bin Amin",
                "Sudeep Pillai"
            ],
            "affiliations": [
                "VLM Run"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.14210.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚",
                    "desc": "Orion â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ vision-language, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Orion ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¿Ğ°Ğ½Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Orion: Empowering Visual Intelligence through Tool-Driven Reasoning",
                    "desc": "Orion is a visual agent framework that integrates various computer vision tools to perform complex visual tasks. It stands out by using an agentic approach that allows it to call multiple tools for executing workflows, rather than just generating descriptive outputs like traditional models. Orion achieves state-of-the-art performance on several benchmarks, demonstrating its effectiveness in visual AI applications. By merging neural perception with symbolic execution, it enables a shift towards autonomous visual reasoning, enhancing the capabilities of visual intelligence systems."
                },
                "zh": {
                    "title": "Orionï¼šä¸»åŠ¨è§†è§‰æ¨ç†çš„æ–°çºªå…ƒ",
                    "desc": "Orionæ˜¯ä¸€ä¸ªè§†è§‰ä»£ç†æ¡†æ¶ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§è¾“å…¥å’Œè¾“å‡ºæ¨¡å¼ã€‚å®ƒç»“åˆäº†å¤šç§è®¡ç®—æœºè§†è§‰å·¥å…·ï¼Œæ‰§è¡Œå¤æ‚çš„è§†è§‰å·¥ä½œæµç¨‹ï¼Œè¡¨ç°å‡ºè‰²ã€‚ä¸ä¼ ç»Ÿçš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸åŒï¼ŒOrioné€šè¿‡è°ƒç”¨å·¥å…·å®ç°ä¸»åŠ¨çš„è§†è§‰æ¨ç†ã€‚è¯¥ç³»ç»Ÿåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ç«äº‰åŠ›ï¼Œæ¨åŠ¨äº†è§†è§‰æ™ºèƒ½çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.14582",
            "title": "OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models",
            "url": "https://huggingface.co/papers/2511.14582",
            "abstract": "OmniZip is a training-free framework that compresses audio-visual tokens by dynamically pruning video tokens based on audio retention scores, achieving significant inference speedup and memory reduction without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Omnimodal large language models (OmniLLMs) have attracted increasing research attention of late towards unified audio-video understanding, wherein processing audio-video token sequences creates a significant computational bottleneck, however. Existing token compression methods have yet to accommodate this emerging need of jointly compressing multimodal tokens. To bridge this gap, we present OmniZip, a training-free, audio-guided audio-visual token-compression framework that optimizes multimodal token representation and accelerates inference. Specifically, OmniZip first identifies salient audio tokens, then computes an audio retention score for each time group to capture information density, thereby dynamically guiding video token pruning and preserving cues from audio anchors enhanced by cross-modal similarity. For each time window, OmniZip compresses the video tokens using an interleaved spatio-temporal scheme. Extensive empirical results demonstrate the merits of OmniZip - it achieves 3.42X inference speedup and 1.4X memory reduction over other top-performing counterparts, while maintaining performance with no training.",
            "score": 17,
            "issue_id": 1,
            "pub_date": "2025-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "82d8f11a42e65eb9",
            "authors": [
                "Keda Tao",
                "Kele Shao",
                "Bohan Yu",
                "Weiqiang Wang",
                "Jian liu",
                "Huan Wang"
            ],
            "affiliations": [
                "Ant Group",
                "Shanghai Innovation Institute",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.14582.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#inference",
                    "#video",
                    "#audio"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾",
                    "desc": "OmniZip â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 3.42 Ñ€Ğ°Ğ·Ğ° Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² 1.4 Ñ€Ğ°Ğ·Ğ° Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Dynamic Token Pruning for Fast and Efficient Audio-Visual Processing",
                    "desc": "OmniZip is a novel framework designed to compress audio-visual tokens without the need for training. It works by evaluating audio retention scores to determine which video tokens can be pruned, thus optimizing the representation of multimodal data. This method significantly speeds up inference and reduces memory usage while preserving the essential information from audio cues. The results show that OmniZip can achieve over three times faster inference and nearly 1.5 times less memory usage compared to existing methods, all without any training involved."
                },
                "zh": {
                    "title": "OmniZipï¼šæ— è®­ç»ƒçš„éŸ³è§†é¢‘æ ‡è®°å‹ç¼©æ¡†æ¶",
                    "desc": "OmniZipæ˜¯ä¸€ä¸ªæ— è®­ç»ƒçš„æ¡†æ¶ï¼Œé€šè¿‡æ ¹æ®éŸ³é¢‘ä¿ç•™åˆ†æ•°åŠ¨æ€ä¿®å‰ªè§†é¢‘æ ‡è®°æ¥å‹ç¼©éŸ³è§†é¢‘æ ‡è®°ï¼Œä»è€Œå®ç°æ˜¾è‘—çš„æ¨ç†åŠ é€Ÿå’Œå†…å­˜å‡å°‘ï¼Œè€Œä¸ç‰ºç‰²æ€§èƒ½ã€‚è¯¥æ¡†æ¶é¦–å…ˆè¯†åˆ«é‡è¦çš„éŸ³é¢‘æ ‡è®°ï¼Œç„¶åè®¡ç®—æ¯ä¸ªæ—¶é—´ç»„çš„éŸ³é¢‘ä¿ç•™åˆ†æ•°ï¼Œä»¥æ•æ‰ä¿¡æ¯å¯†åº¦ï¼Œä»è€ŒåŠ¨æ€æŒ‡å¯¼è§†é¢‘æ ‡è®°çš„ä¿®å‰ªã€‚OmniZipé‡‡ç”¨äº¤é”™çš„æ—¶ç©ºæ–¹æ¡ˆå‹ç¼©è§†é¢‘æ ‡è®°ï¼Œä¿ç•™ç”±éŸ³é¢‘é”šç‚¹å¢å¼ºçš„çº¿ç´¢ã€‚å®éªŒè¯æ˜ï¼ŒOmniZipåœ¨æ¨ç†é€Ÿåº¦ä¸Šæé«˜äº†3.42å€ï¼Œå†…å­˜å‡å°‘äº†1.4å€ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½ï¼Œæ— éœ€è®­ç»ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.14460",
            "title": "Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning",
            "url": "https://huggingface.co/papers/2511.14460",
            "abstract": "A new training framework for RL-based LLM Agents is introduced, extending MDP methodology and demonstrating effectiveness on Multihop QA tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.",
            "score": 17,
            "issue_id": 1,
            "pub_date": "2025-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "3a15f7d47e3a2c9f",
            "authors": [
                "Mingyue Cheng",
                "Jie Ouyang",
                "Shuo Yu",
                "Ruiran Yan",
                "Yucong Luo",
                "Zirui Liu",
                "Daoyu Wang",
                "Qi Liu",
                "Enhong Chen"
            ],
            "affiliations": [
                "State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China, Hefei, China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.14460.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#training",
                    "#agents",
                    "#rl"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Agent-R1 Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² reinforcement learning. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ»Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ (MDP) Ğ´Ğ»Ñ Ğ°Ğ´ĞµĞºĞ²Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ»ĞµĞ³ĞºĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ RL-Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ (Multihop QA)."
                },
                "en": {
                    "title": "Empowering LLM Agents with Reinforcement Learning: Introducing Agent-R1",
                    "desc": "This paper presents a new training framework for Reinforcement Learning (RL) applied to Large Language Model (LLM) Agents, enhancing the traditional Markov Decision Process (MDP) methodology. It addresses the challenges faced in training LLM Agents for complex tasks, particularly in active environments where they can interact and use tools. The proposed framework, named Agent-R1, is modular and adaptable, allowing for easy customization across various tasks and scenarios. Initial experiments on Multihop Question Answering tasks demonstrate the effectiveness of this approach, paving the way for further advancements in RL for LLM Agents."
                },
                "zh": {
                    "title": "ä¸ºRLä»£ç†æ‰“é€ çµæ´»çš„è®­ç»ƒæ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œä¸“ä¸ºåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†è€Œè®¾è®¡ï¼Œæ‰©å±•äº†é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰çš„æ–¹æ³•è®ºã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³å½“å‰RLåœ¨LLMä»£ç†åº”ç”¨ä¸­çš„æŒ‘æˆ˜ï¼Œå¹¶æä¾›çµæ´»ã€å¯æ‰©å±•çš„è®­ç»ƒæ–¹æ¡ˆã€‚é€šè¿‡ç³»ç»Ÿåœ°å®šä¹‰LLMä»£ç†çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œæœ¬æ–‡ä¸ºè¯¥é¢†åŸŸçš„æ·±å…¥æ¢ç´¢å¥ å®šäº†åŸºç¡€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸Šå…·æœ‰æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.14366",
            "title": "ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning",
            "url": "https://huggingface.co/papers/2511.14366",
            "abstract": "ATLAS, a large-scale, cross-disciplinary evaluation suite, addresses the limitations of existing benchmarks by providing high-difficulty, original, and high-fidelity scientific problems to assess the reasoning capabilities of Large Language Models.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable \"ruler\" for progress toward Artificial General Intelligence.",
            "score": 15,
            "issue_id": 1,
            "pub_date": "2025-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "63ee4433c7a942e8",
            "authors": [
                "Hongwei Liu",
                "Junnan Liu",
                "Shudong Liu",
                "Haodong Duan",
                "Yuqiang Li",
                "Mao Su",
                "Xiaohong Liu",
                "Guangtao Zhai",
                "Xinyu Fang",
                "Qianhong Ma",
                "Taolin Zhang",
                "Zihan Ma",
                "Yufeng Zhao",
                "Peiheng Zhou",
                "Linchen Xiao",
                "Wenlong Zhang",
                "Shijie Zhou",
                "Xingjian Ma",
                "Siqi Sun",
                "Jiaye Ge",
                "Meng Li",
                "Yuhong Liu",
                "Jianxin Dong",
                "Jiaying Li",
                "Hui Wu",
                "Hanwen Liang",
                "Jintai Lin",
                "Yanting Wang",
                "Jie Dong",
                "Tong Zhu",
                "Tianfan Fu",
                "Conghui He",
                "Qi Zhang",
                "Songyang Zhang",
                "Lei Bai",
                "Kai Chen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.14366.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#open_source",
                    "#leakage",
                    "#reasoning",
                    "#agi"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ°Ğ´Ñ‘Ğ¶Ğ½Ğ°Ñ Ğ»Ğ¸Ğ½ĞµĞ¹ĞºĞ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ AI",
                    "desc": "ATLAS â€” ÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ’ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¾ĞºĞ¾Ğ»Ğ¾ 800 Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑĞµĞ¼ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ PhD-ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ½Ğ°ÑƒÑ‡Ğ½ÑƒÑ Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ³Ğ´Ğµ LLM-ÑÑƒĞ´ÑŒĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "ATLAS: A New Benchmark for Testing Scientific Reasoning in AI",
                    "desc": "ATLAS is a new evaluation suite designed to test the reasoning abilities of Large Language Models (LLMs) using challenging and original scientific problems. It addresses the shortcomings of existing benchmarks by offering high-difficulty questions across multiple scientific disciplines, ensuring that models can integrate knowledge and reason effectively. The suite includes rigorous quality control measures to maintain the integrity and complexity of the questions, which require multi-step reasoning and detailed answers. Preliminary results indicate that ATLAS can effectively differentiate the advanced reasoning capabilities of leading LLMs, paving the way for a more reliable assessment of progress towards Artificial General Intelligence."
                },
                "zh": {
                    "title": "ATLASï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ ‡å°º",
                    "desc": "ATLASæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„è·¨å­¦ç§‘è¯„ä¼°å¥—ä»¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•çš„å±€é™æ€§ã€‚å®ƒæä¾›äº†çº¦800ä¸ªé«˜éš¾åº¦ã€åŸåˆ›æ€§å¼ºçš„ç§‘å­¦é—®é¢˜ï¼Œä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ATLASæ¶µç›–æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ã€ç”Ÿç‰©ã€è®¡ç®—æœºç§‘å­¦ã€åœ°çƒç§‘å­¦å’Œææ–™ç§‘å­¦ç­‰ä¸ƒä¸ªæ ¸å¿ƒç§‘å­¦é¢†åŸŸï¼Œå¼ºè°ƒè·¨å­¦ç§‘çŸ¥è¯†çš„æ•´åˆå’Œå¤æ‚çš„å¤šæ­¥éª¤æ¨ç†ã€‚è¯¥å¹³å°è¿˜é‡‡ç”¨ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶å’Œä¸“å®¶è¯„å®¡ï¼Œç¡®ä¿é—®é¢˜çš„ç§‘å­¦ä»·å€¼å’Œæ­£ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.11270",
            "title": "Î¦eat: Physically-Grounded Feature Representation",
            "url": "https://huggingface.co/papers/2511.11270",
            "abstract": "A physically-grounded visual backbone, $Î¦$eat, is introduced to capture material identity through self-supervised training, demonstrating robust features invariant to external physical factors.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation models have emerged as effective backbones for many vision tasks. However, current self-supervised features entangle high-level semantics with low-level physical factors, such as geometry and illumination, hindering their use in tasks requiring explicit physical reasoning. In this paper, we introduce Î¦eat, a novel physically-grounded visual backbone that encourages a representation sensitive to material identity, including reflectance cues and geometric mesostructure. Our key idea is to employ a pretraining strategy that contrasts spatial crops and physical augmentations of the same material under varying shapes and lighting conditions. While similar data have been used in high-end supervised tasks such as intrinsic decomposition or material estimation, we demonstrate that a pure self-supervised training strategy, without explicit labels, already provides a strong prior for tasks requiring robust features invariant to external physical factors. We evaluate the learned representations through feature similarity analysis and material selection, showing that Î¦eat captures physically-grounded structure beyond semantic grouping. These findings highlight the promise of unsupervised physical feature learning as a foundation for physics-aware perception in vision and graphics. These findings highlight the promise of unsupervised physical feature learning as a foundation for physics-aware perception in vision and graphics.",
            "score": 10,
            "issue_id": 1,
            "pub_date": "2025-11-14",
            "pub_date_card": {
                "ru": "14 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 14",
                "zh": "11æœˆ14æ—¥"
            },
            "hash": "6be7995006bc181a",
            "authors": [
                "Giuseppe Vecchio",
                "Adrien Kaiser",
                "Rouffet Romain",
                "Rosalie Martin",
                "Elena Garces",
                "Tamy Boubekeur"
            ],
            "affiliations": [
                "Adobe Research"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.11270.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞœĞ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Î¦eat, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ½Ğ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºÑ€Ğ¾Ğ¿Ğ¾Ğ² Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ‚Ğ¾Ğ³Ğ¾ Ğ¶Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ… Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… foundation models, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°ÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ (Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼), Î¦eat Ğ¿Ğ¾Ñ€Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğº Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ° Ğ¸ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼."
                },
                "en": {
                    "title": "Revolutionizing Material Recognition with Self-Supervised Learning",
                    "desc": "This paper presents Î¦eat, a new visual backbone designed to understand material identity through self-supervised learning. Unlike traditional models that mix high-level concepts with low-level physical details, Î¦eat focuses on capturing features that remain consistent despite changes in geometry and lighting. The authors use a pretraining method that contrasts different views and physical variations of the same material, allowing the model to learn robust representations without needing labeled data. The results show that Î¦eat effectively identifies physical properties, paving the way for better physics-aware applications in vision and graphics."
                },
                "zh": {
                    "title": "ç‰©ç†æ„ŸçŸ¥çš„æ–°åŸºç¡€ï¼šÎ¦eatè§†è§‰éª¨å¹²ç½‘ç»œ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºÎ¦eatçš„ç‰©ç†åŸºç¡€è§†è§‰éª¨å¹²ç½‘ç»œï¼Œé€šè¿‡è‡ªç›‘ç£è®­ç»ƒæ•æ‰ææ–™ç‰¹æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒÎ¦eatèƒ½å¤Ÿæå–å¯¹å¤–éƒ¨ç‰©ç†å› ç´ ä¸æ•æ„Ÿçš„å¼ºç‰¹å¾ï¼Œé¿å…äº†é«˜å±‚è¯­ä¹‰ä¸ä½å±‚ç‰©ç†å› ç´ çš„çº ç¼ ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§å¯¹æ¯”é¢„è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨ç›¸åŒææ–™åœ¨ä¸åŒå½¢çŠ¶å’Œå…‰ç…§æ¡ä»¶ä¸‹çš„ç©ºé—´è£å‰ªå’Œç‰©ç†å¢å¼ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒÎ¦eatåœ¨ç‰©ç†ç‰¹å¾å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸ºè§†è§‰å’Œå›¾å½¢ä¸­çš„ç‰©ç†æ„ŸçŸ¥å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.14385",
            "title": "Mitigating Label Length Bias in Large Language Models",
            "url": "https://huggingface.co/papers/2511.14385",
            "abstract": "Normalized contextual calibration addresses label length bias in large language models, improving performance and reliability across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "887b34bedbc803db",
            "authors": [
                "Mario Sanz-Guerrero",
                "Katharina von der Wense"
            ],
            "affiliations": [
                "Johannes Gutenberg University Mainz, Germany",
                "University of Colorado Boulder, USA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.14385.jpg",
            "data": {
                "categories": [
                    "#optimization"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞšĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğº",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğº Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ (NCC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ñ… Ğ´Ğ»Ğ¸Ğ½Ñƒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ°. NCC Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ."
                },
                "en": {
                    "title": "Mitigating Label Length Bias for Better Language Model Performance",
                    "desc": "This paper introduces a method called normalized contextual calibration (NCC) to address label length bias in large language models (LLMs). Label length bias occurs when LLMs inconsistently treat labels of varying lengths, which can lead to inaccurate predictions. NCC normalizes and calibrates predictions at the full-label level, resulting in significant performance improvements across various tasks and datasets. The method enhances the reliability of LLMs, especially in scenarios like multiple-choice question answering, by providing better confidence estimates and requiring fewer examples for effective learning."
                },
                "zh": {
                    "title": "å½’ä¸€åŒ–ä¸Šä¸‹æ–‡æ ¡å‡†ï¼šæå‡è¯­è¨€æ¨¡å‹çš„å¯é æ€§ä¸æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºå½’ä¸€åŒ–ä¸Šä¸‹æ–‡æ ¡å‡†ï¼ˆNCCï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ ‡ç­¾é•¿åº¦åå·®é—®é¢˜ã€‚æ ‡ç­¾é•¿åº¦åå·®æ˜¯æŒ‡åœ¨å¤„ç†ä¸åŒé•¿åº¦çš„æ ‡ç­¾æ—¶ï¼Œæ¨¡å‹çš„é¢„æµ‹è¡¨ç°ä¸ä¸€è‡´ï¼Œå½±å“äº†æ¨¡å‹çš„æ€§èƒ½å’Œå¯é æ€§ã€‚NCCé€šè¿‡åœ¨å®Œæ•´æ ‡ç­¾çº§åˆ«ä¸Šè¿›è¡Œå½’ä¸€åŒ–å’Œæ ¡å‡†ï¼Œæ˜¾è‘—æé«˜äº†å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œæå‡å¹…åº¦å¯è¾¾10%çš„F1åˆ†æ•°ã€‚æ­¤å¤–ï¼ŒNCCè¿˜æ‰©å±•äº†åå·®ç¼“è§£çš„åº”ç”¨èŒƒå›´ï¼Œé€‚ç”¨äºå¤šé¡¹é€‰æ‹©é¢˜å›ç­”ç­‰æ›´å¹¿æ³›çš„ä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.11473",
            "title": "Proactive Hearing Assistants that Isolate Egocentric Conversations",
            "url": "https://huggingface.co/papers/2511.11473",
            "abstract": "A proactive hearing assistant system identifies and separates conversation partners in real-time using a dual-model architecture on binaural audio, adapting to conversational dynamics without explicit prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-11-14",
            "pub_date_card": {
                "ru": "14 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 14",
                "zh": "11æœˆ14æ—¥"
            },
            "hash": "2369b0421f03af73",
            "authors": [
                "Guilin Hu",
                "Malek Itani",
                "Tuochao Chen",
                "Shyamnath Gollakota"
            ],
            "affiliations": [
                "Paul G. Allen School of Computer Science & Engineering, University of Washington"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.11473.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ‘‚",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ ÑĞ»ÑƒÑ…Ğ°, ÑƒĞ¼ĞµÑÑ‰Ğ¸Ğ¹ ÑƒĞ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ‚ÑŒ ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸ĞºĞ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»ÑƒÑ…Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ±ĞµĞ· ÑĞ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ±Ğ¸Ğ½Ğ°ÑƒÑ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ñ€ĞµÑ‡ÑŒ ĞºĞ°Ğº Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ», Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ¾Ğ²Ğ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. Ğ”Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğµ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ²ÑƒÑ…Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾ÑÑ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑˆĞ°Ğ³Ğ¾Ğ¼ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ»ÑƒÑ…Ğ¾Ğ²Ñ‹Ğ¼ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°Ğ¼."
                },
                "en": {
                    "title": "Proactive Hearing Assistants: Real-Time Speaker Separation",
                    "desc": "This paper presents a proactive hearing assistant system that can identify and separate conversation partners in real-time using binaural audio. The system employs a dual-model architecture, where a lightweight streaming model processes audio every 12.5 ms for quick identification, while a slower model analyzes longer conversational patterns. By using the wearer's own speech as a reference, the system effectively distinguishes between different speakers and suppresses background noise. The results demonstrate the system's ability to generalize across various real-world conversation scenarios, enhancing the user experience in multi-speaker environments."
                },
                "zh": {
                    "title": "ä¸»åŠ¨é€‚åº”å¯¹è¯çš„å¬åŠ›åŠ©æ‰‹",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸»åŠ¨å¬åŠ›åŠ©æ‰‹ç³»ç»Ÿï¼Œèƒ½å¤Ÿå®æ—¶è¯†åˆ«å’Œåˆ†ç¦»å¯¹è¯ä¼™ä¼´ï¼Œè€Œæ— éœ€æ˜ç¡®çš„æç¤ºã€‚è¯¥ç³»ç»ŸåŸºäºè‡ªæˆ‘ä¸­å¿ƒçš„åŒè€³éŸ³é¢‘ï¼Œåˆ©ç”¨ä½©æˆ´è€…çš„è‡ªæˆ‘å‘è¨€ä½œä¸ºé”šç‚¹ï¼Œæ¨æ–­å¯¹è¯ä¼™ä¼´å¹¶æŠ‘åˆ¶å…¶ä»–å£°éŸ³ã€‚ä¸ºäº†å®ç°å®æ—¶çš„è®¾å¤‡æ“ä½œï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŒæ¨¡å‹æ¶æ„ï¼šä¸€ä¸ªè½»é‡çº§çš„æµå¼æ¨¡å‹æ¯12.5æ¯«ç§’è¿è¡Œä¸€æ¬¡ï¼Œä»¥ä½å»¶è¿Ÿæå–å¯¹è¯ä¼™ä¼´ï¼Œè€Œä¸€ä¸ªè¾ƒæ…¢çš„æ¨¡å‹åˆ™ä¸é‚£ä¹ˆé¢‘ç¹åœ°è¿è¡Œï¼Œä»¥æ•æ‰æ›´é•¿æ—¶é—´èŒƒå›´çš„å¯¹è¯åŠ¨æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¤šå¯¹è¯åœºæ™¯ä¸­èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å’Œéš”ç¦»å¯¹è¯ä¼™ä¼´ï¼Œæ ‡å¿—ç€ä¸»åŠ¨é€‚åº”å¯¹è¯åŠ¨æ€çš„å¬åŠ›åŠ©æ‰‹çš„è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.12884",
            "title": "Agent READMEs: An Empirical Study of Context Files for Agentic Coding",
            "url": "https://huggingface.co/papers/2511.12884",
            "abstract": "Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files (\"READMEs for agents\") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2025-11-17",
            "pub_date_card": {
                "ru": "17 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 17",
                "zh": "11æœˆ17æ—¥"
            },
            "hash": "4ac888750cbe8e61",
            "authors": [
                "Worawalan Chatlatanagulchai",
                "Hao Li",
                "Yutaro Kashiwa",
                "Brittany Reid",
                "Kundjanasith Thonglek",
                "Pattara Leelaprute",
                "Arnon Rungsawang",
                "Bundit Manaskasemsak",
                "Bram Adams",
                "Ahmed E. Hassan",
                "Hajimu Iida"
            ],
            "affiliations": [
                "Faculty of Engineering, Kasetsart University",
                "Nara Institute of Science and Technology",
                "Queens University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.12884.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#agents",
                    "#plp",
                    "#security"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¾Ñ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² (ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°), Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ 2303 Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ¸Ğ· 1925 Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ğ´Ñƒ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑÑ‚ÑÑ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ: ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ ÑĞ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° (62,3%), Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (69,9%) Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° (67,7%). Ğ’Ğ¼ĞµÑÑ‚Ğµ Ñ Ñ‚ĞµĞ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ»Ğ°ĞºÑƒĞ½Ğ°: Ğ½ĞµÑ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ (14,5%) Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ (14,5%), Ñ€ĞµĞ´ĞºĞ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ² Ñ„Ğ°Ğ¹Ğ»Ğ°Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Enhancing Agent Context for Safer Code Generation",
                    "desc": "This paper explores the role of agent context files, which serve as detailed instructions for coding agents that convert natural language goals into executable code. The study analyzes 2,303 context files from various repositories, revealing that these files are dynamic and evolve over time, similar to configuration code. The analysis shows that developers focus heavily on functional aspects, such as build commands and implementation details, while neglecting important non-functional requirements like security and performance. The findings suggest a need for better tools and practices to enhance the quality and safety of code generated by these agents."
                },
                "zh": {
                    "title": "æå‡ä»£ç†ä»£ç å®‰å…¨æ€§ä¸æ€§èƒ½çš„å¿…è¦æ€§",
                    "desc": "æœ¬æ–‡ç ”ç©¶äº†ä»£ç†ä¸Šä¸‹æ–‡æ–‡ä»¶çš„ç»“æ„ã€ç»´æŠ¤å’Œå†…å®¹ï¼Œè¿™äº›æ–‡ä»¶ä¸ºä»£ç†æä¾›é¡¹ç›®çº§çš„æŒ‡ä»¤ã€‚æˆ‘ä»¬åˆ†æäº†2303ä¸ªä»£ç†ä¸Šä¸‹æ–‡æ–‡ä»¶ï¼Œå‘ç°å®ƒä»¬å¹¶ä¸æ˜¯é™æ€æ–‡æ¡£ï¼Œè€Œæ˜¯åƒé…ç½®ä»£ç ä¸€æ ·ä¸æ–­æ¼”å˜çš„å¤æ‚æ–‡æ¡£ã€‚å¼€å‘è€…åœ¨è¿™äº›æ–‡ä»¶ä¸­ä¼˜å…ˆè€ƒè™‘åŠŸèƒ½æ€§ä¸Šä¸‹æ–‡ï¼Œå¦‚æ„å»ºå’Œè¿è¡Œå‘½ä»¤ã€å®ç°ç»†èŠ‚å’Œæ¶æ„ï¼Œä½†å¯¹éåŠŸèƒ½æ€§éœ€æ±‚å¦‚å®‰å…¨æ€§å’Œæ€§èƒ½çš„å…³æ³¨è¾ƒå°‘ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶å¼€å‘è€…åˆ©ç”¨ä¸Šä¸‹æ–‡æ–‡ä»¶ä½¿ä»£ç†åŠŸèƒ½æ­£å¸¸ï¼Œä½†ç¼ºä¹ç¡®ä¿ä»£ç†ç”Ÿæˆä»£ç å®‰å…¨å’Œé«˜æ•ˆçš„æŒ‡å¯¼ï¼Œå¼ºè°ƒäº†æ”¹è¿›å·¥å…·å’Œå®è·µçš„å¿…è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.14086",
            "title": "Error-Driven Scene Editing for 3D Grounding in Large Language Models",
            "url": "https://huggingface.co/papers/2511.14086",
            "abstract": "An error-driven framework, DEER-3D, improves the grounding accuracy of 3D large language models by iteratively editing and retraining them with targeted counterfactuals.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as a key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or large-scale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following a structured \"Decompose, Diagnostic Evaluation, Edit, and Re-train\" workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying a grounding failure of the 3D-LLM, our framework first diagnoses the exact predicate-level error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "3b9099885a757ea6",
            "authors": [
                "Yue Zhang",
                "Zun Wang",
                "Han Lin",
                "Jialu Li",
                "Jianing Yang",
                "Yonatan Bitton",
                "Idan Szpektor",
                "Mohit Bansal"
            ],
            "affiliations": [
                "Google Research",
                "UNC Chapel Hill",
                "University of Michigan"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.14086.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#interpretability",
                    "#multimodal",
                    "#3d",
                    "#synthetic"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¦ĞµĞ»ĞµĞ²Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ² 3D LLM",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° DEER-3D Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ° Ğº 3D-ÑÑ†ĞµĞ½Ğ°Ğ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 3D-ÑÑ†ĞµĞ½ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ² (Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹). Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑƒ: Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ² 3D LLM."
                },
                "en": {
                    "title": "DEER-3D: Targeted Edits for Enhanced 3D Grounding Accuracy",
                    "desc": "The paper introduces DEER-3D, an innovative framework designed to enhance the grounding accuracy of 3D large language models (3D-LLMs). It addresses the challenge of these models struggling to connect language with visual and spatial elements due to biases in training data. By employing a structured approach of decomposing errors, diagnosing them, editing scenes, and retraining the model, DEER-3D generates targeted counterfactuals that improve model performance. The framework demonstrates significant improvements in grounding accuracy through iterative refinements across various benchmarks for 3D scene understanding tasks."
                },
                "zh": {
                    "title": "DEER-3Dï¼šç²¾å‡†æå‡3Dè¯­è¨€æ¨¡å‹çš„åŸºç¡€å‡†ç¡®æ€§",
                    "desc": "DEER-3Dæ˜¯ä¸€ä¸ªåŸºäºé”™è¯¯é©±åŠ¨çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜3Då¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºç¡€å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£ç¼–è¾‘å’Œé‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨é’ˆå¯¹æ€§çš„åäº‹å®æ¥è§£å†³æ¨¡å‹åœ¨3Dç¯å¢ƒä¸­è¯­è¨€ä¸è§†è§‰å…ƒç´ çš„å¯¹æ¥é—®é¢˜ã€‚DEER-3Dé‡‡ç”¨â€œåˆ†è§£ã€è¯Šæ–­è¯„ä¼°ã€ç¼–è¾‘å’Œå†è®­ç»ƒâ€çš„ç»“æ„åŒ–å·¥ä½œæµç¨‹ï¼Œç²¾ç¡®è¯†åˆ«æ¨¡å‹çš„é”™è¯¯å¹¶è¿›è¡Œæœ€å°åŒ–çš„3Dåœºæ™¯ç¼–è¾‘ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒDEER-3Dæ˜¾è‘—æå‡äº†æ¨¡å‹çš„åŸºç¡€å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†é’ˆå¯¹æ€§åœºæ™¯ç¼–è¾‘åœ¨3Dè¯­è¨€æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.13954",
            "title": "A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition",
            "url": "https://huggingface.co/papers/2511.13954",
            "abstract": "RBTransformer, a Transformer-based model, enhances EEG-based emotion recognition by capturing inter-cortical neural dynamics in latent space, outperforming existing methods on multiple datasets and dimensions.  \t\t\t\t\tAI-generated summary \t\t\t\t Human emotions are difficult to convey through words and are often abstracted in the process; however, electroencephalogram (EEG) signals can offer a more direct lens into emotional brain activity. Recent studies show that deep learning models can process these signals to perform emotion recognition with high accuracy. However, many existing approaches overlook the dynamic interplay between distinct brain regions, which can be crucial to understanding how emotions unfold and evolve over time, potentially aiding in more accurate emotion recognition. To address this, we propose RBTransformer, a Transformer-based neural network architecture that models inter-cortical neural dynamics of the brain in latent space to better capture structured neural interactions for effective EEG-based emotion recognition. First, the EEG signals are converted into Band Differential Entropy (BDE) tokens, which are then passed through Electrode Identity embeddings to retain spatial provenance. These tokens are processed through successive inter-cortical multi-head attention blocks that construct an electrode x electrode attention matrix, allowing the model to learn the inter-cortical neural dependencies. The resulting features are then passed through a classification head to obtain the final prediction. We conducted extensive experiments, specifically under subject-dependent settings, on the SEED, DEAP, and DREAMER datasets, over all three dimensions, Valence, Arousal, and Dominance (for DEAP and DREAMER), under both binary and multi-class classification settings. The results demonstrate that the proposed RBTransformer outperforms all previous state-of-the-art methods across all three datasets, over all three dimensions under both classification settings. The source code is available at: https://github.com/nnilayy/RBTransformer.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-11-17",
            "pub_date_card": {
                "ru": "17 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 17",
                "zh": "11æœˆ17æ—¥"
            },
            "hash": "099ff4901f9e53ef",
            "authors": [
                "Nilay Kumar",
                "Priyansh Bhandari",
                "G. Maragatham"
            ],
            "affiliations": [
                "Department of Computational Intelligence, SRM Institute of Science and Technology"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.13954.jpg",
            "data": {
                "categories": [
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ­Ğ­Ğ“",
                    "desc": "RBTransformer â€” ÑÑ‚Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Transformer, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ­Ğ­Ğ“-ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ·Ğ³Ğ° Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ­Ğ­Ğ“-ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»Ğ¾ÑĞ°Ğ¼ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ embeddings ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ± Ğ¸Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾Ğ²ÑƒÑ attention Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑĞµÑ‚Ğ¸ Ğ²Ñ‹ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼ĞµĞ¶ĞºĞ¾Ñ€Ñ‚Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ RBTransformer Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… SEED, DEAP Ğ¸ DREAMER Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Emotions: RBTransformer Revolutionizes EEG Analysis",
                    "desc": "The RBTransformer is a novel Transformer-based model designed to improve emotion recognition from EEG signals by effectively capturing the dynamic interactions between different brain regions. It converts EEG data into Band Differential Entropy (BDE) tokens and utilizes Electrode Identity embeddings to maintain spatial information. By employing inter-cortical multi-head attention mechanisms, the model learns the dependencies between electrodes, enhancing its ability to recognize emotions accurately. Extensive experiments show that RBTransformer surpasses existing methods on multiple datasets, demonstrating its effectiveness in classifying emotional states across various dimensions."
                },
                "zh": {
                    "title": "RBTransformerï¼šæå‡æƒ…æ„Ÿè¯†åˆ«çš„ç¥ç»åŠ¨æ€æ•æ‰",
                    "desc": "RBTransformeræ˜¯ä¸€ç§åŸºäºTransformerçš„æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡æ•æ‰å¤§è„‘çš®å±‚é—´çš„ç¥ç»åŠ¨æ€æ¥å¢å¼ºåŸºäºè„‘ç”µå›¾ï¼ˆEEGï¼‰çš„æƒ…æ„Ÿè¯†åˆ«ã€‚è¯¥æ¨¡å‹å°†EEGä¿¡å·è½¬æ¢ä¸ºå¸¦å·®ç†µï¼ˆBDEï¼‰æ ‡è®°ï¼Œå¹¶é€šè¿‡ç”µæèº«ä»½åµŒå…¥ä¿ç•™ç©ºé—´ä¿¡æ¯ã€‚é€šè¿‡å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ŒRBTransformerèƒ½å¤Ÿå­¦ä¹ ä¸åŒè„‘åŒºä¹‹é—´çš„ç¥ç»ä¾èµ–å…³ç³»ï¼Œä»è€Œæé«˜æƒ…æ„Ÿè¯†åˆ«çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRBTransformeråœ¨å¤šä¸ªæ•°æ®é›†å’Œç»´åº¦ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.07865",
            "title": "LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost",
            "url": "https://huggingface.co/papers/2511.07865",
            "abstract": "ChaosEater automates the Chaos Engineering cycle using Large Language Models to enhance the resilience of Kubernetes systems with low time and cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Chaos Engineering (CE) is an engineering technique aimed at improving the resilience of distributed systems. It involves intentionally injecting faults into a system to test its resilience, uncover weaknesses, and address them before they cause failures in production. Recent CE tools automate the execution of predefined CE experiments. However, planning such experiments and improving the system based on the experimental results still remain manual. These processes are labor-intensive and require multi-domain expertise. To address these challenges and enable anyone to build resilient systems at low cost, this paper proposes ChaosEater, a system that automates the entire CE cycle with Large Language Models (LLMs). It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. ChaosEater targets CE for software systems built on Kubernetes. Therefore, the LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging. We evaluate ChaosEater through case studies on small- and large-scale Kubernetes systems. The results demonstrate that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-11-11",
            "pub_date_card": {
                "ru": "11 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 11",
                "zh": "11æœˆ11æ—¥"
            },
            "hash": "0d698a7084a9e5d6",
            "authors": [
                "Daisuke Kikuta",
                "Hiroki Ikeuchi",
                "Kengo Tajiri"
            ],
            "affiliations": [
                "NTT, Inc., Japan"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.07865.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#science"
                ],
                "emoji": "ğŸ‰",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ñ…Ğ°Ğ¾ÑĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM Ğ´Ğ»Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Kubernetes-ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "ChaosEater â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑÑŒ Ñ†Ğ¸ĞºĞ» Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ñ…Ğ°Ğ¾ÑĞ° (Chaos Engineering) Ğ´Ğ»Ñ Kubernetes-ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ğ¹ workflow, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ: Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ°, Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºÑƒ. LLM Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ ChaosEater ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Automating Chaos Engineering for Resilient Kubernetes Systems",
                    "desc": "ChaosEater is a system that automates the Chaos Engineering (CE) process using Large Language Models (LLMs) to improve the resilience of Kubernetes systems. It simplifies the traditionally manual tasks of planning and executing CE experiments by defining a structured workflow that assigns specific tasks to LLMs. This automation allows for efficient requirement gathering, code generation, testing, and debugging, making it accessible for users without extensive expertise. The evaluation shows that ChaosEater can effectively conduct CE cycles with reduced time and cost, while also receiving validation from both human engineers and LLMs."
                },
                "zh": {
                    "title": "è‡ªåŠ¨åŒ–æ··æ²Œå·¥ç¨‹ï¼Œæå‡ç³»ç»ŸéŸ§æ€§ï¼",
                    "desc": "ChaosEater æ˜¯ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨åŒ–æ··æ²Œå·¥ç¨‹ï¼ˆChaos Engineeringï¼‰å‘¨æœŸçš„ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜ Kubernetes ç³»ç»Ÿçš„éŸ§æ€§ã€‚æ··æ²Œå·¥ç¨‹æ˜¯ä¸€ç§é€šè¿‡æ•…æ„å¼•å…¥æ•…éšœæ¥æµ‹è¯•åˆ†å¸ƒå¼ç³»ç»ŸéŸ§æ€§çš„å·¥ç¨‹æŠ€æœ¯ã€‚ä¼ ç»Ÿçš„æ··æ²Œå·¥ç¨‹å·¥å…·è™½ç„¶å¯ä»¥è‡ªåŠ¨æ‰§è¡Œé¢„å®šä¹‰çš„å®éªŒï¼Œä½†å®éªŒè§„åˆ’å’Œç»“æœæ”¹è¿›ä»éœ€äººå·¥å®Œæˆï¼Œè€—æ—¶ä¸”éœ€è¦å¤šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ã€‚ChaosEater é€šè¿‡é¢„å®šä¹‰çš„å·¥ä½œæµç¨‹å’Œ LLMsï¼Œç®€åŒ–äº†æ•´ä¸ªæ··æ²Œå·¥ç¨‹å‘¨æœŸï¼Œä½¿å¾—ä»»ä½•äººéƒ½èƒ½ä»¥ä½æˆæœ¬æ„å»ºéŸ§æ€§ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.11831",
            "title": "TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models",
            "url": "https://huggingface.co/papers/2511.11831",
            "abstract": "Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-14",
            "pub_date_card": {
                "ru": "14 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 14",
                "zh": "11æœˆ14æ—¥"
            },
            "hash": "a82135a4d5efda57",
            "authors": [
                "Wenhao Zhou",
                "Hao Zheng",
                "Rong Zhao"
            ],
            "affiliations": [
                "Center for Brain-Inspired Computing Research (CBICR), Tsinghua University, Beijing, China",
                "Department of Precision Instruments, Tsinghua University, Beijing, China",
                "IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.11831.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ¢Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº TopoPerception Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹ Ğº Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ² Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ÑĞµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ñ‹ÑˆĞ°ÑÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ğ¾Ğµ ÑƒĞ·ĞºĞ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ LVLMs. ĞŸĞ°Ñ€Ğ°Ğ´Ğ¾ĞºÑĞ°Ğ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ…ÑƒĞ´ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Rethinking Global Visual Perception in LVLMs with TopoPerception",
                    "desc": "This paper introduces TopoPerception, a new benchmark designed to evaluate the global visual perception capabilities of Large Vision-Language Models (LVLMs) without the influence of local shortcuts. Traditional benchmarks often overestimate model performance due to their reliance on local visual features, which can mislead assessments of a model's true perceptual abilities. The authors find that state-of-the-art LVLMs perform poorly on TopoPerception, often no better than random chance, indicating a significant gap in their ability to understand global visual structures. The study suggests that simply increasing model size and complexity may not solve these issues, highlighting the need for innovative training methods or architectures to enhance global perception in LVLMs."
                },
                "zh": {
                    "title": "æ‹“æ‰‘æ„ŸçŸ¥ï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å…¨çƒæ„ŸçŸ¥èƒ½åŠ›",
                    "desc": "å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰é€šå¸¸å°†ç¼–ç å™¨çš„è§†è§‰ç‰¹å¾ä¸é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½ã€‚ç„¶è€Œï¼Œè¿™ä½¿å¾—è§†è§‰æ„ŸçŸ¥æ¨¡å—æˆä¸ºç“¶é¢ˆï¼Œé™åˆ¶äº†LVLMsçš„æ•´ä½“èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†TopoPerceptionï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œåˆ©ç”¨æ‹“æ‰‘ç‰¹æ€§ä¸¥æ ¼è¯„ä¼°LVLMsåœ¨ä¸åŒç²’åº¦ä¸‹çš„å…¨çƒè§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨TopoPerceptionä¸Šçš„è¡¨ç°ä¸éšæœºçŒœæµ‹æ— å¼‚ï¼Œè¡¨æ˜å®ƒä»¬åœ¨æ„ŸçŸ¥å…¨çƒè§†è§‰ç‰¹å¾æ–¹é¢å­˜åœ¨ä¸¥é‡ä¸è¶³ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-11-18.html",
    "link_next": "2025-11-20.html",
    "link_month": "2025-11.html",
    "short_date_prev": {
        "ru": "18.11",
        "en": "11/18",
        "zh": "11æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "20.11",
        "en": "11/20",
        "zh": "11æœˆ20æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 0,
        "#benchmark": 9,
        "#agents": 3,
        "#cv": 5,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 2,
        "#3d": 1,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 10,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 5,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 7,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 1
    }
}