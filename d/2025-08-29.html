
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 19 papers. August 29.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°</span> | <span id="title-articles-count">19 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-08-28.html">â¬…ï¸ <span id="prev-date">28.08</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-09-01.html">â¡ï¸ <span id="next-date">01.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-08.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '29 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 29', 'zh': '8æœˆ29æ—¥'};
        let feedDateNext = {'ru': '01.09', 'en': '09/01', 'zh': '9æœˆ1æ—¥'};
        let feedDatePrev = {'ru': '28.08', 'en': '08/28', 'zh': '8æœˆ28æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.20751', 'title': 'Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable\n  Text-to-Image Reinforcement Learning', 'url': 'https://huggingface.co/papers/2508.20751', 'abstract': 'Pref-GRPO, a pairwise preference reward-based GRPO method, enhances text-to-image generation by mitigating reward hacking and improving stability, while UniGenBench provides a comprehensive benchmark for evaluating T2I models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements highlight the importance of GRPO-based reinforcement learning methods and benchmarking in enhancing text-to-image (T2I) generation. However, current methods using pointwise reward models (RM) for scoring generated images are susceptible to reward hacking. We reveal that this happens when minimal score differences between images are amplified after normalization, creating illusory advantages that drive the model to over-optimize for trivial gains, ultimately destabilizing the image generation process. To address this, we propose Pref-GRPO, a pairwise preference reward-based GRPO method that shifts the optimization objective from score maximization to preference fitting, ensuring more stable training. In Pref-GRPO, images are pairwise compared within each group using preference RM, and the win rate is used as the reward signal. Extensive experiments demonstrate that PREF-GRPO differentiates subtle image quality differences, providing more stable advantages and mitigating reward hacking. Additionally, existing T2I benchmarks are limited by coarse evaluation criteria, hindering comprehensive model assessment. To solve this, we introduce UniGenBench, a unified T2I benchmark comprising 600 prompts across 5 main themes and 20 subthemes. It evaluates semantic consistency through 10 primary and 27 sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our benchmarks uncover the strengths and weaknesses of both open and closed-source T2I models and validate the effectiveness of Pref-GRPO.', 'score': 69, 'issue_id': 5609, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '865588150c1bd5fd', 'authors': ['Yibin Wang', 'Zhimin Li', 'Yuhang Zang', 'Yujie Zhou', 'Jiazi Bu', 'Chunyu Wang', 'Qinglin Lu', 'Cheng Jin', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Hunyuan, Tencent', 'Shanghai AI Lab', 'Shanghai Innovation Institute', 'Shanghai Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.20751.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#rl', '#survey', '#games'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Pref-GRPO Ğ¸ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ UniGenBench', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Pref-GRPO - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ UniGenBench - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ text-to-image, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 600 Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Pref-GRPO Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ UniGenBench Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing T2I Generation with Stable Preference-Based Rewards', 'desc': 'The paper introduces Pref-GRPO, a new method that improves text-to-image (T2I) generation by using pairwise preference rewards instead of traditional pointwise rewards. This approach helps prevent reward hacking, where models exploit small score differences, leading to unstable image generation. By focusing on preference fitting, Pref-GRPO ensures that the training process is more stable and effective in distinguishing subtle differences in image quality. Additionally, the paper presents UniGenBench, a comprehensive benchmark for evaluating T2I models, which assesses semantic consistency across various themes and criteria, enhancing the evaluation of model performance.'}, 'zh': {'title': 'æå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ç¨³å®šæ€§ä¸è¯„ä¼°æ ‡å‡†', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºPref-GRPOï¼Œå®ƒåŸºäºæˆå¯¹åå¥½å¥–åŠ±ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆè¿‡ç¨‹çš„ç¨³å®šæ€§å¹¶å‡å°‘å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚ä¼ ç»Ÿçš„ç‚¹å¯¹ç‚¹å¥–åŠ±æ¨¡å‹å®¹æ˜“å—åˆ°å¥–åŠ±é»‘å®¢çš„å½±å“ï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦ä¼˜åŒ–å¾®å°çš„å¾—åˆ†å·®å¼‚ã€‚Pref-GRPOé€šè¿‡å°†ä¼˜åŒ–ç›®æ ‡ä»å¾—åˆ†æœ€å¤§åŒ–è½¬å˜ä¸ºåå¥½æ‹Ÿåˆï¼Œç¡®ä¿äº†æ›´ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†UniGenBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ€§èƒ½ï¼Œæ¶µç›–äº†600ä¸ªæç¤ºå’Œå¤šä¸ªä¸»é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.20722', 'title': 'rStar2-Agent: Agentic Reasoning Technical Report', 'url': 'https://huggingface.co/papers/2508.20722', 'abstract': 'rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning, achieves state-of-the-art performance by efficiently handling complex problem-solving with advanced cognitive behaviors and minimal computational resources.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar.', 'score': 59, 'issue_id': 5608, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '4e0a26cb3998d9dd', 'authors': ['Ning Shang', 'Yifei Liu', 'Yi Zhu', 'Li Lyna Zhang', 'Weijiang Xu', 'Xinyu Guan', 'Buze Zhang', 'Bingcheng Dong', 'Xudong Zhou', 'Bowen Zhang', 'Ying Xin', 'Ziming Miao', 'Scarlett Li', 'Fan Yang', 'Mao Yang'], 'affiliations': ['Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2508.20722.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#agents', '#alignment', '#optimization', '#rlhf', '#science'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ RL ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸', 'desc': 'rStar2-Agent - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Python Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ RL, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ GRPO-RoC Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. rStar2-Agent Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Revolutionizing Math Reasoning with rStar2-Agent!', 'desc': 'The rStar2-Agent is a 14 billion parameter model designed for advanced math reasoning, utilizing agentic reinforcement learning to achieve top performance. It showcases sophisticated cognitive behaviors, such as careful consideration before executing Python code and self-reflection on code outcomes to enhance problem-solving. Key innovations include a robust RL infrastructure that minimizes computational costs, a novel GRPO-RoC algorithm that improves reasoning in coding environments, and a structured training approach that builds cognitive skills efficiently. This model not only excels in mathematical tasks but also generalizes well to other domains like alignment and scientific reasoning.'}, 'zh': {'title': 'rStar2-Agentï¼šé«˜æ•ˆçš„æ•°å­¦æ¨ç†ä¸è®¤çŸ¥èƒ½åŠ›', 'desc': 'rStar2-Agentæ˜¯ä¸€ç§14Bçš„æ•°å­¦æ¨ç†æ¨¡å‹ï¼Œé‡‡ç”¨ä»£ç†å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†å¤æ‚é—®é¢˜ï¼Œè¡¨ç°å‡ºè‰²ã€‚è¯¥æ¨¡å‹å…·å¤‡å…ˆè¿›çš„è®¤çŸ¥è¡Œä¸ºï¼Œä¾‹å¦‚åœ¨ä½¿ç”¨Pythonç¼–ç å·¥å…·å‰ä»”ç»†æ€è€ƒï¼Œå¹¶æ ¹æ®ä»£ç æ‰§è¡Œåé¦ˆè¿›è¡Œåæ€ï¼Œä»è€Œè‡ªä¸»æ¢ç´¢å’ŒéªŒè¯ä¸­é—´æ­¥éª¤ã€‚å…¶æˆåŠŸå¾—ç›Šäºä¸‰é¡¹å…³é”®åˆ›æ–°ï¼ŒåŒ…æ‹¬é«˜æ•ˆçš„RLåŸºç¡€è®¾æ–½ã€GRPO-RoCç®—æ³•å’Œé«˜æ•ˆçš„ä»£ç†è®­ç»ƒæ–¹æ¡ˆï¼Œä½¿å¾—åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹ä¹Ÿèƒ½å®ç°å“è¶Šæ€§èƒ½ã€‚rStar2-Agentåœ¨ä»…ç”¨510ä¸ªRLæ­¥éª¤å†…ï¼Œä¾¿åœ¨AIME24å’ŒAIME25ä¸Šå–å¾—äº†80.6%å’Œ69.8%çš„å¹³å‡é€šè¿‡ç‡ï¼Œè¶…è¶Šäº†æ›´å¤§æ¨¡å‹DeepSeek-R1ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.18966', 'title': 'USO: Unified Style and Subject-Driven Generation via Disentangled and\n  Reward Learning', 'url': 'https://huggingface.co/papers/2508.18966', 'abstract': "USO, a unified model, achieves state-of-the-art performance in both style similarity and subject consistency by disentangling and re-composing content and style through a disentangled learning scheme and style reward-learning paradigm.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing literature typically treats style-driven and subject-driven generation as two disjoint tasks: the former prioritizes stylistic similarity, whereas the latter insists on subject consistency, resulting in an apparent antagonism. We argue that both objectives can be unified under a single framework because they ultimately concern the disentanglement and re-composition of content and style, a long-standing theme in style-driven research. To this end, we present USO, a Unified Style-Subject Optimized customization model. First, we construct a large-scale triplet dataset consisting of content images, style images, and their corresponding stylized content images. Second, we introduce a disentangled learning scheme that simultaneously aligns style features and disentangles content from style through two complementary objectives, style-alignment training and content-style disentanglement training. Third, we incorporate a style reward-learning paradigm denoted as SRL to further enhance the model's performance. Finally, we release USO-Bench, the first benchmark that jointly evaluates style similarity and subject fidelity across multiple metrics. Extensive experiments demonstrate that USO achieves state-of-the-art performance among open-source models along both dimensions of subject consistency and style similarity. Code and model: https://github.com/bytedance/USO", 'score': 40, 'issue_id': 5609, 'pub_date': '2025-08-26', 'pub_date_card': {'ru': '26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 26', 'zh': '8æœˆ26æ—¥'}, 'hash': '67afb07a9693ab76', 'authors': ['Shaojin Wu', 'Mengqi Huang', 'Yufeng Cheng', 'Wenxu Wu', 'Jiahe Tian', 'Yiming Luo', 'Fei Ding', 'Qian He'], 'affiliations': ['UXO Team, Intelligent Creation Lab, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2508.18966.jpg', 'data': {'categories': ['#story_generation', '#open_source', '#training', '#benchmark', '#dataset', '#cv'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'USO - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ² ÑÑ‚Ğ¸Ğ»ĞµĞ²Ğ¾Ğ¼ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ…ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ñ€ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸. USO Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ° ÑÑ‚Ğ¸Ğ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ USO-Bench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ²Ğ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Unifying Style and Subject for Superior Image Generation', 'desc': 'The paper introduces USO, a unified model that excels in both style similarity and subject consistency by effectively separating and recombining content and style. It challenges the traditional view of treating style-driven and subject-driven generation as separate tasks, proposing that they can be integrated within a single framework. USO employs a disentangled learning scheme that aligns style features while disentangling content from style, supported by a style reward-learning paradigm to boost performance. The authors also present USO-Bench, a benchmark for evaluating both style and subject fidelity, demonstrating that USO achieves leading results in these areas.'}, 'zh': {'title': 'ç»Ÿä¸€æ¨¡å‹USOï¼šé£æ ¼ä¸ä¸»é¢˜çš„å®Œç¾ç»“åˆ', 'desc': 'USOæ˜¯ä¸€ç§ç»Ÿä¸€æ¨¡å‹ï¼Œé€šè¿‡è§£è€¦å’Œé‡æ–°ç»„åˆå†…å®¹ä¸é£æ ¼ï¼Œè¾¾åˆ°äº†é£æ ¼ç›¸ä¼¼æ€§å’Œä¸»é¢˜ä¸€è‡´æ€§çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†è§£è€¦å­¦ä¹ æ–¹æ¡ˆå’Œé£æ ¼å¥–åŠ±å­¦ä¹ èŒƒå¼ï¼Œè§£å†³äº†é£æ ¼é©±åŠ¨å’Œä¸»é¢˜é©±åŠ¨ç”Ÿæˆä»»åŠ¡ä¹‹é—´çš„çŸ›ç›¾ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„ä¸‰å…ƒç»„æ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†é£æ ¼å¯¹é½è®­ç»ƒå’Œå†…å®¹é£æ ¼è§£è€¦è®­ç»ƒçš„äº’è¡¥ç›®æ ‡ã€‚USO-Benchæ˜¯ç¬¬ä¸€ä¸ªåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå…±åŒè¯„ä¼°é£æ ¼ç›¸ä¼¼æ€§å’Œä¸»é¢˜ä¿çœŸåº¦çš„åŸºå‡†ï¼Œå®éªŒç»“æœè¡¨æ˜USOåœ¨è¿™ä¸¤ä¸ªç»´åº¦ä¸Šéƒ½è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.20453', 'title': 'MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World\n  Tasks via MCP Servers', 'url': 'https://huggingface.co/papers/2508.20453', 'abstract': "MCP-Bench evaluates large language models on complex, multi-step tasks requiring tool use, coordination, and planning across various domains.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MCP-Bench, a benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand tool use, cross-tool coordination, precise parameter control, and planning/reasoning for solving tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search. Unlike prior API-based benchmarks, each MCP server provides a set of complementary tools designed to work together, enabling the construction of authentic, multi-step tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability to retrieve relevant tools from fuzzy instructions without explicit tool names, plan multi-hop execution trajectories for complex objectives, ground responses in intermediate tool outputs, and orchestrate cross-domain workflows - capabilities not adequately evaluated by existing benchmarks that rely on explicit tool specifications, shallow few-step workflows, and isolated domain operations. We propose a multi-faceted evaluation framework covering tool-level schema understanding and usage, trajectory-level planning, and task completion. Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code and data: https://github.com/Accenture/mcp-bench.", 'score': 34, 'issue_id': 5608, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': 'dcd0de7d4f5a0eee', 'authors': ['Zhenting Wang', 'Qi Chang', 'Hemani Patel', 'Shashank Biju', 'Cheng-En Wu', 'Quan Liu', 'Aolin Ding', 'Alireza Rezazadeh', 'Ankit Shah', 'Yujia Bao', 'Eugene Siow'], 'affiliations': ['Center for Advanced AI, Accenture', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2508.20453.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#benchmark', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MCP-Bench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'MCP-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 28 ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ² MCP Ñ 250 Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ñ‚Ğ°ĞºĞ¸Ñ… ÑÑ„ĞµÑ€Ğ°Ñ… ĞºĞ°Ğº Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹, Ğ¿ÑƒÑ‚ĞµÑˆĞµÑÑ‚Ğ²Ğ¸Ñ, Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº. MCP-Bench Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· Ğ½ĞµÑ‡ĞµÑ‚ĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 20 Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… LLM Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ MCP-Bench.'}, 'en': {'title': 'MCP-Bench: Evaluating LLMs on Complex Multi-Step Tasks', 'desc': "MCP-Bench is a new benchmark designed to assess large language models (LLMs) on complex tasks that require the use of multiple tools and careful planning. It connects LLMs to a variety of live servers that provide complementary tools across different domains, allowing for realistic multi-step task execution. The benchmark evaluates the models' abilities to understand fuzzy instructions, plan execution paths, and coordinate workflows across different domains. Results from testing 20 advanced LLMs show that there are still significant challenges in effectively completing these complex tasks."}, 'zh': {'title': 'è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ­¥éª¤ä»»åŠ¡èƒ½åŠ›', 'desc': 'MCP-Benchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŸºå‡†ï¼Œä¸“æ³¨äºå¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦å·¥å…·ä½¿ç”¨ã€è·¨å·¥å…·åè°ƒå’Œè§„åˆ’èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŸºäºæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰ï¼Œè¿æ¥åˆ°28ä¸ªä»£è¡¨æ€§çš„MCPæœåŠ¡å™¨ï¼Œæ¶µç›–250ç§å·¥å…·ï¼Œæ¶‰åŠé‡‘èã€æ—…è¡Œã€ç§‘å­¦è®¡ç®—å’Œå­¦æœ¯æœç´¢ç­‰å¤šä¸ªé¢†åŸŸã€‚ä¸ä»¥å¾€çš„APIåŸºå‡†ä¸åŒï¼ŒMCPæœåŠ¡å™¨æä¾›äº†ä¸€ç»„äº’è¡¥å·¥å…·ï¼Œèƒ½å¤Ÿæ„å»ºçœŸå®çš„å¤šæ­¥éª¤ä»»åŠ¡ï¼Œæµ‹è¯•æ¨¡å‹åœ¨æ¨¡ç³ŠæŒ‡ä»¤ä¸‹æ£€ç´¢ç›¸å…³å·¥å…·çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰çš„LLMsåœ¨MCP-Benchä¸­é¢ä¸´æŒç»­çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å·¥å…·ç†è§£ã€è§„åˆ’å’Œä»»åŠ¡å®Œæˆæ–¹é¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.20404', 'title': 'AWorld: Orchestrating the Training Recipe for Agentic AI', 'url': 'https://huggingface.co/papers/2508.20404', 'abstract': "AWorld, an open-source system for large-scale agent-environment interaction, accelerates experience collection and enhances reinforcement learning, leading to significant improvements in agentic AI performance on complex benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The learning from practice paradigm is crucial for developing capable Agentic AI systems, yet it is severely hampered by inefficient experience generation, a bottleneck especially pronounced in complex benchmarks like GAIA. To address this, we introduce AWorld, an open-source system engineered for large-scale agent-environment interaction. By distributing tasks across a cluster, AWorld accelerates experience collection by 14.6x compared to standard single-node, sequential execution. This critical speedup makes extensive reinforcement learning practical and scalable. Leveraging this capability, we trained a Qwen3-32B-based agent that significantly outperforms its base model, increasing its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most challenging levels, our agent achieves a score of 16.33%, surpassing the performance of leading proprietary models. Our open-source system and resulting agent provide a practical blueprint for a complete agentic AI training pipeline, from efficient interaction to demonstrable model improvement.", 'score': 28, 'issue_id': 5610, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': 'c4a4ef0aa8be9928', 'authors': ['Chengyue Yu', 'Siyuan Lu', 'Chenyi Zhuang', 'Dong Wang', 'Qintong Wu', 'Zongyue Li', 'Runsheng Gan', 'Chunfeng Wang', 'Siqi Hou', 'Gaochi Huang', 'Wenlong Yan', 'Lifeng Hong', 'Aohui Xue', 'Yanfeng Wang', 'Jinjie Gu', 'David Tsai', 'Tao Lin'], 'affiliations': ['AWorld Team, Inclusion AI', 'Shanghai Innovation Institution', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2508.20404.jpg', 'data': {'categories': ['#benchmark', '#games', '#open_source', '#agents', '#rl', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'AWorld: ÑƒÑĞºĞ¾Ñ€ÑĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ˜Ğ˜ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ', 'desc': 'AWorld - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑĞ±Ğ¾Ñ€ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ² 14,6 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑƒĞ·Ğ»Ğµ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ AWorld, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-32B, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ GAIA. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Accelerating Agentic AI with AWorld: 14.6x Faster Experience Collection!', 'desc': 'AWorld is an open-source platform designed to enhance the interaction between agents and their environments, significantly improving the efficiency of experience collection in reinforcement learning. By utilizing a distributed system, AWorld accelerates the process of gathering experiences by 14.6 times compared to traditional methods, which is crucial for training complex AI models. This system has been successfully used to train a Qwen3-32B-based agent, resulting in a notable increase in performance on the GAIA benchmark, with accuracy rising from 21.59% to 32.23%. AWorld not only streamlines the training process but also sets a new standard for developing capable agentic AI systems that can tackle challenging tasks effectively.'}, 'zh': {'title': 'AWorldï¼šåŠ é€Ÿæ™ºèƒ½ä½“å­¦ä¹ çš„å¼€æºç³»ç»Ÿ', 'desc': 'AWorldæ˜¯ä¸€ä¸ªå¼€æºç³»ç»Ÿï¼Œæ—¨åœ¨åŠ é€Ÿå¤§è§„æ¨¡çš„æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’ï¼Œä»è€Œæé«˜å¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡ã€‚é€šè¿‡åœ¨é›†ç¾¤ä¸­åˆ†é…ä»»åŠ¡ï¼ŒAWorldçš„ç»éªŒæ”¶é›†é€Ÿåº¦æ¯”ä¼ ç»Ÿçš„å•èŠ‚ç‚¹æ‰§è¡Œå¿«äº†14.6å€ã€‚è¿™ç§æ˜¾è‘—çš„åŠ é€Ÿä½¿å¾—åœ¨å¤æ‚åŸºå‡†ä¸Šè¿›è¡Œå¹¿æ³›çš„å¼ºåŒ–å­¦ä¹ å˜å¾—å¯è¡Œä¸”å¯æ‰©å±•ã€‚æˆ‘ä»¬åŸºäºQwen3-32Bè®­ç»ƒçš„æ™ºèƒ½ä½“åœ¨GAIAåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡ä»21.59%æå‡è‡³32.23%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.20374', 'title': 'TCIA: A Task-Centric Instruction Augmentation Method for Instruction\n  Finetuning', 'url': 'https://huggingface.co/papers/2508.20374', 'abstract': "Task Centric Instruction Augmentation (TCIA) enhances large language models' performance on specific tasks while maintaining general instruction-following ability.  \t\t\t\t\tAI-generated summary \t\t\t\t Diverse instruction data is vital for effective instruction tuning of large language models, as it enables the model to generalize across different types of inputs . Building such diversified instruction dataset is an essential step in this process. Existing approaches often leverage large language models to automatically explore and generate diverse instructions, ensuring both data diversity and quality. However, they tend to overlook an important factor in real-world applications: on-task relevance. In practice, only a few real-world applications require a truly general-purpose model; most benefit from task-specific knowledge tailored to their particular use case. Therefore, it is vital to develop instruction augmentation methods that not only maintain diversity but are also optimized for specific, real-world scenarios.   We thus introduce Task Centric Instruction Augmentation (TCIA), a framework that systematically expands instructions while preserving both diversity and task alignment. By representing instructions in a discrete query-constraints space, TCIA creates a rich set of task-relevant instructions and enables models to generalize to these task-specific instructions without sacrificing overall performance. Experiments show that TCIA improves open-source LLMs' performance by an average of 8.7% across four real-world, task-specific applications, and in some cases outperforming leading closed-source models. These improvements do not compromise general instruction-following ability, making TCIA a scalable and efficient solution for adapting LLMs to real-world, task-focused applications.", 'score': 19, 'issue_id': 5610, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': 'ff3ef7f9e0dd29da', 'authors': ['Simin Ma', 'Shujian Liu', 'Jun Tan', 'Yebowen Hu', 'Song Wang', 'Sathish Reddy Indurthi', 'Sanqiang Zhao', 'Liwei Wu', 'Jianbing Han', 'Kaiqiang Song'], 'affiliations': ['Zoom Communications Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2508.20374.jpg', 'data': {'categories': ['#data', '#optimization', '#open_source', '#training', '#dataset'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'ĞœĞµÑ‚Ğ¾Ğ´ Task Centric Instruction Augmentation (TCIA) ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ñ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. TCIA ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ñ… Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TCIA ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 8.7% Ğ´Ğ»Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ´ĞµĞ»Ğ°Ñ TCIA Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼.'}, 'en': {'title': 'Enhancing Task-Specific Performance in Language Models with TCIA', 'desc': 'Task Centric Instruction Augmentation (TCIA) is a method designed to improve large language models (LLMs) by enhancing their performance on specific tasks while still allowing them to follow general instructions. It focuses on creating a diverse set of task-relevant instructions, which helps the model adapt to various real-world applications. By using a structured approach to represent instructions, TCIA ensures that the generated data is both diverse and aligned with specific tasks. Experiments demonstrate that TCIA significantly boosts the performance of open-source LLMs, making them more effective for targeted applications without losing their general instruction-following capabilities.'}, 'zh': {'title': 'ä»»åŠ¡ä¸­å¿ƒæŒ‡ä»¤å¢å¼ºï¼šæå‡æ¨¡å‹æ€§èƒ½çš„å…³é”®', 'desc': 'ä»»åŠ¡ä¸­å¿ƒæŒ‡ä»¤å¢å¼ºï¼ˆTCIAï¼‰æ˜¯ä¸€ç§æå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒå…¶é€šç”¨æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºå¤šæ ·åŒ–çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒè¾“å…¥ç±»å‹ä¹‹é—´è¿›è¡Œæœ‰æ•ˆçš„æ³›åŒ–ã€‚TCIAæ¡†æ¶ç³»ç»Ÿæ€§åœ°æ‰©å±•æŒ‡ä»¤ï¼Œæ—¢ä¿æŒå¤šæ ·æ€§ï¼Œåˆä¼˜åŒ–ä¸ç‰¹å®šä»»åŠ¡çš„ç›¸å…³æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒTCIAåœ¨å››ä¸ªçœŸå®ä¸–ç•Œçš„ä»»åŠ¡ç‰¹å®šåº”ç”¨ä¸­å¹³å‡æé«˜äº†8.7%çš„æ€§èƒ½ï¼Œä¸”åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†é¢†å…ˆçš„å°é—­æºæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21058', 'title': 'Mixture of Contexts for Long Video Generation', 'url': 'https://huggingface.co/papers/2508.21058', 'abstract': 'Long video generation is addressed by introducing a sparse attention routing module, Mixture of Contexts, to efficiently manage long-term memory and retrieval in diffusion transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.', 'score': 18, 'issue_id': 5608, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': 'b61607b2a4b6bc58', 'authors': ['Shengqu Cai', 'Ceyuan Yang', 'Lvmin Zhang', 'Yuwei Guo', 'Junfei Xiao', 'Ziyan Yang', 'Yinghao Xu', 'Zhenheng Yang', 'Alan Yuille', 'Leonidas Guibas', 'Maneesh Agrawala', 'Lu Jiang', 'Gordon Wetzstein'], 'affiliations': ['ByteDance', 'ByteDance Seed', 'CUHK', 'Johns Hopkins University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2508.21058.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#video', '#long_context', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Mixture of Contexts (MoC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. MoC Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Efficient Long Video Generation with Sparse Attention', 'desc': "This paper presents a new approach to generating long videos using a method called Mixture of Contexts (MoC), which enhances the efficiency of memory retrieval in diffusion transformers. The challenge of long video generation lies in managing long-term memory to retain important events without losing coherence. MoC addresses this by allowing the model to focus on a few key segments of information while maintaining essential context, thus optimizing the use of computational resources. As a result, this method not only improves the model's ability to generate consistent and coherent long videos but also makes the training process more efficient."}, 'zh': {'title': 'é«˜æ•ˆç”Ÿæˆé•¿è§†é¢‘çš„è®°å¿†æ£€ç´¢æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¨€ç–æ³¨æ„åŠ›è·¯ç”±æ¨¡å—ï¼Œç§°ä¸ºä¸Šä¸‹æ–‡æ··åˆï¼ˆMixture of Contexts, MoCï¼‰ï¼Œç”¨äºé«˜æ•ˆç®¡ç†æ‰©æ•£å˜æ¢å™¨ä¸­çš„é•¿æœŸè®°å¿†å’Œæ£€ç´¢ï¼Œä»¥ç”Ÿæˆé•¿è§†é¢‘ã€‚é•¿è§†é¢‘ç”Ÿæˆæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªé•¿æœŸä¸Šä¸‹æ–‡è®°å¿†é—®é¢˜ï¼Œæ¨¡å‹éœ€è¦åœ¨é•¿æ—¶é—´èŒƒå›´å†…ä¿ç•™å’Œæ£€ç´¢é‡è¦äº‹ä»¶ã€‚ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨å¤„ç†é•¿åºåˆ—æ—¶è®¡ç®—æˆæœ¬å‘ˆäºŒæ¬¡å¢é•¿ï¼Œå¯¼è‡´å†…å­˜å’Œè®¡ç®—å˜å¾—éš¾ä»¥ä¼˜åŒ–ã€‚MoCæ¨¡å—é€šè¿‡åŠ¨æ€é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„ç‰‡æ®µå’Œå¿…è¦çš„é”šç‚¹ï¼Œæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„é•¿æœŸè®°å¿†æ£€ç´¢å¼•æ“ï¼Œä»è€Œæé«˜äº†ç”Ÿæˆé•¿è§†é¢‘çš„æ•ˆç‡å’Œä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.20766', 'title': 'Turning the Spell Around: Lightweight Alignment Amplification via\n  Rank-One Safety Injection', 'url': 'https://huggingface.co/papers/2508.20766', 'abstract': "Rank-One Safety Injection (ROSI) enhances Large Language Model safety by amplifying refusal-mediating subspace activations without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Safety alignment in Large Language Models (LLMs) often involves mediating internal representations to refuse harmful requests. Recent research has demonstrated that these safety mechanisms can be bypassed by ablating or removing specific representational directions within the model. In this paper, we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box method that amplifies a model's safety alignment by permanently steering its activations toward the refusal-mediating subspace. ROSI operates as a simple, fine-tuning-free rank-one weight modification applied to all residual stream write matrices. The required safety direction can be computed from a small set of harmful and harmless instruction pairs. We show that ROSI consistently increases safety refusal rates - as evaluated by Llama Guard 3 - while preserving the utility of the model on standard benchmarks such as MMLU, HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align 'uncensored' models by amplifying their own latent safety directions, demonstrating its utility as an effective last-mile safety procedure. Our results suggest that targeted, interpretable weight steering is a cheap and potent mechanism to improve LLM safety, complementing more resource-intensive fine-tuning paradigms.", 'score': 12, 'issue_id': 5612, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '2088d5e566bf6d1b', 'authors': ['Harethah Abu Shairah', 'Hasan Abed Al Kader Hammoud', 'George Turkiyyah', 'Bernard Ghanem'], 'affiliations': ['King Abdullah University of Science and Technology (KAUST) Thuwal, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2508.20766.jpg', 'data': {'categories': ['#training', '#alignment', '#security', '#rlhf'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Rank-One Safety Injection (ROSI) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ROSI ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ, Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ·Ğ° Ğ¾Ñ‚ĞºĞ°Ğ· Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ñ€Ğ°Ğ½Ğ³Ğ° Ğ¾Ğ´Ğ¸Ğ½ ĞºĞ¾ Ğ²ÑĞµĞ¼ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğ°Ğ¼ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°. ROSI ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ¾Ğ² Ğ¿Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Amplifying Safety in Language Models with ROSI', 'desc': "Rank-One Safety Injection (ROSI) is a novel method designed to enhance the safety of Large Language Models (LLMs) by amplifying specific internal representations that help refuse harmful requests. Instead of fine-tuning the model, ROSI modifies the model's weight matrices to steer activations towards a subspace that promotes safety alignment. This approach is effective in increasing the rates at which the model refuses harmful instructions while maintaining its performance on standard tasks. The method also allows for the re-alignment of models that lack safety measures, making it a valuable tool for improving LLM safety without extensive resource investment."}, 'zh': {'title': 'å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨æ€§çš„æœ‰æ•ˆæ–¹æ³•', 'desc': 'Rank-One Safety Injection (ROSI) æ˜¯ä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨æ€§çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ”¾å¤§æ‹’ç»ä¸­ä»‹å­ç©ºé—´çš„æ¿€æ´»æ¥å®ç°ï¼Œè€Œæ— éœ€è¿›è¡Œå¾®è°ƒã€‚è¯¥æ–¹æ³•é€šè¿‡ç®€å•çš„ç§©ä¸€æƒé‡ä¿®æ”¹ï¼Œæ°¸ä¹…æ€§åœ°å¼•å¯¼æ¨¡å‹çš„æ¿€æ´»æœå‘æ‹’ç»ä¸­ä»‹å­ç©ºé—´ã€‚ROSI å¯ä»¥é€šè¿‡ä¸€å°ç»„æœ‰å®³å’Œæ— å®³æŒ‡ä»¤å¯¹è®¡ç®—æ‰€éœ€çš„å®‰å…¨æ–¹å‘ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å®‰å…¨æ‹’ç»ç‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒROSI åœ¨ä¿æŒæ¨¡å‹æ•ˆç”¨çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å®‰å…¨æ€§ï¼Œæˆä¸ºä¸€ç§é«˜æ•ˆçš„å®‰å…¨å¢å¼ºæ‰‹æ®µã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21066', 'title': 'OneReward: Unified Mask-Guided Image Generation via Multi-Task Human\n  Preference Learning', 'url': 'https://huggingface.co/papers/2508.21066', 'abstract': "A unified reinforcement learning framework using a single vision-language model enhances generative capabilities across multiple tasks without task-specific fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only One Reward model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io", 'score': 9, 'issue_id': 5615, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '6bed77c3f6dcc325', 'authors': ['Yuan Gong', 'Xionghui Wang', 'Jie Wu', 'Shiyin Wang', 'Yitong Wang', 'Xinglong Wu'], 'affiliations': ['ByteDance Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2508.21066.jpg', 'data': {'categories': ['#open_source', '#rlhf', '#cv', '#games', '#optimization', '#training', '#rl', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ OneReward - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ¾Ğ´ ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ. OneReward Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ°ÑĞºĞ¾Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ, Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ, ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Seedream 3.0 Fill, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ OneReward, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸.'}, 'en': {'title': 'OneReward: Unifying Generative Tasks with a Single Model', 'desc': 'This paper presents OneReward, a unified reinforcement learning framework that improves generative capabilities across various tasks without needing specific fine-tuning for each task. It utilizes a single vision-language model (VLM) as a generative reward model to evaluate and distinguish between successful and unsuccessful outcomes for different tasks. The framework is particularly effective for mask-guided image generation, which includes tasks like image filling and object removal, all while maintaining a consistent conditioning approach. By avoiding task-specific supervised fine-tuning, OneReward enhances generalization and efficiency, outperforming existing models in multiple evaluations.'}, 'zh': {'title': 'ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶æå‡å¤šä»»åŠ¡ç”Ÿæˆèƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºOneRewardçš„ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å•ä¸€çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥å¢å¼ºæ¨¡å‹åœ¨å¤šä»»åŠ¡ç”Ÿæˆä¸­çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚OneRewardé€šè¿‡ç”Ÿæˆå¥–åŠ±æ¨¡å‹æ¥åŒºåˆ†ä»»åŠ¡çš„èƒœè€…å’Œè´¥è€…ï¼Œé€‚ç”¨äºå¤šç§æ•°æ®å’Œä»»åŠ¡ç›®æ ‡çš„ç”Ÿæˆæ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨OneRewardè¿›è¡Œæ©ç å¼•å¯¼çš„å›¾åƒç”Ÿæˆï¼Œæ¶‰åŠå›¾åƒå¡«å……ã€å›¾åƒæ‰©å±•ã€ç‰©ä½“ç§»é™¤å’Œæ–‡æœ¬æ¸²æŸ“ç­‰å­ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç»Ÿä¸€ç¼–è¾‘æ¨¡å‹åœ¨å¤šä¸ªè¯„ä¼°ç»´åº¦ä¸Šä¼˜äºå•†ä¸šå’Œå¼€æºç«äº‰å¯¹æ‰‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21060', 'title': 'Multi-View 3D Point Tracking', 'url': 'https://huggingface.co/papers/2508.21060', 'abstract': 'A multi-view 3D point tracker using a feed-forward model with transformers and k-nearest-neighbors achieves robust tracking with fewer cameras and less optimization compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce the first data-driven multi-view 3D point tracker, designed to track arbitrary points in dynamic scenes using multiple camera views. Unlike existing monocular trackers, which struggle with depth ambiguities and occlusion, or prior multi-camera methods that require over 20 cameras and tedious per-sequence optimization, our feed-forward model directly predicts 3D correspondences using a practical number of cameras (e.g., four), enabling robust and accurate online tracking. Given known camera poses and either sensor-based or estimated multi-view depth, our tracker fuses multi-view features into a unified point cloud and applies k-nearest-neighbors correlation alongside a transformer-based update to reliably estimate long-range 3D correspondences, even under occlusion. We train on 5K synthetic multi-view Kubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and DexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively. Our method generalizes well to diverse camera setups of 1-8 views with varying vantage points and video lengths of 24-150 frames. By releasing our tracker alongside training and evaluation datasets, we aim to set a new standard for multi-view 3D tracking research and provide a practical tool for real-world applications. Project page available at https://ethz-vlg.github.io/mvtracker.', 'score': 9, 'issue_id': 5613, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': 'b707acb84814aa86', 'authors': ['Frano RajiÄ', 'Haofei Xu', 'Marko Mihajlovic', 'Siyuan Li', 'Irem Demir', 'Emircan GÃ¼ndoÄŸdu', 'Lei Ke', 'Sergey Prokudin', 'Marc Pollefeys', 'Siyu Tang'], 'affiliations': ['Balgrist University Hospital', 'Carnegie Mellon University', 'ETH Zurich', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2508.21060.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#3d', '#optimization', '#dataset', '#open_source'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğµ 3D-Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ ĞºĞ°Ğ¼ĞµÑ€', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğ¹ Ñ‚Ñ€ĞµĞºĞµÑ€ 3D-Ñ‚Ğ¾Ñ‡ĞµĞº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¸ k-Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… ÑĞ¾ÑĞµĞ´ĞµĞ¹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ…. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ñ€ĞµĞºĞµÑ€ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ ĞºĞ°Ğ¼ĞµÑ€ (Ğ¾ĞºĞ¾Ğ»Ğ¾ 4) Ğ±ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€.'}, 'en': {'title': 'Efficient Multi-View 3D Tracking with Fewer Cameras', 'desc': 'This paper presents a novel multi-view 3D point tracker that utilizes a feed-forward model combined with transformers and k-nearest-neighbors for effective tracking in dynamic environments. Unlike traditional monocular trackers that face challenges with depth perception and occlusions, this approach requires significantly fewer cameras, typically around four, while maintaining high accuracy. The model integrates multi-view features into a cohesive point cloud and employs a transformer-based mechanism to enhance the estimation of 3D correspondences, even in challenging conditions. The tracker has been trained on synthetic data and tested on real-world datasets, demonstrating impressive performance with low trajectory errors across various camera configurations.'}, 'zh': {'title': 'æ–°æ ‡å‡†ï¼šé«˜æ•ˆçš„å¤šè§†è§’ä¸‰ç»´è·Ÿè¸ªå™¨', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„å¤šè§†è§’ä¸‰ç»´ç‚¹è·Ÿè¸ªå™¨ï¼Œåˆ©ç”¨å‰é¦ˆæ¨¡å‹ã€å˜æ¢å™¨å’Œkè¿‘é‚»ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨åŠ¨æ€åœºæ™¯ä¸­ä½¿ç”¨å°‘é‡æ‘„åƒå¤´ï¼ˆå¦‚å››ä¸ªï¼‰è¿›è¡Œç¨³å¥çš„è·Ÿè¸ªã€‚ä¸ä¼ ç»Ÿçš„å•ç›®è·Ÿè¸ªå™¨å’Œéœ€è¦è¶…è¿‡20ä¸ªæ‘„åƒå¤´çš„å¤šæ‘„åƒå¤´æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ·±åº¦æ¨¡ç³Šå’Œé®æŒ¡é—®é¢˜ä¸Šè¡¨ç°æ›´ä½³ï¼Œä¸”æ— éœ€ç¹ççš„ä¼˜åŒ–è¿‡ç¨‹ã€‚è¯¥è·Ÿè¸ªå™¨é€šè¿‡èåˆå¤šè§†è§’ç‰¹å¾ç”Ÿæˆç»Ÿä¸€çš„ç‚¹äº‘ï¼Œå¹¶ç»“åˆkè¿‘é‚»ç›¸å…³æ€§å’Œå˜æ¢å™¨æ›´æ–°ï¼Œèƒ½å¤Ÿå¯é åœ°ä¼°è®¡é•¿è·ç¦»çš„ä¸‰ç»´å¯¹åº”å…³ç³»ã€‚æˆ‘ä»¬åœ¨5Kåˆæˆå¤šè§†è§’æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨çœŸå®ä¸–ç•ŒåŸºå‡†ä¸Šè¯„ä¼°ï¼Œå–å¾—äº†ä¼˜å¼‚çš„è·Ÿè¸ªç²¾åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.17450', 'title': 'Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability\n  in Knowledge and Safety with DuET-PD', 'url': 'https://huggingface.co/papers/2508.17450', 'abstract': "DuET-PD evaluates LLMs in persuasive dialogues, revealing challenges with misinformation and corrections, and introduces Holistic DPO to improve model reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can struggle to balance gullibility to misinformation and resistance to valid corrections in persuasive dialogues, a critical challenge for reliable deployment. We introduce DuET-PD (Dual Evaluation for Trust in Persuasive Dialogues), a framework evaluating multi-turn stance-change dynamics across dual dimensions: persuasion type (corrective/misleading) and domain (knowledge via MMLU-Pro, and safety via SALAD-Bench). We find that even a state-of-the-art model like GPT-4o achieves only 27.32% accuracy in MMLU-Pro under sustained misleading persuasions. Moreover, results reveal a concerning trend of increasing sycophancy in newer open-source models. To address this, we introduce Holistic DPO, a training approach balancing positive and negative persuasion examples. Unlike prompting or resist-only training, Holistic DPO enhances both robustness to misinformation and receptiveness to corrections, improving Llama-3.1-8B-Instruct's accuracy under misleading persuasion in safety contexts from 4.21% to 76.54%. These contributions offer a pathway to developing more reliable and adaptable LLMs for multi-turn dialogue. Code is available at https://github.com/Social-AI-Studio/DuET-PD.", 'score': 8, 'issue_id': 5614, 'pub_date': '2025-08-24', 'pub_date_card': {'ru': '24 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 24', 'zh': '8æœˆ24æ—¥'}, 'hash': 'ca1a413d8e6dbab9', 'authors': ['Bryan Chen Zhengyu Tan', 'Daniel Wai Kit Chin', 'Zhengyuan Liu', 'Nancy F. Chen', 'Roy Ka-Wei Lee'], 'affiliations': ['Institute for Infocomm Research (I2R), A*STAR, Singapore', 'Singapore University of Technology and Design (SUTD)'], 'pdf_title_img': 'assets/pdf/title_img/2508.17450.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#open_source', '#training', '#hallucinations'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑƒĞ±ĞµĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DuET-PD - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ÑƒĞ±ĞµĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸ĞµĞ¹ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ°Ñ‚Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Holistic DPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ¸Ğ¼Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Trust in Persuasive Dialogues with DuET-PD', 'desc': 'This paper presents DuET-PD, a framework designed to evaluate large language models (LLMs) in persuasive dialogues, focusing on their ability to handle misinformation and corrections. The study reveals that even advanced models like GPT-4o struggle significantly with accuracy when faced with misleading information, achieving only 27.32% accuracy in specific tests. To improve model performance, the authors introduce Holistic DPO, a training method that balances exposure to both positive and negative persuasion examples, leading to a substantial increase in accuracy for Llama-3.1-8B-Instruct. This research highlights the importance of developing more reliable LLMs that can effectively navigate complex dialogue scenarios.'}, 'zh': {'title': 'æå‡å¯¹è¯æ¨¡å‹çš„å¯é æ€§ä¸é€‚åº”æ€§', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†DuET-PDæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯´æœæ€§å¯¹è¯ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯é¢å¯¹é”™è¯¯ä¿¡æ¯å’Œä¿®æ­£çš„æŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹å¦‚GPT-4oï¼Œåœ¨æŒç»­çš„è¯¯å¯¼æ€§è¯´æœä¸‹ï¼Œå…¶å‡†ç¡®ç‡ä»…ä¸º27.32%ã€‚æ­¤å¤–ï¼Œæ–°çš„å¼€æºæ¨¡å‹æ˜¾ç¤ºå‡ºè¶Šæ¥è¶Šæ˜æ˜¾çš„è¿åˆæ€§è¶‹åŠ¿ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†Holistic DPOè®­ç»ƒæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å®‰å…¨ä¸Šä¸‹æ–‡ä¸­å¯¹è¯¯å¯¼æ€§è¯´æœçš„æŠµæŠ—åŠ›å’Œå¯¹ä¿®æ­£çš„æ¥å—åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21046', 'title': 'CogVLA: Cognition-Aligned Vision-Language-Action Model via\n  Instruction-Driven Routing & Sparsification', 'url': 'https://huggingface.co/papers/2508.21046', 'abstract': 'CogVLA, a Cognition-Aligned Vision-Language-Action framework, enhances efficiency and performance through instruction-driven routing and sparsification, achieving state-of-the-art results with reduced computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent Vision-Language-Action (VLA) models built on pre-trained Vision-Language Models (VLMs) require extensive post-training, resulting in high computational overhead that limits scalability and deployment.We propose CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages instruction-driven routing and sparsification to improve both efficiency and performance. CogVLA draws inspiration from human multimodal coordination and introduces a 3-stage progressive architecture. 1) Encoder-FiLM based Aggregation Routing (EFA-Routing) injects instruction information into the vision encoder to selectively aggregate and compress dual-stream visual tokens, forming a instruction-aware latent representation. 2) Building upon this compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing) introduces action intent into the language model by pruning instruction-irrelevant visually grounded tokens, thereby achieving token-level sparsity. 3) To ensure that compressed perception inputs can still support accurate and coherent action generation, we introduce V-L-A Coupled Attention (CAtten), which combines causal vision-language attention with bidirectional action parallel decoding. Extensive experiments on the LIBERO benchmark and real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art performance with success rates of 97.4% and 70.0%, respectively, while reducing training costs by 2.5-fold and decreasing inference latency by 2.8-fold compared to OpenVLA. CogVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/CogVLA.', 'score': 7, 'issue_id': 5611, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '45b5eec9ac97923c', 'authors': ['Wei Li', 'Renshan Zhang', 'Rui Shao', 'Jie He', 'Liqiang Nie'], 'affiliations': ['School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2508.21046.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#video', '#training', '#optimization', '#robotics', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'CogVLA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. CogVLA Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ EFA-Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹, LFP-Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ V-L-A ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CogVLA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'CogVLA: Efficient Vision-Language-Action through Smart Routing', 'desc': 'CogVLA is a new framework designed to improve the efficiency and performance of Vision-Language-Action (VLA) models by using instruction-driven routing and sparsification techniques. It consists of a three-stage architecture that enhances the processing of visual and language inputs, making it more aligned with human cognitive processes. The framework employs methods like Encoder-FiLM based Aggregation Routing and LLM-FiLM based Pruning Routing to create a compact representation of visual data and to eliminate unnecessary tokens. As a result, CogVLA achieves impressive performance metrics while significantly reducing computational costs and latency, making it suitable for real-world applications.'}, 'zh': {'title': 'CogVLAï¼šé«˜æ•ˆçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶', 'desc': 'CogVLAæ˜¯ä¸€ä¸ªè®¤çŸ¥å¯¹é½çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶ï¼Œé€šè¿‡æŒ‡ä»¤é©±åŠ¨çš„è·¯ç”±å’Œç¨€ç–åŒ–æ¥æé«˜æ•ˆç‡å’Œæ€§èƒ½ã€‚è¯¥æ¡†æ¶çµæ„Ÿæ¥æºäºäººç±»çš„å¤šæ¨¡æ€åè°ƒï¼Œé‡‡ç”¨ä¸‰é˜¶æ®µçš„æ¸è¿›å¼æ¶æ„ã€‚é¦–å…ˆï¼ŒEFA-Routingå°†æŒ‡ä»¤ä¿¡æ¯æ³¨å…¥è§†è§‰ç¼–ç å™¨ï¼Œä»¥é€‰æ‹©æ€§åœ°èšåˆå’Œå‹ç¼©è§†è§‰æ ‡è®°ã€‚å…¶æ¬¡ï¼ŒLFP-Routingé€šè¿‡ä¿®å‰ªä¸æŒ‡ä»¤æ— å…³çš„è§†è§‰æ ‡è®°ï¼Œå¼•å…¥åŠ¨ä½œæ„å›¾ï¼Œä»è€Œå®ç°æ ‡è®°çº§ç¨€ç–æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21052', 'title': 'FakeParts: a New Family of AI-Generated DeepFakes', 'url': 'https://huggingface.co/papers/2508.21052', 'abstract': 'FakeParts introduces a new type of deepfake with subtle, localized manipulations that are difficult to detect, and FakePartsBench provides a large-scale dataset to evaluate detection methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations, ranging from altered facial expressions to object substitutions and background modifications, blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection capabilities, we present FakePartsBench, the first large-scale benchmark dataset specifically designed to capture the full spectrum of partial deepfakes. Comprising over 25K videos with pixel-level and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by over 30% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current deepfake detection approaches and provides the necessary resources to develop more robust methods for partial video manipulations.', 'score': 4, 'issue_id': 5611, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': 'f5b81af6ff50bf44', 'authors': ['Gaetan Brison', 'Soobash Daiboo', 'Samy Aimeur', 'Awais Hussain Sani', 'Xi Wang', 'Gianni Franchi', 'Vicky Kalogeiton'], 'affiliations': ['Hi!PARIS, Institut Polytechnique de Paris', 'LIX, Ã‰cole Polytechnique, CNRS, Institut Polytechnique de Paris', 'U2IS, ENSTA Paris, Institut Polytechnique de Paris'], 'pdf_title_img': 'assets/pdf/title_img/2508.21052.jpg', 'data': {'categories': ['#security', '#video', '#dataset', '#benchmark'], 'emoji': 'ğŸ­', 'ru': {'title': 'FakeParts: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ¿Ñ„ĞµĞ¹ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚Ğ¸Ğ¿ Ğ´Ğ¸Ğ¿Ñ„ĞµĞ¹ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FakeParts, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… FakePartsBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ´Ğ¸Ğ¿Ñ„ĞµĞ¹ĞºĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ FakeParts ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ½Ğ° 30% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ğ¿Ñ„ĞµĞ¹ĞºĞ°Ğ¼Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ñ… Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ¿Ñ„ĞµĞ¹ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'Unmasking Subtle Deepfakes: The Challenge of FakeParts', 'desc': 'FakeParts presents a novel category of deepfakes that involve subtle and localized alterations in videos, making them harder to identify than traditional deepfakes. These manipulations can include changes to facial expressions, object replacements, or background edits, which blend seamlessly with genuine content. To facilitate the detection of these sophisticated deepfakes, the authors introduce FakePartsBench, a comprehensive dataset containing over 25,000 videos annotated for pixel-level and frame-level manipulations. The findings reveal that these partial deepfakes significantly impair detection accuracy, highlighting a critical need for improved detection techniques in the face of evolving deepfake technology.'}, 'zh': {'title': 'æ­ç¤ºæ·±åº¦ä¼ªé€ çš„æ–°æŒ‘æˆ˜', 'desc': 'FakePartsæ˜¯ä¸€ç§æ–°å‹çš„æ·±åº¦ä¼ªé€ æŠ€æœ¯ï¼Œç‰¹ç‚¹æ˜¯å¯¹è§†é¢‘ç‰¹å®šåŒºåŸŸæˆ–æ—¶é—´æ®µè¿›è¡Œå¾®å¦™çš„å±€éƒ¨æ“æ§ï¼Œè¿™ä½¿å¾—æ£€æµ‹å˜å¾—å›°éš¾ã€‚è¿™äº›å±€éƒ¨æ“æ§åŒ…æ‹¬æ”¹å˜é¢éƒ¨è¡¨æƒ…ã€æ›¿æ¢ç‰©ä½“å’Œä¿®æ”¹èƒŒæ™¯ç­‰ï¼Œèƒ½å¤Ÿä¸çœŸå®å…ƒç´ æ— ç¼èåˆï¼Œå…·æœ‰å¾ˆå¼ºçš„æ¬ºéª—æ€§ã€‚ä¸ºäº†è§£å†³æ£€æµ‹èƒ½åŠ›çš„ä¸è¶³ï¼Œç ”ç©¶è€…ä»¬æ¨å‡ºäº†FakePartsBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å±€éƒ¨æ·±åº¦ä¼ªé€ çš„è§„æ¨¡åŒ–åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡25,000ä¸ªè§†é¢‘åŠå…¶åƒç´ çº§å’Œå¸§çº§çš„æ“æ§æ³¨é‡Šã€‚ç ”ç©¶è¡¨æ˜ï¼ŒFakePartsä½¿äººç±»æ£€æµ‹å‡†ç¡®ç‡ä¸‹é™è¶…è¿‡30%ï¼Œå¹¶ä¸”åœ¨æœ€å…ˆè¿›çš„æ£€æµ‹æ¨¡å‹ä¸­ä¹Ÿè§‚å¯Ÿåˆ°äº†ç±»ä¼¼çš„æ€§èƒ½ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.20755', 'title': 'Provable Benefits of In-Tool Learning for Large Language Models', 'url': 'https://huggingface.co/papers/2508.20755', 'abstract': 'Tool-augmented language models achieve better factual recall and scalability through external retrieval rather than memorization in their weights.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool-augmented language models, equipped with retrieval, memory, or external APIs, are reshaping AI, yet their theoretical advantages remain underexplored. In this paper, we address this question by demonstrating the benefits of in-tool learning (external retrieval) over in-weight learning (memorization) for factual recall. We show that the number of facts a model can memorize solely in its weights is fundamentally limited by its parameter count. In contrast, we prove that tool-use enables unbounded factual recall via a simple and efficient circuit construction. These results are validated in controlled experiments, where tool-using models consistently outperform memorizing ones. We further show that for pretrained large language models, teaching tool-use and general rules is more effective than finetuning facts into memory. Our work provides both a theoretical and empirical foundation, establishing why tool-augmented workflows are not just practical, but provably more scalable.', 'score': 4, 'issue_id': 5611, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '28be2dcc2de1f643', 'authors': ['Sam Houliston', 'Ambroise Odonnat', 'Charles Arnal', 'Vivien Cabannes'], 'affiliations': ['ETH ZÃ¼rich', 'FAIR at Meta', 'Inria, Univ. Rennes 2 and IRISA'], 'pdf_title_img': 'assets/pdf/title_img/2508.20755.jpg', 'data': {'categories': ['#rag', '#optimization', '#multimodal', '#alignment'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ’Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ² ÑĞ²Ğ¾Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ñ‚ÑŒ Ğ² Ğ²ĞµÑĞ°Ñ…, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ĞµÑ‚, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹, Ğ½Ğ¾ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹.'}, 'en': {'title': 'Unlocking Unlimited Knowledge with Tool-Augmented Models', 'desc': 'This paper explores how tool-augmented language models can improve factual recall by using external retrieval methods instead of relying solely on memorization. It highlights that the ability of a model to remember facts is limited by its number of parameters, while using tools allows for unlimited access to information. The authors demonstrate through experiments that models utilizing external tools consistently outperform those that only memorize facts. Additionally, they argue that teaching models to use tools is more effective than simply fine-tuning them to remember specific facts, establishing a strong case for the scalability of tool-augmented approaches in AI.'}, 'zh': {'title': 'å·¥å…·å¢å¼ºæ¨¡å‹ï¼šè¶…è¶Šè®°å¿†çš„æ— é™å¯èƒ½', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å·¥å…·å¢å¼ºè¯­è¨€æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨äº‹å®å›å¿†å’Œå¯æ‰©å±•æ€§æ–¹é¢ã€‚é€šè¿‡å¤–éƒ¨æ£€ç´¢ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿè¶…è¶Šä»…ä¾èµ–äºæƒé‡è®°å¿†çš„é™åˆ¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡å‹çš„è®°å¿†èƒ½åŠ›å—é™äºå…¶å‚æ•°æ•°é‡ï¼Œè€Œå·¥å…·ä½¿ç”¨åˆ™å¯ä»¥å®ç°æ— é™çš„äº‹å®å›å¿†ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœéªŒè¯äº†å·¥å…·ä½¿ç”¨æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºä»…ä¾èµ–è®°å¿†çš„æ¨¡å‹ï¼Œå¹¶ä¸”å¯¹äºé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ•™æˆå·¥å…·ä½¿ç”¨å’Œé€šç”¨è§„åˆ™æ¯”å¾®è°ƒè®°å¿†ä¸­çš„äº‹å®æ›´æœ‰æ•ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.18633', 'title': 'ROSE: Remove Objects with Side Effects in Videos', 'url': 'https://huggingface.co/papers/2508.18633', 'abstract': "ROSE, a video inpainting framework using diffusion transformers, effectively removes objects and their side effects like shadows and reflections by leveraging synthetic data and differential masks.  \t\t\t\t\tAI-generated summary \t\t\t\t Video object removal has achieved advanced performance due to the recent success of video generative models. However, when addressing the side effects of objects, e.g., their shadows and reflections, existing works struggle to eliminate these effects for the scarcity of paired video data as supervision. This paper presents ROSE, termed Remove Objects with Side Effects, a framework that systematically studies the object's effects on environment, which can be categorized into five common cases: shadows, reflections, light, translucency and mirror. Given the challenges of curating paired videos exhibiting the aforementioned effects, we leverage a 3D rendering engine for synthetic data generation. We carefully construct a fully-automatic pipeline for data preparation, which simulates a large-scale paired dataset with diverse scenes, objects, shooting angles, and camera trajectories. ROSE is implemented as an video inpainting model built on diffusion transformer. To localize all object-correlated areas, the entire video is fed into the model for reference-based erasing. Moreover, additional supervision is introduced to explicitly predict the areas affected by side effects, which can be revealed through the differential mask between the paired videos. To fully investigate the model performance on various side effect removal, we presents a new benchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five special side effects for comprehensive evaluation. Experimental results demonstrate that ROSE achieves superior performance compared to existing video object erasing models and generalizes well to real-world video scenarios. The project page is https://rose2025-inpaint.github.io/.", 'score': 4, 'issue_id': 5609, 'pub_date': '2025-08-26', 'pub_date_card': {'ru': '26 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 26', 'zh': '8æœˆ26æ—¥'}, 'hash': '844c1b2076f2c4b2', 'authors': ['Chenxuan Miao', 'Yutong Feng', 'Jianshu Zeng', 'Zixiang Gao', 'Hantang Liu', 'Yunfeng Yan', 'Donglian Qi', 'Xi Chen', 'Bin Wang', 'Hengshuang Zhao'], 'affiliations': ['KunByte AI', 'Peking University', 'The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.18633.jpg', 'data': {'categories': ['#synthetic', '#diffusion', '#benchmark', '#video', '#dataset'], 'emoji': 'ğŸ­', 'ru': {'title': 'ROSE: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… ÑĞ»ĞµĞ´Ğ¾Ğ² Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'ROSE - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹. ĞĞ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ‚ĞµĞ½Ğ¸ Ğ¸ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°ÑĞºĞ¸. ROSE Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 3D-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹, Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑĞµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼, Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ¾Ğ½Ñ‹, Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµÑ‘ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ².'}, 'en': {'title': 'ROSE: Mastering Object Removal with Side Effects in Videos', 'desc': 'ROSE is a video inpainting framework that uses diffusion transformers to effectively remove objects from videos along with their associated side effects, such as shadows and reflections. It addresses the challenge of limited paired video data by generating synthetic data through a 3D rendering engine, creating a diverse dataset for training. The framework categorizes object effects into five common cases and employs a reference-based erasing technique to localize affected areas. ROSE also introduces a new benchmark, ROSE-Bench, to evaluate its performance on various side effect removal tasks, demonstrating superior results compared to existing models.'}, 'zh': {'title': 'ROSEï¼šå»é™¤ç‰©ä½“åŠå…¶å‰¯ä½œç”¨çš„åˆ›æ–°æ¡†æ¶', 'desc': 'ROSEæ˜¯ä¸€ä¸ªè§†é¢‘ä¿®å¤æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£å˜æ¢å™¨æœ‰æ•ˆå»é™¤è§†é¢‘ä¸­çš„ç‰©ä½“åŠå…¶å‰¯ä½œç”¨ï¼Œå¦‚é˜´å½±å’Œåå°„ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆæˆæ•°æ®å’Œå·®åˆ†æ©ç æ¥è§£å†³ç°æœ‰æ–¹æ³•åœ¨å»é™¤å‰¯ä½œç”¨æ—¶é¢ä¸´çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚ROSEç³»ç»Ÿåœ°ç ”ç©¶äº†ç‰©ä½“å¯¹ç¯å¢ƒçš„å½±å“ï¼Œå¹¶å°†å…¶åˆ†ç±»ä¸ºäº”ç§å¸¸è§æƒ…å†µï¼šé˜´å½±ã€åå°„ã€å…‰ã€åŠé€æ˜å’Œé•œé¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒROSEåœ¨è§†é¢‘ç‰©ä½“å»é™¤ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå¹¶èƒ½å¾ˆå¥½åœ°é€‚åº”çœŸå®ä¸–ç•Œçš„è§†é¢‘åœºæ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21070', 'title': 'Dress&Dance: Dress up and Dance as You Like It - Technical Preview', 'url': 'https://huggingface.co/papers/2508.21070', 'abstract': 'A video diffusion framework generates high-quality virtual try-on videos using a conditioning network that unifies multi-modal inputs, outperforming existing solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Dress&Dance, a video diffusion framework that generates high quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a user wearing desired garments while moving in accordance with a given reference video. Our approach requires a single user image and supports a range of tops, bottoms, and one-piece garments, as well as simultaneous tops and bottoms try-on in a single pass. Key to our framework is CondNet, a novel conditioning network that leverages attention to unify multi-modal inputs (text, images, and videos), thereby enhancing garment registration and motion fidelity. CondNet is trained on heterogeneous training data, combining limited video data and a larger, more readily available image dataset, in a multistage progressive manner. Dress&Dance outperforms existing open source and commercial solutions and enables a high quality and flexible try-on experience.', 'score': 3, 'issue_id': 5611, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': '132ee843142b08ff', 'authors': ['Jun-Kun Chen', 'Aayush Bansal', 'Minh Phuoc Vo', 'Yu-Xiong Wang'], 'affiliations': ['SpreeAI', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2508.21070.jpg', 'data': {'categories': ['#multimodal', '#video', '#training', '#diffusion', '#open_source'], 'emoji': 'ğŸ‘š', 'ru': {'title': 'Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ° Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Dress&Dance Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞµÑ‚ÑŒ CondNet, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ´ĞµĞ¾) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Dress&Dance Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸.'}, 'en': {'title': 'Revolutionizing Virtual Try-Ons with Dress&Dance!', 'desc': 'The paper introduces Dress&Dance, a video diffusion framework designed to create high-quality virtual try-on videos. It generates 5-second videos at 24 FPS, allowing users to see themselves in various garments while mimicking movements from a reference video. The key innovation is CondNet, a conditioning network that integrates multiple input types, such as text, images, and videos, to improve garment fitting and motion accuracy. By training on diverse datasets, the framework surpasses current solutions, offering a more flexible and realistic virtual try-on experience.'}, 'zh': {'title': 'é«˜è´¨é‡è™šæ‹Ÿè¯•ç©¿è§†é¢‘ç”Ÿæˆ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDress&Danceçš„è§†é¢‘æ‰©æ•£æ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è™šæ‹Ÿè¯•ç©¿è§†é¢‘ã€‚è¯¥æ¡†æ¶ä½¿ç”¨ä¸€ä¸ªæ–°çš„æ¡ä»¶ç½‘ç»œCondNetï¼Œèƒ½å¤Ÿç»Ÿä¸€å¤šæ¨¡æ€è¾“å…¥ï¼ˆæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ï¼‰ï¼Œä»è€Œæé«˜æœè£…åŒ¹é…å’Œè¿åŠ¨çœŸå®æ„Ÿã€‚åªéœ€ä¸€å¼ ç”¨æˆ·å›¾åƒï¼Œç³»ç»Ÿå°±å¯ä»¥ç”Ÿæˆ5ç§’é•¿çš„è™šæ‹Ÿè¯•ç©¿è§†é¢‘ï¼Œæ”¯æŒå¤šç§æœè£…çš„å°è¯•ã€‚Dress&Danceåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¼€æºå’Œå•†ä¸šè§£å†³æ–¹æ¡ˆï¼Œæä¾›äº†é«˜è´¨é‡å’Œçµæ´»çš„è¯•ç©¿ä½“éªŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.15228', 'title': 'Collaborative Multi-Modal Coding for High-Quality 3D Generation', 'url': 'https://huggingface.co/papers/2508.15228', 'abstract': 'TriMM, a feed-forward 3D-native generative model, integrates multi-modal data (RGB, RGBD, point clouds) to enhance 3D asset generation quality and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D content inherently encompasses multi-modal characteristics and can be projected into different modalities (e.g., RGB images, RGBD, and point clouds). Each modality exhibits distinct advantages in 3D asset modeling: RGB images contain vivid 3D textures, whereas point clouds define fine-grained 3D geometries. However, most existing 3D-native generative architectures either operate predominantly within single-modality paradigms-thus overlooking the complementary benefits of multi-modality data-or restrict themselves to 3D structures, thereby limiting the scope of available training datasets. To holistically harness multi-modalities for 3D modeling, we present TriMM, the first feed-forward 3D-native generative model that learns from basic multi-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM first introduces collaborative multi-modal coding, which integrates modality-specific features while preserving their unique representational strengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to raise the robustness and performance of multi-modal coding. 3) Based on the embedded multi-modal code, TriMM employs a triplane latent diffusion model to generate 3D assets of superior quality, enhancing both the texture and the geometric detail. Extensive experiments on multiple well-known datasets demonstrate that TriMM, by effectively leveraging multi-modality, achieves competitive performance with models trained on large-scale datasets, despite utilizing a small amount of training data. Furthermore, we conduct additional experiments on recent RGB-D datasets, verifying the feasibility of incorporating other multi-modal datasets into 3D generation.', 'score': 3, 'issue_id': 5609, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 21', 'zh': '8æœˆ21æ—¥'}, 'hash': '125740d5cf6d2107', 'authors': ['Ziang Cao', 'Zhaoxi Chen', 'Liang Pan', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University, Singapore', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.15228.jpg', 'data': {'categories': ['#synthetic', '#diffusion', '#multimodal', '#3d'], 'emoji': 'ğŸ¨', 'ru': {'title': 'TriMM: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'TriMM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (RGB, RGBD, Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº). ĞĞ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¸Ñ… ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ 2D Ğ¸ 3D Ğ½Ğ°Ğ´Ğ·Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° TriMM Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ñ€Ğ¸Ğ¿Ğ»Ğ°Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¾Ğ².'}, 'en': {'title': 'TriMM: Uniting Multi-Modal Data for Superior 3D Asset Generation', 'desc': 'TriMM is a novel generative model designed for creating high-quality 3D assets by utilizing multiple types of data, including RGB images, RGBD data, and point clouds. It combines the strengths of these different modalities through a process called collaborative multi-modal coding, which allows the model to learn from the unique features of each data type. By incorporating both 2D and 3D supervision, TriMM enhances the robustness and performance of its multi-modal learning. The model demonstrates impressive results, generating detailed 3D assets even with limited training data, and shows potential for integrating additional multi-modal datasets.'}, 'zh': {'title': 'TriMMï¼šå¤šæ¨¡æ€é©±åŠ¨çš„é«˜è´¨é‡3Dç”Ÿæˆæ¨¡å‹', 'desc': 'TriMMæ˜¯ä¸€ç§å‰é¦ˆå¼çš„3DåŸç”Ÿç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆå¤šæ¨¡æ€æ•°æ®ï¼ˆå¦‚RGBã€RGBDå’Œç‚¹äº‘ï¼‰æ¥æå‡3Dèµ„äº§ç”Ÿæˆçš„è´¨é‡å’Œé²æ£’æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡åä½œå¤šæ¨¡æ€ç¼–ç ï¼Œç»“åˆäº†ä¸åŒæ¨¡æ€çš„ç‰¹å¾ï¼ŒåŒæ—¶ä¿ç•™äº†å®ƒä»¬ç‹¬ç‰¹çš„è¡¨ç¤ºä¼˜åŠ¿ã€‚TriMMè¿˜å¼•å…¥äº†è¾…åŠ©çš„2Då’Œ3Dç›‘ç£ï¼Œä»¥æé«˜å¤šæ¨¡æ€ç¼–ç çš„é²æ£’æ€§å’Œæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTriMMåœ¨ä½¿ç”¨å°‘é‡è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿä¸åŸºäºå¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ç«äº‰ï¼Œå±•ç¤ºäº†å…¶åœ¨3Dç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.21061', 'title': 'OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn\n  Dialogue with Large Language Models', 'url': 'https://huggingface.co/papers/2508.21061', 'abstract': 'OnGoal, an LLM chat interface with goal tracking, improves user efficiency and engagement in complex dialogues by providing real-time feedback and visualizations.  \t\t\t\t\tAI-generated summary \t\t\t\t As multi-turn dialogues with large language models (LLMs) grow longer and more complex, how can users better evaluate and review progress on their conversational goals? We present OnGoal, an LLM chat interface that helps users better manage goal progress. OnGoal provides real-time feedback on goal alignment through LLM-assisted evaluation, explanations for evaluation results with examples, and overviews of goal progression over time, enabling users to navigate complex dialogues more effectively. Through a study with 20 participants on a writing task, we evaluate OnGoal against a baseline chat interface without goal tracking. Using OnGoal, participants spent less time and effort to achieve their goals while exploring new prompting strategies to overcome miscommunication, suggesting tracking and visualizing goals can enhance engagement and resilience in LLM dialogues. Our findings inspired design implications for future LLM chat interfaces that improve goal communication, reduce cognitive load, enhance interactivity, and enable feedback to improve LLM performance.', 'score': 2, 'issue_id': 5611, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 28', 'zh': '8æœˆ28æ—¥'}, 'hash': 'c2cc9d13156cca49', 'authors': ['Adam Coscia', 'Shunan Guo', 'Eunyee Koh', 'Alex Endert'], 'affiliations': ['Adobe Research', 'Georgia Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.21061.jpg', 'data': {'categories': ['#interpretability', '#training', '#multimodal', '#alignment'], 'emoji': 'ğŸ¯', 'ru': {'title': 'OnGoal: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ†ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ… Ñ Ğ˜Ğ˜', 'desc': 'OnGoal - ÑÑ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Ñ‡Ğ°Ñ‚Ğ° Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ»ÑĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM. OnGoal Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ OnGoal Ñ‚Ñ€Ğ°Ñ‚Ğ¸Ğ»Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ ÑƒÑĞ¸Ğ»Ğ¸Ğ¹ Ğ½Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ†ĞµĞ»ĞµĞ¹, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ².'}, 'en': {'title': 'Enhancing Dialogue Efficiency with Goal Tracking in LLMs', 'desc': 'OnGoal is a chat interface that uses large language models (LLMs) to help users track their conversational goals during complex dialogues. It provides real-time feedback on how well users are aligning with their goals, along with explanations and examples to clarify evaluation results. A study showed that participants using OnGoal were able to achieve their writing tasks more efficiently and effectively, exploring new strategies to communicate better. The results suggest that goal tracking and visualization can significantly enhance user engagement and reduce cognitive load in LLM interactions.'}, 'zh': {'title': 'OnGoalï¼šæå‡å¯¹è¯æ•ˆç‡ä¸å‚ä¸æ„Ÿçš„æ™ºèƒ½åŠ©æ‰‹', 'desc': 'OnGoalæ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èŠå¤©ç•Œé¢ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·åœ¨å¤æ‚å¯¹è¯ä¸­æ›´å¥½åœ°è·Ÿè¸ªå’Œç®¡ç†ç›®æ ‡è¿›å±•ã€‚å®ƒé€šè¿‡å®æ—¶åé¦ˆå’Œå¯è§†åŒ–å±•ç¤ºï¼Œå¸®åŠ©ç”¨æˆ·è¯„ä¼°ä¸ç›®æ ‡çš„ä¸€è‡´æ€§ï¼Œå¹¶æä¾›è¯„ä¼°ç»“æœçš„è§£é‡Šå’Œç¤ºä¾‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨OnGoalçš„å‚ä¸è€…åœ¨å®Œæˆå†™ä½œä»»åŠ¡æ—¶ï¼Œæ‰€éœ€æ—¶é—´å’Œç²¾åŠ›æ˜¾è‘—å‡å°‘ï¼ŒåŒæ—¶èƒ½å¤Ÿæ¢ç´¢æ–°çš„æç¤ºç­–ç•¥ä»¥å…‹æœæ²Ÿé€šéšœç¢ã€‚è¿™è¡¨æ˜ï¼Œè·Ÿè¸ªå’Œå¯è§†åŒ–ç›®æ ‡å¯ä»¥å¢å¼ºç”¨æˆ·åœ¨LLMå¯¹è¯ä¸­çš„å‚ä¸æ„Ÿå’Œåº”å¯¹èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.17502', 'title': 'Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and\n  Voice', 'url': 'https://huggingface.co/papers/2508.17502', 'abstract': 'Social-MAE, an extended Contrastive Audio-Visual Masked Auto-Encoder, achieves state-of-the-art performance in multimodal emotion and laughter recognition through in-domain self-supervised pre-training.  \t\t\t\t\tAI-generated summary \t\t\t\t Human social behaviors are inherently multimodal necessitating the development of powerful audiovisual models for their perception. In this paper, we present Social-MAE, our pre-trained audiovisual Masked Autoencoder based on an extended version of Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE), which is pre-trained on audiovisual social data. Specifically, we modify CAV-MAE to receive a larger number of frames as input and pre-train it on a large dataset of human social interaction (VoxCeleb2) in a self-supervised manner. We demonstrate the effectiveness of this model by finetuning and evaluating the model on different social and affective downstream tasks, namely, emotion recognition, laughter detection and apparent personality estimation. The model achieves state-of-the-art results on multimodal emotion recognition and laughter recognition and competitive results for apparent personality estimation, demonstrating the effectiveness of in-domain self-supervised pre-training. Code and model weight are available here https://github.com/HuBohy/SocialMAE.', 'score': 1, 'issue_id': 5619, 'pub_date': '2025-08-24', 'pub_date_card': {'ru': '24 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 24', 'zh': '8æœˆ24æ—¥'}, 'hash': '53ed4781ccb4a962', 'authors': ['Hugo Bohy', 'Minh Tran', 'Kevin El Haddad', 'Thierry Dutoit', 'Mohammad Soleymani'], 'affiliations': ['Institute for Creative Technologies, University of Southern California, Los Angeles, CA, USA', 'Numediart Institute, ISIA Lab, University of Mons, Mons, Belgium'], 'pdf_title_img': 'assets/pdf/title_img/2508.17502.jpg', 'data': {'categories': ['#multimodal', '#training', '#dataset'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Social-MAE - ÑÑ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ¼ĞµÑ…Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ VoxCeleb2 Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Social-MAE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ¼ĞµÑ…Ğ°. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': "Unlocking Emotions: Social-MAE's Multimodal Mastery", 'desc': 'Social-MAE is a novel audiovisual model designed to recognize emotions and laughter by leveraging self-supervised learning on social interaction data. It builds upon the Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE) by allowing for a greater number of input frames, enhancing its ability to process complex social cues. The model is pre-trained on a large dataset, VoxCeleb2, which helps it learn meaningful representations of human behavior. After fine-tuning, Social-MAE achieves top performance in multimodal emotion and laughter recognition tasks, showcasing the benefits of in-domain self-supervised pre-training.'}, 'zh': {'title': 'Social-MAEï¼šå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«çš„æ–°çªç ´', 'desc': 'Social-MAEæ˜¯ä¸€ç§æ‰©å±•çš„å¯¹æ¯”éŸ³è§†é¢‘æ©è”½è‡ªç¼–ç å™¨ï¼Œä¸“æ³¨äºå¤šæ¨¡æ€æƒ…æ„Ÿå’Œç¬‘å£°è¯†åˆ«ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨å¤§è§„æ¨¡äººç±»ç¤¾äº¤äº’åŠ¨æ•°æ®é›†ï¼ˆVoxCeleb2ï¼‰ä¸Šè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œæå‡äº†éŸ³è§†é¢‘æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¯¹CAV-MAEè¿›è¡Œäº†æ”¹è¿›ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†æ›´å¤šå¸§çš„è¾“å…¥ï¼Œå¹¶åœ¨å¤šä¸ªç¤¾äº¤å’Œæƒ…æ„Ÿä»»åŠ¡ä¸Šè¿›è¡Œäº†å¾®è°ƒå’Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSocial-MAEåœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«å’Œç¬‘å£°è¯†åˆ«æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ•ˆæœï¼ŒéªŒè¯äº†è‡ªç›‘ç£é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (3)', '#agi', '#alignment (5)', '#architecture (2)', '#audio', '#benchmark (7)', '#cv (2)', '#data (1)', '#dataset (6)', '#diffusion (4)', '#ethics', '#games (3)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference', '#interpretability (1)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (8)', '#open_source (8)', '#optimization (8)', '#plp', '#rag (1)', '#reasoning (2)', '#rl (4)', '#rlhf (4)', '#robotics (1)', '#science (1)', '#security (2)', '#small_models', '#story_generation (1)', '#survey (1)', '#synthetic (3)', '#training (11)', '#transfer_learning', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-08-31 01:57',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-08-31 01:57')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-08-31 01:57')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    