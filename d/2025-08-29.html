
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 9 papers. August 29.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">29 августа</span> | <span id="title-articles-count">9 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-08-28.html">⬅️ <span id="prev-date">28.08</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-09-01.html">➡️ <span id="next-date">01.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-08.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '29 августа', 'en': 'August 29', 'zh': '8月29日'};
        let feedDateNext = {'ru': '01.09', 'en': '09/01', 'zh': '9月1日'};
        let feedDatePrev = {'ru': '28.08', 'en': '08/28', 'zh': '8月28日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.20751', 'title': 'Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable\n  Text-to-Image Reinforcement Learning', 'url': 'https://huggingface.co/papers/2508.20751', 'abstract': 'Pref-GRPO, a pairwise preference reward-based GRPO method, enhances text-to-image generation by mitigating reward hacking and improving stability, while UniGenBench provides a comprehensive benchmark for evaluating T2I models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements highlight the importance of GRPO-based reinforcement learning methods and benchmarking in enhancing text-to-image (T2I) generation. However, current methods using pointwise reward models (RM) for scoring generated images are susceptible to reward hacking. We reveal that this happens when minimal score differences between images are amplified after normalization, creating illusory advantages that drive the model to over-optimize for trivial gains, ultimately destabilizing the image generation process. To address this, we propose Pref-GRPO, a pairwise preference reward-based GRPO method that shifts the optimization objective from score maximization to preference fitting, ensuring more stable training. In Pref-GRPO, images are pairwise compared within each group using preference RM, and the win rate is used as the reward signal. Extensive experiments demonstrate that PREF-GRPO differentiates subtle image quality differences, providing more stable advantages and mitigating reward hacking. Additionally, existing T2I benchmarks are limited by coarse evaluation criteria, hindering comprehensive model assessment. To solve this, we introduce UniGenBench, a unified T2I benchmark comprising 600 prompts across 5 main themes and 20 subthemes. It evaluates semantic consistency through 10 primary and 27 sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our benchmarks uncover the strengths and weaknesses of both open and closed-source T2I models and validate the effectiveness of Pref-GRPO.', 'score': 26, 'issue_id': 5609, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': '865588150c1bd5fd', 'authors': ['Yibin Wang', 'Zhimin Li', 'Yuhang Zang', 'Yujie Zhou', 'Jiazi Bu', 'Chunyu Wang', 'Qinglin Lu', 'Cheng Jin', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Hunyuan, Tencent', 'Shanghai AI Lab', 'Shanghai Innovation Institute', 'Shanghai Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.20751.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#rl', '#survey', '#games'], 'emoji': '🎨', 'ru': {'title': 'Стабильная генерация изображений с Pref-GRPO и всесторонняя оценка с UniGenBench', 'desc': 'Статья представляет Pref-GRPO - метод генерации изображений по тексту, основанный на попарных предпочтениях и обучении с подкреплением. Этот подход решает проблему эксплуатации наград и повышает стабильность обучения по сравнению с существующими методами. Авторы также предлагают UniGenBench - комплексный бенчмарк для оценки моделей text-to-image, включающий 600 промптов и множество критериев. Эксперименты показывают эффективность Pref-GRPO и полезность UniGenBench для анализа сильных и слабых сторон различных моделей генерации изображений.'}, 'en': {'title': 'Enhancing T2I Generation with Stable Preference-Based Rewards', 'desc': 'The paper introduces Pref-GRPO, a new method that improves text-to-image (T2I) generation by using pairwise preference rewards instead of traditional pointwise rewards. This approach helps prevent reward hacking, where models exploit small score differences, leading to unstable image generation. By focusing on preference fitting, Pref-GRPO ensures that the training process is more stable and effective in distinguishing subtle differences in image quality. Additionally, the paper presents UniGenBench, a comprehensive benchmark for evaluating T2I models, which assesses semantic consistency across various themes and criteria, enhancing the evaluation of model performance.'}, 'zh': {'title': '提升文本到图像生成的稳定性与评估标准', 'desc': '本文提出了一种新的文本到图像生成方法，称为Pref-GRPO，它基于成对偏好奖励，旨在提高生成过程的稳定性并减少奖励黑客行为。传统的点对点奖励模型容易受到奖励黑客的影响，导致模型过度优化微小的得分差异。Pref-GRPO通过将优化目标从得分最大化转变为偏好拟合，确保了更稳定的训练过程。此外，本文还介绍了UniGenBench，这是一个全面的基准测试，用于评估文本到图像模型的性能，涵盖了600个提示和多个主题。'}}}, {'id': 'https://huggingface.co/papers/2508.20722', 'title': 'rStar2-Agent: Agentic Reasoning Technical Report', 'url': 'https://huggingface.co/papers/2508.20722', 'abstract': 'rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning, achieves state-of-the-art performance by efficiently handling complex problem-solving with advanced cognitive behaviors and minimal computational resources.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar.', 'score': 17, 'issue_id': 5608, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': '4e0a26cb3998d9dd', 'authors': ['Ning Shang', 'Yifei Liu', 'Yi Zhu', 'Li Lyna Zhang', 'Weijiang Xu', 'Xinyu Guan', 'Buze Zhang', 'Bingcheng Dong', 'Xudong Zhou', 'Bowen Zhang', 'Ying Xin', 'Ziming Miao', 'Scarlett Li', 'Fan Yang', 'Mao Yang'], 'affiliations': ['Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2508.20722.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#agents', '#alignment', '#optimization', '#rlhf', '#science'], 'emoji': '🧠', 'ru': {'title': 'Агентное RL создает математического гения с минимальными ресурсами', 'desc': 'rStar2-Agent - это модель машинного обучения с 14 миллиардами параметров, обученная с помощью агентного обучения с подкреплением для решения математических задач. Модель демонстрирует продвинутое когнитивное поведение, включая использование инструментов Python и анализ обратной связи от выполнения кода. Ключевые инновации включают эффективную инфраструктуру RL, алгоритм GRPO-RoC и эффективный рецепт обучения агента. rStar2-Agent достигает наилучших результатов на ряде математических тестов, превосходя более крупные модели.'}, 'en': {'title': 'Revolutionizing Math Reasoning with rStar2-Agent!', 'desc': 'The rStar2-Agent is a 14 billion parameter model designed for advanced math reasoning, utilizing agentic reinforcement learning to achieve top performance. It showcases sophisticated cognitive behaviors, such as careful consideration before executing Python code and self-reflection on code outcomes to enhance problem-solving. Key innovations include a robust RL infrastructure that minimizes computational costs, a novel GRPO-RoC algorithm that improves reasoning in coding environments, and a structured training approach that builds cognitive skills efficiently. This model not only excels in mathematical tasks but also generalizes well to other domains like alignment and scientific reasoning.'}, 'zh': {'title': 'rStar2-Agent：高效的数学推理与认知能力', 'desc': 'rStar2-Agent是一种14B的数学推理模型，采用代理强化学习进行训练，能够高效处理复杂问题，表现出色。该模型具备先进的认知行为，例如在使用Python编码工具前仔细思考，并根据代码执行反馈进行反思，从而自主探索和验证中间步骤。其成功得益于三项关键创新，包括高效的RL基础设施、GRPO-RoC算法和高效的代理训练方案，使得在有限的计算资源下也能实现卓越性能。rStar2-Agent在仅用510个RL步骤内，便在AIME24和AIME25上取得了80.6%和69.8%的平均通过率，超越了更大模型DeepSeek-R1。'}}}, {'id': 'https://huggingface.co/papers/2508.20404', 'title': 'AWorld: Orchestrating the Training Recipe for Agentic AI', 'url': 'https://huggingface.co/papers/2508.20404', 'abstract': "AWorld, an open-source system for large-scale agent-environment interaction, accelerates experience collection and enhances reinforcement learning, leading to significant improvements in agentic AI performance on complex benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The learning from practice paradigm is crucial for developing capable Agentic AI systems, yet it is severely hampered by inefficient experience generation, a bottleneck especially pronounced in complex benchmarks like GAIA. To address this, we introduce AWorld, an open-source system engineered for large-scale agent-environment interaction. By distributing tasks across a cluster, AWorld accelerates experience collection by 14.6x compared to standard single-node, sequential execution. This critical speedup makes extensive reinforcement learning practical and scalable. Leveraging this capability, we trained a Qwen3-32B-based agent that significantly outperforms its base model, increasing its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most challenging levels, our agent achieves a score of 16.33%, surpassing the performance of leading proprietary models. Our open-source system and resulting agent provide a practical blueprint for a complete agentic AI training pipeline, from efficient interaction to demonstrable model improvement.", 'score': 14, 'issue_id': 5610, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': 'c4a4ef0aa8be9928', 'authors': ['Chengyue Yu', 'Siyuan Lu', 'Chenyi Zhuang', 'Dong Wang', 'Qintong Wu', 'Zongyue Li', 'Runsheng Gan', 'Chunfeng Wang', 'Siqi Hou', 'Gaochi Huang', 'Wenlong Yan', 'Lifeng Hong', 'Aohui Xue', 'Yanfeng Wang', 'Jinjie Gu', 'David Tsai', 'Tao Lin'], 'affiliations': ['AWorld Team, Inclusion AI', 'Shanghai Innovation Institution', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2508.20404.jpg', 'data': {'categories': ['#benchmark', '#games', '#open_source', '#agents', '#rl', '#training'], 'emoji': '🚀', 'ru': {'title': 'AWorld: ускоряем обучение агентов ИИ в масштабе', 'desc': 'AWorld - это открытая система для крупномасштабного взаимодействия агентов и окружающей среды, которая ускоряет сбор опыта для обучения с подкреплением. Система распределяет задачи по кластеру, что позволяет ускорить генерацию опыта в 14,6 раз по сравнению со стандартным последовательным выполнением на одном узле. Используя AWorld, исследователи обучили агента на основе модели Qwen3-32B, который значительно превзошел базовую модель в бенчмарке GAIA. Этот подход демонстрирует практический способ создания полного конвейера обучения агентных систем искусственного интеллекта.'}, 'en': {'title': 'Accelerating Agentic AI with AWorld: 14.6x Faster Experience Collection!', 'desc': 'AWorld is an open-source platform designed to enhance the interaction between agents and their environments, significantly improving the efficiency of experience collection in reinforcement learning. By utilizing a distributed system, AWorld accelerates the process of gathering experiences by 14.6 times compared to traditional methods, which is crucial for training complex AI models. This system has been successfully used to train a Qwen3-32B-based agent, resulting in a notable increase in performance on the GAIA benchmark, with accuracy rising from 21.59% to 32.23%. AWorld not only streamlines the training process but also sets a new standard for developing capable agentic AI systems that can tackle challenging tasks effectively.'}, 'zh': {'title': 'AWorld：加速智能体学习的开源系统', 'desc': 'AWorld是一个开源系统，旨在加速大规模的智能体与环境的交互，从而提高强化学习的效率。通过在集群中分配任务，AWorld的经验收集速度比传统的单节点执行快了14.6倍。这种显著的加速使得在复杂基准上进行广泛的强化学习变得可行且可扩展。我们基于Qwen3-32B训练的智能体在GAIA基准测试中表现优异，准确率从21.59%提升至32.23%。'}}}, {'id': 'https://huggingface.co/papers/2508.18966', 'title': 'USO: Unified Style and Subject-Driven Generation via Disentangled and\n  Reward Learning', 'url': 'https://huggingface.co/papers/2508.18966', 'abstract': "USO, a unified model, achieves state-of-the-art performance in both style similarity and subject consistency by disentangling and re-composing content and style through a disentangled learning scheme and style reward-learning paradigm.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing literature typically treats style-driven and subject-driven generation as two disjoint tasks: the former prioritizes stylistic similarity, whereas the latter insists on subject consistency, resulting in an apparent antagonism. We argue that both objectives can be unified under a single framework because they ultimately concern the disentanglement and re-composition of content and style, a long-standing theme in style-driven research. To this end, we present USO, a Unified Style-Subject Optimized customization model. First, we construct a large-scale triplet dataset consisting of content images, style images, and their corresponding stylized content images. Second, we introduce a disentangled learning scheme that simultaneously aligns style features and disentangles content from style through two complementary objectives, style-alignment training and content-style disentanglement training. Third, we incorporate a style reward-learning paradigm denoted as SRL to further enhance the model's performance. Finally, we release USO-Bench, the first benchmark that jointly evaluates style similarity and subject fidelity across multiple metrics. Extensive experiments demonstrate that USO achieves state-of-the-art performance among open-source models along both dimensions of subject consistency and style similarity. Code and model: https://github.com/bytedance/USO", 'score': 12, 'issue_id': 5609, 'pub_date': '2025-08-26', 'pub_date_card': {'ru': '26 августа', 'en': 'August 26', 'zh': '8月26日'}, 'hash': '67afb07a9693ab76', 'authors': ['Shaojin Wu', 'Mengqi Huang', 'Yufeng Cheng', 'Wenxu Wu', 'Jiahe Tian', 'Yiming Luo', 'Fei Ding', 'Qian He'], 'affiliations': ['UXO Team, Intelligent Creation Lab, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2508.18966.jpg', 'data': {'categories': ['#story_generation', '#open_source', '#training', '#benchmark', '#dataset', '#cv'], 'emoji': '🎨', 'ru': {'title': 'Единая модель для оптимизации стиля и содержания изображений', 'desc': 'USO - это унифицированная модель для генерации изображений, которая достигает наилучших результатов как в стилевом сходстве, так и в сохранении содержания. Модель использует схему разделенного обучения для отделения содержания от стиля и их последующей рекомпозиции. USO применяет парадигму обучения с вознаграждением за стиль для улучшения производительности. Авторы также представили USO-Bench - первый бенчмарк для совместной оценки стилевого сходства и точности передачи содержания.'}, 'en': {'title': 'Unifying Style and Subject for Superior Image Generation', 'desc': 'The paper introduces USO, a unified model that excels in both style similarity and subject consistency by effectively separating and recombining content and style. It challenges the traditional view of treating style-driven and subject-driven generation as separate tasks, proposing that they can be integrated within a single framework. USO employs a disentangled learning scheme that aligns style features while disentangling content from style, supported by a style reward-learning paradigm to boost performance. The authors also present USO-Bench, a benchmark for evaluating both style and subject fidelity, demonstrating that USO achieves leading results in these areas.'}, 'zh': {'title': '统一模型USO：风格与主题的完美结合', 'desc': 'USO是一种统一模型，通过解耦和重新组合内容与风格，达到了风格相似性和主题一致性的最先进性能。该模型采用了解耦学习方案和风格奖励学习范式，解决了风格驱动和主题驱动生成任务之间的矛盾。我们构建了一个大规模的三元组数据集，并引入了风格对齐训练和内容风格解耦训练的互补目标。USO-Bench是第一个在多个指标上共同评估风格相似性和主题保真度的基准，实验结果表明USO在这两个维度上都表现出色。'}}}, {'id': 'https://huggingface.co/papers/2508.20374', 'title': 'TCIA: A Task-Centric Instruction Augmentation Method for Instruction\n  Finetuning', 'url': 'https://huggingface.co/papers/2508.20374', 'abstract': "Task Centric Instruction Augmentation (TCIA) enhances large language models' performance on specific tasks while maintaining general instruction-following ability.  \t\t\t\t\tAI-generated summary \t\t\t\t Diverse instruction data is vital for effective instruction tuning of large language models, as it enables the model to generalize across different types of inputs . Building such diversified instruction dataset is an essential step in this process. Existing approaches often leverage large language models to automatically explore and generate diverse instructions, ensuring both data diversity and quality. However, they tend to overlook an important factor in real-world applications: on-task relevance. In practice, only a few real-world applications require a truly general-purpose model; most benefit from task-specific knowledge tailored to their particular use case. Therefore, it is vital to develop instruction augmentation methods that not only maintain diversity but are also optimized for specific, real-world scenarios.   We thus introduce Task Centric Instruction Augmentation (TCIA), a framework that systematically expands instructions while preserving both diversity and task alignment. By representing instructions in a discrete query-constraints space, TCIA creates a rich set of task-relevant instructions and enables models to generalize to these task-specific instructions without sacrificing overall performance. Experiments show that TCIA improves open-source LLMs' performance by an average of 8.7% across four real-world, task-specific applications, and in some cases outperforming leading closed-source models. These improvements do not compromise general instruction-following ability, making TCIA a scalable and efficient solution for adapting LLMs to real-world, task-focused applications.", 'score': 11, 'issue_id': 5610, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': 'ff3ef7f9e0dd29da', 'authors': ['Simin Ma', 'Shujian Liu', 'Jun Tan', 'Yebowen Hu', 'Song Wang', 'Sathish Reddy Indurthi', 'Sanqiang Zhao', 'Liwei Wu', 'Jianbing Han', 'Kaiqiang Song'], 'affiliations': ['Zoom Communications Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2508.20374.jpg', 'data': {'categories': ['#data', '#optimization', '#open_source', '#training', '#dataset'], 'emoji': '🎯', 'ru': {'title': 'Точная настройка языковых моделей на конкретные задачи без потери универсальности', 'desc': 'Метод Task Centric Instruction Augmentation (TCIA) улучшает производительность больших языковых моделей на конкретных задачах, сохраняя при этом общую способность следовать инструкциям. TCIA систематически расширяет инструкции, сохраняя разнообразие и соответствие задаче, представляя их в дискретном пространстве запросов и ограничений. Эксперименты показывают, что TCIA улучшает производительность открытых языковых моделей в среднем на 8.7% для четырех реальных приложений. Этот метод не ухудшает общую способность модели следовать инструкциям, делая TCIA масштабируемым решением для адаптации языковых моделей к реальным задачам.'}, 'en': {'title': 'Enhancing Task-Specific Performance in Language Models with TCIA', 'desc': 'Task Centric Instruction Augmentation (TCIA) is a method designed to improve large language models (LLMs) by enhancing their performance on specific tasks while still allowing them to follow general instructions. It focuses on creating a diverse set of task-relevant instructions, which helps the model adapt to various real-world applications. By using a structured approach to represent instructions, TCIA ensures that the generated data is both diverse and aligned with specific tasks. Experiments demonstrate that TCIA significantly boosts the performance of open-source LLMs, making them more effective for targeted applications without losing their general instruction-following capabilities.'}, 'zh': {'title': '任务中心指令增强：提升模型性能的关键', 'desc': '任务中心指令增强（TCIA）是一种提升大型语言模型在特定任务上表现的方法，同时保持其通用指令跟随能力。该方法通过构建多样化的指令数据集，确保模型能够在不同输入类型之间进行有效的泛化。TCIA框架系统性地扩展指令，既保持多样性，又优化与特定任务的相关性。实验表明，TCIA在四个真实世界的任务特定应用中平均提高了8.7%的性能，且在某些情况下超越了领先的封闭源模型。'}}}, {'id': 'https://huggingface.co/papers/2508.21058', 'title': 'Mixture of Contexts for Long Video Generation', 'url': 'https://huggingface.co/papers/2508.21058', 'abstract': 'Long video generation is addressed by introducing a sparse attention routing module, Mixture of Contexts, to efficiently manage long-term memory and retrieval in diffusion transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.', 'score': 4, 'issue_id': 5608, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': 'b61607b2a4b6bc58', 'authors': ['Shengqu Cai', 'Ceyuan Yang', 'Lvmin Zhang', 'Yuwei Guo', 'Junfei Xiao', 'Ziyan Yang', 'Yinghao Xu', 'Zhenheng Yang', 'Alan Yuille', 'Leonidas Guibas', 'Maneesh Agrawala', 'Lu Jiang', 'Gordon Wetzstein'], 'affiliations': ['ByteDance', 'ByteDance Seed', 'CUHK', 'Johns Hopkins University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2508.21058.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#video', '#long_context', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Эффективная генерация длинных видео с помощью разреженного внимания', 'desc': 'Эта статья представляет новый метод для генерации длинных видео с использованием разреженного внимания в диффузионных трансформерах. Авторы предлагают модуль Mixture of Contexts (MoC), который эффективно управляет долговременной памятью и извлечением информации. MoC динамически выбирает наиболее информативные фрагменты для обработки, что позволяет сохранять согласованность на протяжении нескольких минут видео. Этот подход решает проблему квадратичной сложности самовнимания и делает возможным практическое обучение и синтез длинных видео.'}, 'en': {'title': 'Efficient Long Video Generation with Sparse Attention', 'desc': "This paper presents a new approach to generating long videos using a method called Mixture of Contexts (MoC), which enhances the efficiency of memory retrieval in diffusion transformers. The challenge of long video generation lies in managing long-term memory to retain important events without losing coherence. MoC addresses this by allowing the model to focus on a few key segments of information while maintaining essential context, thus optimizing the use of computational resources. As a result, this method not only improves the model's ability to generate consistent and coherent long videos but also makes the training process more efficient."}, 'zh': {'title': '高效生成长视频的记忆检索新方法', 'desc': '本文提出了一种稀疏注意力路由模块，称为上下文混合（Mixture of Contexts, MoC），用于高效管理扩散变换器中的长期记忆和检索，以生成长视频。长视频生成本质上是一个长期上下文记忆问题，模型需要在长时间范围内保留和检索重要事件。传统的自注意力机制在处理长序列时计算成本呈二次增长，导致内存和计算变得难以优化。MoC模块通过动态选择信息丰富的片段和必要的锚点，提供了一种有效的长期记忆检索引擎，从而提高了生成长视频的效率和一致性。'}}}, {'id': 'https://huggingface.co/papers/2508.20453', 'title': 'MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World\n  Tasks via MCP Servers', 'url': 'https://huggingface.co/papers/2508.20453', 'abstract': "MCP-Bench evaluates large language models on complex, multi-step tasks requiring tool use, coordination, and planning across various domains.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MCP-Bench, a benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand tool use, cross-tool coordination, precise parameter control, and planning/reasoning for solving tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search. Unlike prior API-based benchmarks, each MCP server provides a set of complementary tools designed to work together, enabling the construction of authentic, multi-step tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability to retrieve relevant tools from fuzzy instructions without explicit tool names, plan multi-hop execution trajectories for complex objectives, ground responses in intermediate tool outputs, and orchestrate cross-domain workflows - capabilities not adequately evaluated by existing benchmarks that rely on explicit tool specifications, shallow few-step workflows, and isolated domain operations. We propose a multi-faceted evaluation framework covering tool-level schema understanding and usage, trajectory-level planning, and task completion. Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code and data: https://github.com/Accenture/mcp-bench.", 'score': 4, 'issue_id': 5608, 'pub_date': '2025-08-28', 'pub_date_card': {'ru': '28 августа', 'en': 'August 28', 'zh': '8月28日'}, 'hash': 'dcd0de7d4f5a0eee', 'authors': ['Zhenting Wang', 'Qi Chang', 'Hemani Patel', 'Shashank Biju', 'Cheng-En Wu', 'Quan Liu', 'Aolin Ding', 'Alireza Rezazadeh', 'Ankit Shah', 'Yujia Bao', 'Eugene Siow'], 'affiliations': ['Center for Advanced AI, Accenture', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2508.20453.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#benchmark', '#agents'], 'emoji': '🧠', 'ru': {'title': 'MCP-Bench: Новый рубеж в оценке многозадачных языковых моделей', 'desc': 'MCP-Bench - это новый бенчмарк для оценки больших языковых моделей (LLM) на сложных многоэтапных задачах, требующих использования инструментов и планирования в различных областях. Он включает 28 серверов MCP с 250 инструментами в таких сферах как финансы, путешествия, научные вычисления и академический поиск. MCP-Bench оценивает способность моделей извлекать релевантные инструменты из нечетких инструкций, планировать многоэтапные траектории выполнения и координировать рабочие процессы между различными доменами. Эксперименты на 20 продвинутых LLM выявили устойчивые проблемы в решении задач MCP-Bench.'}, 'en': {'title': 'MCP-Bench: Evaluating LLMs on Complex Multi-Step Tasks', 'desc': "MCP-Bench is a new benchmark designed to assess large language models (LLMs) on complex tasks that require the use of multiple tools and careful planning. It connects LLMs to a variety of live servers that provide complementary tools across different domains, allowing for realistic multi-step task execution. The benchmark evaluates the models' abilities to understand fuzzy instructions, plan execution paths, and coordinate workflows across different domains. Results from testing 20 advanced LLMs show that there are still significant challenges in effectively completing these complex tasks."}, 'zh': {'title': '评估大型语言模型的多步骤任务能力', 'desc': 'MCP-Bench是一个用于评估大型语言模型（LLMs）的基准，专注于复杂的多步骤任务，这些任务需要工具使用、跨工具协调和规划能力。该基准基于模型上下文协议（MCP），连接到28个代表性的MCP服务器，涵盖250种工具，涉及金融、旅行、科学计算和学术搜索等多个领域。与以往的API基准不同，MCP服务器提供了一组互补工具，能够构建真实的多步骤任务，测试模型在模糊指令下检索相关工具的能力。实验表明，当前的LLMs在MCP-Bench中面临持续的挑战，特别是在工具理解、规划和任务完成方面。'}}}, {'id': 'https://huggingface.co/papers/2508.18633', 'title': 'ROSE: Remove Objects with Side Effects in Videos', 'url': 'https://huggingface.co/papers/2508.18633', 'abstract': "ROSE, a video inpainting framework using diffusion transformers, effectively removes objects and their side effects like shadows and reflections by leveraging synthetic data and differential masks.  \t\t\t\t\tAI-generated summary \t\t\t\t Video object removal has achieved advanced performance due to the recent success of video generative models. However, when addressing the side effects of objects, e.g., their shadows and reflections, existing works struggle to eliminate these effects for the scarcity of paired video data as supervision. This paper presents ROSE, termed Remove Objects with Side Effects, a framework that systematically studies the object's effects on environment, which can be categorized into five common cases: shadows, reflections, light, translucency and mirror. Given the challenges of curating paired videos exhibiting the aforementioned effects, we leverage a 3D rendering engine for synthetic data generation. We carefully construct a fully-automatic pipeline for data preparation, which simulates a large-scale paired dataset with diverse scenes, objects, shooting angles, and camera trajectories. ROSE is implemented as an video inpainting model built on diffusion transformer. To localize all object-correlated areas, the entire video is fed into the model for reference-based erasing. Moreover, additional supervision is introduced to explicitly predict the areas affected by side effects, which can be revealed through the differential mask between the paired videos. To fully investigate the model performance on various side effect removal, we presents a new benchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five special side effects for comprehensive evaluation. Experimental results demonstrate that ROSE achieves superior performance compared to existing video object erasing models and generalizes well to real-world video scenarios. The project page is https://rose2025-inpaint.github.io/.", 'score': 1, 'issue_id': 5609, 'pub_date': '2025-08-26', 'pub_date_card': {'ru': '26 августа', 'en': 'August 26', 'zh': '8月26日'}, 'hash': '844c1b2076f2c4b2', 'authors': ['Chenxuan Miao', 'Yutong Feng', 'Jianshu Zeng', 'Zixiang Gao', 'Hantang Liu', 'Yunfeng Yan', 'Donglian Qi', 'Xi Chen', 'Bin Wang', 'Hengshuang Zhao'], 'affiliations': ['KunByte AI', 'Peking University', 'The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.18633.jpg', 'data': {'categories': ['#synthetic', '#diffusion', '#benchmark', '#video', '#dataset'], 'emoji': '🎭', 'ru': {'title': 'ROSE: Интеллектуальное удаление объектов и их следов из видео', 'desc': 'ROSE - это фреймворк для удаления объектов из видео, использующий диффузионные трансформеры. Он эффективно устраняет объекты и их побочные эффекты, такие как тени и отражения, используя синтетические данные и дифференциальные маски. ROSE обучается на синтетическом наборе данных, созданном с помощью 3D-рендеринга, что позволяет моделировать различные сцены, объекты и эффекты. Модель способна локализовать все области, связанные с объектом, и предсказывать зоны, подверженные побочным эффектам, что делает её особенно эффективной для реальных видеосценариев.'}, 'en': {'title': 'ROSE: Mastering Object Removal with Side Effects in Videos', 'desc': 'ROSE is a video inpainting framework that uses diffusion transformers to effectively remove objects from videos along with their associated side effects, such as shadows and reflections. It addresses the challenge of limited paired video data by generating synthetic data through a 3D rendering engine, creating a diverse dataset for training. The framework categorizes object effects into five common cases and employs a reference-based erasing technique to localize affected areas. ROSE also introduces a new benchmark, ROSE-Bench, to evaluate its performance on various side effect removal tasks, demonstrating superior results compared to existing models.'}, 'zh': {'title': 'ROSE：去除物体及其副作用的创新框架', 'desc': 'ROSE是一个视频修复框架，利用扩散变换器有效去除视频中的物体及其副作用，如阴影和反射。该框架通过合成数据和差分掩码来解决现有方法在去除副作用时面临的数据稀缺问题。ROSE系统地研究了物体对环境的影响，并将其分类为五种常见情况：阴影、反射、光、半透明和镜面。实验结果表明，ROSE在视频物体去除任务中表现优越，并能很好地适应真实世界的视频场景。'}}}, {'id': 'https://huggingface.co/papers/2508.15228', 'title': 'Collaborative Multi-Modal Coding for High-Quality 3D Generation', 'url': 'https://huggingface.co/papers/2508.15228', 'abstract': 'TriMM, a feed-forward 3D-native generative model, integrates multi-modal data (RGB, RGBD, point clouds) to enhance 3D asset generation quality and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D content inherently encompasses multi-modal characteristics and can be projected into different modalities (e.g., RGB images, RGBD, and point clouds). Each modality exhibits distinct advantages in 3D asset modeling: RGB images contain vivid 3D textures, whereas point clouds define fine-grained 3D geometries. However, most existing 3D-native generative architectures either operate predominantly within single-modality paradigms-thus overlooking the complementary benefits of multi-modality data-or restrict themselves to 3D structures, thereby limiting the scope of available training datasets. To holistically harness multi-modalities for 3D modeling, we present TriMM, the first feed-forward 3D-native generative model that learns from basic multi-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM first introduces collaborative multi-modal coding, which integrates modality-specific features while preserving their unique representational strengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to raise the robustness and performance of multi-modal coding. 3) Based on the embedded multi-modal code, TriMM employs a triplane latent diffusion model to generate 3D assets of superior quality, enhancing both the texture and the geometric detail. Extensive experiments on multiple well-known datasets demonstrate that TriMM, by effectively leveraging multi-modality, achieves competitive performance with models trained on large-scale datasets, despite utilizing a small amount of training data. Furthermore, we conduct additional experiments on recent RGB-D datasets, verifying the feasibility of incorporating other multi-modal datasets into 3D generation.', 'score': 1, 'issue_id': 5609, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 августа', 'en': 'August 21', 'zh': '8月21日'}, 'hash': '125740d5cf6d2107', 'authors': ['Ziang Cao', 'Zhaoxi Chen', 'Liang Pan', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University, Singapore', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.15228.jpg', 'data': {'categories': ['#synthetic', '#diffusion', '#multimodal', '#3d'], 'emoji': '🎨', 'ru': {'title': 'TriMM: Мультимодальная революция в 3D-генерации', 'desc': 'TriMM - это новая модель генеративного 3D-моделирования, использующая мультимодальные данные (RGB, RGBD, облака точек). Она применяет коллаборативное мультимодальное кодирование для интеграции особенностей разных модальностей, сохраняя их уникальные преимущества. Модель использует вспомогательный 2D и 3D надзор для повышения надежности и производительности кодирования. На основе встроенного мультимодального кода TriMM применяет латентную диффузионную модель трипланов для генерации высококачественных 3D-активов.'}, 'en': {'title': 'TriMM: Uniting Multi-Modal Data for Superior 3D Asset Generation', 'desc': 'TriMM is a novel generative model designed for creating high-quality 3D assets by utilizing multiple types of data, including RGB images, RGBD data, and point clouds. It combines the strengths of these different modalities through a process called collaborative multi-modal coding, which allows the model to learn from the unique features of each data type. By incorporating both 2D and 3D supervision, TriMM enhances the robustness and performance of its multi-modal learning. The model demonstrates impressive results, generating detailed 3D assets even with limited training data, and shows potential for integrating additional multi-modal datasets.'}, 'zh': {'title': 'TriMM：多模态驱动的高质量3D生成模型', 'desc': 'TriMM是一种前馈式的3D原生生成模型，旨在通过整合多模态数据（如RGB、RGBD和点云）来提升3D资产生成的质量和鲁棒性。该模型通过协作多模态编码，结合了不同模态的特征，同时保留了它们独特的表示优势。TriMM还引入了辅助的2D和3D监督，以提高多模态编码的鲁棒性和性能。实验结果表明，TriMM在使用少量训练数据的情况下，能够与基于大规模数据集训练的模型竞争，展示了其在3D生成中的有效性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (3)', '#agi', '#alignment (1)', '#architecture (1)', '#audio', '#benchmark (5)', '#cv (1)', '#data (1)', '#dataset (3)', '#diffusion (3)', '#ethics', '#games (2)', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (2)', '#open_source (3)', '#optimization (4)', '#plp', '#rag', '#reasoning (2)', '#rl (3)', '#rlhf (1)', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation (1)', '#survey (1)', '#synthetic (2)', '#training (4)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-08-29 04:14',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-08-29 04:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-08-29 04:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    