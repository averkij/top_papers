{
    "date": {
        "ru": "4 апреля",
        "en": "April 4",
        "zh": "4月4日"
    },
    "time_utc": "2025-04-04 11:09",
    "weekday": 4,
    "issue_id": 3071,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.01990",
            "title": "Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems",
            "url": "https://huggingface.co/papers/2504.01990",
            "abstract": "The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This survey provides a comprehensive overview, framing intelligent agents within a modular, brain-inspired architecture that integrates principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we delve into the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities, and elucidating core components such as memory, world modeling, reward processing, and emotion-like systems. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms, including emerging AutoML and LLM-driven optimization strategies. Third, we examine collaborative and evolutionary multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures, highlighting parallels to human social dynamics. Finally, we address the critical imperative of building safe, secure, and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment.",
            "score": 59,
            "issue_id": 3069,
            "pub_date": "2025-03-31",
            "pub_date_card": {
                "ru": "31 марта",
                "en": "March 31",
                "zh": "3月31日"
            },
            "hash": "f72a29b6411b97b1",
            "authors": [
                "Bang Liu",
                "Xinfeng Li",
                "Jiayi Zhang",
                "Jinlin Wang",
                "Tanjin He",
                "Sirui Hong",
                "Hongzhang Liu",
                "Shaokun Zhang",
                "Kaitao Song",
                "Kunlun Zhu",
                "Yuheng Cheng",
                "Suyuchen Wang",
                "Xiaoqiang Wang",
                "Yuyu Luo",
                "Haibo Jin",
                "Peiyan Zhang",
                "Ollie Liu",
                "Jiaqi Chen",
                "Huan Zhang",
                "Zhaoyang Yu",
                "Haochen Shi",
                "Boyan Li",
                "Dekun Wu",
                "Fengwei Teng",
                "Xiaojun Jia",
                "Jiawei Xu",
                "Jinyu Xiang",
                "Yizhang Lin",
                "Tianming Liu",
                "Tongliang Liu",
                "Yu Su",
                "Huan Sun",
                "Glen Berseth",
                "Jianyun Nie",
                "Ian Foster",
                "Logan Ward",
                "Qingyun Wu",
                "Yu Gu",
                "Mingchen Zhuge",
                "Xiangru Tang",
                "Haohan Wang",
                "Jiaxuan You",
                "Chi Wang",
                "Jian Pei",
                "Qiang Yang",
                "Xiaoliang Qi",
                "Chenglin Wu"
            ],
            "affiliations": [
                "Argonne National Laboratory",
                "Canada CIFAR AI Chair",
                "Duke University",
                "Google DeepMind",
                "King Abdullah University of Science and Technology",
                "MetaGPT",
                "Microsoft Research Asia",
                "Mila - Quebec AI Institute",
                "Nanyang Technological University",
                "Penn State University",
                "Stanford University",
                "The Hong Kong Polytechnic University",
                "The Hong Kong University of Science and Technology",
                "The Ohio State University",
                "University of Georgia",
                "University of Illinois at Urbana-Champaign",
                "University of Southern California",
                "University of Sydney",
                "Université de Montréal",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01990.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#survey",
                    "#security",
                    "#ethics",
                    "#agi",
                    "#agents",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Интеллектуальные агенты нового поколения: от нейронауки к безопасному ИИ",
                    "desc": "Эта статья представляет собой всесторонний обзор интеллектуальных агентов на основе больших языковых моделей (LLM). Авторы предлагают модульную архитектуру агентов, вдохновленную человеческим мозгом, интегрируя принципы когнитивной науки, нейронауки и вычислительных исследований. Рассматриваются механизмы самосовершенствования агентов, их адаптивная эволюция и взаимодействие в многоагентных системах. Особое внимание уделяется вопросам безопасности, этики и надежности при разработке и внедрении таких систем искусственного интеллекта."
                },
                "en": {
                    "title": "Building Intelligent Agents: From Brain-Inspired Design to Safe AI Deployment",
                    "desc": "This paper explores the development of large language models (LLMs) and their role in creating advanced intelligent agents that can reason, perceive, and act in various environments. It presents a modular architecture inspired by the human brain, detailing how cognitive, perceptual, and operational modules correspond to brain functions like memory and emotion. The paper also discusses how these agents can improve themselves through adaptive learning and optimization techniques, including AutoML. Finally, it emphasizes the importance of ensuring that AI systems are safe, ethical, and reliable for real-world applications."
                },
                "zh": {
                    "title": "智能体的未来：从大脑启发到安全应用",
                    "desc": "本文探讨了大型语言模型（LLMs）在人工智能领域的变革性影响，强调了智能体的设计、评估和持续改进所面临的复杂挑战。我们将智能体框架置于模块化的、受大脑启发的架构中，结合了认知科学、神经科学和计算研究的原则。文章分为四个部分，首先分析智能体的模块化基础，映射其认知、感知和操作模块与人类大脑功能的相似性。接着讨论自我增强和适应性进化机制，最后强调构建安全、可靠和有益的人工智能系统的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02826",
            "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing",
            "url": "https://huggingface.co/papers/2504.02826",
            "abstract": "Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at https://github.com/PhoenixZ810/RISEBench.",
            "score": 45,
            "issue_id": 3064,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "dbb1c07cd5a01838",
            "authors": [
                "Xiangyu Zhao",
                "Peiyuan Zhang",
                "Kexian Tang",
                "Hao Li",
                "Zicheng Zhang",
                "Guangtao Zhai",
                "Junchi Yan",
                "Hua Yang",
                "Xue Yang",
                "Haodong Duan"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02826.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#benchmark",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "RISEBench: Новый рубеж в оценке визуального редактирования с рассуждениями",
                    "desc": "RISEBench - это новый эталонный тест для оценки визуального редактирования с учетом рассуждений в мультимодальных моделях. Он фокусируется на четырех типах рассуждений: временном, причинно-следственном, пространственном и логическом. Тест оценивает понимание инструкций, сохранение внешнего вида и визуальную правдоподобность с помощью как человеческих оценщиков, так и LLM-судей. Эксперименты показали, что даже современные модели, такие как GPT-4, испытывают трудности с задачами логического рассуждения."
                },
                "en": {
                    "title": "RISEBench: Advancing Reasoning in Visual Editing",
                    "desc": "This paper introduces RISEBench, a new benchmark designed to evaluate Reasoning-Informed Visual Editing (RISE) in Large Multi-modality Models (LMMs). It identifies challenges in visual editing, such as following complex instructions and maintaining appearance consistency. RISEBench categorizes reasoning into four types: Temporal, Causal, Spatial, and Logical, and provides a framework for assessing these reasoning types through both human and model evaluations. The findings indicate that even advanced models like GPT-4o-Native struggle with logical reasoning, suggesting a need for further research in this area."
                },
                "zh": {
                    "title": "推理驱动的视觉编辑新基准",
                    "desc": "大型多模态模型（LMMs）在视觉理解和生成方面取得了显著进展，但在通用视觉编辑中仍面临挑战，尤其是在遵循复杂指令、保持外观一致性和支持灵活输入格式方面。为了解决这一问题，我们引入了RISEBench，这是第一个用于评估推理驱动视觉编辑（RISE）的基准。RISEBench专注于四种关键推理类型：时间推理、因果推理、空间推理和逻辑推理。我们的实验表明，尽管GPT-4o-Native在性能上显著优于其他模型，但在逻辑推理任务上仍然存在困难，显示出这一领域仍需深入探索。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02782",
            "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation",
            "url": "https://huggingface.co/papers/2504.02782",
            "abstract": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval.",
            "score": 28,
            "issue_id": 3064,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "5346697bd326eed4",
            "authors": [
                "Zhiyuan Yan",
                "Junyan Ye",
                "Weijia Li",
                "Zilong Huang",
                "Shenghai Yuan",
                "Xiangyang He",
                "Kaiqing Lin",
                "Jun He",
                "Conghui He",
                "Li Yuan"
            ],
            "affiliations": [
                "Peking University, Shenzhen Graduate School",
                "Rabbitpre AI",
                "Shanghai AI Laboratory",
                "Shenzhen University",
                "Sun Yat-sen University",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02782.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#benchmark",
                    "#architecture",
                    "#diffusion",
                    "#interpretability",
                    "#optimization",
                    "#hallucinations"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "GPT-4o: Новый рубеж в генерации и редактировании изображений с помощью ИИ",
                    "desc": "Статья представляет первый оценочный бенчмарк (GPT-ImgEval) для модели GPT-4o от OpenAI, анализирующий ее способности в генерации и редактировании изображений. Исследование оценивает качество генерации, мастерство редактирования и семантический синтез на основе мировых знаний, демонстрируя превосходство GPT-4o над существующими методами. Авторы также предполагают, что архитектура GPT-4o включает авторегрессионную модель в сочетании с диффузионной головкой для декодирования изображений. Кроме того, статья анализирует ограничения GPT-4o, сравнивает ее с Gemini 2.0 Flash и обсуждает вопросы безопасности, связанные с выходными данными модели."
                },
                "en": {
                    "title": "Unleashing the Power of GPT-4o in Image Generation and Editing",
                    "desc": "This paper evaluates the performance of OpenAI's GPT-4o model in image generation and editing using a new benchmark called GPT-ImgEval. The evaluation focuses on three key areas: the quality of generated images, the model's ability to edit images, and its understanding of semantic context. Results show that GPT-4o outperforms existing models in both image generation and editing, while also demonstrating strong reasoning capabilities. The paper also explores the model's architecture and limitations, providing insights for future research in image generation."
                },
                "zh": {
                    "title": "GPT-4o：图像生成与编辑的新突破",
                    "desc": "本论文介绍了OpenAI的GPT-4o模型在图像生成和编辑方面的最新突破。我们提出了一个名为GPT-ImgEval的评估基准，定量和定性地分析了GPT-4o在生成质量、编辑能力和知识推理等三个关键维度的表现。研究表明，GPT-4o在图像生成控制和输出质量上显著优于现有方法，并展示了卓越的知识推理能力。此外，我们还探讨了GPT-4o的架构，并识别了其在图像生成中常见的合成伪影和局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02587",
            "title": "Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme",
            "url": "https://huggingface.co/papers/2504.02587",
            "abstract": "Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research.",
            "score": 18,
            "issue_id": 3063,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "58300c3a6e30995f",
            "authors": [
                "Yan Ma",
                "Steffi Chern",
                "Xuyang Shen",
                "Yiran Zhong",
                "Pengfei Liu"
            ],
            "affiliations": [
                "Fudan University",
                "Generative Artificial Intelligence Lab (GAIR)",
                "Minimax",
                "SII",
                "Shanghai Jiao Tong University (SJTU)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02587.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Прозрачное обучение с подкреплением для визуально-языковых моделей",
                    "desc": "Эта статья представляет новый подход к обучению с подкреплением (RL) для визуально-языковых моделей (VLM). Авторы предлагают прозрачную и воспроизводимую систему для применения RL в VLM, включающую четырехэтапный конвейер. Они также вводят стандартизированную схему оценки для анализа динамики обучения и рефлексивного поведения моделей. Эксперименты показывают, что RL превосходит обычное обучение с учителем в задачах визуального рассуждения и обобщения."
                },
                "en": {
                    "title": "Reinforcement Learning Revolutionizes Vision-Language Models!",
                    "desc": "This paper presents a new framework for applying reinforcement learning (RL) to vision-language models (VLMs), addressing issues of reproducibility and accessibility in existing methods. The authors propose a simple four-step pipeline that can be easily validated across different models and datasets. They also introduce a standardized evaluation scheme to better assess training dynamics and reflective behaviors in VLMs. The experiments reveal that RL outperforms supervised fine-tuning in generalization, highlighting the importance of response length and reflection in visual reasoning tasks."
                },
                "zh": {
                    "title": "建立可重复的强化学习框架",
                    "desc": "强化学习（RL）在提升大型语言模型的推理能力方面展现出强大的潜力，并正在积极扩展到视觉语言模型（VLMs）。然而，现有的RL应用往往依赖于复杂的框架，限制了可重复性和可访问性，同时缺乏标准化的评估协议，使得结果比较和训练动态解释变得困难。本文提出了一个透明的、从零开始的RL框架，提供了一个经过多个模型和数据集验证的最小功能四步流程。此外，提出了一种标准化的评估方案，以评估训练动态和反思行为。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02436",
            "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2504.02436",
            "abstract": "This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.",
            "score": 14,
            "issue_id": 3063,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "86b46513a72dbd76",
            "authors": [
                "Zhengcong Fei",
                "Debang Li",
                "Di Qiu",
                "Jiahua Wang",
                "Yikun Dou",
                "Rui Wang",
                "Jingtao Xu",
                "Mingyuan Fan",
                "Guibin Chen",
                "Yang Li",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Skywork AI, Kunlun Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02436.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#video",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Контролируемая генерация видео из отдельных элементов",
                    "desc": "SkyReels-A2 - это система генерации видео, способная собирать произвольные визуальные элементы в синтезированные видео на основе текстовых подсказок. Она использует модель совместного встраивания изображений и текста для сохранения согласованности элементов и глобальной связности. Авторы оптимизировали процесс вывода для скорости и стабильности, а также создали специальный набор данных для оценки. SkyReels-A2 является первой моделью с открытым исходным кодом коммерческого уровня для генерации видео из элементов (E2V)."
                },
                "en": {
                    "title": "SkyReels-A2: Mastering Video Generation with Element Control",
                    "desc": "This paper introduces SkyReels-A2, a framework for generating videos by combining various visual elements based on text descriptions. The main challenge is to keep each visual element true to its reference image while ensuring that the overall scene looks coherent and natural. To tackle this, the authors developed a data pipeline for training the model with specific triplets of prompts, references, and videos, and created a new image-text joint embedding model to enhance the generative process. The results show that SkyReels-A2 can produce high-quality, diverse videos with precise control over the elements, marking a significant advancement in the field of controllable video generation."
                },
                "zh": {
                    "title": "SkyReels-A2：可控视频生成的新突破",
                    "desc": "本文介绍了SkyReels-A2，一个可控的视频生成框架，能够根据文本提示将任意视觉元素（如角色、物体、背景）组合成合成视频，同时保持与每个元素的参考图像的一致性。我们将这一任务称为元素到视频（E2V），其主要挑战在于保持每个参考元素的真实性，确保场景的连贯性，以及实现自然的输出。为了解决这些问题，我们首先设计了一个全面的数据管道，以构建提示-参考-视频三元组用于模型训练。实验表明，我们的框架能够生成多样化、高质量的视频，并实现精确的元素控制。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02398",
            "title": "Scaling Analysis of Interleaved Speech-Text Language Models",
            "url": "https://huggingface.co/papers/2504.02398",
            "abstract": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. They predict that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper we answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling-dynamics are significantly different than textless-SLMs, suggesting one should allocate notably more of the compute budget for increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest, that our scaled up model achieves comparable performance with leading models on speech semantic metrics while using less compute and data than other approaches. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims.",
            "score": 11,
            "issue_id": 3067,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "c03d1b64b7e6e276",
            "authors": [
                "Gallil Maimon",
                "Michael Hassid",
                "Amit Roth",
                "Yossi Adi"
            ],
            "affiliations": [
                "Department of Computer Science and Engineering, Hebrew University of Jerusalem"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02398.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#audio",
                    "#open_source",
                    "#synthetic",
                    "#transfer_learning",
                    "#training"
                ],
                "emoji": "🎙️",
                "ru": {
                    "title": "Эффективное масштабирование речевых моделей через инициализацию текстовыми моделями",
                    "desc": "Исследование показывает, что речевые языковые модели (SLM), инициализированные с помощью предобученных текстовых моделей, масштабируются более эффективно, чем чисто речевые модели. Авторы провели анализ масштабирования таких интерлейвных SLM, обучив несколько десятков моделей и изучив тенденции. Результаты указывают на то, что при таком подходе следует выделять больше вычислительных ресурсов на увеличение размера модели, а не на увеличение объема обучающих данных. Масштабированная модель авторов достигает сопоставимой производительности с ведущими моделями при использовании меньшего количества вычислений и данных."
                },
                "en": {
                    "title": "Interleaved SLMs: Efficient Scaling for Speech Models!",
                    "desc": "This paper investigates the efficiency of interleaved Speech Language Models (SLMs) compared to traditional textless SLMs. It finds that interleaved SLMs, which leverage pre-trained Text Language Models (TextLMs), require less compute and data while achieving competitive performance on speech tasks. The authors conduct a scaling analysis that reveals distinct scaling dynamics, suggesting a need for more compute allocation towards model size rather than training data. Additionally, the study highlights the importance of synthetic data and various TextLM families in enhancing the performance of SLMs."
                },
                "zh": {
                    "title": "交错SLM：更高效的扩展之路",
                    "desc": "本论文探讨了交错语音语言模型（SLM）的扩展效率。研究表明，交错SLM在计算资源的使用上比无文本SLM更为高效。我们发现，交错SLM的扩展动态与无文本SLM显著不同，建议在增加模型规模时应更多地分配计算预算。最终，经过扩展的模型在语音语义指标上表现出与领先模型相当的性能，同时使用的计算和数据量更少。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00502",
            "title": "ShortV: Efficient Multimodal Large Language Models by Freezing Visual\n  Tokens in Ineffective Layers",
            "url": "https://huggingface.co/papers/2504.00502",
            "abstract": "Multimodal Large Language Models (MLLMs) suffer from high computational costs due to their massive size and the large number of visual tokens. In this paper, we investigate layer-wise redundancy in MLLMs by introducing a novel metric, Layer Contribution (LC), which quantifies the impact of a layer's transformations on visual and text tokens, respectively. The calculation of LC involves measuring the divergence in model output that results from removing the layer's transformations on the specified tokens. Our pilot experiment reveals that many layers of MLLMs exhibit minimal contribution during the processing of visual tokens. Motivated by this observation, we propose ShortV, a training-free method that leverages LC to identify ineffective layers, and freezes visual token updates in these layers. Experiments show that ShortV can freeze visual token in approximately 60\\% of the MLLM layers, thereby dramatically reducing computational costs related to updating visual tokens. For example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while maintaining superior performance. The code will be publicly available at https://github.com/icip-cas/ShortV",
            "score": 10,
            "issue_id": 3067,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "1b236225c1d92fd7",
            "authors": [
                "Qianhao Yuan",
                "Qingyu Zhang",
                "Yanjiang Liu",
                "Jiawei Chen",
                "Yaojie Lu",
                "Hongyu Lin",
                "Jia Zheng",
                "Xianpei Han",
                "Le Sun"
            ],
            "affiliations": [
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00502.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#inference"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Оптимизация MLLM: меньше вычислений, та же эффективность",
                    "desc": "Исследователи представили новый метод оценки вклада слоев в мультимодальных больших языковых моделях (MLLM) с помощью метрики Layer Contribution (LC). Они обнаружили, что многие слои MLLM минимально влияют на обработку визуальных токенов. На основе этого наблюдения был разработан метод ShortV, который идентифицирует неэффективные слои и замораживает обновления визуальных токенов в них. ShortV позволяет заморозить визуальные токены примерно в 60% слоев MLLM, значительно снижая вычислительные затраты без потери производительности."
                },
                "en": {
                    "title": "Optimizing MLLMs: Freeze the Unnecessary Layers!",
                    "desc": "This paper addresses the high computational costs associated with Multimodal Large Language Models (MLLMs) by analyzing layer-wise redundancy. It introduces a new metric called Layer Contribution (LC) to measure how much each layer affects the processing of visual and text tokens. The findings indicate that many layers contribute little to the processing of visual tokens, allowing for optimization. The authors propose a method called ShortV, which identifies and freezes these ineffective layers, resulting in significant reductions in computational costs while preserving model performance."
                },
                "zh": {
                    "title": "优化多模态模型，降低计算成本",
                    "desc": "多模态大型语言模型（MLLMs）由于其庞大的规模和大量的视觉标记，面临着高计算成本的问题。本文提出了一种新颖的度量标准——层贡献（Layer Contribution，LC），用于量化模型中各层对视觉和文本标记的影响。通过计算去除某层变换后模型输出的差异，LC能够评估该层的贡献。实验表明，许多层在处理视觉标记时的贡献很小，因此我们提出了ShortV方法，能够识别并冻结这些无效层，从而显著降低计算成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02542",
            "title": "Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation",
            "url": "https://huggingface.co/papers/2504.02542",
            "abstract": "Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce ACTalker, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict.",
            "score": 8,
            "issue_id": 3064,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "fa93ea3aeacd0dbc",
            "authors": [
                "Fa-Ting Hong",
                "Zunnan Xu",
                "Zixiang Zhou",
                "Jun Zhou",
                "Xiu Li",
                "Qin Lin",
                "Qinglin Lu",
                "Dan Xu"
            ],
            "affiliations": [
                "HKUST",
                "Tencent",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02542.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Гибкий синтез говорящей головы с множественным контролем",
                    "desc": "ACTalker - это новая модель для синтеза видео с говорящей головой, использующая диффузионный подход. Она поддерживает как мультимодальное, так и одномодальное управление генерацией видео. Модель использует параллельную структуру mamba с несколькими ветвями для обработки различных управляющих сигналов. ACTalker применяет механизм маскирования для предотвращения конфликтов между разными модальностями управления."
                },
                "en": {
                    "title": "ACTalker: Multi-Signal Control for Natural Talking Head Synthesis",
                    "desc": "This paper presents ACTalker, a novel framework for generating talking head videos that can be controlled by multiple signals simultaneously. Unlike traditional methods that rely on a single control modality, ACTalker employs a parallel mamba structure with multiple branches, each dedicated to a specific facial region. A gate mechanism allows for flexible control, ensuring that different driving signals can manipulate facial features without interference. The introduction of a mask-drop strategy further enhances this capability, enabling independent control of facial regions and resulting in more natural and coordinated video outputs."
                },
                "zh": {
                    "title": "多信号控制的对话头像生成新方法",
                    "desc": "本文介绍了一种名为ACTalker的端到端视频扩散框架，旨在生成虚拟头像的对话视频。该方法支持多信号和单信号控制，克服了现有方法的局限性。通过设计并行的mamba结构，允许不同的驱动信号控制面部的特定区域，并使用门控机制实现灵活控制。实验结果表明，ACTalker能够生成自然的面部视频，并且能够无冲突地整合多种驱动信号。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02507",
            "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training",
            "url": "https://huggingface.co/papers/2504.02507",
            "abstract": "Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes. These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping. Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention. In this work, we propose ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time. Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms. At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise. Our code is available at: https://github.com/bluorion-com/ZClip.",
            "score": 7,
            "issue_id": 3065,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "290019150fe5b4c9",
            "authors": [
                "Abhay Kumar",
                "Louis Owen",
                "Nilabhra Roy Chowdhury",
                "Fabian Güra"
            ],
            "affiliations": [
                "BluOrion"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02507.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "ZClip: адаптивное ограничение градиентов для стабильного обучения языковых моделей",
                    "desc": "В статье представлен новый алгоритм адаптивного ограничения градиентов под названием ZClip для обучения больших языковых моделей (LLM). ZClip динамически корректирует порог ограничения на основе статистических свойств норм градиентов во времени. Алгоритм использует обнаружение аномалий на основе z-оценки для выявления и смягчения больших всплесков градиентов. ZClip помогает предотвратить вредные всплески потерь, не мешая сходимости в остальных случаях."
                },
                "en": {
                    "title": "ZClip: Smart Gradient Clipping for Stable LLM Training",
                    "desc": "This paper addresses the challenges of training large language models (LLMs) by introducing ZClip, an adaptive gradient clipping algorithm. Traditional methods often fail to manage gradient instability and loss spikes effectively, leading to inefficient training and the need for manual adjustments. ZClip improves upon these methods by dynamically adjusting the clipping threshold based on the statistical behavior of gradient norms, allowing for a more responsive training process. By using z-score-based anomaly detection, ZClip prevents harmful loss spikes while maintaining the overall convergence of the model."
                },
                "zh": {
                    "title": "自适应梯度裁剪，提升训练稳定性",
                    "desc": "在训练大型语言模型时，常常会遇到梯度不稳定和损失峰值等问题，这可能导致灾难性的发散。传统的梯度裁剪技术无法有效解决这些问题，因为它们依赖于固定的阈值或启发式方法，导致学习效率低下。我们提出了一种名为ZClip的自适应梯度裁剪算法，它根据梯度范数的统计特性动态调整裁剪阈值。ZClip通过基于z-score的异常检测来识别和减轻大梯度峰值，从而防止损失峰值，同时不干扰收敛过程。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02012",
            "title": "Instruction-Guided Autoregressive Neural Network Parameter Generation",
            "url": "https://huggingface.co/papers/2504.02012",
            "abstract": "Learning to generate neural network parameters conditioned on task descriptions and architecture specifications is pivotal for advancing model adaptability and transfer learning. Existing methods especially those based on diffusion models suffer from limited scalability to large architectures, rigidity in handling varying network depths, and disjointed parameter generation that undermines inter-layer coherence. In this work, we propose IGPG (Instruction Guided Parameter Generation), an autoregressive framework that unifies parameter synthesis across diverse tasks and architectures. IGPG leverages a VQ-VAE and an autoregressive model to generate neural network parameters, conditioned on task instructions, dataset, and architecture details. By autoregressively generating neural network weights' tokens, IGPG ensures inter-layer coherence and enables efficient adaptation across models and datasets. Operating at the token level, IGPG effectively captures complex parameter distributions aggregated from a broad spectrum of pretrained models. Extensive experiments on multiple vision datasets demonstrate that IGPG consolidates diverse pretrained models into a single, flexible generative framework. The synthesized parameters achieve competitive or superior performance relative to state-of-the-art methods, especially in terms of scalability and efficiency when applied to large architectures. These results underscore ICPG potential as a powerful tool for pretrained weight retrieval, model selection, and rapid task-specific fine-tuning.",
            "score": 5,
            "issue_id": 3065,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "cbdd586ccd2b682d",
            "authors": [
                "Soro Bedionita",
                "Bruno Andreis",
                "Song Chong",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai, South Korea",
                "KAIST AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02012.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#cv",
                    "#optimization",
                    "#training",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "IGPG: Универсальный генератор параметров нейросетей для различных задач и архитектур",
                    "desc": "IGPG - это авторегрессивная система для генерации параметров нейронных сетей, основанная на VQ-VAE и авторегрессивной модели. Она создает параметры на основе описания задачи, датасета и архитектуры сети, обеспечивая согласованность между слоями. IGPG работает на уровне токенов, эффективно охватывая сложные распределения параметров из широкого спектра предобученных моделей. Эксперименты показывают, что IGPG превосходит современные методы по масштабируемости и эффективности для крупных архитектур."
                },
                "en": {
                    "title": "IGPG: Unifying Neural Network Parameter Generation for Enhanced Adaptability",
                    "desc": "This paper introduces IGPG (Instruction Guided Parameter Generation), a new framework for generating neural network parameters based on task descriptions and architecture specifications. Unlike previous methods, IGPG uses an autoregressive approach that ensures coherence between layers and adapts efficiently to different models and datasets. By employing a VQ-VAE and generating weights at the token level, IGPG captures complex parameter distributions from various pretrained models. The results show that IGPG outperforms existing methods in scalability and efficiency, making it a valuable tool for model adaptation and fine-tuning."
                },
                "zh": {
                    "title": "IGPG：灵活的神经网络参数生成工具",
                    "desc": "本文提出了一种名为IGPG（指令引导参数生成）的自回归框架，旨在生成神经网络参数，以适应不同的任务描述和架构规范。IGPG通过结合VQ-VAE和自回归模型，能够在多种任务和架构中统一参数合成，确保层间一致性。该方法在生成神经网络权重时，采用了基于token的生成方式，有效捕捉来自多种预训练模型的复杂参数分布。实验结果表明，IGPG在多个视觉数据集上表现出色，尤其在大规模架构的可扩展性和效率方面，超越了现有的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02119",
            "title": "Efficient Model Selection for Time Series Forecasting via LLMs",
            "url": "https://huggingface.co/papers/2504.02119",
            "abstract": "Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting.",
            "score": 4,
            "issue_id": 3063,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "7c31e20ce0a7813b",
            "authors": [
                "Wang Wei",
                "Tiankai Yang",
                "Hongjie Chen",
                "Ryan A. Rossi",
                "Yue Zhao",
                "Franck Dernoncourt",
                "Hoda Eldardiry"
            ],
            "affiliations": [
                "Adobe Research San Jose, CA, USA",
                "Adobe Research Seattle, WA, USA",
                "Department of Computer Science University of South California Los Angeles, CA, USA",
                "Department of Computer Science Virginia Tech Blacksburg, VA, USA",
                "Dolby Labs Atlanta, GA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02119.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "LLM как эффективный инструмент выбора моделей в прогнозировании временных рядов",
                    "desc": "Статья предлагает использовать большие языковые модели (LLM) для автоматизации выбора моделей в прогнозировании временных рядов. Этот подход устраняет необходимость в предварительно созданных матрицах производительности, опираясь на внутренние знания и способности рассуждения LLM. Эксперименты с моделями LLaMA, GPT и Gemini показывают, что предложенный метод превосходит традиционные техники мета-обучения и эвристические базовые линии. Результаты подчеркивают потенциал LLM в эффективном выборе моделей для прогнозирования временных рядов."
                },
                "en": {
                    "title": "Revolutionizing Model Selection with Large Language Models",
                    "desc": "This paper addresses the challenge of model selection in time series forecasting, which usually requires evaluating many models across different datasets. The authors introduce a novel approach that uses Large Language Models (LLMs) to automate this selection process without needing costly performance matrices. By leveraging the reasoning abilities of LLMs, their method simplifies the model selection task and reduces computational costs. Experimental results show that this approach outperforms traditional meta-learning methods and heuristic techniques, highlighting the effectiveness of LLMs in this domain."
                },
                "zh": {
                    "title": "利用大型语言模型优化时间序列预测的模型选择",
                    "desc": "本研究探讨了时间序列预测中的模型选择问题，传统方法需要在多个数据集上进行广泛的性能评估。我们提出了一种利用大型语言模型（LLMs）作为轻量级替代方案的方法，避免了构建昂贵的性能矩阵。通过与LLaMA、GPT和Gemini的广泛实验，我们的方法在性能上超越了传统的元学习技术和启发式基线，同时显著降低了计算开销。这些结果强调了LLMs在时间序列预测中高效模型选择的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00891",
            "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning",
            "url": "https://huggingface.co/papers/2504.00891",
            "abstract": "Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, a generative process reward model that performs explicit Chain-of-Thought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and a rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as a critic model for policy model refinement. This work establishes a new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in https://ryanliu112.github.io/GenPRM.",
            "score": 4,
            "issue_id": 3066,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 апреля",
                "en": "April 1",
                "zh": "4月1日"
            },
            "hash": "b22a54f43f9d7a89",
            "authors": [
                "Jian Zhao",
                "Runze Liu",
                "Kaiyan Zhang",
                "Zhimu Zhou",
                "Junqi Gao",
                "Dong Li",
                "Jiafei Lyu",
                "Zhouyi Qian",
                "Biqing Qi",
                "Xiu Li",
                "Bowen Zhou"
            ],
            "affiliations": [
                "BUPT",
                "Harbin Institute of Technology",
                "Shanghai AI Laboratory",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00891.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#math",
                    "#reasoning",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "GenPRM: Новая парадигма контроля процесса рассуждений в больших языковых моделях",
                    "desc": "Статья представляет GenPRM - генеративную модель вознаграждения процесса, которая использует рассуждения по цепочке мыслей и верификацию кода для оценки каждого шага рассуждений. Авторы предлагают методы относительной оценки прогресса и синтеза обоснований для получения качественных обучающих данных. Эксперименты показывают, что GenPRM значительно превосходит предыдущие модели PRM на нескольких задачах математических рассуждений, используя всего 23 тысячи обучающих примеров. Модель также демонстрирует способность выступать в роли критика для улучшения политики в больших языковых моделях."
                },
                "en": {
                    "title": "GenPRM: Elevating LLMs with Generative Process Reward Models",
                    "desc": "This paper presents GenPRM, a generative process reward model designed to improve the performance of large language models (LLMs) by addressing key challenges faced by existing process reward models (PRMs). GenPRM utilizes Chain-of-Thought (CoT) reasoning and incorporates code verification to enhance the quality of its judgments at each reasoning step. The authors introduce a novel method called Relative Progress Estimation (RPE) to generate high-quality supervision labels and rationale data, leading to significant performance improvements on various reasoning tasks. Experimental results demonstrate that GenPRM outperforms previous PRMs and shows strong capabilities as a critic model for refining policy models, establishing a new approach for process supervision in LLMs."
                },
                "zh": {
                    "title": "生成性过程奖励模型：提升LLMs的新范式",
                    "desc": "最近，大型语言模型（LLMs）的进展表明，使用过程奖励模型（PRMs）作为验证器可以提升LLMs的性能。然而，当前的PRMs面临三个主要挑战：有限的过程监督和泛化能力、依赖于标量值预测而未利用LLMs的生成能力，以及无法扩展PRMs的测试时间计算。本文提出了GenPRM，这是一种生成性过程奖励模型，通过代码验证进行明确的思维链推理，然后对每个推理步骤进行判断。实验结果表明，GenPRM在多个数学推理任务上显著优于之前的PRMs，展示了其作为政策模型精炼的批评模型的强大能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.22444",
            "title": "Scaling Laws in Scientific Discovery with AI and Robot Scientists",
            "url": "https://huggingface.co/papers/2503.22444",
            "abstract": "Scientific discovery is poised for rapid advancement through advanced robotics and artificial intelligence. Current scientific practices face substantial limitations as manual experimentation remains time-consuming and resource-intensive, while multidisciplinary research demands knowledge integration beyond individual researchers' expertise boundaries. Here, we envision an autonomous generalist scientist (AGS) concept combines agentic AI and embodied robotics to automate the entire research lifecycle. This system could dynamically interact with both physical and virtual environments while facilitating the integration of knowledge across diverse scientific disciplines. By deploying these technologies throughout every research stage -- spanning literature review, hypothesis generation, experimentation, and manuscript writing -- and incorporating internal reflection alongside external feedback, this system aims to significantly reduce the time and resources needed for scientific discovery. Building on the evolution from virtual AI scientists to versatile generalist AI-based robot scientists, AGS promises groundbreaking potential. As these autonomous systems become increasingly integrated into the research process, we hypothesize that scientific discovery might adhere to new scaling laws, potentially shaped by the number and capabilities of these autonomous systems, offering novel perspectives on how knowledge is generated and evolves. The adaptability of embodied robots to extreme environments, paired with the flywheel effect of accumulating scientific knowledge, holds the promise of continually pushing beyond both physical and intellectual frontiers.",
            "score": 4,
            "issue_id": 3069,
            "pub_date": "2025-03-28",
            "pub_date_card": {
                "ru": "28 марта",
                "en": "March 28",
                "zh": "3月28日"
            },
            "hash": "c2d75e49d08273c1",
            "authors": [
                "Pengsong Zhang",
                "Heng Zhang",
                "Huazhe Xu",
                "Renjun Xu",
                "Zhenting Wang",
                "Cong Wang",
                "Animesh Garg",
                "Zhibin Li",
                "Arash Ajoudani",
                "Xinyu Liu"
            ],
            "affiliations": [
                "Georgia Tech",
                "Harvard University",
                "Istituto Italiano di Tecnologia",
                "Rutgers University",
                "Tsinghua University",
                "Universita di Genova",
                "University College of London",
                "University of Toronto",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.22444.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#robotics",
                    "#agi",
                    "#science"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Автономный учёный-универсал: революция в научных открытиях",
                    "desc": "Статья представляет концепцию автономного учёного-универсала (AGS), объединяющего агентный ИИ и воплощенную робототехнику для автоматизации всего цикла научных исследований. Система AGS может взаимодействовать с физической и виртуальной средой, интегрируя знания из различных научных дисциплин. Предполагается, что внедрение AGS значительно сократит время и ресурсы, необходимые для научных открытий. Авторы предполагают, что с развитием таких систем научные открытия могут подчиняться новым законам масштабирования, зависящим от количества и возможностей автономных систем."
                },
                "en": {
                    "title": "Revolutionizing Science with Autonomous Generalist Scientists",
                    "desc": "This paper introduces the concept of an Autonomous Generalist Scientist (AGS) that combines artificial intelligence and robotics to automate the entire scientific research process. The AGS can interact with both physical and virtual environments, facilitating knowledge integration across various scientific fields. By automating tasks such as literature review, hypothesis generation, experimentation, and manuscript writing, the AGS aims to significantly reduce the time and resources required for scientific discovery. The authors suggest that as these systems become more integrated into research, they could change how knowledge is generated and evolve, potentially leading to new scaling laws in scientific discovery."
                },
                "zh": {
                    "title": "自主科学家：加速科学发现的未来",
                    "desc": "这篇论文提出了一种自主通用科学家（AGS）的概念，结合了智能代理AI和具身机器人，旨在自动化整个研究生命周期。该系统能够动态与物理和虚拟环境互动，并促进不同科学学科之间的知识整合。通过在文献回顾、假设生成、实验和论文写作等研究阶段应用这些技术，AGS希望显著减少科学发现所需的时间和资源。随着这些自主系统越来越多地融入研究过程，科学发现可能会遵循新的规模法则，提供关于知识生成和演变的新视角。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.01871",
            "title": "Interpreting Emergent Planning in Model-Free Reinforcement Learning",
            "url": "https://huggingface.co/papers/2504.01871",
            "abstract": "We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by Guez et al. (2019), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in the agent's representations) have a causal effect on the agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, which is important given the recent trend of emergent planning and reasoning capabilities in LLMs through RL",
            "score": 3,
            "issue_id": 3069,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "bf12d8cbe28bb942",
            "authors": [
                "Thomas Bush",
                "Stephen Chung",
                "Usman Anwar",
                "Adrià Garriga-Alonso",
                "David Krueger"
            ],
            "affiliations": [
                "FAR AI",
                "Mila, University of Montreal",
                "University of Cambridge"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.01871.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#benchmark",
                    "#rl",
                    "#games",
                    "#agents"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Агенты обучения с подкреплением без модели способны к планированию",
                    "desc": "Исследователи представили первое механистическое доказательство того, что агенты обучения с подкреплением без модели могут научиться планировать. Они применили методологию, основанную на интерпретируемости концепций, к агенту без модели в игре Sokoban. Было продемонстрировано, что агент DRC использует выученные концептуальные представления для внутреннего формулирования планов, которые предсказывают долгосрочные эффекты действий и влияют на выбор действий. Методология включала зондирование релевантных для планирования концепций, исследование формирования планов в представлениях агента и проверку причинно-следственной связи обнаруженных планов с поведением агента через вмешательства."
                },
                "en": {
                    "title": "Unveiling Planning in Model-Free Reinforcement Learning Agents",
                    "desc": "This paper provides evidence that model-free reinforcement learning agents can learn to plan by using concept-based interpretability. The authors focus on a model-free agent called DRC, which learns to create internal plans that predict the outcomes of its actions in the Sokoban environment. They explore how the agent identifies relevant concepts, forms plans, and how these plans affect its behavior through interventions. The study reveals that as the agent develops planning capabilities, it also shows improved performance with additional computational resources, resembling advanced search algorithms."
                },
                "zh": {
                    "title": "无模型强化学习中的规划能力",
                    "desc": "本文首次提供了无模型强化学习代理能够学习规划的机制性证据。我们通过在Sokoban这一常用基准上应用基于概念的可解释性方法，展示了DRC代理如何利用学习到的概念表示来内部制定计划。具体而言，代理能够预测行动对环境的长期影响，并影响行动选择。我们的研究表明，代理的计划形成与其行为之间存在因果关系，并且这种计划的出现与代理在测试时利用额外计算能力的能力相吻合。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02821",
            "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models",
            "url": "https://huggingface.co/papers/2504.02821",
            "abstract": "Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity in vision representations. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons while also exhibiting hierarchical representations that align well with expert-defined structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that applying SAEs to intervene on a CLIP vision encoder, directly steer output from multimodal LLMs (e.g., LLaVA) without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised approach for enhancing both the interpretability and control of VLMs.",
            "score": 2,
            "issue_id": 3069,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "d4568f020b5f2eca",
            "authors": [
                "Mateusz Pach",
                "Shyamgopal Karthik",
                "Quentin Bouniot",
                "Serge Belongie",
                "Zeynep Akata"
            ],
            "affiliations": [
                "Helmholtz Munich",
                "Munich Center of Machine Learning",
                "Munich Data Science Institute",
                "Technical University of Munich",
                "University of Copenhagen",
                "University of Tubingen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02821.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Разреженные автоэнкодеры раскрывают потенциал мультимодальных моделей",
                    "desc": "Исследователи применили разреженные автоэнкодеры (SAE) к мультимодальным моделям, объединяющим зрение и язык (VLM). Эксперименты показали, что SAE значительно улучшают моносемантичность отдельных нейронов в VLM и формируют иерархические представления, соответствующие экспертным таксономиям. Авторы продемонстрировали возможность управления выходными данными мультимодальных языковых моделей путем вмешательства в энкодер зрения CLIP с помощью SAE. Результаты подчеркивают эффективность SAE для повышения интерпретируемости и управляемости VLM."
                },
                "en": {
                    "title": "Enhancing Vision-Language Models with Sparse Autoencoders",
                    "desc": "This paper explores the use of Sparse Autoencoders (SAEs) to improve the interpretability and steerability of Vision-Language Models (VLMs) like CLIP. The authors present a framework to assess how well these models represent single concepts, known as monosemanticity. Their experiments show that SAEs can enhance the clarity of individual neurons in VLMs and align these representations with established expert categories. Importantly, they demonstrate that SAEs can influence the output of multimodal language models without changing the original model architecture."
                },
                "zh": {
                    "title": "稀疏自编码器提升视觉-语言模型的可解释性与操控性",
                    "desc": "稀疏自编码器（SAEs）最近被证明可以提高大型语言模型（LLMs）的可解释性和可操控性。本文将SAEs的应用扩展到视觉-语言模型（VLMs），如CLIP，并引入了一个全面的框架来评估视觉表示的单义性。实验结果表明，在VLMs上训练的SAEs显著增强了单个神经元的单义性，并展示了与专家定义结构（如iNaturalist分类法）良好对齐的层次表示。最重要的是，我们证明了将SAEs应用于CLIP视觉编码器，可以直接操控多模态LLMs（如LLaVA）的输出，而无需对基础模型进行任何修改。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02495",
            "title": "Inference-Time Scaling for Generalist Reward Modeling",
            "url": "https://huggingface.co/papers/2504.02495",
            "abstract": "Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that proper learning methods could enable effective inference-time scalability. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the inference-time scalability of generalist RM, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, we adopt pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, we propose Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in DeepSeek-GRM models. Furthermore, for effective inference-time scaling, we use parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced.",
            "score": 2,
            "issue_id": 3071,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "07408fa4b72ccb6c",
            "authors": [
                "Zijun Liu",
                "Peiyi Wang",
                "Runxin Xu",
                "Shirong Ma",
                "Chong Ruan",
                "Peng Li",
                "Yang Liu",
                "Yu Wu"
            ],
            "affiliations": [
                "DeepSeek-AI",
                "Dept. of Computer Sci. & Tech., Tsinghua University",
                "Institute for AI Industry Research (AIR), Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02495.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#rl",
                    "#rlhf",
                    "#reasoning",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Масштабируемое моделирование вознаграждений для LLM с помощью самокритики",
                    "desc": "Эта статья исследует улучшение моделирования вознаграждений (RM) для крупных языковых моделей (LLM) с использованием обучения с подкреплением (RL). Авторы предлагают метод Self-Principled Critique Tuning (SPCT) для улучшения генерации вознаграждений и масштабируемости моделей. Они также вводят мета-RM для руководства процессом голосования при масштабировании во время вывода. Результаты показывают, что SPCT значительно улучшает качество и масштабируемость генеративных моделей вознаграждения (GRM), превосходя существующие методы в различных тестах RM."
                },
                "en": {
                    "title": "Enhancing Language Models with Scalable Reward Learning",
                    "desc": "This paper explores advancements in reinforcement learning (RL) for enhancing large language models (LLMs) by focusing on reward modeling (RM) for general queries. The authors introduce a novel approach called Self-Principled Critique Tuning (SPCT) that improves the generation of reward signals, enabling better inference-time scalability. They also propose a pointwise generative reward modeling (GRM) technique that allows flexibility in handling different input types. Empirical results demonstrate that SPCT significantly enhances the quality and scalability of GRMs, outperforming existing methods while addressing challenges in generalist reward systems."
                },
                "zh": {
                    "title": "提升语言模型推理能力的强化学习方法",
                    "desc": "强化学习（RL）在大规模语言模型（LLMs）的后训练中得到了广泛应用。本文探讨了如何通过改进奖励建模（RM）来提高一般查询的推理时间可扩展性，并提出了自我原则批评调优（SPCT）方法，以促进GRM中的可扩展奖励生成行为。我们采用点对点生成奖励建模（GRM），以适应不同输入类型并实现推理时间的可扩展性。实验结果表明，SPCT显著提高了GRM的质量和可扩展性，超越了现有方法和模型。"
                }
            }
        }
    ],
    "link_prev": "2025-04-03.html",
    "link_next": "2025-04-07.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "03.04",
        "en": "04/03",
        "zh": "4月3日"
    },
    "short_date_next": {
        "ru": "07.04",
        "en": "04/07",
        "zh": "4月7日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 3,
        "#cv": 4,
        "#rl": 3,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 5,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 1,
        "#agi": 2,
        "#games": 1,
        "#interpretability": 3,
        "#reasoning": 6,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 9,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的基准测试，称为RISEBench，用于评估大型多模态模型（LMMs）在视觉编辑中的推理能力。RISEBench专注于四种关键推理类型：时间、因果、空间和逻辑推理。通过高质量的测试案例和评估框架，RISEBench能够评估指令推理、外观一致性和视觉合理性。实验结果显示，尽管GPT-4o-Native在某些方面表现出色，但在逻辑推理任务上仍存在挑战。作者希望RISEBench能为未来的研究提供基础性的见解，并承诺不断扩展和完善这一基准测试。",
        "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing",
        "pinyin": "Zhè piān wén zhāng jiè shào le yī zhǒng xīn de jī zhǔn cè shì, chēng wéi RISEBench, yòng yú píng gū dà xíng duō mó shù mó xíng (LMMs) zài shì jué biān jí zhōng de tuī lǐ néng lì. RISEBench zhuān zhú yú sì zhǒng guǎn jiàn tuī lǐ lèi xíng: shí jiān, yīn guǒ, kòng jiān hé luó jì tuī lǐ. Tōng guò gāo zhì lìang de cè shì àn lì hé píng gū kuàng jià, RISEBench néng gǒu píng gū zhǐ lǐng tuī lǐ, wài guǎn yī zhì xíng hé shì jué hé lǐ xíng. Shí yàn jié guǒ xiǎn shì, jǐn guǎn GPT-4o-Native zài mǒu xiē fāng miàn biǎo xiàn chū sè, dàn zài luó jì tuī lǐ rèn wù shàng réng cún zài tiǎo zhàn. Zuò zhě xī wàng RISEBench néng wéi wèi lái de yán jiū tí gōng jī chǔ xìng de jiàn shè, bìng chéng nuò dàn kuò zhǎn hé wán shàn zhè yī jī zhǔn cè shì.",
        "vocab": "[{'word': '基准', 'pinyin': 'jīzhǔn', 'trans': 'benchmark'},\n{'word': '测试', 'pinyin': 'cèshì', 'trans': 'test'},\n{'word': '称为', 'pinyin': 'chēngwéi', 'trans': 'called'},\n{'word': '用于', 'pinyin': 'yòngyú', 'trans': 'used for'},\n{'word': '评估', 'pinyin': 'pínggū', 'trans': 'evaluate'},\n{'word': '大型', 'pinyin': 'dàxíng', 'trans': 'large-scale'},\n{'word': '多模态', 'pinyin': 'duōmóshuài', 'trans': 'multimodal'},\n{'word': '模型', 'pinyin': 'móxíng', 'trans': 'model'},\n{'word': '视觉', 'pinyin': 'shìjué', 'trans': 'visual'},\n{'word': '编辑', 'pinyin': 'biānjí', 'trans': 'editing'},\n{'word': '推理', 'pinyin': 'tuīlǐ', 'trans': 'reasoning'},\n{'word': '能力', 'pinyin': 'nénglì', 'trans': 'ability'},\n{'word': '专注', 'pinyin': 'zhuānzhù', 'trans': 'focus on'},\n{'word': '关键', 'pinyin': 'guǎnjiàn', 'trans': 'key'},\n{'word': '类型', 'pinyin': 'lèixíng', 'trans': 'type'},\n{'word': '时间', 'pinyin': 'shíjiān', 'trans': 'time'},\n{'word': '因果', 'pinyin': 'yīnguǒ', 'trans': 'causality'},\n{'word': '空间', 'pinyin': 'kōngjiān', 'trans': 'space'},\n{'word': '逻辑', 'pinyin': 'luóji', 'trans': 'logic'},\n{'word': '高质量', 'pinyin': 'gāozhìliàng', 'trans': 'high-quality'},\n{'word': '案例', 'pinyin': 'ànlì', 'trans': 'case'},\n{'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'},\n{'word': '指令', 'pinyin': 'zhǐlìng', 'trans': 'instruction'},\n{'word': '一致性', 'pinyin': 'yīzhìxìng', 'trans': 'consistency'},\n{'word': '合理性', 'pinyin': 'hélǐxìng', 'trans': 'reasonableness'},\n{'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'},\n{'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'},\n{'word': '显示', 'pinyin': 'xiǎnshì', 'trans': 'show'},\n{'word': '尽管', 'pinyin': 'jǐnguǎn', 'trans': 'although'},\n{'word': '出色', 'pinyin': 'chūsè', 'trans': 'outstanding'},\n{'word': '但在', 'pinyin': 'dànzài', 'trans': 'but in'},\n{'word': '任务', 'pinyin': 'rènwù', 'trans': 'task'},\n{'word': '仍存在', 'pinyin': 'réngcúnzài', 'trans': 'still exist'},\n{'word': '挑战', 'pinyin': 'tiǎozhàn', 'trans': 'challenge'},\n{'word': '作者', 'pinyin': 'zuòzhě', 'trans': 'author'},\n{'word': '希望', 'pinyin': 'xīwàng', 'trans': 'hope'},\n{'word': '提供', 'pinyin': 'tígōng', 'trans': 'provide'},\n{'word': '基础性', 'pinyin': 'jīchǔxìng', 'trans': 'foundational'},\n{'word': '见解', 'pinyin': 'jiànjiě', 'trans': 'insight'},\n{'word': '承诺', 'pinyin': 'chéngnuò', 'trans': 'promise'},\n{'word': '不断', 'pinyin': 'bùduàn', 'trans': 'continuously'},\n{'word': '扩展', 'pinyin': 'kuòzhǎn', 'trans': 'expand'},\n{'word': '完善', 'pinyin': 'wánshàn', 'trans': 'perfect'}]",
        "trans": "This article introduces a new benchmark test called RISEBench, designed to evaluate the reasoning capabilities of large multimodal models (LMMs) in visual editing. RISEBench focuses on four key types of reasoning: temporal, causal, spatial, and logical reasoning. Through high-quality test cases and an evaluation framework, RISEBench can assess instruction reasoning, appearance consistency, and visual plausibility. Experimental results indicate that while GPT-4o-Native performs well in certain aspects, it still faces challenges in logical reasoning tasks. The authors hope that RISEBench will provide foundational insights for future research and are committed to continually expanding and refining this benchmark test.",
        "update_ts": "2025-04-04 09:11"
    }
}