{
    "date": {
        "ru": "4 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 4",
        "zh": "4æœˆ4æ—¥"
    },
    "time_utc": "2025-04-04 05:11",
    "weekday": 4,
    "issue_id": 3066,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.02782",
            "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation",
            "url": "https://huggingface.co/papers/2504.02782",
            "abstract": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval.",
            "score": 16,
            "issue_id": 3064,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "5346697bd326eed4",
            "authors": [
                "Zhiyuan Yan",
                "Junyan Ye",
                "Weijia Li",
                "Zilong Huang",
                "Shenghai Yuan",
                "Xiangyang He",
                "Kaiqing Lin",
                "Jun He",
                "Conghui He",
                "Li Yuan"
            ],
            "affiliations": [
                "Peking University, Shenzhen Graduate School",
                "Rabbitpre AI",
                "Shanghai AI Laboratory",
                "Shenzhen University",
                "Sun Yat-sen University",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02782.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#benchmark",
                    "#architecture",
                    "#diffusion",
                    "#interpretability",
                    "#optimization",
                    "#hallucinations"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "GPT-4o: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº (GPT-ImgEval) Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-4o Ğ¾Ñ‚ OpenAI, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ĞµĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¼Ğ°ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ GPT-4o Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° GPT-4o Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ GPT-4o, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞµĞµ Ñ Gemini 2.0 Flash Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Unleashing the Power of GPT-4o in Image Generation and Editing",
                    "desc": "This paper evaluates the performance of OpenAI's GPT-4o model in image generation and editing using a new benchmark called GPT-ImgEval. The evaluation focuses on three key areas: the quality of generated images, the model's ability to edit images, and its understanding of semantic context. Results show that GPT-4o outperforms existing models in both image generation and editing, while also demonstrating strong reasoning capabilities. The paper also explores the model's architecture and limitations, providing insights for future research in image generation."
                },
                "zh": {
                    "title": "GPT-4oï¼šå›¾åƒç”Ÿæˆä¸ç¼–è¾‘çš„æ–°çªç ´",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†OpenAIçš„GPT-4oæ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢çš„æœ€æ–°çªç ´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºGPT-ImgEvalçš„è¯„ä¼°åŸºå‡†ï¼Œå®šé‡å’Œå®šæ€§åœ°åˆ†æäº†GPT-4oåœ¨ç”Ÿæˆè´¨é‡ã€ç¼–è¾‘èƒ½åŠ›å’ŒçŸ¥è¯†æ¨ç†ç­‰ä¸‰ä¸ªå…³é”®ç»´åº¦çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGPT-4oåœ¨å›¾åƒç”Ÿæˆæ§åˆ¶å’Œè¾“å‡ºè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å“è¶Šçš„çŸ¥è¯†æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†GPT-4oçš„æ¶æ„ï¼Œå¹¶è¯†åˆ«äº†å…¶åœ¨å›¾åƒç”Ÿæˆä¸­å¸¸è§çš„åˆæˆä¼ªå½±å’Œå±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02587",
            "title": "Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme",
            "url": "https://huggingface.co/papers/2504.02587",
            "abstract": "Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research.",
            "score": 9,
            "issue_id": 3063,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "58300c3a6e30995f",
            "authors": [
                "Yan Ma",
                "Steffi Chern",
                "Xuyang Shen",
                "Yiran Zhong",
                "Pengfei Liu"
            ],
            "affiliations": [
                "Fudan University",
                "Generative Artificial Intelligence Lab (GAIR)",
                "Minimax",
                "SII",
                "Shanghai Jiao Tong University (SJTU)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02587.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½ÑƒÑ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ RL Ğ² VLM, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Reinforcement Learning Revolutionizes Vision-Language Models!",
                    "desc": "This paper presents a new framework for applying reinforcement learning (RL) to vision-language models (VLMs), addressing issues of reproducibility and accessibility in existing methods. The authors propose a simple four-step pipeline that can be easily validated across different models and datasets. They also introduce a standardized evaluation scheme to better assess training dynamics and reflective behaviors in VLMs. The experiments reveal that RL outperforms supervised fine-tuning in generalization, highlighting the importance of response length and reflection in visual reasoning tasks."
                },
                "zh": {
                    "title": "å»ºç«‹å¯é‡å¤çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
                    "desc": "å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œå¹¶æ­£åœ¨ç§¯ææ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RLåº”ç”¨å¾€å¾€ä¾èµ–äºå¤æ‚çš„æ¡†æ¶ï¼Œé™åˆ¶äº†å¯é‡å¤æ€§å’Œå¯è®¿é—®æ€§ï¼ŒåŒæ—¶ç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼Œä½¿å¾—ç»“æœæ¯”è¾ƒå’Œè®­ç»ƒåŠ¨æ€è§£é‡Šå˜å¾—å›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€æ˜çš„ã€ä»é›¶å¼€å§‹çš„RLæ¡†æ¶ï¼Œæä¾›äº†ä¸€ä¸ªç»è¿‡å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†éªŒè¯çš„æœ€å°åŠŸèƒ½å››æ­¥æµç¨‹ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ ‡å‡†åŒ–çš„è¯„ä¼°æ–¹æ¡ˆï¼Œä»¥è¯„ä¼°è®­ç»ƒåŠ¨æ€å’Œåæ€è¡Œä¸ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02436",
            "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers",
            "url": "https://huggingface.co/papers/2504.02436",
            "abstract": "This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.",
            "score": 5,
            "issue_id": 3063,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "86b46513a72dbd76",
            "authors": [
                "Zhengcong Fei",
                "Debang Li",
                "Di Qiu",
                "Jiahua Wang",
                "Yikun Dou",
                "Rui Wang",
                "Jingtao Xu",
                "Mingyuan Fan",
                "Guibin Chen",
                "Yang Li",
                "Yahui Zhou"
            ],
            "affiliations": [
                "Skywork AI, Kunlun Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02436.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#multimodal",
                    "#video",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "SkyReels-A2 - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. SkyReels-A2 ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² (E2V)."
                },
                "en": {
                    "title": "SkyReels-A2: Mastering Video Generation with Element Control",
                    "desc": "This paper introduces SkyReels-A2, a framework for generating videos by combining various visual elements based on text descriptions. The main challenge is to keep each visual element true to its reference image while ensuring that the overall scene looks coherent and natural. To tackle this, the authors developed a data pipeline for training the model with specific triplets of prompts, references, and videos, and created a new image-text joint embedding model to enhance the generative process. The results show that SkyReels-A2 can produce high-quality, diverse videos with precise control over the elements, marking a significant advancement in the field of controllable video generation."
                },
                "zh": {
                    "title": "SkyReels-A2ï¼šå¯æ§è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†SkyReels-A2ï¼Œä¸€ä¸ªå¯æ§çš„è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºå°†ä»»æ„è§†è§‰å…ƒç´ ï¼ˆå¦‚è§’è‰²ã€ç‰©ä½“ã€èƒŒæ™¯ï¼‰ç»„åˆæˆåˆæˆè§†é¢‘ï¼ŒåŒæ—¶ä¿æŒä¸æ¯ä¸ªå…ƒç´ çš„å‚è€ƒå›¾åƒçš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å°†è¿™ä¸€ä»»åŠ¡ç§°ä¸ºå…ƒç´ åˆ°è§†é¢‘ï¼ˆE2Vï¼‰ï¼Œå…¶ä¸»è¦æŒ‘æˆ˜åœ¨äºä¿æŒæ¯ä¸ªå‚è€ƒå…ƒç´ çš„çœŸå®æ€§ï¼Œç¡®ä¿åœºæ™¯çš„è¿è´¯æ€§ï¼Œä»¥åŠå®ç°è‡ªç„¶çš„è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®ç®¡é“ï¼Œä»¥æ„å»ºæç¤º-å‚è€ƒ-è§†é¢‘ä¸‰å…ƒç»„ç”¨äºæ¨¡å‹è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„è§†é¢‘ï¼Œå¹¶å®ç°ç²¾ç¡®çš„å…ƒç´ æ§åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02826",
            "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing",
            "url": "https://huggingface.co/papers/2504.02826",
            "abstract": "Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at https://github.com/PhoenixZ810/RISEBench.",
            "score": 4,
            "issue_id": 3064,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "dbb1c07cd5a01838",
            "authors": [
                "Xiangyu Zhao",
                "Peiyuan Zhang",
                "Kexian Tang",
                "Hao Li",
                "Zicheng Zhang",
                "Guangtao Zhai",
                "Junchi Yan",
                "Hua Yang",
                "Xue Yang",
                "Haodong Duan"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02826.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#benchmark",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "RISEBench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸",
                    "desc": "RISEBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼, Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼. Ğ¢ĞµÑÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ LLM-ÑÑƒĞ´ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4, Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "RISEBench: Advancing Reasoning in Visual Editing",
                    "desc": "This paper introduces RISEBench, a new benchmark designed to evaluate Reasoning-Informed Visual Editing (RISE) in Large Multi-modality Models (LMMs). It identifies challenges in visual editing, such as following complex instructions and maintaining appearance consistency. RISEBench categorizes reasoning into four types: Temporal, Causal, Spatial, and Logical, and provides a framework for assessing these reasoning types through both human and model evaluations. The findings indicate that even advanced models like GPT-4o-Native struggle with logical reasoning, suggesting a need for further research in this area."
                },
                "zh": {
                    "title": "æ¨ç†é©±åŠ¨çš„è§†è§‰ç¼–è¾‘æ–°åŸºå‡†",
                    "desc": "å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é€šç”¨è§†è§‰ç¼–è¾‘ä¸­ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éµå¾ªå¤æ‚æŒ‡ä»¤ã€ä¿æŒå¤–è§‚ä¸€è‡´æ€§å’Œæ”¯æŒçµæ´»è¾“å…¥æ ¼å¼æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RISEBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°æ¨ç†é©±åŠ¨è§†è§‰ç¼–è¾‘ï¼ˆRISEï¼‰çš„åŸºå‡†ã€‚RISEBenchä¸“æ³¨äºå››ç§å…³é”®æ¨ç†ç±»å‹ï¼šæ—¶é—´æ¨ç†ã€å› æœæ¨ç†ã€ç©ºé—´æ¨ç†å’Œé€»è¾‘æ¨ç†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå°½ç®¡GPT-4o-Nativeåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œä½†åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ï¼Œæ˜¾ç¤ºå‡ºè¿™ä¸€é¢†åŸŸä»éœ€æ·±å…¥æ¢ç´¢ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02542",
            "title": "Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation",
            "url": "https://huggingface.co/papers/2504.02542",
            "abstract": "Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce ACTalker, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict.",
            "score": 4,
            "issue_id": 3064,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "fa93ea3aeacd0dbc",
            "authors": [
                "Fa-Ting Hong",
                "Zunnan Xu",
                "Zixiang Zhou",
                "Jun Zhou",
                "Xiu Li",
                "Qin Lin",
                "Qinglin Lu",
                "Dan Xu"
            ],
            "affiliations": [
                "HKUST",
                "Tencent",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02542.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼",
                    "desc": "ACTalker - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´. ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ mamba Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ²ĞµÑ‚Ğ²ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². ACTalker Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "ACTalker: Multi-Signal Control for Natural Talking Head Synthesis",
                    "desc": "This paper presents ACTalker, a novel framework for generating talking head videos that can be controlled by multiple signals simultaneously. Unlike traditional methods that rely on a single control modality, ACTalker employs a parallel mamba structure with multiple branches, each dedicated to a specific facial region. A gate mechanism allows for flexible control, ensuring that different driving signals can manipulate facial features without interference. The introduction of a mask-drop strategy further enhances this capability, enabling independent control of facial regions and resulting in more natural and coordinated video outputs."
                },
                "zh": {
                    "title": "å¤šä¿¡å·æ§åˆ¶çš„å¯¹è¯å¤´åƒç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºACTalkerçš„ç«¯åˆ°ç«¯è§†é¢‘æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆè™šæ‹Ÿå¤´åƒçš„å¯¹è¯è§†é¢‘ã€‚è¯¥æ–¹æ³•æ”¯æŒå¤šä¿¡å·å’Œå•ä¿¡å·æ§åˆ¶ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚é€šè¿‡è®¾è®¡å¹¶è¡Œçš„mambaç»“æ„ï¼Œå…è®¸ä¸åŒçš„é©±åŠ¨ä¿¡å·æ§åˆ¶é¢éƒ¨çš„ç‰¹å®šåŒºåŸŸï¼Œå¹¶ä½¿ç”¨é—¨æ§æœºåˆ¶å®ç°çµæ´»æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒACTalkerèƒ½å¤Ÿç”Ÿæˆè‡ªç„¶çš„é¢éƒ¨è§†é¢‘ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ— å†²çªåœ°æ•´åˆå¤šç§é©±åŠ¨ä¿¡å·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02119",
            "title": "Efficient Model Selection for Time Series Forecasting via LLMs",
            "url": "https://huggingface.co/papers/2504.02119",
            "abstract": "Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting.",
            "score": 3,
            "issue_id": 3063,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "7c31e20ce0a7813b",
            "authors": [
                "Wang Wei",
                "Tiankai Yang",
                "Hongjie Chen",
                "Ryan A. Rossi",
                "Yue Zhao",
                "Franck Dernoncourt",
                "Hoda Eldardiry"
            ],
            "affiliations": [
                "Adobe Research San Jose, CA, USA",
                "Adobe Research Seattle, WA, USA",
                "Department of Computer Science University of South California Los Angeles, CA, USA",
                "Department of Computer Science Virginia Tech Blacksburg, VA, USA",
                "Dolby Labs Atlanta, GA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02119.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "LLM ĞºĞ°Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ LLaMA, GPT Ğ¸ Gemini Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¼ĞµÑ‚Ğ°-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» LLM Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Model Selection with Large Language Models",
                    "desc": "This paper addresses the challenge of model selection in time series forecasting, which usually requires evaluating many models across different datasets. The authors introduce a novel approach that uses Large Language Models (LLMs) to automate this selection process without needing costly performance matrices. By leveraging the reasoning abilities of LLMs, their method simplifies the model selection task and reduces computational costs. Experimental results show that this approach outperforms traditional meta-learning methods and heuristic techniques, highlighting the effectiveness of LLMs in this domain."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–æ—¶é—´åºåˆ—é¢„æµ‹çš„æ¨¡å‹é€‰æ‹©",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„æ¨¡å‹é€‰æ‹©é—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•éœ€è¦åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›çš„æ€§èƒ½è¯„ä¼°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè½»é‡çº§æ›¿ä»£æ–¹æ¡ˆçš„æ–¹æ³•ï¼Œé¿å…äº†æ„å»ºæ˜‚è´µçš„æ€§èƒ½çŸ©é˜µã€‚é€šè¿‡ä¸LLaMAã€GPTå’ŒGeminiçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„å…ƒå­¦ä¹ æŠ€æœ¯å’Œå¯å‘å¼åŸºçº¿ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†LLMsåœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­é«˜æ•ˆæ¨¡å‹é€‰æ‹©çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02507",
            "title": "ZClip: Adaptive Spike Mitigation for LLM Pre-Training",
            "url": "https://huggingface.co/papers/2504.02507",
            "abstract": "Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes. These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping. Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention. In this work, we propose ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time. Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms. At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise. Our code is available at: https://github.com/bluorion-com/ZClip.",
            "score": 2,
            "issue_id": 3065,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 3",
                "zh": "4æœˆ3æ—¥"
            },
            "hash": "290019150fe5b4c9",
            "authors": [
                "Abhay Kumar",
                "Louis Owen",
                "Nilabhra Roy Chowdhury",
                "Fabian GÃ¼ra"
            ],
            "affiliations": [
                "BluOrion"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02507.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "ZClip: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ZClip Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ZClip Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ½Ğ¾Ñ€Ğ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ z-Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²ÑĞ¿Ğ»ĞµÑĞºĞ¾Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ZClip Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ğµ Ğ²ÑĞ¿Ğ»ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ½Ğµ Ğ¼ĞµÑˆĞ°Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ…."
                },
                "en": {
                    "title": "ZClip: Smart Gradient Clipping for Stable LLM Training",
                    "desc": "This paper addresses the challenges of training large language models (LLMs) by introducing ZClip, an adaptive gradient clipping algorithm. Traditional methods often fail to manage gradient instability and loss spikes effectively, leading to inefficient training and the need for manual adjustments. ZClip improves upon these methods by dynamically adjusting the clipping threshold based on the statistical behavior of gradient norms, allowing for a more responsive training process. By using z-score-based anomaly detection, ZClip prevents harmful loss spikes while maintaining the overall convergence of the model."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”æ¢¯åº¦è£å‰ªï¼Œæå‡è®­ç»ƒç¨³å®šæ€§",
                    "desc": "åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¸¸å¸¸ä¼šé‡åˆ°æ¢¯åº¦ä¸ç¨³å®šå’ŒæŸå¤±å³°å€¼ç­‰é—®é¢˜ï¼Œè¿™å¯èƒ½å¯¼è‡´ç¾éš¾æ€§çš„å‘æ•£ã€‚ä¼ ç»Ÿçš„æ¢¯åº¦è£å‰ªæŠ€æœ¯æ— æ³•æœ‰æ•ˆè§£å†³è¿™äº›é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºå›ºå®šçš„é˜ˆå€¼æˆ–å¯å‘å¼æ–¹æ³•ï¼Œå¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºZClipçš„è‡ªé€‚åº”æ¢¯åº¦è£å‰ªç®—æ³•ï¼Œå®ƒæ ¹æ®æ¢¯åº¦èŒƒæ•°çš„ç»Ÿè®¡ç‰¹æ€§åŠ¨æ€è°ƒæ•´è£å‰ªé˜ˆå€¼ã€‚ZClipé€šè¿‡åŸºäºz-scoreçš„å¼‚å¸¸æ£€æµ‹æ¥è¯†åˆ«å’Œå‡è½»å¤§æ¢¯åº¦å³°å€¼ï¼Œä»è€Œé˜²æ­¢æŸå¤±å³°å€¼ï¼ŒåŒæ—¶ä¸å¹²æ‰°æ”¶æ•›è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02012",
            "title": "Instruction-Guided Autoregressive Neural Network Parameter Generation",
            "url": "https://huggingface.co/papers/2504.02012",
            "abstract": "Learning to generate neural network parameters conditioned on task descriptions and architecture specifications is pivotal for advancing model adaptability and transfer learning. Existing methods especially those based on diffusion models suffer from limited scalability to large architectures, rigidity in handling varying network depths, and disjointed parameter generation that undermines inter-layer coherence. In this work, we propose IGPG (Instruction Guided Parameter Generation), an autoregressive framework that unifies parameter synthesis across diverse tasks and architectures. IGPG leverages a VQ-VAE and an autoregressive model to generate neural network parameters, conditioned on task instructions, dataset, and architecture details. By autoregressively generating neural network weights' tokens, IGPG ensures inter-layer coherence and enables efficient adaptation across models and datasets. Operating at the token level, IGPG effectively captures complex parameter distributions aggregated from a broad spectrum of pretrained models. Extensive experiments on multiple vision datasets demonstrate that IGPG consolidates diverse pretrained models into a single, flexible generative framework. The synthesized parameters achieve competitive or superior performance relative to state-of-the-art methods, especially in terms of scalability and efficiency when applied to large architectures. These results underscore ICPG potential as a powerful tool for pretrained weight retrieval, model selection, and rapid task-specific fine-tuning.",
            "score": 1,
            "issue_id": 3065,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 2",
                "zh": "4æœˆ2æ—¥"
            },
            "hash": "cbdd586ccd2b682d",
            "authors": [
                "Soro Bedionita",
                "Bruno Andreis",
                "Song Chong",
                "Sung Ju Hwang"
            ],
            "affiliations": [
                "DeepAuto.ai, South Korea",
                "KAIST AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02012.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#cv",
                    "#optimization",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "IGPG: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€",
                    "desc": "IGPG - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° VQ-VAE Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ ÑĞµÑ‚Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸. IGPG Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸Ğ· ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ IGPG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€."
                },
                "en": {
                    "title": "IGPG: Unifying Neural Network Parameter Generation for Enhanced Adaptability",
                    "desc": "This paper introduces IGPG (Instruction Guided Parameter Generation), a new framework for generating neural network parameters based on task descriptions and architecture specifications. Unlike previous methods, IGPG uses an autoregressive approach that ensures coherence between layers and adapts efficiently to different models and datasets. By employing a VQ-VAE and generating weights at the token level, IGPG captures complex parameter distributions from various pretrained models. The results show that IGPG outperforms existing methods in scalability and efficiency, making it a valuable tool for model adaptation and fine-tuning."
                },
                "zh": {
                    "title": "IGPGï¼šçµæ´»çš„ç¥ç»ç½‘ç»œå‚æ•°ç”Ÿæˆå·¥å…·",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºIGPGï¼ˆæŒ‡ä»¤å¼•å¯¼å‚æ•°ç”Ÿæˆï¼‰çš„è‡ªå›å½’æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆç¥ç»ç½‘ç»œå‚æ•°ï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡æè¿°å’Œæ¶æ„è§„èŒƒã€‚IGPGé€šè¿‡ç»“åˆVQ-VAEå’Œè‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡å’Œæ¶æ„ä¸­ç»Ÿä¸€å‚æ•°åˆæˆï¼Œç¡®ä¿å±‚é—´ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆç¥ç»ç½‘ç»œæƒé‡æ—¶ï¼Œé‡‡ç”¨äº†åŸºäºtokençš„ç”Ÿæˆæ–¹å¼ï¼Œæœ‰æ•ˆæ•æ‰æ¥è‡ªå¤šç§é¢„è®­ç»ƒæ¨¡å‹çš„å¤æ‚å‚æ•°åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIGPGåœ¨å¤šä¸ªè§†è§‰æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨å¤§è§„æ¨¡æ¶æ„çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡æ–¹é¢ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.00891",
            "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning",
            "url": "https://huggingface.co/papers/2504.00891",
            "abstract": "Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, a generative process reward model that performs explicit Chain-of-Thought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and a rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as a critic model for policy model refinement. This work establishes a new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in https://ryanliu112.github.io/GenPRM.",
            "score": 1,
            "issue_id": 3066,
            "pub_date": "2025-04-01",
            "pub_date_card": {
                "ru": "1 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 1",
                "zh": "4æœˆ1æ—¥"
            },
            "hash": "b22a54f43f9d7a89",
            "authors": [
                "Jian Zhao",
                "Runze Liu",
                "Kaiyan Zhang",
                "Zhimu Zhou",
                "Junqi Gao",
                "Dong Li",
                "Jiafei Lyu",
                "Zhouyi Qian",
                "Biqing Qi",
                "Xiu Li",
                "Bowen Zhou"
            ],
            "affiliations": [
                "BUPT",
                "Harbin Institute of Technology",
                "Shanghai AI Laboratory",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.00891.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#math",
                    "#reasoning",
                    "#rlhf"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "GenPRM: ĞĞ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GenPRM - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GenPRM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ PRM Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 23 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°Ñ‚ÑŒ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "GenPRM: Elevating LLMs with Generative Process Reward Models",
                    "desc": "This paper presents GenPRM, a generative process reward model designed to improve the performance of large language models (LLMs) by addressing key challenges faced by existing process reward models (PRMs). GenPRM utilizes Chain-of-Thought (CoT) reasoning and incorporates code verification to enhance the quality of its judgments at each reasoning step. The authors introduce a novel method called Relative Progress Estimation (RPE) to generate high-quality supervision labels and rationale data, leading to significant performance improvements on various reasoning tasks. Experimental results demonstrate that GenPRM outperforms previous PRMs and shows strong capabilities as a critic model for refining policy models, establishing a new approach for process supervision in LLMs."
                },
                "zh": {
                    "title": "ç”Ÿæˆæ€§è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼šæå‡LLMsçš„æ–°èŒƒå¼",
                    "desc": "æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•è¡¨æ˜ï¼Œä½¿ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ä½œä¸ºéªŒè¯å™¨å¯ä»¥æå‡LLMsçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰çš„PRMsé¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šæœ‰é™çš„è¿‡ç¨‹ç›‘ç£å’Œæ³›åŒ–èƒ½åŠ›ã€ä¾èµ–äºæ ‡é‡å€¼é¢„æµ‹è€Œæœªåˆ©ç”¨LLMsçš„ç”Ÿæˆèƒ½åŠ›ï¼Œä»¥åŠæ— æ³•æ‰©å±•PRMsçš„æµ‹è¯•æ—¶é—´è®¡ç®—ã€‚æœ¬æ–‡æå‡ºäº†GenPRMï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆæ€§è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œé€šè¿‡ä»£ç éªŒè¯è¿›è¡Œæ˜ç¡®çš„æ€ç»´é“¾æ¨ç†ï¼Œç„¶åå¯¹æ¯ä¸ªæ¨ç†æ­¥éª¤è¿›è¡Œåˆ¤æ–­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGenPRMåœ¨å¤šä¸ªæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¹‹å‰çš„PRMsï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºæ”¿ç­–æ¨¡å‹ç²¾ç‚¼çš„æ‰¹è¯„æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-03.html",
    "link_next": "2025-04-07.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "03.04",
        "en": "04/03",
        "zh": "4æœˆ3æ—¥"
    },
    "short_date_next": {
        "ru": "07.04",
        "en": "04/07",
        "zh": "4æœˆ7æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå«åš MergeVQã€‚å®ƒç»“åˆäº†å‘é‡é‡åŒ–å’Œ token åˆå¹¶æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³å›¾åƒç”Ÿæˆè´¨é‡å’Œè¡¨ç¤ºå­¦ä¹ æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚MergeVQ åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡ token åˆå¹¶æ¨¡å—æå–è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åœ¨è§£ç é˜¶æ®µæ¢å¤ç»†èŠ‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMergeVQ åœ¨å›¾åƒç”Ÿæˆå’Œè¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œä¸”æ•ˆç‡é«˜ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨ç½‘ä¸Šå…¬å¼€ã€‚",
        "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå«åš MergeVQã€‚å®ƒç»“åˆäº†å‘é‡é‡åŒ–å’Œ token åˆå¹¶æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³å›¾åƒç”Ÿæˆè´¨é‡å’Œè¡¨ç¤ºå­¦ä¹ æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚MergeVQ åœ¨é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡ token åˆå¹¶æ¨¡å—æå–è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶åœ¨è§£ç é˜¶æ®µæ¢å¤ç»†èŠ‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMergeVQ åœ¨å›¾åƒç”Ÿæˆå’Œè¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ï¼Œä¸”æ•ˆç‡é«˜ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨ç½‘ä¸Šå…¬å¼€ã€‚\n\nZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le yÄ«zhÇ’ng xÄ«n de tÃºxiÃ ng shÄ“ngchÃ©ng mÃ³xÃ­ng, jiÃ ozuÃ² MergeVQ. TÄ jiÄ“hÃ© le xiÃ ngliÃ ng liÃ nggÃ©huÃ  hÃ© token hÃ©bÃ¬ng jÃ¬shÃ¹, zhÇyÃº jiÄ›juÃ© tÃºxiÃ ng shÄ“ngchÃ©ng zhÃ¬liÃ ng hÃ© biÇoshÃ¬ xuÃ©xÃ­ xiÃ oyÃ¬ zhÄ«jiÄn de pÃ­nghÃ©ng wÃ¨ntÃ­. MergeVQ zÃ i yÃ¹xÃ¹nliÃ n jiÄ“duÃ n tÅngguÃ² token hÃ©bÃ¬ng mÃ³kuÃ i tÃ­qÇ” yÇ”yÃ¬ xÃ¬nxÄ«, bÃ¬ng zÃ i jiÄ›mÇ jiÄ“duÃ n huÄ«fÃ¹ xÃ¬jiÃ¨. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, MergeVQ zÃ i tÃºxiÃ ng shÄ“ngchÃ©ng hÃ© biÇoshÃ¬ xuÃ©xÃ­ rÃ¨nwÃ¹ zhÅng dÅu biÇoxiÃ n chÅ«sÃ¨, qiÄ› xiÃ oyÃ¬ gÄo. DÃ imÇ hÃ© mÃ³xÃ­ng jiÄng zÃ i wÇngshÃ ng gÅngkÄi.",
        "vocab": "[\n    {\"word\": \"å‘é‡é‡åŒ–\", \"pinyin\": \"xiÃ ngliÃ ng liÃ ngzhÃ¬\", \"trans\": \"vector quantization\"},\n    {\"word\": \"token\", \"pinyin\": \"tÅukÃ¨n\", \"trans\": \"token\"},\n    {\"word\": \"åˆå¹¶\", \"pinyin\": \"hÃ©bÃ¬ng\", \"trans\": \"merge\"},\n    {\"word\": \"æ—¨åœ¨\", \"pinyin\": \"zhÇzÃ i\", \"trans\": \"aim to\"},\n    {\"word\": \"å¹³è¡¡\", \"pinyin\": \"pÃ­nghÃ©ng\", \"trans\": \"balance\"},\n    {\"word\": \"é¢„è®­ç»ƒ\", \"pinyin\": \"yÃ¹ xÃ¹nliÃ n\", \"trans\": \"pre-training\"},\n    {\"word\": \"è¯­ä¹‰\", \"pinyin\": \"yÇ”yÃ¬\", \"trans\": \"semantic\"},\n    {\"word\": \"è§£ç \", \"pinyin\": \"jiÄ›mÇ\", \"trans\": \"decode\"},\n    {\"word\": \"æ¢å¤\", \"pinyin\": \"huÄ«fÃ¹\", \"trans\": \"recover\"},\n    {\"word\": \"ç»†èŠ‚\", \"pinyin\": \"xÃ¬jiÃ©\", \"trans\": \"detail\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇoxiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ«sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"æ•ˆç‡\", \"pinyin\": \"xiÃ olÇœ\", \"trans\": \"efficiency\"},\n    {\"word\": \"å…¬å¼€\", \"pinyin\": \"gÅngkÄi\", \"trans\": \"public\"}\n]",
        "trans": "This article introduces a new image generation model called MergeVQ. It combines vector quantization and token merging techniques to address the balance between image generation quality and representation learning efficiency. MergeVQ extracts semantic information through a token merging module during the pre-training phase and recovers details during the decoding phase. Experimental results show that MergeVQ performs excellently in both image generation and representation learning tasks, with high efficiency. The code and model will be made publicly available online.",
        "update_ts": "2025-04-03 09:11"
    }
}