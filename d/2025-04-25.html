
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. April 25.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">25 апреля</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-04-24.html">⬅️ <span id="prev-date">24.04</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-04-28.html">➡️ <span id="next-date">28.04</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-04.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '25 апреля', 'en': 'April 25', 'zh': '4月25日'};
        let feedDateNext = {'ru': '28.04', 'en': '04/28', 'zh': '4月28日'};
        let feedDatePrev = {'ru': '24.04', 'en': '04/24', 'zh': '4月24日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.17761', 'title': 'Step1X-Edit: A Practical Framework for General Image Editing', 'url': 'https://huggingface.co/papers/2504.17761', 'abstract': "In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a vast majority of user-driven editing requirements, marking a significant advancement in the field of image manipulation. However, there is still a large gap between the open-source algorithm with these closed-source models. Thus, in this paper, we aim to release a state-of-the-art image editing model, called Step1X-Edit, which can provide comparable performance against the closed-source models like GPT-4o and Gemini2 Flash. More specifically, we adopt the Multimodal LLM to process the reference image and the user's editing instruction. A latent embedding has been extracted and integrated with a diffusion image decoder to obtain the target image. To train the model, we build a data generation pipeline to produce a high-quality dataset. For evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world user instructions. Experimental results on GEdit-Bench demonstrate that Step1X-Edit outperforms existing open-source baselines by a substantial margin and approaches the performance of leading proprietary models, thereby making significant contributions to the field of image editing.", 'score': 55, 'issue_id': 3427, 'pub_date': '2025-04-24', 'pub_date_card': {'ru': '24 апреля', 'en': 'April 24', 'zh': '4月24日'}, 'hash': '2896842e49a93757', 'authors': ['Shiyu Liu', 'Yucheng Han', 'Peng Xing', 'Fukun Yin', 'Rui Wang', 'Wei Cheng', 'Jiaqi Liao', 'Yingming Wang', 'Honghao Fu', 'Chunrui Han', 'Guopeng Li', 'Yuang Peng', 'Quan Sun', 'Jingwei Wu', 'Yan Cai', 'Zheng Ge', 'Ranchen Ming', 'Lei Xia', 'Xianfang Zeng', 'Yibo Zhu', 'Binxing Jiao', 'Xiangyu Zhang', 'Gang Yu', 'Daxin Jiang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.17761.jpg', 'data': {'categories': ['#data', '#diffusion', '#training', '#multimodal', '#cv', '#dataset', '#open_source', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'Step1X-Edit: Открытая модель редактирования изображений на уровне лучших закрытых решений', 'desc': 'Статья представляет новую модель редактирования изображений Step1X-Edit, сопоставимую по производительности с закрытыми моделями GPT-4o и Gemini2 Flash. Модель использует мультимодальную LLM для обработки изображения и инструкций пользователя, а также диффузионный декодер для генерации целевого изображения. Для обучения модели был создан высококачественный датасет, а для оценки разработан новый бенчмарк GEdit-Bench. Эксперименты показывают, что Step1X-Edit значительно превосходит существующие открытые модели и приближается к производительности ведущих проприетарных решений.'}, 'en': {'title': 'Bridging the Gap in Image Editing with Step1X-Edit', 'desc': 'This paper presents Step1X-Edit, a new image editing model that aims to match the performance of advanced closed-source models like GPT-4o and Gemini2 Flash. It utilizes a Multimodal LLM to effectively process both the reference image and user editing instructions, integrating a latent embedding with a diffusion image decoder to generate the final edited image. The authors also introduce GEdit-Bench, a benchmark designed to evaluate the model based on real-world user instructions. Experimental results show that Step1X-Edit significantly outperforms existing open-source models and approaches the capabilities of leading proprietary systems, marking a notable advancement in image editing technology.'}, 'zh': {'title': '开源图像编辑的未来：Step1X-Edit', 'desc': '近年来，图像编辑模型取得了显著的发展。新发布的多模态模型如GPT-4o和Gemini2 Flash展现了强大的图像编辑能力，能够满足大多数用户的编辑需求。本文提出了一种名为Step1X-Edit的先进图像编辑模型，旨在与这些闭源模型的性能相媲美。通过采用多模态大语言模型处理参考图像和用户编辑指令，并结合扩散图像解码器，Step1X-Edit在真实用户指令的基准测试中表现优异，显著超越现有的开源基线。'}}}, {'id': 'https://huggingface.co/papers/2504.17502', 'title': 'RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image\n  Generation', 'url': 'https://huggingface.co/papers/2504.17502', 'abstract': 'Subject-driven text-to-image (T2I) generation aims to produce images that align with a given textual description, while preserving the visual identity from a referenced subject image. Despite its broad downstream applicability -- ranging from enhanced personalization in image generation to consistent character representation in video rendering -- progress in this field is limited by the lack of reliable automatic evaluation. Existing methods either assess only one aspect of the task (i.e., textual alignment or subject preservation), misalign with human judgments, or rely on costly API-based evaluation. To address this, we introduce RefVNLI, a cost-effective metric that evaluates both textual alignment and subject preservation in a single prediction. Trained on a large-scale dataset derived from video-reasoning benchmarks and image perturbations, RefVNLI outperforms or matches existing baselines across multiple benchmarks and subject categories (e.g., Animal, Object), achieving up to 6.4-point gains in textual alignment and 8.5-point gains in subject consistency. It also excels with lesser-known concepts, aligning with human preferences at over 87\\% accuracy.', 'score': 47, 'issue_id': 3433, 'pub_date': '2025-04-24', 'pub_date_card': {'ru': '24 апреля', 'en': 'April 24', 'zh': '4月24日'}, 'hash': '5f4407bb2bd57352', 'authors': ['Aviv Slobodkin', 'Hagai Taitelbaum', 'Yonatan Bitton', 'Brian Gordon', 'Michal Sokolik', 'Nitzan Bitton Guetta', 'Almog Gueta', 'Royi Rassin', 'Itay Laish', 'Dani Lischinski', 'Idan Szpektor'], 'affiliations': ['Ben Gurion University', 'Google Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.17502.jpg', 'data': {'categories': ['#cv', '#benchmark', '#dataset', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'RefVNLI: Новый стандарт оценки генерации изображений по тексту', 'desc': 'Статья представляет новый метод оценки генерации изображений по текстовому описанию с сохранением визуальной идентичности объекта. Авторы предлагают метрику RefVNLI, которая оценивает как соответствие текстовому описанию, так и сохранение характеристик объекта. RefVNLI обучена на большом наборе данных из видео и модифицированных изображений. Метрика превосходит существующие методы оценки на различных тестовых наборах и категориях объектов, показывая улучшение до 6.4 пунктов в соответствии тексту и 8.5 пунктов в сохранении характеристик объекта.'}, 'en': {'title': 'RefVNLI: A New Standard for Evaluating Text-to-Image Generation', 'desc': 'This paper presents RefVNLI, a new metric for evaluating subject-driven text-to-image (T2I) generation. It effectively measures both textual alignment with descriptions and preservation of visual identity from reference images. By training on a large dataset, RefVNLI shows significant improvements over existing evaluation methods, achieving higher accuracy in aligning generated images with human preferences. This advancement allows for better personalization and consistency in image generation tasks, making it a valuable tool for future developments in the field.'}, 'zh': {'title': '提升文本与图像一致性的评估新方法', 'desc': '本文介绍了一种新的文本到图像生成方法，旨在根据给定的文本描述生成与参考图像一致的图像。尽管该领域有广泛的应用，但由于缺乏可靠的自动评估方法，进展受到限制。我们提出了RefVNLI，这是一种经济高效的评估指标，可以同时评估文本对齐和主题保留。经过大规模数据集的训练，RefVNLI在多个基准测试中表现优异，显著提高了文本对齐和主题一致性。'}}}, {'id': 'https://huggingface.co/papers/2504.17192', 'title': 'Paper2Code: Automating Code Generation from Scientific Papers in Machine\n  Learning', 'url': 'https://huggingface.co/papers/2504.17192', 'abstract': 'Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, specifically from the original paper authors, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins.', 'score': 46, 'issue_id': 3430, 'pub_date': '2025-04-24', 'pub_date_card': {'ru': '24 апреля', 'en': 'April 24', 'zh': '4月24日'}, 'hash': '01c5ca3f5c3906ce', 'authors': ['Minju Seo', 'Jinheon Baek', 'Seongyun Lee', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2504.17192.jpg', 'data': {'categories': ['#agents', '#architecture', '#benchmark', '#dataset', '#open_source', '#science'], 'emoji': '🤖', 'ru': {'title': 'От статьи к коду: автоматизация реализации алгоритмов машинного обучения', 'desc': 'PaperCoder - это мультиагентная система на основе больших языковых моделей (LLM), которая преобразует научные статьи по машинному обучению в функциональные репозитории кода. Система работает в три этапа: планирование, анализ и генерация, используя специализированных агентов для каждой фазы. PaperCoder демонстрирует эффективность в создании высококачественных и точных реализаций алгоритмов из статей. Система превосходит сильные базовые модели в недавно выпущенном бенчмарке PaperBench.'}, 'en': {'title': 'Transforming Research into Code with PaperCoder', 'desc': 'This paper presents PaperCoder, a framework that uses Large Language Models (LLMs) to convert machine learning research papers into functional code repositories. The process involves three main stages: planning, analysis, and generation, each handled by specialized agents that work together. In the planning stage, a roadmap and system architecture are created, while the analysis stage focuses on understanding implementation details. Finally, the generation stage produces modular code that respects dependencies, and the framework has been shown to outperform existing methods in generating high-quality implementations.'}, 'zh': {'title': 'PaperCoder：将论文转化为代码的智能助手', 'desc': '尽管机器学习研究迅速发展，但相应的代码实现往往缺乏，导致研究人员在重现结果和基于先前工作进行构建时耗时费力。为了解决这个问题，我们提出了PaperCoder，这是一个多智能体的大型语言模型框架，可以将机器学习论文转化为功能性代码库。PaperCoder分为三个阶段：规划、分析和生成，每个阶段都有专门的智能体协作完成任务。我们的评估结果表明，PaperCoder在生成高质量、忠实的实现方面表现出色，且在最新的PaperBench基准测试中超越了强有力的基线。'}}}, {'id': 'https://huggingface.co/papers/2504.17432', 'title': 'Breaking the Modality Barrier: Universal Embedding Learning with\n  Multimodal LLMs', 'url': 'https://huggingface.co/papers/2504.17432', 'abstract': "The Contrastive Language-Image Pre-training (CLIP) framework has become a widely used approach for multimodal representation learning, particularly in image-text retrieval and clustering. However, its efficacy is constrained by three key limitations: (1) text token truncation, (2) isolated image-text encoding, and (3) deficient compositionality due to bag-of-words behavior. While recent Multimodal Large Language Models (MLLMs) have demonstrated significant advances in generalized vision-language understanding, their potential for learning transferable multimodal representations remains underexplored.In this work, we present UniME (Universal Multimodal Embedding), a novel two-stage framework that leverages MLLMs to learn discriminative representations for diverse downstream tasks. In the first stage, we perform textual discriminative knowledge distillation from a powerful LLM-based teacher model to enhance the embedding capability of the MLLM\\'s language component. In the second stage, we introduce hard negative enhanced instruction tuning to further advance discriminative representation learning. Specifically, we initially mitigate false negative contamination and then sample multiple hard negatives per instance within each batch, forcing the model to focus on challenging samples. This approach not only improves discriminative power but also enhances instruction-following ability in downstream tasks. We conduct extensive experiments on the MMEB benchmark and multiple retrieval tasks, including short and long caption retrieval and compositional retrieval. Results demonstrate that UniME achieves consistent performance improvement across all tasks, exhibiting superior discriminative and compositional capabilities.", 'score': 28, 'issue_id': 3427, 'pub_date': '2025-04-24', 'pub_date_card': {'ru': '24 апреля', 'en': 'April 24', 'zh': '4月24日'}, 'hash': '3a77377667aeb98f', 'authors': ['Tiancheng Gu', 'Kaicheng Yang', 'Ziyong Feng', 'Xingjun Wang', 'Yanzhao Zhang', 'Dingkun Long', 'Yingda Chen', 'Weidong Cai', 'Jiankang Deng'], 'affiliations': ['DeepGlint Tongyi Lab, Alibaba Group', 'Imperial College London', 'The University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2504.17432.jpg', 'data': {'categories': ['#benchmark', '#transfer_learning', '#optimization', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'UniME: Улучшение мультимодальных представлений через дистилляцию знаний и инструктивную настройку', 'desc': 'UniME - это новый двухэтапный фреймворк для обучения дискриминативным мультимодальным представлениям с использованием мультимодальных больших языковых моделей (MLLM). На первом этапе происходит дистилляция знаний от мощной LLM-модели для улучшения языкового компонента MLLM. Второй этап включает инструктивную настройку с усиленными сложными негативными примерами для повышения дискриминативной способности. Эксперименты на бенчмарке MMEB и задачах поиска показывают превосходство UniME в дискриминативных и композиционных возможностях.'}, 'en': {'title': 'Enhancing Multimodal Learning with UniME', 'desc': "The paper introduces UniME, a new framework designed to improve multimodal representation learning by addressing limitations in existing models like CLIP. It utilizes a two-stage process where the first stage enhances the language component of a Multimodal Large Language Model (MLLM) through knowledge distillation from a teacher model. The second stage employs hard negative enhanced instruction tuning to refine the model's ability to distinguish between challenging samples. Experimental results show that UniME significantly boosts performance in various image-text retrieval tasks, demonstrating better discriminative and compositional skills."}, 'zh': {'title': '提升多模态学习的区分能力与组合能力', 'desc': '本文介绍了一种新的多模态嵌入框架UniME（通用多模态嵌入），旨在克服现有CLIP框架的局限性。UniME通过两个阶段的学习，首先从强大的语言模型教师模型中进行文本知识蒸馏，以增强多模态大语言模型的语言组件嵌入能力。其次，通过引入困难负样本增强的指令调优，进一步提升模型的区分能力和指令跟随能力。实验结果表明，UniME在多个检索任务中表现出色，具有更强的区分性和组合能力。'}}}, {'id': 'https://huggingface.co/papers/2504.17207', 'title': 'Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery\n  Simulation', 'url': 'https://huggingface.co/papers/2504.17207', 'abstract': 'We present a framework for perspective-aware reasoning in vision-language models (VLMs) through mental imagery simulation. Perspective-taking, the ability to perceive an environment or situation from an alternative viewpoint, is a key benchmark for human-level visual understanding, essential for environmental interaction and collaboration with autonomous agents. Despite advancements in spatial reasoning within VLMs, recent research has shown that modern VLMs significantly lack perspective-aware reasoning capabilities and exhibit a strong bias toward egocentric interpretations. To bridge the gap between VLMs and human perception, we focus on the role of mental imagery, where humans perceive the world through abstracted representations that facilitate perspective shifts. Motivated by this, we propose a framework for perspective-aware reasoning, named Abstract Perspective Change (APC), that effectively leverages vision foundation models, such as object detection, segmentation, and orientation estimation, to construct scene abstractions and enable perspective transformations. Our experiments on synthetic and real-image benchmarks, compared with various VLMs, demonstrate significant improvements in perspective-aware reasoning with our framework, further outperforming fine-tuned spatial reasoning models and novel-view-synthesis-based approaches.', 'score': 18, 'issue_id': 3427, 'pub_date': '2025-04-24', 'pub_date_card': {'ru': '24 апреля', 'en': 'April 24', 'zh': '4月24日'}, 'hash': 'a45aa292431b93b2', 'authors': ['Phillip Y. Lee', 'Jihyeon Je', 'Chanho Park', 'Mikaela Angelina Uy', 'Leonidas Guibas', 'Minhyuk Sung'], 'affiliations': ['KAIST', 'NVIDIA', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.17207.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#cv', '#benchmark', '#agents'], 'emoji': '🔄', 'ru': {'title': 'Улучшение пространственного мышления ИИ через абстрактное изменение перспективы', 'desc': 'Статья представляет фреймворк для перспективно-ориентированных рассуждений в визуально-языковых моделях (VLM) через симуляцию ментальных образов. Авторы предлагают подход под названием Abstract Perspective Change (APC), который использует модели компьютерного зрения для создания абстракций сцен и трансформации перспектив. Эксперименты на синтетических и реальных изображениях показывают значительное улучшение способности VLM к рассуждениям с учетом перспективы. Предложенный метод превосходит как дообученные модели пространственных рассуждений, так и подходы на основе синтеза новых ракурсов.'}, 'en': {'title': 'Enhancing VLMs with Perspective-Aware Reasoning through Mental Imagery', 'desc': 'This paper introduces a new framework called Abstract Perspective Change (APC) aimed at enhancing perspective-aware reasoning in vision-language models (VLMs). The authors highlight that current VLMs struggle with understanding different viewpoints, often defaulting to egocentric perspectives. By simulating mental imagery, the APC framework allows VLMs to create abstract representations of scenes, facilitating perspective shifts. Experimental results show that APC significantly improves perspective reasoning compared to existing models, including those specifically fine-tuned for spatial reasoning.'}, 'zh': {'title': '提升视觉语言模型的视角感知能力', 'desc': '本文提出了一种通过心理图像模拟实现视觉语言模型（VLMs）中视角感知推理的框架。视角转换是指从不同的视角感知环境或情境，这对于人类的视觉理解至关重要，尤其是在与自主代理的互动中。尽管VLMs在空间推理方面取得了一定进展，但研究表明现代VLMs在视角感知推理能力上存在显著不足，且偏向于自我中心的解释。为了解决这一问题，我们提出了名为抽象视角变化（APC）的框架，利用视觉基础模型构建场景抽象并实现视角转换，从而显著提升了视角感知推理的能力。'}}}, {'id': 'https://huggingface.co/papers/2504.16511', 'title': 'QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM\n  Pretraining', 'url': 'https://huggingface.co/papers/2504.16511', 'abstract': 'Quality and diversity are two critical metrics for the training data of large language models (LLMs), positively impacting performance. Existing studies often optimize these metrics separately, typically by first applying quality filtering and then adjusting data proportions. However, these approaches overlook the inherent trade-off between quality and diversity, necessitating their joint consideration. Given a fixed training quota, it is essential to evaluate both the quality of each data point and its complementary effect on the overall dataset. In this paper, we introduce a unified data selection framework called QuaDMix, which automatically optimizes the data distribution for LLM pretraining while balancing both quality and diversity. Specifically, we first propose multiple criteria to measure data quality and employ domain classification to distinguish data points, thereby measuring overall diversity. QuaDMix then employs a unified parameterized data sampling function that determines the sampling probability of each data point based on these quality and diversity related labels. To accelerate the search for the optimal parameters involved in the QuaDMix framework, we conduct simulated experiments on smaller models and use LightGBM for parameters searching, inspired by the RegMix method. Our experiments across diverse models and datasets demonstrate that QuaDMix achieves an average performance improvement of 7.2% across multiple benchmarks. These results outperform the independent strategies for quality and diversity, highlighting the necessity and ability to balance data quality and diversity.', 'score': 14, 'issue_id': 3431, 'pub_date': '2025-04-23', 'pub_date_card': {'ru': '23 апреля', 'en': 'April 23', 'zh': '4月23日'}, 'hash': '5f8a634a52f613d1', 'authors': ['Fengze Liu', 'Weidong Zhou', 'Binbin Liu', 'Zhimiao Yu', 'Yifan Zhang', 'Haobin Lin', 'Yifeng Yu', 'Xiaohuan Zhou', 'Taifeng Wang', 'Yong Cao'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2504.16511.jpg', 'data': {'categories': ['#benchmark', '#training', '#data', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'Баланс качества и разнообразия данных для улучшения языковых моделей', 'desc': 'Эта статья представляет QuaDMix - новый подход к оптимизации данных для обучения больших языковых моделей (LLM). QuaDMix учитывает как качество, так и разнообразие данных, используя параметризованную функцию выборки. Метод применяет различные критерии для оценки качества данных и классификацию по доменам для измерения разнообразия. Эксперименты показали, что QuaDMix улучшает производительность моделей в среднем на 7.2% по сравнению с независимыми стратегиями оптимизации качества и разнообразия.'}, 'en': {'title': 'Balancing Quality and Diversity in LLM Training with QuaDMix', 'desc': 'This paper presents QuaDMix, a new framework for selecting training data for large language models (LLMs) that optimally balances quality and diversity. Traditional methods often treat these two metrics separately, which can lead to suboptimal performance. QuaDMix introduces a unified approach that evaluates both the quality of individual data points and their contribution to the overall diversity of the dataset. The framework uses a parameterized sampling function and employs machine learning techniques to enhance the selection process, resulting in a significant performance boost in LLM training.'}, 'zh': {'title': '平衡质量与多样性，提升模型性能', 'desc': '本文提出了一种名为QuaDMix的统一数据选择框架，旨在同时优化大语言模型（LLM）训练数据的质量和多样性。传统方法通常分别优化这两个指标，忽视了它们之间的内在权衡。QuaDMix通过多个标准来评估数据质量，并利用领域分类来区分数据点，从而衡量整体多样性。实验结果表明，QuaDMix在多个基准测试中平均提高了7.2%的性能，证明了同时平衡数据质量和多样性的必要性和有效性。'}}}, {'id': 'https://huggingface.co/papers/2504.17789', 'title': 'Token-Shuffle: Towards High-Resolution Image Generation with\n  Autoregressive Models', 'url': 'https://huggingface.co/papers/2504.17789', 'abstract': 'Autoregressive (AR) models, long dominant in language generation, are increasingly applied to image synthesis but are often considered less competitive than Diffusion-based models. A primary limitation is the substantial number of image tokens required for AR models, which constrains both training and inference efficiency, as well as image resolution. To address this, we present Token-Shuffle, a novel yet simple method that reduces the number of image tokens in Transformer. Our key insight is the dimensional redundancy of visual vocabularies in Multimodal Large Language Models (MLLMs), where low-dimensional visual codes from visual encoder are directly mapped to high-dimensional language vocabularies. Leveraging this, we consider two key operations: token-shuffle, which merges spatially local tokens along channel dimension to decrease the input token number, and token-unshuffle, which untangles the inferred tokens after Transformer blocks to restore the spatial arrangement for output. Jointly training with textual prompts, our strategy requires no additional pretrained text-encoder and enables MLLMs to support extremely high-resolution image synthesis in a unified next-token prediction way while maintaining efficient training and inference. For the first time, we push the boundary of AR text-to-image generation to a resolution of 2048x2048 with gratifying generation performance. In GenAI-benchmark, our 2.7B model achieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen by 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human evaluations also demonstrate our prominent image generation ability in terms of text-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle can serve as a foundational design for efficient high-resolution image generation within MLLMs.', 'score': 8, 'issue_id': 3431, 'pub_date': '2025-04-24', 'pub_date_card': {'ru': '24 апреля', 'en': 'April 24', 'zh': '4月24日'}, 'hash': '1c3c71248cdb19ca', 'authors': ['Xu Ma', 'Peize Sun', 'Haoyu Ma', 'Hao Tang', 'Chih-Yao Ma', 'Jialiang Wang', 'Kunpeng Li', 'Xiaoliang Dai', 'Yujun Shi', 'Xuan Ju', 'Yushi Hu', 'Artsiom Sanakoyeu', 'Felix Juefei-Xu', 'Ji Hou', 'Junjiao Tian', 'Tao Xu', 'Tingbo Hou', 'Yen-Cheng Liu', 'Zecheng He', 'Zijian He', 'Matt Feiszli', 'Peizhao Zhang', 'Peter Vajda', 'Sam Tsai', 'Yun Fu'], 'affiliations': ['Meta FAIR', 'Meta GenAI', 'National University of Singapore', 'Northeastern University', 'The Chinese University of Hong Kong', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2504.17789.jpg', 'data': {'categories': ['#architecture', '#training', '#diffusion', '#cv', '#benchmark', '#optimization', '#multimodal'], 'emoji': '🔀', 'ru': {'title': 'Token-Shuffle: прорыв в высокоэффективной генерации изображений высокого разрешения', 'desc': 'Статья представляет метод Token-Shuffle для улучшения авторегрессионных моделей в задаче генерации изображений. Этот метод уменьшает количество токенов изображения в Transformer, используя размерную избыточность визуальных словарей в мультимодальных больших языковых моделях. Token-Shuffle позволяет генерировать изображения высокого разрешения (до 2048x2048) при сохранении эффективности обучения и вывода. Результаты показывают, что модель с 2,7 млрд параметров превосходит другие авторегрессионные и диффузионные модели по различным метрикам.'}, 'en': {'title': 'Token-Shuffle: Revolutionizing High-Resolution Image Generation with Efficiency', 'desc': 'This paper introduces Token-Shuffle, a new method that enhances autoregressive (AR) models for image synthesis by reducing the number of image tokens needed. The authors leverage the redundancy in visual vocabularies of Multimodal Large Language Models (MLLMs) to improve efficiency in both training and inference. By implementing token-shuffle and token-unshuffle operations, they can maintain high-resolution outputs while simplifying the input requirements. Their approach achieves impressive results, generating images at a resolution of 2048x2048 and outperforming existing AR and diffusion models in various benchmarks.'}, 'zh': {'title': 'Token-Shuffle：高效高分辨率图像生成的新方法', 'desc': '本文提出了一种名为Token-Shuffle的新方法，旨在提高自回归（AR）模型在图像合成中的效率。通过减少输入图像标记的数量，Token-Shuffle能够在不需要额外预训练文本编码器的情况下，实现高分辨率图像生成。该方法利用多模态大语言模型中的视觉词汇的维度冗余，结合了标记混洗和标记解混洗操作。最终，我们的2.7B模型在GenAI基准测试中表现优异，推动了AR文本到图像生成的分辨率达到2048x2048。'}}}, {'id': 'https://huggingface.co/papers/2504.17040', 'title': 'DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs', 'url': 'https://huggingface.co/papers/2504.17040', 'abstract': 'We present DyMU, an efficient, training-free framework that dynamically reduces the computational burden of vision-language models (VLMs) while maintaining high task performance. Our approach comprises two key components. First, Dynamic Token Merging (DToMe) reduces the number of visual token embeddings by merging similar tokens based on image complexity, addressing the inherent inefficiency of fixed-length outputs in vision transformers. Second, Virtual Token Unmerging (VTU) simulates the expected token sequence for large language models (LLMs) by efficiently reconstructing the attention dynamics of a full sequence, thus preserving the downstream performance without additional fine-tuning. Unlike previous approaches, our method dynamically adapts token compression to the content of the image and operates completely training-free, making it readily applicable to most state-of-the-art VLM architectures. Extensive experiments on image and video understanding tasks demonstrate that DyMU can reduce the average visual token count by 32%-85% while achieving comparable performance to full-length models across diverse VLM architectures, including the recently popularized AnyRes-based visual encoders. Furthermore, through qualitative analyses, we demonstrate that DToMe effectively adapts token reduction based on image complexity and, unlike existing systems, provides users more control over computational costs. Project page: https://mikewangwzhl.github.io/dymu/.', 'score': 8, 'issue_id': 3432, 'pub_date': '2025-04-23', 'pub_date_card': {'ru': '23 апреля', 'en': 'April 23', 'zh': '4月23日'}, 'hash': '38841d87701816a0', 'authors': ['Zhenhailong Wang', 'Senthil Purushwalkam', 'Caiming Xiong', 'Silvio Savarese', 'Heng Ji', 'Ran Xu'], 'affiliations': ['Salesforce Research', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2504.17040.jpg', 'data': {'categories': ['#cv', '#optimization', '#inference', '#multimodal'], 'emoji': '🔬', 'ru': {'title': 'Динамическое сокращение токенов для эффективных VLM без переобучения', 'desc': 'DyMU - это эффективный фреймворк для снижения вычислительной нагрузки vision-language моделей без дополнительного обучения. Он включает в себя Dynamic Token Merging (DToMe) для уменьшения количества визуальных токенов и Virtual Token Unmerging (VTU) для симуляции полной последовательности токенов. DyMU динамически адаптирует сжатие токенов к содержанию изображения и применим к большинству современных архитектур VLM. Эксперименты показывают, что DyMU может сократить среднее количество визуальных токенов на 32%-85% при сохранении сопоставимой производительности.'}, 'en': {'title': 'Dynamic Efficiency in Vision-Language Models', 'desc': 'DyMU is a novel framework designed to optimize vision-language models (VLMs) by reducing their computational load without sacrificing performance. It features Dynamic Token Merging (DToMe), which intelligently merges similar visual tokens based on the complexity of the image, thus addressing inefficiencies in traditional fixed-length outputs. Additionally, Virtual Token Unmerging (VTU) reconstructs the attention dynamics of large language models, allowing for effective token sequence simulation without the need for fine-tuning. This training-free approach enables significant reductions in visual token counts, achieving up to 85% less while maintaining performance across various VLM architectures.'}, 'zh': {'title': 'DyMU：动态减少视觉语言模型计算负担的创新框架', 'desc': '我们提出了DyMU，这是一个高效的、无需训练的框架，能够动态减少视觉语言模型（VLMs）的计算负担，同时保持高任务性能。该方法包括两个关键组件：动态标记合并（DToMe）通过根据图像复杂性合并相似的视觉标记嵌入，解决了视觉变换器中固定长度输出的低效问题。虚拟标记解合并（VTU）则通过高效重建完整序列的注意力动态，模拟大型语言模型（LLMs）的预期标记序列，从而在不进行额外微调的情况下保持下游性能。我们的实验表明，DyMU能够将平均视觉标记数量减少32%-85%，同时在多种VLM架构中实现与全长模型相当的性能。'}}}, {'id': 'https://huggingface.co/papers/2504.16828', 'title': 'Process Reward Models That Think', 'url': 'https://huggingface.co/papers/2504.16828', 'abstract': "Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at https://github.com/mukhal/thinkprm.", 'score': 6, 'issue_id': 3436, 'pub_date': '2025-04-23', 'pub_date_card': {'ru': '23 апреля', 'en': 'April 23', 'zh': '4月23日'}, 'hash': '713a9c53e5d41228', 'authors': ['Muhammad Khalifa', 'Rishabh Agarwal', 'Lajanugen Logeswaran', 'Jaekyeom Kim', 'Hao Peng', 'Moontae Lee', 'Honglak Lee', 'Lu Wang'], 'affiliations': ['Mila LG AI Research', 'University of Illinois Urbana-Champaign', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2504.16828.jpg', 'data': {'categories': ['#optimization', '#long_context', '#data', '#reasoning', '#training', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'ThinkPRM: Эффективная верификация решений с минимальным обучением', 'desc': 'Статья представляет ThinkPRM - новый подход к созданию процессных моделей вознаграждения (PRM) для верификации пошаговых решений в машинном обучении. ThinkPRM использует генеративную цепочку рассуждений для проверки каждого шага решения, что позволяет обучать модель на значительно меньшем количестве размеченных данных по сравнению с дискриминативными PRM. Эксперименты показывают, что ThinkPRM превосходит базовые методы на нескольких сложных бенчмарках, используя всего 1% процессных меток из набора PRM800K. Авторы демонстрируют эффективность ThinkPRM как в доменной, так и в междоменной оценке, подчеркивая потенциал генеративных PRM с длинной цепочкой рассуждений для масштабирования вычислений при верификации.'}, 'en': {'title': 'ThinkPRM: Efficient Verification with Minimal Supervision', 'desc': 'This paper introduces ThinkPRM, a new type of process reward model (PRM) that uses verbalized step-wise reasoning to verify solutions efficiently. Unlike traditional PRMs that need extensive step-level supervision, ThinkPRM is trained with significantly fewer process labels, making it more data-efficient. The model leverages the reasoning capabilities of long chain-of-thought (CoT) models, outperforming existing methods like LLM-as-a-Judge and discriminative verifiers on various benchmarks. Overall, ThinkPRM demonstrates that generative PRMs can effectively scale verification tasks while minimizing the training data required.'}, 'zh': {'title': '高效的过程奖励模型：ThinkPRM', 'desc': '本文介绍了一种新的过程奖励模型（PRM），称为ThinkPRM，旨在提高测试时的验证效率。ThinkPRM通过生成逐步验证的思维链（CoT），在训练过程中只需极少的过程标签，从而降低了训练成本。与传统的判别式PRM相比，ThinkPRM在多个基准测试中表现更优，尤其是在ProcessBench和MATH-500等挑战性任务上。该研究强调了生成性长CoT PRM在验证计算中的价值，能够在最小监督下有效扩展测试时的计算能力。'}}}, {'id': 'https://huggingface.co/papers/2504.16921', 'title': 'IberBench: LLM Evaluation on Iberian Languages', 'url': 'https://huggingface.co/papers/2504.16921', 'abstract': 'Large Language Models (LLMs) remain difficult to evaluate comprehensively, particularly for languages other than English, where high-quality data is often limited. Existing benchmarks and leaderboards are predominantly English-centric, with only a few addressing other languages. These benchmarks fall short in several key areas: they overlook the diversity of language varieties, prioritize fundamental Natural Language Processing (NLP) capabilities over tasks of industrial relevance, and are static. With these aspects in mind, we present IberBench, a comprehensive and extensible benchmark designed to assess LLM performance on both fundamental and industry-relevant NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America. IberBench integrates 101 datasets from evaluation campaigns and recent benchmarks, covering 22 task categories such as sentiment and emotion analysis, toxicity detection, and summarization. The benchmark addresses key limitations in current evaluation practices, such as the lack of linguistic diversity and static evaluation setups by enabling continual updates and community-driven model and dataset submissions moderated by a committee of experts. We evaluate 23 LLMs ranging from 100 million to 14 billion parameters and provide empirical insights into their strengths and limitations. Our findings indicate that (i) LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii) performance is on average lower for Galician and Basque, (iii) some tasks show results close to random, and (iv) in other tasks LLMs perform above random but below shared task systems. IberBench offers open-source implementations for the entire evaluation pipeline, including dataset normalization and hosting, incremental evaluation of LLMs, and a publicly accessible leaderboard.', 'score': 5, 'issue_id': 3432, 'pub_date': '2025-04-23', 'pub_date_card': {'ru': '23 апреля', 'en': 'April 23', 'zh': '4月23日'}, 'hash': '05f62a3747dea135', 'authors': ['José Ángel González', 'Ian Borrego Obrador', 'Álvaro Romo Herrero', 'Areg Mikael Sarvazyan', 'Mara Chinea-Ríos', 'Angelo Basile', 'Marc Franco-Salvador'], 'affiliations': ['Keepler Data Tech, Madrid, 28014, Spain', 'Symanto Research, Valencia, 46011, Spain', 'United Nations International Computing Centre (UNICC), Valencia, 46930, Spain', 'Universitat Polit`ecnica de Val`encia, Valencia, 46022, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2504.16921.jpg', 'data': {'categories': ['#open_source', '#multilingual', '#dataset', '#low_resource', '#benchmark'], 'emoji': '🌐', 'ru': {'title': 'IberBench: многоязычная оценка LLM за пределами английского', 'desc': 'IberBench - это комплексный и расширяемый бенчмарк для оценки производительности крупных языковых моделей (LLM) на фундаментальных и промышленно-релевантных задачах обработки естественного языка (NLP) для языков Иберийского полуострова и Ибероамерики. Он включает 101 набор данных из оценочных кампаний и современных бенчмарков, охватывая 22 категории задач. IberBench решает ключевые ограничения текущих практик оценки, такие как отсутствие языкового разнообразия и статичные схемы оценки. Результаты показывают, что LLM хуже справляются с промышленно-релевантными задачами, чем с фундаментальными, и демонстрируют более низкую производительность для галисийского и баскского языков.'}, 'en': {'title': 'IberBench: A New Standard for Evaluating Language Models Beyond English', 'desc': 'This paper introduces IberBench, a new benchmark for evaluating Large Language Models (LLMs) on various Natural Language Processing (NLP) tasks in languages from the Iberian Peninsula and Ibero-America. It addresses the limitations of existing benchmarks that are primarily focused on English and lack diversity in language varieties and industry-relevant tasks. IberBench includes 101 datasets across 22 task categories, allowing for a more comprehensive assessment of LLM performance. The benchmark also supports continuous updates and community contributions, providing insights into the strengths and weaknesses of 23 evaluated LLMs, particularly highlighting their challenges with industry-relevant tasks and specific languages like Galician and Basque.'}, 'zh': {'title': 'IberBench：多语言LLM评估的新基准', 'desc': '大型语言模型（LLMs）的评估仍然面临挑战，尤其是在英语以外的语言中，高质量的数据往往有限。现有的基准测试主要集中在英语，缺乏对其他语言的全面评估。为了解决这些问题，我们提出了IberBench，这是一个全面且可扩展的基准，旨在评估LLM在伊比利亚半岛和伊比利亚美洲语言中的自然语言处理（NLP）任务表现。IberBench整合了101个数据集，涵盖22个任务类别，并允许社区驱动的模型和数据集提交，以应对当前评估实践中的关键局限性。'}}}, {'id': 'https://huggingface.co/papers/2504.16064', 'title': 'Boosting Generative Image Modeling via Joint Image-Feature Synthesis', 'url': 'https://huggingface.co/papers/2504.16064', 'abstract': 'Latent diffusion models (LDMs) dominate high-quality image generation, yet integrating representation learning with generative modeling remains a challenge. We introduce a novel generative image modeling framework that seamlessly bridges this gap by leveraging a diffusion model to jointly model low-level image latents (from a variational autoencoder) and high-level semantic features (from a pretrained self-supervised encoder like DINO). Our latent-semantic diffusion approach learns to generate coherent image-feature pairs from pure noise, significantly enhancing both generative quality and training efficiency, all while requiring only minimal modifications to standard Diffusion Transformer architectures. By eliminating the need for complex distillation objectives, our unified design simplifies training and unlocks a powerful new inference strategy: Representation Guidance, which leverages learned semantics to steer and refine image generation. Evaluated in both conditional and unconditional settings, our method delivers substantial improvements in image quality and training convergence speed, establishing a new direction for representation-aware generative modeling.', 'score': 5, 'issue_id': 3433, 'pub_date': '2025-04-22', 'pub_date_card': {'ru': '22 апреля', 'en': 'April 22', 'zh': '4月22日'}, 'hash': 'a72d3bf45681bce8', 'authors': ['Theodoros Kouzelis', 'Efstathios Karypidis', 'Ioannis Kakogeorgiou', 'Spyros Gidaris', 'Nikos Komodakis'], 'affiliations': ["Archimedes, Athena RC IIT, NCSR 'Demokritos'", 'Archimedes, Athena RC National Technical University of Athens', 'Archimedes, Athena RC University of Crete IACM-Forth', 'valeо.ai'], 'pdf_title_img': 'assets/pdf/title_img/2504.16064.jpg', 'data': {'categories': ['#cv', '#inference', '#optimization', '#training', '#diffusion', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Объединение латентных и семантических представлений для улучшенной генерации изображений', 'desc': 'Статья представляет новый подход к генеративному моделированию изображений, объединяющий латентные диффузионные модели с обучением представлений. Авторы предлагают совместно моделировать низкоуровневые латентные переменные изображений и высокоуровневые семантические признаки с помощью диффузионной модели. Этот метод, названный латентно-семантической диффузией, позволяет генерировать согласованные пары изображение-признаки из чистого шума, повышая качество генерации и эффективность обучения. Подход также вводит новую стратегию вывода - Representation Guidance, использующую изученную семантику для управления генерацией изображений.'}, 'en': {'title': 'Bridging Latent and Semantic Features for Superior Image Generation', 'desc': 'This paper presents a new framework for generating high-quality images using latent diffusion models (LDMs). It combines low-level image features from a variational autoencoder with high-level semantic information from a self-supervised encoder. The proposed latent-semantic diffusion method generates image-feature pairs from noise, improving both the quality of generated images and the efficiency of training. Additionally, it introduces a novel inference strategy called Representation Guidance, which uses learned semantics to enhance the image generation process.'}, 'zh': {'title': '潜在-语义扩散：提升图像生成的新方法', 'desc': '潜在扩散模型（LDMs）在高质量图像生成中占据主导地位，但将表示学习与生成建模结合仍然是一个挑战。我们提出了一种新颖的生成图像建模框架，通过利用扩散模型，联合建模低级图像潜变量（来自变分自编码器）和高级语义特征（来自预训练的自监督编码器，如DINO）。我们的潜在-语义扩散方法能够从纯噪声中生成一致的图像特征对，显著提高了生成质量和训练效率，同时对标准扩散变换器架构的修改最小化。通过消除复杂的蒸馏目标，我们的统一设计简化了训练过程，并开启了一种强大的新推理策略：表示引导，利用学习到的语义来引导和优化图像生成。'}}}, {'id': 'https://huggingface.co/papers/2504.17414', 'title': '3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models', 'url': 'https://huggingface.co/papers/2504.17414', 'abstract': 'Video try-on replaces clothing in videos with target garments. Existing methods struggle to generate high-quality and temporally consistent results when handling complex clothing patterns and diverse body poses. We present 3DV-TON, a novel diffusion-based framework for generating high-fidelity and temporally consistent video try-on results. Our approach employs generated animatable textured 3D meshes as explicit frame-level guidance, alleviating the issue of models over-focusing on appearance fidelity at the expanse of motion coherence. This is achieved by enabling direct reference to consistent garment texture movements throughout video sequences. The proposed method features an adaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe for initial 2D image try-on, followed by (2) reconstructing and animating a textured 3D mesh synchronized with original video poses. We further introduce a robust rectangular masking strategy that successfully mitigates artifact propagation caused by leaking clothing information during dynamic human and garment movements. To advance video try-on research, we introduce HR-VVT, a high-resolution benchmark dataset containing 130 videos with diverse clothing types and scenarios. Quantitative and qualitative results demonstrate our superior performance over existing methods. The project page is at this link https://2y7c3.github.io/3DV-TON/', 'score': 4, 'issue_id': 3432, 'pub_date': '2025-04-24', 'pub_date_card': {'ru': '24 апреля', 'en': 'April 24', 'zh': '4月24日'}, 'hash': 'e3156af621b6aa6c', 'authors': ['Min Wei', 'Chaohui Yu', 'Jingkai Zhou', 'Fan Wang'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.17414.jpg', 'data': {'categories': ['#video', '#3d', '#leakage', '#diffusion', '#dataset', '#benchmark'], 'emoji': '👚', 'ru': {'title': '3D-модели для реалистичной виртуальной примерки одежды в видео', 'desc': '3DV-TON - это новый подход к виртуальной примерке одежды в видео, основанный на диффузионных моделях. Метод использует анимируемые текстурированные 3D-модели в качестве покадрового ориентира, что позволяет добиться высокого качества и временной согласованности результатов. В статье предлагается адаптивный алгоритм генерации динамических 3D-ориентиров и стратегия прямоугольного маскирования для устранения артефактов. Авторы также представляют новый набор данных HR-VVT для исследований в области виртуальной примерки одежды в видео.'}, 'en': {'title': 'Revolutionizing Video Try-On with 3D Guidance', 'desc': "The paper introduces 3DV-TON, a new framework for video try-on that enhances the quality and consistency of clothing replacement in videos. It utilizes a diffusion-based approach that generates detailed 3D meshes to guide the video frames, ensuring that the clothing movements remain coherent with the body poses. By focusing on both appearance and motion, the method addresses common challenges in handling complex clothing patterns. Additionally, the authors present a new dataset, HR-VVT, to support further research in this area, showcasing their method's superior performance through extensive evaluations."}, 'zh': {'title': '高保真视频试穿的新方法', 'desc': '视频试穿技术可以在视频中替换目标服装，但现有方法在处理复杂服装图案和多样化身体姿势时，生成的结果质量和时间一致性较差。我们提出了3DV-TON，这是一种新颖的基于扩散的框架，能够生成高保真和时间一致的视频试穿结果。该方法使用生成的可动画纹理3D网格作为明确的帧级指导，解决了模型在外观保真度与运动一致性之间的过度关注问题。我们还引入了一种强大的矩形遮罩策略，有效减轻了动态人类和服装运动中信息泄漏导致的伪影传播。'}}}, {'id': 'https://huggingface.co/papers/2504.17343', 'title': 'TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming\n  Videos', 'url': 'https://huggingface.co/papers/2504.17343', 'abstract': "The rapid growth of online video platforms, particularly live streaming services, has created an urgent need for real-time video understanding systems. These systems must process continuous video streams and respond to user queries instantaneously, presenting unique challenges for current Video Large Language Models (VideoLLMs). While existing VideoLLMs excel at processing complete videos, they face significant limitations in streaming scenarios due to their inability to handle dense, redundant frames efficiently. We introduce TimeChat-Online, a novel online VideoLLM that revolutionizes real-time video interaction. At its core lies our innovative Differential Token Drop (DTD) module, which addresses the fundamental challenge of visual redundancy in streaming videos. Drawing inspiration from human visual perception's Change Blindness phenomenon, DTD preserves meaningful temporal changes while filtering out static, redundant content between frames. Remarkably, our experiments demonstrate that DTD achieves an 82.8% reduction in video tokens while maintaining 98% performance on StreamingBench, revealing that over 80% of visual content in streaming videos is naturally redundant without requiring language guidance. To enable seamless real-time interaction, we present TimeChat-Online-139K, a comprehensive streaming video dataset featuring diverse interaction patterns including backward-tracing, current-perception, and future-responding scenarios. TimeChat-Online's unique Proactive Response capability, naturally achieved through continuous monitoring of video scene transitions via DTD, sets it apart from conventional approaches. Our extensive evaluation demonstrates TimeChat-Online's superior performance on streaming benchmarks (StreamingBench and OvOBench) and maintaining competitive results on long-form video tasks such as Video-MME and MLVU.", 'score': 4, 'issue_id': 3432, 'pub_date': '2025-04-24', 'pub_date_card': {'ru': '24 апреля', 'en': 'April 24', 'zh': '4月24日'}, 'hash': '4d81b69e4ad78f8a', 'authors': ['Linli Yao', 'Yicheng Li', 'Yuancheng Wei', 'Lei Li', 'Shuhuai Ren', 'Yuanxin Liu', 'Kun Ouyang', 'Lean Wang', 'Shicheng Li', 'Sida Li', 'Lingpeng Kong', 'Qi Liu', 'Yuanxing Zhang', 'Xu Sun'], 'affiliations': ['Kuaishou Technology', 'Peking University', 'South China University of Technology', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.17343.jpg', 'data': {'categories': ['#video', '#long_context', '#dataset', '#games', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'TimeChat-Online: революция в понимании потокового видео', 'desc': 'TimeChat-Online - это новая модель VideoLLM для обработки потокового видео в реальном времени. Ключевой компонент - модуль Differential Token Drop (DTD), который сокращает избыточность кадров на 82.8%, сохраняя 98% производительности. Авторы также представили датасет TimeChat-Online-139K для обучения взаимодействию с потоковым видео. Модель демонстрирует превосходные результаты на стриминговых бенчмарках и сохраняет конкурентоспособность в задачах с длинными видео.'}, 'en': {'title': 'Revolutionizing Real-Time Video Interaction with TimeChat-Online', 'desc': 'This paper presents TimeChat-Online, a new Video Large Language Model (VideoLLM) designed for real-time video understanding, particularly in live streaming contexts. It introduces the Differential Token Drop (DTD) module, which effectively reduces visual redundancy by filtering out static frames while preserving important changes, inspired by the human perception phenomenon known as Change Blindness. The model significantly decreases the number of video tokens processed, achieving an 82.8% reduction while maintaining high performance on streaming benchmarks. Additionally, TimeChat-Online features a unique Proactive Response capability that enhances real-time interaction by continuously monitoring video transitions.'}, 'zh': {'title': '实时视频理解的革命性进展', 'desc': '随着在线直播平台的快速发展，实时视频理解系统的需求变得迫切。这些系统需要处理连续的视频流并即时响应用户查询，给现有的视频大语言模型（VideoLLMs）带来了挑战。我们提出了TimeChat-Online，这是一种新型的在线VideoLLM，采用了创新的差分令牌丢弃（DTD）模块，有效解决了流媒体视频中的视觉冗余问题。实验表明，DTD在保持高性能的同时，减少了82.8%的视频令牌，显示出流媒体视频中超过80%的视觉内容是冗余的。'}}}, {'id': 'https://huggingface.co/papers/2504.17069', 'title': 'Distilling semantically aware orders for autoregressive image generation', 'url': 'https://huggingface.co/papers/2504.17069', 'abstract': 'Autoregressive patch-based image generation has recently shown competitive results in terms of image quality and scalability. It can also be easily integrated and scaled within Vision-Language models. Nevertheless, autoregressive models require a defined order for patch generation. While a natural order based on the dictation of the words makes sense for text generation, there is no inherent generation order that exists for image generation. Traditionally, a raster-scan order (from top-left to bottom-right) guides autoregressive image generation models. In this paper, we argue that this order is suboptimal, as it fails to respect the causality of the image content: for instance, when conditioned on a visual description of a sunset, an autoregressive model may generate clouds before the sun, even though the color of clouds should depend on the color of the sun and not the inverse. In this work, we show that first by training a model to generate patches in any-given-order, we can infer both the content and the location (order) of each patch during generation. Secondly, we use these extracted orders to finetune the any-given-order model to produce better-quality images. Through our experiments, we show on two datasets that this new generation method produces better images than the traditional raster-scan approach, with similar training costs and no extra annotations.', 'score': 4, 'issue_id': 3430, 'pub_date': '2025-04-23', 'pub_date_card': {'ru': '23 апреля', 'en': 'April 23', 'zh': '4月23日'}, 'hash': '8311350122eafda9', 'authors': ['Rishav Pramanik', 'Antoine Poupon', 'Juan A. Rodriguez', 'Masih Aminbeidokhti', 'David Vazquez', 'Christopher Pal', 'Zhaozheng Yin', 'Marco Pedersoli'], 'affiliations': ['Canada CIFAR AI Chair', 'Ecole de technologie superieure, QC, Canada', 'International Laboratory on Learning Systems (ILLS)', 'Mila-Quebec AI Institute', 'Polytechnique Montreal', 'ServiceNow Research', 'Stony Brook University, NY, USA', 'Universite Paris-Saclay, CentraleSupelec, France'], 'pdf_title_img': 'assets/pdf/title_img/2504.17069.jpg', 'data': {'categories': ['#optimization', '#cv', '#training'], 'emoji': '🧩', 'ru': {'title': 'Умная генерация изображений: правильный порядок имеет значение', 'desc': 'Статья представляет новый подход к авторегрессивной генерации изображений по патчам. Авторы предлагают обучать модель генерировать патчи в произвольном порядке, а затем во время генерации определять как содержимое, так и расположение каждого патча. Этот метод позволяет учитывать причинно-следственные связи в содержании изображения, в отличие от традиционного подхода с фиксированным порядком генерации. Эксперименты показывают, что предложенный метод позволяет получать изображения лучшего качества при аналогичных затратах на обучение.'}, 'en': {'title': 'Revolutionizing Image Generation with Causal Patch Ordering', 'desc': 'This paper discusses a new approach to autoregressive image generation that improves upon the traditional raster-scan method. The authors argue that the raster-scan order does not respect the natural dependencies in image content, leading to suboptimal results. They propose a model that can generate image patches in any order, allowing for better alignment with the causal relationships in the image. Their experiments demonstrate that this method yields higher quality images while maintaining similar training costs and requiring no additional annotations.'}, 'zh': {'title': '优化图像生成顺序，提升质量', 'desc': '本文探讨了自回归基于补丁的图像生成方法，指出传统的光栅扫描顺序在生成图像时并不理想，因为它未能考虑图像内容的因果关系。我们提出了一种新方法，通过训练模型以任意顺序生成补丁，从而在生成过程中推断每个补丁的内容和位置。接着，我们利用提取的顺序对模型进行微调，以提高生成图像的质量。实验结果表明，这种新方法在两个数据集上生成的图像质量优于传统的光栅扫描方法，且训练成本相似，无需额外的标注。'}}}, {'id': 'https://huggingface.co/papers/2504.15921', 'title': 'ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting', 'url': 'https://huggingface.co/papers/2504.15921', 'abstract': "We introduce ViSMap: Unsupervised Video Summarisation by Meta Prompting, a system to summarise hour long videos with no-supervision. Most existing video understanding models work well on short videos of pre-segmented events, yet they struggle to summarise longer videos where relevant events are sparsely distributed and not pre-segmented. Moreover, long-form video understanding often relies on supervised hierarchical training that needs extensive annotations which are costly, slow and prone to inconsistency. With ViSMaP we bridge the gap between short videos (where annotated data is plentiful) and long ones (where it's not). We rely on LLMs to create optimised pseudo-summaries of long videos using segment descriptions from short ones. These pseudo-summaries are used as training data for a model that generates long-form video summaries, bypassing the need for expensive annotations of long videos. Specifically, we adopt a meta-prompting strategy to iteratively generate and refine creating pseudo-summaries of long videos. The strategy leverages short clip descriptions obtained from a supervised short video model to guide the summary. Each iteration uses three LLMs working in sequence: one to generate the pseudo-summary from clip descriptions, another to evaluate it, and a third to optimise the prompt of the generator. This iteration is necessary because the quality of the pseudo-summaries is highly dependent on the generator prompt, and varies widely among videos. We evaluate our summaries extensively on multiple datasets; our results show that ViSMaP achieves performance comparable to fully supervised state-of-the-art models while generalising across domains without sacrificing performance. Code will be released upon publication.", 'score': 4, 'issue_id': 3432, 'pub_date': '2025-04-22', 'pub_date_card': {'ru': '22 апреля', 'en': 'April 22', 'zh': '4月22日'}, 'hash': 'ed8621b93c4f4cbe', 'authors': ['Jian Hu', 'Dimitrios Korkinof', 'Shaogang Gong', 'Mariano Beguerisse-Diaz'], 'affiliations': ['Queen Mary University of London', 'Spotify'], 'pdf_title_img': 'assets/pdf/title_img/2504.15921.jpg', 'data': {'categories': ['#video', '#multimodal', '#dataset', '#optimization', '#transfer_learning'], 'emoji': '🎥', 'ru': {'title': 'ViSMap: Умное суммирование видео без учителя', 'desc': 'ViSMap - это система для несупервизорного суммаризации длинных видео, использующая метапромптинг. Она преодолевает разрыв между короткими видео, для которых есть много аннотированных данных, и длинными, для которых их нет. Система использует языковые модели для создания оптимизированных псевдо-саммари длинных видео на основе описаний сегментов коротких. Эти псевдо-саммари затем используются для обучения модели генерации саммари длинных видео без необходимости дорогостоящей разметки.'}, 'en': {'title': 'Unsupervised Video Summarization Made Easy with ViSMap!', 'desc': 'ViSMap is an innovative system designed for unsupervised video summarization, particularly effective for long videos where relevant events are not clearly defined. Unlike traditional models that require extensive annotations and struggle with longer content, ViSMap utilizes large language models (LLMs) to create optimized pseudo-summaries from short video segment descriptions. This approach allows the model to generate long-form video summaries without the need for costly and time-consuming annotations. By employing a meta-prompting strategy, ViSMap iteratively refines these pseudo-summaries, achieving performance that rivals fully supervised models while maintaining versatility across different video domains.'}, 'zh': {'title': '无监督视频摘要的新突破', 'desc': '我们介绍了ViSMap：一种通过元提示进行无监督视频摘要的系统，能够对长达一小时的视频进行总结。现有的视频理解模型在短视频上表现良好，但在长视频中，由于相关事件分布稀疏且未预先分段，效果较差。ViSMap通过利用短视频的片段描述，生成优化的伪摘要，作为训练数据来生成长视频摘要，从而避免了昂贵的长视频注释需求。该方法采用元提示策略，迭代生成和优化长视频的伪摘要，最终实现了与完全监督的最先进模型相当的性能。'}}}, {'id': 'https://huggingface.co/papers/2504.17601', 'title': 'Interpretable non-linear dimensionality reduction using gaussian\n  weighted linear transformation', 'url': 'https://huggingface.co/papers/2504.17601', 'abstract': 'Dimensionality reduction techniques are fundamental for analyzing and visualizing high-dimensional data. With established methods like t-SNE and PCA presenting a trade-off between representational power and interpretability. This paper introduces a novel approach that bridges this gap by combining the interpretability of linear methods with the expressiveness of non-linear transformations. The proposed algorithm constructs a non-linear mapping between high-dimensional and low-dimensional spaces through a combination of linear transformations, each weighted by Gaussian functions. This architecture enables complex non-linear transformations while preserving the interpretability advantages of linear methods, as each transformation can be analyzed independently. The resulting model provides both powerful dimensionality reduction and transparent insights into the transformed space. Techniques for interpreting the learned transformations are presented, including methods for identifying suppressed dimensions and how space is expanded and contracted. These tools enable practitioners to understand how the algorithm preserves and modifies geometric relationships during dimensionality reduction. To ensure the practical utility of this algorithm, the creation of user-friendly software packages is emphasized, facilitating its adoption in both academia and industry.', 'score': 2, 'issue_id': 3431, 'pub_date': '2025-04-24', 'pub_date_card': {'ru': '24 апреля', 'en': 'April 24', 'zh': '4月24日'}, 'hash': 'd796e92b08ec0b7e', 'authors': ['Erik Bergh'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.17601.jpg', 'data': {'categories': ['#optimization', '#data', '#interpretability', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'Интерпретируемое нелинейное снижение размерности: мощность и прозрачность', 'desc': 'Эта статья представляет новый метод снижения размерности данных, сочетающий интерпретируемость линейных методов с выразительностью нелинейных преобразований. Алгоритм создает нелинейное отображение между пространствами высокой и низкой размерности, используя комбинацию линейных преобразований, взвешенных гауссовыми функциями. Предложенный подход обеспечивает мощное снижение размерности и прозрачное понимание преобразованного пространства. Авторы также представляют методы интерпретации полученных преобразований, включая идентификацию подавленных измерений и анализ расширения и сжатия пространства.'}, 'en': {'title': 'Bridging Interpretability and Expressiveness in Dimensionality Reduction', 'desc': 'This paper presents a new method for dimensionality reduction that combines the strengths of linear and non-linear techniques. It uses a series of linear transformations weighted by Gaussian functions to create a non-linear mapping from high-dimensional to low-dimensional spaces. This approach maintains the interpretability of linear methods while allowing for complex transformations, making it easier to analyze the results. Additionally, the paper offers tools for understanding how the algorithm modifies geometric relationships, ensuring that users can gain insights into the data effectively.'}, 'zh': {'title': '结合可解释性与表现力的降维新方法', 'desc': '本文介绍了一种新的降维方法，旨在解决现有方法（如t-SNE和PCA）在表现力和可解释性之间的权衡。该算法通过线性变换的组合，结合高斯函数，构建高维与低维空间之间的非线性映射。这样可以实现复杂的非线性变换，同时保持线性方法的可解释性，使每个变换都可以独立分析。文章还提供了理解学习到的变换的工具，帮助用户理解算法在降维过程中如何保持和修改几何关系。'}}}, {'id': 'https://huggingface.co/papers/2504.17788', 'title': 'Dynamic Camera Poses and Where to Find Them', 'url': 'https://huggingface.co/papers/2504.17788', 'abstract': 'Annotating camera poses on dynamic Internet videos at scale is critical for advancing fields like realistic video generation and simulation. However, collecting such a dataset is difficult, as most Internet videos are unsuitable for pose estimation. Furthermore, annotating dynamic Internet videos present significant challenges even for state-of-theart methods. In this paper, we introduce DynPose-100K, a large-scale dataset of dynamic Internet videos annotated with camera poses. Our collection pipeline addresses filtering using a carefully combined set of task-specific and generalist models. For pose estimation, we combine the latest techniques of point tracking, dynamic masking, and structure-from-motion to achieve improvements over the state-of-the-art approaches. Our analysis and experiments demonstrate that DynPose-100K is both large-scale and diverse across several key attributes, opening up avenues for advancements in various downstream applications.', 'score': 1, 'issue_id': 3446, 'pub_date': '2025-04-24', 'pub_date_card': {'ru': '24 апреля', 'en': 'April 24', 'zh': '4月24日'}, 'hash': '323b716eb3d3508c', 'authors': ['Chris Rockwell', 'Joseph Tung', 'Tsung-Yi Lin', 'Ming-Yu Liu', 'David F. Fouhey', 'Chen-Hsuan Lin'], 'affiliations': ['NVIDIA', 'New York University', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2504.17788.jpg', 'data': {'categories': ['#dataset', '#data', '#cv'], 'emoji': '🎥', 'ru': {'title': 'DynPose-100K: Масштабный датасет для оценки положения камеры в динамических видео', 'desc': 'DynPose-100K - это крупномасштабный набор данных динамических видео из интернета с аннотациями положения камеры. Авторы разработали конвейер сбора данных, использующий комбинацию специализированных и общих моделей для фильтрации видео. Для оценки положения камеры применяются современные методы отслеживания точек, динамического маскирования и восстановления структуры из движения. Эксперименты показывают, что DynPose-100K является масштабным и разнообразным набором данных, открывающим возможности для развития различных приложений.'}, 'en': {'title': 'Unlocking Dynamic Video Analysis with DynPose-100K', 'desc': "This paper presents DynPose-100K, a new large-scale dataset designed for annotating camera poses in dynamic Internet videos. The authors highlight the challenges of using typical Internet videos for pose estimation and propose a novel collection pipeline that utilizes a mix of specialized and general models for effective filtering. They employ advanced techniques such as point tracking, dynamic masking, and structure-from-motion to enhance pose estimation accuracy. The dataset's diversity and scale are shown to provide significant opportunities for improving various applications in video generation and simulation."}, 'zh': {'title': '动态视频姿态标注的新突破', 'desc': '本论文介绍了DynPose-100K，这是一个大规模的动态互联网视频数据集，专门标注了相机姿态。收集这样的数据集非常困难，因为大多数互联网视频不适合进行姿态估计。我们提出了一种数据收集流程，结合了特定任务和通用模型进行过滤，以确保数据的质量。通过结合最新的点跟踪、动态遮罩和运动重建技术，我们在姿态估计上取得了显著的进展，DynPose-100K为多个下游应用的研究提供了新的机会。'}}}, {'id': 'https://huggingface.co/papers/2504.17670', 'title': 'DiMeR: Disentangled Mesh Reconstruction Model', 'url': 'https://huggingface.co/papers/2504.17670', 'abstract': "With the advent of large-scale 3D datasets, feed-forward 3D generative models, such as the Large Reconstruction Model (LRM), have gained significant attention and achieved remarkable success. However, we observe that RGB images often lead to conflicting training objectives and lack the necessary clarity for geometry reconstruction. In this paper, we revisit the inductive biases associated with mesh reconstruction and introduce DiMeR, a novel disentangled dual-stream feed-forward model for sparse-view mesh reconstruction. The key idea is to disentangle both the input and framework into geometry and texture parts, thereby reducing the training difficulty for each part according to the Principle of Occam's Razor. Given that normal maps are strictly consistent with geometry and accurately capture surface variations, we utilize normal maps as exclusive input for the geometry branch to reduce the complexity between the network's input and output. Moreover, we improve the mesh extraction algorithm to introduce 3D ground truth supervision. As for texture branch, we use RGB images as input to obtain the textured mesh. Overall, DiMeR demonstrates robust capabilities across various tasks, including sparse-view reconstruction, single-image-to-3D, and text-to-3D. Numerous experiments show that DiMeR significantly outperforms previous methods, achieving over 30% improvement in Chamfer Distance on the GSO and OmniObject3D dataset.", 'score': 1, 'issue_id': 3440, 'pub_date': '2025-04-24', 'pub_date_card': {'ru': '24 апреля', 'en': 'April 24', 'zh': '4月24日'}, 'hash': '8c9910e39da45e41', 'authors': ['Lutao Jiang', 'Jiantao Lin', 'Kanghao Chen', 'Wenhang Ge', 'Xin Yang', 'Yifan Jiang', 'Yuanhuiyi Lyu', 'Xu Zheng', 'Yingcong Chen'], 'affiliations': ['HKUST', 'HKUST(GZ)'], 'pdf_title_img': 'assets/pdf/title_img/2504.17670.jpg', 'data': {'categories': ['#3d', '#optimization', '#dataset'], 'emoji': '🧊', 'ru': {'title': 'Разделяй и властвуй: новый подход к 3D-реконструкции', 'desc': 'Статья представляет DiMeR - новую модель для реконструкции трехмерных объектов по нескольким изображениям. Основная идея заключается в разделении входных данных и архитектуры на геометрическую и текстурную части, что упрощает обучение согласно принципу бритвы Оккама. Для геометрической ветви используются карты нормалей, а для текстурной - RGB-изображения. DiMeR показывает значительное улучшение по сравнению с предыдущими методами, достигая более 30% улучшения метрики Chamfer Distance на наборах данных GSO и OmniObject3D.'}, 'en': {'title': 'Disentangling Geometry and Texture for Superior 3D Mesh Reconstruction', 'desc': "This paper presents DiMeR, a new model designed for reconstructing 3D meshes from limited views. It separates the input data into geometry and texture components, which simplifies the training process and aligns with the Principle of Occam's Razor. By using normal maps for the geometry part, the model enhances the accuracy of surface details, while RGB images are utilized for the texture part. DiMeR shows significant improvements in various tasks, outperforming existing methods by over 30% in Chamfer Distance metrics on specific datasets."}, 'zh': {'title': '解耦双流模型，提升3D重建精度', 'desc': '随着大规模3D数据集的出现，前馈3D生成模型如大型重建模型（LRM）受到了广泛关注并取得了显著成功。然而，我们发现RGB图像常常导致训练目标冲突，并且在几何重建中缺乏必要的清晰度。本文重新审视了网格重建的归纳偏差，提出了一种新颖的解耦双流前馈模型DiMeR，用于稀疏视图网格重建。DiMeR通过将输入和框架解耦为几何和纹理部分，降低了每个部分的训练难度，并在多个任务中表现出强大的能力。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi', '#alignment', '#architecture (4)', '#audio', '#benchmark (11)', '#cv (8)', '#data (5)', '#dataset (9)', '#diffusion (4)', '#ethics', '#games (1)', '#graphs', '#hallucinations', '#healthcare', '#inference (2)', '#interpretability (1)', '#leakage (1)', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (7)', '#open_source (3)', '#optimization (10)', '#plp', '#rag', '#reasoning (2)', '#rl', '#rlhf', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation', '#survey', '#synthetic', '#training (6)', '#transfer_learning (2)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-04-25 22:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-25 22:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-25 22:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    