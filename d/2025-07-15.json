{
    "date": {
        "ru": "15 июля",
        "en": "July 15",
        "zh": "7月15日"
    },
    "time_utc": "2025-07-15 05:17",
    "weekday": 1,
    "issue_id": 4817,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.10548",
            "title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments",
            "url": "https://huggingface.co/papers/2507.10548",
            "abstract": "A new dataset, EmRACE-3K, evaluates vision-language models in embodied settings, showing limitations in spatial reasoning and long-horizon planning, and demonstrates improvements through supervised and reinforcement learning fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advanced vision-language models(VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings, which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from a first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass a wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as a multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to evaluate the embodied reasoning capabilities of VLMs across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the dataset's effectiveness in enabling the development of embodied reasoning capabilities.",
            "score": 11,
            "issue_id": 4817,
            "pub_date": "2025-07-14",
            "pub_date_card": {
                "ru": "14 июля",
                "en": "July 14",
                "zh": "7月14日"
            },
            "hash": "046b6dd86b975bd1",
            "authors": [
                "Mingxian Lin",
                "Wei Huang",
                "Yitang Li",
                "Chengjie Jiang",
                "Kui Wu",
                "Fangwei Zhong",
                "Shengju Qian",
                "Xin Wang",
                "Xiaojuan Qi"
            ],
            "affiliations": [
                "Beijing Normal University",
                "LIGHTSPEED",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.10548.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#long_context",
                    "#multimodal",
                    "#benchmark",
                    "#rl",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "EmRACE-3K: Новый рубеж в обучении воплощенного ИИ",
                    "desc": "Статья представляет новый набор данных EmRACE-3K для оценки моделей компьютерного зрения и обработки естественного языка в воплощенных средах. Исследование выявляет ограничения современных моделей в пространственных рассуждениях и долгосрочном планировании. Авторы демонстрируют улучшения производительности моделей через дообучение с учителем и обучение с подкреплением. EmRACE-3K содержит более 3000 задач, управляемых естественным языком, в разнообразных фотореалистичных средах."
                },
                "en": {
                    "title": "Enhancing Embodied Reasoning with EmRACE-3K Dataset",
                    "desc": "The paper introduces EmRACE-3K, a new dataset designed to evaluate vision-language models (VLMs) in embodied settings, where agents interact with environments in real-time. It highlights the limitations of current state-of-the-art models in spatial reasoning and long-horizon planning, particularly in open-environment interactions. The dataset includes over 3,000 tasks that require navigation, object manipulation, and multi-stage goal execution, providing a benchmark for assessing embodied reasoning capabilities. Fine-tuning a model using supervised and reinforcement learning on this dataset shows significant improvements, demonstrating its potential to enhance VLM performance in interactive scenarios."
                },
                "zh": {
                    "title": "EmRACE-3K：提升视觉-语言模型的具身推理能力",
                    "desc": "本论文介绍了一个新的数据集EmRACE-3K，用于评估视觉-语言模型在具身环境中的表现。研究发现，当前的模型在空间推理和长远规划方面存在明显的局限性，尤其是在需要实时互动的场景中。通过使用EmRACE-3K数据集，研究者们建立了一个基准，评估模型在探索、动态空间语义推理和多阶段目标执行等方面的能力。最后，通过监督学习和强化学习的微调，显著提升了模型在这些挑战中的表现，证明了该数据集在发展具身推理能力方面的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.09862",
            "title": "SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual\n  Dyadic Interactive Human Generation",
            "url": "https://huggingface.co/papers/2507.09862",
            "abstract": "A large-scale dataset named SpeakerVid-5M is introduced for audio-visual dyadic interactive virtual human generation, featuring diverse interactions and high-quality data for various virtual human tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid development of large-scale models has catalyzed significant breakthroughs in the digital human domain. These advanced methodologies offer high-fidelity solutions for avatar driving and rendering, leading academia to focus on the next major challenge: audio-visual dyadic interactive virtual human. To facilitate research in this emerging area, we present SpeakerVid-5M dataset, the first large-scale, high-quality dataset designed for audio-visual dyadic interactive virtual human generation. Totaling over 8,743 hours, SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It covers diverse scales and interaction types, including monadic talking, listening, and dyadic conversations. Crucially, the dataset is structured along two key dimensions: interaction type and data quality. First, it is categorized into four types (dialogue branch, single branch, listening branch and multi-turn branch) based on the interaction scenario. Second, it is stratified into a large-scale pre-training subset and a curated, high-quality subset for Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of 2D virtual human tasks. In addition, we provide an autoregressive (AR)-based video chat baseline trained on this data, accompanied by a dedicated set of metrics and test data to serve as a benchmark VidChatBench for future work. Both the dataset and the corresponding data processing code will be publicly released. Project page: https://dorniwang.github.io/SpeakerVid-5M/",
            "score": 10,
            "issue_id": 4814,
            "pub_date": "2025-07-14",
            "pub_date_card": {
                "ru": "14 июля",
                "en": "July 14",
                "zh": "7月14日"
            },
            "hash": "67b82ab227be6ce9",
            "authors": [
                "Youliang Zhang",
                "Zhaoyang Li",
                "Duomin Wang",
                "Jiahe Zhang",
                "Deyu Zhou",
                "Zixin Yin",
                "Xili Dai",
                "Gang Yu",
                "Xiu Li"
            ],
            "affiliations": [
                "StepFun",
                "The Hong Kong University of Science and Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.09862.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "SpeakerVid-5M: Революция в создании интерактивных виртуальных людей",
                    "desc": "Представлен крупномасштабный набор данных SpeakerVid-5M для генерации аудиовизуальных диадических интерактивных виртуальных людей. Датасет содержит более 5,2 миллиона видеоклипов с портретами людей, охватывающих различные типы взаимодействий. Структура набора данных организована по двум ключевым измерениям: типу взаимодействия и качеству данных. Авторы также предоставляют базовую модель видеочата на основе авторегрессии и набор метрик для оценки будущих работ в этой области."
                },
                "en": {
                    "title": "Unlocking Interactive Virtual Humans with SpeakerVid-5M",
                    "desc": "The paper introduces the SpeakerVid-5M dataset, a large-scale resource designed for generating audio-visual dyadic interactive virtual humans. It consists of over 8,743 hours of video data, featuring more than 5.2 million clips that capture various interaction types, such as monadic and dyadic conversations. The dataset is organized into two main categories: interaction type and data quality, allowing for effective pre-training and fine-tuning of models. Additionally, the authors present a baseline video chat model and a benchmark called VidChatBench to facilitate future research in this area."
                },
                "zh": {
                    "title": "SpeakerVid-5M：音视频互动虚拟人的新里程碑",
                    "desc": "本文介绍了一个名为SpeakerVid-5M的大规模数据集，旨在生成音视频双向互动的虚拟人。该数据集包含超过8743小时的高质量视频片段，涵盖多种互动类型，如单向对话和双向对话。数据集按照互动类型和数据质量两个维度进行结构化，支持多种2D虚拟人任务。我们还提供了基于自回归模型的视频聊天基线，并设立了VidChatBench作为未来研究的基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.09104",
            "title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards",
            "url": "https://huggingface.co/papers/2507.09104",
            "abstract": "CompassJudger-2, a generalist judge model, achieves superior performance across multiple benchmarks through task-driven data curation, verifiable rewards, and a refined learning objective with margin policy gradient loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, the role of LLM-as-judge in evaluating large language models has gained prominence. However, current judge models suffer from narrow specialization and limited robustness, undermining their capacity for comprehensive evaluations. In this work, we present CompassJudger-2, a novel generalist judge model that overcomes these limitations via a task-driven, multi-domain data curation strategy. Central to our approach is supervising judgment tasks with verifiable rewards, guiding intrinsic critical reasoning through rejection sampling to foster robust, generalizable judgment capabilities. We introduce a refined learning objective with margin policy gradient loss to enhance performance. Empirically, CompassJudger-2 achieves superior results across multiple judge and reward benchmarks, and our 7B model demonstrates competitive judgment accuracy with significantly larger models like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a comprehensive benchmark evaluating cross-domain judgment accuracy and rank consistency to standardize judge model evaluation. These contributions advance robust, scalable LLM judgment and establish new performance and evaluation standards.",
            "score": 2,
            "issue_id": 4816,
            "pub_date": "2025-07-12",
            "pub_date_card": {
                "ru": "12 июля",
                "en": "July 12",
                "zh": "7月12日"
            },
            "hash": "40a4c36902a53203",
            "authors": [
                "Taolin Zhang",
                "Maosong Cao",
                "Alexander Lam",
                "Songyang Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.09104.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#architecture",
                    "#training",
                    "#benchmark",
                    "#dataset",
                    "#agi",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "CompassJudger-2: универсальный судья для ИИ-моделей",
                    "desc": "CompassJudger-2 - это новая модель-судья общего назначения для оценки больших языковых моделей. Она использует многодоменную стратегию курации данных и обучение с подкреплением для развития надежных способностей к суждению. Модель применяет усовершенствованную целевую функцию с градиентным спуском по политике с маржой. CompassJudger-2 демонстрирует превосходные результаты на различных тестовых наборах для оценки моделей-судей."
                },
                "en": {
                    "title": "Revolutionizing Model Evaluation with CompassJudger-2",
                    "desc": "CompassJudger-2 is a versatile judge model designed to evaluate large language models more effectively. It improves upon previous models by using a task-driven approach to curate diverse data, ensuring robust evaluations across different domains. The model employs verifiable rewards and a refined learning objective with margin policy gradient loss to enhance its judgment capabilities. With its superior performance on various benchmarks, CompassJudger-2 sets new standards for accuracy and consistency in model evaluation."
                },
                "zh": {
                    "title": "CompassJudger-2：通用评判模型的突破",
                    "desc": "CompassJudger-2是一种通用评判模型，通过任务驱动的数据整理、可验证的奖励和改进的学习目标（边际策略梯度损失）在多个基准测试中表现优异。该模型克服了现有评判模型的专业化狭窄和鲁棒性不足的问题，能够进行全面的评估。我们的方法通过可验证的奖励来监督判断任务，并通过拒绝采样促进内在的批判性推理，从而增强判断能力。CompassJudger-2在多个评判和奖励基准测试中取得了优异的结果，展示了与更大模型的竞争性判断准确性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.04404",
            "title": "LayerCake: Token-Aware Contrastive Decoding within Large Language Model\n  Layers",
            "url": "https://huggingface.co/papers/2507.04404",
            "abstract": "A token-aware, layer-localized contrastive decoding method improves factual accuracy in large language models by selectively suppressing attention to specific token types at their respective depths.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at natural language understanding and generation but remain vulnerable to factual errors, limiting their reliability in knowledge-intensive tasks. While decoding-time strategies provide a promising efficient solution without training, existing methods typically treat token-level and layer-level signals in isolation, overlooking the joint dynamics between them. In this work, we introduce a token-aware, layer-localized contrastive decoding method that aligns specific token types with their most influential transformer layers to improve factual generation. Through empirical attention analysis, we identify two key patterns: punctuation tokens receive dominant attention in early layers, while conceptual tokens govern semantic reasoning in intermediate layers. By selectively suppressing attention to these token types at their respective depths, we achieve the induction of controlled factual degradation and derive contrastive signals to guide the final factual decoding. Our method requires no additional training or model modification, and experiments demonstrate that our method consistently improves factuality across multiple LLMs and various benchmarks.",
            "score": 2,
            "issue_id": 4815,
            "pub_date": "2025-07-06",
            "pub_date_card": {
                "ru": "6 июля",
                "en": "July 6",
                "zh": "7月6日"
            },
            "hash": "67be09e9b6db6fc6",
            "authors": [
                "Jingze Zhu",
                "Yongliang Wu",
                "Wenbo Zhu",
                "Jiawang Cao",
                "Yanqiang Zheng",
                "Jiawei Chen",
                "Xu Yang",
                "Bernt Schiele",
                "Jonas Fischer",
                "Xinting Hu"
            ],
            "affiliations": [
                "Growth, Xiaomi Corporation",
                "Max Planck Institute for Informatics",
                "Opus AI Research",
                "Southeast University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.04404.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#hallucinations",
                    "#benchmark",
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Повышение фактической точности языковых моделей через токен-ориентированное контрастное декодирование",
                    "desc": "Статья представляет новый метод контрастного декодирования для улучшения фактической точности больших языковых моделей (LLM). Метод основан на избирательном подавлении внимания к определенным типам токенов на соответствующих глубинах трансформера. Авторы обнаружили, что знаки препинания получают доминирующее внимание на ранних слоях, а концептуальные токены управляют семантическими рассуждениями на промежуточных слоях. Эксперименты показывают, что предложенный метод последовательно улучшает фактическую точность различных LLM на нескольких эталонных наборах данных."
                },
                "en": {
                    "title": "Enhancing Factual Accuracy in LLMs with Layer-Localized Attention",
                    "desc": "This paper presents a new method called token-aware, layer-localized contrastive decoding to enhance the factual accuracy of large language models (LLMs). The approach focuses on managing attention to different types of tokens at specific layers of the model, which helps in reducing factual errors during text generation. By analyzing how punctuation and conceptual tokens are processed in different layers, the method effectively suppresses attention to these tokens when necessary. The results show that this technique improves the factuality of generated text without requiring any additional training or changes to the model architecture."
                },
                "zh": {
                    "title": "提升大型语言模型事实性的对比解码方法",
                    "desc": "本文提出了一种基于令牌感知和层局部对比解码的方法，以提高大型语言模型的事实准确性。该方法通过在不同层次上选择性地抑制特定类型令牌的注意力，来改善生成的事实内容。研究发现，标点符号在早期层中占主导地位，而概念令牌则在中间层中主导语义推理。通过这种方式，我们的方法在不需要额外训练或模型修改的情况下，显著提高了多个大型语言模型的事实性表现。"
                }
            }
        }
    ],
    "link_prev": "2025-07-14.html",
    "link_next": "2025-07-16.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "14.07",
        "en": "07/14",
        "zh": "7月14日"
    },
    "short_date_next": {
        "ru": "16.07",
        "en": "07/16",
        "zh": "7月16日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}