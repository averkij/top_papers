{
    "date": {
        "ru": "15 июля",
        "en": "July 15",
        "zh": "7月15日"
    },
    "time_utc": "2025-07-15 03:59",
    "weekday": 1,
    "issue_id": 4815,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.09862",
            "title": "SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual\n  Dyadic Interactive Human Generation",
            "url": "https://huggingface.co/papers/2507.09862",
            "abstract": "A large-scale dataset named SpeakerVid-5M is introduced for audio-visual dyadic interactive virtual human generation, featuring diverse interactions and high-quality data for various virtual human tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid development of large-scale models has catalyzed significant breakthroughs in the digital human domain. These advanced methodologies offer high-fidelity solutions for avatar driving and rendering, leading academia to focus on the next major challenge: audio-visual dyadic interactive virtual human. To facilitate research in this emerging area, we present SpeakerVid-5M dataset, the first large-scale, high-quality dataset designed for audio-visual dyadic interactive virtual human generation. Totaling over 8,743 hours, SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It covers diverse scales and interaction types, including monadic talking, listening, and dyadic conversations. Crucially, the dataset is structured along two key dimensions: interaction type and data quality. First, it is categorized into four types (dialogue branch, single branch, listening branch and multi-turn branch) based on the interaction scenario. Second, it is stratified into a large-scale pre-training subset and a curated, high-quality subset for Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of 2D virtual human tasks. In addition, we provide an autoregressive (AR)-based video chat baseline trained on this data, accompanied by a dedicated set of metrics and test data to serve as a benchmark VidChatBench for future work. Both the dataset and the corresponding data processing code will be publicly released. Project page: https://dorniwang.github.io/SpeakerVid-5M/",
            "score": 7,
            "issue_id": 4814,
            "pub_date": "2025-07-14",
            "pub_date_card": {
                "ru": "14 июля",
                "en": "July 14",
                "zh": "7月14日"
            },
            "hash": "67b82ab227be6ce9",
            "authors": [
                "Youliang Zhang",
                "Zhaoyang Li",
                "Duomin Wang",
                "Jiahe Zhang",
                "Deyu Zhou",
                "Zixin Yin",
                "Xili Dai",
                "Gang Yu",
                "Xiu Li"
            ],
            "affiliations": [
                "StepFun",
                "The Hong Kong University of Science and Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.09862.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "SpeakerVid-5M: Революция в создании интерактивных виртуальных людей",
                    "desc": "Представлен крупномасштабный набор данных SpeakerVid-5M для генерации аудиовизуальных диадических интерактивных виртуальных людей. Датасет содержит более 5,2 миллиона видеоклипов с портретами людей, охватывающих различные типы взаимодействий. Структура набора данных организована по двум ключевым измерениям: типу взаимодействия и качеству данных. Авторы также предоставляют базовую модель видеочата на основе авторегрессии и набор метрик для оценки будущих работ в этой области."
                },
                "en": {
                    "title": "Unlocking Interactive Virtual Humans with SpeakerVid-5M",
                    "desc": "The paper introduces the SpeakerVid-5M dataset, a large-scale resource designed for generating audio-visual dyadic interactive virtual humans. It consists of over 8,743 hours of video data, featuring more than 5.2 million clips that capture various interaction types, such as monadic and dyadic conversations. The dataset is organized into two main categories: interaction type and data quality, allowing for effective pre-training and fine-tuning of models. Additionally, the authors present a baseline video chat model and a benchmark called VidChatBench to facilitate future research in this area."
                },
                "zh": {
                    "title": "SpeakerVid-5M：音视频互动虚拟人的新里程碑",
                    "desc": "本文介绍了一个名为SpeakerVid-5M的大规模数据集，旨在生成音视频双向互动的虚拟人。该数据集包含超过8743小时的高质量视频片段，涵盖多种互动类型，如单向对话和双向对话。数据集按照互动类型和数据质量两个维度进行结构化，支持多种2D虚拟人任务。我们还提供了基于自回归模型的视频聊天基线，并设立了VidChatBench作为未来研究的基准。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.04404",
            "title": "LayerCake: Token-Aware Contrastive Decoding within Large Language Model\n  Layers",
            "url": "https://huggingface.co/papers/2507.04404",
            "abstract": "A token-aware, layer-localized contrastive decoding method improves factual accuracy in large language models by selectively suppressing attention to specific token types at their respective depths.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at natural language understanding and generation but remain vulnerable to factual errors, limiting their reliability in knowledge-intensive tasks. While decoding-time strategies provide a promising efficient solution without training, existing methods typically treat token-level and layer-level signals in isolation, overlooking the joint dynamics between them. In this work, we introduce a token-aware, layer-localized contrastive decoding method that aligns specific token types with their most influential transformer layers to improve factual generation. Through empirical attention analysis, we identify two key patterns: punctuation tokens receive dominant attention in early layers, while conceptual tokens govern semantic reasoning in intermediate layers. By selectively suppressing attention to these token types at their respective depths, we achieve the induction of controlled factual degradation and derive contrastive signals to guide the final factual decoding. Our method requires no additional training or model modification, and experiments demonstrate that our method consistently improves factuality across multiple LLMs and various benchmarks.",
            "score": 1,
            "issue_id": 4815,
            "pub_date": "2025-07-06",
            "pub_date_card": {
                "ru": "6 июля",
                "en": "July 6",
                "zh": "7月6日"
            },
            "hash": "67be09e9b6db6fc6",
            "authors": [
                "Jingze Zhu",
                "Yongliang Wu",
                "Wenbo Zhu",
                "Jiawang Cao",
                "Yanqiang Zheng",
                "Jiawei Chen",
                "Xu Yang",
                "Bernt Schiele",
                "Jonas Fischer",
                "Xinting Hu"
            ],
            "affiliations": [
                "Growth, Xiaomi Corporation",
                "Max Planck Institute for Informatics",
                "Opus AI Research",
                "Southeast University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.04404.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#hallucinations",
                    "#benchmark",
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "🎯",
                "ru": {
                    "title": "Повышение фактической точности языковых моделей через токен-ориентированное контрастное декодирование",
                    "desc": "Статья представляет новый метод контрастного декодирования для улучшения фактической точности больших языковых моделей (LLM). Метод основан на избирательном подавлении внимания к определенным типам токенов на соответствующих глубинах трансформера. Авторы обнаружили, что знаки препинания получают доминирующее внимание на ранних слоях, а концептуальные токены управляют семантическими рассуждениями на промежуточных слоях. Эксперименты показывают, что предложенный метод последовательно улучшает фактическую точность различных LLM на нескольких эталонных наборах данных."
                },
                "en": {
                    "title": "Enhancing Factual Accuracy in LLMs with Layer-Localized Attention",
                    "desc": "This paper presents a new method called token-aware, layer-localized contrastive decoding to enhance the factual accuracy of large language models (LLMs). The approach focuses on managing attention to different types of tokens at specific layers of the model, which helps in reducing factual errors during text generation. By analyzing how punctuation and conceptual tokens are processed in different layers, the method effectively suppresses attention to these tokens when necessary. The results show that this technique improves the factuality of generated text without requiring any additional training or changes to the model architecture."
                },
                "zh": {
                    "title": "提升大型语言模型事实性的对比解码方法",
                    "desc": "本文提出了一种基于令牌感知和层局部对比解码的方法，以提高大型语言模型的事实准确性。该方法通过在不同层次上选择性地抑制特定类型令牌的注意力，来改善生成的事实内容。研究发现，标点符号在早期层中占主导地位，而概念令牌则在中间层中主导语义推理。通过这种方式，我们的方法在不需要额外训练或模型修改的情况下，显著提高了多个大型语言模型的事实性表现。"
                }
            }
        }
    ],
    "link_prev": "2025-07-14.html",
    "link_next": "2025-07-16.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "14.07",
        "en": "07/14",
        "zh": "7月14日"
    },
    "short_date_next": {
        "ru": "16.07",
        "en": "07/16",
        "zh": "7月16日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}