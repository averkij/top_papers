
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 21 papers. March 24.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">24 марта</span> | <span id="title-articles-count">21 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-21.html">⬅️ <span id="prev-date">21.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-25.html">➡️ <span id="next-date">25.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'};
        let feedDateNext = {'ru': '25.03', 'en': '03/25', 'zh': '3月25日'};
        let feedDatePrev = {'ru': '21.03', 'en': '03/21', 'zh': '3月21日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.16660', 'title': 'When Less is Enough: Adaptive Token Reduction for Efficient Image\n  Representation', 'url': 'https://huggingface.co/papers/2503.16660', 'abstract': 'Vision encoders typically generate a large number of visual tokens, providing information-rich representations but significantly increasing computational demands. This raises the question of whether all generated tokens are equally valuable or if some of them can be discarded to reduce computational costs without compromising quality. In this paper, we introduce a new method for determining feature utility based on the idea that less valuable features can be reconstructed from more valuable ones. We implement this concept by integrating an autoencoder with a Gumbel-Softmax selection mechanism, that allows identifying and retaining only the most informative visual tokens. To validate our approach, we compared the performance of the LLaVA-NeXT model, using features selected by our method with randomly selected features. We found that on OCR-based tasks, more than 50% of the visual context can be removed with minimal performance loss, whereas randomly discarding the same proportion of features significantly affects the model capabilities. Furthermore, in general-domain tasks, even randomly retaining only 30% of tokens achieves performance comparable to using the full set of visual tokens. Our results highlight a promising direction towards adaptive and efficient multimodal pruning that facilitates scalable and low-overhead inference without compromising performance.', 'score': 53, 'issue_id': 2857, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '22ce97b85d4cf4dd', 'authors': ['Eduard Allakhverdov', 'Elizaveta Goncharova', 'Andrey Kuznetsov'], 'affiliations': ['AIRI Moscow, Russia', 'MIPT Dolgoprudny, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2503.16660.jpg', 'data': {'categories': ['#multimodal', '#inference', '#optimization', '#cv'], 'emoji': '✂️', 'ru': {'title': 'Умная обрезка визуальных токенов для эффективных мультимодальных моделей', 'desc': 'Эта статья представляет новый метод для определения полезности визуальных токенов в энкодерах изображений. Авторы предлагают использовать автоэнкодер с механизмом выбора Gumbel-Softmax для идентификации наиболее информативных токенов. Эксперименты с моделью LLaVA-NeXT показали, что можно удалить более 50% визуального контекста с минимальной потерей производительности на задачах OCR. Результаты открывают перспективы для адаптивной и эффективной мультимодальной обрезки, позволяющей масштабируемый вывод с низкими накладными расходами.'}, 'en': {'title': 'Efficient Feature Selection for Vision Encoders', 'desc': 'This paper addresses the challenge of managing the large number of visual tokens generated by vision encoders, which can lead to high computational costs. The authors propose a method to evaluate the utility of these features, suggesting that less important tokens can be reconstructed from more significant ones. By combining an autoencoder with a Gumbel-Softmax selection mechanism, they effectively identify and keep only the most informative tokens. Their experiments show that significant reductions in visual context can be made with minimal impact on performance, paving the way for more efficient multimodal processing.'}, 'zh': {'title': '智能选择，提升视觉编码效率', 'desc': '本文探讨了视觉编码器生成大量视觉标记所带来的计算需求问题。我们提出了一种新方法，通过重构不太有价值的特征来判断特征的实用性，从而减少计算成本。该方法结合了自编码器和Gumbel-Softmax选择机制，仅保留最有信息量的视觉标记。实验结果表明，在OCR任务中，去除超过50%的视觉上下文对性能影响很小，而随机去除相同比例的特征则会显著降低模型能力。'}}}, {'id': 'https://huggingface.co/papers/2503.16905', 'title': 'MAPS: A Multi-Agent Framework Based on Big Seven Personality and\n  Socratic Guidance for Multimodal Scientific Problem Solving', 'url': 'https://huggingface.co/papers/2503.16905', 'abstract': "Multimodal scientific problems (MSPs) involve complex issues that require the integration of multiple modalities, such as text and diagrams, presenting a significant challenge in artificial intelligence. While progress has been made in addressing traditional scientific problems, MSPs still face two primary issues: the challenge of multi-modal comprehensive reasoning in scientific problem-solving and the lack of reflective and rethinking capabilities. To address these issues, we introduce a Multi-Agent framework based on the Big Seven Personality and Socratic guidance (MAPS). This framework employs seven distinct agents that leverage feedback mechanisms and the Socratic method to guide the resolution of MSPs. To tackle the first issue, we propose a progressive four-agent solving strategy, where each agent focuses on a specific stage of the problem-solving process. For the second issue, we introduce a Critic agent, inspired by Socratic questioning, which prompts critical thinking and stimulates autonomous learning. We conduct extensive experiments on the EMMA, Olympiad, and MathVista datasets, achieving promising results that outperform the current SOTA model by 15.84% across all tasks. Meanwhile, the additional analytical experiments also verify the model's progress as well as generalization ability.", 'score': 45, 'issue_id': 2852, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '996510a66b2e236f', 'authors': ['Jian Zhang', 'Zhiyuan Wang', 'Zhangqi Wang', 'Xinyu Zhang', 'Fangzhi Xu', 'Qika Lin', 'Rui Mao', 'Erik Cambria', 'Jun Liu'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16905.jpg', 'data': {'categories': ['#science', '#dataset', '#agents', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Мультиагентный подход к решению сложных научных задач', 'desc': 'Статья представляет новый подход к решению мультимодальных научных задач (MSP) с использованием искусственного интеллекта. Авторы предлагают мультиагентную систему MAPS, основанную на концепции семи личностей и сократовском методе. Система включает четыре агента для поэтапного решения задач и агента-критика для стимулирования критического мышления. Эксперименты на наборах данных EMMA, Olympiad и MathVista показали превосходство MAPS над современными моделями на 15.84%.'}, 'en': {'title': 'Enhancing Multimodal Problem Solving with MAPS Framework', 'desc': 'This paper addresses the challenges of Multimodal Scientific Problems (MSPs) that require integrating different types of information, like text and diagrams, for effective problem-solving. It introduces a Multi-Agent framework called MAPS, which utilizes seven distinct agents to enhance reasoning and critical thinking through feedback and Socratic questioning. The framework includes a four-agent strategy that focuses on different stages of the problem-solving process and a Critic agent that encourages reflective thinking. Experimental results show that this approach significantly improves performance, surpassing the current state-of-the-art models by 15.84%.'}, 'zh': {'title': '多模态科学问题的智能解决方案', 'desc': '多模态科学问题（MSPs）涉及需要整合多种模态（如文本和图表）的复杂问题，这对人工智能提出了重大挑战。尽管在传统科学问题的解决上已有进展，但MSPs仍面临多模态综合推理和缺乏反思能力的主要问题。为了解决这些问题，我们提出了一种基于大七人格和苏格拉底指导的多智能体框架（MAPS），该框架利用七个不同的智能体，通过反馈机制和苏格拉底方法来指导MSPs的解决。我们的实验表明，该模型在EMMA、奥林匹克和MathVista数据集上取得了超过当前最优模型15.84%的优异结果，验证了模型的进步和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.16874', 'title': 'MARS: A Multi-Agent Framework Incorporating Socratic Guidance for\n  Automated Prompt Optimization', 'url': 'https://huggingface.co/papers/2503.16874', 'abstract': "The basic question-answering format of large language models involves inputting a prompt and receiving a response, and the quality of the prompt directly impacts the effectiveness of the response. Automated Prompt Optimization (APO) aims to break free from the cognitive biases of manually designed prompts and explores a broader design space for prompts. However, existing APO methods suffer from limited flexibility of fixed templates and inefficient search in prompt spaces as key issues. To this end, we propose a Multi-Agent framework Incorporating Socratic guidance (MARS), which utilizes multi-agent fusion technology for automatic planning, with gradual continuous optimization and evaluation. Specifically, MARS comprises seven agents, each with distinct functionalities, which autonomously use the Planner to devise an optimization path that ensures flexibility. Additionally, it employs a Teacher-Critic-Student Socratic dialogue pattern to iteratively optimize the prompts while conducting effective search. We conduct extensive experiments on various datasets to validate the effectiveness of our method, and perform additional analytical experiments to assess the model's advancement as well as the interpretability.", 'score': 38, 'issue_id': 2853, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '995715e84f49b2c2', 'authors': ['Jian Zhang', 'Zhangqi Wang', 'Haiping Zhu', 'Jun Liu', 'Qika Lin', 'Erik Cambria'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16874.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#dataset', '#agents', '#training'], 'emoji': '🤖', 'ru': {'title': 'MARS: мультиагентная система для оптимизации промптов в больших языковых моделях', 'desc': 'Статья представляет новый подход к автоматической оптимизации промптов (APO) для больших языковых моделей. Авторы предлагают мультиагентную систему MARS, использующую технологию слияния агентов для автоматического планирования. MARS состоит из семи агентов с различными функциями и использует сократический диалог для итеративной оптимизации промптов. Эксперименты на различных датасетах подтверждают эффективность метода.'}, 'en': {'title': 'Revolutionizing Prompt Optimization with MARS', 'desc': 'This paper introduces a new approach called MARS, which stands for Multi-Agent framework Incorporating Socratic guidance, to improve the process of prompt optimization in large language models. MARS addresses the limitations of existing Automated Prompt Optimization methods by using a multi-agent system that allows for flexible and efficient exploration of prompt designs. The framework includes seven specialized agents that collaborate to create an optimization strategy, while a Socratic dialogue method helps refine prompts through iterative feedback. Extensive experiments demonstrate that MARS enhances the quality of responses generated by language models and improves the interpretability of the optimization process.'}, 'zh': {'title': '智能体驱动的自动化提示优化', 'desc': '这篇论文探讨了大型语言模型中的问答格式，强调了提示的质量对回答效果的重要性。提出了一种名为MARS的多智能体框架，旨在通过自动化提示优化（APO）来克服手动设计提示的认知偏见。MARS框架包含七个功能不同的智能体，能够灵活地规划优化路径，并通过苏格拉底对话模式进行迭代优化。通过在多个数据集上的实验，验证了该方法的有效性，并评估了模型的进步和可解释性。'}}}, {'id': 'https://huggingface.co/papers/2503.16408', 'title': 'RoboFactory: Exploring Embodied Agent Collaboration with Compositional\n  Constraints', 'url': 'https://huggingface.co/papers/2503.16408', 'abstract': 'Designing effective embodied multi-agent systems is critical for solving complex real-world tasks across domains. Due to the complexity of multi-agent embodied systems, existing methods fail to automatically generate safe and efficient training data for such systems. To this end, we propose the concept of compositional constraints for embodied multi-agent systems, addressing the challenges arising from collaboration among embodied agents. We design various interfaces tailored to different types of constraints, enabling seamless interaction with the physical world. Leveraging compositional constraints and specifically designed interfaces, we develop an automated data collection framework for embodied multi-agent systems and introduce the first benchmark for embodied multi-agent manipulation, RoboFactory. Based on RoboFactory benchmark, we adapt and evaluate the method of imitation learning and analyzed its performance in different difficulty agent tasks. Furthermore, we explore the architectures and training strategies for multi-agent imitation learning, aiming to build safe and efficient embodied multi-agent systems.', 'score': 32, 'issue_id': 2853, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '6ff0e203d45f1c06', 'authors': ['Yiran Qin', 'Li Kang', 'Xiufeng Song', 'Zhenfei Yin', 'Xiaohong Liu', 'Xihui Liu', 'Ruimao Zhang', 'Lei Bai'], 'affiliations': ['HKU', 'Oxford', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Sun Yat-sen University', 'The Chinese University of Hong Kong, Shenzhen', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16408.jpg', 'data': {'categories': ['#optimization', '#games', '#dataset', '#agents', '#training', '#benchmark', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Композиционные ограничения для эффективных воплощенных мультиагентных систем', 'desc': 'Статья представляет концепцию композиционных ограничений для воплощенных мультиагентных систем, решающую проблемы взаимодействия физических агентов. Авторы разработали интерфейсы для различных типов ограничений и автоматизированную систему сбора данных. На основе этого создан первый бенчмарк RoboFactory для воплощенной мультиагентной манипуляции. Исследованы методы имитационного обучения и архитектуры для построения безопасных и эффективных воплощенных мультиагентных систем.'}, 'en': {'title': 'Empowering Multi-Agent Systems with Compositional Constraints', 'desc': 'This paper addresses the challenges of training embodied multi-agent systems, which are crucial for tackling complex tasks in various fields. It introduces the idea of compositional constraints to improve collaboration among agents, allowing for better interaction with their environment. The authors present an automated data collection framework and a new benchmark called RoboFactory, specifically designed for multi-agent manipulation tasks. They also investigate imitation learning techniques and training strategies to enhance the safety and efficiency of these systems.'}, 'zh': {'title': '构建安全高效的多智能体系统', 'desc': '本文提出了一种针对多智能体系统的组合约束概念，以解决智能体之间协作带来的挑战。我们设计了多种接口，适应不同类型的约束，从而实现与物理世界的无缝互动。基于这些组合约束和专门设计的接口，我们开发了一个自动化数据收集框架，并引入了第一个多智能体操作基准RoboFactory。通过RoboFactory基准，我们评估了模仿学习的方法，并探讨了多智能体模仿学习的架构和训练策略，以构建安全高效的多智能体系统。'}}}, {'id': 'https://huggingface.co/papers/2503.16430', 'title': 'Bridging Continuous and Discrete Tokens for Autoregressive Visual\n  Generation', 'url': 'https://huggingface.co/papers/2503.16430', 'abstract': 'Autoregressive visual generation models typically rely on tokenizers to compress images into tokens that can be predicted sequentially. A fundamental dilemma exists in token representation: discrete tokens enable straightforward modeling with standard cross-entropy loss, but suffer from information loss and tokenizer training instability; continuous tokens better preserve visual details, but require complex distribution modeling, complicating the generation pipeline. In this paper, we propose TokenBridge, which bridges this gap by maintaining the strong representation capacity of continuous tokens while preserving the modeling simplicity of discrete tokens. To achieve this, we decouple discretization from the tokenizer training process through post-training quantization that directly obtains discrete tokens from continuous representations. Specifically, we introduce a dimension-wise quantization strategy that independently discretizes each feature dimension, paired with a lightweight autoregressive prediction mechanism that efficiently model the resulting large token space. Extensive experiments show that our approach achieves reconstruction and generation quality on par with continuous methods while using standard categorical prediction. This work demonstrates that bridging discrete and continuous paradigms can effectively harness the strengths of both approaches, providing a promising direction for high-quality visual generation with simple autoregressive modeling. Project page: https://yuqingwang1029.github.io/TokenBridge.', 'score': 28, 'issue_id': 2852, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'e5dba2d5b4674553', 'authors': ['Yuqing Wang', 'Zhijie Lin', 'Yao Teng', 'Yuanzhi Zhu', 'Shuhuai Ren', 'Jiashi Feng', 'Xihui Liu'], 'affiliations': ['ByteDance Seed', 'Ecole Polytechnique', 'Peking University', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.16430.jpg', 'data': {'categories': ['#diffusion', '#inference', '#cv'], 'emoji': '🌉', 'ru': {'title': 'TokenBridge: мост между дискретным и непрерывным в генерации изображений', 'desc': 'Статья представляет TokenBridge - новый подход к автoрегрессивной генерации изображений. Он объединяет преимущества дискретных и непрерывных токенов, используя постобучающее квантование для получения дискретных токенов из непрерывных представлений. Авторы предлагают стратегию поразмерного квантования и легковесный механизм авторегрессивного предсказания для моделирования большого пространства токенов. Эксперименты показывают, что TokenBridge достигает качества реконструкции и генерации на уровне непрерывных методов, используя стандартное категориальное предсказание.'}, 'en': {'title': 'Bridging the Gap: High-Quality Visual Generation with TokenBridge', 'desc': 'This paper introduces TokenBridge, a novel approach for visual generation that addresses the challenges of using discrete and continuous tokens. Discrete tokens are easy to model but can lose important visual information, while continuous tokens preserve details but complicate the generation process. TokenBridge combines the advantages of both by using post-training quantization to convert continuous representations into discrete tokens without losing quality. The method employs a dimension-wise quantization strategy and a lightweight autoregressive model, achieving high-quality image generation with simpler modeling techniques.'}, 'zh': {'title': '桥接离散与连续，提升视觉生成质量', 'desc': '本文提出了一种名为TokenBridge的方法，旨在解决自回归视觉生成模型中离散和连续标记之间的矛盾。离散标记易于建模，但会导致信息损失，而连续标记则能更好地保留视觉细节，但建模复杂。TokenBridge通过后训练量化将离散化与标记器训练过程解耦，从而在保持连续标记强大表示能力的同时，简化建模过程。实验结果表明，该方法在重建和生成质量上与连续方法相当，同时使用标准的分类预测。'}}}, {'id': 'https://huggingface.co/papers/2503.17352', 'title': 'OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement', 'url': 'https://huggingface.co/papers/2503.17352', 'abstract': "Recent advancements demonstrated by DeepSeek-R1 have shown that complex reasoning abilities in large language models (LLMs), including sophisticated behaviors such as self-verification and self-correction, can be achieved by RL with verifiable rewards and significantly improves model performance on challenging tasks such as AIME. Motivated by these findings, our study investigates whether similar reasoning capabilities can be successfully integrated into large vision-language models (LVLMs) and assesses their impact on challenging multimodal reasoning tasks. We consider an approach that iteratively leverages supervised fine-tuning (SFT) on lightweight training data and Reinforcement Learning (RL) to further improve model generalization. Initially, reasoning capabilities were distilled from pure-text R1 models by generating reasoning steps using high-quality captions of the images sourced from diverse visual datasets. Subsequently, iterative RL training further enhance reasoning skills, with each iteration's RL-improved model generating refined SFT datasets for the next round. This iterative process yielded OpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on challenging benchmarks such as MathVista, MathVerse, and MathVision, demonstrating the potential of our strategy for robust vision-language reasoning. The code, model and data are held at https://github.com/yihedeng9/OpenVLThinker.", 'score': 19, 'issue_id': 2852, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '8717a707abecd3ed', 'authors': ['Yihe Deng', 'Hritik Bansal', 'Fan Yin', 'Nanyun Peng', 'Wei Wang', 'Kai-Wei Chang'], 'affiliations': ['University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.17352.jpg', 'data': {'categories': ['#training', '#reasoning', '#benchmark', '#optimization', '#rl', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Совершенствование мультимодальных рассуждений с помощью RL и SFT', 'desc': 'Исследование посвящено внедрению сложных способностей рассуждения в крупные визуально-языковые модели (LVLM) с использованием обучения с подкреплением (RL) и контролируемой тонкой настройки (SFT). Авторы разработали итеративный подход, сочетающий SFT на легких обучающих данных и RL для улучшения обобщающей способности модели. В результате была создана модель OpenVLThinker, демонстрирующая улучшенную производительность на сложных мультимодальных задачах рассуждения. Исследование показывает потенциал предложенной стратегии для развития надежных визуально-языковых рассуждений в LVLM.'}, 'en': {'title': 'Empowering Vision-Language Models with Enhanced Reasoning Skills', 'desc': "This paper presents OpenVLThinker, a large vision-language model (LVLM) that enhances reasoning abilities through a combination of supervised fine-tuning (SFT) and reinforcement learning (RL). The authors demonstrate that by distilling reasoning capabilities from text-based models and iteratively refining the training data, the LVLM can achieve improved performance on complex multimodal reasoning tasks. The approach involves generating reasoning steps from high-quality image captions and using RL to iteratively enhance the model's reasoning skills. The results show significant advancements in benchmarks like MathVista, MathVerse, and MathVision, highlighting the effectiveness of their methodology for robust vision-language reasoning."}, 'zh': {'title': '通过强化学习提升视觉语言模型的推理能力', 'desc': 'DeepSeek-R1的最新进展表明，通过可验证奖励的强化学习（RL），大型语言模型（LLMs）可以实现复杂的推理能力，如自我验证和自我纠正。这项研究探讨了是否可以将类似的推理能力成功整合到大型视觉语言模型（LVLMs）中，并评估其在多模态推理任务中的影响。我们采用了一种迭代的方法，通过轻量级训练数据进行监督微调（SFT），并结合强化学习（RL）进一步提高模型的泛化能力。最终，OpenVLThinker模型在MathVista、MathVerse和MathVision等挑战性基准上展现了持续改进的推理性能，证明了我们策略在视觉语言推理中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.17126', 'title': 'Modifying Large Language Model Post-Training for Diverse Creative\n  Writing', 'url': 'https://huggingface.co/papers/2503.17126', 'abstract': 'As creative writing tasks do not have singular correct answers, large language models (LLMs) trained to perform these tasks should be able to generate diverse valid outputs. However, LLM post-training often focuses on improving generation quality but neglects to facilitate output diversity. Hence, in creative writing generation, we investigate post-training approaches to promote both output diversity and quality. Our core idea is to include deviation -- the degree of difference between a training sample and all other samples with the same prompt -- in the training objective to facilitate learning from rare high-quality instances. By adopting our approach to direct preference optimization (DPO) and odds ratio preference optimization (ORPO), we demonstrate that we can promote the output diversity of trained models while minimally decreasing quality. Our best model with 8B parameters could achieve on-par diversity as a human-created dataset while having output quality similar to the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We further validate our approaches with a human evaluation, an ablation, and a comparison to an existing diversification approach, DivPO.', 'score': 18, 'issue_id': 2852, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '66908ad7080180ee', 'authors': ['John Joon Young Chung', 'Vishakh Padmakumar', 'Melissa Roemmele', 'Yuqian Sun', 'Max Kreminski'], 'affiliations': ['Midjourney', 'New York University'], 'pdf_title_img': 'assets/pdf/title_img/2503.17126.jpg', 'data': {'categories': ['#rlhf', '#training', '#story_generation'], 'emoji': '🎨', 'ru': {'title': 'Повышение креативности ИИ: баланс между разнообразием и качеством', 'desc': 'Статья исследует методы постобучения больших языковых моделей (LLM) для улучшения разнообразия и качества генерации творческих текстов. Авторы предлагают включить в целевую функцию обучения показатель отклонения, чтобы модель лучше училась на редких высококачественных примерах. Применение этого подхода к методам DPO и ORPO позволило повысить разнообразие выходных данных при минимальном снижении качества. Лучшая модель авторов с 8 миллиардами параметров достигла разнообразия на уровне текстов, созданных людьми, при сохранении качества на уровне лучших инструктированных моделей.'}, 'en': {'title': 'Boosting Creativity: Balancing Diversity and Quality in LLM Outputs', 'desc': 'This paper addresses the challenge of generating diverse outputs in creative writing tasks using large language models (LLMs). It proposes a novel post-training approach that incorporates deviation into the training objective, which helps the model learn from unique, high-quality examples. By utilizing techniques like direct preference optimization (DPO) and odds ratio preference optimization (ORPO), the authors demonstrate that their method can enhance output diversity without significantly compromising quality. The results show that their optimized model, with 8 billion parameters, achieves diversity comparable to human-generated datasets while maintaining high output quality similar to leading instruction-tuned models.'}, 'zh': {'title': '提升创意写作的多样性与质量', 'desc': '这篇论文探讨了如何在创意写作生成中提高输出的多样性和质量。研究者提出了一种后训练方法，通过引入偏差来优化训练目标，从而学习稀有的高质量实例。采用直接偏好优化（DPO）和赔率比偏好优化（ORPO）的方法，研究表明可以在保持质量的同时促进模型输出的多样性。最终，最佳模型在多样性和质量上与人类创作的数据集相当，验证了所提方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.17032', 'title': 'TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented\n  Reality via 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2503.17032', 'abstract': 'Realistic 3D full-body talking avatars hold great potential in AR, with applications ranging from e-commerce live streaming to holographic communication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike avatar creation, existing methods struggle with fine-grained control of facial expressions and body movements in full-body talking tasks. Additionally, they often lack sufficient details and cannot run in real-time on mobile devices. We present TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking avatar driven by various signals. Our approach starts by creating a personalized clothed human parametric template that binds Gaussians to represent appearances. We then pre-train a StyleUnet-based network to handle complex pose-dependent non-rigid deformation, which can capture high-frequency appearance details but is too resource-intensive for mobile devices. To overcome this, we "bake" the non-rigid deformations into a lightweight MLP-based network using a distillation technique and develop blend shapes to compensate for details. Extensive experiments show that TaoAvatar achieves state-of-the-art rendering quality while running in real-time across various devices, maintaining 90 FPS on high-definition stereo devices such as the Apple Vision Pro.', 'score': 12, 'issue_id': 2861, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '80e12497629a92eb', 'authors': ['Jianchuan Chen', 'Jingchuan Hu', 'Gaige Wang', 'Zhonghua Jiang', 'Tiansong Zhou', 'Zhiwen Chen', 'Chengfei Lv'], 'affiliations': ['Alibaba Group, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.17032.jpg', 'data': {'categories': ['#3d', '#cv', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Реалистичные говорящие 3D-аватары для AR в реальном времени', 'desc': 'Статья представляет TaoAvatar - технологию создания реалистичных 3D-аватаров полного тела для дополненной реальности. Метод использует 3D Gaussian Splatting и параметрический шаблон человека для представления внешнего вида. Для захвата сложных деформаций применяется предобученная сеть StyleUnet, результаты которой затем переносятся в легковесную MLP-сеть. TaoAvatar достигает высокого качества рендеринга в реальном времени на различных устройствах, включая мобильные.'}, 'en': {'title': 'TaoAvatar: Real-Time Realism for 3D Talking Avatars', 'desc': 'This paper introduces TaoAvatar, a new method for creating realistic 3D full-body talking avatars suitable for augmented reality applications. It addresses the limitations of existing 3D Gaussian Splatting techniques, particularly in controlling facial expressions and body movements while ensuring real-time performance on mobile devices. The authors utilize a personalized parametric template and a pre-trained StyleUnet network to capture detailed non-rigid deformations, which are then distilled into a lightweight MLP-based network for efficiency. The results demonstrate that TaoAvatar achieves high-quality rendering at 90 FPS on advanced devices, making it a significant advancement in avatar technology.'}, 'zh': {'title': 'TaoAvatar：实时高保真全身虚拟形象的创新之路', 'desc': '本文介绍了一种名为TaoAvatar的高保真轻量级3D全身说话虚拟形象，旨在提升增强现实中的应用效果。该方法利用3D高斯点云技术，创建个性化的穿衣人类参数模板，以实现更细致的面部表情和身体动作控制。通过预训练StyleUnet网络，TaoAvatar能够处理复杂的姿态依赖非刚性变形，并捕捉高频外观细节。最终，采用蒸馏技术将非刚性变形整合到轻量级的多层感知机网络中，使其在各种设备上实现实时渲染，保持高达90帧每秒的流畅度。'}}}, {'id': 'https://huggingface.co/papers/2503.16549', 'title': 'MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical\n  Problems', 'url': 'https://huggingface.co/papers/2503.16549', 'abstract': 'Despite impressive performance across diverse tasks, Multimodal Large Language Models (MLLMs) have yet to fully demonstrate their potential in visual mathematical problem-solving, particularly in accurately perceiving and interpreting diagrams. Inspired by typical processes of humans, we hypothesize that the perception capabilities to extract meaningful information from diagrams is crucial, as it directly impacts subsequent inference processes. To validate this hypothesis, we developed FlowVerse, a comprehensive benchmark that categorizes all information used during problem-solving into four components, which are then combined into six problem versions for evaluation. Our preliminary results on FlowVerse reveal that existing MLLMs exhibit substantial limitations when extracting essential information and reasoned property from diagrams and performing complex reasoning based on these visual inputs. In response, we introduce MathFlow, a modular problem-solving pipeline that decouples perception and inference into distinct stages, thereby optimizing each independently. Given the perceptual limitations observed in current MLLMs, we trained MathFlow-P-7B as a dedicated perception model. Experimental results indicate that MathFlow-P-7B yields substantial performance gains when integrated with various closed-source and open-source inference models. This demonstrates the effectiveness of the MathFlow pipeline and its compatibility to diverse inference frameworks. The FlowVerse benchmark and code are available at https://github.com/MathFlow-zju/MathFlow.', 'score': 10, 'issue_id': 2853, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': 'b40b0c3ceb851451', 'authors': ['Felix Chen', 'Hangjie Yuan', 'Yunqiu Xu', 'Tao Feng', 'Jun Cen', 'Pengwei Liu', 'Zeying Huang', 'Yi Yang'], 'affiliations': ['Intelligent Learning', 'The Hong Kong University of Science and Technology', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16549.jpg', 'data': {'categories': ['#optimization', '#open_source', '#cv', '#multimodal', '#reasoning', '#inference', '#training', '#math', '#benchmark'], 'emoji': '🧮', 'ru': {'title': 'MathFlow: Улучшение визуального математического мышления ИИ', 'desc': 'Статья представляет FlowVerse - комплексный бенчмарк для оценки способностей мультимодальных больших языковых моделей (MLLM) в решении визуальных математических задач. Авторы выявили ограничения существующих MLLM в извлечении информации из диаграмм и рассуждениях на их основе. В ответ на это они разработали MathFlow - модульный конвейер, разделяющий процессы восприятия и логического вывода. Эксперименты показали, что специализированная модель восприятия MathFlow-P-7B значительно улучшает производительность при интеграции с различными моделями логического вывода.'}, 'en': {'title': 'Enhancing Visual Reasoning in MLLMs with MathFlow', 'desc': "This paper addresses the challenges faced by Multimodal Large Language Models (MLLMs) in visual mathematical problem-solving, particularly in interpreting diagrams. The authors propose that effective perception of diagrams is essential for accurate reasoning and problem-solving. They introduce FlowVerse, a benchmark that categorizes information used in problem-solving into four components, which helps evaluate MLLMs' performance. To improve perception capabilities, they developed MathFlow, a modular pipeline that separates perception from inference, leading to significant performance improvements when integrated with existing models."}, 'zh': {'title': '提升视觉数学问题解决能力的创新方法', 'desc': '尽管多模态大型语言模型（MLLMs）在多种任务中表现出色，但在视觉数学问题解决方面仍未充分发挥其潜力，尤其是在准确感知和解释图表方面。我们提出了FlowVerse基准，旨在验证从图表中提取有意义信息的感知能力对推理过程的重要性。研究结果表明，现有的MLLMs在提取图表中的关键信息和进行复杂推理时存在显著局限。为此，我们引入了MathFlow，一个将感知和推理分为不同阶段的模块化问题解决管道，从而优化每个阶段的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.16983', 'title': 'Enabling Versatile Controls for Video Diffusion Models', 'url': 'https://huggingface.co/papers/2503.16983', 'abstract': 'Despite substantial progress in text-to-video generation, achieving precise and flexible control over fine-grained spatiotemporal attributes remains a significant unresolved challenge in video generation research. To address these limitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework designed to enable fine-grained control over pre-trained video diffusion models in a unified manner. VCtrl integrates diverse user-specified control signals-such as Canny edges, segmentation masks, and human keypoints-into pretrained video diffusion models via a generalizable conditional module capable of uniformly encoding multiple types of auxiliary signals without modifying the underlying generator. Additionally, we design a unified control signal encoding pipeline and a sparse residual connection mechanism to efficiently incorporate control representations. Comprehensive experiments and human evaluations demonstrate that VCtrl effectively enhances controllability and generation quality. The source code and pre-trained models are publicly available and implemented using the PaddlePaddle framework at http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.', 'score': 9, 'issue_id': 2853, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '7d2634a04fc68d45', 'authors': ['Xu Zhang', 'Hao Zhou', 'Haoming Qin', 'Xiaobin Lu', 'Jiaxing Yan', 'Guanzhong Wang', 'Zeyu Chen', 'Yi Liu'], 'affiliations': ['PaddlePaddle Team, Baidu Inc.', 'Sun Yat-sen University', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16983.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#multimodal', '#video', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Точный контроль над генерацией видео с помощью VCtrl', 'desc': 'VCtrl - это новый фреймворк для точного контроля над предобученными видео-диффузионными моделями. Он позволяет интегрировать различные пользовательские сигналы управления, такие как контуры Кэнни, маски сегментации и ключевые точки человека, без изменения базового генератора. VCtrl использует унифицированный конвейер кодирования сигналов управления и механизм разреженных остаточных соединений. Эксперименты показывают, что VCtrl эффективно улучшает контролируемость и качество генерации видео.'}, 'en': {'title': 'Empowering Video Generation with Fine-Grained Control', 'desc': "This paper presents VCtrl, a new framework that improves control over video generation using pre-trained diffusion models. It allows users to specify various control signals, like Canny edges and segmentation masks, to guide the video generation process without altering the original model. The framework includes a unique encoding pipeline and a sparse residual connection to efficiently integrate these control signals. Experiments show that VCtrl significantly enhances both the quality of generated videos and the user's ability to control specific attributes."}, 'zh': {'title': '细粒度控制视频生成的新框架', 'desc': '尽管文本到视频生成技术取得了显著进展，但在视频生成中实现对细粒度时空属性的精确和灵活控制仍然是一个重要挑战。为了解决这些问题，我们提出了VCtrl（也称为PP-VCtrl），这是一个新颖的框架，旨在以统一的方式实现对预训练视频扩散模型的细粒度控制。VCtrl通过一个通用的条件模块，将用户指定的控制信号（如Canny边缘、分割掩码和人体关键点）整合到预训练的视频扩散模型中，而无需修改底层生成器。此外，我们设计了一个统一的控制信号编码管道和稀疏残差连接机制，以高效地整合控制表示。'}}}, {'id': 'https://huggingface.co/papers/2503.16025', 'title': 'Single Image Iterative Subject-driven Generation and Editing', 'url': 'https://huggingface.co/papers/2503.16025', 'abstract': 'Personalizing image generation and editing is particularly challenging when we only have a few images of the subject, or even a single image. A common approach to personalization is concept learning, which can integrate the subject into existing models relatively quickly, but produces images whose quality tends to deteriorate quickly when the number of subject images is small. Quality can be improved by pre-training an encoder, but training restricts generation to the training distribution, and is time consuming. It is still an open hard challenge to personalize image generation and editing from a single image without training. Here, we present SISO, a novel, training-free approach based on optimizing a similarity score with an input subject image. More specifically, SISO iteratively generates images and optimizes the model based on loss of similarity with the given subject image until a satisfactory level of similarity is achieved, allowing plug-and-play optimization to any image generator. We evaluated SISO in two tasks, image editing and image generation, using a diverse data set of personal subjects, and demonstrate significant improvements over existing methods in image quality, subject fidelity, and background preservation.', 'score': 9, 'issue_id': 2860, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '917237ee980eb66a', 'authors': ['Yair Shpitzer', 'Gal Chechik', 'Idan Schwartz'], 'affiliations': ['Bar-Ilan University', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.16025.jpg', 'data': {'categories': ['#training', '#cv', '#optimization', '#synthetic', '#dataset'], 'emoji': '🖼️', 'ru': {'title': 'Персонализация генерации изображений без обучения', 'desc': 'SISO - это новый подход к персонализации генерации и редактирования изображений без обучения модели. Метод основан на оптимизации оценки сходства с входным изображением субъекта. SISO итеративно генерирует изображения и оптимизирует модель на основе потери сходства с заданным изображением субъекта. Этот подход позволяет улучшить качество изображения, точность передачи субъекта и сохранение фона по сравнению с существующими методами.'}, 'en': {'title': 'SISO: Personalization Without Training', 'desc': 'This paper introduces SISO, a new method for personalizing image generation and editing without the need for training on multiple images. SISO works by optimizing a similarity score between generated images and a single input image, allowing for iterative improvements until the desired similarity is reached. This approach overcomes the limitations of traditional methods that require extensive training data and can lead to quality degradation with fewer images. The authors demonstrate that SISO significantly enhances image quality, maintains subject fidelity, and preserves background details compared to existing techniques.'}, 'zh': {'title': '无训练个性化图像生成的新方法', 'desc': '个性化图像生成和编辑在只有少量或单张图像时特别具有挑战性。传统的个性化方法依赖于概念学习，但在图像数量较少时，生成的图像质量往往会迅速下降。我们提出了一种名为SISO的新方法，它不需要训练，通过优化与输入图像的相似度分数来实现个性化。SISO在图像编辑和生成任务中表现出显著的质量提升，能够更好地保留主题特征和背景。'}}}, {'id': 'https://huggingface.co/papers/2503.17287', 'title': 'FastCuRL: Curriculum Reinforcement Learning with Progressive Context\n  Extension for Efficient Training R1-like Reasoning Models', 'url': 'https://huggingface.co/papers/2503.17287', 'abstract': 'In this paper, we propose \\textsc{FastCuRL}, a simple yet efficient Curriculum Reinforcement Learning approach with context window extending strategy to accelerate the reinforcement learning training efficiency for R1-like reasoning models while enhancing their performance in tackling complex reasoning tasks with long chain-of-thought rationales, particularly with a 1.5B parameter language model. \\textsc{FastCuRL} consists of two main procedures: length-aware training data segmentation and context window extension training. Specifically, the former first splits the original training data into three different levels by the input prompt length, and then the latter leverages segmented training datasets with a progressively increasing context window length to train the reasoning model. Experimental results demonstrate that \\textsc{FastCuRL}-1.5B-Preview surpasses DeepScaleR-1.5B-Preview across all five datasets (including MATH 500, AIME 2024, AMC 2023, Minerva Math, and OlympiadBench) while only utilizing 50\\% of training steps. Furthermore, all training stages for FastCuRL-1.5B-Preview are completed using just a single node with 8 GPUs.', 'score': 8, 'issue_id': 2852, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '17806ebf2abff1ed', 'authors': ['Mingyang Song', 'Mao Zheng', 'Zheng Li', 'Wenjie Yang', 'Xuan Luo', 'Yue Pan', 'Feng Zhang'], 'affiliations': ['Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2503.17287.jpg', 'data': {'categories': ['#training', '#long_context', '#reasoning', '#optimization', '#rl'], 'emoji': '🚀', 'ru': {'title': 'Ускоренное обучение языковых моделей для сложных рассуждений', 'desc': 'В этой статье представлен метод FastCuRL для ускорения обучения с подкреплением языковых моделей для решения сложных задач рассуждения. FastCuRL использует сегментацию обучающих данных по длине и постепенное расширение контекстного окна. Эксперименты показывают, что FastCuRL-1.5B-Preview превосходит DeepScaleR-1.5B-Preview на пяти наборах данных, используя всего 50% шагов обучения. Обучение FastCuRL-1.5B-Preview выполняется на одном узле с 8 GPU.'}, 'en': {'title': 'Accelerating Reasoning with FastCuRL: Efficient Training for Complex Tasks', 'desc': 'This paper introduces FastCuRL, a novel approach in Curriculum Reinforcement Learning designed to improve training efficiency for reasoning models with 1.5 billion parameters. It employs a two-step process: first, it segments training data based on the length of input prompts, and then it extends the context window during training to enhance model performance on complex reasoning tasks. The results show that FastCuRL outperforms existing models like DeepScaleR while requiring only half the training steps. Additionally, the entire training process is efficiently executed on a single node equipped with 8 GPUs.'}, 'zh': {'title': '快速提升推理模型训练效率的课程强化学习', 'desc': '本文提出了一种简单而高效的课程强化学习方法，称为FastCuRL，旨在加速R1类推理模型的强化学习训练效率，同时提高其在复杂推理任务中的表现。FastCuRL包括两个主要步骤：长度感知的训练数据分割和上下文窗口扩展训练。具体而言，前者将原始训练数据根据输入提示长度分为三个不同的级别，后者则利用分段的训练数据集，逐步增加上下文窗口长度来训练推理模型。实验结果表明，FastCuRL-1.5B-Preview在所有五个数据集上均优于DeepScaleR-1.5B-Preview，同时仅使用了50%的训练步骤。'}}}, {'id': 'https://huggingface.co/papers/2503.16867', 'title': 'ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question\n  Generation and Answering', 'url': 'https://huggingface.co/papers/2503.16867', 'abstract': "Precisely evaluating semantic alignment between text prompts and generated videos remains a challenge in Text-to-Video (T2V) Generation. Existing text-to-video alignment metrics like CLIPScore only generate coarse-grained scores without fine-grained alignment details, failing to align with human preference. To address this limitation, we propose ETVA, a novel Evaluation method of Text-to-Video Alignment via fine-grained question generation and answering. First, a multi-agent system parses prompts into semantic scene graphs to generate atomic questions. Then we design a knowledge-augmented multi-stage reasoning framework for question answering, where an auxiliary LLM first retrieves relevant common-sense knowledge (e.g., physical laws), and then video LLM answers the generated questions through a multi-stage reasoning mechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's correlation coefficient of 58.47, showing a much higher correlation with human judgment than existing metrics which attain only 31.0. We also construct a comprehensive benchmark specifically designed for text-to-video alignment evaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10 categories. Through a systematic evaluation of 15 existing text-to-video models, we identify their key capabilities and limitations, paving the way for next-generation T2V generation.", 'score': 8, 'issue_id': 2859, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '36366558e2b46107', 'authors': ['Kaisi Guan', 'Zhengfeng Lai', 'Yuchong Sun', 'Peng Zhang', 'Wei Liu', 'Kieran Liu', 'Meng Cao', 'Ruihua Song'], 'affiliations': ['Apple', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.16867.jpg', 'data': {'categories': ['#alignment', '#video', '#benchmark', '#reasoning', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'Точная оценка соответствия текста и видео с помощью вопросно-ответной системы', 'desc': 'Статья представляет ETVA - новый метод оценки соответствия между текстовыми запросами и сгенерированными видео. Метод использует мультиагентную систему для генерации атомарных вопросов на основе семантических графов сцен. Затем применяется многоступенчатая система рассуждений с использованием языковых моделей для ответов на вопросы. ETVA показывает значительно более высокую корреляцию с человеческими оценками по сравнению с существующими метриками.'}, 'en': {'title': 'Enhancing Text-to-Video Alignment with ETVA', 'desc': 'This paper addresses the challenge of accurately evaluating how well text prompts align with generated videos in Text-to-Video (T2V) Generation. The authors introduce ETVA, a new evaluation method that uses fine-grained question generation and answering to improve alignment metrics. By creating semantic scene graphs and employing a multi-agent system, ETVA generates specific questions that are answered using a knowledge-augmented reasoning framework. The results show that ETVA significantly correlates with human judgment, outperforming existing metrics and providing a comprehensive benchmark for future T2V evaluations.'}, 'zh': {'title': '提升文本与视频对齐的评估精度', 'desc': '在文本到视频生成（T2V）中，准确评估文本提示与生成视频之间的语义对齐仍然是一个挑战。现有的对齐指标如CLIPScore只能生成粗略的评分，缺乏细致的对齐信息，无法满足人类的偏好。为了解决这个问题，我们提出了一种新颖的文本到视频对齐评估方法ETVA，通过生成和回答细粒度问题来实现。我们的实验表明，ETVA与人类判断的相关性显著高于现有指标，且我们还构建了一个专门用于文本到视频对齐评估的基准。'}}}, {'id': 'https://huggingface.co/papers/2503.12821', 'title': 'From Head to Tail: Towards Balanced Representation in Large\n  Vision-Language Models through Adaptive Data Calibration', 'url': 'https://huggingface.co/papers/2503.12821', 'abstract': 'Large Vision-Language Models (LVLMs) have achieved significant progress in combining visual comprehension with language generation. Despite this success, the training data of LVLMs still suffers from Long-Tail (LT) problems, where the data distribution is highly imbalanced. Previous works have mainly focused on traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as recognition and classification. Nevertheless, the exploration of LVLM (e.g. LLaVA) and more general tasks (e.g. Visual Question Answering and Visual Reasoning) remains under-explored. In this paper, we first conduct an in-depth analysis of the LT issues in LVLMs and identify two core causes: the overrepresentation of head concepts and the underrepresentation of tail concepts. Based on the above observation, we propose an Adaptive Data Refinement Framework (ADR), which consists of two stages: Data Rebalancing (DR) and Data Synthesis (DS). In the DR stage, we adaptively rebalance the redundant data based on entity distributions, while in the DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and scarce images to supplement underrepresented portions. Through comprehensive evaluations across eleven benchmarks, our proposed ADR effectively mitigates the long-tail problem in the training data, improving the average performance of LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.', 'score': 7, 'issue_id': 2855, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '16457f914f8e71df', 'authors': ['Mingyang Song', 'Xiaoye Qu', 'Jiawei Zhou', 'Yu Cheng'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Stony Brook University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.12821.jpg', 'data': {'categories': ['#training', '#long_context', '#synthetic', '#optimization', '#dataset', '#data'], 'emoji': '🦒', 'ru': {'title': 'Балансировка данных для улучшения мультимодальных моделей', 'desc': 'Эта статья посвящена проблеме несбалансированности данных (проблема длинного хвоста) в обучении крупных моделей машинного зрения и языка (LVLM). Авторы предлагают адаптивную систему улучшения данных (ADR), состоящую из двух этапов: ребалансировка данных и синтез данных. Система ADR эффективно решает проблему длинного хвоста в обучающих данных, не увеличивая их объем. Эксперименты показали относительное улучшение средней производительности модели LLaVA 1.5 на 4.36% на одиннадцати тестовых наборах.'}, 'en': {'title': 'Balancing the Data for Better Vision-Language Models', 'desc': "This paper addresses the challenges faced by Large Vision-Language Models (LVLMs) due to imbalanced training data, known as the Long-Tail (LT) problem. The authors identify that certain concepts are overrepresented while others are underrepresented, which affects the model's performance on diverse tasks. To tackle this issue, they introduce an Adaptive Data Refinement Framework (ADR) that includes two key stages: Data Rebalancing to adjust the data distribution and Data Synthesis to generate additional data for underrepresented concepts. Their approach shows significant improvements in model performance across various benchmarks without the need for more training data."}, 'zh': {'title': '自适应数据精炼，解决长尾问题！', 'desc': '大型视觉语言模型（LVLMs）在视觉理解与语言生成的结合上取得了显著进展。然而，LVLMs的训练数据仍然面临长尾问题，数据分布极不平衡。本文深入分析了LVLM中的长尾问题，识别出头部概念过度代表和尾部概念不足代表两个核心原因。我们提出了一种自适应数据精炼框架（ADR），通过数据重平衡和数据合成两个阶段，有效缓解了训练数据中的长尾问题，提升了LLaVA 1.5的平均性能。'}}}, {'id': 'https://huggingface.co/papers/2503.17069', 'title': 'PVChat: Personalized Video Chat with One-Shot Learning', 'url': 'https://huggingface.co/papers/2503.17069', 'abstract': 'Video large language models (ViLLMs) excel in general video understanding, e.g., recognizing activities like talking and eating, but struggle with identity-aware comprehension, such as "Wilson is receiving chemotherapy" or "Tom is discussing with Sarah", limiting their applicability in smart healthcare and smart home environments. To address this limitation, we propose a one-shot learning framework PVChat, the first personalized ViLLM that enables subject-aware question answering (QA) from a single video for each subject. Our approach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically augmented video-QA dataset, leveraging a progressive image-to-video learning strategy. Specifically, we introduce an automated augmentation pipeline that synthesizes identity-preserving positive samples and retrieves hard negatives from existing video corpora, generating a diverse training dataset with four QA types: existence, appearance, action, and location inquiries. To enhance subject-specific learning, we propose a ReLU Routing MoH attention mechanism, alongside two novel objectives: (1) Smooth Proximity Regularization for progressive learning through exponential distance scaling and (2) Head Activation Enhancement for balanced attention routing. Finally, we adopt a two-stage training strategy, transitioning from image pre-training to video fine-tuning, enabling a gradual learning process from static attributes to dynamic representations. We evaluate PVChat on diverse datasets covering medical scenarios, TV series, anime, and real-world footage, demonstrating its superiority in personalized feature understanding after learning from a single video, compared to state-of-the-art ViLLMs.', 'score': 6, 'issue_id': 2859, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': 'd7cfcebc43949d14', 'authors': ['Yufei Shi', 'Weilong Yan', 'Gang Xu', 'Yumeng Li', 'Yuchen Li', 'Zhenxi Li', 'Fei Richard Yu', 'Ming Li', 'Si Yong Yeo'], 'affiliations': ['Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)', 'Nankai University', 'Nanyang Technological University', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.17069.jpg', 'data': {'categories': ['#video', '#dataset', '#transfer_learning', '#healthcare', '#training', '#synthetic'], 'emoji': '🎥', 'ru': {'title': 'PVChat: Персонализированное понимание видео с одного просмотра', 'desc': 'Статья представляет PVChat - новую модель для персонализированного понимания видео с использованием больших языковых моделей. PVChat решает проблему идентификации личностей в видео, что важно для умных домов и здравоохранения. Модель использует одноразовое обучение и синтетически дополненные данные для улучшения распознавания субъектов. Предложенный подход включает механизм внимания Mixture-of-Heads и двухэтапную стратегию обучения от изображений к видео.'}, 'en': {'title': 'Personalized Video Understanding with One-Shot Learning', 'desc': "This paper introduces PVChat, a novel one-shot learning framework designed to enhance personalized video understanding in large language models. Unlike traditional models that struggle with identity-aware comprehension, PVChat allows for subject-specific question answering from just one video per subject. The framework employs a Mixture-of-Heads attention mechanism and a unique training strategy that combines image pre-training with video fine-tuning, optimizing the model's ability to recognize and respond to various types of inquiries. Evaluation results show that PVChat outperforms existing models in understanding personalized features across multiple datasets, including healthcare and entertainment scenarios."}, 'zh': {'title': '个性化视频理解的新突破', 'desc': '视频大型语言模型（ViLLMs）在一般视频理解方面表现出色，但在身份感知理解上存在困难。为了解决这个问题，我们提出了一种一-shot学习框架PVChat，这是首个个性化的ViLLM，能够从每个主体的单个视频中进行主体感知问答。我们的方法通过增强的混合头（MoH）优化ViLLM，并利用合成增强的视频问答数据集，采用渐进的图像到视频学习策略。最终，我们在多个数据集上评估PVChat，显示其在个性化特征理解方面的优越性。'}}}, {'id': 'https://huggingface.co/papers/2503.16921', 'title': 'When Preferences Diverge: Aligning Diffusion Models with Minority-Aware\n  Adaptive DPO', 'url': 'https://huggingface.co/papers/2503.16921', 'abstract': "In recent years, the field of image generation has witnessed significant advancements, particularly in fine-tuning methods that align models with universal human preferences. This paper explores the critical role of preference data in the training process of diffusion models, particularly in the context of Diffusion-DPO and its subsequent adaptations. We investigate the complexities surrounding universal human preferences in image generation, highlighting the subjective nature of these preferences and the challenges posed by minority samples in preference datasets. Through pilot experiments, we demonstrate the existence of minority samples and their detrimental effects on model performance. We propose Adaptive-DPO -- a novel approach that incorporates a minority-instance-aware metric into the DPO objective. This metric, which includes intra-annotator confidence and inter-annotator stability, distinguishes between majority and minority samples. We introduce an Adaptive-DPO loss function which improves the DPO loss in two ways: enhancing the model's learning of majority labels while mitigating the negative impact of minority samples. Our experiments demonstrate that this method effectively handles both synthetic minority data and real-world preference data, paving the way for more effective training methodologies in image generation tasks.", 'score': 5, 'issue_id': 2852, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '6bd4875e712b7a36', 'authors': ['Lingfan Zhang', 'Chen Liu', 'Chengming Xu', 'Kai Hu', 'Donghao Luo', 'Chengjie Wang', 'Yanwei Fu', 'Yuan Yao'], 'affiliations': ['Carnegie Mellon University', 'Fudan University', 'Tencent', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.16921.jpg', 'data': {'categories': ['#rlhf', '#training', '#diffusion', '#alignment'], 'emoji': '🖼️', 'ru': {'title': 'Адаптивное обучение генеративных моделей с учетом предпочтений большинства', 'desc': 'Статья исследует роль данных о предпочтениях в обучении диффузионных моделей генерации изображений. Авторы выявляют проблему субъективности универсальных человеческих предпочтений и негативное влияние выборок меньшинства на производительность модели. Предлагается новый подход Adaptive-DPO, который учитывает метрику для различения выборок большинства и меньшинства. Эксперименты показывают, что метод эффективно обрабатывает как синтетические, так и реальные данные о предпочтениях.'}, 'en': {'title': 'Enhancing Image Generation with Adaptive-DPO: Balancing Preferences for Better Models', 'desc': 'This paper discusses advancements in image generation, focusing on how preference data can improve diffusion models. It highlights the challenges of incorporating universal human preferences, especially the impact of minority samples in preference datasets. The authors introduce Adaptive-DPO, a new method that uses a metric to better handle these minority samples during training. Their experiments show that Adaptive-DPO enhances model performance by balancing the influence of majority and minority preferences.'}, 'zh': {'title': '适应性DPO：提升图像生成模型的偏好学习', 'desc': '近年来，图像生成领域取得了显著进展，尤其是在微调方法方面，这些方法使模型与普遍的人类偏好对齐。本文探讨了偏好数据在扩散模型训练过程中的关键作用，特别是在Diffusion-DPO及其后续适应中。我们研究了图像生成中普遍人类偏好的复杂性，强调了这些偏好的主观性以及偏好数据集中少数样本带来的挑战。我们提出了一种新方法Adaptive-DPO，通过引入关注少数实例的度量，改善了模型对多数标签的学习，同时减轻了少数样本的负面影响。'}}}, {'id': 'https://huggingface.co/papers/2503.16423', 'title': 'GAEA: A Geolocation Aware Conversational Model', 'url': 'https://huggingface.co/papers/2503.16423', 'abstract': 'Image geolocalization, in which, traditionally, an AI model predicts the precise GPS coordinates of an image is a challenging task with many downstream applications. However, the user cannot utilize the model to further their knowledge other than the GPS coordinate; the model lacks an understanding of the location and the conversational ability to communicate with the user. In recent days, with tremendous progress of large multimodal models (LMMs) proprietary and open-source researchers have attempted to geolocalize images via LMMs. However, the issues remain unaddressed; beyond general tasks, for more specialized downstream tasks, one of which is geolocalization, LMMs struggle. In this work, we propose to solve this problem by introducing a conversational model GAEA that can provide information regarding the location of an image, as required by a user. No large-scale dataset enabling the training of such a model exists. Thus we propose a comprehensive dataset GAEA with 800K images and around 1.6M question answer pairs constructed by leveraging OpenStreetMap (OSM) attributes and geographical context clues. For quantitative evaluation, we propose a diverse benchmark comprising 4K image-text pairs to evaluate conversational capabilities equipped with diverse question types. We consider 11 state-of-the-art open-source and proprietary LMMs and demonstrate that GAEA significantly outperforms the best open-source model, LLaVA-OneVision by 25.69% and the best proprietary model, GPT-4o by 8.28%. Our dataset, model and codes are available', 'score': 5, 'issue_id': 2864, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'ca9d5aa3c56f03d9', 'authors': ['Ron Campos', 'Ashmal Vayani', 'Parth Parag Kulkarni', 'Rohit Gupta', 'Aritra Dutta', 'Mubarak Shah'], 'affiliations': ['University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2503.16423.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#dataset', '#science', '#multimodal'], 'emoji': '🗺️', 'ru': {'title': 'GAEA: диалоговая геолокализация изображений на новом уровне', 'desc': 'Статья представляет модель GAEA для геолокализации изображений с помощью диалогового интерфейса. Авторы создали обширный датасет из 800 тысяч изображений и 1,6 миллиона пар вопросов-ответов, используя данные OpenStreetMap и географические подсказки. Для оценки разработан бенчмарк из 4000 пар изображение-текст с различными типами вопросов. GAEA превосходит лучшие открытые и проприетарные мультимодальные модели на 25,69% и 8,28% соответственно.'}, 'en': {'title': 'GAEA: Conversational Geolocalization for Enhanced User Interaction', 'desc': 'This paper addresses the challenge of image geolocalization, where AI models predict GPS coordinates but lack conversational understanding of the location. The authors introduce GAEA, a conversational model that not only geolocalizes images but also provides contextual information to users. To train this model, they created a new dataset called GAEA, consisting of 800,000 images and 1.6 million question-answer pairs derived from OpenStreetMap data. The results show that GAEA outperforms existing large multimodal models in conversational capabilities related to geolocalization tasks.'}, 'zh': {'title': 'GAEA：图像地理定位的新对话模型', 'desc': '本文提出了一种新的对话模型GAEA，用于图像地理定位。传统的AI模型只能预测图像的GPS坐标，缺乏与用户的互动能力。GAEA通过提供与图像位置相关的信息，解决了这一问题。我们还构建了一个包含80万张图像和160万个问答对的综合数据集，以支持模型的训练和评估。'}}}, {'id': 'https://huggingface.co/papers/2503.16282', 'title': 'Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language\n  Model', 'url': 'https://huggingface.co/papers/2503.16282', 'abstract': 'Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to new classes with few support samples while retaining base class segmentation. Existing GFS-PCS methods enhance prototypes via interacting with support or query features but remain limited by sparse knowledge from few-shot samples. Meanwhile, 3D vision-language models (3D VLMs), generalizing across open-world novel classes, contain rich but noisy novel class knowledge. In this work, we introduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels from 3D VLMs with precise yet sparse few-shot samples to maximize the strengths of both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label selection to filter low-quality regions, followed by an adaptive infilling strategy that combines knowledge from pseudo-label contexts and few-shot samples to adaptively label the filtered, unlabeled areas. Additionally, we design a novel-base mix strategy to embed few-shot samples into training scenes, preserving essential context for improved novel class learning. Moreover, recognizing the limited diversity in current GFS-PCS benchmarks, we introduce two challenging benchmarks with diverse novel classes for comprehensive generalization evaluation. Experiments validate the effectiveness of our framework across models and datasets. Our approach and benchmarks provide a solid foundation for advancing GFS-PCS in the real world. The code is at https://github.com/ZhaochongAn/GFS-VL', 'score': 5, 'issue_id': 2862, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '32970c716a041be9', 'authors': ['Zhaochong An', 'Guolei Sun', 'Yun Liu', 'Runjia Li', 'Junlin Han', 'Ender Konukoglu', 'Serge Belongie'], 'affiliations': ['College of Computer Science, Nankai University', 'Computer Vision Laboratory, ETH Zurich', 'Department of Computer Science, University of Copenhagen', 'Department of Engineering Science, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.16282.jpg', 'data': {'categories': ['#games', '#transfer_learning', '#3d', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Синергия 3D VLM и few-shot обучения для улучшенной сегментации облаков точек', 'desc': 'Статья представляет новый подход к обобщенной сегментации облаков точек 3D с малым количеством примеров (GFS-PCS). Авторы предлагают framework GFS-VL, который объединяет плотные, но шумные псевдо-метки из 3D vision-language моделей с точными, но редкими образцами few-shot. Метод включает отбор псевдо-меток на основе прототипов и адаптивное заполнение неразмеченных областей. Также представлены новые наборы данных для оценки обобщения на разнообразные новые классы.'}, 'en': {'title': 'Maximizing Learning with Few Samples in 3D Segmentation', 'desc': "This paper presents a new framework called GFS-VL for generalized few-shot 3D point cloud segmentation, which allows models to learn new classes with only a few examples while still recognizing previously learned classes. The approach combines the strengths of 3D vision-language models, which provide rich but noisy information about new classes, with precise but limited few-shot samples. It introduces a method for selecting high-quality pseudo-labels and an adaptive strategy to fill in gaps in the data, enhancing the labeling of unlabeled areas. Additionally, the authors propose new benchmarks to evaluate the model's performance on diverse novel classes, demonstrating the framework's effectiveness across various datasets and models."}, 'zh': {'title': '融合伪标签与少样本的3D点云分割新方法', 'desc': '本文提出了一种新的框架GFS-VL，用于解决少样本3D点云分割问题。该框架结合了来自3D视觉语言模型的丰富伪标签和稀疏的少样本数据，以提高模型在新类别上的适应能力。通过原型引导的伪标签选择和自适应填充策略，GFS-VL能够有效过滤低质量区域并标记未标记区域。我们还引入了新的基准测试，以评估模型在多样化新类别上的泛化能力，实验结果验证了该框架的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.11572', 'title': 'Implicit Bias-Like Patterns in Reasoning Models', 'url': 'https://huggingface.co/papers/2503.11572', 'abstract': "Implicit bias refers to automatic or spontaneous mental processes that shape perceptions, judgments, and behaviors. Previous research examining `implicit bias' in large language models (LLMs) has often approached the phenomenon differently than how it is studied in humans by focusing primarily on model outputs rather than on model processing. To examine model processing, we present a method called the Reasoning Model Implicit Association Test (RM-IAT) for studying implicit bias-like patterns in reasoning models: LLMs that employ step-by-step reasoning to solve complex tasks. Using this method, we find that reasoning models require more tokens when processing association-incompatible information compared to association-compatible information. These findings suggest AI systems harbor patterns in processing information that are analogous to human implicit bias. We consider the implications of these implicit bias-like patterns for their deployment in real-world applications.", 'score': 5, 'issue_id': 2854, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'f45b684d5c68e00c', 'authors': ['Messi H. J. Lee', 'Calvin K. Lai'], 'affiliations': ['Department of Psychology Rutgers University', 'Division of Computational and Data Sciences Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2503.11572.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#ethics', '#multimodal', '#training'], 'emoji': '🧠', 'ru': {'title': 'Скрытые предубеждения ИИ: новый взгляд на обработку информации', 'desc': 'Статья представляет метод RM-IAT для изучения имплицитных предубеждений в языковых моделях с рассуждениями (reasoning models). Исследователи обнаружили, что таким моделям требуется больше токенов для обработки информации, несовместимой с ассоциациями, по сравнению с совместимой. Это указывает на наличие в ИИ-системах паттернов обработки информации, аналогичных человеческим имплицитным предубеждениям. Авторы рассматривают последствия этих предубеждений для реальных приложений.'}, 'en': {'title': 'Unveiling Implicit Bias in AI Reasoning Models', 'desc': 'This paper introduces a new method called the Reasoning Model Implicit Association Test (RM-IAT) to study implicit bias in large language models (LLMs). Unlike previous research that focused on outputs, this approach examines how LLMs process information, particularly in reasoning tasks. The study finds that LLMs take longer to process information that conflicts with established associations, similar to human implicit biases. These insights raise important considerations for the use of AI in real-world applications, highlighting the need to understand underlying processing patterns.'}, 'zh': {'title': '揭示AI中的隐性偏见模式', 'desc': '隐性偏见是指自动或自发的心理过程，这些过程影响我们的感知、判断和行为。以往对大型语言模型（LLMs）中隐性偏见的研究，主要关注模型输出，而非模型处理过程。我们提出了一种名为推理模型隐性联想测试（RM-IAT）的方法，用于研究推理模型中的隐性偏见模式。研究发现，推理模型在处理与联想不兼容的信息时，需要更多的标记，这表明AI系统在信息处理上存在类似于人类隐性偏见的模式。'}}}, {'id': 'https://huggingface.co/papers/2503.14607', 'title': 'Can Large Vision Language Models Read Maps Like a Human?', 'url': 'https://huggingface.co/papers/2503.14607', 'abstract': 'In this paper, we introduce MapBench-the first dataset specifically designed for human-readable, pixel-based map-based outdoor navigation, curated from complex path finding scenarios. MapBench comprises over 1600 pixel space map path finding problems from 100 diverse maps. In MapBench, LVLMs generate language-based navigation instructions given a map image and a query with beginning and end landmarks. For each map, MapBench provides Map Space Scene Graph (MSSG) as an indexing data structure to convert between natural language and evaluate LVLM-generated results. We demonstrate that MapBench significantly challenges state-of-the-art LVLMs both zero-shot prompting and a Chain-of-Thought (CoT) augmented reasoning framework that decomposes map navigation into sequential cognitive processes. Our evaluation of both open-source and closed-source LVLMs underscores the substantial difficulty posed by MapBench, revealing critical limitations in their spatial reasoning and structured decision-making capabilities. We release all the code and dataset in https://github.com/taco-group/MapBench.', 'score': 3, 'issue_id': 2868, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'a67ca23976ed87ed', 'authors': ['Shuo Xing', 'Zezhou Sun', 'Shuangyu Xie', 'Kaiyuan Chen', 'Yanjia Huang', 'Yuping Wang', 'Jiachen Li', 'Dezhen Song', 'Zhengzhong Tu'], 'affiliations': ['MBZUAI', 'Texas A&M University', 'UC Berkeley', 'UC Riverside', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2503.14607.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#open_source', '#reasoning'], 'emoji': '🗺️', 'ru': {'title': 'MapBench: Новый вызов для языковых моделей в навигации по картам', 'desc': 'MapBench - это первый набор данных, специально разработанный для навигации на основе читаемых человеком пиксельных карт. Он содержит более 1600 задач по поиску пути на 100 разнообразных картах. В MapBench языковые модели с большим объемом данных (LVLMs) генерируют инструкции по навигации на основе изображения карты и запроса с начальными и конечными ориентирами. Оценка показала, что MapBench представляет значительную сложность для современных LVLMs, выявляя ограничения в их пространственном мышлении и структурированном принятии решений.'}, 'en': {'title': 'MapBench: Navigating the Future of Language and Vision Models', 'desc': 'This paper presents MapBench, a novel dataset tailored for evaluating language-based navigation in outdoor environments using pixel-based maps. It includes over 1600 pathfinding scenarios across 100 unique maps, allowing for comprehensive testing of language-vision models (LVLMs). The dataset features a Map Space Scene Graph (MSSG) to facilitate the conversion between natural language instructions and map images, enhancing the evaluation of LVLM outputs. The findings reveal significant challenges for current LVLMs in spatial reasoning and decision-making, highlighting the need for improved models in complex navigation tasks.'}, 'zh': {'title': 'MapBench：挑战语言视觉模型的地图导航数据集', 'desc': '本文介绍了MapBench，这是第一个专门为人类可读的基于像素的地图导航设计的数据集，来源于复杂的路径寻找场景。MapBench包含来自100个不同地图的1600多个像素空间地图路径寻找问题。在MapBench中，LVLM（语言视觉大模型）根据地图图像和起始与结束地标的查询生成基于语言的导航指令。我们的评估显示，MapBench对现有的LVLM在零-shot 提示和链式思维推理框架下提出了显著挑战，揭示了它们在空间推理和结构化决策能力方面的关键局限性。'}}}, {'id': 'https://huggingface.co/papers/2503.17095', 'title': 'FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields', 'url': 'https://huggingface.co/papers/2503.17095', 'abstract': 'Recent 3D face editing methods using masks have produced high-quality edited images by leveraging Neural Radiance Fields (NeRF). Despite their impressive performance, existing methods often provide limited user control due to the use of pre-trained segmentation masks. To utilize masks with a desired layout, an extensive training dataset is required, which is challenging to gather. We present FFaceNeRF, a NeRF-based face editing technique that can overcome the challenge of limited user control due to the use of fixed mask layouts. Our method employs a geometry adapter with feature injection, allowing for effective manipulation of geometry attributes. Additionally, we adopt latent mixing for tri-plane augmentation, which enables training with a few samples. This facilitates rapid model adaptation to desired mask layouts, crucial for applications in fields like personalized medical imaging or creative face editing. Our comparative evaluations demonstrate that FFaceNeRF surpasses existing mask based face editing methods in terms of flexibility, control, and generated image quality, paving the way for future advancements in customized and high-fidelity 3D face editing. The code is available on the {https://kwanyun.github.io/FFaceNeRF_page/{project-page}}.', 'score': 2, 'issue_id': 2862, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': 'ebada6e24359eec7', 'authors': ['Kwan Yun', 'Chaelin Kim', 'Hangyeul Shin', 'Junyong Noh'], 'affiliations': ['Handong Global University', 'KAIST, Visual Media Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.17095.jpg', 'data': {'categories': ['#open_source', '#optimization', '#3d'], 'emoji': '🎭', 'ru': {'title': 'Гибкое редактирование 3D-лиц с помощью адаптивных масок и NeRF', 'desc': 'FFaceNeRF - это метод редактирования 3D-лиц на основе нейронных полей излучения (NeRF), который преодолевает ограничения существующих подходов с фиксированными масками сегментации. Он использует геометрический адаптер с внедрением признаков для эффективного манипулирования геометрическими атрибутами. Метод также применяет смешивание латентных представлений для аугментации трехплоскостных данных, что позволяет обучаться на небольшом количестве образцов. FFaceNeRF превосходит существующие методы редактирования лиц на основе масок по гибкости, контролю и качеству генерируемых изображений.'}, 'en': {'title': 'Empowering 3D Face Editing with FFaceNeRF', 'desc': 'FFaceNeRF is a novel face editing technique that enhances user control in 3D face editing by utilizing Neural Radiance Fields (NeRF). Unlike traditional methods that rely on fixed segmentation masks, FFaceNeRF introduces a geometry adapter with feature injection, allowing users to manipulate geometry attributes more freely. The method also incorporates latent mixing for tri-plane augmentation, enabling effective training with limited data samples. Our evaluations show that FFaceNeRF outperforms existing methods in flexibility, control, and image quality, making it a significant advancement in personalized and high-fidelity 3D face editing.'}, 'zh': {'title': 'FFaceNeRF：灵活高效的人脸编辑新方法', 'desc': '最近的3D人脸编辑方法利用神经辐射场（NeRF）通过掩膜生成高质量的编辑图像。然而，现有方法由于使用预训练的分割掩膜，往往限制了用户的控制能力。我们提出了FFaceNeRF，这是一种基于NeRF的人脸编辑技术，能够克服固定掩膜布局带来的控制限制。该方法采用几何适配器和特征注入，允许有效操控几何属性，并通过潜在混合进行三平面增强，使得在少量样本下也能快速适应所需的掩膜布局。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (3)', '#agi', '#alignment (2)', '#architecture (2)', '#audio', '#benchmark (7)', '#cv (5)', '#data (1)', '#dataset (8)', '#diffusion (3)', '#ethics (1)', '#games (2)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (3)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (9)', '#open_source (5)', '#optimization (9)', '#plp', '#rag', '#reasoning (7)', '#rl (2)', '#rlhf (2)', '#robotics', '#science (2)', '#security', '#small_models', '#story_generation (1)', '#survey', '#synthetic (3)', '#training (11)', '#transfer_learning (2)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-24 22:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-24 22:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-24 22:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    