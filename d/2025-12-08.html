
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 15 papers. December 8.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">8 –¥–µ–∫–∞–±—Ä—è</span> | <span id="title-articles-count">15 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-12-05.html">‚¨ÖÔ∏è <span id="prev-date">05.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-12-09.html">‚û°Ô∏è <span id="next-date">09.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-12.html">üìà <span id='top-month-label'>–ú–µ—Å—è—Ü</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '8 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 8', 'zh': '12Êúà8Êó•'};
        let feedDateNext = {'ru': '09.12', 'en': '12/09', 'zh': '12Êúà9Êó•'};
        let feedDatePrev = {'ru': '05.12', 'en': '12/05', 'zh': '12Êúà5Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2512.05150', 'title': 'TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows', 'url': 'https://huggingface.co/papers/2512.05150', 'abstract': 'TwinFlow is a 1-step generative model framework that enhances inference efficiency without requiring fixed pretrained teacher models or standard adversarial networks, achieving high performance on text-to-image tasks and scaling efficiently.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by 100times with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.', 'score': 62, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 3', 'zh': '12Êúà3Êó•'}, 'hash': '477aa98dcf909813', 'authors': ['Zhenglin Cheng', 'Peng Sun', 'Jianguo Li', 'Tao Lin'], 'affiliations': ['Inclusion AI', 'Shanghai Innovation Institute', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05150.jpg', 'data': {'categories': ['#open_source', '#training', '#optimization', '#multimodal', '#inference', '#benchmark', '#diffusion'], 'emoji': '‚ö°', 'ru': {'title': '–û–¥–Ω–æ–ø—Ä–æ—Ö–æ–¥–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è: –±—ã—Å—Ç—Ä–æ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ', 'desc': 'TwinFlow ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —É—á–∏—Ç–µ–ª—å—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ –≤—Ä–∞–∂–¥–µ–±–Ω—ã—Ö —Å–µ—Ç–µ–π. –ú–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –Ω–æ–≤–æ–º –ø–æ–¥—Ö–æ–¥–µ –∫ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–±–µ–≥–∞–µ—Ç –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã—Å–æ–∫–∏—Ö –∑–∞—Ç—Ä–∞—Ç –ø–∞–º—è—Ç–∏, –ø—Ä–∏—Å—É—â–∏—Ö GAN-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –º–µ—Ç–æ–¥–∞–º. –ù–∞ –∑–∞–¥–∞—á–∞—Ö text-to-image –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ (GenEval 0.83) –ø—Ä–∏ –≤—Å–µ–≥–æ –æ–¥–Ω–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ —É—Å–ø–µ—à–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö (Qwen-Image-20B) –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–Ω–∏–∑–∏—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –≤ 100 —Ä–∞–∑ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π –∫–∞—á–µ—Å—Ç–≤–∞.'}, 'en': {'title': 'TwinFlow: Revolutionizing Generative Models with 1-Step Efficiency', 'desc': 'TwinFlow is a novel 1-step generative model framework designed to improve inference efficiency in multi-modal generation tasks, particularly in text-to-image applications. Unlike traditional models that rely on multi-step processes and fixed pretrained teacher models, TwinFlow operates effectively without these constraints, achieving high performance with just one function evaluation (1-NFE). This approach not only enhances speed but also significantly reduces computational costs, achieving results comparable to models that require 100 function evaluations. By demonstrating scalability and efficiency, TwinFlow sets a new standard for generative modeling in AI.'}, 'zh': {'title': 'TwinFlowÔºöÈ´òÊïàÁöÑ‰∏ÄÊ≠•ÁîüÊàêÊ®°Âûã', 'desc': 'TwinFlowÊòØ‰∏ÄÁßç‰∏ÄÊ≠•ÁîüÊàêÊ®°ÂûãÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÊé®ÁêÜÊïàÁéáÔºåÊó†ÈúÄÂõ∫ÂÆöÁöÑÈ¢ÑËÆ≠ÁªÉÊïôÂ∏àÊ®°ÂûãÊàñÊ†áÂáÜÂØπÊäóÁΩëÁªú„ÄÇËØ•ÊñπÊ≥ïÂú®ÊñáÊú¨Âà∞ÂõæÂÉè‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§üÈ´òÊïàÊâ©Â±ï„ÄÇ‰∏é‰º†ÁªüÁöÑÂ§öÊ≠•È™§Ê°ÜÊû∂Áõ∏ÊØîÔºåTwinFlowÂú®1-NFE‰∏ãÂÆûÁé∞‰∫Ü0.83ÁöÑGenEvalÂæóÂàÜÔºåË∂ÖË∂ä‰∫ÜËÆ∏Â§öÂº∫Âü∫Á∫øÊ®°Âûã„ÄÇÈÄöËøáÂÖ®ÂèÇÊï∞ËÆ≠ÁªÉÔºåTwinFlowÂú®ËÆ°ÁÆóÊàêÊú¨‰∏äÈôç‰Ωé‰∫Ü100ÂÄçÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËæÉÈ´òÁöÑÁîüÊàêË¥®Èáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.02580', 'title': 'From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks', 'url': 'https://huggingface.co/papers/2512.02580', 'abstract': 'CAPO, a curriculum advantage policy optimization, enhances reinforcement learning for large language models by strategically introducing positive and negative advantage signals, improving reasoning capabilities and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.', 'score': 27, 'issue_id': 1, 'pub_date': '2025-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': 'f9a55720f5a85fb2', 'authors': ['Changpeng Yang', 'Jinyang Wu', 'Yuchen Liu', 'Shuai Zhang', 'Yang Li', 'Qiliang Liang', 'Hongzhen Wang', 'Shuai Nie', 'Jiaming Xu', 'Runyu Shi', 'Ying Huang', 'Guoquan Zhang'], 'affiliations': ['Peking University', 'Tsinghua University', 'Xiaomi Corporation'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.02580.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#optimization', '#multimodal'], 'emoji': 'üìö', 'ru': {'title': '–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –≤–≤–µ–¥–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–æ–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è CAPO ‚Äî –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º curriculum, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–≥–Ω–∞–ª—ã –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ (advantage signals), –Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏: —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –æ—Å–Ω–æ–≤–∞–Ω–∏—è, –∞ –∑–∞—Ç–µ–º –≤–≤–æ–¥–∏—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞, –≤–æ–∑–Ω–∏–∫–∞—é—â—É—é –ø—Ä–∏ —Å–º–µ—à–∏–≤–∞–Ω–∏–∏ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ —Å —Å–∞–º–æ–≥–æ –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è. CAPO —Å–æ–≤–º–µ—Å—Ç–∏–º —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (GRPO, PPO, RLOO, Reinforce++) –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö GUI.'}, 'en': {'title': 'Boosting Language Models with Smart Advantage Signals', 'desc': "CAPO, or Curriculum Advantage Policy Optimization, is a method that enhances reinforcement learning for large language models by using both positive and negative advantage signals. It starts by using only positive signals to build a strong foundation through imitation learning, which helps the model learn effectively. After establishing this base, it gradually introduces negative signals to improve the model's ability to differentiate between better and worse outcomes. This approach leads to better reasoning and generalization in complex tasks, making CAPO a flexible and powerful tool for optimizing language models."}, 'zh': {'title': 'CAPOÔºö‰ºòÂåñÂº∫ÂåñÂ≠¶‰π†ÁöÑËØæÁ®ã‰ºòÂäøÁ≠ñÁï•', 'desc': 'CAPOÔºàËØæÁ®ã‰ºòÂäøÁ≠ñÁï•‰ºòÂåñÔºâÊòØ‰∏ÄÁßçÂ¢ûÂº∫Âº∫ÂåñÂ≠¶‰π†ÁöÑÊñπÊ≥ïÔºå‰∏ìÈó®Áî®‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã„ÄÇÂÆÉÈÄöËøáÂºïÂÖ•Ê≠£Ë¥ü‰ºòÂäø‰ø°Âè∑ÔºåÊîπÂñÑÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÁé∞ÊúâÊñπÊ≥ïÂú®Êó©ÊúüÈò∂ÊÆµÊ∑∑Âêà‰ø°Âè∑ÂèØËÉΩÂØºËá¥Ê®°Á≥äÁöÑÊåáÂØºÔºåËÄåCAPOÂàôÈááÁî®ÈÄÇÂ∫îÊÄßËØæÁ®ãÊú∫Âà∂ÔºåÂÖà‰ΩøÁî®Ê≠£‰ø°Âè∑Âª∫Á´ãÂü∫Á°ÄÔºåÂÜçÂºïÂÖ•Ë¥ü‰ø°Âè∑‰ª•ÊèêÈ´òÂå∫ÂàÜËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ï‰∏éÂ§öÁßç‰ºòÂåñÊñπÊ≥ïÂÖºÂÆπÔºåÂπ∂Âú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°ÂíåÂ§öÊ®°ÊÄÅÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢Êé®ÁêÜÂú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊîπËøõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.05905', 'title': 'SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations', 'url': 'https://huggingface.co/papers/2512.05905', 'abstract': 'SCAIL framework improves character animation by using a novel 3D pose representation and a diffusion-transformer architecture with full-context pose injection, achieving studio-grade quality and realism.  \t\t\t\t\tAI-generated summary \t\t\t\t Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present SCAIL (Studio-grade Character Animation via In-context Learning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that SCAIL achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.', 'score': 17, 'issue_id': 1, 'pub_date': '2025-12-05', 'pub_date_card': {'ru': '5 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 5', 'zh': '12Êúà5Êó•'}, 'hash': '378c369fc6e7e469', 'authors': ['Wenhao Yan', 'Sheng Ye', 'Zhuoyi Yang', 'Jiayan Teng', 'ZhenHui Dong', 'Kairui Wen', 'Xiaotao Gu', 'Yong-Jin Liu', 'Jie Tang'], 'affiliations': ['Tsinghua University', 'Z.ai'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05905.jpg', 'data': {'categories': ['#reasoning', '#architecture', '#3d', '#video', '#benchmark', '#diffusion', '#dataset'], 'emoji': 'üé¨', 'ru': {'title': '–°—Ç—É–¥–∏–π–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–ª–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–µ–π –¥–≤–∏–∂–µ–Ω–∏–π', 'desc': 'SCAIL ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —Å—Ç—É–¥–∏–π–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –¥–≤—É—Ö –∫–ª—é—á–µ–≤—ã—Ö –∏–Ω–Ω–æ–≤–∞—Ü–∏—è—Ö: –Ω–æ–≤–æ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ 3D-–ø–æ–∑—ã –∏ –º–µ—Ö–∞–Ω–∏–∑–º–µ –ø–æ–ª–Ω–æ–∫–æ–Ω\xad—Ç–µ–∫—Å—Ç–Ω–æ–π –∏–Ω—ä–µ–∫—Ü–∏–∏ –ø–æ–∑—ã –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞. –°–∏—Å—Ç–µ–º–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–µ—Ä–µ–¥–∞–µ—Ç –¥–≤–∏–∂–µ–Ω–∏–µ –∏–∑ –≤–∏–¥–µ–æ-–∏—Å—Ç–æ—á–Ω–∏–∫–∞ –Ω–∞ —ç—Ç–∞–ª–æ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, —Å–æ—Ö—Ä–∞–Ω—è—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –¥–∞–∂–µ –≤ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å –∫—Ä–æ—Å—Å-–∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–Ω–æ–π –∞–Ω–∏–º–∞—Ü–∏–µ–π. –î–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Å—Ç—É–¥–∏–π–Ω—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫—É—Ä–∏—Ä–æ\xad–≤–∞–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ SCAIL –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∏ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç —Å–∏–Ω—Ç–µ–∑ –¥–≤–∏–∂–µ–Ω–∏–π –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∫ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–º—É —É—Ä–æ–≤–Ω—é –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Revolutionizing Character Animation with SCAIL', 'desc': 'The SCAIL framework enhances character animation by introducing a new 3D pose representation that captures motion more effectively. It utilizes a diffusion-transformer architecture with full-context pose injection, allowing for better understanding of motion over time. This approach addresses issues of structural fidelity and temporal consistency, especially in complex scenarios. The framework also includes a curated data pipeline for high-quality training, leading to state-of-the-art results in character animation.'}, 'zh': {'title': 'ÊèêÂçáËßíËâ≤Âä®ÁîªË¥®ÈáèÁöÑSCAILÊ°ÜÊû∂', 'desc': 'SCAILÊ°ÜÊû∂ÈÄöËøáÈááÁî®Êñ∞È¢ñÁöÑ3DÂßøÊÄÅË°®Á§∫ÂíåÊâ©Êï£ÂèòÊç¢Âô®Êû∂ÊûÑÔºåÊòæËëóÊèêÂçá‰∫ÜËßíËâ≤Âä®ÁîªÁöÑË¥®ÈáèÂíåÁúüÂÆûÊÑü„ÄÇËØ•Ê°ÜÊû∂Ëß£ÂÜ≥‰∫ÜÂú®Â§çÊùÇËøêÂä®ÂíåË∑®Ë∫´‰ªΩÂä®Áîª‰∏≠‰øùÊåÅÁªìÊûÑ‰∏ÄËá¥ÊÄßÂíåÊó∂Èó¥‰∏ÄËá¥ÊÄßÁöÑÈóÆÈ¢ò„ÄÇSCAILÂºïÂÖ•‰∫ÜÂÖ®‰∏ä‰∏ãÊñáÂßøÊÄÅÊ≥®ÂÖ•Êú∫Âà∂Ôºå‰ΩøÂæóÂØπÂÆåÊï¥ËøêÂä®Â∫èÂàóÁöÑÊó∂Á©∫Êé®ÁêÜÊõ¥Âä†ÊúâÊïà„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSCAILÂú®ËßíËâ≤Âä®ÁîªÈ¢ÜÂüüËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂêëÂ∑•‰ΩúÂÆ§Á∫ßÂà´ÁöÑÂèØÈù†ÊÄßÂíåÁúüÂÆûÊÑüËøàËøõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.05145', 'title': 'Self-Improving VLM Judges Without Human Annotations', 'url': 'https://huggingface.co/papers/2512.05145', 'abstract': 'A framework for self-training a Vision-Language Model (VLM) judge using self-synthesized data improves judge accuracy on VL-RewardBench, surpassing larger models in several dimensions.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities.', 'score': 17, 'issue_id': 1, 'pub_date': '2025-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': '344947ff680da049', 'authors': ['Inna Wanyin Lin', 'Yushi Hu', 'Shuyue Stella Li', 'Scott Geng', 'Pang Wei Koh', 'Luke Zettlemoyer', 'Tim Althoff', 'Marjan Ghazvininejad'], 'affiliations': ['FAIR at Meta', 'University of Washington'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05145.jpg', 'data': {'categories': ['#reasoning', '#training', '#multimodal', '#interpretability', '#benchmark', '#synthetic', '#dataset'], 'emoji': 'üîÑ', 'ru': {'title': '–°–∞–º–æ–æ–±—É—á–∞—é—â–∏–π—Å—è —Å—É–¥—å—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –º–µ—Ç–æ–¥–∏–∫–∞ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è —Å—É–¥—å–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ Vision-Language Model –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π, –ø—Ä–∏–º–µ–Ω—è—è —Ç–æ–ª—å–∫–æ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –ü–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç —Ç—Ä–∏ —ç—Ç–∞–ø–∞: –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ø–∞—Ä –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ –æ—Ç–≤–µ—Ç–æ–≤ —Ä–∞–∑–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞, —Å–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–æ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —Å—É–∂–¥–µ–Ω–∏–π —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π, –∞ –∑–∞—Ç–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –æ—Ç–≤–µ—Ç–∞—Ö. –û—Ü–µ–Ω–∫–∞ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö VL-RewardBench –ø–æ–∫–∞–∑–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å 0.38 –¥–æ 0.51 –¥–ª—è –º–æ–¥–µ–ª–∏ Llama-3.2-11B, –ø—Ä–µ–≤–∑–æ–π–¥—è –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤—Ä–æ–¥–µ GPT-4o –∏ Claude 3.5 Sonnet. –¢–∞–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∑ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Å–æ–∑–¥–∞–Ω–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Å—É–¥—å–∏, —Ä–∞–∑–≤–∏–≤–∞—é—â–µ–≥–æ—Å—è –≤–º–µ—Å—Ç–µ —Å —Ä–∞—Å—Ç—É—â–∏–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ Vision-Language –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Self-Training VLM Judges: Evolving Without Human Annotations', 'desc': "This paper introduces a new framework for training a Vision-Language Model (VLM) judge without relying on human preference annotations. Instead, it uses self-synthesized data to iteratively improve the judge's accuracy. The process involves generating multimodal instruction-response pairs, evaluating their quality, and training the judge on the best examples. The results show that this self-training method significantly enhances the judge's performance on VL-RewardBench, even surpassing larger models in various evaluation metrics."}, 'zh': {'title': 'Ëá™ÊàëËÆ≠ÁªÉËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãËØÑÂà§Âô®ÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™ÊàëËÆ≠ÁªÉËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâËØÑÂà§Âô®ÁöÑÊ°ÜÊû∂ÔºåÂà©Áî®Ëá™ÊàëÂêàÊàêÁöÑÊï∞ÊçÆÊù•ÊèêÈ´òËØÑÂà§ÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ï‰∏ç‰æùËµñ‰∫éÊòÇË¥µÁöÑ‰∫∫Á±ªÂÅèÂ•ΩÊ≥®ÈáäÔºåËÄåÊòØÈÄöËøáÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÂ§öÊ®°ÊÄÅÊåá‰ª§-ÂìçÂ∫îÂØπËøõË°åËÆ≠ÁªÉ„ÄÇÊ°ÜÊû∂ÂàÜ‰∏∫‰∏â‰∏™Èò∂ÊÆµÔºöÁîüÊàê‰∏çÂêåË¥®ÈáèÊ∞¥Âπ≥ÁöÑÊåá‰ª§-ÂìçÂ∫îÂØπ„ÄÅ‰∏∫ÊØèÂØπÁîüÊàêÊé®ÁêÜËΩ®ËøπÂíåÂà§Êñ≠ÔºåÂπ∂ÂéªÈô§‰∏çÁ¨¶ÂêàÈ¢ÑÊúüË¥®ÈáèÁöÑÂØπÔºåÊúÄÂêéÂú®Ê≠£Á°ÆÁöÑËØÑÂà§Á≠îÊ°àÂíåÊé®ÁêÜËΩ®Ëøπ‰∏äËøõË°åËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®VL-RewardBench‰∏äÊòæËëóÊèêÈ´ò‰∫ÜËØÑÂà§Âô®ÁöÑÂáÜÁ°ÆÊÄßÔºåË∂ÖË∂ä‰∫ÜËÆ∏Â§öÊõ¥Â§ßÁöÑÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÊó†‰∫∫Â∑•Ê≥®ÈáäÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.05044', 'title': 'Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image', 'url': 'https://huggingface.co/papers/2512.05044', 'abstract': 'MoRe4D generates high-quality 4D scenes with multi-view consistency and dynamic details from a single image using a diffusion-based trajectory generator and depth-guided motion normalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating interactive and dynamic 4D scenes from a single static image remains a core challenge. Most existing generate-then-reconstruct and reconstruct-then-generate methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To address these, we extend the reconstruct-then-generate framework to jointly perform Motion generation and geometric Reconstruction for 4D Synthesis (MoRe4D). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image. Code: https://github.com/Zhangyr2022/MoRe4D.', 'score': 15, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '283405418ffc5d21', 'authors': ['Yanran Zhang', 'Ziyi Wang', 'Wenzhao Zheng', 'Zheng Zhu', 'Jie Zhou', 'Jiwen Lu'], 'affiliations': ['Department of Automation, Tsinghua University, China', 'GigaAI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05044.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#3d', '#video', '#synthetic', '#diffusion', '#dataset'], 'emoji': 'üé¨', 'ru': {'title': '–ò–∑ —Å—Ç–∞—Ç–∏–∫–∏ –≤ –¥–∏–Ω–∞–º–∏–∫—É: —Å–∏–Ω—Ç–µ–∑ 4D —Å—Ü–µ–Ω —Å –µ–¥–∏–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è', 'desc': 'MoRe4D ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö 4D —Å—Ü–µ–Ω –∏–∑ –æ–¥–Ω–æ–≥–æ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–≤–∏–∂–µ–Ω–∏—è —Å –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç TrajScene-60K —Å –ø–ª–æ—Ç–Ω—ã–º–∏ —Ç–æ—á–µ—á–Ω—ã–º–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å 4D-STraG –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π. –ö–ª—é—á–µ–≤–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏—è ‚Äî —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è, —É–ø—Ä–∞–≤–ª—è–µ–º–∞—è –∫–∞—Ä—Ç–æ–π –≥–ª—É–±–∏–Ω—ã, –∫–æ—Ç–æ—Ä–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—é –∏ –¥–∏–Ω–∞–º–∏–∫—É –∏–∑ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ú–æ–¥—É–ª—å —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–æ–≤ 4D-ViSM –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–µ–Ω–¥–µ—Ä–∏—Ç—å –≤–∏–¥–µ–æ —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ –∫–∞–º–µ—Ä—ã, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –º–Ω–æ–≥–æ–≤—å—é–ø–æ—Ä—Ç–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ –±–æ–≥–∞—Ç—ã–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏.'}, 'en': {'title': 'Transforming Single Images into Dynamic 4D Scenes!', 'desc': 'MoRe4D is a novel approach that generates high-quality 4D scenes from a single image by integrating motion generation and geometric reconstruction. It introduces a large dataset, TrajScene-60K, which contains 60,000 video samples with detailed point trajectories to improve the training of models. The method employs a diffusion-based trajectory generator to create consistent and plausible 4D point trajectories while using depth-guided motion normalization for better integration of geometry and dynamics. Ultimately, MoRe4D enables the rendering of dynamic videos with multi-view consistency, addressing challenges in existing methods that often lead to inconsistencies in spatiotemporal data.'}, 'zh': {'title': '‰ªéÂçïÂõæÂÉèÁîüÊàêÈ´òË¥®Èáè4DÂú∫ÊôØÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'MoRe4DÊòØ‰∏ÄÁßç‰ªéÂçïÂº†ÂõæÂÉèÁîüÊàêÈ´òË¥®Èáè4DÂú∫ÊôØÁöÑÊñπÊ≥ïÔºåÂÖ∑ÊúâÂ§öËßÜËßí‰∏ÄËá¥ÊÄßÂíåÂä®ÊÄÅÁªÜËäÇ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊâ©Â±ïÈáçÂª∫-ÁîüÊàêÊ°ÜÊû∂ÔºåËÅîÂêàËøõË°åËøêÂä®ÁîüÊàêÂíåÂá†‰ΩïÈáçÂª∫Ôºå‰ª•Ëß£ÂÜ≥Êó∂Á©∫‰∏ç‰∏ÄËá¥ÊÄßÈóÆÈ¢ò„ÄÇÁ†îÁ©∂ËÄÖ‰ª¨ËøòÂºïÂÖ•‰∫ÜTrajScene-60KÊï∞ÊçÆÈõÜÔºåÂåÖÂê´60,000‰∏™ËßÜÈ¢ëÊ†∑Êú¨ÔºåÊèê‰æõ‰∫Ü‰∏∞ÂØåÁöÑ4DÂú∫ÊôØÊï∞ÊçÆ„ÄÇÈÄöËøáÊ∑±Â∫¶ÂºïÂØºÁöÑËøêÂä®ÂΩí‰∏ÄÂåñÁ≠ñÁï•ÔºåMoRe4DËÉΩÂ§üÊúâÊïàÊï¥ÂêàÂá†‰ΩïÂíåÂä®ÊÄÅ‰ø°ÊÅØÔºåÁîüÊàêÈ´òË¥®ÈáèÁöÑ4DÂú∫ÊôØ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.04563', 'title': 'COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence', 'url': 'https://huggingface.co/papers/2512.04563', 'abstract': 'A unified multimodal large language model (MLLM) that integrates depth and segmentation modalities enhances spatial reasoning and perception through adaptive interleaved reasoning, improving spatial intelligence and general performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose COOPER, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average 6.91\\% improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a 7.92\\% gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.', 'score': 13, 'issue_id': 1, 'pub_date': '2025-12-04', 'pub_date_card': {'ru': '4 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 4', 'zh': '12Êúà4Êó•'}, 'hash': '3f39f24628b95ffe', 'authors': ['Zefeng Zhang', 'Xiangzhao Hao', 'Hengzhu Tang', 'Zhenyu Zhang', 'Jiawei Sheng', 'Xiaodong Li', 'Zhenyang Li', 'Li Gao', 'Daiting Shi', 'Dawei Yin', 'Tingwen Liu'], 'affiliations': ['Baidu Inc.', 'Institute of Automation, Chinese Academy of Sciences', 'Institute of Information Engineering, Chinese Academy of Sciences', 'School of Cyber Security, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04563.jpg', 'data': {'categories': ['#reasoning', '#training', '#multimodal', '#3d', '#cv'], 'emoji': 'üß†', 'ru': {'title': '–û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ: –≥–ª—É–±–∏–Ω–∞ –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (MLLM) COOPER, –∫–æ—Ç–æ—Ä–∞—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥–ª—É–±–∏–Ω–µ –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞: —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–æ–±—Ä–µ—Ç–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏, –∑–∞—Ç–µ–º —Ä–∞–∑–≤–∏–≤–∞–µ—Ç –Ω–∞–≤—ã–∫–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–µ—Ä–µ–º–µ–∂–∞—é—â–µ–≥–æ—Å—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Å—Ä–µ–¥–Ω–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 6,91% –≤ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ–±—â–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –ö–ª—é—á–µ–≤–æ–π –≤—ã–≤–æ–¥ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—é –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ –∏–Ω—Ç–µ—Ä–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è.'}, 'en': {'title': 'Enhancing Spatial Intelligence with Unified Multimodal Learning', 'desc': 'This paper presents COOPER, a unified multimodal large language model (MLLM) designed to improve spatial reasoning by integrating depth and segmentation data. Unlike previous models that treat perception and reasoning separately, COOPER employs adaptive interleaved reasoning to enhance both aspects simultaneously. The model is trained in two stages, focusing on generating auxiliary modalities and refining reasoning capabilities, leading to significant improvements in spatial intelligence. Results show an average increase of 6.91% in spatial reasoning performance, indicating that the model effectively internalizes spatial knowledge through its training process.'}, 'zh': {'title': 'Áªü‰∏ÄÂ§öÊ®°ÊÄÅÊ®°ÂûãÊèêÂçáÁ©∫Èó¥Êô∫ËÉΩ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÔºåÂêç‰∏∫COOPERÔºåÊó®Âú®ÈÄöËøáÊ∑±Â∫¶ÂíåÂàÜÂâ≤Ê®°ÊÄÅÁöÑÁªìÂêàÊù•Â¢ûÂº∫Á©∫Èó¥Êé®ÁêÜÂíåÊÑüÁü•ËÉΩÂäõ„ÄÇËØ•Ê®°ÂûãÈááÁî®Ëá™ÈÄÇÂ∫î‰∫§ÈîôÊé®ÁêÜÁöÑÊñπÊ≥ïÔºåÊèêÂçá‰∫ÜÁ©∫Èó¥Êô∫ËÉΩÂíåÊï¥‰ΩìÊÄßËÉΩ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåCOOPERÂú®Á©∫Èó¥Êé®ÁêÜÊñπÈù¢Âπ≥ÂùáÊèêÈ´ò‰∫Ü6.91%ÔºåÂπ∂‰∏îÂç≥‰ΩøÊòØ‰ªÖËÆ≠ÁªÉËæÖÂä©Ê®°ÊÄÅÁîüÊàêÁöÑÂèò‰ΩìÔºåÂú®Ë∑ùÁ¶ªÂíåÂ§ßÂ∞è‰º∞ËÆ°‰∏ä‰πüËé∑Âæó‰∫Ü7.92%ÁöÑÊèêÂçá„ÄÇËøôË°®ÊòéÔºåÂ≠¶‰π†ÁîüÊàêËæÖÂä©Ê®°ÊÄÅÊúâÂä©‰∫éÂÜÖÂåñÁ©∫Èó¥Áü•ËØÜÔºåÂ¢ûÂº∫Á©∫Èó¥ÁêÜËß£ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.05927', 'title': "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty", 'url': 'https://huggingface.co/papers/2512.05927', 'abstract': "C3 is an uncertainty quantification method for training controllable video models that provides dense confidence estimation and out-of-distribution detection, addressing hallucination issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.", 'score': 10, 'issue_id': 1, 'pub_date': '2025-12-05', 'pub_date_card': {'ru': '5 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 5', 'zh': '12Êúà5Êó•'}, 'hash': '784e732b562082f3', 'authors': ['Zhiting Mei', 'Tenny Yin', 'Micah Baker', 'Ola Shorinwa', 'Anirudha Majumdar'], 'affiliations': ['Princeton University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05927.jpg', 'data': {'categories': ['#training', '#interpretability', '#hallucinations', '#video', '#robotics'], 'emoji': 'üé¨', 'ru': {'title': '–î–æ–≤–µ—Ä—è–π –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–∏, –Ω–æ –ø—Ä–æ–≤–µ—Ä—è–π –µ—ë —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å', 'desc': 'C3 - —ç—Ç–æ –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã—Ö –≤–∏–¥–µ–æ–º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–ª–æ—Ç–Ω–æ–µ‰º∞estimation —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—á–µ–π –∫–∞–¥—Ä–æ–≤. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä–æ–≥–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –º–æ–¥–µ–ª–∏, –∏–∑–±–µ–≥–∞—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ –≤ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å –∏–∑ —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ —Ç–µ–ø–ª–æ–≤—ã–µ –∫–∞—Ä—Ç—ã –≤ RGB-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ–Ω–∞–¥—ë–∂–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–∞–ª–∏–±—Ä—É–µ–º—ã–µ –æ—Ü–µ–Ω–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π, –≤—ã—Ö–æ–¥—è—â–∏—Ö –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏.'}, 'en': {'title': 'C3: Calibrating Confidence in Controllable Video Generation', 'desc': 'C3 is a novel method designed to improve the reliability of controllable video models by quantifying uncertainty during training. It addresses the problem of hallucination, where generated video frames do not align with reality, which is critical for applications like robotics. The method introduces a framework that ensures video models can accurately assess their confidence in generated outputs, using proper scoring rules and latent space uncertainty estimation. Additionally, it provides visual representations of uncertainty through high-resolution heatmaps, helping to identify areas in the video that may be unreliable.'}, 'zh': {'title': 'C3ÔºöÊèêÂçáÂèØÊéßËßÜÈ¢ëÊ®°ÂûãÁöÑÁΩÆ‰ø°Â∫¶‰∏éÂºÇÂ∏∏Ê£ÄÊµã', 'desc': 'C3ÊòØ‰∏ÄÁßç‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñÊñπÊ≥ïÔºåÁî®‰∫éËÆ≠ÁªÉÂèØÊéßËßÜÈ¢ëÊ®°ÂûãÔºåÊèê‰æõÂØÜÈõÜÁöÑÁΩÆ‰ø°Â∫¶‰º∞ËÆ°ÂíåÂºÇÂ∏∏Ê£ÄÊµãÔºåËß£ÂÜ≥‰∫ÜÂπªËßâÈóÆÈ¢ò„ÄÇÂ∞ΩÁÆ°ÂèØÊéßËßÜÈ¢ëÊ®°ÂûãÂú®ÁîüÊàêÈ´ò‰øùÁúüËßÜÈ¢ëÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÆÉ‰ª¨Â∏∏Â∏∏‰ºöÁîüÊàê‰∏éÁâ©ÁêÜÁé∞ÂÆû‰∏çÁ¨¶ÁöÑÊú™Êù•ËßÜÈ¢ëÂ∏ß„ÄÇC3ÈÄöËøáÂºïÂÖ•‰∏âÈ°πÊ†∏ÂøÉÂàõÊñ∞ÔºåÂ∏ÆÂä©ËßÜÈ¢ëÊ®°ÂûãËØÑ‰º∞ÂíåË°®ËææÂÖ∂‰∏çÁ°ÆÂÆöÊÄßÔºå‰ªéËÄåÂáèËΩªÂπªËßâÁé∞Ë±°„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåC3‰∏ç‰ªÖÂú®ËÆ≠ÁªÉÂàÜÂ∏ÉÂÜÖÊèê‰æõ‰∫ÜÊ†°ÂáÜÁöÑ‰∏çÁ°ÆÂÆöÊÄß‰º∞ËÆ°ÔºåËøòÊúâÊïàÂú∞ÂÆûÁé∞‰∫ÜÂºÇÂ∏∏Ê£ÄÊµã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.02835', 'title': 'ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2512.02835', 'abstract': 'ReVSeg, a reasoning-centric video object segmentation framework, uses sequential decision-making in pretrained vision language models and reinforcement learning to achieve state-of-the-art performance and interpretable reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .', 'score': 9, 'issue_id': 1, 'pub_date': '2025-12-02', 'pub_date_card': {'ru': '2 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 2', 'zh': '12Êúà2Êó•'}, 'hash': 'f85a1eb3f3cd60a0', 'authors': ['Yifan Li', 'Yingda Yin', 'Lingting Zhu', 'Weikai Chen', 'Shengju Qian', 'Xin Wang', 'Yanwei Fu'], 'affiliations': ['Fudan University', 'LIGHTSPEED', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.02835.jpg', 'data': {'categories': ['#rl', '#reasoning', '#multimodal', '#interpretability', '#video', '#benchmark', '#cv'], 'emoji': 'üé¨', 'ru': {'title': '–ü—Ä–æ–∑—Ä–∞—á–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏', 'desc': 'ReVSeg ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —è–≤–Ω–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏ —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã —Å–≤—ë—Ä—Ç—ã–≤–∞—Ç—å –≤—Å–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ –æ–¥–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ, –º–æ–¥–µ–ª—å —è–≤–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ç—Ä–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏: –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é —Å–µ–º–∞–Ω—Ç–∏–∫–∏, –≤—ã–±–æ—Ä –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é. –î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ü–µ–ø–∏ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –ø–æ–∑–≤–æ–ª—è—é—â–µ–µ –º–æ–¥–µ–ª–∏ —É–ª—É—á—à–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ç–æ–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏. –ü–æ–¥—Ö–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'ReVSeg: Unraveling Video Segmentation with Reasoning', 'desc': 'ReVSeg is a novel framework designed for video object segmentation that emphasizes reasoning through sequential decision-making. It leverages pretrained vision language models (VLMs) to handle complex tasks involving dynamics and temporal interactions, rather than relying on simplified latent embeddings. The framework breaks down reasoning into three clear operations: interpreting semantics, selecting temporal evidence, and grounding spatial information. By incorporating reinforcement learning, ReVSeg enhances its decision-making process, leading to improved performance and clearer reasoning paths in video segmentation tasks.'}, 'zh': {'title': 'Êé®ÁêÜÈ©±Âä®ÁöÑËßÜÈ¢ëÁâ©‰ΩìÂàÜÂâ≤Êñ∞Ê°ÜÊû∂', 'desc': 'ReVSegÊòØ‰∏Ä‰∏™‰ª•Êé®ÁêÜ‰∏∫‰∏≠ÂøÉÁöÑËßÜÈ¢ëÁâ©‰ΩìÂàÜÂâ≤Ê°ÜÊû∂ÔºåÂà©Áî®È¢ÑËÆ≠ÁªÉÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂíåÂº∫ÂåñÂ≠¶‰π†ÂÆûÁé∞‰∫ÜÂÖàËøõÁöÑÊÄßËÉΩÂíåÂèØËß£ÈáäÁöÑÊé®ÁêÜ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÊòæÂºèÂàÜËß£Êé®ÁêÜËøáÁ®ãÔºåÊâßË°åËØ≠‰πâËß£Èáä„ÄÅÊó∂Èó¥ËØÅÊçÆÈÄâÊã©ÂíåÁ©∫Èó¥ÂÆö‰Ωç‰∏â‰∏™ÊòéÁ°ÆÊìç‰ΩúÔºåËÄå‰∏çÊòØÂ∞ÜÊâÄÊúâÊé®ÁêÜÂêàÂπ∂‰∏∫ÂçïÊ≠•È¢ÑÊµã„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåReVSegËÉΩÂ§üÂ§ÑÁêÜÂä®ÊÄÅ„ÄÅÂõ†ÊûúÂÖ≥Á≥ªÂíåÊó∂Èó¥‰∫§‰∫íÁ≠âÂ§çÊùÇÂõ†Á¥†ÔºåÊèê‰æõÊõ¥Ê∏ÖÊô∞ÁöÑÊé®ÁêÜÈìæ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåReVSegÂú®Ê†áÂáÜËßÜÈ¢ëÁâ©‰ΩìÂàÜÂâ≤Âü∫ÂáÜ‰∏äËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÂπ∂ÁîüÊàê‰∫ÜÂèØËß£ÈáäÁöÑÊé®ÁêÜËΩ®Ëøπ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.03514', 'title': 'M3DR: Towards Universal Multilingual Multimodal Document Retrieval', 'url': 'https://huggingface.co/papers/2512.03514', 'abstract': 'M3DR is a multilingual multimodal document retrieval framework using contrastive training to achieve robust cross-lingual and cross-modal alignment across diverse languages and document types.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.', 'score': 7, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 3', 'zh': '12Êúà3Êó•'}, 'hash': '450a516e3415c50b', 'authors': ['Adithya S Kolavi', 'Vyoman Jain'], 'affiliations': ['CognitiveLab'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.03514.jpg', 'data': {'categories': ['#open_source', '#multilingual', '#multimodal', '#benchmark', '#synthetic', '#rag', '#dataset', '#low_resource', '#transfer_learning'], 'emoji': 'üåç', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ —è–∑—ã–∫–æ–≤—ã—Ö –≥—Ä–∞–Ω–∏—Ü', 'desc': 'M3DR ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –µ–¥–∏–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ vision-language –º–æ–¥–µ–ª–µ–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –Ω–∞–¥–µ–∂–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏ –∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏. –ê–≤—Ç–æ—Ä—ã –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–ª–∏ –ø–æ–¥—Ö–æ–¥ –Ω–∞ 22 —Ç–∏–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –∏ —Å–æ–∑–¥–∞–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤ –º–æ–Ω–æ–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö, –º–Ω–æ–≥–æ–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –∏ —Å–º–µ—à–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ NetraEmbed –∏ ColNetraEmbed –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∫—Ä–æ—Å—Å-–ª–∏–Ω–≥–≤–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'Bridging Languages and Formats with M3DR!', 'desc': 'M3DR is a framework designed for multilingual multimodal document retrieval, which means it can find documents in different languages and formats, like text and images. It uses contrastive training to create strong connections between text and images, allowing it to work well across various languages and document types. The framework is tested on 22 different languages, showing that it can adapt to different linguistic and cultural contexts effectively. M3DR outperforms existing methods, achieving significant improvements in retrieving documents across languages.'}, 'zh': {'title': 'Ë∑®ËØ≠Ë®ÄÂ§öÊ®°ÊÄÅÊñáÊ°£Ê£ÄÁ¥¢ÁöÑÊú™Êù•', 'desc': 'M3DRÊòØ‰∏Ä‰∏™Â§öËØ≠Ë®ÄÂ§öÊ®°ÊÄÅÊñáÊ°£Ê£ÄÁ¥¢Ê°ÜÊû∂ÔºåÈááÁî®ÂØπÊØîËÆ≠ÁªÉÊñπÊ≥ïÔºåÂÆûÁé∞‰∫ÜË∑®ËØ≠Ë®ÄÂíåË∑®Ê®°ÊÄÅÁöÑÂº∫Â§ßÂØπÈΩêËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Êó®Âú®Ëß£ÂÜ≥Áé∞ÊúâÊñπÊ≥ïËøá‰∫éÈõÜ‰∏≠‰∫éËã±ËØ≠ÁöÑÈóÆÈ¢òÔºå‰ΩøÂÖ∂Âú®Â§öËØ≠Ë®ÄÁéØÂ¢É‰∏≠Êõ¥ÊúâÊïà„ÄÇM3DRÂà©Áî®ÂêàÊàêÁöÑÂ§öËØ≠Ë®ÄÊñáÊ°£Êï∞ÊçÆÔºåËÉΩÂ§üÂú®‰∏çÂêåÁöÑËßÜËßâ-ËØ≠Ë®ÄÊû∂ÊûÑÂíåÊ®°ÂûãËßÑÊ®°‰∏≠ËøõË°åÊ≥õÂåñ„ÄÇÈÄöËøáÂØπÊØîËÆ≠ÁªÉÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂ≠¶‰π†Âà∞Áªü‰∏ÄÁöÑÊñáÊú¨ÂíåÊñáÊ°£ÂõæÂÉèË°®Á§∫ÔºåËÉΩÂ§üÊúâÊïàÂú∞Âú®Â§öÁßçËØ≠Ë®Ä‰πãÈó¥ËΩ¨Áßª„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.05774', 'title': 'Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding', 'url': 'https://huggingface.co/papers/2512.05774', 'abstract': 'Active Video Perception (AVP) improves long video understanding by iteratively selecting and evaluating query-relevant video evidence, achieving higher accuracy with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens.', 'score': 5, 'issue_id': 1, 'pub_date': '2025-12-05', 'pub_date_card': {'ru': '5 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 5', 'zh': '12Êúà5Êó•'}, 'hash': '77d3a45363a326ec', 'authors': ['Ziyang Wang', 'Honglu Zhou', 'Shijie Wang', 'Junnan Li', 'Caiming Xiong', 'Silvio Savarese', 'Mohit Bansal', 'Michael S. Ryoo', 'Juan Carlos Niebles'], 'affiliations': ['Salesforce AI Research', 'University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05774.jpg', 'data': {'categories': ['#reasoning', '#agents', '#multimodal', '#video', '#benchmark', '#long_context'], 'emoji': 'üé¨', 'ru': {'title': '–ê–∫—Ç–∏–≤–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –≤–∏–¥–µ–æ: —É–º–Ω—ã–π –ø–æ–∏—Å–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –≤–º–µ—Å—Ç–æ –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Active Video Perception (AVP) ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –≤–º–µ—Å—Ç–æ –ø–∞—Å—Å–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –°–∏—Å—Ç–µ–º–∞ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –≤—ã–±–∏—Ä–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤–∏–¥–µ–æ, –ø—Ä–∏–º–µ–Ω—è—è –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π LLM –∞–≥–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–ª–∞–Ω–∏—Ä—É–µ—Ç, –Ω–∞–±–ª—é–¥–∞–µ—Ç –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å. AVP –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –ø—è—Ç–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –Ω–∞ 5.7% –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ç–æ–ª—å–∫–æ 18.4% –≤—Ä–µ–º–µ–Ω–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã, —Ñ–æ–∫—É—Å–∏—Ä—É—è—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤–º–µ—Å—Ç–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Å–æ–≤ –∏–∑–±—ã—Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.'}, 'en': {'title': 'Active Video Perception: Smartly Watching Videos for Better Understanding', 'desc': 'Active Video Perception (AVP) enhances the understanding of long videos by selectively identifying and evaluating relevant video evidence based on specific queries. This approach addresses the challenge of long video understanding, where important cues are often hidden among irrelevant content. By employing an iterative plan-observe-reflect process, AVP allows agents to actively determine what to observe and when, optimizing the use of computational resources. The results show that AVP significantly improves accuracy while reducing inference time and input tokens compared to existing methods.'}, 'zh': {'title': '‰∏ªÂä®ËßÜÈ¢ëÊÑüÁü•ÔºöÊèêÂçáÈïøËßÜÈ¢ëÁêÜËß£ÁöÑÊô∫ËÉΩÈÄâÊã©', 'desc': '‰∏ªÂä®ËßÜÈ¢ëÊÑüÁü•ÔºàAVPÔºâÈÄöËøáËø≠‰ª£ÈÄâÊã©ÂíåËØÑ‰º∞‰∏éÊü•ËØ¢Áõ∏ÂÖ≥ÁöÑËßÜÈ¢ëËØÅÊçÆÔºåÊèêÂçá‰∫ÜÈïøËßÜÈ¢ëÁêÜËß£ÁöÑÊïàÊûúÔºåËÉΩÂ§üÂú®Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÁöÑÂêåÊó∂ÊèêÈ´òÂáÜÁ°ÆÊÄß„ÄÇÈïøËßÜÈ¢ëÁêÜËß£ÔºàLVUÔºâÈù¢‰∏¥ÊåëÊàòÔºåÂõ†‰∏∫ÂõûÁ≠îÁé∞ÂÆû‰∏ñÁïåÁöÑÈóÆÈ¢òÂæÄÂæÄ‰æùËµñ‰∫éÁ®ÄÁñè‰∏îÊó∂Èó¥ÂàÜÊï£ÁöÑÁ∫øÁ¥¢ÔºåËøô‰∫õÁ∫øÁ¥¢ÂüãËóèÂú®Â§ßÈáèÂÜó‰ΩôÂíåÊó†ÂÖ≥ÁöÑÂÜÖÂÆπ‰∏≠„ÄÇAVPÊ°ÜÊû∂Âü∫‰∫é‰∏ªÂä®ÊÑüÁü•ÁêÜËÆ∫ÔºåÂÖÅËÆ∏Êô∫ËÉΩ‰Ωì‰∏ªÂä®ÂÜ≥ÂÆöËßÇÂØüÁöÑÂÜÖÂÆπ„ÄÅÊó∂Èó¥ÂíåÂú∞ÁÇπÔºåÂπ∂ÊåÅÁª≠ËØÑ‰º∞ÂΩìÂâçËßÇÂØüÊòØÂê¶Ë∂≥Â§üÂõûÁ≠îÊü•ËØ¢„ÄÇÈÄöËøáÂú®‰∫î‰∏™LVUÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåAVPÂú®Âπ≥ÂùáÂáÜÁ°ÆÊÄß‰∏äÊØîÊúÄ‰Ω≥Êô∫ËÉΩÊñπÊ≥ïÊèêÈ´ò‰∫Ü5.7%ÔºåÂêåÊó∂‰ªÖÈúÄ18.4%ÁöÑÊé®ÁêÜÊó∂Èó¥Âíå12.4%ÁöÑËæìÂÖ•Ê†áËÆ∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.05564', 'title': 'ProPhy: Progressive Physical Alignment for Dynamic World Simulation', 'url': 'https://huggingface.co/papers/2512.05564', 'abstract': 'ProPhy, a two-stage framework with Semantic and Refinement Experts, enhances video generation by incorporating physics-aware conditioning and anisotropic generation, improving physical consistency and realism.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-12-05', 'pub_date_card': {'ru': '5 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 5', 'zh': '12Êúà5Êó•'}, 'hash': '54cb0001d019f855', 'authors': ['Zijun Wang', 'Panwen Hu', 'Jing Wang', 'Terry Jingchen Zhang', 'Yuhao Cheng', 'Long Chen', 'Yiqiang Yan', 'Zutao Jiang', 'Hanhui Li', 'Xiaodan Liang'], 'affiliations': ['ETH Zurich', 'Lenovo Research', 'Mohamed bin Zayed University of Artificial Intelligence', 'Peng Cheng Laboratory', 'Shenzhen Campus of Sun Yat-sen University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05564.jpg', 'data': {'categories': ['#video', '#benchmark', '#optimization', '#multimodal'], 'emoji': 'üé¨', 'ru': {'title': '–§–∏–∑–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤', 'desc': 'ProPhy ‚Äî —ç—Ç–æ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–ª–∞–≥–æ–¥–∞—Ä—è —è–≤–Ω–æ–º—É –∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—é —Å —É—á—ë—Ç–æ–º —Ñ–∏–∑–∏–∫–∏ –∏ –∞–Ω–∏–∑–æ—Ç—Ä–æ–ø–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º Mixture-of-Physics-Experts —Å –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ –∏ –£—Ç–æ—á–Ω—è—é—â–∏–º–∏ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–∑–≤–ª–µ–∫–∞—é—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –∏ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—Ç —Ç–æ–∫–µ–Ω-—É—Ä–æ–≤–Ω–µ–≤—É—é –¥–∏–Ω–∞–º–∏–∫—É. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —ç–∫—Å–ø–µ—Ä—Ç—ã, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —è–≤–ª–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ProPhy –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ, –¥–∏–Ω–∞–º–∏—á–Ω—ã–µ –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –ª–æ–≥–∏—á–Ω—ã–µ –≤–∏–¥–µ–æ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'ProPhy: Enhancing Video Generation with Physics-Aware Conditioning', 'desc': 'ProPhy is a two-stage framework designed to improve video generation by focusing on physics-aware conditioning and anisotropic generation. It uses a Mixture-of-Physics-Experts (MoPE) approach, where Semantic Experts extract physical principles from text, and Refinement Experts refine these principles at a token level. This allows the model to create video representations that align closely with real-world physical laws, addressing the limitations of existing models that often produce inconsistent results. The framework has been shown to outperform current state-of-the-art methods in generating realistic and dynamic videos that adhere to physical consistency.'}, 'zh': {'title': 'ProPhyÔºöÊèêÂçáËßÜÈ¢ëÁîüÊàêÁöÑÁâ©ÁêÜ‰∏ÄËá¥ÊÄß‰∏éÁúüÂÆûÊÑü', 'desc': 'ProPhyÊòØ‰∏ÄÁßç‰∏§Èò∂ÊÆµÊ°ÜÊû∂ÔºåÁªìÂêà‰∫ÜËØ≠‰πâ‰∏ìÂÆ∂ÂíåÁ≤æÁªÜÂåñ‰∏ìÂÆ∂ÔºåÊó®Âú®ÊèêÂçáËßÜÈ¢ëÁîüÊàêÁöÑÁâ©ÁêÜ‰∏ÄËá¥ÊÄßÂíåÁúüÂÆûÊÑü„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÁâ©ÁêÜÊÑüÁü•Êù°‰ª∂ÂíåÂêÑÂêëÂºÇÊÄßÁîüÊàêÔºåËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊ®°ÂûãÂú®Â§ÑÁêÜÂ§çÊùÇÂä®ÊÄÅÊó∂ÁöÑÂ±ÄÈôêÊÄß„ÄÇProPhyÈááÁî®Ê∑∑ÂêàÁâ©ÁêÜ‰∏ìÂÆ∂Êú∫Âà∂ÔºåËØ≠‰πâ‰∏ìÂÆ∂‰ªéÊñáÊú¨ÊèèËø∞‰∏≠Êé®Êñ≠Áâ©ÁêÜÂéüÂàôÔºåËÄåÁ≤æÁªÜÂåñ‰∏ìÂÆ∂ÂàôÊçïÊçâÁªÜÁ≤íÂ∫¶ÁöÑÁâ©ÁêÜÂä®ÊÄÅ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåProPhyÂú®Áâ©ÁêÜÊÑüÁü•ËßÜÈ¢ëÁîüÊàêÂü∫ÂáÜ‰∏äË°®Áé∞Âá∫Êõ¥È´òÁöÑÁúüÂÆûÊÑüÂíåÂä®ÊÄÅ‰∏ÄËá¥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.03667', 'title': 'Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning', 'url': 'https://huggingface.co/papers/2512.03667', 'abstract': 'Colon-X advances multimodal intelligence in colonoscopy by constructing comprehensive datasets and developing reasoning-centric models that outperform traditional methods under data scarcity.  \t\t\t\t\tAI-generated summary \t\t\t\t In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 3', 'zh': '12Êúà3Êó•'}, 'hash': '03cf54cce0cd3c66', 'authors': ['Ge-Peng Ji', 'Jingyi Liu', 'Deng-Ping Fan', 'Nick Barnes'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.03667.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#training', '#optimization', '#multimodal', '#benchmark', '#healthcare', '#science', '#dataset'], 'emoji': 'üî¨', 'ru': {'title': '–ö–ª–∏–Ω–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞ Colon-X –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ –∫–æ–ª–æ–Ω–æ—Å–∫–æ–ø–∏–∏. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ ColonVQA ‚Äî —Å–∞–º—ã–π –ø–æ–ª–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –±–æ–ª–µ–µ —á–µ–º 1.1 –º–ª–Ω –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø–æ 76 –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–º –Ω–∞—Ö–æ–¥–∫–∞–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –Ω–∞–¥—ë–∂–Ω—ã –∏ –≤–æ—Å–ø—Ä–∏–∏–º—á–∏–≤—ã –∫ –ø–æ–º–µ—Ö–∞–º –≤ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å ColonR1 —Å —Ç–µ—Ö–Ω–∏–∫–∞–º–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–µ fine-tuning –Ω–∞ 25.22% –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Revolutionizing Colonoscopy with Multimodal Intelligence and Reasoning', 'desc': 'Colon-X is an initiative that enhances multimodal intelligence in colonoscopy by creating extensive datasets and developing advanced reasoning models. The project introduces ColonVQA, a large dataset with over 1.1 million entries for visual question answering related to colonoscopy. It also addresses the need for improved clinical reasoning by assessing the performance of multimodal large language models and developing ColonR1, a model that incorporates innovative optimization techniques. This model demonstrates significant improvements in accuracy under data-scarce conditions, establishing a new standard for multimodal analysis in colonoscopy.'}, 'zh': {'title': 'Êé®Âä®ÁªìËÇ†ÈïúÊ£ÄÊü•ÁöÑÂ§öÊ®°ÊÄÅÊô∫ËÉΩÈù©ÂëΩ', 'desc': 'Êú¨Á†îÁ©∂‰ªãÁªç‰∫ÜColon-XÔºåËøôÊòØ‰∏Ä‰∏™Êó®Âú®Êé®Âä®ÁªìËÇ†ÈïúÊ£ÄÊü•‰∏≠Â§öÊ®°ÊÄÅÊô∫ËÉΩÁöÑÂºÄÊîæÈ°πÁõÆ„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫ÜColonVQAÔºåËøôÊòØËøÑ‰ªä‰∏∫Ê≠¢ÊúÄÂÖ®Èù¢ÁöÑÁªìËÇ†ÈïúÂ§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜÔºåÂåÖÂê´Ë∂ÖËøá110‰∏á‰∏™ËßÜËßâÈóÆÁ≠îÊù°ÁõÆÔºåÊ∂µÁõñ76Áßç‰∏¥Â∫äÂèëÁé∞Âíå18ÁßçÂ§öÊ®°ÊÄÅ‰ªªÂä°„ÄÇÊàë‰ª¨ËøòÊé¢ËÆ®‰∫ÜÁªìËÇ†ÈïúÊ£ÄÊü•‰∏≠‰ªéÂ§öÊ®°ÊÄÅÁêÜËß£Âà∞‰∏¥Â∫äÊé®ÁêÜÁöÑÂÖ≥ÈîÆËΩ¨ÂèòÔºåÂºÄÂèë‰∫ÜColonReasonÊï∞ÊçÆÈõÜÂíåColonR1Ê®°ÂûãÔºå‰ª•ÊèêÈ´òÂú®Êï∞ÊçÆÁ®ÄÁº∫Êù°‰ª∂‰∏ãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑColonR1Ê®°ÂûãÂú®Êï¥‰ΩìÂáÜÁ°ÆÁéá‰∏äËææÂà∞‰∫Ü56.61%ÔºåÊØî‰º†ÁªüÁöÑÁõëÁù£ÂæÆË∞ÉÊñπÊ≥ïÊèêÈ´ò‰∫Ü25.22%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.05409', 'title': 'SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs', 'url': 'https://huggingface.co/papers/2512.05409', 'abstract': 'A Sparse-Quantized Format (SQ-format) is proposed to improve the balance between accuracy and efficiency in post-training quantization of large language models by leveraging sparse and low-precision matrix multiplications.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.', 'score': 2, 'issue_id': 1, 'pub_date': '2025-12-05', 'pub_date_card': {'ru': '5 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 5', 'zh': '12Êúà5Êó•'}, 'hash': 'b90fe4ddbb262b34', 'authors': ['Ruixuan Huang', 'Hao Zeng', 'Hantao Huang', 'Jinyuan Shi', 'Minghui Yu', 'Ian En-Hsu Yen', 'Shuai Wang'], 'affiliations': ['ByteDance', 'HKUST', 'Moffett AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05409.jpg', 'data': {'categories': ['#training', '#optimization', '#inference'], 'emoji': '‚ö°', 'ru': {'title': '–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è: –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç Sparse-Quantized (SQ-format) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –∏ —Ä–∞–∑—Ä–µ–∂–∏–≤–∞–Ω–∏—è –º–∞—Ç—Ä–∏—Ü. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –Ω–∏–∑–∫–æ–±–∏—Ç–æ–≤–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π –∏ —Ä–∞–∑—Ä–µ–∂–∏–≤–∞–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ —Å–Ω–∏–∂–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∞–ø–ø–∞—Ä–∞—Ç–Ω–æ–π —á–∞—Å—Ç–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—Å–∫–æ—Ä—è—Ç—å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å –Ω–∏–∑–∫–æ–±–∏—Ç–æ–≤—É—é –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫—É, –¥–æ—Å—Ç–∏–≥–∞—è –ø–∞—Ä–µ—Ç–æ–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –∞–ø–ø–∞—Ä–∞—Ç–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è AI-—É—Å–∫–æ—Ä–∏—Ç–µ–ª–µ–π.'}, 'en': {'title': 'Achieving Efficiency and Accuracy with Sparse-Quantized Format', 'desc': 'The paper introduces a new data format called Sparse-Quantized Format (SQ-format) aimed at enhancing the efficiency and accuracy of post-training quantization (PTQ) for large language models. It addresses the challenges of existing low-bit quantization methods that struggle to maintain performance due to hardware limitations. By leveraging both sparse and low-precision matrix multiplications, SQ-format allows for better utilization of GPU resources while minimizing accuracy loss. The authors demonstrate that this format can lead to significant improvements in PTQ performance and provide insights for future AI hardware designs.'}, 'zh': {'title': 'Á®ÄÁñèÈáèÂåñÊ†ºÂºèÔºöÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊïàÁéá‰∏éÂáÜÁ°ÆÊÄß', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ®ÄÁñèÈáèÂåñÊ†ºÂºèÔºàSQ-formatÔºâÔºåÊó®Âú®ÊîπÂñÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂêéËÆ≠ÁªÉÈáèÂåñ‰∏≠ÁöÑÂáÜÁ°ÆÊÄß‰∏éÊïàÁéá‰πãÈó¥ÁöÑÂπ≥Ë°°„ÄÇÁé∞ÊúâÁöÑ‰Ωé‰ΩçÈáèÂåñÂíåÁ®ÄÁñèÂåñÊäÄÊúØÁî±‰∫éÁ°¨‰ª∂ÊîØÊåÅÁöÑÈôêÂà∂ÔºåÈöæ‰ª•ÂÆûÁé∞Ëøô‰∏ÄÂπ≥Ë°°„ÄÇSQ-formatÂà©Áî®Á®ÄÁñèÁü©ÈòµÂú®È´òÁ≤æÂ∫¶‰∏ãÁöÑÂä†ÈÄüÁâπÊÄßÔºåÂêåÊó∂‰πüËÉΩÂä†ÈÄü‰ΩéÁ≤æÂ∫¶Áü©Èòµ‰πòÊ≥ïÔºå‰ªéËÄåÂÆûÁé∞ÊÄßËÉΩ‰∏éÂêûÂêêÈáèÁöÑÂ∏ïÁ¥ØÊâòÊîπËøõ„ÄÇËØ•Ê†ºÂºèÁâπÂà´ÈÄÇÂêàÂ§ÑÁêÜÂÖ∑ÊúâÂºÇÂ∏∏ÂÄº‰∏çÂπ≥Á≠âÁä∂ÊÄÅÁöÑÊøÄÊ¥ªÔºåÂπ∂‰ΩøÂÖ∂ÈùôÊÄÅÂéãÁº©Êàê‰∏∫ÂèØËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.04142', 'title': 'From FLOPs to Footprints: The Resource Cost of Artificial Intelligence', 'url': 'https://huggingface.co/papers/2512.04142', 'abstract': 'The study quantifies the material footprint of AI training, focusing on the Nvidia A100 GPU, and examines the environmental impact of training models like GPT-4, emphasizing the need for resource-efficient strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t As computational demands continue to rise, assessing the environmental footprint of AI requires moving beyond energy and water consumption to include the material demands of specialized hardware. This study quantifies the material footprint of AI training by linking computational workloads to physical hardware needs. The elemental composition of the Nvidia A100 SXM 40 GB graphics processing unit (GPU) was analyzed using inductively coupled plasma optical emission spectroscopy, which identified 32 elements. The results show that AI hardware consists of about 90% heavy metals and only trace amounts of precious metals. The elements copper, iron, tin, silicon, and nickel dominate the GPU composition by mass. In a multi-step methodology, we integrate these measurements with computational throughput per GPU across varying lifespans, accounting for the computational requirements of training specific AI models at different training efficiency regimes. Scenario-based analyses reveal that, depending on Model FLOPs Utilization (MFU) and hardware lifespan, training GPT-4 requires between 1,174 and 8,800 A100 GPUs, corresponding to the extraction and eventual disposal of up to 7 tons of toxic elements. Combined software and hardware optimization strategies can reduce material demands: increasing MFU from 20% to 60% lowers GPU requirements by 67%, while extending lifespan from 1 to 3 years yields comparable savings; implementing both measures together reduces GPU needs by up to 93%. Our findings highlight that incremental performance gains, such as those observed between GPT-3.5 and GPT-4, come at disproportionately high material costs. The study underscores the necessity of incorporating material resource considerations into discussions of AI scalability, emphasizing that future progress in AI must align with principles of resource efficiency and environmental responsibility.', 'score': 1, 'issue_id': 1, 'pub_date': '2025-12-03', 'pub_date_card': {'ru': '3 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 3', 'zh': '12Êúà3Êó•'}, 'hash': '4edb871569fa468e', 'authors': ['Sophia Falk', 'Nicholas Kluge Corr√™a', 'Sasha Luccioni', 'Lisa Biber-Freudenberger', 'Aimee van Wynsberghe'], 'affiliations': ['Center for Development Research, Bonn University, Germany', 'Center for Science and Thought, Bonn University, Germany', 'Hugging Face', 'Sustainable AI Lab, Institute for Science and Ethics, Bonn University, Germany'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.04142.jpg', 'data': {'categories': ['#ethics', '#training', '#optimization', '#inference'], 'emoji': '‚ôªÔ∏è', 'ru': {'title': '–°–∫—Ä—ã—Ç–∞—è —Ü–µ–Ω–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–π —Å–ª–µ–¥ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —ç—Ç–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–π —Å–ª–µ–¥ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ GPU Nvidia A100, –≤—ã—è–≤–ª—è—è, —á—Ç–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ –Ω–∞ 90% —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—è–∂–µ–ª—ã—Ö –º–µ—Ç–∞–ª–ª–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ GPT-4 —Ç—Ä–µ–±—É–µ—Ç –æ—Ç 1,174 –¥–æ 8,800 GPU –∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –∏ —É—Ç–∏–ª–∏–∑–∞—Ü–∏–∏ –¥–æ 7 —Ç–æ–Ω–Ω —Ç–æ–∫—Å–∏—á–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å—Ä–æ–∫–∞ —Å–ª—É–∂–±—ã –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è –º–æ–≥—É—Ç —Å–Ω–∏–∑–∏—Ç—å –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ GPU –Ω–∞ 93%. –†–∞–±–æ—Ç–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –≤–∞–∂–Ω–æ—Å—Ç—å —É—á–µ—Ç–∞ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ —Ä–∞–∑–≤–∏—Ç–∏—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.'}, 'en': {'title': 'Optimizing AI Training for a Greener Future', 'desc': 'This study evaluates the material footprint of AI training, particularly focusing on the Nvidia A100 GPU. It highlights that the environmental impact of training models like GPT-4 extends beyond just energy and water usage to include the physical materials required for specialized hardware. By analyzing the elemental composition of the A100 GPU, the research reveals that it is primarily made up of heavy metals, with significant implications for resource extraction and disposal. The findings advocate for optimizing both software and hardware to reduce material demands, emphasizing the importance of sustainability in AI development.'}, 'zh': {'title': 'AIËÆ≠ÁªÉÁöÑÊùêÊñôË∂≥Ëøπ‰∏éÁéØÂ¢ÉË¥£‰ªª', 'desc': 'Êú¨Á†îÁ©∂ÈáèÂåñ‰∫Ü‰∫∫Â∑•Êô∫ËÉΩËÆ≠ÁªÉÁöÑÊùêÊñôË∂≥ËøπÔºåÈáçÁÇπÂàÜÊûê‰∫ÜNvidia A100 GPUÁöÑÁéØÂ¢ÉÂΩ±Âìç„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåAIÁ°¨‰ª∂‰∏≠Á∫¶90%ÊòØÈáçÈáëÂ±ûÔºåÂè™ÊúâÂæÆÈáèÁöÑË¥µÈáëÂ±ûÔºå‰∏ªË¶ÅÊàêÂàÜÂåÖÊã¨Èìú„ÄÅÈìÅ„ÄÅÈî°„ÄÅÁ°ÖÂíåÈïç„ÄÇÈÄöËøáÂ§öÊ≠•È™§ÁöÑÊñπÊ≥ïÔºåÊàë‰ª¨Â∞ÜËøô‰∫õÊµãÈáèÁªìÊûú‰∏é‰∏çÂêåËÆ≠ÁªÉÊïàÁéá‰∏ãÁöÑËÆ°ÁÆóÈúÄÊ±ÇÁõ∏ÁªìÂêàÔºåÂèëÁé∞ËÆ≠ÁªÉGPT-4ÈúÄË¶ÅÁöÑA100 GPUÊï∞ÈáèÂú®1174Âà∞8800‰πãÈó¥„ÄÇÁ†îÁ©∂Âº∫Ë∞ÉÔºåÊú™Êù•ÁöÑAIËøõÂ±ïÂøÖÈ°ª‰∏éËµÑÊ∫êÊïàÁéáÂíåÁéØÂ¢ÉË¥£‰ªªÂéüÂàôÁõ∏‰∏ÄËá¥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2512.05339', 'title': 'Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models', 'url': 'https://huggingface.co/papers/2512.05339', 'abstract': 'Roblox Guard 1.0 is an instruction fine-tuned LLM that enhances safety through comprehensive input-output moderation using a pipeline of LLMs and demonstrates strong performance on out-of-domain safety benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.', 'score': 0, 'issue_id': 1, 'pub_date': '2025-12-05', 'pub_date_card': {'ru': '5 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 5', 'zh': '12Êúà5Êó•'}, 'hash': '34096ba1e4d22c6e', 'authors': ['Mahesh Kumar Nandwana', 'Youngwan Lim', 'Joseph Liu', 'Alex Yang', 'Varun Notibala', 'Nishchaie Khanna'], 'affiliations': ['Roblox'], 'pdf_title_img': 'assets\\pdf\\title_img\\2512.05339.jpg', 'data': {'categories': ['#security', '#open_source', '#training', '#architecture', '#alignment', '#benchmark', '#synthetic', '#dataset'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –º–æ–¥–µ—Ä–∞—Ü–∏—é –≤—Ö–æ–¥–æ–≤ –∏ –≤—ã—Ö–æ–¥–æ–≤', 'desc': 'Roblox Guard 1.0 ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–∞, –ø–æ–ª—É—á–µ–Ω–Ω–∞—è –ø—É—Ç—ë–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–≤—ã—à–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å LLM-—Å–∏—Å—Ç–µ–º —á–µ—Ä–µ–∑ –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –º–æ–¥–µ—Ä–∞—Ü–∏—é –≤—Ö–æ–¥–æ–≤ –∏ –≤—ã—Ö–æ–¥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π. –ú–æ–¥–µ–ª—å, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –Ω–∞ –æ—Å–Ω–æ–≤–µ Llama-3.1-8B-Instruct, –æ–±—É—á–µ–Ω–∞ –æ–±–æ–±—â–∞—Ç—å –∑–Ω–∞–Ω–∏—è –Ω–∞ –Ω–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —É–≥—Ä–æ–∑ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö, –Ω–µ –≤–∫–ª—é—á—ë–Ω–Ω—ã—Ö –≤ –æ–±—É—á–µ–Ω–∏–µ. –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∏ –æ—Ç–∫—Ä—ã—Ç—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ —Ç–∏–ø–∞ ¬´—Ü–µ–ø–æ—á–∫–∞ –º—ã—Å–ª–µ–π¬ª –∏ –∏–Ω–≤–µ—Ä—Å–∏–µ–π –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ RobloxGuard-Eval —Å —Ä–∞—Å—à–∏—Ä—è–µ–º–æ–π —Ç–∞–∫—Å–æ–Ω–æ–º–∏–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∑–∞—â–∏—Ç–Ω—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ LLM.'}, 'en': {'title': 'Enhancing Safety in LLMs with Roblox Guard 1.0', 'desc': "Roblox Guard 1.0 is a specialized large language model (LLM) that focuses on improving safety by moderating both inputs and outputs effectively. It is fine-tuned with instructions to ensure it can handle various safety scenarios, even those it hasn't encountered before. The model uses a combination of synthetic and open-source datasets, along with techniques like chain-of-thought reasoning, to enhance its understanding and decision-making capabilities. Additionally, the introduction of RobloxGuard-Eval provides a new benchmark for evaluating the safety measures of LLMs, ensuring they meet high standards of moderation."}, 'zh': {'title': 'Â¢ûÂº∫ÂÆâÂÖ®ÊÄßÁöÑ Roblox Guard 1.0', 'desc': 'Roblox Guard 1.0 ÊòØ‰∏ÄÁßçÁªèËøáÊåá‰ª§ÂæÆË∞ÉÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÔºåÊó®Âú®ÈÄöËøáÂÖ®Èù¢ÁöÑËæìÂÖ•ËæìÂá∫ÂÆ°Ê†∏Êù•Â¢ûÂº∫ÂÆâÂÖ®ÊÄß„ÄÇËØ•Ê®°Âûã‰ΩøÁî®‰∫Ü‰∏ÄÁ≥ªÂàó LLM ÁöÑÁÆ°ÈÅìÔºåËÉΩÂ§üÊúâÊïàÂú∞Â§ÑÁêÜÊΩúÂú®ÁöÑÈ£éÈô©ËæìÂá∫„ÄÇÈÄöËøáÁªìÂêàÂêàÊàêÂíåÂºÄÊ∫êÁöÑÂÆâÂÖ®Êï∞ÊçÆÈõÜÔºåRoblox Guard 1.0 Âú®Êú™ËßÅËøáÁöÑÂÆâÂÖ®ÂàÜÁ±ª‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇÊàë‰ª¨ËøòÊé®Âá∫‰∫Ü RobloxGuard-Eval Âü∫ÂáÜÔºå‰ª•Á≥ªÁªüÂú∞ËØÑ‰º∞ LLM ÁöÑÂÆâÂÖ®Èò≤Êä§ÂíåÂÆ°Ê†∏Ê°ÜÊû∂ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (1)', '#agi', '#alignment (1)', '#architecture (2)', '#audio', '#benchmark (9)', '#cv (2)', '#data', '#dataset (6)', '#diffusion (3)', '#ethics (1)', '#games', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (3)', '#interpretability (3)', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (10)', '#open_source (5)', '#optimization (6)', '#plp', '#rag (1)', '#reasoning (7)', '#rl (2)', '#rlhf', '#robotics (1)', '#science (1)', '#security (1)', '#small_models', '#story_generation', '#survey', '#synthetic (4)', '#training (9)', '#transfer_learning (1)', '#video (6)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2025-12-08 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-12-08 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-12-08 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    