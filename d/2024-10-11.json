{
    "date": "11 –æ–∫—Ç—è–±—Ä—è",
    "time_utc": "2024-10-14 20:13",
    "issue_id": 104,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.08565",
            "title": "Baichuan-Omni Technical Report",
            "url": "https://huggingface.co/papers/2410.08565",
            "abstract": "The salient multimodal capabilities and interactive experience of GPT-4o highlight its critical role in practical applications, yet it lacks a high-performing open-source counterpart. In this paper, we introduce Baichuan-Omni, the first open-source 7B Multimodal Large Language Model (MLLM) adept at concurrently processing and analyzing modalities of image, video, audio, and text, while delivering an advanced multimodal interactive experience and strong performance. We propose an effective multimodal training schema starting with 7B model and proceeding through two stages of multimodal alignment and multitask fine-tuning across audio, image, video, and text modal. This approach equips the language model with the ability to handle visual and audio data effectively. Demonstrating strong performance across various omni-modal and multimodal benchmarks, we aim for this contribution to serve as a competitive baseline for the open-source community in advancing multimodal understanding and real-time interaction.",
            "score": 59,
            "issue_id": 90,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Baichuan-Omni - –ø–µ—Ä–≤—É—é –æ—Ç–∫—Ä—ã—Ç—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –Ω–∞ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ú–æ–¥–µ–ª—å —Å–ø–æ—Å–æ–±–Ω–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≤–∏–¥–µ–æ, –∞—É–¥–∏–æ –∏ —Ç–µ–∫—Å—Ç, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –æ–ø—ã—Ç. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Å—Ö–µ–º—É –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞: –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏ –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω–∞—è –¥–æ–Ω–∞—Å—Ç—Ä–æ–π–∫–∞. Baichuan-Omni –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.",
                "tags": [
                    "#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è–ú–æ–¥–µ–ª—å",
                    "#Baichuan-Omni",
                    "#–æ—Ç–∫—Ä—ã—Ç—ã–π–ò—Å—Ö–æ–¥–Ω—ã–π–ö–æ–¥"
                ],
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#audio",
                    "#benchmark"
                ],
                "emoji": "üåê",
                "title": "Baichuan-Omni: –æ—Ç–∫—Ä—ã—Ç–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ"
            },
            "hash": "635ebaf87323c35f"
        },
        {
            "id": "https://huggingface.co/papers/2410.08261",
            "title": "Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis",
            "url": "https://huggingface.co/papers/2410.08261",
            "abstract": "Diffusion models, such as Stable Diffusion, have made significant strides in visual generation, yet their paradigm remains fundamentally different from autoregressive language models, complicating the development of unified language-vision models. Recent efforts like LlamaGen have attempted autoregressive image generation using discrete VQVAE tokens, but the large number of tokens involved renders this approach inefficient and slow. In this work, we present Meissonic, which elevates non-autoregressive masked image modeling (MIM) text-to-image to a level comparable with state-of-the-art diffusion models like SDXL. By incorporating a comprehensive suite of architectural innovations, advanced positional encoding strategies, and optimized sampling conditions, Meissonic substantially improves MIM's performance and efficiency. Additionally, we leverage high-quality training data, integrate micro-conditions informed by human preference scores, and employ feature compression layers to further enhance image fidelity and resolution. Our model not only matches but often exceeds the performance of existing models like SDXL in generating high-quality, high-resolution images. Extensive experiments validate Meissonic's capabilities, demonstrating its potential as a new standard in text-to-image synthesis. We release a model checkpoint capable of producing 1024 times 1024 resolution images.",
            "score": 35,
            "issue_id": 91,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "Meissonic - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –ø–æ–¥—Ö–æ–¥–µ masked image modeling (MIM). –û–Ω–∞ —Å–æ—á–µ—Ç–∞–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏, –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —É—Å–ª–æ–≤–∏—è —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ MIM. Meissonic –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ, –º–∏–∫—Ä–æ-—É—Å–ª–æ–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ü–µ–Ω–æ–∫ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Å–ª–æ–∏ —Å–∂–∞—Ç–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤—Ä–æ–¥–µ SDXL.",
                "tags": [
                    "#masked_image_modeling",
                    "#text_to_image",
                    "#high_resolution_generation"
                ],
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "üé®",
                "title": "Meissonic: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å MIM"
            },
            "hash": "3b6012d644e53308"
        },
        {
            "id": "https://huggingface.co/papers/2410.06456",
            "title": "From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning",
            "url": "https://huggingface.co/papers/2410.06456",
            "abstract": "Large vision language models (VLMs) combine large language models with vision encoders, demonstrating promise across various tasks. However, they often underperform in task-specific applications due to domain gaps between pre-training and fine-tuning. We introduce VITask, a novel framework that enhances task-specific adaptability of VLMs by integrating task-specific models (TSMs). VITask employs three key strategies: exemplar prompting (EP), response distribution alignment (RDA), and contrastive response tuning (CRT) to improve the task-specific performance of VLMs by adjusting their response distributions. EP allows TSM features to guide VLMs, while RDA enables VLMs to adapt without TSMs during inference by learning from exemplar-prompted models. CRT further optimizes the ranking of correct image-response pairs, thereby reducing the risk of generating undesired responses. Experiments on 12 medical diagnosis datasets across 9 imaging modalities show that VITask outperforms both vanilla instruction-tuned VLMs and TSMs, showcasing its ability to integrate complementary features from both models effectively. Additionally, VITask offers practical advantages such as flexible TSM integration and robustness to incomplete instructions, making it a versatile and efficient solution for task-specific VLM tuning. Our code are available at https://github.com/baiyang4/VITask.",
            "score": 25,
            "issue_id": 91,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "VITask - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫—Ä—É–ø–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM) –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∑–∞–¥–∞—á–∞–º. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –ø–æ–¥—Å–∫–∞–∑—ã–≤–∞–Ω–∏–µ, –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –æ—Ç–≤–µ—Ç–æ–≤. VITask –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á, —á—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å VLM. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ 12 –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ VITask –Ω–∞–¥ –æ–±—ã—á–Ω—ã–º–∏ VLM –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.",
                "tags": [
                    "#VITask",
                    "#–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è_–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞",
                    "#–∞–¥–∞–ø—Ç–∞—Ü–∏—è_–º–æ–¥–µ–ª–µ–π"
                ],
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#medicine",
                    "#training"
                ],
                "emoji": "üî¨",
                "title": "VITask: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ–º –∏ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö"
            },
            "hash": "b9de6611a2cfd8cb"
        },
        {
            "id": "https://huggingface.co/papers/2410.07133",
            "title": "EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models",
            "url": "https://huggingface.co/papers/2410.07133",
            "abstract": "Recent advancements in generation models have showcased remarkable capabilities in generating fantastic content. However, most of them are trained on proprietary high-quality data, and some models withhold their parameters and only provide accessible application programming interfaces (APIs), limiting their benefits for downstream tasks. To explore the feasibility of training a text-to-image generation model comparable to advanced models using publicly available resources, we introduce EvolveDirector. This framework interacts with advanced models through their public APIs to obtain text-image data pairs to train a base model. Our experiments with extensive data indicate that the model trained on generated data of the advanced model can approximate its generation capability. However, it requires large-scale samples of 10 million or more. This incurs significant expenses in time, computational resources, and especially the costs associated with calling fee-based APIs. To address this problem, we leverage pre-trained large vision-language models (VLMs) to guide the evolution of the base model. VLM continuously evaluates the base model during training and dynamically updates and refines the training dataset by the discrimination, expansion, deletion, and mutation operations. Experimental results show that this paradigm significantly reduces the required data volume. Furthermore, when approaching multiple advanced models, EvolveDirector can select the best samples generated by them to learn powerful and balanced abilities. The final trained model Edgen is demonstrated to outperform these advanced models. The code and model weights are available at https://github.com/showlab/EvolveDirector.",
            "score": 15,
            "issue_id": 90,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç EvolveDirector - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–π —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç API —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–∞—Ä —Ç–µ–∫—Å—Ç-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –î–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—Ç –¥–∞—Ç–∞—Å–µ—Ç –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —Å—Ç–∞–ª–∞ –º–æ–¥–µ–ª—å Edgen, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∞—è –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º –∏—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏.",
                "tags": [
                    "#text2image",
                    "#modelDistillation",
                    "#datasetEvolution"
                ],
                "categories": [
                    "#multimodal",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "üé®",
                "title": "–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø—É–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
            },
            "hash": "c49f4ef8183585ee"
        },
        {
            "id": "https://huggingface.co/papers/2410.08815",
            "title": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization",
            "url": "https://huggingface.co/papers/2410.08815",
            "abstract": "Retrieval-augmented generation (RAG) is a key means to effectively enhance large language models (LLMs) in many knowledge-based tasks. However, existing RAG methods struggle with knowledge-intensive reasoning tasks, because useful information required to these tasks are badly scattered. This characteristic makes it difficult for existing RAG methods to accurately identify key information and perform global reasoning with such noisy augmentation. In this paper, motivated by the cognitive theories that humans convert raw information into various structured knowledge when tackling knowledge-intensive reasoning, we proposes a new framework, StructRAG, which can identify the optimal structure type for the task at hand, reconstruct original documents into this structured format, and infer answers based on the resulting structure. Extensive experiments across various knowledge-intensive tasks show that StructRAG achieves state-of-the-art performance, particularly excelling in challenging scenarios, demonstrating its potential as an effective solution for enhancing LLMs in complex real-world applications.",
            "score": 12,
            "issue_id": 92,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "StructRAG - –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π. –û–Ω –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ RAG, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É—è –∏—Å—Ö–æ–¥–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏ –æ–±—Ä–∞–∑–æ–º. StructRAG –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ç–∏–ø —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —ç—Ç–æ—Ç —Ñ–æ—Ä–º–∞—Ç –∏ –¥–µ–ª–∞–µ—Ç –≤—ã–≤–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ StructRAG –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.",
                "tags": [
                    "#structrag",
                    "#knowledge_intensive_reasoning",
                    "#structured_knowledge"
                ],
                "categories": [
                    "#rag",
                    "#benchmark"
                ],
                "emoji": "üß†",
                "title": "StructRAG: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞–Ω–∏–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"
            },
            "hash": "0b6586df53e81f2e"
        },
        {
            "id": "https://huggingface.co/papers/2410.07035",
            "title": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness",
            "url": "https://huggingface.co/papers/2410.07035",
            "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities across various domains, including role-playing, creative writing, mathematical reasoning, and coding. Despite these advancements, LLMs still encounter challenges with length control, frequently failing to adhere to specific length constraints due to their token-level operations and insufficient training on data with strict length limitations. We identify this issue as stemming from a lack of positional awareness and propose novel approaches--PositionID Prompting and PositionID Fine-Tuning--to address it. These methods enhance the model's ability to continuously monitor and manage text length during generation. Additionally, we introduce PositionID CP Prompting to enable LLMs to perform copy and paste operations accurately. Furthermore, we develop two benchmarks for evaluating length control and copy-paste abilities. Our experiments demonstrate that our methods significantly improve the model's adherence to length constraints and copy-paste accuracy without compromising response quality.",
            "score": 11,
            "issue_id": 91,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –û–Ω–∏ –≤–≤–æ–¥—è—Ç PositionID Prompting –∏ PositionID Fine-Tuning –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π. –¢–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ PositionID CP Prompting –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤—Å—Ç–∞–≤–∫–∏. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –¥–≤–∞ –Ω–æ–≤—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–ª–∏–Ω—ã –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è-–≤—Å—Ç–∞–≤–∫–∏.",
                "tags": [
                    "#–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ_–≤–Ω–µ–¥—Ä–µ–Ω–∏–µ",
                    "#–∫–æ–Ω—Ç—Ä–æ–ª—å_–¥–ª–∏–Ω—ã_—Ç–µ–∫—Å—Ç–∞",
                    "#–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ_–≤—Å—Ç–∞–≤–∫–∞_–≤_LLM"
                ],
                "categories": [
                    "#benchmark",
                    "#rlhf"
                ],
                "emoji": "üìè",
                "title": "–¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞ –≤ LLM —Å –ø–æ–º–æ—â—å—é –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç–∏"
            },
            "hash": "20096ea1f7372f0f"
        },
        {
            "id": "https://huggingface.co/papers/2410.09008",
            "title": "SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights",
            "url": "https://huggingface.co/papers/2410.09008",
            "abstract": "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown significant improvements in various reasoning tasks. However, smaller models such as Llama-3-8B and DeepSeekMath-Base still struggle with complex mathematical reasoning because they fail to effectively identify and correct reasoning errors. Recent reflection-based methods aim to address these issues by enabling self-reflection and self-correction, but they still face challenges in independently detecting errors in their reasoning steps. To overcome these limitations, we propose SuperCorrect, a novel two-stage framework that uses a large teacher model to supervise and correct both the reasoning and reflection processes of a smaller student model. In the first stage, we extract hierarchical high-level and detailed thought templates from the teacher model to guide the student model in eliciting more fine-grained reasoning thoughts. In the second stage, we introduce cross-model collaborative direct preference optimization (DPO) to enhance the self-correction abilities of the student model by following the teacher's correction traces during training. This cross-model DPO approach teaches the student model to effectively locate and resolve erroneous thoughts with error-driven insights from the teacher model, breaking the bottleneck of its thoughts and acquiring new skills and knowledge to tackle challenging problems. Extensive experiments consistently demonstrate our superiority over previous methods. Notably, our SuperCorrect-7B model significantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models. Code: https://github.com/YangLing0818/SuperCorrect-llm",
            "score": 11,
            "issue_id": 90,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "SuperCorrect - —ç—Ç–æ –Ω–æ–≤—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–∞–ª–µ–Ω—å–∫–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª—å—à—É—é –º–æ–¥–µ–ª—å-—É—á–∏—Ç–µ–ª—è –¥–ª—è —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –º–µ–Ω—å—à–µ–π –º–æ–¥–µ–ª–∏-—É—á–µ–Ω–∏–∫–∞. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —à–∞–±–ª–æ–Ω—ã –º—ã—Å–ª–µ–π –∏–∑ –º–æ–¥–µ–ª–∏-—É—á–∏—Ç–µ–ª—è. –ù–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫—Ä–æ—Å—Å-–º–æ–¥–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä—è–º—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏-—É—á–µ–Ω–∏–∫–∞ –∫ —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏.",
                "tags": [
                    "#–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è",
                    "#–¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ–û–±—É—á–µ–Ω–∏–µ",
                    "#–∫—Ä–æ—Å—Å-–º–æ–¥–µ–ª—å–Ω–∞—è–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è"
                ],
                "categories": [
                    "#rlhf",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "üßÆ",
                "title": "SuperCorrect: –ü–æ–¥—Ö–æ–¥ '—É—á–∏—Ç–µ–ª—å-—É—á–µ–Ω–∏–∫' –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
            },
            "hash": "5c5ecb064656bbe6"
        },
        {
            "id": "https://huggingface.co/papers/2410.09009",
            "title": "Semantic Score Distillation Sampling for Compositional Text-to-3D Generation",
            "url": "https://huggingface.co/papers/2410.09009",
            "abstract": "Generating high-quality 3D assets from textual descriptions remains a pivotal challenge in computer graphics and vision research. Due to the scarcity of 3D data, state-of-the-art approaches utilize pre-trained 2D diffusion priors, optimized through Score Distillation Sampling (SDS). Despite progress, crafting complex 3D scenes featuring multiple objects or intricate interactions is still difficult. To tackle this, recent methods have incorporated box or layout guidance. However, these layout-guided compositional methods often struggle to provide fine-grained control, as they are generally coarse and lack expressiveness. To overcome these challenges, we introduce a novel SDS approach, Semantic Score Distillation Sampling (SemanticSDS), designed to effectively improve the expressiveness and accuracy of compositional text-to-3D generation. Our approach integrates new semantic embeddings that maintain consistency across different rendering views and clearly differentiate between various objects and parts. These embeddings are transformed into a semantic map, which directs a region-specific SDS process, enabling precise optimization and compositional generation. By leveraging explicit semantic guidance, our method unlocks the compositional capabilities of existing pre-trained diffusion models, thereby achieving superior quality in 3D content generation, particularly for complex objects and scenes. Experimental results demonstrate that our SemanticSDS framework is highly effective for generating state-of-the-art complex 3D content. Code: https://github.com/YangLing0818/SemanticSDS-3D",
            "score": 10,
            "issue_id": 90,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π - Semantic Score Distillation Sampling (SemanticSDS). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤. SemanticSDS —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∫–∞—Ä—Ç—É, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ 3D-—Å—Ü–µ–Ω—ã –∏ –æ–±—ä–µ–∫—Ç—ã –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.",
                "tags": [
                    "#3D-–≥–µ–Ω–µ—Ä–∞—Ü–∏—è",
                    "#—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ_—ç–º–±–µ–¥–¥–∏–Ω–≥–∏",
                    "#score_distillation_sampling"
                ],
                "categories": [
                    "#3d",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "üé®",
                "title": "SemanticSDS: –¢–æ—á–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Å–ª–æ–∂–Ω—ã—Ö 3D-—Å—Ü–µ–Ω"
            },
            "hash": "2c0887a19dd7fec9"
        },
        {
            "id": "https://huggingface.co/papers/2410.07656",
            "title": "Mechanistic Permutability: Match Features Across Layers",
            "url": "https://huggingface.co/papers/2410.07656",
            "abstract": "Understanding how features evolve across layers in deep neural networks is a fundamental challenge in mechanistic interpretability, particularly due to polysemanticity and feature superposition. While Sparse Autoencoders (SAEs) have been used to extract interpretable features from individual layers, aligning these features across layers has remained an open problem. In this paper, we introduce SAE Match, a novel, data-free method for aligning SAE features across different layers of a neural network. Our approach involves matching features by minimizing the mean squared error between the folded parameters of SAEs, a technique that incorporates activation thresholds into the encoder and decoder weights to account for differences in feature scales. Through extensive experiments on the Gemma 2 language model, we demonstrate that our method effectively captures feature evolution across layers, improving feature matching quality. We also show that features persist over several layers and that our approach can approximate hidden states across layers. Our work advances the understanding of feature dynamics in neural networks and provides a new tool for mechanistic interpretability studies.",
            "score": 8,
            "issue_id": 96,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ SAE Match –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º–∏ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞–º–∏ (SAE), –º–µ–∂–¥—É —Å–ª–æ—è–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. –ú–µ—Ç–æ–¥ –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç —Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—É—é –æ—à–∏–±–∫—É –º–µ–∂–¥—É —Å–≤–µ—Ä–Ω—É—Ç—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ SAE, —É—á–∏—Ç—ã–≤–∞—è –ø–æ—Ä–æ–≥–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Gemma 2 –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –≤ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–∏ —ç–≤–æ–ª—é—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–µ–≤.",
                "tags": [
                    "#SAEMatch",
                    "#MechanisticInterpretability",
                    "#FeatureAlignment"
                ],
                "categories": [
                    "#interpretability",
                    "#architecture"
                ],
                "emoji": "üîç",
                "title": "SAE Match: –ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ —ç–≤–æ–ª—é—Ü–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –≥–ª—É–±–æ–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö"
            },
            "hash": "6a4c86357dc6667d"
        },
        {
            "id": "https://huggingface.co/papers/2410.08102",
            "title": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining",
            "url": "https://huggingface.co/papers/2410.08102",
            "abstract": "Efficient data selection is crucial to accelerate the pretraining of large language models (LLMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these approaches to achieve optimal data selection for LLM pretraining. To tackle this problem, we propose a novel multi-agent collaborative data selection mechanism. In this framework, each data selection method serves as an independent agent, and an agent console is designed to dynamically integrate the information from all agents throughout the LLM training process. We conduct extensive empirical studies to evaluate our multi-agent framework. The experimental results demonstrate that our approach significantly improves data efficiency, accelerates convergence in LLM training, and achieves an average performance gain of 10.5% across multiple language model benchmarks compared to the state-of-the-art methods.",
            "score": 8,
            "issue_id": 93,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ö–∞–∂–¥—ã–π –º–µ—Ç–æ–¥ –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –≤—ã—Å—Ç—É–ø–∞–µ—Ç –≤ —Ä–æ–ª–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–≥–æ –∞–≥–µ–Ω—Ç–∞, –∞ –∫–æ–Ω—Å–æ–ª—å –∞–≥–µ–Ω—Ç–æ–≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è LLM. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ —É—Å–∫–æ—Ä–µ–Ω–∏–µ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ LLM. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ –ø—Ä–∏—Ä–æ—Å—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ 10.5% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.",
                "tags": [
                    "#–º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–π_–æ—Ç–±–æ—Ä_–¥–∞–Ω–Ω—ã—Ö",
                    "#—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å_–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è",
                    "#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è_LLM"
                ],
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ü§ñ",
                "title": "–ú–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM"
            },
            "hash": "e6e97c0fdfa09a15"
        },
        {
            "id": "https://huggingface.co/papers/2410.08391",
            "title": "KV Prediction for Improved Time to First Token",
            "url": "https://huggingface.co/papers/2410.08391",
            "abstract": "Inference with transformer-based language models begins with a prompt processing step. In this step, the model generates the first output token and stores the KV cache needed for future generation steps. This prompt processing step can be computationally expensive, taking 10s of seconds or more for billion-parameter models on edge devices when prompt lengths or batch sizes rise. This degrades user experience by introducing significant latency into the model's outputs. To reduce the time spent producing the first output (known as the ``time to first token'', or TTFT) of a pretrained model, we introduce a novel method called KV Prediction. In our method, a small auxiliary model is used to process the prompt and produce an approximation of the KV cache used by a base model. This approximated KV cache is then used with the base model for autoregressive generation without the need to query the auxiliary model again. We demonstrate that our method produces a pareto-optimal efficiency-accuracy trade-off when compared to baselines. On TriviaQA, we demonstrate relative accuracy improvements in the range of 15%-50% across a range of TTFT FLOPs budgets. We also demonstrate accuracy improvements of up to 30% on HumanEval python code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark models on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs translates to a TTFT speedup on hardware. We release our code at https://github.com/apple/corenet/tree/main/projects/kv-prediction .",
            "score": 7,
            "issue_id": 91,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º KV Prediction –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ KV-–∫—ç—à–∞ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—Ä–µ–º—è –¥–æ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ (TTFT). –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ 15-50% –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö TriviaQA –∏ –¥–æ 30% –Ω–∞ HumanEval –ø—Ä–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –±—é–¥–∂–µ—Ç–µ FLOPS –¥–ª—è TTFT. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ CPU Apple M2 Pro –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ TTFT –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏.",
                "tags": [
                    "#KVPrediction",
                    "#TimeToFirstToken",
                    "#EdgeInference"
                ],
                "categories": [
                    "#inference",
                    "#benchmark"
                ],
                "emoji": "‚ö°",
                "title": "KV Prediction: –º–æ–ª–Ω–∏–µ–Ω–æ—Å–Ω—ã–π —Å—Ç–∞—Ä—Ç –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"
            },
            "hash": "447ad32759dde03e"
        },
        {
            "id": "https://huggingface.co/papers/2410.08168",
            "title": "ZeroComp: Zero-shot Object Compositing from Image Intrinsics via Diffusion",
            "url": "https://huggingface.co/papers/2410.08168",
            "abstract": "We present ZeroComp, an effective zero-shot 3D object compositing approach that does not require paired composite-scene images during training. Our method leverages ControlNet to condition from intrinsic images and combines it with a Stable Diffusion model to utilize its scene priors, together operating as an effective rendering engine. During training, ZeroComp uses intrinsic images based on geometry, albedo, and masked shading, all without the need for paired images of scenes with and without composite objects. Once trained, it seamlessly integrates virtual 3D objects into scenes, adjusting shading to create realistic composites. We developed a high-quality evaluation dataset and demonstrate that ZeroComp outperforms methods using explicit lighting estimations and generative techniques in quantitative and human perception benchmarks. Additionally, ZeroComp extends to real and outdoor image compositing, even when trained solely on synthetic indoor data, showcasing its effectiveness in image compositing.",
            "score": 4,
            "issue_id": 98,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "ZeroComp ‚Äî —ç—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏–π –ø–∞—Ä–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å—Ü–µ–Ω –≤–æ –≤—Ä–µ–º—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ControlNet –¥–ª—è –æ–±—É—Å–ª–æ–≤–ª–∏–≤–∞–Ω–∏—è –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –∏ –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –µ–≥–æ —Å –º–æ–¥–µ–ª—å—é Stable Diffusion –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –µ—ë –∞–ø—Ä–∏–æ—Ä–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –æ —Å—Ü–µ–Ω–∞—Ö. ZeroComp –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –≥–µ–æ–º–µ—Ç—Ä–∏–∏, –∞–ª—å–±–µ–¥–æ –∏ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –∑–∞—Ç–µ–Ω–µ–Ω–∏–∏. –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–µ—Ç–æ–¥ –º–æ–∂–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ 3D-–æ–±—ä–µ–∫—Ç—ã –≤ —Å—Ü–µ–Ω—ã, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—è –∑–∞—Ç–µ–Ω–µ–Ω–∏–µ.",
                "tags": [
                    "#3DCompositing",
                    "#ZeroShot",
                    "#IntrinsicImages"
                ],
                "categories": [
                    "#3d",
                    "#cv",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "üé≠",
                "title": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –∫–æ–º–ø–æ–∑–∏—Ü–∏—è 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –±–µ–∑ –ø–∞—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
            },
            "hash": "8857b485f78603ed"
        },
        {
            "id": "https://huggingface.co/papers/2410.09045",
            "title": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection",
            "url": "https://huggingface.co/papers/2410.09045",
            "abstract": "The proliferation of inflammatory or misleading \"fake\" news content has become increasingly common in recent years. Simultaneously, it has become easier than ever to use AI tools to generate photorealistic images depicting any scene imaginable. Combining these two -- AI-generated fake news content -- is particularly potent and dangerous. To combat the spread of AI-generated fake news, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real and AI-generated image-caption pairs from state-of-the-art generators. We find that our dataset poses a significant challenge to humans (60% F-1) and state-of-the-art multi-modal LLMs (< 24% F-1). Using our dataset we train a multi-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art baselines on image-caption pairs from out-of-domain image generators and news publishers. We release our code and data to aid future work on detecting AI-generated content.",
            "score": 2,
            "issue_id": 101,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö MiRAGeNews, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ 12 500 –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –ø–æ–¥–ø–∏—Å–µ–π, –∫–∞–∫ —Ä–µ–∞–ª—å–Ω—ã—Ö, —Ç–∞–∫ –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ò–ò. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —ç—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∫–∞–∫ –¥–ª—è –ª—é–¥–µ–π (60% F1-–º–µ—Ä—ã), —Ç–∞–∫ –∏ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (<24% F1-–º–µ—Ä—ã). –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –±—ã–ª –æ–±—É—á–µ–Ω –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä MiRAGe, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ 5,1% –ø–æ F1-–º–µ—Ä–µ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –ø–∞—Ä–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-–ø–æ–¥–ø–∏—Å—å –∏–∑ —Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∏–∑–¥–∞—Ç–µ–ª–µ–π. –ö–æ–¥ –∏ –¥–∞–Ω–Ω—ã–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω—ã –¥–ª—è —Å–æ–¥–µ–π—Å—Ç–≤–∏—è –±—É–¥—É—â–∏–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º –ø–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ò–ò.",
                "tags": [
                    "#fake_news_detection",
                    "#image_caption_pairs",
                    "#AI_generated_content"
                ],
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "üïµÔ∏è",
                "title": "MiRAGeNews: –ë–æ—Ä—å–±–∞ —Å –¥–µ–∑–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –≤ —ç–ø–æ—Ö—É –ò–ò-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
            },
            "hash": "0fc6dec55a36d111"
        },
        {
            "id": "https://huggingface.co/papers/2410.07331",
            "title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models",
            "url": "https://huggingface.co/papers/2410.07331",
            "abstract": "We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously design the evaluation suite to ensure the accuracy and robustness of the evaluation. We develop the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at https://da-code-bench.github.io.",
            "score": 2,
            "issue_id": 95,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "DA-Code - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, —Ç—Ä–µ–±—É—é—â–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∑–∞–¥–∞—á –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏ –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–∑–¥–∞–Ω –≤ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –∏—Å–ø–æ–ª–Ω—è–µ–º–æ–π —Å—Ä–µ–¥–µ, –∏–º–∏—Ç–∏—Ä—É—é—â–µ–π —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–∞–∂–µ –ª—É—á—à–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –ª–∏—à—å 30.5% –Ω–∞ —ç—Ç–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ.",
                "tags": [
                    "#–∫–æ–¥–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è",
                    "#–∞–Ω–∞–ª–∏–∑–¥–∞–Ω–Ω—ã—Ö",
                    "#–∞–≥–µ–Ω—Ç–Ω–æ–µ–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"
                ],
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#agents"
                ],
                "emoji": "üìä",
                "title": "DA-Code: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ –ò–ò –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö"
            },
            "hash": "c2d3d40ef1bad864"
        },
        {
            "id": "https://huggingface.co/papers/2410.08612",
            "title": "Synth-SONAR: Sonar Image Synthesis with Enhanced Diversity and Realism via Dual Diffusion Models and GPT Prompting",
            "url": "https://huggingface.co/papers/2410.08612",
            "abstract": "Sonar image synthesis is crucial for advancing applications in underwater exploration, marine biology, and defence. Traditional methods often rely on extensive and costly data collection using sonar sensors, jeopardizing data quality and diversity. To overcome these limitations, this study proposes a new sonar image synthesis framework, Synth-SONAR leveraging diffusion models and GPT prompting. The key novelties of Synth-SONAR are threefold: First, by integrating Generative AI-based style injection techniques along with publicly available real/simulated data, thereby producing one of the largest sonar data corpus for sonar research. Second, a dual text-conditioning sonar diffusion model hierarchy synthesizes coarse and fine-grained sonar images with enhanced quality and diversity. Third, high-level (coarse) and low-level (detailed) text-based sonar generation methods leverage advanced semantic information available in visual language models (VLMs) and GPT-prompting. During inference, the method generates diverse and realistic sonar images from textual prompts, bridging the gap between textual descriptions and sonar image generation. This marks the application of GPT-prompting in sonar imagery for the first time, to the best of our knowledge. Synth-SONAR achieves state-of-the-art results in producing high-quality synthetic sonar datasets, significantly enhancing their diversity and realism.",
            "score": 1,
            "issue_id": 104,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Synth-SONAR –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Å–æ–Ω–∞—Ä–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ GPT-–ø—Ä–æ–º–ø—Ç–∏–Ω–≥. –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –≤–∫–ª—é—á–∞—é—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –º–µ—Ç–æ–¥–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò –¥–ª—è –∏–Ω–∂–µ–∫—Ü–∏–∏ —Å—Ç–∏–ª—è, –¥–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤—É—é –∏–µ—Ä–∞—Ä—Ö–∏—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ä–∞–∑–Ω–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ GPT-–ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π. Synth-SONAR –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Å–æ–Ω–∞—Ä–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä–≤—ã–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º GPT-–ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞ –≤ –æ–±–ª–∞—Å—Ç–∏ —Å–æ–Ω–∞—Ä–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.",
                "tags": [
                    "#—Å–æ–Ω–∞—Ä–Ω—ã–µ–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
                    "#–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ–ú–æ–¥–µ–ª–∏",
                    "#GPT–ø—Ä–æ–º–ø—Ç–∏–Ω–≥"
                ],
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "üåä",
                "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ —Å–∏–Ω—Ç–µ–∑–µ —Å–æ–Ω–∞—Ä–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –ò–ò"
            },
            "hash": "8000e8d6f78ec110"
        },
        {
            "id": "https://huggingface.co/papers/2410.09038",
            "title": "SimpleStrat: Diversifying Language Model Generation with Stratification",
            "url": "https://huggingface.co/papers/2410.09038",
            "abstract": "Generating diverse responses from large language models (LLMs) is crucial for applications such as planning/search and synthetic data generation, where diversity provides distinct answers across generations. Prior approaches rely on increasing temperature to increase diversity. However, contrary to popular belief, we show not only does this approach produce lower quality individual generations as temperature increases, but it depends on model's next-token probabilities being similar to the true distribution of answers. We propose , an alternative approach that uses the language model itself to partition the space into strata. At inference, a random stratum is selected and a sample drawn from within the strata. To measure diversity, we introduce CoverageQA, a dataset of underspecified questions with multiple equally plausible answers, and assess diversity by measuring KL Divergence between the output distribution and uniform distribution over valid ground truth answers. As computing probability per response/solution for proprietary models is infeasible, we measure recall on ground truth solutions. Our evaluation show using SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36 average reduction in KL Divergence compared to Llama 3.",
            "score": 1,
            "issue_id": 102,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º SimpleStrat. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏–º–µ–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏. SimpleStrat –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∞–º—É —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –Ω–∞ —Å—Ç—Ä–∞—Ç—ã, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –∑–∞—Ç–µ–º –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è —Å–ª—É—á–∞–π–Ω–∞—è —Å—Ç—Ä–∞—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞. –î–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –±—ã–ª —Å–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç CoverageQA —Å –Ω–µ–¥–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏, –∏–º–µ—é—â–∏–º–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–≤–Ω–æ–≤–µ—Ä–æ—è—Ç–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ SimpleStrat –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ recall –∏ –º–µ–Ω—å—à–µ–≥–æ KL-—Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.",
                "tags": [
                    "#SimpleStrat",
                    "#CoverageQA",
                    "#—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ_–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
                ],
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "üé≠",
                "title": "SimpleStrat: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"
            },
            "hash": "06e284b0650ef57d"
        },
        {
            "id": "https://huggingface.co/papers/2410.07536",
            "title": "I-Max: Maximize the Resolution Potential of Pre-trained Rectified Flow Transformers with Projected Flow",
            "url": "https://huggingface.co/papers/2410.07536",
            "abstract": "Rectified Flow Transformers (RFTs) offer superior training and inference efficiency, making them likely the most viable direction for scaling up diffusion models. However, progress in generation resolution has been relatively slow due to data quality and training costs. Tuning-free resolution extrapolation presents an alternative, but current methods often reduce generative stability, limiting practical application. In this paper, we review existing resolution extrapolation methods and introduce the I-Max framework to maximize the resolution potential of Text-to-Image RFTs. I-Max features: (i) a novel Projected Flow strategy for stable extrapolation and (ii) an advanced inference toolkit for generalizing model knowledge to higher resolutions. Experiments with Lumina-Next-2K and Flux.1-dev demonstrate I-Max's ability to enhance stability in resolution extrapolation and show that it can bring image detail emergence and artifact correction, confirming the practical value of tuning-free resolution extrapolation.",
            "score": 1,
            "issue_id": 99,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ I-Max –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö —Å –≤—ã–ø—Ä—è–º–ª–µ–Ω–Ω—ã–º –ø–æ—Ç–æ–∫–æ–º (RFT) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É. I-Max –≤–∫–ª—é—á–∞–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é Projected Flow –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ä–∏–π –¥–ª—è –≤—ã–≤–æ–¥–∞, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –æ–±–æ–±—â–∞—Ç—å –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ I-Max –ø–æ–≤—ã—à–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏ —É–ª—É—á—à–∞–µ—Ç –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é —Ü–µ–Ω–Ω–æ—Å—Ç—å —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.",
                "tags": [
                    "#RFT",
                    "#ResolutionExtrapolation",
                    "#I-Max"
                ],
                "categories": [
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "üîç",
                "title": "I-Max: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
            },
            "hash": "2d11f82cc2214114"
        },
        {
            "id": "https://huggingface.co/papers/2410.09037",
            "title": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners",
            "url": "https://huggingface.co/papers/2410.09037",
            "abstract": "Large Language Models (LLMs) have displayed remarkable performances across various complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently, studies have proposed a Knowledge Distillation (KD) approach, reasoning distillation, which transfers such reasoning ability of LLMs through fine-tuning language models of multi-step rationales generated by LLM teachers. However, they have inadequately considered two challenges regarding insufficient distillation sets from the LLM teacher model, in terms of 1) data quality and 2) soft label provision. In this paper, we propose Mentor-KD, which effectively distills the multi-step reasoning capability of LLMs to smaller LMs while addressing the aforementioned challenges. Specifically, we exploit a mentor, intermediate-sized task-specific fine-tuned model, to augment additional CoT annotations and provide soft labels for the student model during reasoning distillation. We conduct extensive experiments and confirm Mentor-KD's effectiveness across various models and complex reasoning tasks.",
            "score": 1,
            "issue_id": 97,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Mentor-KD –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –æ—Ç –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –º–µ–Ω—å—à–∏–º –º–æ–¥–µ–ª—è–º. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é –º–æ–¥–µ–ª—å-–Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–∞ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –Ω–∞–±–æ—Ä–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π Chain-of-Thought –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –º—è–≥–∫–∏—Ö –º–µ—Ç–æ–∫ –¥–ª—è –º–æ–¥–µ–ª–∏-—É—á–µ–Ω–∏–∫–∞. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∫–∞—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º –º—è–≥–∫–∏—Ö –º–µ—Ç–æ–∫ –ø—Ä–∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å Mentor-KD –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.",
                "tags": [
                    "#knowledge_distillation",
                    "#chain_of_thought",
                    "#reasoning_distillation"
                ],
                "categories": [
                    "#training",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "title": "Mentor-KD: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ –Ω–∞–≤—ã–∫–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –æ—Ç –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–µ–Ω—å—à–∏–º"
            },
            "hash": "2a269c59131978ed"
        },
        {
            "id": "https://huggingface.co/papers/2410.08193",
            "title": "GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment",
            "url": "https://huggingface.co/papers/2410.08193",
            "abstract": "Large Language Models (LLMs) exhibit impressive capabilities but require careful alignment with human preferences. Traditional training-time methods finetune LLMs using human preference datasets but incur significant training costs and require repeated training to handle diverse user preferences. Test-time alignment methods address this by using reward models (RMs) to guide frozen LLMs without retraining. However, existing test-time approaches rely on trajectory-level RMs which are designed to evaluate complete responses, making them unsuitable for autoregressive text generation that requires computing next-token rewards from partial responses. To address this, we introduce GenARM, a test-time alignment approach that leverages the Autoregressive Reward Model--a novel reward parametrization designed to predict next-token rewards for efficient and effective autoregressive generation. Theoretically, we demonstrate that this parametrization can provably guide frozen LLMs toward any distribution achievable by traditional RMs within the KL-regularized reinforcement learning framework. Experimental results show that GenARM significantly outperforms prior test-time alignment baselines and matches the performance of training-time methods. Additionally, GenARM enables efficient weak-to-strong guidance, aligning larger LLMs with smaller RMs without the high costs of training larger models. Furthermore, GenARM supports multi-objective alignment, allowing real-time trade-offs between preference dimensions and catering to diverse user preferences without retraining.",
            "score": 0,
            "issue_id": 102,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 –æ–∫—Ç—è–±—Ä—è",
            "data": {
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç GenARM - –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –Ω–∞–ø—Ä–∞–≤–ª—è—Ç—å –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ LLM –∫ –∂–µ–ª–∞–µ–º–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ GenARM –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è. –ü–æ–¥—Ö–æ–¥ —Ç–∞–∫–∂–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –æ—Ç —Å–ª–∞–±–æ–≥–æ –∫ —Å–∏–ª—å–Ω–æ–º—É –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–ª—è —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.",
                "tags": [
                    "#–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è_–º–æ–¥–µ–ª—å",
                    "#–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è_–º–æ–¥–µ–ª—å",
                    "#–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ_LLM"
                ],
                "categories": [
                    "#rlhf",
                    "#alignment"
                ],
                "emoji": "üéØ",
                "title": "GenARM: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ç–µ—Å—Ç–æ–≤–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ LLM –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"
            },
            "hash": "ebce9bee397ae5dd"
        }
    ],
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫ÜBaichuan-OmniÔºå‰∏Ä‰∏™ÂºÄÊ∫êÁöÑ7BÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜÂíåÂàÜÊûêÂõæÂÉè„ÄÅËßÜÈ¢ë„ÄÅÈü≥È¢ëÂíåÊñáÊú¨ÔºåÊèê‰æõÂÖàËøõÁöÑÂ§öÊ®°ÊÄÅ‰∫§‰∫í‰ΩìÈ™åÂíåÂº∫Â§ßÊÄßËÉΩ„ÄÇÊñáÁ´†ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊúâÊïàÁöÑÂ§öÊ®°ÊÄÅËÆ≠ÁªÉÊñπÊ°àÔºåÈÄöËøáÂ§öÊ®°ÊÄÅÂØπÈΩêÂíåÂ§ö‰ªªÂä°ÂæÆË∞É‰∏§‰∏™Èò∂ÊÆµÔºå‰ΩøËØ≠Ë®ÄÊ®°ÂûãËÉΩÂ§üÊúâÊïàÂ§ÑÁêÜËßÜËßâÂíåÈü≥È¢ëÊï∞ÊçÆÔºåÂπ∂Âú®ÂêÑÁßçÂÖ®Ê®°ÊÄÅÂíåÂ§öÊ®°ÊÄÅÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ",
        "pinyin": "zh√® piƒÅn w√©n zhƒÅng ji√® sh√†o le Baichuan-Omni, yƒ´ g√® kƒÅi yu√°n de 7B du≈ç m√≥ t√†i d√† y«î y√°n m√≥ x√≠ng, n√©ng g√≤u t√≥ng sh√≠ ch«î l«ê h√© fƒìn xƒ´ t√∫ xi√†ng, sh√¨ p«ên, yƒ´n p«ên h√© w√©n bƒõn, t√≠ g≈çng xiƒÅn j√¨n de du≈ç m√≥ t√†i jiƒÅo h√π t«ê y√†n h√© qi√°ng d√† x√≠ng n√©ng. w√©n zhƒÅng t√≠ ch≈´ le yƒ´ zh«íng y«íu xi√†o de du≈ç m√≥ t√†i x√πn li√†n fƒÅng √†n, t≈çng gu√≤ du≈ç m√≥ t√†i du√¨ q√≠ h√© du≈ç r√®n w«î ti√°o li«éng √®r g√® jiƒì du√†n, sh«ê y«î y√°n m√≥ x√≠ng n√©ng g√≤u y«íu xi√†o ch«î l«ê sh√¨ ju√© h√© yƒ´n p«ên sh√π j√π, b√¨ng z√†i g√® zh«íng qu√°n m√≥ t√†i h√© du≈ç m√≥ t√†i bƒõn zh«în c√® sh√¨ zh≈çng bi«éo xi√†n ch≈´ s√®.",
        "vocab": "[\n    {\"word\": \"Â§öÊ®°ÊÄÅ\", \"pinyin\": \"du≈ç m√≥ shu√†i\", \"trans\": \"multimodal\"},\n    {\"word\": \"Â§ßËØ≠Ë®ÄÊ®°Âûã\", \"pinyin\": \"d√† y«îy√°n m√≥x√≠ng\", \"trans\": \"large language model\"},\n    {\"word\": \"Â§ÑÁêÜ\", \"pinyin\": \"ch«îl«ê\", \"trans\": \"process\"},\n    {\"word\": \"ÂàÜÊûê\", \"pinyin\": \"fƒìnxƒ´\", \"trans\": \"analyze\"},\n    {\"word\": \"ÂÖàËøõ\", \"pinyin\": \"xiƒÅnj√¨n\", \"trans\": \"advanced\"},\n    {\"word\": \"‰∫§‰∫í\", \"pinyin\": \"jiƒÅoh√π\", \"trans\": \"interaction\"},\n    {\"word\": \"‰ΩìÈ™å\", \"pinyin\": \"t«êy√†n\", \"trans\": \"experience\"},\n    {\"word\": \"ÊÄßËÉΩ\", \"pinyin\": \"x√¨ngn√©ng\", \"trans\": \"performance\"},\n    {\"word\": \"ÊèêÂá∫\", \"pinyin\": \"t√≠ch≈´\", \"trans\": \"propose\"},\n    {\"word\": \"ÊúâÊïà\", \"pinyin\": \"y«íuxi√†o\", \"trans\": \"effective\"},\n    {\"word\": \"ËÆ≠ÁªÉ\", \"pinyin\": \"x√πnli√†n\", \"trans\": \"training\"},\n    {\"word\": \"ÊñπÊ°à\", \"pinyin\": \"fƒÅng'√†n\", \"trans\": \"scheme\"},\n    {\"word\": \"ÂØπÈΩê\", \"pinyin\": \"du√¨q√≠\", \"trans\": \"alignment\"},\n    {\"word\": \"ÂæÆË∞É\", \"pinyin\": \"wƒìiti√°o\", \"trans\": \"fine-tuning\"},\n    {\"word\": \"Èò∂ÊÆµ\", \"pinyin\": \"jiƒìdu√†n\", \"trans\": \"stage\"},\n    {\"word\": \"ËßÜËßâ\", \"pinyin\": \"sh√¨ju√©\", \"trans\": \"visual\"},\n    {\"word\": \"Èü≥È¢ë\", \"pinyin\": \"yƒ´np√≠n\", \"trans\": \"audio\"},\n    {\"word\": \"Êï∞ÊçÆ\", \"pinyin\": \"sh√πj√π\", \"trans\": \"data\"},"
    },
    "weekday": 4,
    "link_prev": "2024-10-10.html",
    "link_next": "2024-10-14.html",
    "date_en": "11 October",
    "date_prev": "10 –æ–∫—Ç—è–±—Ä—è",
    "date_next": "14 –æ–∫—Ç—è–±—Ä—è",
    "short_date_prev": "10.10",
    "short_date_next": "14.10"
}