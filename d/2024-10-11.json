{
    "date": {
        "ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 11",
        "zh": "10æœˆ11æ—¥"
    },
    "time_utc": "2024-10-14 20:13",
    "issue_id": 104,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.08565",
            "title": "Baichuan-Omni Technical Report",
            "url": "https://huggingface.co/papers/2410.08565",
            "abstract": "The salient multimodal capabilities and interactive experience of GPT-4o highlight its critical role in practical applications, yet it lacks a high-performing open-source counterpart. In this paper, we introduce Baichuan-Omni, the first open-source 7B Multimodal Large Language Model (MLLM) adept at concurrently processing and analyzing modalities of image, video, audio, and text, while delivering an advanced multimodal interactive experience and strong performance. We propose an effective multimodal training schema starting with 7B model and proceeding through two stages of multimodal alignment and multitask fine-tuning across audio, image, video, and text modal. This approach equips the language model with the ability to handle visual and audio data effectively. Demonstrating strong performance across various omni-modal and multimodal benchmarks, we aim for this contribution to serve as a competitive baseline for the open-source community in advancing multimodal understanding and real-time interaction.",
            "score": 59,
            "issue_id": 90,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#audio",
                    "#benchmark",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Baichuan-Omni: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Baichuan-Omni - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ°Ñ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°. Baichuan-Omni Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Baichuan-Omni: Bridging Multimodal Gaps in Open-Source AI",
                    "desc": "The paper introduces Baichuan-Omni, an open-source 7 billion parameter Multimodal Large Language Model (MLLM) capable of processing images, videos, audio, and text simultaneously. It employs a two-stage training process involving multimodal alignment and multitask fine-tuning to enhance its ability to handle diverse data types. The model demonstrates strong performance across various benchmarks, showcasing its potential as a competitive tool for multimodal understanding. This work aims to provide a robust baseline for the open-source community to further develop real-time multimodal interaction capabilities."
                },
                "zh": {
                    "title": "Baichuan-Omniï¼šå¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å…ˆé”‹",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†Baichuan-Omniï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¼€æºçš„7Bå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬ã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„å¤šæ¨¡æ€å¯¹é½å’Œå¤šä»»åŠ¡å¾®è°ƒè®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ¨¡å‹åœ¨å¤šæ¨¡æ€äº¤äº’ä½“éªŒå’Œæ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ã€‚Baichuan-Omniå±•ç¤ºäº†åœ¨å¤šç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­çš„å¼ºå¤§æ€§èƒ½ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸€è´¡çŒ®èƒ½æˆä¸ºå¼€æºç¤¾åŒºåœ¨å¤šæ¨¡æ€ç†è§£å’Œå®æ—¶äº¤äº’æ–¹é¢çš„ç«äº‰åŸºå‡†ã€‚"
                }
            },
            "hash": "635ebaf87323c35f",
            "pub_date_card": {
                "ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 11",
                "zh": "10æœˆ11æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08261",
            "title": "Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis",
            "url": "https://huggingface.co/papers/2410.08261",
            "abstract": "Diffusion models, such as Stable Diffusion, have made significant strides in visual generation, yet their paradigm remains fundamentally different from autoregressive language models, complicating the development of unified language-vision models. Recent efforts like LlamaGen have attempted autoregressive image generation using discrete VQVAE tokens, but the large number of tokens involved renders this approach inefficient and slow. In this work, we present Meissonic, which elevates non-autoregressive masked image modeling (MIM) text-to-image to a level comparable with state-of-the-art diffusion models like SDXL. By incorporating a comprehensive suite of architectural innovations, advanced positional encoding strategies, and optimized sampling conditions, Meissonic substantially improves MIM's performance and efficiency. Additionally, we leverage high-quality training data, integrate micro-conditions informed by human preference scores, and employ feature compression layers to further enhance image fidelity and resolution. Our model not only matches but often exceeds the performance of existing models like SDXL in generating high-quality, high-resolution images. Extensive experiments validate Meissonic's capabilities, demonstrating its potential as a new standard in text-to-image synthesis. We release a model checkpoint capable of producing 1024 times 1024 resolution images.",
            "score": 35,
            "issue_id": 91,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Meissonic: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ MIM",
                    "desc": "Meissonic - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ masked image modeling (MIM). ĞĞ½Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ MIM. Meissonic Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¼Ğ¸ĞºÑ€Ğ¾-ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ ÑĞ»Ğ¾Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ SDXL."
                },
                "en": {
                    "title": "Meissonic: Redefining Text-to-Image Synthesis with Efficiency and Quality",
                    "desc": "The paper introduces Meissonic, a new approach to text-to-image generation that improves upon existing diffusion models like Stable Diffusion. Unlike autoregressive models, Meissonic uses non-autoregressive masked image modeling, which is more efficient and faster. The model incorporates advanced architectural designs, positional encoding, and optimized sampling to enhance image quality and resolution. Extensive testing shows that Meissonic not only matches but often surpasses current state-of-the-art models in generating high-resolution images."
                },
                "zh": {
                    "title": "Meissonicï¼šå¼•é¢†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ–°æ ‡å‡†",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMeissonicçš„æ–°æ¨¡å‹ï¼Œå®ƒåœ¨éè‡ªå›å½’çš„æ©ç å›¾åƒå»ºæ¨¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚é€šè¿‡å¼•å…¥ä¸€ç³»åˆ—æ¶æ„åˆ›æ–°ã€å…ˆè¿›çš„ä½ç½®ç¼–ç ç­–ç•¥å’Œä¼˜åŒ–çš„é‡‡æ ·æ¡ä»¶ï¼ŒMeissonicå¤§å¤§æé«˜äº†æ€§èƒ½å’Œæ•ˆç‡ã€‚è¯¥æ¨¡å‹åˆ©ç”¨é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®å’Œäººç±»åå¥½è¯„åˆ†çš„å¾®æ¡ä»¶ï¼Œç»“åˆç‰¹å¾å‹ç¼©å±‚ï¼Œæå‡äº†å›¾åƒçš„æ¸…æ™°åº¦å’Œåˆ†è¾¨ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMeissonicåœ¨ç”Ÿæˆé«˜è´¨é‡ã€é«˜åˆ†è¾¨ç‡å›¾åƒæ–¹é¢ä¸ä»…èƒ½ä¸ç°æœ‰æ¨¡å‹åª²ç¾ï¼Œç”šè‡³åœ¨æŸäº›æ–¹é¢è¶…è¶Šäº†å®ƒä»¬ã€‚"
                }
            },
            "hash": "3b6012d644e53308",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.06456",
            "title": "From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning",
            "url": "https://huggingface.co/papers/2410.06456",
            "abstract": "Large vision language models (VLMs) combine large language models with vision encoders, demonstrating promise across various tasks. However, they often underperform in task-specific applications due to domain gaps between pre-training and fine-tuning. We introduce VITask, a novel framework that enhances task-specific adaptability of VLMs by integrating task-specific models (TSMs). VITask employs three key strategies: exemplar prompting (EP), response distribution alignment (RDA), and contrastive response tuning (CRT) to improve the task-specific performance of VLMs by adjusting their response distributions. EP allows TSM features to guide VLMs, while RDA enables VLMs to adapt without TSMs during inference by learning from exemplar-prompted models. CRT further optimizes the ranking of correct image-response pairs, thereby reducing the risk of generating undesired responses. Experiments on 12 medical diagnosis datasets across 9 imaging modalities show that VITask outperforms both vanilla instruction-tuned VLMs and TSMs, showcasing its ability to integrate complementary features from both models effectively. Additionally, VITask offers practical advantages such as flexible TSM integration and robustness to incomplete instructions, making it a versatile and efficient solution for task-specific VLM tuning. Our code are available at https://github.com/baiyang4/VITask.",
            "score": 25,
            "issue_id": 91,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#cv",
                    "#medicine",
                    "#multimodal"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "VITask: ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "VITask - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸: Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². VITask Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ VLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 12 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ VITask Ğ½Ğ°Ğ´ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ VLM Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "\"VITask: Bridging the Gap for Task-Specific Vision Language Models\"",
                    "desc": "The paper introduces VITask, a framework designed to improve the task-specific performance of large vision language models (VLMs) by integrating task-specific models (TSMs). VITask uses exemplar prompting, response distribution alignment, and contrastive response tuning to enhance adaptability and accuracy in specific tasks. These strategies help VLMs learn from task-specific features and improve their response accuracy without relying on TSMs during inference. Experiments demonstrate that VITask outperforms traditional VLMs and TSMs, especially in medical diagnosis tasks, by effectively combining features from both models."
                },
                "zh": {
                    "title": "VITaskï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹ä»»åŠ¡é€‚åº”æ€§çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVITaskçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„é€‚åº”æ€§ã€‚VITaské€šè¿‡æ•´åˆä»»åŠ¡ç‰¹å®šæ¨¡å‹ï¼ˆTSMsï¼‰æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œå¹¶é‡‡ç”¨äº†ç¤ºä¾‹æç¤ºã€å“åº”åˆ†å¸ƒå¯¹é½å’Œå¯¹æ¯”å“åº”è°ƒä¼˜ä¸‰ç§ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒVITaskåœ¨12ä¸ªåŒ»å­¦è¯Šæ–­æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç»“åˆVLMså’ŒTSMsçš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒVITaskè¿˜å…·æœ‰çµæ´»çš„TSMé›†æˆå’Œå¯¹ä¸å®Œæ•´æŒ‡ä»¤çš„é²æ£’æ€§ï¼Œæˆä¸ºä»»åŠ¡ç‰¹å®šVLMè°ƒä¼˜çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚"
                }
            },
            "hash": "b9de6611a2cfd8cb",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07133",
            "title": "EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models",
            "url": "https://huggingface.co/papers/2410.07133",
            "abstract": "Recent advancements in generation models have showcased remarkable capabilities in generating fantastic content. However, most of them are trained on proprietary high-quality data, and some models withhold their parameters and only provide accessible application programming interfaces (APIs), limiting their benefits for downstream tasks. To explore the feasibility of training a text-to-image generation model comparable to advanced models using publicly available resources, we introduce EvolveDirector. This framework interacts with advanced models through their public APIs to obtain text-image data pairs to train a base model. Our experiments with extensive data indicate that the model trained on generated data of the advanced model can approximate its generation capability. However, it requires large-scale samples of 10 million or more. This incurs significant expenses in time, computational resources, and especially the costs associated with calling fee-based APIs. To address this problem, we leverage pre-trained large vision-language models (VLMs) to guide the evolution of the base model. VLM continuously evaluates the base model during training and dynamically updates and refines the training dataset by the discrimination, expansion, deletion, and mutation operations. Experimental results show that this paradigm significantly reduces the required data volume. Furthermore, when approaching multiple advanced models, EvolveDirector can select the best samples generated by them to learn powerful and balanced abilities. The final trained model Edgen is demonstrated to outperform these advanced models. The code and model weights are available at https://github.com/showlab/EvolveDirector.",
            "score": 15,
            "issue_id": 90,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EvolveDirector - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ±Ñ‰ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ API ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ”Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Edgen, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ°Ñ Ğ¿Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "EvolveDirector: Democratizing Advanced Text-to-Image Generation",
                    "desc": "The paper introduces EvolveDirector, a framework designed to train a text-to-image generation model using publicly available resources by interacting with advanced models through their APIs. It addresses the challenge of high costs and resource demands by employing pre-trained vision-language models to guide the training process, reducing the need for large datasets. The framework dynamically refines the training data through operations like discrimination and mutation, enhancing the model's learning efficiency. The resulting model, Edgen, not only approximates but also surpasses the capabilities of existing advanced models, demonstrating the potential of this innovative approach."
                },
                "zh": {
                    "title": "EvolveDirectorï¼šç”¨å…¬å…±èµ„æºå®ç°é«˜çº§ç”Ÿæˆèƒ½åŠ›",
                    "desc": "EvolveDirector æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡ä¸é«˜çº§æ¨¡å‹çš„å…¬å…± API äº¤äº’ï¼Œè·å–æ–‡æœ¬-å›¾åƒæ•°æ®å¯¹æ¥è®­ç»ƒåŸºç¡€æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨é«˜çº§æ¨¡å‹ç”Ÿæˆçš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹å¯ä»¥æ¥è¿‘å…¶ç”Ÿæˆèƒ½åŠ›ï¼Œä½†éœ€è¦å¤§é‡æ ·æœ¬ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥æŒ‡å¯¼åŸºç¡€æ¨¡å‹çš„æ¼”åŒ–ï¼Œæ˜¾è‘—å‡å°‘æ‰€éœ€æ•°æ®é‡ã€‚æœ€ç»ˆè®­ç»ƒçš„æ¨¡å‹ Edgen è¶…è¶Šäº†è¿™äº›é«˜çº§æ¨¡å‹ã€‚"
                }
            },
            "hash": "c49f4ef8183585ee",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08815",
            "title": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization",
            "url": "https://huggingface.co/papers/2410.08815",
            "abstract": "Retrieval-augmented generation (RAG) is a key means to effectively enhance large language models (LLMs) in many knowledge-based tasks. However, existing RAG methods struggle with knowledge-intensive reasoning tasks, because useful information required to these tasks are badly scattered. This characteristic makes it difficult for existing RAG methods to accurately identify key information and perform global reasoning with such noisy augmentation. In this paper, motivated by the cognitive theories that humans convert raw information into various structured knowledge when tackling knowledge-intensive reasoning, we proposes a new framework, StructRAG, which can identify the optimal structure type for the task at hand, reconstruct original documents into this structured format, and infer answers based on the resulting structure. Extensive experiments across various knowledge-intensive tasks show that StructRAG achieves state-of-the-art performance, particularly excelling in challenging scenarios, demonstrating its potential as an effective solution for enhancing LLMs in complex real-world applications.",
            "score": 12,
            "issue_id": 92,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rag"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "StructRAG: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "StructRAG - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ½ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² RAG, ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€ÑƒÑ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼. StructRAG Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¸Ğ¿ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ² ÑÑ‚Ğ¾Ñ‚ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¸ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ StructRAG Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "StructRAG: Structuring Knowledge for Superior Reasoning",
                    "desc": "The paper introduces StructRAG, a novel framework designed to improve retrieval-augmented generation (RAG) methods for large language models (LLMs) in knowledge-intensive reasoning tasks. Traditional RAG methods often struggle because the necessary information is scattered and noisy, making it hard to perform accurate reasoning. StructRAG addresses this by converting raw information into structured knowledge, allowing for better identification and reasoning of key information. Experiments show that StructRAG significantly enhances performance in complex tasks, setting a new standard for LLMs in real-world applications."
                },
                "zh": {
                    "title": "StructRAGï¼šç»“æ„åŒ–çŸ¥è¯†æå‡å¤§è¯­è¨€æ¨¡å‹",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºStructRAGçš„æ–°æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›å¤§è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç°æœ‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•åœ¨å¤„ç†è¿™äº›ä»»åŠ¡æ—¶å¸¸å¸¸éš¾ä»¥å‡†ç¡®è¯†åˆ«å…³é”®ä¿¡æ¯ï¼Œå› ä¸ºä¿¡æ¯åˆ†æ•£ä¸”å™ªå£°è¾ƒå¤šã€‚StructRAGé€šè¿‡å°†åŸå§‹ä¿¡æ¯è½¬æ¢ä¸ºç»“æ„åŒ–çŸ¥è¯†ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°è¿›è¡Œå…¨å±€æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒStructRAGåœ¨å¤šç§å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å±•ç°äº†å…¶æ½œåŠ›ã€‚"
                }
            },
            "hash": "0b6586df53e81f2e",
            "pub_date_card": {
                "ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 11",
                "zh": "10æœˆ11æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07035",
            "title": "PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness",
            "url": "https://huggingface.co/papers/2410.07035",
            "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities across various domains, including role-playing, creative writing, mathematical reasoning, and coding. Despite these advancements, LLMs still encounter challenges with length control, frequently failing to adhere to specific length constraints due to their token-level operations and insufficient training on data with strict length limitations. We identify this issue as stemming from a lack of positional awareness and propose novel approaches--PositionID Prompting and PositionID Fine-Tuning--to address it. These methods enhance the model's ability to continuously monitor and manage text length during generation. Additionally, we introduce PositionID CP Prompting to enable LLMs to perform copy and paste operations accurately. Furthermore, we develop two benchmarks for evaluating length control and copy-paste abilities. Our experiments demonstrate that our methods significantly improve the model's adherence to length constraints and copy-paste accuracy without compromising response quality.",
            "score": 11,
            "issue_id": 91,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#long_context",
                    "#rlhf"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ PositionID Prompting Ğ¸ PositionID Fine-Tuning Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ PositionID CP Prompting Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ-Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸."
                },
                "en": {
                    "title": "Mastering Length: Enhancing LLMs with PositionID Techniques",
                    "desc": "Large Language Models (LLMs) are powerful but struggle with controlling the length of their outputs due to token-level operations and insufficient training on length-specific data. This paper identifies the problem as a lack of positional awareness and introduces PositionID Prompting and PositionID Fine-Tuning to improve length management. Additionally, PositionID CP Prompting is developed to enhance copy-paste accuracy. The proposed methods are tested with new benchmarks, showing significant improvements in length control and copy-paste tasks without affecting the quality of responses."
                },
                "zh": {
                    "title": "æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„é•¿åº¦æ§åˆ¶èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®¸å¤šé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ§åˆ¶æ–‡æœ¬é•¿åº¦æ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‘ç°é—®é¢˜çš„æ ¹æºåœ¨äºæ¨¡å‹ç¼ºä¹ä½ç½®æ„ŸçŸ¥èƒ½åŠ›ï¼Œå› æ­¤æå‡ºäº†PositionIDæç¤ºå’Œå¾®è°ƒæ–¹æ³•æ¥è§£å†³ã€‚é€šè¿‡è¿™äº›æ–¹æ³•ï¼Œæ¨¡å‹å¯ä»¥æ›´å¥½åœ°ç›‘æ§å’Œç®¡ç†ç”Ÿæˆæ–‡æœ¬çš„é•¿åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†PositionID CPæç¤ºæ¥æé«˜å¤åˆ¶ç²˜è´´æ“ä½œçš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨é•¿åº¦æ§åˆ¶å’Œå¤åˆ¶ç²˜è´´æ–¹é¢çš„è¡¨ç°ã€‚"
                }
            },
            "hash": "20096ea1f7372f0f",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09008",
            "title": "SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights",
            "url": "https://huggingface.co/papers/2410.09008",
            "abstract": "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown significant improvements in various reasoning tasks. However, smaller models such as Llama-3-8B and DeepSeekMath-Base still struggle with complex mathematical reasoning because they fail to effectively identify and correct reasoning errors. Recent reflection-based methods aim to address these issues by enabling self-reflection and self-correction, but they still face challenges in independently detecting errors in their reasoning steps. To overcome these limitations, we propose SuperCorrect, a novel two-stage framework that uses a large teacher model to supervise and correct both the reasoning and reflection processes of a smaller student model. In the first stage, we extract hierarchical high-level and detailed thought templates from the teacher model to guide the student model in eliciting more fine-grained reasoning thoughts. In the second stage, we introduce cross-model collaborative direct preference optimization (DPO) to enhance the self-correction abilities of the student model by following the teacher's correction traces during training. This cross-model DPO approach teaches the student model to effectively locate and resolve erroneous thoughts with error-driven insights from the teacher model, breaking the bottleneck of its thoughts and acquiring new skills and knowledge to tackle challenging problems. Extensive experiments consistently demonstrate our superiority over previous methods. Notably, our SuperCorrect-7B model significantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models. Code: https://github.com/YangLing0818/SuperCorrect-llm",
            "score": 11,
            "issue_id": 90,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#rlhf"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "SuperCorrect: ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ 'ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ-ÑƒÑ‡ĞµĞ½Ğ¸Ğº' Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "SuperCorrect - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ÑÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ° Ğº ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "SuperCorrect: Elevating Small Models with Big Guidance",
                    "desc": "The paper introduces SuperCorrect, a two-stage framework designed to improve the reasoning and self-correction abilities of smaller language models by using a large teacher model for guidance. In the first stage, the teacher model provides hierarchical templates to help the student model develop more detailed reasoning processes. The second stage involves cross-model collaborative direct preference optimization, where the student model learns to identify and correct errors by following the teacher's correction patterns. This approach significantly enhances the performance of smaller models, as demonstrated by the SuperCorrect-7B model's superior results on mathematical reasoning benchmarks compared to other models of similar size."
                },
                "zh": {
                    "title": "SuperCorrectï¼šå°æ¨¡å‹çš„æ¨ç†ä¸çº é”™æ–°çªç ´",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å°å‹æ¨¡å‹åœ¨å¤æ‚æ•°å­¦æ¨ç†ä¸Šä»æœ‰å›°éš¾ã€‚åæ€æ–¹æ³•è¯•å›¾è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†åœ¨ç‹¬ç«‹æ£€æµ‹æ¨ç†é”™è¯¯ä¸Šä»æœ‰æŒ‘æˆ˜ã€‚SuperCorrectæ¡†æ¶é€šè¿‡å¤§æ¨¡å‹æŒ‡å¯¼å°æ¨¡å‹ï¼Œæå‡å…¶æ¨ç†å’Œè‡ªæˆ‘çº æ­£èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒSuperCorrectåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ã€‚"
                }
            },
            "hash": "5c5ecb064656bbe6",
            "pub_date_card": {
                "ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 11",
                "zh": "10æœˆ11æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09009",
            "title": "Semantic Score Distillation Sampling for Compositional Text-to-3D Generation",
            "url": "https://huggingface.co/papers/2410.09009",
            "abstract": "Generating high-quality 3D assets from textual descriptions remains a pivotal challenge in computer graphics and vision research. Due to the scarcity of 3D data, state-of-the-art approaches utilize pre-trained 2D diffusion priors, optimized through Score Distillation Sampling (SDS). Despite progress, crafting complex 3D scenes featuring multiple objects or intricate interactions is still difficult. To tackle this, recent methods have incorporated box or layout guidance. However, these layout-guided compositional methods often struggle to provide fine-grained control, as they are generally coarse and lack expressiveness. To overcome these challenges, we introduce a novel SDS approach, Semantic Score Distillation Sampling (SemanticSDS), designed to effectively improve the expressiveness and accuracy of compositional text-to-3D generation. Our approach integrates new semantic embeddings that maintain consistency across different rendering views and clearly differentiate between various objects and parts. These embeddings are transformed into a semantic map, which directs a region-specific SDS process, enabling precise optimization and compositional generation. By leveraging explicit semantic guidance, our method unlocks the compositional capabilities of existing pre-trained diffusion models, thereby achieving superior quality in 3D content generation, particularly for complex objects and scenes. Experimental results demonstrate that our SemanticSDS framework is highly effective for generating state-of-the-art complex 3D content. Code: https://github.com/YangLing0818/SemanticSDS-3D",
            "score": 10,
            "issue_id": 90,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "SemanticSDS: Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ - Semantic Score Distillation Sampling (SemanticSDS). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². SemanticSDS Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ 3D-ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Unlocking 3D Creativity: SemanticSDS for Detailed Text-to-3D Generation",
                    "desc": "The paper addresses the challenge of generating high-quality 3D assets from text descriptions by introducing a new method called Semantic Score Distillation Sampling (SemanticSDS). This approach enhances the expressiveness and accuracy of text-to-3D generation by using semantic embeddings that ensure consistency across different views and distinguish between various objects. These embeddings are used to create a semantic map that guides a region-specific optimization process, allowing for precise and detailed 3D scene generation. The method significantly improves the quality of 3D content, especially for complex scenes, by leveraging existing pre-trained diffusion models with explicit semantic guidance."
                },
                "zh": {
                    "title": "è¯­ä¹‰å¼•å¯¼ï¼Œæå‡3Dç”Ÿæˆç²¾åº¦",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•ä»æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡çš„3Dèµ„äº§ï¼Œè¿™æ˜¯è®¡ç®—æœºå›¾å½¢å­¦å’Œè§†è§‰ç ”ç©¶ä¸­çš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚ç”±äº3Dæ•°æ®ç¨€ç¼ºï¼Œç°æœ‰æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„2Dæ‰©æ•£å…ˆéªŒï¼Œé€šè¿‡å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰è¿›è¡Œä¼˜åŒ–ã€‚ä¸ºäº†æé«˜å¤æ‚3Dåœºæ™¯çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„è¯­ä¹‰å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSemanticSDSï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥æ–°çš„è¯­ä¹‰åµŒå…¥ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒæ¸²æŸ“è§†å›¾ä¸­ä¿æŒä¸€è‡´æ€§ï¼Œå¹¶æ¸…æ™°åŒºåˆ†ä¸åŒå¯¹è±¡å’Œéƒ¨åˆ†ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„ä¼˜åŒ–å’Œç»„åˆç”Ÿæˆã€‚"
                }
            },
            "hash": "2c0887a19dd7fec9",
            "pub_date_card": {
                "ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 11",
                "zh": "10æœˆ11æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07656",
            "title": "Mechanistic Permutability: Match Features Across Layers",
            "url": "https://huggingface.co/papers/2410.07656",
            "abstract": "Understanding how features evolve across layers in deep neural networks is a fundamental challenge in mechanistic interpretability, particularly due to polysemanticity and feature superposition. While Sparse Autoencoders (SAEs) have been used to extract interpretable features from individual layers, aligning these features across layers has remained an open problem. In this paper, we introduce SAE Match, a novel, data-free method for aligning SAE features across different layers of a neural network. Our approach involves matching features by minimizing the mean squared error between the folded parameters of SAEs, a technique that incorporates activation thresholds into the encoder and decoder weights to account for differences in feature scales. Through extensive experiments on the Gemma 2 language model, we demonstrate that our method effectively captures feature evolution across layers, improving feature matching quality. We also show that features persist over several layers and that our approach can approximate hidden states across layers. Our work advances the understanding of feature dynamics in neural networks and provides a new tool for mechanistic interpretability studies.",
            "score": 8,
            "issue_id": 96,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#architecture",
                    "#interpretability"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "SAE Match: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SAE Match Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸ (SAE), Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾ÑĞ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ²ĞµÑ€Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ SAE, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Gemma 2 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ»Ğ¾ĞµĞ²."
                },
                "en": {
                    "title": "Aligning Features Across Layers: A New Approach to Neural Network Interpretability",
                    "desc": "This paper introduces SAE Match, a new method to align features across layers in deep neural networks using Sparse Autoencoders. The technique minimizes the mean squared error between folded parameters, incorporating activation thresholds to handle different feature scales. Experiments on the Gemma 2 language model show that SAE Match effectively tracks feature evolution and improves feature matching quality. This work enhances our understanding of feature dynamics and offers a tool for mechanistic interpretability in neural networks."
                },
                "zh": {
                    "title": "æ­ç¤ºç¥ç»ç½‘ç»œç‰¹å¾æ¼”å˜çš„å¥¥ç§˜",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ·±åº¦ç¥ç»ç½‘ç»œä¸­ç‰¹å¾åœ¨å„å±‚ä¹‹é—´çš„æ¼”å˜ï¼Œç‰¹åˆ«æ˜¯å¤šä¹‰æ€§å’Œç‰¹å¾å åŠ çš„é—®é¢˜ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸ºSAE Matchçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡æœ€å°åŒ–ç¨€ç–è‡ªç¼–ç å™¨å‚æ•°çš„å‡æ–¹è¯¯å·®æ¥å¯¹é½ä¸åŒå±‚çš„ç‰¹å¾ã€‚è¯¥æ–¹æ³•åœ¨Gemma 2è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå®ƒèƒ½æœ‰æ•ˆæ•æ‰ç‰¹å¾åœ¨å„å±‚çš„æ¼”å˜ï¼Œæé«˜ç‰¹å¾åŒ¹é…è´¨é‡ã€‚ç ”ç©¶ç»“æœæœ‰åŠ©äºç†è§£ç¥ç»ç½‘ç»œä¸­çš„ç‰¹å¾åŠ¨æ€ï¼Œå¹¶ä¸ºæœºæ¢°è§£é‡Šæ€§ç ”ç©¶æä¾›äº†æ–°å·¥å…·ã€‚"
                }
            },
            "hash": "6a4c86357dc6667d",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08102",
            "title": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining",
            "url": "https://huggingface.co/papers/2410.08102",
            "abstract": "Efficient data selection is crucial to accelerate the pretraining of large language models (LLMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these approaches to achieve optimal data selection for LLM pretraining. To tackle this problem, we propose a novel multi-agent collaborative data selection mechanism. In this framework, each data selection method serves as an independent agent, and an agent console is designed to dynamically integrate the information from all agents throughout the LLM training process. We conduct extensive empirical studies to evaluate our multi-agent framework. The experimental results demonstrate that our approach significantly improves data efficiency, accelerates convergence in LLM training, and achieves an average performance gain of 10.5% across multiple language model benchmarks compared to the state-of-the-art methods.",
            "score": 8,
            "issue_id": 93,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ LLM",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ° ĞºĞ¾Ğ½ÑĞ¾Ğ»ÑŒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ Ğ²ÑĞµÑ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² 10.5% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Collaborative Agents: Boosting Language Model Training Efficiency",
                    "desc": "The paper introduces a new way to choose data for training large language models more efficiently by using a multi-agent system. Each data selection method acts as an independent agent, and a central console combines their insights to optimize the training process. This approach helps the model learn faster and better, showing a 10.5% improvement in performance over existing methods. The study highlights the importance of collaboration between different data selection strategies to enhance the pretraining of language models."
                },
                "zh": {
                    "title": "å¤šæ™ºèƒ½ä½“åä½œï¼šæå‡å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ•ˆç‡çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“åä½œçš„æ•°æ®é€‰æ‹©æœºåˆ¶ï¼Œä»¥æé«˜å¤§è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒæ•ˆç‡ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œæ¯ç§æ•°æ®é€‰æ‹©æ–¹æ³•éƒ½ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„æ™ºèƒ½ä½“ï¼Œé€šè¿‡ä¸€ä¸ªæ§åˆ¶å°åŠ¨æ€æ•´åˆæ‰€æœ‰æ™ºèƒ½ä½“çš„ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•æ˜¾è‘—æé«˜äº†æ•°æ®æ•ˆç‡ï¼ŒåŠ é€Ÿäº†å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ”¶æ•›é€Ÿåº¦ã€‚ä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªè¯­è¨€æ¨¡å‹åŸºå‡†ä¸Šå¹³å‡æ€§èƒ½æå‡äº†10.5%ã€‚"
                }
            },
            "hash": "e6e97c0fdfa09a15",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08391",
            "title": "KV Prediction for Improved Time to First Token",
            "url": "https://huggingface.co/papers/2410.08391",
            "abstract": "Inference with transformer-based language models begins with a prompt processing step. In this step, the model generates the first output token and stores the KV cache needed for future generation steps. This prompt processing step can be computationally expensive, taking 10s of seconds or more for billion-parameter models on edge devices when prompt lengths or batch sizes rise. This degrades user experience by introducing significant latency into the model's outputs. To reduce the time spent producing the first output (known as the ``time to first token'', or TTFT) of a pretrained model, we introduce a novel method called KV Prediction. In our method, a small auxiliary model is used to process the prompt and produce an approximation of the KV cache used by a base model. This approximated KV cache is then used with the base model for autoregressive generation without the need to query the auxiliary model again. We demonstrate that our method produces a pareto-optimal efficiency-accuracy trade-off when compared to baselines. On TriviaQA, we demonstrate relative accuracy improvements in the range of 15%-50% across a range of TTFT FLOPs budgets. We also demonstrate accuracy improvements of up to 30% on HumanEval python code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark models on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs translates to a TTFT speedup on hardware. We release our code at https://github.com/apple/corenet/tree/main/projects/kv-prediction .",
            "score": 7,
            "issue_id": 91,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#inference"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "KV Prediction: Ğ¼Ğ¾Ğ»Ğ½Ğ¸ĞµĞ½Ğ¾ÑĞ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ KV Prediction Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ KV-ĞºÑÑˆĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ´Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° (TTFT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 15-50% Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… TriviaQA Ğ¸ Ğ´Ğ¾ 30% Ğ½Ğ° HumanEval Ğ¿Ñ€Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ FLOPS Ğ´Ğ»Ñ TTFT. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° CPU Apple M2 Pro Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ TTFT Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Speed Up Your AI: Faster First Outputs with KV Prediction",
                    "desc": "The paper introduces a method called KV Prediction to reduce the time to first token (TTFT) in transformer-based language models. This method uses a small auxiliary model to approximate the key-value (KV) cache, which speeds up the initial output generation without repeatedly querying the auxiliary model. The approach achieves a balance between efficiency and accuracy, showing significant improvements in tasks like TriviaQA and HumanEval. The method also demonstrates hardware speedup on an Apple M2 Pro CPU, enhancing user experience by reducing latency."
                },
                "zh": {
                    "title": "KVé¢„æµ‹ï¼šåŠ é€Ÿç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºKVé¢„æµ‹çš„æ–°æ–¹æ³•ï¼Œç”¨äºå‡å°‘é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆç¬¬ä¸€ä¸ªè¾“å‡ºçš„æ—¶é—´ã€‚é€šè¿‡ä½¿ç”¨ä¸€ä¸ªå°å‹è¾…åŠ©æ¨¡å‹æ¥è¿‘ä¼¼ç”ŸæˆKVç¼“å­˜ï¼Œä»è€ŒåŠ å¿«ç”Ÿæˆé€Ÿåº¦ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´è¾¾åˆ°äº†å¸•ç´¯æ‰˜æœ€ä¼˜çš„å¹³è¡¡ã€‚åœ¨TriviaQAå’ŒHumanEvalç­‰ä»»åŠ¡ä¸­ï¼Œæ–¹æ³•è¡¨ç°å‡ºæ˜¾è‘—çš„å‡†ç¡®æ€§æå‡ã€‚"
                }
            },
            "hash": "447ad32759dde03e",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08168",
            "title": "ZeroComp: Zero-shot Object Compositing from Image Intrinsics via Diffusion",
            "url": "https://huggingface.co/papers/2410.08168",
            "abstract": "We present ZeroComp, an effective zero-shot 3D object compositing approach that does not require paired composite-scene images during training. Our method leverages ControlNet to condition from intrinsic images and combines it with a Stable Diffusion model to utilize its scene priors, together operating as an effective rendering engine. During training, ZeroComp uses intrinsic images based on geometry, albedo, and masked shading, all without the need for paired images of scenes with and without composite objects. Once trained, it seamlessly integrates virtual 3D objects into scenes, adjusting shading to create realistic composites. We developed a high-quality evaluation dataset and demonstrate that ZeroComp outperforms methods using explicit lighting estimations and generative techniques in quantitative and human perception benchmarks. Additionally, ZeroComp extends to real and outdoor image compositing, even when trained solely on synthetic indoor data, showcasing its effectiveness in image compositing.",
            "score": 4,
            "issue_id": 98,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "ZeroComp â€” ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑÑ†ĞµĞ½ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ControlNet Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ĞµĞ³Ğ¾ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Stable Diffusion Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞµÑ‘ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ ÑÑ†ĞµĞ½Ğ°Ñ…. ZeroComp Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ğ°Ğ»ÑŒĞ±ĞµĞ´Ğ¾ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ·Ğ°Ñ‚ĞµĞ½ĞµĞ½Ğ¸Ğ¸. ĞŸĞ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ² ÑÑ†ĞµĞ½Ñ‹, ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑ Ğ·Ğ°Ñ‚ĞµĞ½ĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "ZeroComp: Mastering 3D Object Integration Without Paired Data",
                    "desc": "ZeroComp is a novel method for integrating 3D objects into images without needing paired training data. It uses ControlNet to guide the process with intrinsic images and combines it with Stable Diffusion to leverage scene knowledge. The approach focuses on geometry, albedo, and masked shading to create realistic composites without explicit lighting data. ZeroComp excels in both synthetic and real-world scenarios, outperforming traditional methods in quality and perception tests."
                },
                "zh": {
                    "title": "ZeroCompï¼šæ— éœ€é…å¯¹å›¾åƒçš„3Då¯¹è±¡åˆæˆæ–°çªç ´",
                    "desc": "ZeroCompæ˜¯ä¸€ç§æ— éœ€é…å¯¹å›¾åƒçš„é›¶æ ·æœ¬3Då¯¹è±¡åˆæˆæ–¹æ³•ã€‚å®ƒåˆ©ç”¨ControlNetä»å†…åœ¨å›¾åƒä¸­è·å–æ¡ä»¶ï¼Œå¹¶ç»“åˆç¨³å®šæ‰©æ•£æ¨¡å‹æ¥ä½¿ç”¨åœºæ™¯å…ˆéªŒçŸ¥è¯†ã€‚è®­ç»ƒæ—¶ï¼ŒZeroCompä½¿ç”¨åŸºäºå‡ ä½•ã€åç…§ç‡å’Œé®ç½©é˜´å½±çš„å†…åœ¨å›¾åƒï¼Œæ— éœ€é…å¯¹çš„åœºæ™¯å›¾åƒã€‚ç»è¿‡è®­ç»ƒåï¼Œå®ƒå¯ä»¥æ— ç¼åœ°å°†è™šæ‹Ÿ3Då¯¹è±¡æ•´åˆåˆ°åœºæ™¯ä¸­ï¼Œè°ƒæ•´é˜´å½±ä»¥åˆ›å»ºé€¼çœŸçš„åˆæˆæ•ˆæœã€‚"
                }
            },
            "hash": "8857b485f78603ed",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09045",
            "title": "MiRAGeNews: Multimodal Realistic AI-Generated News Detection",
            "url": "https://huggingface.co/papers/2410.09045",
            "abstract": "The proliferation of inflammatory or misleading \"fake\" news content has become increasingly common in recent years. Simultaneously, it has become easier than ever to use AI tools to generate photorealistic images depicting any scene imaginable. Combining these two -- AI-generated fake news content -- is particularly potent and dangerous. To combat the spread of AI-generated fake news, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real and AI-generated image-caption pairs from state-of-the-art generators. We find that our dataset poses a significant challenge to humans (60% F-1) and state-of-the-art multi-modal LLMs (< 24% F-1). Using our dataset we train a multi-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art baselines on image-caption pairs from out-of-domain image generators and news publishers. We release our code and data to aid future work on detecting AI-generated content.",
            "score": 2,
            "issue_id": 101,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "MiRAGeNews: Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ˜Ğ˜-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MiRAGeNews, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· 12 500 Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, ĞºĞ°Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ˜Ğ˜. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ»ÑĞ´ĞµĞ¹ (60% F1-Ğ¼ĞµÑ€Ñ‹), Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (<24% F1-Ğ¼ĞµÑ€Ñ‹). ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ MiRAGe, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 5,1% Ğ¿Ğ¾ F1-Ğ¼ĞµÑ€Ğµ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑŒ Ğ¸Ğ· ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ´Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞšĞ¾Ğ´ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¿Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Detecting the Mirage: Battling AI-Generated Fake News",
                    "desc": "The paper addresses the growing issue of AI-generated fake news by introducing the MiRAGeNews Dataset, which includes 12,500 image-caption pairs from both real and AI-generated sources. This dataset is challenging for both humans and advanced AI models to accurately classify, highlighting the difficulty in detecting AI-generated content. The authors developed a multi-modal detector called MiRAGe, which outperforms existing models by 5.1% in F-1 score on unseen data. By releasing their dataset and code, they aim to support further research in identifying AI-generated news content."
                },
                "zh": {
                    "title": "MiRAGeNewsï¼šæ‰“å‡»AIç”Ÿæˆè™šå‡æ–°é—»çš„åˆ©å™¨",
                    "desc": "è¿‘å¹´æ¥ï¼Œè™šå‡æ–°é—»çš„ä¼ æ’­å˜å¾—è¶Šæ¥è¶Šæ™®éï¼Œè€Œä½¿ç”¨äººå·¥æ™ºèƒ½å·¥å…·ç”Ÿæˆé€¼çœŸå›¾åƒä¹Ÿå˜å¾—æ›´åŠ å®¹æ˜“ã€‚ä¸ºäº†åº”å¯¹è¿™ç§ç»“åˆäº†AIç”Ÿæˆçš„è™šå‡æ–°é—»çš„å¨èƒï¼Œæˆ‘ä»¬æå‡ºäº†MiRAGeNewsæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«12,500å¯¹é«˜è´¨é‡çš„çœŸå®å’ŒAIç”Ÿæˆçš„å›¾åƒ-æ ‡é¢˜å¯¹ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè¿™ä¸ªæ•°æ®é›†å¯¹äººç±»å’Œæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹éƒ½æ„æˆäº†æ˜¾è‘—æŒ‘æˆ˜ã€‚é€šè¿‡ä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå¤šæ¨¡æ€æ£€æµ‹å™¨MiRAGeï¼Œåœ¨æ£€æµ‹è·¨é¢†åŸŸå›¾åƒç”Ÿæˆå™¨å’Œæ–°é—»å‘å¸ƒè€…çš„å›¾åƒ-æ ‡é¢˜å¯¹æ—¶ï¼Œæ€§èƒ½æé«˜äº†5.1%ã€‚"
                }
            },
            "hash": "0fc6dec55a36d111",
            "pub_date_card": {
                "ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 11",
                "zh": "10æœˆ11æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07331",
            "title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models",
            "url": "https://huggingface.co/papers/2410.07331",
            "abstract": "We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously design the evaluation suite to ensure the accuracy and robustness of the evaluation. We develop the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at https://da-code-bench.github.io.",
            "score": 2,
            "issue_id": 95,
            "pub_date": "2024-10-09",
            "pub_date_ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "DA-Code: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "DA-Code - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸ÑˆÑŒ 30.5% Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ."
                },
                "en": {
                    "title": "Pushing LLMs to Master Data Science Challenges",
                    "desc": "DA-Code is a new benchmark designed to test large language models (LLMs) on complex data science tasks that require advanced coding skills. It includes challenging tasks based on real-world data, focusing on data wrangling and analytics. The benchmark requires models to use sophisticated programming languages to process data and find solutions. Initial tests show that even the best current models only achieve 30.5% accuracy, indicating significant potential for improvement."
                },
                "zh": {
                    "title": "DA-Codeï¼šæŒ‘æˆ˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°æ®ç§‘å­¦ä»»åŠ¡",
                    "desc": "DA-Code æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŸºäºä»£ç†çš„æ•°æ®ç§‘å­¦ä»»åŠ¡ä¸Šçš„ä»£ç ç”ŸæˆåŸºå‡†ã€‚è¿™ä¸ªåŸºå‡†åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒå…ƒç´ ï¼šé¦–å…ˆï¼ŒDA-Code ä¸­çš„ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦é«˜çº§ç¼–ç æŠ€èƒ½ã€‚å…¶æ¬¡ï¼ŒDA-Code çš„ä¾‹å­åŸºäºçœŸå®å¤šæ ·çš„æ•°æ®ï¼Œæ¶µç›–å¤æ‚çš„æ•°æ®å¤„ç†å’Œåˆ†æä»»åŠ¡ã€‚æœ€åï¼Œæ¨¡å‹éœ€è¦ä½¿ç”¨å¤æ‚çš„æ•°æ®ç§‘å­¦ç¼–ç¨‹è¯­è¨€æ¥è§£å†³ä»»åŠ¡ã€‚"
                }
            },
            "hash": "c2d3d40ef1bad864",
            "pub_date_card": {
                "ru": "9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 9",
                "zh": "10æœˆ9æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08612",
            "title": "Synth-SONAR: Sonar Image Synthesis with Enhanced Diversity and Realism via Dual Diffusion Models and GPT Prompting",
            "url": "https://huggingface.co/papers/2410.08612",
            "abstract": "Sonar image synthesis is crucial for advancing applications in underwater exploration, marine biology, and defence. Traditional methods often rely on extensive and costly data collection using sonar sensors, jeopardizing data quality and diversity. To overcome these limitations, this study proposes a new sonar image synthesis framework, Synth-SONAR leveraging diffusion models and GPT prompting. The key novelties of Synth-SONAR are threefold: First, by integrating Generative AI-based style injection techniques along with publicly available real/simulated data, thereby producing one of the largest sonar data corpus for sonar research. Second, a dual text-conditioning sonar diffusion model hierarchy synthesizes coarse and fine-grained sonar images with enhanced quality and diversity. Third, high-level (coarse) and low-level (detailed) text-based sonar generation methods leverage advanced semantic information available in visual language models (VLMs) and GPT-prompting. During inference, the method generates diverse and realistic sonar images from textual prompts, bridging the gap between textual descriptions and sonar image generation. This marks the application of GPT-prompting in sonar imagery for the first time, to the best of our knowledge. Synth-SONAR achieves state-of-the-art results in producing high-quality synthetic sonar datasets, significantly enhancing their diversity and realism.",
            "score": 1,
            "issue_id": 104,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset",
                    "#diffusion",
                    "#multimodal",
                    "#synthetic"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ ÑĞ¾Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Synth-SONAR Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑĞ¾Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ GPT-Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¶ĞµĞºÑ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ, Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ GPT-Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. Synth-SONAR Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ¾Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ GPT-Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ¾Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "\"Revolutionizing Sonar Imagery with AI: Synth-SONAR's Breakthrough\"",
                    "desc": "The paper introduces Synth-SONAR, a novel framework for generating synthetic sonar images using diffusion models and GPT prompting. By integrating Generative AI techniques with existing data, it creates a large and diverse sonar data corpus. The framework employs a dual text-conditioning model to produce high-quality, varied sonar images from textual descriptions. This approach marks the first use of GPT-prompting in sonar imagery, achieving state-of-the-art results in dataset quality and diversity."
                },
                "zh": {
                    "title": "Synth-SONARï¼šç”¨GPTæç¤ºé©æ–°å£°å‘å›¾åƒåˆæˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å£°å‘å›¾åƒåˆæˆæ¡†æ¶ï¼Œåä¸ºSynth-SONARï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹å’ŒGPTæç¤ºæŠ€æœ¯ã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆå¼AIé£æ ¼æ³¨å…¥æŠ€æœ¯å’Œå…¬å¼€çš„çœŸå®/æ¨¡æ‹Ÿæ•°æ®ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤§å‹å£°å‘æ•°æ®é›†ã€‚å®ƒé‡‡ç”¨åŒé‡æ–‡æœ¬æ¡ä»¶çš„å£°å‘æ‰©æ•£æ¨¡å‹å±‚æ¬¡ç»“æ„ï¼Œç”Ÿæˆé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„ç²—ç²’åº¦å’Œç»†ç²’åº¦å£°å‘å›¾åƒã€‚è¯¥æ–¹æ³•é¦–æ¬¡åœ¨å£°å‘å›¾åƒç”Ÿæˆä¸­åº”ç”¨GPTæç¤ºï¼Œæ˜¾è‘—æé«˜äº†åˆæˆå£°å‘æ•°æ®é›†çš„å¤šæ ·æ€§å’ŒçœŸå®æ€§ã€‚"
                }
            },
            "hash": "8000e8d6f78ec110",
            "pub_date_card": {
                "ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 11",
                "zh": "10æœˆ11æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09038",
            "title": "SimpleStrat: Diversifying Language Model Generation with Stratification",
            "url": "https://huggingface.co/papers/2410.09038",
            "abstract": "Generating diverse responses from large language models (LLMs) is crucial for applications such as planning/search and synthetic data generation, where diversity provides distinct answers across generations. Prior approaches rely on increasing temperature to increase diversity. However, contrary to popular belief, we show not only does this approach produce lower quality individual generations as temperature increases, but it depends on model's next-token probabilities being similar to the true distribution of answers. We propose , an alternative approach that uses the language model itself to partition the space into strata. At inference, a random stratum is selected and a sample drawn from within the strata. To measure diversity, we introduce CoverageQA, a dataset of underspecified questions with multiple equally plausible answers, and assess diversity by measuring KL Divergence between the output distribution and uniform distribution over valid ground truth answers. As computing probability per response/solution for proprietary models is infeasible, we measure recall on ground truth solutions. Our evaluation show using SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36 average reduction in KL Divergence compared to Llama 3.",
            "score": 1,
            "issue_id": 102,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#multimodal",
                    "#synthetic"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "SimpleStrat: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SimpleStrat. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸. SimpleStrat Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ°Ğ¼Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚Ñ‹, Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CoverageQA Ñ Ğ½ĞµĞ´Ğ¾Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ğ¸Ğ¼ĞµÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SimpleStrat Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ recall Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ KL-Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "SimpleStrat: Boosting Diversity Without Sacrificing Quality",
                    "desc": "The paper discusses a new method for generating diverse responses from large language models, which is important for tasks like planning and synthetic data generation. Traditional methods increase the temperature to boost diversity, but this can lower the quality of responses. The authors propose a novel approach called SimpleStrat, which uses the model to divide the response space into different sections and randomly selects from these sections to generate diverse outputs. They introduce a new dataset, CoverageQA, to measure diversity and show that SimpleStrat improves recall and reduces KL Divergence compared to other models."
                },
                "zh": {
                    "title": "ç”¨è¯­è¨€æ¨¡å‹è‡ªèº«æå‡å“åº”å¤šæ ·æ€§",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ç”Ÿæˆå¤šæ ·åŒ–çš„å“åº”ï¼Œè¿™å¯¹äºè§„åˆ’ã€æœç´¢å’Œåˆæˆæ•°æ®ç”Ÿæˆç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•é€šè¿‡å¢åŠ æ¸©åº¦æ¥æé«˜å¤šæ ·æ€§ï¼Œä½†è¿™å¯èƒ½å¯¼è‡´ç”Ÿæˆè´¨é‡ä¸‹é™ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨è¯­è¨€æ¨¡å‹è‡ªèº«å°†ç©ºé—´åˆ’åˆ†ä¸ºä¸åŒçš„å±‚æ¬¡ï¼Œå¹¶ä»ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªå±‚æ¬¡è¿›è¡Œé‡‡æ ·ã€‚é€šè¿‡å¼•å…¥CoverageQAæ•°æ®é›†ï¼Œä½œè€…è¯æ˜äº†è¿™ç§æ–¹æ³•åœ¨å¤šæ ·æ€§å’Œå¬å›ç‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            },
            "hash": "06e284b0650ef57d",
            "pub_date_card": {
                "ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 11",
                "zh": "10æœˆ11æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.07536",
            "title": "I-Max: Maximize the Resolution Potential of Pre-trained Rectified Flow Transformers with Projected Flow",
            "url": "https://huggingface.co/papers/2410.07536",
            "abstract": "Rectified Flow Transformers (RFTs) offer superior training and inference efficiency, making them likely the most viable direction for scaling up diffusion models. However, progress in generation resolution has been relatively slow due to data quality and training costs. Tuning-free resolution extrapolation presents an alternative, but current methods often reduce generative stability, limiting practical application. In this paper, we review existing resolution extrapolation methods and introduce the I-Max framework to maximize the resolution potential of Text-to-Image RFTs. I-Max features: (i) a novel Projected Flow strategy for stable extrapolation and (ii) an advanced inference toolkit for generalizing model knowledge to higher resolutions. Experiments with Lumina-Next-2K and Flux.1-dev demonstrate I-Max's ability to enhance stability in resolution extrapolation and show that it can bring image detail emergence and artifact correction, confirming the practical value of tuning-free resolution extrapolation.",
            "score": 1,
            "issue_id": 99,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "I-Max: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº I-Max Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ñ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼ (RFT) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. I-Max Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Projected Flow Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ I-Max Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "\"Boosting Image Resolution: The I-Max Revolution in RFTs\"",
                    "desc": "The paper introduces Rectified Flow Transformers (RFTs) as a promising approach for improving the efficiency of training and inference in diffusion models. It highlights the challenges of increasing generation resolution due to data quality and training costs, and critiques current tuning-free resolution extrapolation methods for their instability. The authors propose the I-Max framework, which includes a Projected Flow strategy and an advanced inference toolkit, to enhance the resolution potential of Text-to-Image RFTs. Experiments demonstrate that I-Max improves stability and detail in image generation, making tuning-free resolution extrapolation more practical."
                },
                "zh": {
                    "title": "æå‡åˆ†è¾¨ç‡çš„I-Maxæ¡†æ¶ï¼šç¨³å®šä¸”æ— éœ€è°ƒå‚",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥æé«˜æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„åˆ†è¾¨ç‡ï¼Œç§°ä¸ºI-Maxæ¡†æ¶ã€‚I-Maxæ¡†æ¶é€šè¿‡å¼•å…¥æŠ•å½±æµç­–ç•¥ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨åˆ†è¾¨ç‡å¤–æ¨æ—¶çš„ä¸ç¨³å®šæ€§é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒI-Maxä¸ä»…èƒ½æé«˜ç”Ÿæˆå›¾åƒçš„ç»†èŠ‚å’Œä¿®æ­£ç‘•ç–µï¼Œè¿˜èƒ½åœ¨ä¸éœ€è¦è°ƒå‚çš„æƒ…å†µä¸‹å®ç°é«˜åˆ†è¾¨ç‡ç”Ÿæˆã€‚è¯¥æ–¹æ³•åœ¨Lumina-Next-2Kå’ŒFlux.1-devä¸Šçš„å®éªŒéªŒè¯äº†å…¶å®é™…åº”ç”¨ä»·å€¼ã€‚"
                }
            },
            "hash": "2d11f82cc2214114",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09037",
            "title": "Mentor-KD: Making Small Language Models Better Multi-step Reasoners",
            "url": "https://huggingface.co/papers/2410.09037",
            "abstract": "Large Language Models (LLMs) have displayed remarkable performances across various complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently, studies have proposed a Knowledge Distillation (KD) approach, reasoning distillation, which transfers such reasoning ability of LLMs through fine-tuning language models of multi-step rationales generated by LLM teachers. However, they have inadequately considered two challenges regarding insufficient distillation sets from the LLM teacher model, in terms of 1) data quality and 2) soft label provision. In this paper, we propose Mentor-KD, which effectively distills the multi-step reasoning capability of LLMs to smaller LMs while addressing the aforementioned challenges. Specifically, we exploit a mentor, intermediate-sized task-specific fine-tuned model, to augment additional CoT annotations and provide soft labels for the student model during reasoning distillation. We conduct extensive experiments and confirm Mentor-KD's effectiveness across various models and complex reasoning tasks.",
            "score": 1,
            "issue_id": 97,
            "pub_date": "2024-10-11",
            "pub_date_ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Mentor-KD: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ° Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Mentor-KD Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ½Ğ°ÑÑ‚Ğ°Ğ²Ğ½Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Chain-of-Thought Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼ÑĞ³ĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ÑĞ³ĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Mentor-KD Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Mentor-KD: Elevating Small Models with Big Reasoning Skills",
                    "desc": "The paper introduces Mentor-KD, a method to improve the transfer of reasoning skills from large language models (LLMs) to smaller models. It addresses challenges in knowledge distillation, such as the quality of data and the provision of soft labels, by using an intermediate-sized mentor model. This mentor model generates additional Chain-of-Thought annotations and soft labels to enhance the learning process of the student model. Experiments show that Mentor-KD effectively enhances the reasoning capabilities of smaller models across different tasks."
                },
                "zh": {
                    "title": "Mentor-KDï¼šæå‡å°æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯å°†å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ä¼ é€’ç»™è¾ƒå°çš„æ¨¡å‹ã€‚ç ”ç©¶ä¸­æå‡ºäº†ä¸€ç§åä¸ºMentor-KDçš„æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨ä¸­ç­‰å¤§å°çš„ä»»åŠ¡ç‰¹å®šæ¨¡å‹æ¥å¢å¼ºæ•°æ®è´¨é‡å’Œæä¾›è½¯æ ‡ç­¾ã€‚Mentor-KDæœ‰æ•ˆåœ°è§£å†³äº†ä»¥å¾€æ–¹æ³•ä¸­æ•°æ®è´¨é‡å’Œè½¯æ ‡ç­¾ä¸è¶³çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMentor-KDåœ¨å¤šç§æ¨¡å‹å’Œå¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            },
            "hash": "2a269c59131978ed",
            "pub_date_card": {
                "ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 11",
                "zh": "10æœˆ11æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.08193",
            "title": "GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment",
            "url": "https://huggingface.co/papers/2410.08193",
            "abstract": "Large Language Models (LLMs) exhibit impressive capabilities but require careful alignment with human preferences. Traditional training-time methods finetune LLMs using human preference datasets but incur significant training costs and require repeated training to handle diverse user preferences. Test-time alignment methods address this by using reward models (RMs) to guide frozen LLMs without retraining. However, existing test-time approaches rely on trajectory-level RMs which are designed to evaluate complete responses, making them unsuitable for autoregressive text generation that requires computing next-token rewards from partial responses. To address this, we introduce GenARM, a test-time alignment approach that leverages the Autoregressive Reward Model--a novel reward parametrization designed to predict next-token rewards for efficient and effective autoregressive generation. Theoretically, we demonstrate that this parametrization can provably guide frozen LLMs toward any distribution achievable by traditional RMs within the KL-regularized reinforcement learning framework. Experimental results show that GenARM significantly outperforms prior test-time alignment baselines and matches the performance of training-time methods. Additionally, GenARM enables efficient weak-to-strong guidance, aligning larger LLMs with smaller RMs without the high costs of training larger models. Furthermore, GenARM supports multi-objective alignment, allowing real-time trade-offs between preference dimensions and catering to diverse user preferences without retraining.",
            "score": 0,
            "issue_id": 102,
            "pub_date": "2024-10-10",
            "pub_date_ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "data": {
                "categories": [
                    "#alignment",
                    "#rlhf"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "GenARM: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ LLM Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GenARM - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğº Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GenARM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ ÑĞ»Ğ°Ğ±Ğ¾Ğ³Ğ¾ Ğº ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Efficiently Aligning Language Models with GenARM: No Retraining Needed!",
                    "desc": "The paper introduces GenARM, a novel method for aligning large language models (LLMs) with human preferences at test time without retraining. GenARM uses an Autoregressive Reward Model to predict next-token rewards, making it suitable for autoregressive text generation. This approach allows frozen LLMs to be guided efficiently, matching the performance of traditional training-time methods while avoiding high retraining costs. GenARM also supports multi-objective alignment, enabling real-time adjustments to cater to diverse user preferences."
                },
                "zh": {
                    "title": "GenARMï¼šé«˜æ•ˆå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ–¹æ³•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åŠŸèƒ½å¼ºå¤§ï¼Œä½†éœ€è¦ä¸äººç±»åå¥½å¯¹é½ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨è®­ç»ƒæ—¶å¾®è°ƒæ¨¡å‹ï¼Œä½†æˆæœ¬é«˜ä¸”éœ€å¤šæ¬¡è®­ç»ƒã€‚GenARMæ˜¯ä¸€ç§æµ‹è¯•æ—¶å¯¹é½æ–¹æ³•ï¼Œä½¿ç”¨è‡ªå›å½’å¥–åŠ±æ¨¡å‹æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„å¥–åŠ±ã€‚å®éªŒè¡¨æ˜ï¼ŒGenARMåœ¨æ•ˆç‡å’Œæ•ˆæœä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æ”¯æŒå¤šç›®æ ‡å¯¹é½ã€‚"
                }
            },
            "hash": "ebce9bee397ae5dd",
            "pub_date_card": {
                "ru": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 10",
                "zh": "10æœˆ10æ—¥"
            }
        }
    ],
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†Baichuan-Omniï¼Œä¸€ä¸ªå¼€æºçš„7Bå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å’Œåˆ†æå›¾åƒã€è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬ï¼Œæä¾›å…ˆè¿›çš„å¤šæ¨¡æ€äº¤äº’ä½“éªŒå’Œå¼ºå¤§æ€§èƒ½ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å¤šæ¨¡æ€è®­ç»ƒæ–¹æ¡ˆï¼Œé€šè¿‡å¤šæ¨¡æ€å¯¹é½å’Œå¤šä»»åŠ¡å¾®è°ƒä¸¤ä¸ªé˜¶æ®µï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†è§†è§‰å’ŒéŸ³é¢‘æ•°æ®ï¼Œå¹¶åœ¨å„ç§å…¨æ¨¡æ€å’Œå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚",
        "pinyin": "zhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le Baichuan-Omni, yÄ« gÃ¨ kÄi yuÃ¡n de 7B duÅ mÃ³ tÃ i dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng, nÃ©ng gÃ²u tÃ³ng shÃ­ chÇ” lÇ hÃ© fÄ“n xÄ« tÃº xiÃ ng, shÃ¬ pÇn, yÄ«n pÇn hÃ© wÃ©n bÄ›n, tÃ­ gÅng xiÄn jÃ¬n de duÅ mÃ³ tÃ i jiÄo hÃ¹ tÇ yÃ n hÃ© qiÃ¡ng dÃ  xÃ­ng nÃ©ng. wÃ©n zhÄng tÃ­ chÅ« le yÄ« zhÇ’ng yÇ’u xiÃ o de duÅ mÃ³ tÃ i xÃ¹n liÃ n fÄng Ã n, tÅng guÃ² duÅ mÃ³ tÃ i duÃ¬ qÃ­ hÃ© duÅ rÃ¨n wÇ” tiÃ¡o liÇng Ã¨r gÃ¨ jiÄ“ duÃ n, shÇ yÇ” yÃ¡n mÃ³ xÃ­ng nÃ©ng gÃ²u yÇ’u xiÃ o chÇ” lÇ shÃ¬ juÃ© hÃ© yÄ«n pÇn shÃ¹ jÃ¹, bÃ¬ng zÃ i gÃ¨ zhÇ’ng quÃ¡n mÃ³ tÃ i hÃ© duÅ mÃ³ tÃ i bÄ›n zhÇ”n cÃ¨ shÃ¬ zhÅng biÇo xiÃ n chÅ« sÃ¨.",
        "vocab": "[\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ shuÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ”yÃ¡n mÃ³xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ”lÇ\", \"trans\": \"process\"},\n    {\"word\": \"åˆ†æ\", \"pinyin\": \"fÄ“nxÄ«\", \"trans\": \"analyze\"},\n    {\"word\": \"å…ˆè¿›\", \"pinyin\": \"xiÄnjÃ¬n\", \"trans\": \"advanced\"},\n    {\"word\": \"äº¤äº’\", \"pinyin\": \"jiÄohÃ¹\", \"trans\": \"interaction\"},\n    {\"word\": \"ä½“éªŒ\", \"pinyin\": \"tÇyÃ n\", \"trans\": \"experience\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ngnÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"æœ‰æ•ˆ\", \"pinyin\": \"yÇ’uxiÃ o\", \"trans\": \"effective\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹nliÃ n\", \"trans\": \"training\"},\n    {\"word\": \"æ–¹æ¡ˆ\", \"pinyin\": \"fÄng'Ã n\", \"trans\": \"scheme\"},\n    {\"word\": \"å¯¹é½\", \"pinyin\": \"duÃ¬qÃ­\", \"trans\": \"alignment\"},\n    {\"word\": \"å¾®è°ƒ\", \"pinyin\": \"wÄ“itiÃ¡o\", \"trans\": \"fine-tuning\"},\n    {\"word\": \"é˜¶æ®µ\", \"pinyin\": \"jiÄ“duÃ n\", \"trans\": \"stage\"},\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬juÃ©\", \"trans\": \"visual\"},\n    {\"word\": \"éŸ³é¢‘\", \"pinyin\": \"yÄ«npÃ­n\", \"trans\": \"audio\"},\n    {\"word\": \"æ•°æ®\", \"pinyin\": \"shÃ¹jÃ¹\", \"trans\": \"data\"},"
    },
    "weekday": 4,
    "link_prev": "2024-10-10.html",
    "link_next": "2024-10-14.html",
    "date_en": "11 October",
    "date_prev": "10 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
    "date_next": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
    "short_date_prev": {
        "ru": "10.10",
        "en": "10/10",
        "zh": "10æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "14.10",
        "en": "10/14",
        "zh": "10æœˆ14æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 0,
        "#benchmark": 11,
        "#agents": 1,
        "#cv": 9,
        "#rl": 0,
        "#rlhf": 3,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 1,
        "#video": 0,
        "#multimodal": 9,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#medicine": 1,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#quantum": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0
    }
}