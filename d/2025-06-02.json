{
    "date": {
        "ru": "2 июня",
        "en": "June 2",
        "zh": "6月2日"
    },
    "time_utc": "2025-06-02 02:47",
    "weekday": 0,
    "issue_id": 4066,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.24863",
            "title": "AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time",
            "url": "https://huggingface.co/papers/2505.24863",
            "abstract": "This paper presents AlphaOne (alpha1), a universal framework for modulating reasoning progress in large reasoning models (LRMs) at test time. alpha1 first introduces alpha moment, which represents the scaled thinking phase with a universal parameter alpha. Within this scaled pre-alpha moment phase, it dynamically schedules slow thinking transitions by modeling the insertion of reasoning transition tokens as a Bernoulli stochastic process. After the alpha moment, alpha1 deterministically terminates slow thinking with the end-of-thinking token, thereby fostering fast reasoning and efficient answer generation. This approach unifies and generalizes existing monotonic scaling methods by enabling flexible and dense slow-to-fast reasoning modulation. Extensive empirical studies on various challenging benchmarks across mathematical, coding, and scientific domains demonstrate alpha1's superior reasoning capability and efficiency. Project page: https://alphaone-project.github.io/",
            "score": 13,
            "issue_id": 4066,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 мая",
                "en": "May 30",
                "zh": "5月30日"
            },
            "hash": "a30c2004fdd2d154",
            "authors": [
                "Junyu Zhang",
                "Runpei Dong",
                "Han Wang",
                "Xuying Ning",
                "Haoran Geng",
                "Peihao Li",
                "Xialin He",
                "Yutong Bai",
                "Jitendra Malik",
                "Saurabh Gupta",
                "Huan Zhang"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24863.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#training",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "AlphaOne: Универсальная модуляция рассуждений в ИИ",
                    "desc": "AlphaOne (alpha1) - это универсальная система для модуляции процесса рассуждений в крупных моделях рассуждений (LRM) во время тестирования. Она вводит понятие альфа-момента, представляющего масштабированную фазу мышления с универсальным параметром альфа. Система динамически планирует переходы между медленным и быстрым мышлением, моделируя вставку токенов перехода рассуждений как стохастический процесс Бернулли. AlphaOne превосходит существующие методы монотонного масштабирования, обеспечивая гибкую модуляцию рассуждений."
                },
                "en": {
                    "title": "AlphaOne: Revolutionizing Reasoning in Large Models",
                    "desc": "This paper introduces AlphaOne, a framework designed to enhance the reasoning capabilities of large reasoning models (LRMs) during testing. It introduces the concept of the alpha moment, which allows for a controlled thinking phase using a universal parameter. By employing a Bernoulli stochastic process, AlphaOne dynamically manages the transition from slow to fast reasoning, optimizing the model's performance. Empirical results show that AlphaOne outperforms existing methods in various complex tasks, demonstrating its effectiveness in improving reasoning efficiency."
                },
                "zh": {
                    "title": "灵活调节推理进程的AlphaOne框架",
                    "desc": "本文提出了AlphaOne（alpha1），这是一个在测试时调节大型推理模型（LRMs）推理进程的通用框架。alpha1首先引入了alpha时刻，表示带有通用参数alpha的缩放思维阶段。在这个缩放的前alpha时刻阶段中，它通过将推理过渡标记的插入建模为伯努利随机过程，动态调度缓慢思维的过渡。在alpha时刻之后，alpha1通过思维结束标记确定性地终止缓慢思维，从而促进快速推理和高效答案生成。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24417",
            "title": "EasyText: Controllable Diffusion Transformer for Multilingual Text\n  Rendering",
            "url": "https://huggingface.co/papers/2505.24417",
            "abstract": "Generating accurate multilingual text with diffusion models has long been desired but remains challenging. Recent methods have made progress in rendering text in a single language, but rendering arbitrary languages is still an unexplored area. This paper introduces EasyText, a text rendering framework based on DiT (Diffusion Transformer), which connects denoising latents with multilingual character tokens encoded as character tokens. We propose character positioning encoding and position encoding interpolation techniques to achieve controllable and precise text rendering. Additionally, we construct a large-scale synthetic text image dataset with 1 million multilingual image-text annotations as well as a high-quality dataset of 20K annotated images, which are used for pretraining and fine-tuning respectively. Extensive experiments and evaluations demonstrate the effectiveness and advancement of our approach in multilingual text rendering, visual quality, and layout-aware text integration.",
            "score": 2,
            "issue_id": 4066,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 мая",
                "en": "May 30",
                "zh": "5月30日"
            },
            "hash": "f28c426fafe8156a",
            "authors": [
                "Runnan Lu",
                "Yuxuan Zhang",
                "Jailing Liu",
                "Haifa Wang",
                "Yiren Song"
            ],
            "affiliations": [
                "Liblib AI",
                "National University of Singapore",
                "The Chinese University of Hong Kong",
                "Tiamat AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.24417.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#dataset",
                    "#multilingual",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "EasyText: прорыв в многоязычном рендеринге текста с помощью диффузионных моделей",
                    "desc": "Статья представляет EasyText - фреймворк для рендеринга многоязычного текста, основанный на модели диффузии DiT. Авторы предлагают методы кодирования позиций символов и интерполяции позиционного кодирования для точного рендеринга текста. Для обучения модели был создан большой синтетический датасет с 1 миллионом аннотаций изображений с текстом на разных языках. Эксперименты показывают эффективность подхода в многоязычном рендеринге текста, визуальном качестве и интеграции текста с учетом макета."
                },
                "en": {
                    "title": "EasyText: Multilingual Text Rendering Made Simple",
                    "desc": "This paper presents EasyText, a novel framework for generating multilingual text using diffusion models. It leverages a Diffusion Transformer (DiT) to connect denoising latents with multilingual character tokens, addressing the challenge of rendering text in various languages. The authors introduce innovative techniques such as character positioning encoding and position encoding interpolation to enhance the control and precision of text rendering. They also create a large-scale dataset with 1 million multilingual image-text pairs, which significantly improves the model's performance in multilingual text rendering and visual quality."
                },
                "zh": {
                    "title": "多语言文本渲染的新突破",
                    "desc": "本论文介绍了一种名为EasyText的文本渲染框架，基于扩散变换器（DiT）技术。该框架通过将去噪潜变量与多语言字符令牌连接，实现了对多语言文本的精确渲染。我们提出了字符位置编码和位置编码插值技术，以实现可控和精确的文本渲染。此外，我们构建了一个包含100万条多语言图像-文本注释的大规模合成文本图像数据集，用于预训练和微调，实验结果表明我们的方法在多语言文本渲染和视觉质量方面具有显著优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.24293",
            "title": "Large Language Models are Locally Linear Mappings",
            "url": "https://huggingface.co/papers/2505.24293",
            "abstract": "We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process.",
            "score": 1,
            "issue_id": 4066,
            "pub_date": "2025-05-30",
            "pub_date_card": {
                "ru": "30 мая",
                "en": "May 30",
                "zh": "5月30日"
            },
            "hash": "42a9e20ff9742560",
            "authors": [
                "James R. Golden"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.24293.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#interpretability",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Линейное представление нелинейных языковых моделей",
                    "desc": "Исследователи показали, что операции вывода нескольких открытых большим языковых моделей (LLM) можно отобразить в эквивалентную линейную систему для входной последовательности без изменения весов модели или предсказаний. Они расширили методы из моделей диффузии изображений, проявляющих локальную или кусочную линейность, стратегически изменив вычисление градиента для предсказания следующего токена. Этот подход был продемонстрирован на различных моделях, включая Llama 3, Gemma 3 и другие. Анализ сингулярного разложения отсоединенного якобиана показал, что эти LLM работают в экстремально низкоразмерных подпространствах, где многие из крупнейших сингулярных векторов декодируются в концепции, связанные с наиболее вероятным выходным токеном."
                },
                "en": {
                    "title": "Unlocking LLMs: Linear Insights into Complex Predictions",
                    "desc": "This paper shows that the inference processes of large language models (LLMs) can be represented as linear systems without changing the model's weights or outputs. By modifying the gradient calculations for next-token predictions, the authors create a Jacobian that closely mirrors the model's predictions using linear methods. They analyze various LLMs and find that these models operate in low-dimensional spaces, where significant singular vectors correspond to key concepts for predicting the next token. This method allows for a deeper understanding of how each layer functions and reveals interpretable semantic structures in the predictions of LLMs."
                },
                "zh": {
                    "title": "揭示大型语言模型的线性本质",
                    "desc": "本文展示了多个开放权重的大型语言模型（LLMs）的推理操作可以映射到一个完全等价的线性系统，而无需修改模型权重或改变输出预测。我们借鉴了图像扩散模型的技术，通过战略性地改变相对于给定输入序列的梯度计算，使得模型的雅可比矩阵几乎完全重现了线性系统的前向预测。我们在多个模型上验证了这种方法，并通过对分离雅可比矩阵的奇异值分解，发现这些LLMs在极低维的子空间中操作，许多最大的奇异向量解码出与最可能输出标记相关的概念。尽管现代LLMs具有强大的表达能力和全局非线性，但可以通过几乎精确的局部线性分解进行解释，从而提供对其内部表示的洞察，并揭示下一个标记预测过程中的可解释语义结构。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.23844",
            "title": "Enabling Flexible Multi-LLM Integration for Scalable Knowledge\n  Aggregation",
            "url": "https://huggingface.co/papers/2505.23844",
            "abstract": "Large language models (LLMs) have shown remarkable promise but remain challenging to continually improve through traditional finetuning, particularly when integrating capabilities from other specialized LLMs. Popular methods like ensemble and weight merging require substantial memory and struggle to adapt to changing data environments. Recent efforts have transferred knowledge from multiple LLMs into a single target model; however, they suffer from interference and degraded performance among tasks, largely due to limited flexibility in candidate selection and training pipelines. To address these issues, we propose a framework that adaptively selects and aggregates knowledge from diverse LLMs to build a single, stronger model, avoiding the high memory overhead of ensemble and inflexible weight merging. Specifically, we design an adaptive selection network that identifies the most relevant source LLMs based on their scores, thereby reducing knowledge interference. We further propose a dynamic weighted fusion strategy that accounts for the inherent strengths of candidate LLMs, along with a feedback-driven loss function that prevents the selector from converging on a single subset of sources. Experimental results demonstrate that our method can enable a more stable and scalable knowledge aggregation process while reducing knowledge interference by up to 50% compared to existing approaches. Code is avaliable at https://github.com/ZLKong/LLM_Integration",
            "score": 1,
            "issue_id": 4066,
            "pub_date": "2025-05-28",
            "pub_date_card": {
                "ru": "28 мая",
                "en": "May 28",
                "zh": "5月28日"
            },
            "hash": "252af3d7c602c2c3",
            "authors": [
                "Zhenglun Kong",
                "Zheng Zhan",
                "Shiyue Hou",
                "Yifan Gong",
                "Xin Meng",
                "Pengwei Sui",
                "Peiyan Dong",
                "Xuan Shen",
                "Zifeng Wang",
                "Pu Zhao",
                "Hao Tang",
                "Stratis Ioannidis",
                "Yanzhi Wang"
            ],
            "affiliations": [
                "Google",
                "Harvard University",
                "Northeastern University",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.23844.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#multimodal",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Умное слияние языковых моделей: адаптивный подход к интеграции знаний",
                    "desc": "Эта статья представляет новый подход к улучшению больших языковых моделей (LLM) путем адаптивного отбора и объединения знаний из различных LLM. Авторы предлагают фреймворк, который использует сеть адаптивного выбора для определения наиболее релевантных исходных моделей и динамическую стратегию взвешенного слияния для учета сильных сторон каждой модели. Метод позволяет снизить интерференцию знаний на 50% по сравнению с существующими подходами. Экспериментальные результаты показывают, что предложенный метод обеспечивает более стабильный и масштабируемый процесс агрегации знаний."
                },
                "en": {
                    "title": "Adaptive Knowledge Aggregation for Enhanced LLM Performance",
                    "desc": "This paper presents a new framework for improving large language models (LLMs) by adaptively selecting and aggregating knowledge from multiple specialized LLMs. Traditional methods like ensemble and weight merging are limited by high memory usage and performance degradation due to knowledge interference. The proposed approach includes an adaptive selection network that identifies the most relevant LLMs and a dynamic weighted fusion strategy that leverages the strengths of these models. Experimental results show that this method significantly reduces knowledge interference and enhances the stability and scalability of knowledge aggregation."
                },
                "zh": {
                    "title": "自适应知识聚合，构建更强大的语言模型",
                    "desc": "大型语言模型（LLMs）在性能上表现出色，但通过传统的微调方法持续改进仍然具有挑战性，尤其是在整合其他专业LLMs的能力时。现有的方法如集成和权重合并需要大量内存，并且难以适应变化的数据环境。我们提出了一种框架，能够自适应地选择和聚合来自不同LLMs的知识，以构建一个更强大的单一模型，避免了集成方法的高内存开销和权重合并的灵活性不足。实验结果表明，我们的方法能够实现更稳定和可扩展的知识聚合过程，同时将知识干扰减少了50%。"
                }
            }
        }
    ],
    "link_prev": "2025-05-30.html",
    "link_next": "2025-06-03.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "30.05",
        "en": "05/30",
        "zh": "5月30日"
    },
    "short_date_next": {
        "ru": "03.06",
        "en": "06/03",
        "zh": "6月3日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了两种后训练策略，蒸馏和强化学习与可验证奖励（RLVR），用于表格推理任务的推理时缩放。这些策略创建了一个名为Table-R1-Zero的模型，该模型使用较少的参数匹配GPT-4.1的性能，并展示出强大的泛化能力。研究团队评估了这些模型在多种表格推理任务中的表现，包括短答问答、事实验证和自由形式问答。结果显示，Table-R1-Zero模型在使用较少参数的情况下，性能匹配或超越了GPT-4.1和DeepSeek-R1。",
        "title": "Table-R1: Inference-Time Scaling for Table Reasoning",
        "pinyin": "Zhè piān wénzhāng jièshào le liǎng zhǒng hòu xùnliàn cèlüè, zhēngliú hé qiángzhì xuéxí yǔ kě yànzhèng jiǎnglì (RLVR), yòngyú biǎogé tuīlǐ rènwù de tuīlǐ shí suōfàng. Zhèxiē cèlüè chuàngjiàn le yīgè míngyǐ Table-R1-Zero de móxíng, gāi móxíng shǐyòng jiào shǎo de cānshù pǐpèi GPT-4.1 de xíngnéng, bìng zhànshì chū qiángdà de fànhuà nénglì. Yánjiū tuánduì pínggū le zhèxiē móxíng zài duō zhǒng biǎogé tuīlǐ rènwù zhōng de biǎoxiàn, bāokuò duǎn dá wèndá, shìshí yànzhèng hé zìyóu xíngshì wèndá. Jiéguǒ xiǎnshì, Table-R1-Zero móxíng zài shǐyòng jiào shǎo cānshù de qíngkuàng xià, xíngnéng pǐpèi huò chāoyuè le GPT-4.1 hé DeepSeek-R1.",
        "vocab": "[\n    {\"word\": \"蒸馏\", \"pinyin\": \"zhēngliú\", \"trans\": \"distillation\"},\n    {\"word\": \"强化学习\", \"pinyin\": \"qiáng huà xué xí\", \"trans\": \"reinforcement learning\"},\n    {\"word\": \"可验证奖励\", \"pinyin\": \"kě yàn zhèng jiǎng lì\", \"trans\": \"verifiable reward\"},\n    {\"word\": \"表格推理\", \"pinyin\": \"biǎo gé tuī lǐ\", \"trans\": \"table reasoning\"},\n    {\"word\": \"推理时缩放\", \"pinyin\": \"tuī lǐ shí suō fàng\", \"trans\": \"inference-time scaling\"},\n    {\"word\": \"泛化能力\", \"pinyin\": \"fàn huà néng lì\", \"trans\": \"generalization capability\"},\n    {\"word\": \"短答问答\", \"pinyin\": \"duǎn dá wèn dá\", \"trans\": \"short answer Q&A\"},\n    {\"word\": \"事实验证\", \"pinyin\": \"shì shí yàn zhèng\", \"trans\": \"fact verification\"},\n    {\"word\": \"自由形式问答\", \"pinyin\": \"zì yóu xíng shì wèn dá\", \"trans\": \"free-form Q&A\"}\n]",
        "trans": "This article introduces two post-training strategies, distillation and reinforcement learning with verifiable rewards (RLVR), for scaling inference-time reasoning in table reasoning tasks. These strategies create a model called Table-R1-Zero, which matches the performance of GPT-4.1 with fewer parameters and demonstrates strong generalization capabilities. The research team evaluated the performance of these models on various table reasoning tasks, including short answer question-answering, fact verification, and free-form question-answering. The results show that the Table-R1-Zero model matches or outperforms GPT-4.1 and DeepSeek-R1 with fewer parameters.",
        "update_ts": "2025-06-01 12:47"
    }
}