
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. May 19.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">19 Ğ¼Ğ°Ñ</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-16.html">â¬…ï¸ <span id="prev-date">16.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-20.html">â¡ï¸ <span id="next-date">20.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '19 Ğ¼Ğ°Ñ', 'en': 'May 19', 'zh': '5æœˆ19æ—¥'};
        let feedDateNext = {'ru': '20.05', 'en': '05/20', 'zh': '5æœˆ20æ—¥'};
        let feedDatePrev = {'ru': '16.05', 'en': '05/16', 'zh': '5æœˆ16æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.09388', 'title': 'Qwen3 Technical Report', 'url': 'https://huggingface.co/papers/2505.09388', 'abstract': 'In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.', 'score': 105, 'issue_id': 3823, 'pub_date': '2025-05-14', 'pub_date_card': {'ru': '14 Ğ¼Ğ°Ñ', 'en': 'May 14', 'zh': '5æœˆ14æ—¥'}, 'hash': '69a0f87bb5460e8d', 'authors': ['An Yang', 'Anfeng Li', 'Baosong Yang', 'Beichen Zhang', 'Binyuan Hui', 'Bo Zheng', 'Bowen Yu', 'Chang Gao', 'Chengen Huang', 'Chenxu Lv', 'Chujie Zheng', 'Dayiheng Liu', 'Fan Zhou', 'Fei Huang', 'Feng Hu', 'Hao Ge', 'Haoran Wei', 'Huan Lin', 'Jialong Tang', 'Jian Yang', 'Jianhong Tu', 'Jianwei Zhang', 'Jianxin Yang', 'Jiaxi Yang', 'Jing Zhou', 'Jingren Zhou', 'Junyang Lin', 'Kai Dang', 'Keqin Bao', 'Kexin Yang', 'Le Yu', 'Lianghao Deng', 'Mei Li', 'Mingfeng Xue', 'Mingze Li', 'Pei Zhang', 'Peng Wang', 'Qin Zhu', 'Rui Men', 'Ruize Gao', 'Shixuan Liu', 'Shuang Luo', 'Tianhao Li', 'Tianyi Tang', 'Wenbiao Yin', 'Xingzhang Ren', 'Xinyu Wang', 'Xinyu Zhang', 'Xuancheng Ren', 'Yang Fan', 'Yang Su', 'Yichang Zhang', 'Yinger Zhang', 'Yu Wan', 'Yuqiong Liu', 'Zekun Wang', 'Zeyu Cui', 'Zhenru Zhang', 'Zhipeng Zhou', 'Zihan Qiu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.09388.jpg', 'data': {'categories': ['#low_resource', '#agi', '#reasoning', '#multilingual', '#benchmark', '#architecture', '#training', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Qwen3: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²', 'desc': 'Qwen3 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ Qwen3 ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ğ±ĞµĞ· Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Qwen3 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ 119 ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Qwen3: Unifying Thinking and Efficiency in Language Models', 'desc': 'Qwen3 is the latest version of the Qwen model family, featuring large language models that enhance performance, efficiency, and multilingual capabilities. It includes both dense and Mixture-of-Expert (MoE) architectures with a wide range of parameters, from 0.6 to 235 billion. A notable innovation is the integration of thinking and non-thinking modes, allowing for seamless dynamic switching based on user needs, which improves response times and reasoning capabilities. Additionally, Qwen3 supports 119 languages, significantly increasing its accessibility and effectiveness in diverse applications, while also providing a thinking budget mechanism for optimized resource allocation during inference.'}, 'zh': {'title': 'Qwen3ï¼šç»Ÿä¸€æ€ç»´ä¸å“åº”çš„æ™ºèƒ½è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Qwen3ï¼Œè¿™æ˜¯Qwenæ¨¡å‹ç³»åˆ—çš„æœ€æ–°ç‰ˆæœ¬ã€‚Qwen3åŒ…å«ä¸€ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ€§èƒ½ã€æ•ˆç‡å’Œå¤šè¯­è¨€èƒ½åŠ›ã€‚å…¶åˆ›æ–°ä¹‹å¤„åœ¨äºå°†æ€ç»´æ¨¡å¼å’Œéæ€ç»´æ¨¡å¼æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ï¼Œå®ç°åŠ¨æ€æ¨¡å¼åˆ‡æ¢ï¼Œé€‚åº”ç”¨æˆ·æŸ¥è¯¢ã€‚Qwen3è¿˜å¼•å…¥äº†æ€ç»´é¢„ç®—æœºåˆ¶ï¼Œå…è®¸ç”¨æˆ·åœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªé€‚åº”åˆ†é…è®¡ç®—èµ„æºï¼Œä»è€Œåœ¨ä»»åŠ¡å¤æ‚æ€§åŸºç¡€ä¸Šå¹³è¡¡å»¶è¿Ÿå’Œæ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.11049', 'title': 'GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning', 'url': 'https://huggingface.co/papers/2505.11049', 'abstract': "To enhance the safety of VLMs, this paper introduces a novel reasoning-based VLM guard model dubbed GuardReasoner-VL. The core idea is to incentivize the guard model to deliberatively reason before making moderation decisions via online RL. First, we construct GuardReasoner-VLTrain, a reasoning corpus with 123K samples and 631K reasoning steps, spanning text, image, and text-image inputs. Then, based on it, we cold-start our model's reasoning ability via SFT. In addition, we further enhance reasoning regarding moderation through online RL. Concretely, to enhance diversity and difficulty of samples, we conduct rejection sampling followed by data augmentation via the proposed safety-aware data concatenation. Besides, we use a dynamic clipping parameter to encourage exploration in early stages and exploitation in later stages. To balance performance and token efficiency, we design a length-aware safety reward that integrates accuracy, format, and token cost. Extensive experiments demonstrate the superiority of our model. Remarkably, it surpasses the runner-up by 19.27% F1 score on average. We release data, code, and models (3B/7B) of GuardReasoner-VL at https://github.com/yueliu1999/GuardReasoner-VL/", 'score': 41, 'issue_id': 3829, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ', 'en': 'May 16', 'zh': '5æœˆ16æ—¥'}, 'hash': 'bedca054f1392a71', 'authors': ['Yue Liu', 'Shengfang Zhai', 'Mingzhe Du', 'Yulin Chen', 'Tri Cao', 'Hongcheng Gao', 'Cheng Wang', 'Xinfeng Li', 'Kun Wang', 'Junfeng Fang', 'Jiaheng Zhang', 'Bryan Hooi'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.11049.jpg', 'data': {'categories': ['#multimodal', '#rl', '#dataset', '#reasoning', '#open_source', '#training'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ñ€Ğ°Ğ¶ Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GuardReasoner-VL - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ GuardReasoner-VLTrain, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¼ 123 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GuardReasoner-VL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ F1-Ğ¼ĞµÑ€Ñƒ Ğ½Ğ° 19.27% Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼.'}, 'en': {'title': 'GuardReasoner-VL: Enhancing VLM Safety through Reasoning and Reinforcement Learning', 'desc': 'This paper presents GuardReasoner-VL, a new model designed to improve the safety of Vision-Language Models (VLMs) by incorporating reasoning into moderation decisions. The model is trained using a large reasoning corpus that includes diverse text and image inputs, allowing it to learn from 123K samples and 631K reasoning steps. To enhance its reasoning capabilities, the model employs supervised fine-tuning (SFT) and online reinforcement learning (RL), which helps it adapt and improve over time. The results show that GuardReasoner-VL significantly outperforms existing models, achieving a 19.27% higher F1 score on average, demonstrating its effectiveness in ensuring safer VLM operations.'}, 'zh': {'title': 'æ¨ç†é©±åŠ¨çš„å®‰å…¨ä¿æŠ¤ï¼šGuardReasoner-VL', 'desc': 'ä¸ºäº†æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å®‰å…¨æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ¨ç†çš„VLMä¿æŠ¤æ¨¡å‹ï¼Œç§°ä¸ºGuardReasoner-VLã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¿€åŠ±ä¿æŠ¤æ¨¡å‹åœ¨åšå‡ºå®¡æŸ¥å†³ç­–ä¹‹å‰è¿›è¡Œæ·±æ€ç†Ÿè™‘çš„æ¨ç†ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«123Kæ ·æœ¬å’Œ631Kæ¨ç†æ­¥éª¤çš„æ¨ç†è¯­æ–™åº“ï¼Œå¹¶é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥å†·å¯åŠ¨æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨çº¿RLè¿›ä¸€æ­¥å¢å¼ºäº†å®¡æŸ¥æ¨ç†çš„èƒ½åŠ›ï¼Œæœ€ç»ˆå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ¨¡å‹åœ¨F1åˆ†æ•°ä¸Šå¹³å‡è¶…è¶Šäº†ç¬¬äºŒå19.27%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.10610', 'title': 'MMLongBench: Benchmarking Long-Context Vision-Language Models\n  Effectively and Thoroughly', 'url': 'https://huggingface.co/papers/2505.10610', 'abstract': "The rapid extension of context windows in large vision-language models has given rise to long-context vision-language models (LCVLMs), which are capable of handling hundreds of images with interleaved text tokens in a single forward pass. In this work, we introduce MMLongBench, the first benchmark covering a diverse set of long-context vision-language tasks, to evaluate LCVLMs effectively and thoroughly. MMLongBench is composed of 13,331 examples spanning five different categories of downstream tasks, such as Visual RAG and Many-Shot ICL. It also provides broad coverage of image types, including various natural and synthetic images. To assess the robustness of the models to different input lengths, all examples are delivered at five standardized input lengths (8K-128K tokens) via a cross-modal tokenization scheme that combines vision patches and text tokens. Through a thorough benchmarking of 46 closed-source and open-source LCVLMs, we provide a comprehensive analysis of the current models' vision-language long-context ability. Our results show that: i) performance on a single task is a weak proxy for overall long-context capability; ii) both closed-source and open-source models face challenges in long-context vision-language tasks, indicating substantial room for future improvement; iii) models with stronger reasoning ability tend to exhibit better long-context performance. By offering wide task coverage, various image types, and rigorous length control, MMLongBench provides the missing foundation for diagnosing and advancing the next generation of LCVLMs.", 'score': 37, 'issue_id': 3830, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': '565f788b384ce66a', 'authors': ['Zhaowei Wang', 'Wenhao Yu', 'Xiyu Ren', 'Jipeng Zhang', 'Yu Zhao', 'Rohit Saxena', 'Liang Cheng', 'Ginny Wong', 'Simon See', 'Pasquale Minervini', 'Yangqiu Song', 'Mark Steedman'], 'affiliations': ['CSE Department, HKUST', 'Miniml.AI', 'NVIDIA AI Technology Center (NVAITC), NVIDIA, Santa Clara, USA', 'Tencent AI Seattle Lab', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.10610.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#multimodal', '#long_context'], 'emoji': 'ğŸ“', 'ru': {'title': 'MMLongBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MMLongBench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (LCVLM). Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 13,331 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· 5 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚ 8K Ğ´Ğ¾ 128K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 46 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ².'}, 'en': {'title': 'MMLongBench: Advancing Long-Context Vision-Language Models', 'desc': 'This paper introduces MMLongBench, a new benchmark designed to evaluate long-context vision-language models (LCVLMs) that can process large amounts of images and text simultaneously. It includes 13,331 examples across five task categories, ensuring a diverse assessment of model performance. The benchmark tests models at various input lengths, allowing for a detailed analysis of their capabilities in handling long-context tasks. The findings reveal that current models struggle with long-context challenges, highlighting the need for improvements and suggesting that models with better reasoning skills perform better in these scenarios.'}, 'zh': {'title': 'MMLongBenchï¼šé•¿ä¸Šä¸‹æ–‡è§†è§‰è¯­è¨€æ¨¡å‹çš„è¯„ä¼°æ–°åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†MMLongBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹é•¿ä¸Šä¸‹æ–‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLCVLMsï¼‰çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°è¿™äº›æ¨¡å‹çš„æ€§èƒ½ã€‚MMLongBenchåŒ…å«13331ä¸ªç¤ºä¾‹ï¼Œæ¶µç›–äº”ç§ä¸åŒç±»åˆ«çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶æä¾›å¤šç§è‡ªç„¶å’Œåˆæˆå›¾åƒç±»å‹ã€‚é€šè¿‡å¯¹46ä¸ªé—­æºå’Œå¼€æºLCVLMçš„æ·±å…¥è¯„ä¼°ï¼Œç ”ç©¶å‘ç°å•ä¸€ä»»åŠ¡çš„è¡¨ç°å¹¶ä¸èƒ½æœ‰æ•ˆåæ˜ æ•´ä½“çš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹çš„æ¨ç†èƒ½åŠ›è¶Šå¼ºï¼Œé•¿ä¸Šä¸‹æ–‡è¡¨ç°è¶Šå¥½ï¼Œæœªæ¥åœ¨è¿™ä¸€é¢†åŸŸä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.11409', 'title': "Visual Planning: Let's Think Only with Images", 'url': 'https://huggingface.co/papers/2505.11409', 'abstract': 'Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.', 'score': 29, 'issue_id': 3825, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ', 'en': 'May 16', 'zh': '5æœˆ16æ—¥'}, 'hash': '67875b7838a7b7ea', 'authors': ['Yi Xu', 'Chengzu Li', 'Han Zhou', 'Xingchen Wan', 'Caiqi Zhang', 'Anna Korhonen', 'Ivan VuliÄ‡'], 'affiliations': ['Google', 'Language Technology Lab, University of Cambridge', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2505.11409.jpg', 'data': {'categories': ['#games', '#reasoning', '#multimodal', '#training', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ²', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ VPRL, ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ GRPO Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°."}, 'en': {'title': 'Visual Planning: Reasoning Beyond Text', 'desc': 'This paper introduces a new approach called Visual Planning, which focuses on using visual representations for reasoning instead of relying solely on text. The authors argue that for tasks involving spatial and geometrical information, visual reasoning can be more effective. They present a reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), which enhances planning capabilities in visual navigation tasks. The results show that this visual approach outperforms traditional text-based reasoning methods, suggesting a shift towards image-based inference in machine learning applications.'}, 'zh': {'title': 'è§†è§‰è§„åˆ’ï¼šè¶…è¶Šæ–‡æœ¬çš„æ¨ç†æ–°èŒƒå¼', 'desc': 'æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€æ‰©å±•ï¼ˆMLLMsï¼‰çš„è¿›å±•æ˜¾è‘—æå‡äº†æœºå™¨æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä¸»è¦ä¾èµ–çº¯æ–‡æœ¬æ¥è¡¨è¾¾å’Œæ„å»ºæ¨ç†ï¼Œå³ä½¿åœ¨å­˜åœ¨è§†è§‰ä¿¡æ¯çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„èŒƒå¼ï¼Œç§°ä¸ºè§†è§‰è§„åˆ’ï¼Œå…è®¸é€šè¿‡çº¯è§†è§‰è¡¨ç¤ºè¿›è¡Œè§„åˆ’ï¼Œè€Œä¸ä¾èµ–æ–‡æœ¬ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè§†è§‰è§„åˆ’åœ¨å¤„ç†ç©ºé—´å’Œå‡ ä½•ä¿¡æ¯çš„ä»»åŠ¡ä¸­ï¼Œæ¯”åŸºäºè¯­è¨€çš„æ¨ç†æ›´æœ‰æ•ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.11107', 'title': 'Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token\n  Level Granularity', 'url': 'https://huggingface.co/papers/2505.11107', 'abstract': "Recent advances in large language models (LLMs) have demonstrated the power of reasoning through self-generated chains of thought. Multiple reasoning agents can collaborate to raise joint reasoning quality above individual outcomes. However, such agents typically interact in a turn-based manner, trading increased latency for improved quality. In this paper, we propose Group Think--a single LLM that acts as multiple concurrent reasoning agents, or thinkers. With shared visibility into each other's partial generation progress, Group Think introduces a new concurrent-reasoning paradigm in which multiple reasoning trajectories adapt dynamically to one another at the token level. For example, a reasoning thread may shift its generation mid-sentence upon detecting that another thread is better positioned to continue. This fine-grained, token-level collaboration enables Group Think to reduce redundant reasoning and improve quality while achieving significantly lower latency. Moreover, its concurrent nature allows for efficient utilization of idle computational resources, making it especially suitable for edge inference, where very small batch size often underutilizes local~GPUs. We give a simple and generalizable modification that enables any existing LLM to perform Group Think on a local GPU. We also present an evaluation strategy to benchmark reasoning latency and empirically demonstrate latency improvements using open-source LLMs that were not explicitly trained for Group Think. We hope this work paves the way for future LLMs to exhibit more sophisticated and more efficient collaborative behavior for higher quality generation.", 'score': 14, 'issue_id': 3828, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ', 'en': 'May 16', 'zh': '5æœˆ16æ—¥'}, 'hash': 'b2ea7382367e75ba', 'authors': ['Chan-Jan Hsu', 'Davide Buffelli', 'Jamie McGowan', 'Feng-Ting Liao', 'Yi-Chang Chen', 'Sattar Vakili', 'Da-shan Shiu'], 'affiliations': ['MediaTek Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.11107.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning', '#optimization', '#agents', '#training', '#open_source', '#inference'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Group Think, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ»Ñ Ğ¿ĞµÑ€Ğ¸Ñ„ĞµÑ€Ğ¸Ğ¹Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½ĞµĞ´Ğ¾Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ GPU. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ Ğ»ÑĞ±Ğ¾Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ LLM Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Group Think Ğ½Ğ° Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ GPU.'}, 'en': {'title': 'Group Think: Collaborative Reasoning for Faster, Smarter LLMs', 'desc': "This paper introduces Group Think, a novel approach that allows a single large language model (LLM) to function as multiple reasoning agents working together simultaneously. By enabling these agents to share visibility into each other's progress, they can dynamically adjust their reasoning paths at the token level, enhancing the overall quality of the output. This concurrent reasoning reduces redundancy and improves efficiency, leading to lower latency compared to traditional turn-based interactions. The authors also provide a method to adapt existing LLMs for Group Think, demonstrating its effectiveness through empirical evaluations."}, 'zh': {'title': 'Group Thinkï¼šæå‡æ¨ç†è´¨é‡çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGroup Thinkçš„æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å•ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå¤šä¸ªå¹¶å‘æ¨ç†ä»£ç†ã€‚é€šè¿‡å…±äº«å½¼æ­¤çš„ç”Ÿæˆè¿›åº¦ï¼Œè¿™ç§æ–¹æ³•å…è®¸æ¨ç†çº¿ç¨‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€é€‚åº”ï¼Œä»è€Œå‡å°‘å†—ä½™æ¨ç†å¹¶æé«˜ç”Ÿæˆè´¨é‡ã€‚Group Thinkçš„å¹¶å‘ç‰¹æ€§ä½¿å¾—è®¡ç®—èµ„æºå¾—åˆ°æ›´æœ‰æ•ˆçš„åˆ©ç”¨ï¼Œç‰¹åˆ«é€‚åˆè¾¹ç¼˜æ¨ç†åœºæ™¯ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ç§ç®€å•çš„ä¿®æ”¹æ–¹æ³•ï¼Œä½¿ç°æœ‰çš„LLMèƒ½å¤Ÿåœ¨æœ¬åœ°GPUä¸Šå®ç°Group Thinkã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.07675', 'title': 'Simple Semi-supervised Knowledge Distillation from Vision-Language\n  Models via texttt{D}ual-texttt{H}ead\n  texttt{O}ptimization', 'url': 'https://huggingface.co/papers/2505.07675', 'abstract': 'Vision-language models (VLMs) have achieved remarkable success across diverse tasks by leveraging rich textual information with minimal labeled data. However, deploying such large models remains challenging, particularly in resource-constrained environments. Knowledge distillation (KD) offers a well-established solution to this problem; however, recent KD approaches from VLMs often involve multi-stage training or additional tuning, increasing computational overhead and optimization complexity. In this paper, we propose texttt{D}ual-texttt{H}ead texttt{O}ptimization (texttt{DHO}) -- a simple yet effective KD framework that transfers knowledge from VLMs to compact, task-specific models in semi-supervised settings. Specifically, we introduce dual prediction heads that independently learn from labeled data and teacher predictions, and propose to linearly combine their outputs during inference. We observe that DHO mitigates gradient conflicts between supervised and distillation signals, enabling more effective feature learning than single-head KD baselines. As a result, extensive experiments show that DHO consistently outperforms baselines across multiple domains and fine-grained datasets. Notably, on ImageNet, it achieves state-of-the-art performance, improving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively, while using fewer parameters.', 'score': 13, 'issue_id': 3828, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 Ğ¼Ğ°Ñ', 'en': 'May 12', 'zh': '5æœˆ12æ—¥'}, 'hash': '73f4f4dd13e67a25', 'authors': ['Seongjae Kang', 'Dong Bok Lee', 'Hyungjoon Jang', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'VUNO Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.07675.jpg', 'data': {'categories': ['#small_models', '#optimization', '#transfer_learning', '#training', '#cv', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'DHO: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Dual-Head Optimization (DHO) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğº ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. DHO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğµ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑÑ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ±ĞµĞ· Ğ½ĞµĞ³Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DHO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ImageNet Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Streamlining Knowledge Distillation with Dual-Head Optimization', 'desc': 'This paper introduces a new framework called Dual-Head Optimization (DHO) for knowledge distillation from vision-language models (VLMs) to smaller, task-specific models. DHO uses two prediction heads that learn from both labeled data and the predictions of a larger teacher model, which helps to reduce conflicts in learning signals. The method simplifies the distillation process, avoiding the complexity of multi-stage training while still improving feature learning. Experiments show that DHO outperforms existing methods, achieving better accuracy on datasets like ImageNet with fewer parameters.'}, 'zh': {'title': 'DHOï¼šé«˜æ•ˆçš„çŸ¥è¯†è’¸é¦æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDHOçš„çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨å°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„çŸ¥è¯†è½¬ç§»åˆ°ç´§å‡‘çš„ä»»åŠ¡ç‰¹å®šæ¨¡å‹ä¸­ã€‚DHOé€šè¿‡å¼•å…¥åŒé¢„æµ‹å¤´ï¼Œåˆ†åˆ«ä»æ ‡è®°æ•°æ®å’Œæ•™å¸ˆé¢„æµ‹ä¸­ç‹¬ç«‹å­¦ä¹ ï¼Œå¹¶åœ¨æ¨ç†æ—¶çº¿æ€§ç»„åˆå®ƒä»¬çš„è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDHOæœ‰æ•ˆç¼“è§£äº†ç›‘ç£ä¿¡å·å’Œè’¸é¦ä¿¡å·ä¹‹é—´çš„æ¢¯åº¦å†²çªï¼Œä»è€Œå®ç°äº†æ¯”å•å¤´çŸ¥è¯†è’¸é¦åŸºçº¿æ›´æœ‰æ•ˆçš„ç‰¹å¾å­¦ä¹ ã€‚æœ€ç»ˆï¼ŒDHOåœ¨å¤šä¸ªé¢†åŸŸå’Œç»†ç²’åº¦æ•°æ®é›†ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶åœ¨ImageNetä¸Šï¼Œä½¿ç”¨æ›´å°‘çš„å‚æ•°å®ç°äº†3%çš„å‡†ç¡®ç‡æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.11427', 'title': 'Mergenetic: a Simple Evolutionary Model Merging Library', 'url': 'https://huggingface.co/papers/2505.11427', 'abstract': 'Model merging allows combining the capabilities of existing models into a new one - post hoc, without additional training. This has made it increasingly popular thanks to its low cost and the availability of libraries that support merging on consumer GPUs. Recent work shows that pairing merging with evolutionary algorithms can boost performance, but no framework currently supports flexible experimentation with such strategies in language models. We introduce Mergenetic, an open-source library for evolutionary model merging. Mergenetic enables easy composition of merging methods and evolutionary algorithms while incorporating lightweight fitness estimators to reduce evaluation costs. We describe its design and demonstrate that Mergenetic produces competitive results across tasks and languages using modest hardware.', 'score': 10, 'issue_id': 3828, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ', 'en': 'May 16', 'zh': '5æœˆ16æ—¥'}, 'hash': '759eb3fbdec85844', 'authors': ['Adrian Robert Minut', 'Tommaso Mencattini', 'Andrea Santilli', 'Donato Crisostomi', 'Emanuele RodolÃ '], 'affiliations': ['Ecole Polytechnique FÃ©dÃ©rale de Lausanne', 'Sapienza University of Rome'], 'pdf_title_img': 'assets/pdf/title_img/2505.11427.jpg', 'data': {'categories': ['#optimization', '#architecture', '#open_source', '#training'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Mergenetic Ğ´Ğ»Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹. Mergenetic Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞºÑ€Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Mergenetic: Evolving Better Models Through Merging', 'desc': 'This paper presents Mergenetic, an open-source library designed for evolutionary model merging in machine learning. Model merging allows the combination of existing models into a new one without the need for additional training, making it cost-effective and efficient. Mergenetic enhances this process by integrating evolutionary algorithms, which can improve model performance. The library also includes lightweight fitness estimators to minimize evaluation costs, demonstrating competitive results across various tasks and languages using standard hardware.'}, 'zh': {'title': 'Mergeneticï¼šè¿›åŒ–æ¨¡å‹åˆå¹¶çš„æ–°é€‰æ‹©', 'desc': 'æ¨¡å‹åˆå¹¶æ˜¯ä¸€ç§å°†ç°æœ‰æ¨¡å‹çš„èƒ½åŠ›ç»“åˆæˆæ–°æ¨¡å‹çš„æ–¹æ³•ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚è¿™ç§æ–¹æ³•å› å…¶ä½æˆæœ¬å’Œæ”¯æŒæ¶ˆè´¹è€…GPUçš„åº“è€Œè¶Šæ¥è¶Šå—æ¬¢è¿ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå°†åˆå¹¶ä¸è¿›åŒ–ç®—æ³•ç»“åˆå¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†ç›®å‰æ²¡æœ‰æ¡†æ¶æ”¯æŒåœ¨è¯­è¨€æ¨¡å‹ä¸­çµæ´»å®éªŒè¿™äº›ç­–ç•¥ã€‚æˆ‘ä»¬ä»‹ç»äº†Mergeneticï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºåº“ï¼Œç”¨äºè¿›åŒ–æ¨¡å‹åˆå¹¶ï¼Œèƒ½å¤Ÿè½»æ¾ç»„åˆåˆå¹¶æ–¹æ³•å’Œè¿›åŒ–ç®—æ³•ï¼ŒåŒæ—¶å¼•å…¥è½»é‡çº§çš„é€‚åº”åº¦è¯„ä¼°å™¨ä»¥é™ä½è¯„ä¼°æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.10518', 'title': 'Multi-Token Prediction Needs Registers', 'url': 'https://huggingface.co/papers/2505.10518', 'abstract': 'Multi-token prediction has emerged as a promising objective for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning. In this paper, we propose MuToR, a simple and effective approach to multi-token prediction that interleaves learnable register tokens into the input sequence, each tasked with predicting future targets. Compared to existing methods, MuToR offers several key advantages: it introduces only a negligible number of additional parameters, requires no architectural changes--ensuring compatibility with off-the-shelf pretrained language models--and remains aligned with the next-token pretraining objective, making it especially well-suited for supervised fine-tuning. Moreover, it naturally supports scalable prediction horizons. We demonstrate the effectiveness and versatility of MuToR across a range of use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains. Our code will be available at: https://github.com/nasosger/MuToR.', 'score': 8, 'issue_id': 3830, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': '10aef9838701ab6e', 'authors': ['Anastasios Gerontopoulos', 'Spyros Gidaris', 'Nikos Komodakis'], 'affiliations': ['Archimedes, Athena Research Center', 'IACM-Forth', 'University of Crete', 'valeo.ai'], 'pdf_title_img': 'assets/pdf/title_img/2505.10518.jpg', 'data': {'categories': ['#training', '#multimodal', '#optimization'], 'emoji': 'ğŸ”®', 'ru': {'title': 'MuToR: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°Ñ€ĞºĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°Ñ€ĞºĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MuToR. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹-Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ñ‹ Ğ² Ğ²Ñ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ñ†ĞµĞ»ĞµĞ¹. MuToR Ğ¸Ğ¼ĞµĞµÑ‚ Ñ€ÑĞ´ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²: Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ MuToR Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'MuToR: Enhancing Multi-Token Prediction for Language Models', 'desc': "This paper introduces MuToR, a novel approach to multi-token prediction that enhances language model pretraining. MuToR integrates learnable register tokens into the input sequence, allowing the model to predict multiple future tokens effectively. It maintains compatibility with existing pretrained models without requiring architectural changes and adds minimal parameters. The authors demonstrate MuToR's effectiveness across various tasks in both language and vision, showcasing its versatility in supervised and parameter-efficient fine-tuning."}, 'zh': {'title': 'MuToRï¼šé«˜æ•ˆçš„å¤šæ ‡è®°é¢„æµ‹æ–¹æ³•', 'desc': 'å¤šæ ‡è®°é¢„æµ‹æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„ç›®æ ‡ï¼Œç”¨äºæ”¹å–„è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒï¼Œä½†å…¶ä¼˜åŠ¿åœ¨å…¶ä»–è®¾ç½®ï¼ˆå¦‚å¾®è°ƒï¼‰ä¸­å¹¶ä¸æ€»æ˜¯æœ‰æ•ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å¤šæ ‡è®°é¢„æµ‹æ–¹æ³•MuToRï¼Œå®ƒå°†å¯å­¦ä¹ çš„æ³¨å†Œæ ‡è®°äº¤é”™åˆ°è¾“å…¥åºåˆ—ä¸­ï¼Œæ¯ä¸ªæ ‡è®°è´Ÿè´£é¢„æµ‹æœªæ¥çš„ç›®æ ‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒMuToRå…·æœ‰å‡ ä¸ªå…³é”®ä¼˜åŠ¿ï¼šä»…å¼•å…¥æå°‘çš„é¢å¤–å‚æ•°ï¼Œä¸éœ€è¦æ¶æ„æ›´æ”¹ï¼Œç¡®ä¿ä¸ç°æˆçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å…¼å®¹ï¼Œå¹¶ä¸”ä¸ä¸‹ä¸€ä¸ªæ ‡è®°çš„é¢„è®­ç»ƒç›®æ ‡ä¿æŒä¸€è‡´ï¼Œç‰¹åˆ«é€‚åˆç›‘ç£å¾®è°ƒã€‚æ­¤å¤–ï¼Œå®ƒè‡ªç„¶æ”¯æŒå¯æ‰©å±•çš„é¢„æµ‹èŒƒå›´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.10962', 'title': 'MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective\n  Search and Data Curation', 'url': 'https://huggingface.co/papers/2505.10962', 'abstract': 'Automated Theorem Proving (ATP) in formal languages remains a formidable challenge in AI, demanding rigorous logical deduction and navigating vast search spaces. While large language models (LLMs) have shown promising performance, existing stepwise provers often suffer from biased search guidance, leading to inefficiencies and suboptimal proof strategies. This paper introduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise ATP system designed to overcome these limitations. MPS-Prover incorporates two key innovations: a highly effective post-training data curation strategy that prunes approximately 40% of redundant training data without sacrificing performance, and a multi-perspective tree search mechanism. This search integrates a learned critic model with strategically designed heuristic rules to diversify tactic selection, prevent getting trapped in unproductive states, and enhance search robustness. Extensive evaluations demonstrate that MPS-Prover achieves state-of-the-art performance on multiple challenging benchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter models. Furthermore, our analyses reveal that MPS-Prover generates significantly shorter and more diverse proofs compared to existing stepwise and whole-proof methods, highlighting its efficiency and efficacy. Our work advances the capabilities of LLM-based formal reasoning and offers a robust framework and a comprehensive analysis for developing more powerful theorem provers.', 'score': 7, 'issue_id': 3825, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ', 'en': 'May 16', 'zh': '5æœˆ16æ—¥'}, 'hash': '07990204af30ff71', 'authors': ['Zhenwen Liang', 'Linfeng Song', 'Yang Li', 'Tao Yang', 'Feng Zhang', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent AI Lab', 'Tencent LLM Department'], 'pdf_title_img': 'assets/pdf/title_img/2505.10962.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#reasoning', '#data', '#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğµ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MPS-Prover. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸. MPS-Prover Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ MPS-Prover Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Theorem Proving with Multi-Perspective Search', 'desc': 'This paper presents the Multi-Perspective Search Prover (MPS-Prover), a new system for Automated Theorem Proving (ATP) that addresses inefficiencies in existing stepwise provers. MPS-Prover utilizes a post-training data curation strategy to eliminate redundant training data, improving performance without loss of quality. It also features a multi-perspective tree search that combines a learned critic model with heuristic rules to enhance search diversity and prevent unproductive paths. The results show that MPS-Prover not only achieves state-of-the-art performance on various benchmarks but also produces shorter and more diverse proofs than previous models.'}, 'zh': {'title': 'å¤šè§†è§’æœç´¢ï¼Œæå‡å®šç†è¯æ˜æ•ˆç‡', 'desc': 'è‡ªåŠ¨å®šç†è¯æ˜ï¼ˆATPï¼‰åœ¨å½¢å¼è¯­è¨€ä¸­ä»ç„¶æ˜¯äººå·¥æ™ºèƒ½ä¸­çš„ä¸€å¤§æŒ‘æˆ˜ï¼Œéœ€è¦ä¸¥æ ¼çš„é€»è¾‘æ¨ç†å’Œå¹¿æ³›çš„æœç´¢ç©ºé—´ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä½†ç°æœ‰çš„é€æ­¥è¯æ˜å™¨å¸¸å¸¸å—åˆ°åè§æœç´¢æŒ‡å¯¼çš„å½±å“ï¼Œå¯¼è‡´æ•ˆç‡ä½ä¸‹å’Œæ¬¡ä¼˜çš„è¯æ˜ç­–ç•¥ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„é€æ­¥ATPç³»ç»Ÿâ€”â€”å¤šè§†è§’æœç´¢è¯æ˜å™¨ï¼ˆMPS-Proverï¼‰ï¼Œæ—¨åœ¨å…‹æœè¿™äº›å±€é™æ€§ã€‚MPS-Proverç»“åˆäº†é«˜æ•ˆçš„åè®­ç»ƒæ•°æ®æ•´ç†ç­–ç•¥å’Œå¤šè§†è§’æ ‘æœç´¢æœºåˆ¶ï¼Œæ˜¾è‘—æé«˜äº†è¯æ˜çš„æ•ˆç‡å’Œå¤šæ ·æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.11140', 'title': 'Scaling Reasoning can Improve Factuality in Large Language Models', 'url': 'https://huggingface.co/papers/2505.11140', 'abstract': 'Recent studies on large language model (LLM) reasoning capabilities have demonstrated promising improvements in model performance by leveraging a lengthy thinking process and additional computational resources during inference, primarily in tasks involving mathematical reasoning (Muennighoff et al., 2025). However, it remains uncertain if longer reasoning chains inherently enhance factual accuracy, particularly beyond mathematical contexts. In this work, we thoroughly examine LLM reasoning within complex open-domain question-answering (QA) scenarios. We initially distill reasoning traces from advanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then fine-tune a variety of models ranging from smaller, instruction-tuned variants to larger architectures based on Qwen2.5. To enrich reasoning traces, we introduce factual information from knowledge graphs in the form of paths into our reasoning traces. Our experimental setup includes four baseline approaches and six different instruction-tuned models evaluated across a benchmark of six datasets, encompassing over 22.6K questions. Overall, we carry out 168 experimental runs and analyze approximately 1.7 million reasoning traces. Our findings indicate that, within a single run, smaller reasoning models achieve noticeable improvements in factual accuracy compared to their original instruction-tuned counterparts. Moreover, our analysis demonstrates that adding test-time compute and token budgets factual accuracy consistently improves by 2-8%, further confirming the effectiveness of test-time scaling for enhancing performance and consequently improving reasoning accuracy in open-domain QA tasks. We release all the experimental artifacts for further research.', 'score': 5, 'issue_id': 3827, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ', 'en': 'May 16', 'zh': '5æœˆ16æ—¥'}, 'hash': '29d6b0a8040db2ff', 'authors': ['Mike Zhang', 'Johannes Bjerva', 'Russa Biswas'], 'affiliations': ['Department of Computer Science Aalborg University Copenhagen, Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2505.11140.jpg', 'data': {'categories': ['#reasoning', '#graphs', '#dataset', '#benchmark', '#inference', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ğ°Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ· Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸. Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 2-8%.'}, 'en': {'title': 'Enhancing LLM Reasoning with Knowledge and Compute', 'desc': 'This paper investigates the reasoning capabilities of large language models (LLMs) in open-domain question-answering tasks. It analyzes how longer reasoning processes and additional computational resources can impact factual accuracy, especially beyond mathematical reasoning. The authors fine-tune various models and incorporate knowledge graph information to enhance reasoning traces. Their experiments reveal that smaller models can achieve better factual accuracy than larger, instruction-tuned models, and that increasing computational resources during inference can further improve performance.'}, 'zh': {'title': 'æå‡æ¨ç†å‡†ç¡®æ€§çš„å…³é”®åœ¨äºæ¨¡å‹ä¸èµ„æºçš„ç»“åˆ', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚å¼€æ”¾é¢†åŸŸé—®ç­”ï¼ˆQAï¼‰åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä»å…ˆè¿›çš„æ¨ç†æ¨¡å‹ä¸­æå–æ¨ç†è½¨è¿¹ï¼Œå¹¶å¯¹å¤šç§æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜å…¶æ¨ç†å‡†ç¡®æ€§ã€‚é€šè¿‡å¼•å…¥çŸ¥è¯†å›¾è°±ä¸­çš„äº‹å®ä¿¡æ¯ï¼Œæˆ‘ä»¬ä¸°å¯Œäº†æ¨ç†è½¨è¿¹ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œè¾ƒå°çš„æ¨ç†æ¨¡å‹åœ¨äº‹å®å‡†ç¡®æ€§ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œè€Œåœ¨æµ‹è¯•æ—¶å¢åŠ è®¡ç®—èµ„æºå’Œä»¤ç‰Œé¢„ç®—ä¹Ÿèƒ½è¿›ä¸€æ­¥æé«˜å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.11011', 'title': 'Humans expect rationality and cooperation from LLM opponents in\n  strategic games', 'url': 'https://huggingface.co/papers/2505.11011', 'abstract': "As Large Language Models (LLMs) integrate into our social and economic interactions, we need to deepen our understanding of how humans respond to LLMs opponents in strategic settings. We present the results of the first controlled monetarily-incentivised laboratory experiment looking at differences in human behaviour in a multi-player p-beauty contest against other humans and LLMs. We use a within-subject design in order to compare behaviour at the individual level. We show that, in this environment, human subjects choose significantly lower numbers when playing against LLMs than humans, which is mainly driven by the increased prevalence of `zero' Nash-equilibrium choices. This shift is mainly driven by subjects with high strategic reasoning ability. Subjects who play the zero Nash-equilibrium choice motivate their strategy by appealing to perceived LLM's reasoning ability and, unexpectedly, propensity towards cooperation. Our findings provide foundational insights into the multi-player human-LLM interaction in simultaneous choice games, uncover heterogeneities in both subjects' behaviour and beliefs about LLM's play when playing against them, and suggest important implications for mechanism design in mixed human-LLM systems.", 'score': 4, 'issue_id': 3828, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ', 'en': 'May 16', 'zh': '5æœˆ16æ—¥'}, 'hash': '0f12554b6b3b2b83', 'authors': ['Darija Barak', 'Miguel Costa-Gomes'], 'affiliations': ['School of Economics University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.11011.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#games'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ›ÑĞ´Ğ¸ vs Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ… Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼', 'desc': "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ»ÑĞ´ĞµĞ¹ Ğ² ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ³Ñ€Ğµ p-beauty contest Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¿Ñ€Ğ¸ Ğ¸Ğ³Ñ€Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² LLM, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ»ÑĞ´ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡Ğ°ÑÑ‚Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ¸Ñ ĞÑÑˆĞ° 'Ğ½Ğ¾Ğ»ÑŒ'. Ğ¢Ğ°ĞºĞ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ñƒ ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğº ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ insights Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ LLM Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ… Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ² ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-LLM ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…."}, 'en': {'title': 'Understanding Human Behavior in Games Against LLMs', 'desc': "This paper investigates how humans behave when competing against Large Language Models (LLMs) in strategic games, specifically in a multi-player p-beauty contest. The study reveals that participants tend to choose lower numbers when playing against LLMs compared to human opponents, influenced by the perception of LLMs' reasoning capabilities. The results indicate that individuals with strong strategic reasoning are more likely to adopt a 'zero' Nash-equilibrium strategy, believing it aligns with the LLM's cooperative tendencies. These findings highlight the complexities of human-LLM interactions and their implications for designing effective systems that integrate both human and LLM participants."}, 'zh': {'title': 'äººç±»ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„æˆ˜ç•¥äº’åŠ¨æ–°è§†è§’', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†äººç±»åœ¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæˆ˜ç•¥äº’åŠ¨æ—¶çš„è¡Œä¸ºå·®å¼‚ã€‚é€šè¿‡ä¸€ä¸ªå—æ§çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°äººç±»åœ¨ä¸LLMså¯¹æˆ˜æ—¶é€‰æ‹©çš„æ•°å­—æ˜¾è‘—ä½äºä¸å…¶ä»–äººç±»å¯¹æˆ˜æ—¶çš„é€‰æ‹©ã€‚è¿™ç§ç°è±¡ä¸»è¦æ˜¯ç”±äºé«˜æˆ˜ç•¥æ¨ç†èƒ½åŠ›çš„å‚ä¸è€…æ›´å€¾å‘äºé€‰æ‹©é›¶çº³ä»€å‡è¡¡ç­–ç•¥ã€‚æˆ‘ä»¬çš„å‘ç°ä¸ºäººç±»ä¸LLMsåœ¨å¤šç©å®¶åŒæ—¶é€‰æ‹©æ¸¸æˆä¸­çš„äº’åŠ¨æä¾›äº†åŸºç¡€æ€§è§è§£ï¼Œå¹¶æ­ç¤ºäº†å‚ä¸è€…è¡Œä¸ºå’Œå¯¹LLMsæ¸¸æˆæ–¹å¼çš„ä¿¡å¿µå·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.10852', 'title': 'MatTools: Benchmarking Large Language Models for Materials Science Tools', 'url': 'https://huggingface.co/papers/2505.10852', 'abstract': 'Large language models (LLMs) are increasingly applied to materials science questions, including literature comprehension, property prediction, materials discovery and alloy design. At the same time, a wide range of physics-based computational approaches have been developed in which materials properties can be calculated. Here, we propose a benchmark application to evaluate the proficiency of LLMs to answer materials science questions through the generation and safe execution of codes based on such physics-based computational materials science packages. MatTools is built on two complementary components: a materials simulation tool question-answer (QA) benchmark and a real-world tool-usage benchmark. We designed an automated methodology to efficiently collect real-world materials science tool-use examples. The QA benchmark, derived from the pymatgen (Python Materials Genomics) codebase and documentation, comprises 69,225 QA pairs that assess the ability of an LLM to understand materials science tools. The real-world benchmark contains 49 tasks (138 subtasks) requiring the generation of functional Python code for materials property calculations. Our evaluation of diverse LLMs yields three key insights: (1)Generalists outshine specialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a standardized framework for assessing and improving LLM capabilities for materials science tool applications, facilitating the development of more effective AI systems for materials science and general scientific research.', 'score': 4, 'issue_id': 3831, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ', 'en': 'May 16', 'zh': '5æœˆ16æ—¥'}, 'hash': 'a29f8a9973b15514', 'authors': ['Siyu Liu', 'Jiamin Xu', 'Beilin Ye', 'Bo Hu', 'David J. Srolovitz', 'Tongqi Wen'], 'affiliations': ['Center for Structural Materials, Department of Mechanical Engineering, The University of Hong Kong, Hong Kong SAR, China', 'Materials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.10852.jpg', 'data': {'categories': ['#benchmark', '#data', '#dataset', '#science'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'MatTools: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ² Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MatTools - Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. ĞÑ†ĞµĞ½ĞºĞ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ MatTools Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ˜Ğ˜ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ·Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ² Ğ˜Ğ˜, Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚.'}, 'en': {'title': 'Evaluating LLMs in Materials Science: Generalists Win!', 'desc': 'This paper introduces MatTools, a benchmark designed to evaluate how well large language models (LLMs) can handle materials science tasks. It combines a question-answer (QA) benchmark with a real-world tool-usage benchmark, assessing LLMs on their ability to generate and execute code for materials property calculations. The QA benchmark includes over 69,000 question-answer pairs derived from existing materials science resources, while the real-world benchmark consists of 49 tasks that require functional Python code generation. The findings suggest that generalist models perform better than specialized ones, that AI can effectively leverage other AI tools, and that simpler approaches yield better results.'}, 'zh': {'title': 'è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ææ–™ç§‘å­¦ä¸­çš„åº”ç”¨èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ææ–™ç§‘å­¦é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼ŒåŒ…æ‹¬æ–‡çŒ®ç†è§£ã€å±æ€§é¢„æµ‹ã€ææ–™å‘ç°å’Œåˆé‡‘è®¾è®¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºå‡†åº”ç”¨ï¼Œè¯„ä¼°LLMsåœ¨ææ–™ç§‘å­¦é—®é¢˜ä¸Šçš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ç”Ÿæˆå’Œå®‰å…¨æ‰§è¡ŒåŸºäºç‰©ç†çš„è®¡ç®—ææ–™ç§‘å­¦è½¯ä»¶åŒ…çš„ä»£ç ã€‚MatToolsç”±ä¸¤ä¸ªäº’è¡¥ç»„ä»¶æ„æˆï¼šææ–™æ¨¡æ‹Ÿå·¥å…·é—®ç­”åŸºå‡†å’ŒçœŸå®å·¥å…·ä½¿ç”¨åŸºå‡†ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œé€šç”¨æ¨¡å‹ä¼˜äºä¸“ä¸šæ¨¡å‹ï¼ŒAIèƒ½å¤Ÿç†è§£å…¶ä»–AIçš„èƒ½åŠ›ï¼Œå¹¶ä¸”ç®€å•çš„æ–¹æ³•æ›´æœ‰æ•ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.11152', 'title': 'Learning Dense Hand Contact Estimation from Imbalanced Data', 'url': 'https://huggingface.co/papers/2505.11152', 'abstract': 'Hands are essential to human interaction, and understanding contact between hands and the world can promote comprehensive understanding of their function. Recently, there have been growing number of hand interaction datasets that cover interaction with object, other hand, scene, and body. Despite the significance of the task and increasing high-quality data, how to effectively learn dense hand contact estimation remains largely underexplored. There are two major challenges for learning dense hand contact estimation. First, there exists class imbalance issue from hand contact datasets where majority of samples are not in contact. Second, hand contact datasets contain spatial imbalance issue with most of hand contact exhibited in finger tips, resulting in challenges for generalization towards contacts in other hand regions. To tackle these issues, we present a framework that learns dense HAnd COntact estimation (HACO) from imbalanced data. To resolve the class imbalance issue, we introduce balanced contact sampling, which builds and samples from multiple sampling groups that fairly represent diverse contact statistics for both contact and non-contact samples. Moreover, to address the spatial imbalance issue, we propose vertex-level class-balanced (VCB) loss, which incorporates spatially varying contact distribution by separately reweighting loss contribution of each vertex based on its contact frequency across dataset. As a result, we effectively learn to predict dense hand contact estimation with large-scale hand contact data without suffering from class and spatial imbalance issue. The codes will be released.', 'score': 2, 'issue_id': 3822, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ', 'en': 'May 16', 'zh': '5æœˆ16æ—¥'}, 'hash': 'caa702fa71c24606', 'authors': ['Daniel Sungho Jung', 'Kyoung Mu Lee'], 'affiliations': ['IPAI, Dept. of ECE & ASRI, Seoul National University, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2505.11152.jpg', 'data': {'categories': ['#training', '#dataset', '#data'], 'emoji': 'ğŸ–ï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ¾Ğ² Ñ€ÑƒĞº: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ° Ñ€ÑƒĞº Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº HACO Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ»Ğ°ÑÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ°Ñ… Ñ€ÑƒĞº. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ VCB, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ÑƒĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ¾Ğ² Ñ€ÑƒĞº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing Hand Contact Estimation with Balanced Learning Techniques', 'desc': 'This paper addresses the challenge of estimating dense hand contact in various interactions, which is crucial for understanding hand functionality. It identifies two main issues: class imbalance, where most samples do not involve contact, and spatial imbalance, where contact is primarily at the fingertips. To overcome these challenges, the authors propose a framework called HACO that utilizes balanced contact sampling to ensure diverse representation of contact data. Additionally, they introduce a vertex-level class-balanced loss to adjust the learning process based on the frequency of contact across different hand regions, leading to improved predictions in dense hand contact estimation.'}, 'zh': {'title': 'æå‡æ‰‹éƒ¨æ¥è§¦ä¼°è®¡çš„å‡†ç¡®æ€§', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æ‰‹éƒ¨æ¥è§¦ä¼°è®¡çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä¸ç‰©ä½“ã€å…¶ä»–æ‰‹ã€åœºæ™¯å’Œèº«ä½“çš„äº’åŠ¨ä¸­ã€‚å°½ç®¡å·²æœ‰å¤§é‡é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œä½†å¦‚ä½•æœ‰æ•ˆå­¦ä¹ å¯†é›†çš„æ‰‹éƒ¨æ¥è§¦ä¼°è®¡ä»ç„¶æ˜¯ä¸€ä¸ªæœªè¢«å……åˆ†ç ”ç©¶çš„é—®é¢˜ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºHACOï¼Œæ—¨åœ¨è§£å†³ç±»ä¸å¹³è¡¡å’Œç©ºé—´ä¸å¹³è¡¡çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥å¹³è¡¡æ¥è§¦é‡‡æ ·å’Œé¡¶ç‚¹çº§ç±»å¹³è¡¡æŸå¤±ï¼Œç ”ç©¶è€…ä»¬æˆåŠŸåœ°æé«˜äº†æ‰‹éƒ¨æ¥è§¦ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.10496', 'title': 'CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of\n  Synthetic Chest Radiographs', 'url': 'https://huggingface.co/papers/2505.10496', 'abstract': 'We introduce CheXGenBench, a rigorous and multifaceted evaluation framework for synthetic chest radiograph generation that simultaneously assesses fidelity, privacy risks, and clinical utility across state-of-the-art text-to-image generative models. Despite rapid advancements in generative AI for real-world imagery, medical domain evaluations have been hindered by methodological inconsistencies, outdated architectural comparisons, and disconnected assessment criteria that rarely address the practical clinical value of synthetic samples. CheXGenBench overcomes these limitations through standardised data partitioning and a unified evaluation protocol comprising over 20 quantitative metrics that systematically analyse generation quality, potential privacy vulnerabilities, and downstream clinical applicability across 11 leading text-to-image architectures. Our results reveal critical inefficiencies in the existing evaluation protocols, particularly in assessing generative fidelity, leading to inconsistent and uninformative comparisons. Our framework establishes a standardised benchmark for the medical AI community, enabling objective and reproducible comparisons while facilitating seamless integration of both existing and future generative models. Additionally, we release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K radiographs generated by the top-performing model (Sana 0.6B) in our benchmark to support further research in this critical domain. Through CheXGenBench, we establish a new state-of-the-art and release our framework, models, and SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/', 'score': 2, 'issue_id': 3833, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': 'ef303e066da24351', 'authors': ['Raman Dutt', 'Pedro Sanchez', 'Yongchen Yao', 'Steven McDonagh', 'Sotirios A. Tsaftaris', 'Timothy Hospedales'], 'affiliations': ['Samsung AI Center, Cambridge', 'Sinkove', 'The University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2505.10496.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#cv', '#open_source', '#benchmark'], 'emoji': '\U0001fa7b', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ', 'desc': 'CheXGenBench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ³Ñ€ÑƒĞ´Ğ½Ğ¾Ğ¹ ĞºĞ»ĞµÑ‚ĞºĞ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ€Ğ¸ÑĞºĞ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. CheXGenBench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 20 ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ 11 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SynthCheX-75K Ğ¸Ğ· 75 000 Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ² Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ.'}, 'en': {'title': 'Setting New Standards for Synthetic Chest Radiograph Evaluation', 'desc': 'CheXGenBench is a comprehensive evaluation framework designed to assess the generation of synthetic chest radiographs using advanced text-to-image models. It addresses previous challenges in medical image evaluation by providing standardized metrics that measure fidelity, privacy risks, and clinical utility. The framework includes over 20 quantitative metrics and evaluates 11 leading generative architectures, revealing inefficiencies in current evaluation methods. Additionally, it introduces a high-quality synthetic dataset, SynthCheX-75K, to support ongoing research in medical AI.'}, 'zh': {'title': 'å»ºç«‹åŒ»å­¦AIçš„æ–°æ ‡å‡†è¯„ä¼°æ¡†æ¶', 'desc': 'CheXGenBenchæ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºåˆæˆèƒ¸éƒ¨Xå…‰å›¾åƒçš„ç”Ÿæˆï¼Œè¯„ä¼°ç”Ÿæˆçš„çœŸå®æ€§ã€éšç§é£é™©å’Œä¸´åºŠå®ç”¨æ€§ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä»¥å¾€åŒ»å­¦é¢†åŸŸè¯„ä¼°ä¸­çš„æ–¹æ³•ä¸ä¸€è‡´ã€æ¶æ„æ¯”è¾ƒè¿‡æ—¶å’Œè¯„ä¼°æ ‡å‡†è„±èŠ‚ç­‰é—®é¢˜ã€‚é€šè¿‡æ ‡å‡†åŒ–çš„æ•°æ®åˆ’åˆ†å’Œç»Ÿä¸€çš„è¯„ä¼°åè®®ï¼ŒCheXGenBenchä½¿ç”¨è¶…è¿‡20ä¸ªå®šé‡æŒ‡æ ‡ç³»ç»Ÿåˆ†æç”Ÿæˆè´¨é‡å’Œæ½œåœ¨éšç§æ¼æ´ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªé«˜è´¨é‡çš„åˆæˆæ•°æ®é›†SynthCheX-75Kï¼ŒåŒ…å«75,000å¼ ç”±æœ€ä½³æ¨¡å‹ç”Ÿæˆçš„Xå…‰å›¾åƒï¼Œä»¥æ”¯æŒè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.09924', 'title': 'From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework\n  for Large Language Models', 'url': 'https://huggingface.co/papers/2505.09924', 'abstract': 'The rise of Large Language Models (LLMs) has heightened concerns about the misuse of AI-generated text, making watermarking a promising solution. Mainstream watermarking schemes for LLMs fall into two categories: logits-based and sampling-based. However, current schemes entail trade-offs among robustness, text quality, and security. To mitigate this, we integrate logits-based and sampling-based schemes, harnessing their respective strengths to achieve synergy. In this paper, we propose a versatile symbiotic watermarking framework with three strategies: serial, parallel, and hybrid. The hybrid framework adaptively embeds watermarks using token entropy and semantic entropy, optimizing the balance between detectability, robustness, text quality, and security. Furthermore, we validate our approach through comprehensive experiments on various datasets and models. Experimental results indicate that our method outperforms existing baselines and achieves state-of-the-art (SOTA) performance. We believe this framework provides novel insights into diverse watermarking paradigms. Our code is available at https://github.com/redwyd/SymMark{https://github.com/redwyd/SymMark}.', 'score': 2, 'issue_id': 3833, 'pub_date': '2025-05-15', 'pub_date_card': {'ru': '15 Ğ¼Ğ°Ñ', 'en': 'May 15', 'zh': '5æœˆ15æ—¥'}, 'hash': '17456d03961632a9', 'authors': ['Yidan Wang', 'Yubing Ren', 'Yanan Cao', 'Binxing Fang'], 'affiliations': ['Hainan Province Fang Binxing Academician Workstation, Hainan, China', 'Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China', 'School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.09924.jpg', 'data': {'categories': ['#data', '#optimization', '#dataset', '#security', '#open_source', '#benchmark', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¡Ğ¸Ğ¼Ğ±Ğ¸Ğ¾Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğ¼ Ğ·Ğ½Ğ°ĞºĞ°Ğ¼ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ñ‚ĞµĞºÑÑ‚Ñ‹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ² Ğ¸ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ, ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒÑ, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² ÑĞ²Ğ¾ĞµĞ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Synergizing Watermarking Techniques for Robust AI Text Security', 'desc': 'This paper addresses the challenges of watermarking Large Language Models (LLMs) to prevent misuse of AI-generated text. It introduces a new framework that combines logits-based and sampling-based watermarking techniques to enhance robustness and text quality while maintaining security. The proposed hybrid approach uses token and semantic entropy to adaptively embed watermarks, optimizing the balance between detectability and performance. Experimental results demonstrate that this method surpasses existing watermarking techniques, achieving state-of-the-art results across various datasets and models.'}, 'zh': {'title': 'å…±ç”Ÿæ°´å°æ¡†æ¶ï¼šä¼˜åŒ–AIæ–‡æœ¬å®‰å…¨æ€§ä¸è´¨é‡çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å…´èµ·ï¼Œå…³äºAIç”Ÿæˆæ–‡æœ¬æ»¥ç”¨çš„æ‹…å¿§åŠ å‰§ï¼Œå› æ­¤æ°´å°æŠ€æœ¯æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç°æœ‰çš„æ°´å°æ–¹æ¡ˆä¸»è¦åˆ†ä¸ºåŸºäºlogitså’ŒåŸºäºé‡‡æ ·çš„ä¸¤ç±»ï¼Œä½†è¿™äº›æ–¹æ¡ˆåœ¨é²æ£’æ€§ã€æ–‡æœ¬è´¨é‡å’Œå®‰å…¨æ€§ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šåŠŸèƒ½çš„å…±ç”Ÿæ°´å°æ¡†æ¶ï¼Œç»“åˆäº†è¿™ä¸¤ç§æ–¹æ¡ˆçš„ä¼˜ç‚¹ï¼Œé‡‡ç”¨ä¸²è¡Œã€å¹¶è¡Œå’Œæ··åˆä¸‰ç§ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹ä¸Šè¶…è¶Šäº†ç°æœ‰åŸºå‡†ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.11480', 'title': 'Improving Assembly Code Performance with Large Language Models via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.11480', 'abstract': 'Large language models (LLMs) have demonstrated strong performance across a wide range of programming tasks, yet their potential for code optimization remains underexplored. This work investigates whether LLMs can optimize the performance of assembly code, where fine-grained control over execution enables improvements that are difficult to express in high-level languages. We present a reinforcement learning framework that trains LLMs using Proximal Policy Optimization (PPO), guided by a reward function that considers both functional correctness, validated through test cases, and execution performance relative to the industry-standard compiler gcc -O3. To support this study, we introduce a benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO, achieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3 baseline, outperforming all 20 other models evaluated, including Claude-3.7-sonnet. These results indicate that reinforcement learning can unlock the potential of LLMs to serve as effective optimizers for assembly code performance.', 'score': 1, 'issue_id': 3837, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ', 'en': 'May 16', 'zh': '5æœˆ16æ—¥'}, 'hash': '553b664dc4913a2e', 'authors': ['Anjiang Wei', 'Tarun Suresh', 'Huanmi Tan', 'Yinglun Xu', 'Gagandeep Singh', 'Ke Wang', 'Alex Aiken'], 'affiliations': ['Carnegie Mellon University', 'Stanford University', 'University of Illinois Urbana-Champaign', 'Visa Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.11480.jpg', 'data': {'categories': ['#rl', '#plp', '#training', '#benchmark', '#rlhf', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'LLM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ gcc Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑÑĞµĞ¼Ğ±Ğ»ĞµÑ€Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑÑĞµĞ¼Ğ±Ğ»ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Proximal Policy Optimization (PPO) Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ LLM. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ gcc -O3. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen2.5-Coder-7B-PPO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 96% Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¸ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 1.47 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ¸ĞµĞ¹ gcc -O3.'}, 'en': {'title': 'Unlocking LLMs for Assembly Code Optimization', 'desc': 'This paper explores the ability of large language models (LLMs) to optimize assembly code, which allows for precise control over execution. The authors develop a reinforcement learning framework using Proximal Policy Optimization (PPO) to train LLMs, focusing on both correctness and performance improvements. They introduce a benchmark of 8,072 real-world programs to evaluate their model, Qwen2.5-Coder-7B-PPO, which achieves a high test pass rate and significant speedup compared to the standard gcc -O3 compiler. The findings suggest that LLMs, when trained with reinforcement learning, can effectively enhance the performance of assembly code.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ åŠ©åŠ›å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–æ±‡ç¼–ä»£ç ', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¼–ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä»£ç ä¼˜åŒ–æ–¹é¢çš„æ½œåŠ›å°šæœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ç ”ç©¶äº†LLMsæ˜¯å¦èƒ½å¤Ÿä¼˜åŒ–æ±‡ç¼–ä»£ç çš„æ€§èƒ½ï¼Œå› ä¸ºæ±‡ç¼–è¯­è¨€æä¾›äº†å¯¹æ‰§è¡Œçš„ç»†ç²’åº¦æ§åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰è®­ç»ƒLLMsï¼Œå¹¶é€šè¿‡è€ƒè™‘åŠŸèƒ½æ­£ç¡®æ€§å’Œæ‰§è¡Œæ€§èƒ½çš„å¥–åŠ±å‡½æ•°è¿›è¡ŒæŒ‡å¯¼ã€‚æˆ‘ä»¬çš„æ¨¡å‹Qwen2.5-Coder-7B-PPOåœ¨æµ‹è¯•ä¸­è¾¾åˆ°äº†96.0%çš„é€šè¿‡ç‡ï¼Œå¹¶ä¸”åœ¨é€Ÿåº¦ä¸Šæ¯”è¡Œä¸šæ ‡å‡†ç¼–è¯‘å™¨gcc -O3å¿«äº†1.47å€ï¼Œæ˜¾ç¤ºå‡ºå¼ºåŒ–å­¦ä¹ å¯ä»¥æœ‰æ•ˆæå‡LLMsåœ¨æ±‡ç¼–ä»£ç ä¼˜åŒ–ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.11493', 'title': 'GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing', 'url': 'https://huggingface.co/papers/2505.11493', 'abstract': 'Editing images using natural language instructions has become a natural and expressive way to modify visual content; yet, evaluating the performance of such models remains challenging. Existing evaluation approaches often rely on image-text similarity metrics like CLIP, which lack precision. In this work, we introduce a new benchmark designed to evaluate text-guided image editing models in a more grounded manner, along two critical dimensions: (i) functional correctness, assessed via automatically generated multiple-choice questions that verify whether the intended change was successfully applied; and (ii) image content preservation, which ensures that non-targeted regions of the image remain visually consistent using an object-aware masking technique and preservation scoring. The benchmark includes over 1000 high-quality editing examples across 20 diverse content categories, each annotated with detailed editing instructions, evaluation questions, and spatial object masks. We conduct a large-scale study comparing GPT-Image-1, the latest flagship in the text-guided image editing space, against several state-of-the-art editing models, and validate our automatic metrics against human ratings. Results show that GPT-Image-1 leads in instruction-following accuracy, but often over-modifies irrelevant image regions, highlighting a key trade-off in the current model behavior. GIE-Bench provides a scalable, reproducible framework for advancing more accurate evaluation of text-guided image editing.', 'score': 0, 'issue_id': 3838, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ', 'en': 'May 16', 'zh': '5æœˆ16æ—¥'}, 'hash': 'd5f4f14c60b051ec', 'authors': ['Yusu Qian', 'Jiasen Lu', 'Tsu-Jui Fu', 'Xinze Wang', 'Chen Chen', 'Yinfei Yang', 'Wenze Hu', 'Zhe Gan'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2505.11493.jpg', 'data': {'categories': ['#benchmark', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'GIE-Bench: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½ÑĞµĞ¼Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 1000 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¼Ğ°ÑĞºĞ°Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GPT-Image-1 Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ½Ğ¾ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Evaluation of Text-Guided Image Editing with GIE-Bench', 'desc': 'This paper addresses the challenges in evaluating models that edit images based on natural language instructions. It introduces a new benchmark called GIE-Bench, which assesses models on functional correctness and image content preservation. The benchmark includes a large dataset of editing examples with detailed instructions and evaluation metrics. The study compares the performance of GPT-Image-1 with other models, revealing strengths in instruction-following but weaknesses in preserving non-targeted image areas.'}, 'zh': {'title': 'æå‡æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘çš„è¯„ä¼°å‡†ç¡®æ€§', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†ï¼Œæ—¨åœ¨æ›´å‡†ç¡®åœ°è¯„ä¼°æ–‡æœ¬å¼•å¯¼çš„å›¾åƒç¼–è¾‘æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„å¤šé¡¹é€‰æ‹©é¢˜æ¥è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå¹¶ä½¿ç”¨å¯¹è±¡æ„ŸçŸ¥æ©è†œæŠ€æœ¯ç¡®ä¿å›¾åƒå†…å®¹çš„ä¿ç•™ã€‚åŸºå‡†åŒ…å«è¶…è¿‡1000ä¸ªé«˜è´¨é‡çš„ç¼–è¾‘ç¤ºä¾‹ï¼Œæ¶µç›–20ä¸ªä¸åŒçš„å†…å®¹ç±»åˆ«ï¼Œå¹¶é™„æœ‰è¯¦ç»†çš„ç¼–è¾‘æŒ‡ä»¤å’Œè¯„ä¼°é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡GPT-Image-1åœ¨éµå¾ªæŒ‡ä»¤çš„å‡†ç¡®æ€§ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å¤„ç†æ— å…³å›¾åƒåŒºåŸŸæ—¶å¸¸å¸¸è¿‡åº¦ä¿®æ”¹ï¼Œçªæ˜¾äº†å½“å‰æ¨¡å‹è¡Œä¸ºä¸­çš„ä¸€ä¸ªå…³é”®æƒè¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.11315', 'title': 'Improving Inference-Time Optimisation for Vocal Effects Style Transfer\n  with a Gaussian Prior', 'url': 'https://huggingface.co/papers/2505.11315', 'abstract': "Style Transfer with Inference-Time Optimisation (ST-ITO) is a recent approach for transferring the applied effects of a reference audio to a raw audio track. It optimises the effect parameters to minimise the distance between the style embeddings of the processed audio and the reference. However, this method treats all possible configurations equally and relies solely on the embedding space, which can lead to unrealistic or biased results. We address this pitfall by introducing a Gaussian prior derived from a vocal preset dataset, DiffVox, over the parameter space. The resulting optimisation is equivalent to maximum-a-posteriori estimation. Evaluations on vocal effects transfer on the MedleyDB dataset show significant improvements across metrics compared to baselines, including a blind audio effects estimator, nearest-neighbour approaches, and uncalibrated ST-ITO. The proposed calibration reduces parameter mean squared error by up to 33% and matches the reference style better. Subjective evaluations with 16 participants confirm our method's superiority, especially in limited data regimes. This work demonstrates how incorporating prior knowledge in inference time enhances audio effects transfer, paving the way for more effective and realistic audio processing systems.", 'score': 0, 'issue_id': 3836, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ', 'en': 'May 16', 'zh': '5æœˆ16æ—¥'}, 'hash': 'a8b0aeed9083fba7', 'authors': ['Chin-Yun Yu', 'Marco A. MartÃ­nez-RamÃ­rez', 'Junghyun Koo', 'Wei-Hsiang Liao', 'Yuki Mitsufuji', 'GyÃ¶rgy Fazekas'], 'affiliations': ['Centre for Digital Music, Queen Mary University of London, London, UK', 'Sony AI, Tokyo, Japan', 'Sony Group Corporation, Tokyo, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2505.11315.jpg', 'data': {'categories': ['#optimization', '#inference', '#audio', '#dataset'], 'emoji': 'ğŸµ', 'ru': {'title': 'Ğ‘Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑƒ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ST-ITO Ñ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ğ¼ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµÑĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ¡ÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° 16 ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing Audio Effects Transfer with Prior Knowledge', 'desc': 'This paper presents a new method called Style Transfer with Inference-Time Optimisation (ST-ITO) for applying audio effects from a reference track to a raw audio track. The approach improves upon previous methods by introducing a Gaussian prior based on a vocal preset dataset, which helps to guide the optimisation of effect parameters. This results in more realistic audio effects by reducing the mean squared error of the parameters and better matching the reference style. The method shows significant improvements in performance metrics and subjective evaluations, especially when data is limited, highlighting the importance of incorporating prior knowledge in audio processing.'}, 'zh': {'title': 'å¼•å…¥å…ˆéªŒçŸ¥è¯†ï¼Œæå‡éŸ³é¢‘é£æ ¼è½¬ç§»æ•ˆæœ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„éŸ³é¢‘é£æ ¼è½¬ç§»æ–¹æ³•ï¼Œç§°ä¸ºæ¨ç†æ—¶ä¼˜åŒ–ï¼ˆST-ITOï¼‰ï¼Œæ—¨åœ¨å°†å‚è€ƒéŸ³é¢‘çš„æ•ˆæœåº”ç”¨äºåŸå§‹éŸ³é¢‘è½¨é“ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–æ•ˆæœå‚æ•°ï¼Œæœ€å°åŒ–å¤„ç†åéŸ³é¢‘ä¸å‚è€ƒéŸ³é¢‘çš„é£æ ¼åµŒå…¥ä¹‹é—´çš„è·ç¦»ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•å¯¹æ‰€æœ‰é…ç½®çš„å¤„ç†æ˜¯å¹³ç­‰çš„ï¼Œå¯èƒ½å¯¼è‡´ä¸çœŸå®æˆ–æœ‰åå·®çš„ç»“æœã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥åŸºäºå£°ä¹é¢„è®¾æ•°æ®é›†DiffVoxçš„é«˜æ–¯å…ˆéªŒï¼Œæ”¹è¿›äº†å‚æ•°ç©ºé—´çš„ä¼˜åŒ–ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†éŸ³é¢‘æ•ˆæœè½¬ç§»çš„å‡†ç¡®æ€§å’Œæ•ˆæœã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi (1)', '#alignment', '#architecture (4)', '#audio (1)', '#benchmark (10)', '#cv (3)', '#data (5)', '#dataset (7)', '#diffusion', '#ethics', '#games (2)', '#graphs (1)', '#hallucinations', '#healthcare', '#inference (3)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (5)', '#open_source (6)', '#optimization (8)', '#plp (1)', '#rag', '#reasoning (8)', '#rl (3)', '#rlhf (2)', '#robotics', '#science (1)', '#security (1)', '#small_models (1)', '#story_generation', '#survey', '#synthetic (1)', '#training (11)', '#transfer_learning (1)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-05-19 19:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-19 19:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-19 19:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    