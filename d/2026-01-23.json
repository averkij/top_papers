{
    "date": {
        "ru": "23 ÑĞ½Ğ²Ğ°Ñ€Ñ",
        "en": "January 23",
        "zh": "1æœˆ23æ—¥"
    },
    "time_utc": "2026-01-23 07:28",
    "weekday": 4,
    "issue_id": 731,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2601.15165",
            "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
            "url": "https://huggingface.co/papers/2601.15165",
            "abstract": "Arbitrary order generation in diffusion large language models limits reasoning capability by causing premature solution space collapse, making standard policy optimization more effective.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap",
            "score": 40,
            "issue_id": 727,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "0d86f058b66161df",
            "authors": [
                "Zanlin Ni",
                "Shenzhi Wang",
                "Yang Yue",
                "Tianyu Yu",
                "Weilin Zhao",
                "Yeguo Hua",
                "Tianyi Chen",
                "Jun Song",
                "Cheng Yu",
                "Bo Zheng",
                "Gao Huang"
            ],
            "affiliations": [
                "Alibaba Group",
                "LeapLab, Tsinghua University",
                "NLPLab, Tsinghua University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.15165.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#optimization",
                    "#diffusion",
                    "#reasoning",
                    "#rl",
                    "#rlhf",
                    "#training"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ, ÑÑ‚Ğ°Ğ²ÑˆĞ°Ñ Ğ»Ğ¾Ğ²ÑƒÑˆĞºĞ¾Ğ¹: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ»ÑƒÑ‡ÑˆĞµ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ, Ğ½Ğ° ÑĞ°Ğ¼Ğ¾Ğ¼ Ğ´ĞµĞ»Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ¶Ğ´ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑÑƒĞ¶Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Group Relative Policy Optimization (GRPO) ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ, Ñ‡ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ‹Ñ‚Ğ°ÑÑ‚ÑÑ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ JustGRPO Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºĞ°Ğ· Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°Ñ… Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Unlocking Reasoning Potential in Diffusion Models with JustGRPO",
                    "desc": "This paper discusses how the ability of diffusion large language models (dLLMs) to generate tokens in any order can actually limit their reasoning capabilities. The authors argue that this flexibility leads to a premature collapse of the solution space, as dLLMs tend to avoid exploring uncertain tokens that are essential for effective reasoning. They propose a new method called JustGRPO, which uses standard Group Relative Policy Optimization instead of relying on arbitrary order generation. Their results show that JustGRPO can achieve high accuracy while maintaining the advantages of parallel decoding in dLLMs."
                },
                "zh": {
                    "title": "æ”¾å¼ƒä»»æ„é¡ºåºï¼Œæå‡æ¨ç†èƒ½åŠ›",
                    "desc": "åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰åœ¨ç”Ÿæˆé¡ºåºä¸Šçš„çµæ´»æ€§å¦‚ä½•å½±å“æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡dLLMså…è®¸ä»¥ä»»æ„é¡ºåºç”Ÿæˆæ ‡è®°ï¼Œä½†æˆ‘ä»¬å‘ç°è¿™ç§çµæ´»æ€§å®é™…ä¸Šä¼šå¯¼è‡´æ¨ç†è¾¹ç•Œçš„æ”¶çª„ã€‚å…·ä½“æ¥è¯´ï¼ŒdLLMså€¾å‘äºè·³è¿‡é«˜ä¸ç¡®å®šæ€§çš„æ ‡è®°ï¼Œè¿™äº›æ ‡è®°å¯¹äºæ¢ç´¢è‡³å…³é‡è¦ï¼Œä»è€Œå¯¼è‡´è§£å†³æ–¹æ¡ˆç©ºé—´çš„æå‰å´©æºƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•JustGRPOï¼Œé€šè¿‡æ”¾å¼ƒä»»æ„é¡ºåºç”Ÿæˆï¼Œé‡‡ç”¨æ ‡å‡†çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16206",
            "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
            "url": "https://huggingface.co/papers/2601.16206",
            "abstract": "LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.",
            "score": 30,
            "issue_id": 727,
            "pub_date": "2026-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "0d7b6e6eb59f8554",
            "authors": [
                "Daixuan Cheng",
                "Shaohan Huang",
                "Yuxian Gu",
                "Huatong Song",
                "Guoxin Chen",
                "Li Dong",
                "Wayne Xin Zhao",
                "Ji-Rong Wen",
                "Furu Wei"
            ],
            "affiliations": [
                "GSAI, Renmin University of China",
                "Microsoft Research",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16206.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸœï¸",
                "ru": {
                    "title": "Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿ĞµÑĞ¾Ñ‡Ğ½Ğ¸Ñ†Ğ° ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ‚Ğ¾Ñ€ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LLM-in-Sandbox â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM ÑĞ¿Ğ¾Ğ½Ñ‚Ğ°Ğ½Ğ½Ğ¾ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ¡Ğ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑĞ¾Ñ‡Ğ½Ğ¸Ñ†Ñ‹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½ĞµĞ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ»ÑƒÑ‡ÑˆĞµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ°Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ, Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ, Ñ…Ğ¸Ğ¼Ğ¸Ğ¸, Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Unlocking General Intelligence with LLM-in-Sandbox",
                    "desc": "LLM-in-Sandbox is a framework that allows large language models (LLMs) to perform tasks across various domains by utilizing a code sandbox environment. This approach enables LLMs to generalize their capabilities without needing additional training, as they can access external resources and manage long contexts effectively. The framework also incorporates Reinforcement Learning to enhance the models' ability to explore the sandbox, using only non-agentic data for training. Overall, LLM-in-Sandbox demonstrates strong performance in diverse fields such as mathematics, physics, and biomedicine, while being efficient and easy to deploy."
                },
                "zh": {
                    "title": "LLM-in-Sandboxï¼šè®©è¯­è¨€æ¨¡å‹åœ¨æ²™ç®±ä¸­æ¢ç´¢æ™ºèƒ½",
                    "desc": "LLM-in-Sandbox æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå…è®¸å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç æ²™ç®±ç¯å¢ƒä¸­æ¢ç´¢ï¼Œä»è€Œåœ¨ä¸åŒé¢†åŸŸæ‰§è¡Œä¸€èˆ¬æ™ºèƒ½ä»»åŠ¡ã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†å¼ºå¤§çš„è¯­è¨€æ¨¡å‹åœ¨æ²¡æœ‰é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿåˆ©ç”¨ä»£ç æ²™ç®±è¿›è¡Œéä»£ç ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚æ¨¡å‹å¯ä»¥è‡ªå‘è®¿é—®å¤–éƒ¨èµ„æºè·å–æ–°çŸ¥è¯†ï¼Œåˆ©ç”¨æ–‡ä»¶ç³»ç»Ÿå¤„ç†é•¿ä¸Šä¸‹æ–‡ï¼Œå¹¶æ‰§è¡Œè„šæœ¬æ»¡è¶³æ ¼å¼è¦æ±‚ã€‚é€šè¿‡ LLM-in-Sandbox å¼ºåŒ–å­¦ä¹ ï¼Œæ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å¾—åˆ°äº†è¿›ä¸€æ­¥å¢å¼ºï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.15197",
            "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
            "url": "https://huggingface.co/papers/2601.15197",
            "abstract": "BayesianVLA addresses language-action grounding issues in robot manipulation by using Bayesian decomposition to prevent information collapse and improve out-of-distribution generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior Ï€(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.",
            "score": 25,
            "issue_id": 727,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "0c0b4f494acf3cd6",
            "authors": [
                "Shijie Lian",
                "Bin Yu",
                "Xiaopeng Lin",
                "Laurence T. Yang",
                "Zhaolong Shen",
                "Changti Wu",
                "Yuzhuo Miao",
                "Cong Huang",
                "Kai Chen"
            ],
            "affiliations": [
                "BUAA",
                "DeepCybo",
                "ECNU",
                "HIT",
                "HKUST(GZ)",
                "HUST",
                "ZGCA",
                "ZGCI",
                "ZZU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.15197.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#alignment",
                    "#robotics",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¯Ğ·Ñ‹Ğº Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸: Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¾Ğµ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ BayesianVLA â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ vision-language-action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼ ÑĞ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ñ‹ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ²Ñ‹Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ‡Ğ¸ÑÑ‚Ñ‹Ğµ vision-only Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ ÑĞ·Ñ‹Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºÑƒÑ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ñ learnable latent action queries Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ vision-only prior Ğ¸ language-conditioned posterior. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (PMI) Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 11.3% Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Robot Language Understanding with BayesianVLA",
                    "desc": "BayesianVLA is a framework designed to enhance the ability of robots to understand and execute language instructions during manipulation tasks. It addresses the problem of Information Collapse, where models rely too heavily on visual data and ignore language cues, leading to poor performance in new situations. By using Bayesian decomposition and introducing Latent Action Queries, the model learns to differentiate between vision-only actions and those that are conditioned on language. This approach not only improves generalization to out-of-distribution scenarios but also ensures that robots follow instructions more accurately, as demonstrated by significant performance gains in various benchmarks."
                },
                "zh": {
                    "title": "è´å¶æ–¯åˆ†è§£æå‡æœºå™¨äººè¯­è¨€ä¸åŠ¨ä½œçš„ç†è§£èƒ½åŠ›",
                    "desc": "BayesianVLA æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººæ“ä½œä¸­çš„è¯­è¨€ä¸åŠ¨ä½œçš„åŸºç¡€é—®é¢˜ã€‚é€šè¿‡è´å¶æ–¯åˆ†è§£ï¼Œè¯¥æ–¹æ³•é˜²æ­¢ä¿¡æ¯å´©æºƒï¼Œå¹¶æé«˜æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥å¯å­¦ä¹ çš„æ½œåœ¨åŠ¨ä½œæŸ¥è¯¢ï¼Œæ„å»ºåŒåˆ†æ”¯æ¶æ„ï¼Œä»¥åŒæ—¶ä¼°è®¡è§†è§‰ä¼˜å…ˆå’Œè¯­è¨€æ¡ä»¶åéªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBayesianVLA åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„å¤šä»»åŠ¡åœºæ™¯ä¸­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.14724",
            "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding",
            "url": "https://huggingface.co/papers/2601.14724",
            "abstract": "HERMES is a training-free architecture that enables real-time video stream understanding by utilizing a hierarchical memory framework based on KV cache reuse, achieving faster response times and maintained accuracy even with reduced video token input.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.",
            "score": 25,
            "issue_id": 728,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "e6f92cc7d801bfe5",
            "authors": [
                "Haowei Zhang",
                "Shudong Yang",
                "Jinlan Fu",
                "See-Kiong Ng",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "Fudan University",
                "National University of Singapore",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.14724.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#inference",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "HERMES â€” ÑÑ‚Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ KV cache. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ MLLM Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚ĞºĞ»Ğ¸Ğº Ğ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ—Ğ° ÑÑ‡Ñ‘Ñ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 10-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 68% Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ´Ğ¾ 11,4% Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "HERMES: Real-Time Video Understanding Made Easy",
                    "desc": "HERMES is a novel architecture designed for real-time understanding of video streams without the need for training. It utilizes a hierarchical memory framework that reuses a KV cache to efficiently manage video information at different levels of detail. This approach allows HERMES to provide quick responses while maintaining accuracy, even when the amount of video data is significantly reduced. The architecture achieves impressive performance, being up to 10 times faster than previous state-of-the-art models, while also improving accuracy on streaming datasets."
                },
                "zh": {
                    "title": "HERMESï¼šå®æ—¶è§†é¢‘æµç†è§£çš„æ–°çªç ´",
                    "desc": "HERMESæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¶æ„ï¼Œèƒ½å¤Ÿå®æ—¶ç†è§£è§†é¢‘æµã€‚å®ƒåˆ©ç”¨åŸºäºKVç¼“å­˜é‡ç”¨çš„å±‚æ¬¡è®°å¿†æ¡†æ¶ï¼Œå®ç°äº†æ›´å¿«çš„å“åº”æ—¶é—´å’Œåœ¨å‡å°‘è§†é¢‘ä»¤ç‰Œè¾“å…¥æ—¶ä¿æŒçš„å‡†ç¡®æ€§ã€‚é€šè¿‡å¯¹æœºåˆ¶æ³¨æ„åŠ›çš„ç ”ç©¶ï¼ŒHERMESå°†KVç¼“å­˜æ¦‚å¿µåŒ–ä¸ºä¸€ä¸ªå¤šç²’åº¦çš„è§†é¢‘ä¿¡æ¯å­˜å‚¨ç»“æ„ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒHERMESé‡ç”¨ç´§å‡‘çš„KVç¼“å­˜ï¼Œåœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹é«˜æ•ˆåœ°ç†è§£è§†é¢‘æµã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16093",
            "title": "SAMTok: Representing Any Mask with Two Words",
            "url": "https://huggingface.co/papers/2601.16093",
            "abstract": "SAMTok enables pixel-wise capabilities in multi-modal LLMs through discrete mask tokenization and standard training methods, achieving state-of-the-art performance on various vision-language tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.",
            "score": 23,
            "issue_id": 728,
            "pub_date": "2026-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "9998d33914156e79",
            "authors": [
                "Yikang Zhou",
                "Tao Zhang",
                "Dengxian Gong",
                "Yuanzheng Wu",
                "Ye Tian",
                "Haochen Wang",
                "Haobo Yuan",
                "Jiacong Wang",
                "Lu Qi",
                "Hao Fei",
                "Anran Wang",
                "Zhuochen Wang",
                "Yujing Wang",
                "Cheng Chen",
                "Shunping Ji",
                "Xiangtai Li"
            ],
            "affiliations": [
                "ByteDance",
                "NUS",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16093.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#rl",
                    "#training",
                    "#benchmark",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞŸĞ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°ÑĞ¾Ğº",
                    "desc": "SAMTok â€” ÑÑ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¼Ğ°ÑĞ¾Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ±ĞµĞ· Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ²Ğ»Ğ°Ğ´ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ°ÑĞºĞ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 209 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑĞ¾Ğº Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¼Ğ°ÑĞ¾Ğº Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². SAMTok Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Empowering MLLMs with Pixel-Wise Precision through SAMTok",
                    "desc": "SAMTok is a novel approach that enhances multi-modal large language models (MLLMs) by introducing pixel-wise capabilities through discrete mask tokenization. It simplifies the training process by converting region masks into two special tokens, allowing MLLMs to learn from these tokens without needing complex architectures or specialized loss functions. By leveraging standard next-token prediction and reinforcement learning, SAMTok achieves impressive performance on various vision-language tasks, such as region captioning and visual question answering. This method demonstrates a scalable and efficient way to integrate pixel-level understanding into existing MLLMs, making them more interactive and intelligent."
                },
                "zh": {
                    "title": "SAMTokï¼šèµ‹èƒ½å¤šæ¨¡æ€æ¨¡å‹çš„åƒç´ çº§èƒ½åŠ›",
                    "desc": "SAMTok æ˜¯ä¸€ç§ç¦»æ•£æ©ç æ ‡è®°å™¨ï¼Œèƒ½å¤Ÿå°†åŒºåŸŸæ©ç è½¬æ¢ä¸ºä¸¤ä¸ªç‰¹æ®Šçš„æ ‡è®°ï¼Œä»è€Œå®ç°åƒç´ çº§çš„å¤šæ¨¡æ€èƒ½åŠ›ã€‚é€šè¿‡å°†æ©ç è§†ä¸ºæ–°çš„è¯­è¨€æ ‡è®°ï¼ŒSAMTok ä½¿åŸºç¡€çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ QwenVL ç³»åˆ—ï¼‰èƒ½å¤Ÿé€šè¿‡æ ‡å‡†çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’Œç®€å•çš„å¼ºåŒ–å­¦ä¹ æ¥å­¦ä¹ åƒç´ çº§èƒ½åŠ›ï¼Œè€Œæ— éœ€ä¿®æ”¹æ¶æ„æˆ–è®¾è®¡ä¸“é—¨çš„æŸå¤±å‡½æ•°ã€‚è¯¥æ–¹æ³•åœ¨ 209M å¤šæ ·åŒ–æ©ç ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç”Ÿæˆç´§å‡‘ä¸”ä¿¡æ¯ä¸°å¯Œçš„ç¦»æ•£æ ‡è®°ï¼Œæ˜¾è‘—æå‡äº†åœ¨åŒºåŸŸæè¿°ã€åŒºåŸŸè§†è§‰é—®ç­”ç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶å±•ç¤ºäº†ä¸€ç§å¯æ‰©å±•ä¸”ç®€å•çš„èŒƒå¼ï¼Œä½¿å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å…·å¤‡å¼ºå¤§çš„åƒç´ çº§èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16208",
            "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders",
            "url": "https://huggingface.co/papers/2601.16208",
            "abstract": "Representation Autoencoders (RAEs) demonstrate superior performance over VAEs in large-scale text-to-image generation, showing improved stability, faster convergence, and better quality while enabling unified multimodal reasoning in shared representation spaces.  \t\t\t\t\tAI-generated summary \t\t\t\t Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.",
            "score": 20,
            "issue_id": 727,
            "pub_date": "2026-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "d12bb0fdee88065f",
            "authors": [
                "Shengbang Tong",
                "Boyang Zheng",
                "Ziteng Wang",
                "Bingda Tang",
                "Nanye Ma",
                "Ellis Brown",
                "Jihan Yang",
                "Rob Fergus",
                "Yann LeCun",
                "Saining Xie"
            ],
            "affiliations": [
                "New York University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16208.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#multimodal",
                    "#diffusion",
                    "#synthetic",
                    "#open_source",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ VAE Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Representation Autoencoders (RAE) â€” Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ VAE Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ RAE Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğµ SigLIP-2, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ², Ñ‡Ñ‚Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ, Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ĞµĞµ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼, Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğµ Ğ´Ğ°ÑÑ‚ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ RAE Ñ FLUX VAE Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ğ¾Ñ‚ 0.5B Ğ´Ğ¾ 9.8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ RAE-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, Ğ»ÑƒÑ‡ÑˆĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğµ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ ĞºĞ°Ñ‚Ğ°ÑÑ‚Ñ€Ğ¾Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğµ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ â€” Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾."
                },
                "en": {
                    "title": "RAEs: The Superior Choice for Text-to-Image Generation",
                    "desc": "Representation Autoencoders (RAEs) outperform Variational Autoencoders (VAEs) in large-scale text-to-image generation by providing better stability, faster convergence, and higher quality outputs. This study explores the scalability of RAEs in high-dimensional semantic latent spaces, particularly for freeform text-to-image tasks. The findings indicate that while increasing model scale enhances general fidelity, the composition of training data is crucial for specific applications like text. Overall, RAEs demonstrate superior performance and robustness compared to VAEs, making them a more effective choice for multimodal reasoning in shared representation spaces."
                },
                "zh": {
                    "title": "è¡¨ç¤ºè‡ªç¼–ç å™¨ï¼šå¤§è§„æ¨¡ç”Ÿæˆçš„æ›´å¼ºåŸºç¡€",
                    "desc": "è¡¨ç¤ºè‡ªç¼–ç å™¨ï¼ˆRAEsï¼‰åœ¨å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­è¡¨ç°ä¼˜äºå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ï¼Œå±•ç°å‡ºæ›´å¥½çš„ç¨³å®šæ€§ã€æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´é«˜çš„ç”Ÿæˆè´¨é‡ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRAEsåœ¨é«˜ç»´è¯­ä¹‰æ½œåœ¨ç©ºé—´ä¸­çš„æ‰©å±•èƒ½åŠ›ä½¿å…¶åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚é€šè¿‡å¯¹RAEè®¾è®¡é€‰æ‹©çš„ä¸¥æ ¼æµ‹è¯•ï¼Œå‘ç°ç®€åŒ–çš„æ¡†æ¶åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­æ›´ä¸ºæœ‰æ•ˆï¼Œå°½ç®¡å™ªå£°è°ƒåº¦ä»ç„¶é‡è¦ï¼Œä½†å¤æ‚çš„æ¶æ„è®¾è®¡å¸¦æ¥çš„å¥½å¤„å¾®ä¹å…¶å¾®ã€‚æœ€ç»ˆï¼ŒRAEåœ¨å„ä¸ªæ¨¡å‹è§„æ¨¡çš„é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µå‡è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ï¼Œç¡®ç«‹äº†å…¶ä½œä¸ºå¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ›´å¼ºåŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16175",
            "title": "Learning to Discover at Test Time",
            "url": "https://huggingface.co/papers/2601.16175",
            "abstract": "Test-time training enables AI systems to discover optimal solutions for specific scientific problems through continual learning focused on individual challenges rather than generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) ErdÅ‘s' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to 2times faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.",
            "score": 14,
            "issue_id": 727,
            "pub_date": "2026-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "484b889a43728b97",
            "authors": [
                "Mert Yuksekgonul",
                "Daniel Koceja",
                "Xinhao Li",
                "Federico Bianchi",
                "Jed McCaleb",
                "Xiaolong Wang",
                "Jan Kautz",
                "Yejin Choi",
                "James Zou",
                "Carlos Guestrin",
                "Yu Sun"
            ],
            "affiliations": [
                "NVIDIA",
                "Stanford University",
                "Thinking Machines",
                "University of California, San Diego",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16175.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#optimization",
                    "#science",
                    "#rl",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Test-Time Training to Discover (TTT-Discover), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ñ‡ĞµÑ€ĞµĞ· reinforcement learning, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ñ†ĞµĞ»ÑŒÑ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ° Ğ½Ğµ Ğ´Ğ¾Ğ±Ğ¸Ñ‚ÑŒÑÑ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞµĞ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…: Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ GPU ÑĞ´ĞµÑ€, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ. TTT-Discover Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹."
                },
                "en": {
                    "title": "Discovering Optimal Solutions with Test-Time Training",
                    "desc": "This paper introduces Test-Time Training to Discover (TTT-Discover), a method that allows AI systems to adaptively learn and optimize solutions for specific scientific problems during the testing phase. Unlike traditional approaches that focus on generalization, TTT-Discover emphasizes continual learning tailored to individual challenges, aiming to find the best solution rather than an average one. The method employs reinforcement learning to refine a frozen large language model (LLM) based on experiences specific to the test problem, leading to significant improvements in various domains such as mathematics and biology. The results demonstrate that TTT-Discover achieves state-of-the-art performance across multiple tasks, all while using an open model and maintaining reproducibility with publicly available code."
                },
                "zh": {
                    "title": "æµ‹è¯•æ—¶è®­ç»ƒï¼šä¸ºç§‘å­¦é—®é¢˜æ‰¾åˆ°æœ€ä½³è§£å†³æ–¹æ¡ˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºæµ‹è¯•æ—¶è®­ç»ƒå‘ç°ï¼ˆTTT-Discoverï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æŒç»­å­¦ä¹ æ¥è§£å†³ç‰¹å®šç§‘å­¦é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„é€šç”¨åŒ–æ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•ä¸“æ³¨äºä¸ºæ¯ä¸ªå…·ä½“é—®é¢˜æ‰¾åˆ°æœ€ä½³è§£å†³æ–¹æ¡ˆï¼Œè€Œä¸æ˜¯å¹³å‡å¤šä¸ªè‰¯å¥½è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡åœ¨æµ‹è¯•æ—¶è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œæ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç‰¹å®šé—®é¢˜çš„ç»éªŒè¿›è¡Œè®­ç»ƒï¼Œä»è€Œæé«˜è§£å†³é—®é¢˜çš„æ•ˆç‡ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒTTT-Discoveråœ¨å¤šä¸ªé¢†åŸŸï¼ˆå¦‚æ•°å­¦ã€GPUå†…æ ¸å·¥ç¨‹ã€ç®—æ³•è®¾è®¡å’Œç”Ÿç‰©å­¦ï¼‰ä¸­è®¾å®šäº†æ–°çš„æœ€ä¼˜æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16125",
            "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
            "url": "https://huggingface.co/papers/2601.16125",
            "abstract": "A novel fine-grained composed image retrieval benchmark is introduced through image editing techniques, revealing significant capability gaps in existing multimodal models and exposing limitations of current benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.",
            "score": 12,
            "issue_id": 727,
            "pub_date": "2026-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "d4ddd4000601753e",
            "authors": [
                "Tingyu Song",
                "Yanzhao Zhang",
                "Mingxin Li",
                "Zhuoning Guo",
                "Dingkun Long",
                "Pengjun Xie",
                "Siyue Zhang",
                "Yilun Zhao",
                "Shu Wu"
            ],
            "affiliations": [
                "CASIA",
                "HKUST(GZ)",
                "NTU",
                "Tongyi Lab, Alibaba Group",
                "UCAS",
                "Yale"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16125.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¢Ğ¾Ğ½ĞºĞ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… EDIR Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (CIR) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿ÑÑ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ¿ÑÑ‚Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹. ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ‚Ñ€Ğ¸Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ…: Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Bridging the Gap in Composed Image Retrieval with EDIR",
                    "desc": "This paper introduces a new benchmark for fine-grained composed image retrieval (CIR) using image editing techniques. The existing benchmarks are limited in their query categories and do not reflect the complexities of real-world scenarios. The new benchmark, called EDIR, includes 5,000 high-quality queries across various categories and subcategories, revealing significant performance gaps in current multimodal models. The study highlights the limitations of existing benchmarks and demonstrates the challenges faced by state-of-the-art models in accurately retrieving composed images."
                },
                "zh": {
                    "title": "ç»†ç²’åº¦ç»„åˆå›¾åƒæ£€ç´¢çš„æ–°åŸºå‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„ç»†ç²’åº¦ç»„åˆå›¾åƒæ£€ç´¢åŸºå‡†ï¼Œåˆ©ç”¨å›¾åƒç¼–è¾‘æŠ€æœ¯æ­ç¤ºäº†ç°æœ‰å¤šæ¨¡æ€æ¨¡å‹çš„èƒ½åŠ›å·®è·å’Œå½“å‰åŸºå‡†çš„å±€é™æ€§ã€‚ç»„åˆå›¾åƒæ£€ç´¢ï¼ˆCIRï¼‰æ˜¯å¤šæ¨¡æ€ç†è§£ä¸­çš„ä¸€ä¸ªé‡è¦ä¸”å¤æ‚çš„ä»»åŠ¡ï¼Œç°æœ‰åŸºå‡†é€šå¸¸æŸ¥è¯¢ç±»åˆ«æœ‰é™ï¼Œæ— æ³•æ»¡è¶³çœŸå®åœºæ™¯çš„å¤šæ ·åŒ–éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€è¯„ä¼°å·®è·ï¼Œæˆ‘ä»¬æ„å»ºäº†EDIRåŸºå‡†ï¼ŒåŒ…å«5000ä¸ªé«˜è´¨é‡æŸ¥è¯¢ï¼Œæ¶µç›–äº”ä¸ªä¸»è¦ç±»åˆ«å’Œåäº”ä¸ªå­ç±»åˆ«ã€‚é€šè¿‡å¯¹13ç§å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹çš„ç»¼åˆè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æ‰€æœ‰å­ç±»åˆ«ä¸­çš„è¡¨ç°ä¹Ÿä¸ä¸€è‡´ï¼Œçªæ˜¾äº†æˆ‘ä»¬åŸºå‡†çš„ä¸¥æ ¼æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.15892",
            "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model",
            "url": "https://huggingface.co/papers/2601.15892",
            "abstract": "Stable-DiffCoder demonstrates superior code modeling performance compared to autoregressive baselines through block diffusion continual pretraining and efficient training mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.",
            "score": 10,
            "issue_id": 727,
            "pub_date": "2026-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "970db0ef18f55ae2",
            "authors": [
                "Chenghao Fan",
                "Wen Heng",
                "Bo Li",
                "Sichen Liu",
                "Yuxuan Song",
                "Jing Su",
                "Xiaoye Qu",
                "Kai Shen",
                "Wei Wei"
            ],
            "affiliations": [
                "ByteDance",
                "Huazhong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.15892.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#optimization",
                    "#plp",
                    "#diffusion",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€Â­ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Stable-DiffCoder â€” ÑÑ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºÑƒ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµĞ²Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Stable-DiffCoder Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€Â­ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Code Generation with Block Diffusion!",
                    "desc": "Stable-DiffCoder is a new model for generating code that outperforms traditional autoregressive models by using a method called block diffusion continual pretraining. This approach allows the model to generate code in blocks rather than one piece at a time, which improves efficiency and data reuse. The model incorporates a special training process that includes a warmup phase and a noise schedule to ensure stable learning. Overall, Stable-DiffCoder shows better performance on various coding tasks and enhances the ability to work with structured code, making it beneficial for less common programming languages."
                },
                "zh": {
                    "title": "ç¨³å®šæ‰©æ•£ç¼–ç å™¨ï¼šè¶…è¶Šè‡ªå›å½’çš„ä»£ç å»ºæ¨¡æ–°æ–¹æ³•",
                    "desc": "Stable-DiffCoderæ˜¯ä¸€ç§åŸºäºå—æ‰©æ•£çš„ä»£ç æ¨¡å‹ï¼Œå±•ç¤ºäº†æ¯”è‡ªå›å½’åŸºçº¿æ›´ä¼˜è¶Šçš„ä»£ç å»ºæ¨¡æ€§èƒ½ã€‚å®ƒé€šè¿‡å—æ‰©æ•£æŒç»­é¢„è®­ç»ƒå’Œé«˜æ•ˆçš„è®­ç»ƒæœºåˆ¶ï¼Œé‡æ–°åˆ©ç”¨äº†Seed-Coderæ¶æ„å’Œæ•°æ®ã€‚è¯¥æ¨¡å‹åœ¨ç›¸åŒçš„æ•°æ®å’Œæ¶æ„ä¸‹ï¼Œåœ¨å¤šç§ä»£ç åŸºå‡†æµ‹è¯•ä¸­æ•´ä½“è¶…è¶Šäº†è‡ªå›å½’æ¨¡å‹ã€‚é€šè¿‡æ‰©æ•£è®­ç»ƒï¼ŒStable-DiffCoderä¸ä»…æå‡äº†ä»£ç å»ºæ¨¡è´¨é‡ï¼Œè¿˜æ”¹å–„äº†ç»“æ„åŒ–ä»£ç çš„ç¼–è¾‘å’Œæ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹ä½èµ„æºç¼–ç¨‹è¯­è¨€çš„æ”¯æŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.15621",
            "title": "Qwen3-TTS Technical Report",
            "url": "https://huggingface.co/papers/2601.15621",
            "abstract": "The Qwen3-TTS series presents advanced multilingual text-to-speech models with voice cloning and controllable speech generation capabilities, utilizing dual-track LM architecture and specialized speech tokenizers for efficient streaming synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.",
            "score": 7,
            "issue_id": 727,
            "pub_date": "2026-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "5ce347d656d62b1f",
            "authors": [
                "Hangrui Hu",
                "Xinfa Zhu",
                "Ting He",
                "Dake Guo",
                "Bin Zhang",
                "Xiong Wang",
                "Zhifang Guo",
                "Ziyue Jiang",
                "Hongkun Hao",
                "Zishan Guo",
                "Xinyu Zhang",
                "Pei Zhang",
                "Baosong Yang",
                "Jin Xu",
                "Jingren Zhou",
                "Junyang Lin"
            ],
            "affiliations": [
                "Qwen Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.15621.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#multimodal",
                    "#audio",
                    "#dataset",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ¤",
                "ru": {
                    "title": "Ğ¢Ñ€Ñ‘Ñ…ÑĞµĞºÑƒĞ½Ğ´Ğ½Ğ¾Ğµ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ñ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹",
                    "desc": "Qwen3-TTS â€” ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ·Ğ° 3 ÑĞµĞºÑƒĞ½Ğ´Ñ‹ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²ÑƒÑ…ĞºĞ°Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ²Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¹Ğ·ĞµÑ€Ğ°: Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ (25Hz) Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸, Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ (12Hz) Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ° Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ñ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¼ Ğ¿Ğ°ĞºĞµÑ‚Ğ¾Ğ¼ Ğ·Ğ° 97 Ğ¼Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‡Ğ°ÑĞ¾Ğ² Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° 10 ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ Ğ¾Ğ±Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¹Ğ·ĞµÑ€Ğ° Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ĞµĞ¹ Apache 2.0 Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Speech: Multilingual TTS with Voice Cloning and Control",
                    "desc": "The Qwen3-TTS series introduces cutting-edge multilingual text-to-speech models that excel in voice cloning and speech generation control. These models utilize a dual-track language model architecture and specialized speech tokenizers to enable efficient streaming synthesis. With training on over 5 million hours of speech data in 10 languages, they support rapid voice cloning and allow for detailed manipulation of the generated speech. The release of these models and tokenizers under the Apache 2.0 license aims to promote further research and development in the community."
                },
                "zh": {
                    "title": "å…ˆè¿›çš„å¤šè¯­è¨€æ–‡æœ¬è½¬è¯­éŸ³æŠ€æœ¯",
                    "desc": "Qwen3-TTSç³»åˆ—æ˜¯å…ˆè¿›çš„å¤šè¯­è¨€æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ï¼Œå…·å¤‡è¯­éŸ³å…‹éš†å’Œå¯æ§è¯­éŸ³ç”Ÿæˆçš„èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åŒè½¨è¯­è¨€æ¨¡å‹æ¶æ„å’Œä¸“é—¨çš„è¯­éŸ³æ ‡è®°å™¨ï¼Œå®ç°é«˜æ•ˆçš„æµå¼åˆæˆã€‚Qwen3-TTSæ”¯æŒ3ç§’çš„è¯­éŸ³å…‹éš†å’ŒåŸºäºæè¿°çš„æ§åˆ¶ï¼Œèƒ½å¤Ÿåˆ›å»ºå…¨æ–°çš„å£°éŸ³å¹¶ç²¾ç»†è°ƒæ•´è¾“å‡ºè¯­éŸ³ã€‚ç»è¿‡è¶…è¿‡500ä¸‡å°æ—¶çš„è¯­éŸ³æ•°æ®è®­ç»ƒï¼ŒQwen3-TTSåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¨åŠ¨äº†ç¤¾åŒºçš„ç ”ç©¶ä¸å¼€å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.15369",
            "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
            "url": "https://huggingface.co/papers/2601.15369",
            "abstract": "An advanced vision encoder named OpenVision 3 learns a unified visual representation for both image understanding and generation by combining VAE-compressed image latents with ViT architecture and joint optimization of reconstruction and semantic signals.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling.",
            "score": 7,
            "issue_id": 727,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "34ad2f9e33915796",
            "authors": [
                "Letian Zhang",
                "Sucheng Ren",
                "Yanqing Liu",
                "Xianhang Li",
                "Zeyu Wang",
                "Yuyin Zhou",
                "Huaxiu Yao",
                "Zeyu Zheng",
                "Weili Nie",
                "Guilin Liu",
                "Zhiding Yu",
                "Cihang Xie"
            ],
            "affiliations": [
                "JHU",
                "NVIDIA",
                "UC Berkeley",
                "UC Santa Cruz",
                "UNC-Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.15369.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#open_source",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OpenVision 3 â€” Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ°Ğº Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ VAE-Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ñ‹ Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ¼ ViT, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸. Ğ­Ğ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ViT-VAE Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ CLIP."
                },
                "en": {
                    "title": "Unified Visual Representation for Understanding and Generation",
                    "desc": "OpenVision 3 is an advanced vision encoder that creates a unified visual representation for both understanding and generating images. It combines Variational Autoencoder (VAE) compressed image latents with a Vision Transformer (ViT) architecture, optimizing for both reconstruction and semantic understanding. The model is trained to reconstruct images while also learning from contrastive and image-captioning tasks, enhancing its ability to capture meaningful features. Extensive evaluations show that OpenVision 3 performs competitively with existing models, demonstrating its effectiveness in multimodal tasks and image generation."
                },
                "zh": {
                    "title": "ç»Ÿä¸€è§†è§‰è¡¨ç¤ºçš„æœªæ¥",
                    "desc": "OpenVision 3æ˜¯ä¸€ç§å…ˆè¿›çš„è§†è§‰ç¼–ç å™¨ï¼Œå®ƒé€šè¿‡ç»“åˆå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å‹ç¼©çš„å›¾åƒæ½œå˜é‡å’Œè§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰æ¶æ„ï¼Œå­¦ä¹ ç»Ÿä¸€çš„è§†è§‰è¡¨ç¤ºï¼Œé€‚ç”¨äºå›¾åƒç†è§£å’Œç”Ÿæˆã€‚è¯¥æ¨¡å‹çš„æ ¸å¿ƒæ¶æ„ç®€å•ï¼Œå°†VAEå‹ç¼©çš„å›¾åƒæ½œå˜é‡è¾“å…¥ViTç¼–ç å™¨ï¼Œå¹¶è®­ç»ƒå…¶è¾“å‡ºä»¥æ”¯æŒé‡å»ºå’Œè¯­ä¹‰ä¿¡å·çš„åŒé‡è§’è‰²ã€‚é€šè¿‡åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­è”åˆä¼˜åŒ–é‡å»ºå’Œè¯­ä¹‰é©±åŠ¨çš„ä¿¡å·ï¼Œç¼–ç å™¨å­¦ä¹ åˆ°çš„è¡¨ç¤ºèƒ½å¤Ÿåœ¨è¿™ä¸¤ç§ä»»åŠ¡ä¸­è‰¯å¥½ååŒå’Œæ³›åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOpenVision 3åœ¨å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œæ¨åŠ¨äº†ç»Ÿä¸€å»ºæ¨¡çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16163",
            "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
            "url": "https://huggingface.co/papers/2601.16163",
            "abstract": "A pretrained video model is adapted into a robot policy through single-stage post-training, enabling direct action generation and planning capabilities without architectural modifications.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/",
            "score": 4,
            "issue_id": 727,
            "pub_date": "2026-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "e976977b8f54e5e1",
            "authors": [
                "Moo Jin Kim",
                "Yihuai Gao",
                "Tsung-Yi Lin",
                "Yen-Chen Lin",
                "Yunhao Ge",
                "Grace Lam",
                "Percy Liang",
                "Shuran Song",
                "Ming-Yu Liu",
                "Chelsea Finn",
                "Jinwei Gu"
            ],
            "affiliations": [
                "NVIDIA",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16163.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#robotics",
                    "#video",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ†Ğ¸Ğ¸ Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚-Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ² Ğ¾Ğ´Ğ¸Ğ½ ÑÑ‚Ğ°Ğ¿ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Cosmos Policy â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ğº ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Cosmos Policy Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LIBERO Ğ¸ RoboCasa, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ñ€ÑƒĞºĞ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°-Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ."
                },
                "en": {
                    "title": "Transforming Video Models into Robot Action Policies with Ease",
                    "desc": "This paper presents Cosmos Policy, a method that adapts a pretrained video model for use in robotics without changing its architecture. By using a single-stage post-training approach, it allows the model to generate robot actions directly from video data. Cosmos Policy captures complex action distributions and predicts future states, enabling effective planning during robot operations. The method shows superior performance in various benchmarks, demonstrating its efficiency compared to other existing policies."
                },
                "zh": {
                    "title": "ç®€åŒ–æœºå™¨äººç­–ç•¥çš„é¢„è®­ç»ƒè§†é¢‘æ¨¡å‹é€‚é…",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCosmos Policyçš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡å•é˜¶æ®µåè®­ç»ƒå°†é¢„è®­ç»ƒçš„è§†é¢‘æ¨¡å‹ï¼ˆCosmos-Predict2ï¼‰é€‚é…ä¸ºæœºå™¨äººç­–ç•¥ã€‚è¯¥æ–¹æ³•æ— éœ€å¯¹æ¨¡å‹æ¶æ„è¿›è¡Œä¿®æ”¹ï¼Œç›´æ¥åˆ©ç”¨æœºå™¨äººæ¼”ç¤ºæ•°æ®ç”Ÿæˆæœºå™¨äººåŠ¨ä½œã€‚Cosmos Policyèƒ½å¤Ÿç”Ÿæˆæœªæ¥çŠ¶æ€å›¾åƒå’Œä»·å€¼ï¼ˆé¢„æœŸç´¯ç§¯å¥–åŠ±ï¼‰ï¼Œä»è€Œåœ¨æµ‹è¯•æ—¶è¿›è¡Œæ›´é«˜æˆåŠŸç‡çš„åŠ¨ä½œè½¨è¿¹è§„åˆ’ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒCosmos Policyåœ¨LIBEROå’ŒRoboCasaæ¨¡æ‹ŸåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒæˆåŠŸç‡è¾¾åˆ°98.5%å’Œ67.1%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.15549",
            "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations",
            "url": "https://huggingface.co/papers/2601.15549",
            "abstract": "VIOLA is a label-efficient framework that combines minimal expert supervision with abundant unlabeled data to enable effective multimodal large language model adaptation in low-resource video domains through density-uncertainty-weighted sampling and confidence-aware retrieval mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.",
            "score": 2,
            "issue_id": 730,
            "pub_date": "2026-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "5d60289bcd783dcf",
            "authors": [
                "Ryo Fujii",
                "Hideo Saito",
                "Ryo Hachiuma"
            ],
            "affiliations": [
                "Keio AI Research Center",
                "Keio University",
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.15549.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#video",
                    "#data",
                    "#training",
                    "#multimodal",
                    "#low_resource"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹",
                    "desc": "VIOLA â€” ÑÑ‚Ğ¾ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ Ğ½ĞµĞ¼ĞµÑ‡ĞµĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¼ĞµÑ‡ĞµĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¸ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ´ĞµĞ²ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ VIOLA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Efficient Video Adaptation with Minimal Labels",
                    "desc": "VIOLA is a novel framework designed to enhance the adaptation of multimodal large language models (MLLMs) in video domains with limited labeled data. It combines minimal expert supervision with a wealth of unlabeled data, utilizing density-uncertainty-weighted sampling to select the most informative samples for annotation. Additionally, it employs confidence-aware retrieval mechanisms to effectively manage and utilize unlabeled data, ensuring that the model can distinguish between reliable labels and noisy pseudo-labels. Through extensive testing, VIOLA has shown to significantly improve performance in low-resource environments, making it a valuable tool for real-world applications."
                },
                "zh": {
                    "title": "VIOLAï¼šä½èµ„æºè§†é¢‘é¢†åŸŸçš„é«˜æ•ˆé€‚åº”æ¡†æ¶",
                    "desc": "VIOLAæ˜¯ä¸€ä¸ªæ ‡ç­¾é«˜æ•ˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æœ€å°åŒ–ä¸“å®¶ç›‘ç£å’Œåˆ©ç”¨ä¸°å¯Œçš„æœªæ ‡è®°æ•°æ®ï¼Œæ¥å®ç°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºè§†é¢‘é¢†åŸŸçš„æœ‰æ•ˆé€‚åº”ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¯†åº¦-ä¸ç¡®å®šæ€§åŠ æƒé‡‡æ ·å’ŒåŸºäºç½®ä¿¡åº¦çš„æ£€ç´¢æœºåˆ¶ï¼Œä»¥æœ€å¤§åŒ–æ³¨é‡Šé¢„ç®—çš„æ•ˆç‡ã€‚é€šè¿‡è¯†åˆ«å¤šæ ·æ€§ã€ä»£è¡¨æ€§å’Œä¿¡æ¯é‡å…¼å…·çš„æ ·æœ¬ï¼ŒVIOLAèƒ½å¤Ÿæœ‰æ•ˆé€‰æ‹©æ ·æœ¬ï¼ŒåŒæ—¶é¿å…é€‰æ‹©è§†è§‰å¼‚å¸¸å€¼ã€‚æ­¤å¤–ï¼ŒVIOLAè¿˜æ„å»ºäº†ä¸€ä¸ªæ··åˆæ± ï¼Œåˆ©ç”¨ç½®ä¿¡åº¦æ„ŸçŸ¥çš„æ£€ç´¢å’Œæç¤ºï¼Œå¸®åŠ©æ¨¡å‹åœ¨é€‚åº”è¿‡ç¨‹ä¸­åŒºåˆ†çœŸå®æ ‡ç­¾å’Œå™ªå£°ä¼ªæ ‡ç­¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.11868",
            "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces",
            "url": "https://huggingface.co/papers/2601.11868",
            "abstract": "Terminal-Bench 2.0 presents a challenging benchmark with 89 terminal-based tasks to evaluate AI agents' capabilities in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .",
            "score": 1,
            "issue_id": 727,
            "pub_date": "2026-01-17",
            "pub_date_card": {
                "ru": "17 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 17",
                "zh": "1æœˆ17æ—¥"
            },
            "hash": "d9072e7a3723fbd8",
            "authors": [
                "Mike A. Merrill",
                "Alexander G. Shaw",
                "Nicholas Carlini",
                "Boxuan Li",
                "Harsh Raj",
                "Ivan Bercovich",
                "Lin Shi",
                "Jeong Yeon Shin",
                "Thomas Walshe",
                "E. Kelly Buchanan",
                "Junhong Shen",
                "Guanghao Ye",
                "Haowei Lin",
                "Jason Poulos",
                "Maoyu Wang",
                "Marianna Nezhurina",
                "Jenia Jitsev",
                "Di Lu",
                "Orfeas Menis Mastromichalakis",
                "Zhiwei Xu",
                "Zizhao Chen",
                "Yue Liu",
                "Robert Zhang",
                "Leon Liangyu Chen",
                "Anurag Kashyap",
                "Jan-Lucas Uslu",
                "Jeffrey Li",
                "Jianbo Wu",
                "Minghao Yan",
                "Song Bian",
                "Vedang Sharma",
                "Ke Sun",
                "Steven Dillmann",
                "Akshay Anand",
                "Andrew Lanpouthakoun",
                "Bardia Koopah",
                "Changran Hu",
                "Etash Guha",
                "Gabriel H. S. Dreiman",
                "Jiacheng Zhu",
                "Karl Krauth",
                "Li Zhong",
                "Niklas Muennighoff",
                "Robert Amanfu",
                "Shangyin Tan",
                "Shreyas Pimpalgaonkar",
                "Tushar Aggarwal",
                "Xiangning Lin",
                "Xin Lan",
                "Xuandong Zhao",
                "Yiqing Liang",
                "Yuanli Wang",
                "Zilong Wang",
                "Changzhi Zhou",
                "David Heineman",
                "Hange Liu",
                "Harsh Trivedi",
                "John Yang",
                "Junhong Lin",
                "Manish Shetty",
                "Michael Yang",
                "Nabil Omi",
                "Negin Raoof",
                "Shanda Li",
                "Terry Yue Zhuo",
                "Wuwei Lin",
                "Yiwei Dai",
                "Yuxin Wang",
                "Wenhao Chai",
                "Shang Zhou",
                "Dariush Wahdany",
                "Ziyu She",
                "Jiaming Hu",
                "Zhikang Dong",
                "Yuxuan Zhu",
                "Sasha Cui",
                "Ahson Saiyed",
                "ArinbjÃ¶rn Kolbeinsson",
                "Jesse Hu",
                "Christopher Michael Rytting",
                "Ryan Marten",
                "Yixin Wang",
                "Alex Dimakis",
                "Andy Konwinski",
                "Ludwig Schmidt"
            ],
            "affiliations": [
                "Amazon",
                "Anthropic",
                "Carnegie Mellon University",
                "Cornell University",
                "Independent",
                "JSC, FZJ",
                "LAION",
                "Laude Institute",
                "Massachusetts Institute of Technology",
                "Moonshot AI",
                "National Technical University of Athens",
                "National University of Singapore",
                "Nerion",
                "Northeastern University",
                "Peking University",
                "Reflection AI",
                "Snorkel AI",
                "Stanford University",
                "Tencent",
                "University of California, Santa Barbara",
                "University of Michigan",
                "University of Texas at Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.11868.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#agents"
                ],
                "emoji": "ğŸ–¥ï¸",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚",
                    "desc": "Terminal-Bench 2.0 â€” ÑÑ‚Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 89 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ AI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ·Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ»Ğ¸Ğ±Ğ¾ Ğ½Ğµ Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ»Ğ¸Ğ±Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ, Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ğ¾Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 65% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ, Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Challenging AI Agents with Real-World Terminal Tasks",
                    "desc": "Terminal-Bench 2.0 is a new benchmark designed to test AI agents on 89 challenging tasks that mimic real-world terminal environments. These tasks are specifically created to evaluate the capabilities of advanced AI models in completing complex, long-term objectives. The benchmark reveals that even the best current models score below 65%, highlighting significant areas for improvement. By providing a dataset and evaluation tools, this work aims to support further research and development in AI task completion."
                },
                "zh": {
                    "title": "æŒ‘æˆ˜æé™ï¼Œæå‡AIèƒ½åŠ›ï¼",
                    "desc": "Terminal-Bench 2.0 æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«89ä¸ªåŸºäºç»ˆç«¯çš„ä»»åŠ¡ï¼Œç”¨äºè¯„ä¼°äººå·¥æ™ºèƒ½ä»£ç†åœ¨ç°å®åœºæ™¯ä¸­çš„èƒ½åŠ›ã€‚è¿™äº›ä»»åŠ¡çµæ„Ÿæ¥æºäºçœŸå®å·¥ä½œæµç¨‹ä¸­çš„é—®é¢˜ï¼Œæ—¨åœ¨æµ‹é‡å‰æ²¿æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰çš„å‰æ²¿æ¨¡å‹å’Œä»£ç†åœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸­çš„å¾—åˆ†ä½äº65%ï¼Œå¹¶è¿›è¡Œäº†é”™è¯¯åˆ†æä»¥è¯†åˆ«æ”¹è¿›çš„æ–¹å‘ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†æ•°æ®é›†å’Œè¯„ä¼°å·¥å…·ï¼Œä»¥æ”¯æŒå¼€å‘è€…å’Œç ”ç©¶äººå‘˜çš„åç»­å·¥ä½œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.16004",
            "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
            "url": "https://huggingface.co/papers/2601.16004",
            "abstract": "Implementation and benchmarking of quantum circuits for estimating operational inter-branch communication witnesses on IBM Quantum hardware demonstrates visibility and coherence witness measurements under realistic device conditions.  \t\t\t\t\tAI-generated summary \t\t\t\t We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts.   Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.   This work does not test or discriminate among interpretations of quantum mechanics. Instead, it provides a reproducible operational constraint pipeline for evaluating detectability of non-ideal channels relative to calibrated device noise.",
            "score": 0,
            "issue_id": 727,
            "pub_date": "2026-01-22",
            "pub_date_card": {
                "ru": "22 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 22",
                "zh": "1æœˆ22æ—¥"
            },
            "hash": "53270030d77ae09e",
            "authors": [
                "Christopher Altman"
            ],
            "affiliations": [
                "Astradyne"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.16004.jpg",
            "data": {
                "categories": [],
                "emoji": "âš›ï¸",
                "ru": {
                    "title": "ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ²Ğ¸Ğ´ĞµÑ‚ĞµĞ»Ğ¸ Ğ¼ĞµĞ¶Ğ²ĞµÑ‚Ğ²ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ½Ğ° ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğµ IBM ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… ÑÑ…ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ²Ğ¸Ğ´ĞµÑ‚ĞµĞ»ĞµĞ¹ Ğ¼ĞµĞ¶Ğ²ĞµÑ‚Ğ²ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑÑ…, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‚Ğ¸Ğ¿Ğ° Ğ’Ğ¸Ğ³Ğ½ĞµÑ€Ğ°-Ğ´Ñ€ÑƒĞ³Ğ°. ĞŸÑ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ´ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°Ñ‚ĞµĞ»Ñ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºÑƒĞ±Ğ¸Ñ‚Ğ¾Ğ¼, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¼ ÑˆÑƒĞ¼Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸, Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°Ñ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ 0.877 Ğ¸ ÑĞ²Ğ¸Ğ´ĞµÑ‚ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ 0.840 Ğ¸ -0.811. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¹ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Benchmarking Quantum Circuits for Communication Witnesses",
                    "desc": "This paper presents the implementation and benchmarking of quantum circuits on IBM Quantum hardware to estimate operational inter-branch communication witnesses. The authors utilize a five-qubit circuit to analyze correlations in measurement records from Wigner's-friend-style circuits, focusing on the effects of realistic device noise. They report visibility and coherence witness measurements, highlighting the circuit's performance under various noise conditions. The study aims to establish a reproducible framework for assessing the detectability of non-ideal quantum channels without addressing interpretations of quantum mechanics."
                },
                "zh": {
                    "title": "é‡å­ç”µè·¯å®ç°ä¸åŸºå‡†æµ‹è¯•çš„åˆ›æ–°",
                    "desc": "æœ¬æ–‡å®ç°å¹¶åŸºå‡†æµ‹è¯•äº†åœ¨IBMé‡å­ç¡¬ä»¶ä¸Šä¼°è®¡æ“ä½œæ€§è·¨åˆ†æ”¯é€šä¿¡è§è¯çš„é‡å­ç”µè·¯ã€‚æˆ‘ä»¬é‡‡ç”¨äº†Violarisæå‡ºçš„ç”µè·¯ç³»åˆ—ï¼Œå®šä¹‰ä¸ºç”±ç¼–è¯‘çš„Wigneræœ‹å‹é£æ ¼ç”µè·¯äº§ç”Ÿçš„ç»å…¸æµ‹é‡è®°å½•ä¸­çš„ç›¸å…³æ€§ã€‚é€šè¿‡åœ¨å•ä¸ªç”µè·¯ä¸­å®ç°äº”é‡å­æ¯”ç‰¹å®ä¾‹ï¼Œæˆ‘ä»¬è¯„ä¼°äº†åœ¨ç°å®è®¾å¤‡å™ªå£°å’Œç¼–è¯‘çº¦æŸä¸‹çš„è¡Œä¸ºã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œèƒ½è§åº¦å’Œç›¸å¹²æ€§è§è¯åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„è¡¨ç°ï¼Œä¸ºè¯„ä¼°éç†æƒ³é€šé“çš„å¯æ£€æµ‹æ€§æä¾›äº†å¯é‡å¤çš„æ“ä½œçº¦æŸç®¡é“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2601.15440",
            "title": "Numba-Accelerated 2D Diffusion-Limited Aggregation: Implementation and Fractal Characterization",
            "url": "https://huggingface.co/papers/2601.15440",
            "abstract": "We present dla-ideal-solver, a high-performance framework for simulating two-dimensional Diffusion-Limited Aggregation (DLA) using Numba-accelerated Python. By leveraging just-in-time (JIT) compilation, we achieve computational throughput comparable to legacy static implementations while retaining high-level flexibility. We investigate the Laplacian growth instability across varying injection geometries and walker concentrations. Our analysis confirms the robustness of the standard fractal dimension D_f approx 1.71 for dilute regimes, consistent with the Witten-Sander universality class. However, we report a distinct crossover to Eden-like compact growth (D_f approx 1.87) in high-density environments, attributed to the saturation of the screening length. Beyond standard mass-radius scaling, we employ generalized RÃ©nyi dimensions and lacunarity metrics to quantify the monofractal character and spatial heterogeneity of the aggregates. This work establishes a reproducible, open-source testbed for exploring phase transitions in non-equilibrium statistical mechanics.",
            "score": 0,
            "issue_id": 731,
            "pub_date": "2026-01-21",
            "pub_date_card": {
                "ru": "21 ÑĞ½Ğ²Ğ°Ñ€Ñ",
                "en": "January 21",
                "zh": "1æœˆ21æ—¥"
            },
            "hash": "41b551f8998508a8",
            "authors": [
                "Sandy H. S. Herho",
                "Faiz R. Fajary",
                "Iwan P. Anwar",
                "Faruq Khadami",
                "Nurjanna J. Trilaksono",
                "Rusmawan Suwarman",
                "Dasapta E. Irawan"
            ],
            "affiliations": [
                "Bandung Institute of Technology",
                "Ronin Institute for Independent Scholarship",
                "State University of New York"
            ],
            "pdf_title_img": "assets/pdf/title_img/2601.15440.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸŒ¿",
                "ru": {
                    "title": "ĞÑ‚ Ñ…Ğ°Ğ¾ÑĞ° Ğº Ğ¿Ğ¾Ñ€ÑĞ´ĞºÑƒ: Ñ„Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº dla-ideal-solver Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ JIT-ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ° Numba. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ½ĞµÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ»Ğ°Ğ¿Ğ»Ğ°ÑĞ¸Ğ°Ğ½Ğ° Ñ€Ğ¾ÑÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ÑÑ… Ğ²Ğ¿Ñ€Ñ‹ÑĞºĞ° Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ… Ñ‡Ğ°ÑÑ‚Ğ¸Ñ†, Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°Ñ Ñ„Ñ€Ğ°ĞºÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ D_f â‰ˆ 1.71 Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ² Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ’Ğ¸Ñ‚Ñ‚ĞµĞ½Ğ°-Ğ¡Ğ°Ğ½Ğ´ĞµÑ€Ğ°. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğº ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ¾ÑÑ‚Ñƒ Ñ‚Ğ¸Ğ¿Ğ° Ğ­Ğ´ĞµĞ½Ğ° (D_f â‰ˆ 1.87) Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ğ¸Ğ·-Ğ·Ğ° Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ğ°Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. Ğ”Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ½Ğ¾Ñ„Ñ€Ğ°ĞºÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ° Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ ĞµĞ½ÑŒĞ¸ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ»Ğ°ĞºÑƒĞ½Ğ°Ñ€Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Accelerating DLA Simulations: Unveiling Growth Patterns in Fractals",
                    "desc": "The paper introduces dla-ideal-solver, a framework designed for simulating two-dimensional Diffusion-Limited Aggregation (DLA) using Numba to enhance performance through just-in-time (JIT) compilation. It explores the effects of different injection geometries and walker concentrations on Laplacian growth instability, confirming a standard fractal dimension of approximately 1.71 in dilute conditions. The study also identifies a transition to Eden-like compact growth with a fractal dimension of about 1.87 in denser environments, linked to the saturation of the screening length. Additionally, the authors utilize generalized RÃ©nyi dimensions and lacunarity metrics to analyze the aggregates' monofractal nature and spatial variability, providing a valuable open-source resource for studying phase transitions in non-equilibrium statistical mechanics."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨¡æ‹ŸäºŒç»´æ‰©æ•£é™åˆ¶èšé›†çš„æ¡†æ¶",
                    "desc": "æˆ‘ä»¬æå‡ºäº†dla-ideal-solverï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„æ¡†æ¶ï¼Œç”¨äºæ¨¡æ‹ŸäºŒç»´æ‰©æ•£é™åˆ¶èšé›†ï¼ˆDLAï¼‰ï¼Œä½¿ç”¨NumbaåŠ é€Ÿçš„Pythonå®ç°ã€‚é€šè¿‡åˆ©ç”¨å³æ—¶ç¼–è¯‘ï¼ˆJITï¼‰ï¼Œæˆ‘ä»¬å®ç°äº†ä¸ä¼ ç»Ÿé™æ€å®ç°ç›¸å½“çš„è®¡ç®—ååé‡ï¼ŒåŒæ—¶ä¿æŒäº†é«˜å±‚æ¬¡çš„çµæ´»æ€§ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸åŒæ³¨å…¥å‡ ä½•å½¢çŠ¶å’Œè¡Œèµ°è€…æµ“åº¦ä¸‹çš„æ‹‰æ™®æ‹‰æ–¯ç”Ÿé•¿ä¸ç¨³å®šæ€§ï¼Œç¡®è®¤äº†ç¨€è–„çŠ¶æ€ä¸‹æ ‡å‡†åˆ†å½¢ç»´æ•°D_fçº¦ä¸º1.71ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œåœ¨é«˜å¯†åº¦ç¯å¢ƒä¸­ï¼Œç”Ÿé•¿æ¨¡å¼ä¼šå‘ç”Ÿæ˜æ˜¾çš„è½¬å˜ï¼Œå‘ˆç°å‡ºç±»ä¼¼äºä¼Šç™»ï¼ˆEdenï¼‰ç´§å‡‘ç”Ÿé•¿çš„ç‰¹å¾ï¼Œåˆ†å½¢ç»´æ•°D_fçº¦ä¸º1.87ï¼Œè¿™å½’å› äºå±è”½é•¿åº¦çš„é¥±å’Œã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-01-22.html",
    "link_next": "2026-01-26.html",
    "link_month": "2026-01.html",
    "short_date_prev": {
        "ru": "22.01",
        "en": "01/22",
        "zh": "1æœˆ22æ—¥"
    },
    "short_date_next": {
        "ru": "26.01",
        "en": "01/26",
        "zh": "1æœˆ26æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 1,
        "#cv": 4,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 1,
        "#inference": 1,
        "#3d": 0,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 8,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 4,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 2,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 3,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 2
    }
}