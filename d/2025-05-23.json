{
    "date": {
        "ru": "23 –º–∞—è",
        "en": "May 23",
        "zh": "5Êúà23Êó•"
    },
    "time_utc": "2025-05-24 01:53",
    "weekday": 4,
    "issue_id": 3936,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.16938",
            "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop\n  System from Hypothesis to Verification",
            "url": "https://huggingface.co/papers/2505.16938",
            "abstract": "Artificial Intelligence (AI) is accelerating the transformation of scientific research paradigms, not only enhancing research efficiency but also driving innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework to conduct Autonomous Scientific Research (ASR) across various scientific research fields, enabling researchers to tackle complicated problems in these fields with unprecedented speed and precision. NovelSeek highlights three key advantages: 1) Scalability: NovelSeek has demonstrated its versatility across 12 scientific research tasks, capable of generating innovative ideas to enhance the performance of baseline code. 2) Interactivity: NovelSeek provides an interface for human expert feedback and multi-agent interaction in automated end-to-end processes, allowing for the seamless integration of domain expert knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in several scientific fields with significantly less time cost compared to human efforts. For instance, in reaction yield prediction, it increased from 27.6% to 35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from 0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation, precision advanced from 78.8% to 81.0% in a mere 30 hours.",
            "score": 86,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "39a42fc40deae7a6",
            "authors": [
                "NovelSeek Team",
                "Bo Zhang",
                "Shiyang Feng",
                "Xiangchao Yan",
                "Jiakang Yuan",
                "Zhiyin Yu",
                "Xiaohan He",
                "Songtao Huang",
                "Shaowei Hou",
                "Zheng Nie",
                "Zhilong Wang",
                "Jinyao Liu",
                "Runmin Ma",
                "Tianshuo Peng",
                "Peng Ye",
                "Dongzhan Zhou",
                "Shufei Zhang",
                "Xiaosong Wang",
                "Yilan Zhang",
                "Meng Li",
                "Zhongying Tu",
                "Xiangyu Yue",
                "Wangli Ouyang",
                "Bowen Zhou",
                "Lei Bai"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16938.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#multimodal",
                    "#agents",
                    "#science"
                ],
                "emoji": "üß¨",
                "ru": {
                    "title": "NovelSeek: –ò–ò-—É—Å–∫–æ—Ä–∏—Ç–µ–ª—å –Ω–∞—É—á–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç–∏–π",
                    "desc": "NovelSeek - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –û–Ω–∞ –æ–±–ª–∞–¥–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å—é, –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ 12 –Ω–∞—É—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –ø–æ–∑–≤–æ–ª—è—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è. NovelSeek –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –¥–æ—Å—Ç–∏–≥–∞—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π –∑–∞ –∫–æ—Ä–æ—Ç–∫–æ–µ –≤—Ä–µ–º—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –∑–∞–¥–∞—á–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—ã—Ö–æ–¥–∞ —Ä–µ–∞–∫—Ü–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç—å –≤—ã—Ä–æ—Å–ª–∞ —Å 27.6% –¥–æ 35.4% –≤—Å–µ–≥–æ –∑–∞ 12 —á–∞—Å–æ–≤ —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã."
                },
                "en": {
                    "title": "Revolutionizing Research with Autonomous AI Frameworks",
                    "desc": "This paper presents NovelSeek, a multi-agent framework designed for Autonomous Scientific Research (ASR) that enhances the efficiency and innovation in scientific studies. It showcases three main benefits: scalability across various research tasks, interactivity with human experts for feedback, and improved efficiency in achieving results faster than traditional methods. NovelSeek has been tested on multiple scientific tasks, demonstrating significant performance improvements in areas like reaction yield prediction and semantic segmentation. By integrating AI with human expertise, NovelSeek aims to revolutionize how scientific research is conducted."
                },
                "zh": {
                    "title": "NovelSeekÔºöÂä†ÈÄüÁßëÂ≠¶Á†îÁ©∂ÁöÑÊô∫ËÉΩÊ°ÜÊû∂",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫NovelSeekÁöÑÁªü‰∏ÄÈó≠ÁéØÂ§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåÊó®Âú®ÂÆûÁé∞Ëá™‰∏ªÁßëÂ≠¶Á†îÁ©∂ÔºàASRÔºâ„ÄÇËØ•Ê°ÜÊû∂Âú®Â§ö‰∏™ÁßëÂ≠¶Á†îÁ©∂È¢ÜÂüü‰∏≠Â±ïÁé∞Âá∫ÂçìË∂äÁöÑÂèØÊâ©Â±ïÊÄß„ÄÅ‰∫§‰∫íÊÄßÂíåÊïàÁéáÔºåËÉΩÂ§ü‰ª•Á©∫ÂâçÁöÑÈÄüÂ∫¶ÂíåÁ≤æÂ∫¶Ëß£ÂÜ≥Â§çÊùÇÈóÆÈ¢ò„ÄÇNovelSeekÈÄöËøá‰∏é‰∫∫Á±ª‰∏ìÂÆ∂ÁöÑÂèçÈ¶àÂíåÂ§öÊô∫ËÉΩ‰ΩìÁöÑ‰∫íÂä®Ôºå‰øÉËøõ‰∫ÜÈ¢ÜÂüü‰∏ìÂÆ∂Áü•ËØÜÁöÑÊó†ÁºùÊï¥Âêà„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåNovelSeekÂú®ÂèçÂ∫î‰∫ßÁéáÈ¢ÑÊµã„ÄÅÂ¢ûÂº∫Â≠êÊ¥ªÊÄßÈ¢ÑÊµãÂíå2DËØ≠‰πâÂàÜÂâ≤Á≠â‰ªªÂä°‰∏≠ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩÔºåËäÇÁúÅ‰∫ÜÂ§ßÈáèÊó∂Èó¥„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14810",
            "title": "Scaling Reasoning, Losing Control: Evaluating Instruction Following in\n  Large Reasoning Models",
            "url": "https://huggingface.co/papers/2505.14810",
            "abstract": "An empirical analysis of MathIF identifies a tension between enhancing reasoning capacity and maintaining instruction adherence in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-following is essential for aligning large language models (LLMs) with user intent. While recent reasoning-oriented models exhibit impressive performance on complex mathematical problems, their ability to adhere to natural language instructions remains underexplored. In this work, we introduce MathIF, a dedicated benchmark for evaluating instruction-following in mathematical reasoning tasks. Our empirical analysis reveals a consistent tension between scaling up reasoning capacity and maintaining controllability, as models that reason more effectively often struggle to comply with user directives. We find that models tuned on distilled long chains-of-thought or trained with reasoning-oriented reinforcement learning often degrade in instruction adherence, especially when generation length increases. Furthermore, we show that even simple interventions can partially recover obedience, though at the cost of reasoning performance. These findings highlight a fundamental tension in current LLM training paradigms and motivate the need for more instruction-aware reasoning models. We release the code and data at https://github.com/TingchenFu/MathIF.",
            "score": 49,
            "issue_id": 3914,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 –º–∞—è",
                "en": "May 20",
                "zh": "5Êúà20Êó•"
            },
            "hash": "97ed7c1fde734d7e",
            "authors": [
                "Tingchen Fu",
                "Jiawei Gu",
                "Yafu Li",
                "Xiaoye Qu",
                "Yu Cheng"
            ],
            "affiliations": [
                "Renmin University of China",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14810.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#training",
                    "#benchmark",
                    "#optimization",
                    "#alignment"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ä–∞–∑—É–º–æ–º –∏ –ø–æ—Å–ª—É—à–∞–Ω–∏–µ–º –≤ –ò–ò",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ MathIF –≤—ã—è–≤–ª—è–µ—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ –º–µ–∂–¥—É —É–ª—É—á—à–µ–Ω–∏–µ–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –Ω–∞ –¥–ª–∏–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏–ª–∏ –æ–±—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —á–∞—Å—Ç–æ —É—Ö—É–¥—à–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–ª–µ–¥–æ–≤–∞—Ç—å —É–∫–∞–∑–∞–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ü—Ä–æ—Å—Ç—ã–µ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –º–æ–≥—É—Ç —á–∞—Å—Ç–∏—á–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ—Å–ª—É—à–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏, –Ω–æ –∑–∞ —Å—á–µ—Ç —Å–Ω–∏–∂–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠—Ç–∏ –≤—ã–≤–æ–¥—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ –≤ —Ç–µ–∫—É—â–∏—Ö –ø–∞—Ä–∞–¥–∏–≥–º–∞—Ö –æ–±—É—á–µ–Ω–∏—è LLM –∏ –º–æ—Ç–∏–≤–∏—Ä—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –±–æ–ª–µ–µ –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ —Å–ª–µ–¥—É—é—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –ø—Ä–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö."
                },
                "en": {
                    "title": "Balancing Reasoning and Instruction in Language Models",
                    "desc": "This paper presents MathIF, a benchmark designed to evaluate how well large language models (LLMs) follow instructions while solving mathematical problems. The authors find a conflict between improving reasoning abilities and maintaining adherence to user instructions, as models that excel in reasoning often fail to follow directives accurately. They observe that training methods like reinforcement learning can enhance reasoning but may reduce the model's ability to comply with instructions, especially as the complexity of tasks increases. The study suggests that addressing this tension is crucial for developing more effective instruction-aware reasoning models."
                },
                "zh": {
                    "title": "Âπ≥Ë°°Êé®ÁêÜËÉΩÂäõ‰∏éÊåá‰ª§ÈÅµÂæ™ÁöÑÊåëÊàò",
                    "desc": "Êú¨Á†îÁ©∂ÂàÜÊûê‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÊåá‰ª§ÈÅµÂæ™ËÉΩÂäõ‰∏éÊé®ÁêÜËÉΩÂäõ‰πãÈó¥ÁöÑÁüõÁõæ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜMathIFÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Ê®°ÂûãÂú®Êï∞Â≠¶Êé®ÁêÜ‰∏≠ÁöÑÊåá‰ª§ÈÅµÂæ™Ë°®Áé∞„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊé®ÁêÜËÉΩÂäõÊõ¥Âº∫ÁöÑÊ®°ÂûãÂæÄÂæÄÂú®ÈÅµÂæ™Áî®Êà∑Êåá‰ª§Êó∂Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÁîüÊàêÂÜÖÂÆπËæÉÈïøÊó∂„ÄÇÊàë‰ª¨ÁöÑÁªìÊûúË°®ÊòéÔºåÂΩìÂâçÁöÑËÆ≠ÁªÉÊñπÊ≥ïÈúÄË¶ÅÊõ¥Â§öÂÖ≥Ê≥®Êåá‰ª§ÊÑèËØÜÔºå‰ª•Âπ≥Ë°°Êé®ÁêÜËÉΩÂäõÂíåÊåá‰ª§ÈÅµÂæ™„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16410",
            "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2505.16410",
            "abstract": "Tool-Star, an RL-based framework, enables LLMs to autonomously use multiple tools for stepwise reasoning, leveraging data synthesis and hierarchical reward design.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, large language models (LLMs) have shown remarkable reasoning capabilities via large-scale reinforcement learning (RL). However, leveraging the RL algorithm to empower effective multi-tool collaborative reasoning in LLMs remains an open challenge. In this paper, we introduce Tool-Star, an RL-based framework designed to empower LLMs to autonomously invoke multiple external tools during stepwise reasoning. Tool-Star integrates six types of tools and incorporates systematic designs in both data synthesis and training. To address the scarcity of tool-use data, we propose a general tool-integrated reasoning data synthesis pipeline, which combines tool-integrated prompting with hint-based sampling to automatically and scalably generate tool-use trajectories. A subsequent quality normalization and difficulty-aware classification process filters out low-quality samples and organizes the dataset from easy to hard. Furthermore, we propose a two-stage training framework to enhance multi-tool collaborative reasoning by: (1) cold-start fine-tuning, which guides LLMs to explore reasoning patterns via tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with hierarchical reward design, which reinforces reward understanding and promotes effective tool collaboration. Experimental analyses on over 10 challenging reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star. The code is available at https://github.com/dongguanting/Tool-Star.",
            "score": 43,
            "issue_id": 3914,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "acbe5c0b965cb7af",
            "authors": [
                "Guanting Dong",
                "Yifei Chen",
                "Xiaoxi Li",
                "Jiajie Jin",
                "Hongjin Qian",
                "Yutao Zhu",
                "Hangyu Mao",
                "Guorui Zhou",
                "Zhicheng Dou",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "BAAI",
                "Kuaishou Technology",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16410.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "üõ†Ô∏è",
                "ru": {
                    "title": "Tool-Star: –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –º—É–ª—å—Ç–∏–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "Tool-Star - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —à–µ—Å—Ç—å —Ç–∏–ø–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∫ —Å–∏–Ω—Ç–µ–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏—é. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω–≤–µ–π–µ—Ä —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 10 —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å Tool-Star."
                },
                "en": {
                    "title": "Empowering LLMs with Multi-Tool Collaborative Reasoning",
                    "desc": "Tool-Star is a reinforcement learning (RL) framework that enhances large language models (LLMs) by enabling them to autonomously utilize multiple external tools for stepwise reasoning. It addresses the challenge of effective multi-tool collaboration by integrating a systematic approach to data synthesis and hierarchical reward design. The framework includes a novel data synthesis pipeline that generates tool-use trajectories and organizes them by difficulty, ensuring high-quality training data. Tool-Star's two-stage training process improves LLMs' reasoning capabilities through fine-tuning and a self-critic RL algorithm, leading to significant performance gains on various reasoning tasks."
                },
                "zh": {
                    "title": "Tool-StarÔºöËµãËÉΩLLMÁöÑÂ§öÂ∑•ÂÖ∑Âçè‰ΩúÊé®ÁêÜ",
                    "desc": "Tool-StarÊòØ‰∏Ä‰∏™Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®‰ΩøÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâËÉΩÂ§üËá™‰∏ª‰ΩøÁî®Â§ö‰∏™Â∑•ÂÖ∑ËøõË°åÈÄêÊ≠•Êé®ÁêÜ„ÄÇËØ•Ê°ÜÊû∂Êï¥Âêà‰∫ÜÂÖ≠ÁßçÂ∑•ÂÖ∑ÔºåÂπ∂Âú®Êï∞ÊçÆÂêàÊàêÂíåËÆ≠ÁªÉÊñπÈù¢ËøõË°å‰∫ÜÁ≥ªÁªüËÆæËÆ°Ôºå‰ª•Ëß£ÂÜ≥Â∑•ÂÖ∑‰ΩøÁî®Êï∞ÊçÆÁ®ÄÁº∫ÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøáÁªìÂêàÂ∑•ÂÖ∑ÈõÜÊàêÊèêÁ§∫ÂíåÂü∫‰∫éÊèêÁ§∫ÁöÑÈááÊ†∑ÔºåTool-StarËÉΩÂ§üËá™Âä®ÁîüÊàêÂ∑•ÂÖ∑‰ΩøÁî®ËΩ®ËøπÔºåÂπ∂ÈÄöËøáË¥®ÈáèÊ†áÂáÜÂåñÂíåÈöæÂ∫¶ÊÑüÁü•ÂàÜÁ±ªÊù•ËøáÊª§‰ΩéË¥®ÈáèÊ†∑Êú¨„ÄÇÊúÄÂêéÔºåTool-StarÈááÁî®‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÂ¢ûÂº∫Â§öÂ∑•ÂÖ∑Âçè‰ΩúÊé®ÁêÜËÉΩÂäõÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊé®ÁêÜÊïàÊûúÂíåÊïàÁéá„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15966",
            "title": "Pixel Reasoner: Incentivizing Pixel-Space Reasoning with\n  Curiosity-Driven Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.15966",
            "abstract": "Introducing pixel-space reasoning in Vision-Language Models (VLMs) through visual operations like zoom-in and select-frame enhances their performance on visual tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-thought reasoning has significantly improved the performance of Large Language Models (LLMs) across various domains. However, this reasoning process has been confined exclusively to textual space, limiting its effectiveness in visually intensive tasks. To address this limitation, we introduce the concept of reasoning in the pixel-space. Within this novel framework, Vision-Language Models (VLMs) are equipped with a suite of visual reasoning operations, such as zoom-in and select-frame. These operations enable VLMs to directly inspect, interrogate, and infer from visual evidences, thereby enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space reasoning capabilities in VLMs presents notable challenges, including the model's initially imbalanced competence and its reluctance to adopt the newly introduced pixel-space operations. We address these challenges through a two-phase training approach. The first phase employs instruction tuning on synthesized reasoning traces to familiarize the model with the novel visual operations. Following this, a reinforcement learning (RL) phase leverages a curiosity-driven reward scheme to balance exploration between pixel-space reasoning and textual reasoning. With these visual operations, VLMs can interact with complex visual inputs, such as information-rich images or videos to proactively gather necessary information. We demonstrate that this approach significantly improves VLM performance across diverse visual reasoning benchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on TallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy achieved by any open-source model to date. These results highlight the importance of pixel-space reasoning and the effectiveness of our framework.",
            "score": 37,
            "issue_id": 3915,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "a1134b1247c14719",
            "authors": [
                "Alex Su",
                "Haozhe Wang",
                "Weimin Ren",
                "Fangzhen Lin",
                "Wenhu Chen"
            ],
            "affiliations": [
                "HKUST",
                "USTC",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15966.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#cv",
                    "#rl",
                    "#optimization",
                    "#benchmark",
                    "#open_source",
                    "#training"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –ø–∏–∫—Å–µ–ª—è—Ö: –Ω–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ò–ò",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (VLM) –ø—É—Ç–µ–º –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∏ –≤—ã–±–æ—Ä –∫–∞–¥—Ä–∞, —á—Ç–æ–±—ã –º–æ–¥–µ–ª–∏ –º–æ–≥–ª–∏ –Ω–∞–ø—Ä—è–º—É—é –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –î–ª—è –æ–±—É—á–µ–Ω–∏—è —ç—Ç–∏–º –Ω–∞–≤—ã–∫–∞–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–≤—É—Ö—Ñ–∞–∑–Ω—ã–π –ø–æ–¥—Ö–æ–¥: –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Enhancing VLMs with Pixel-Space Reasoning",
                    "desc": "This paper introduces a new way for Vision-Language Models (VLMs) to reason about images by using visual operations like zooming in and selecting frames. Traditionally, reasoning in these models has been limited to text, which makes it hard for them to handle visual tasks effectively. The authors propose a two-phase training method that first teaches the model to use these new visual operations and then encourages it to explore both visual and textual reasoning through reinforcement learning. Their approach leads to significant improvements in VLM performance on various visual reasoning tasks, achieving record accuracy on several benchmarks."
                },
                "zh": {
                    "title": "ÂÉèÁ¥†Á©∫Èó¥Êé®ÁêÜÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑË°®Áé∞",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂú®ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâ‰∏≠ÂºïÂÖ•ÂÉèÁ¥†Á©∫Èó¥Êé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ïÔºåÈÄöËøáËßÜËßâÊìç‰ΩúÂ¶ÇÊîæÂ§ßÂíåÈÄâÊã©Â∏ßÊù•ÊèêÂçáÂÖ∂Âú®ËßÜËßâ‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇ‰º†ÁªüÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÊñáÊú¨Á©∫Èó¥ÁöÑÊé®ÁêÜËÉΩÂäõÊòæËëóÊèêÈ´òÔºå‰ΩÜÂú®ËßÜËßâÂØÜÈõÜ‰ªªÂä°‰∏≠ÊïàÊûúÊúâÈôê„ÄÇÊàë‰ª¨ÈÄöËøá‰∏§Èò∂ÊÆµÁöÑËÆ≠ÁªÉÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÊ®°ÂûãÂú®ÂÉèÁ¥†Á©∫Èó¥Êé®ÁêÜ‰∏≠ÁöÑÊåëÊàòÔºåÈ¶ñÂÖàÈÄöËøáÊåá‰ª§Ë∞É‰ºòËÆ©Ê®°ÂûãÁÜüÊÇâÊñ∞ËßÜËßâÊìç‰ΩúÔºåÁÑ∂ÂêéÂà©Áî®Âº∫ÂåñÂ≠¶‰π†Âπ≥Ë°°ÂÉèÁ¥†Á©∫Èó¥ÂíåÊñáÊú¨Á©∫Èó¥ÁöÑÊé®ÁêÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®Â§ö‰∏™ËßÜËßâÊé®ÁêÜÂü∫ÂáÜ‰∏äÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂ±ïÁ§∫‰∫ÜÂÉèÁ¥†Á©∫Èó¥Êé®ÁêÜÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16707",
            "title": "KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models",
            "url": "https://huggingface.co/papers/2505.16707",
            "abstract": "Recent advances in multi-modal generative models have enabled significant progress in instruction-based image editing. However, while these models produce visually plausible outputs, their capacity for knowledge-based reasoning editing tasks remains under-explored. In this paper, we introduce KRIS-Bench (Knowledge-based Reasoning in Image-editing Systems Benchmark), a diagnostic benchmark designed to assess models through a cognitively informed lens. Drawing from educational theory, KRIS-Bench categorizes editing tasks across three foundational knowledge types: Factual, Conceptual, and Procedural. Based on this taxonomy, we design 22 representative tasks spanning 7 reasoning dimensions and release 1,267 high-quality annotated editing instances. To support fine-grained evaluation, we propose a comprehensive protocol that incorporates a novel Knowledge Plausibility metric, enhanced by knowledge hints and calibrated through human studies. Empirical results on 10 state-of-the-art models reveal significant gaps in reasoning performance, highlighting the need for knowledge-centric benchmarks to advance the development of intelligent image editing systems.",
            "score": 36,
            "issue_id": 3914,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "fa1902dc37796b16",
            "authors": [
                "Yongliang Wu",
                "Zonghui Li",
                "Xinting Hu",
                "Xinyu Ye",
                "Xianfang Zeng",
                "Gang Yu",
                "Wenbo Zhu",
                "Bernt Schiele",
                "Ming-Hsuan Yang",
                "Xu Yang"
            ],
            "affiliations": [
                "Max Planck Institute for Informatics",
                "Shanghai Jiao Tong University",
                "Southeast University",
                "StepFun",
                "University of California, Berkeley",
                "University of California, Merced"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16707.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "KRIS-Bench: –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç KRIS-Bench - –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π. –ë–µ–Ω—á–º–∞—Ä–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ —Ç—Ä–µ–º —Ç–∏–ø–∞–º –∑–Ω–∞–Ω–∏–π: —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º, –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã–º –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω—ã–º. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ 22 —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ 7 –∞—Å–ø–µ–∫—Ç–∞–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —Å–æ–±—Ä–∞–ª–∏ 1267 –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø—Ä–æ—Ç–æ–∫–æ–ª –æ—Ü–µ–Ω–∫–∏, –≤–∫–ª—é—á–∞—é—â–∏–π –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ—Å—Ç–∏ –∑–Ω–∞–Ω–∏–π."
                },
                "en": {
                    "title": "Advancing Image Editing with Knowledge-Based Reasoning",
                    "desc": "This paper presents KRIS-Bench, a new benchmark for evaluating multi-modal generative models in the context of instruction-based image editing. It focuses on assessing the models' ability to perform knowledge-based reasoning tasks, which has not been thoroughly investigated before. The benchmark categorizes editing tasks into three knowledge types: Factual, Conceptual, and Procedural, and includes 22 tasks with 1,267 annotated instances. The study reveals that current state-of-the-art models struggle with reasoning tasks, indicating a need for more knowledge-centric evaluation methods in image editing systems."
                },
                "zh": {
                    "title": "Áü•ËØÜÈ©±Âä®ÁöÑÂõæÂÉèÁºñËæëËØÑ‰º∞Êñ∞Âü∫ÂáÜ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫ÜKRIS-BenchÔºàÁü•ËØÜÂü∫Á°ÄÊé®ÁêÜÂú®ÂõæÂÉèÁºñËæëÁ≥ªÁªüÂü∫ÂáÜÔºâÔºåËøôÊòØ‰∏Ä‰∏™Êó®Âú®ËØÑ‰º∞Â§öÊ®°ÊÄÅÁîüÊàêÊ®°ÂûãÂú®Áü•ËØÜÊé®ÁêÜÁºñËæë‰ªªÂä°‰∏≠ÁöÑËÉΩÂäõÁöÑÂü∫ÂáÜÊµãËØï„ÄÇKRIS-BenchÊ†πÊçÆÊïôËÇ≤ÁêÜËÆ∫Â∞ÜÁºñËæë‰ªªÂä°ÂàÜ‰∏∫‰∏âÁßçÂü∫Á°ÄÁü•ËØÜÁ±ªÂûãÔºö‰∫ãÂÆûÊÄß„ÄÅÊ¶ÇÂøµÊÄßÂíåÁ®ãÂ∫èÊÄßÔºåÂπ∂ËÆæËÆ°‰∫Ü22‰∏™‰ª£Ë°®ÊÄß‰ªªÂä°„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªºÂêàËØÑ‰º∞ÂçèËÆÆÔºåÂåÖÂê´Êñ∞ÁöÑÁü•ËØÜÂêàÁêÜÊÄßÊåáÊ†áÔºåÂπ∂ÈÄöËøá‰∫∫Á±ªÁ†îÁ©∂ËøõË°åÊ†°ÂáÜ„ÄÇÂÆûËØÅÁªìÊûúÊòæÁ§∫ÔºåÂΩìÂâçÊúÄÂÖàËøõÁöÑÊ®°ÂûãÂú®Êé®ÁêÜÊÄßËÉΩ‰∏äÂ≠òÂú®ÊòæËëóÂ∑ÆË∑ùÔºåÂº∫Ë∞É‰∫Ü‰ª•Áü•ËØÜ‰∏∫‰∏≠ÂøÉÁöÑÂü∫ÂáÜÊµãËØïÂú®Êô∫ËÉΩÂõæÂÉèÁºñËæëÁ≥ªÁªüÂèëÂ±ï‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16175",
            "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design",
            "url": "https://huggingface.co/papers/2505.16175",
            "abstract": "QuickVideo accelerates long-video understanding by combining a parallelized video decoder, memory-efficient prefilling, and overlapping video decoding with inference, enabling real-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-video understanding has emerged as a crucial capability in real-world applications such as video surveillance, meeting summarization, educational lecture analysis, and sports broadcasting. However, it remains computationally prohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential video decoding, the process of converting the raw bit stream to RGB frames can take up to a minute for hour-long video inputs, and 2) costly prefilling of up to several million tokens for LLM inference, resulting in high latency and memory use. To address these challenges, we propose QuickVideo, a system-algorithm co-design that substantially accelerates long-video understanding to support real-time downstream applications. It comprises three key innovations: QuickDecoder, a parallelized CPU-based video decoder that achieves 2-3 times speedup by splitting videos into keyframe-aligned intervals processed concurrently; QuickPrefill, a memory-efficient prefilling method using KV-cache pruning to support more frames with less GPU memory; and an overlapping scheme that overlaps CPU video decoding with GPU inference. Together, these components infernece time reduce by a minute on long video inputs, enabling scalable, high-quality video understanding even on limited hardware. Experiments show that QuickVideo generalizes across durations and sampling rates, making long video processing feasible in practice.",
            "score": 30,
            "issue_id": 3914,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "0bcb5c833bad2340",
            "authors": [
                "Benjamin Schneider",
                "Dongfu Jiang",
                "Chao Du",
                "Tianyu Pang",
                "Wenhu Chen"
            ],
            "affiliations": [
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16175.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#video",
                    "#long_context"
                ],
                "emoji": "üöÄ",
                "ru": {
                    "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é QuickVideo",
                    "desc": "QuickVideo - —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞, —É—Å–∫–æ—Ä—è—é—â–∞—è –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –¥–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä –≤–∏–¥–µ–æ, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–æ –ø–∞–º—è—Ç–∏ –ø—Ä–µ–¥–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∏ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –≤—ã–≤–æ–¥–æ–º. QuickVideo –≤–∫–ª—é—á–∞–µ—Ç QuickDecoder –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ –Ω–∞ CPU, QuickPrefill –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ GPU –∏ —Å—Ö–µ–º—É –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è CPU-–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å GPU-–≤—ã–≤–æ–¥–æ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ–±–æ–±—â–∞–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —á–∞—Å—Ç–æ—Ç—ã –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ –≤–∏–¥–µ–æ."
                },
                "en": {
                    "title": "Accelerating Long-Video Understanding for Real-Time Applications",
                    "desc": "QuickVideo is a novel system designed to enhance the understanding of long videos in real-time applications. It addresses two major challenges: the slow sequential video decoding and the high memory requirements for token prefilling in large language models (LLMs). By introducing a parallelized video decoder, a memory-efficient prefilling method, and an overlapping decoding scheme, QuickVideo significantly reduces inference time. This allows for efficient processing of long videos, making advanced video analysis accessible even on limited hardware."
                },
                "zh": {
                    "title": "QuickVideoÔºöÂÆûÊó∂ÈïøËßÜÈ¢ëÁêÜËß£ÁöÑÂä†ÈÄüÂà©Âô®",
                    "desc": "QuickVideo ÊòØ‰∏ÄÁßçÂä†ÈÄüÈïøËßÜÈ¢ëÁêÜËß£ÁöÑÁ≥ªÁªüÔºåÁªìÂêà‰∫ÜÂπ∂Ë°åËßÜÈ¢ëËß£Á†Å„ÄÅÂÜÖÂ≠òÈ´òÊïàÁöÑÈ¢ÑÂ°´ÂÖÖÂíåÈáçÂè†Ëß£Á†Å‰∏éÊé®ÁêÜ„ÄÇÂÆÉÈÄöËøáÂø´ÈÄüËß£Á†ÅÂô®Â∞ÜËßÜÈ¢ëÂàÜÂâ≤ÊàêÂÖ≥ÈîÆÂ∏ßÂØπÈΩêÁöÑÈó¥ÈöîÔºåÂπ∂ÂêåÊó∂Â§ÑÁêÜÔºå‰ªéËÄåÂÆûÁé∞‰∫Ü 2-3 ÂÄçÁöÑÈÄüÂ∫¶ÊèêÂçá„ÄÇQuickPrefill ÊñπÊ≥ïÈÄöËøá KV-cache Ââ™ÊûùÂáèÂ∞ë‰∫ÜÂØπ GPU ÂÜÖÂ≠òÁöÑÈúÄÊ±ÇÔºå‰ΩøÂæóÂèØ‰ª•Â§ÑÁêÜÊõ¥Â§öÂ∏ß„ÄÇËØ•Á≥ªÁªüÊòæËëóÈôç‰Ωé‰∫ÜÈïøËßÜÈ¢ëËæìÂÖ•ÁöÑÊé®ÁêÜÊó∂Èó¥Ôºå‰ΩøÂæóÂú®ÊúâÈôêÁ°¨‰ª∂‰∏ä‰πüËÉΩÂÆûÁé∞È´òË¥®ÈáèÁöÑËßÜÈ¢ëÁêÜËß£„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17022",
            "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation\n  with Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.17022",
            "abstract": "GoT-R1 enhances visual generation by using reinforcement learning to improve semantic-spatial reasoning, outperforming existing models on complex compositional tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual generation models have made remarkable progress in creating realistic images from text prompts, yet struggle with complex prompts that specify multiple objects with precise spatial relationships and attributes. Effective handling of such prompts requires explicit reasoning about the semantic content and spatial layout. We present GoT-R1, a framework that applies reinforcement learning to enhance semantic-spatial reasoning in visual generation. Building upon the Generation Chain-of-Thought approach, GoT-R1 enables models to autonomously discover effective reasoning strategies beyond predefined templates through carefully designed reinforcement learning. To achieve this, we propose a dual-stage multi-dimensional reward framework that leverages MLLMs to evaluate both the reasoning process and final output, enabling effective supervision across the entire generation pipeline. The reward system assesses semantic alignment, spatial accuracy, and visual quality in a unified approach. Experimental results demonstrate significant improvements on T2I-CompBench benchmark, particularly in compositional tasks involving precise spatial relationships and attribute binding. GoT-R1 advances the state-of-the-art in image generation by successfully transferring sophisticated reasoning capabilities to the visual generation domain. To facilitate future research, we make our code and pretrained models publicly available at https://github.com/gogoduan/GoT-R1.",
            "score": 23,
            "issue_id": 3917,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "c3983abb24fa0ce4",
            "authors": [
                "Chengqi Duan",
                "Rongyao Fang",
                "Yuqing Wang",
                "Kun Wang",
                "Linjiang Huang",
                "Xingyu Zeng",
                "Hongsheng Li",
                "Xihui Liu"
            ],
            "affiliations": [
                "Beihang University",
                "CUHK MMLab",
                "HKU MMLab",
                "Sensetime"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17022.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#rl",
                    "#multimodal",
                    "#open_source",
                    "#cv",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–£–º–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è —É–º–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "GoT-R1 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–æ-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–Ω –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ø–æ–¥—Ö–æ–¥–µ Generation Chain-of-Thought –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. GoT-R1 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é –º–Ω–æ–≥–æ–º–µ—Ä–Ω—É—é —Å–∏—Å—Ç–µ–º—É –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π, –æ—Ü–µ–Ω–∏–≤–∞—é—â—É—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ, –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ —Å–ª–æ–∂–Ω—ã—Ö –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Reinforcing Reasoning for Better Visual Generation",
                    "desc": "GoT-R1 is a new framework that improves visual generation by using reinforcement learning to enhance how models understand and create images based on complex text prompts. It focuses on semantic-spatial reasoning, which means it helps models better grasp the meaning of words and how objects should be arranged in space. The framework uses a dual-stage reward system to evaluate both the reasoning process and the final image quality, ensuring that the generated images are accurate and visually appealing. Experimental results show that GoT-R1 outperforms existing models, especially in tasks that require detailed understanding of object relationships and attributes."
                },
                "zh": {
                    "title": "GoT-R1ÔºöÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÊèêÂçáËßÜËßâÁîüÊàêËÉΩÂäõ",
                    "desc": "GoT-R1 ÊòØ‰∏Ä‰∏™ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Â¢ûÂº∫ËØ≠‰πâ-Á©∫Èó¥Êé®ÁêÜÁöÑËßÜËßâÁîüÊàêÊ°ÜÊû∂„ÄÇÂÆÉËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜÂ§çÊùÇÁöÑÊñáÊú¨ÊèêÁ§∫ÔºåÂ∞§ÂÖ∂ÊòØÊ∂âÂèäÂ§ö‰∏™ÂØπË±°ÂèäÂÖ∂Á≤æÁ°ÆÁ©∫Èó¥ÂÖ≥Á≥ªÁöÑ‰ªªÂä°„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®ÂèåÈò∂ÊÆµÂ§öÁª¥Â•ñÂä±Êú∫Âà∂ÔºåÂà©Áî®Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÊù•ËØÑ‰º∞Êé®ÁêÜËøáÁ®ãÂíåÊúÄÁªàËæìÂá∫„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGoT-R1 Âú® T2I-CompBench Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊ®°ÂûãÔºåÁâπÂà´ÊòØÂú®ÁªÑÂêà‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16933",
            "title": "LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning",
            "url": "https://huggingface.co/papers/2505.16933",
            "abstract": "A diffusion-based Multimodal Large Language Model (LLaDA-V) with integrated visual instruction tuning performs competitively on multimodal tasks and outperforms existing models in multimodal understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large Language Model (MLLM) that integrates visual instruction tuning with masked diffusion models, representing a departure from the autoregressive paradigms dominant in current multimodal approaches. Built upon LLaDA, a representative large language diffusion model, LLaDA-V incorporates a vision encoder and MLP connector that projects visual features into the language embedding space, enabling effective multimodal alignment. Our empirical investigation reveals several intriguing results: First, LLaDA-V demonstrates promising multimodal performance despite its language model being weaker on purely textual tasks than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal tasks with better data scalability. It also narrows the performance gap to Qwen2-VL, suggesting the effectiveness of its architecture for multimodal tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal understanding compared to existing hybrid autoregressive-diffusion and purely diffusion-based MLLMs. Our findings suggest that large language diffusion models show promise in multimodal contexts and warrant further investigation in future research. Project page and codes: https://ml-gsai.github.io/LLaDA-V-demo/.",
            "score": 22,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "9ae7746da2ef9a2b",
            "authors": [
                "Zebin You",
                "Shen Nie",
                "Xiaolu Zhang",
                "Jun Hu",
                "Jun Zhou",
                "Zhiwu Lu",
                "Ji-Rong Wen",
                "Chongxuan Li"
            ],
            "affiliations": [
                "Ant Group",
                "Beijing Key Laboratory of Research on Large Models and Intelligent Governance",
                "Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE",
                "Gaoling School of AI, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16933.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –ò–ò-–º–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–Ω–∞–ª–æ–≥–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å LLaDA-V - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—â–∞—è –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏. LLaDA-V –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –±–æ–ª–µ–µ —Å–ª–∞–±—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∞–Ω–∞–ª–æ–≥–∞–º–∏. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –≥–∏–±—Ä–∏–¥–Ω—ã–º–∏ –∏ —á–∏—Å—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö."
                },
                "en": {
                    "title": "LLaDA-V: Bridging Text and Vision with Diffusion Power!",
                    "desc": "LLaDA-V is a new type of Multimodal Large Language Model that uses a diffusion-based approach combined with visual instruction tuning. This model integrates visual features into the language processing space, allowing it to understand and generate responses that involve both text and images. Despite being less effective on purely text tasks compared to some existing models, LLaDA-V performs well in multimodal scenarios and shows strong scalability with data. The results indicate that diffusion models can be highly effective for tasks that require understanding multiple types of information, paving the way for future research in this area."
                },
                "zh": {
                    "title": "Êâ©Êï£Ê®°ÂûãÂºïÈ¢ÜÂ§öÊ®°ÊÄÅÁêÜËß£Êñ∞ÊΩÆÊµÅ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊâ©Êï£ÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãLLaDA-VÔºåËØ•Ê®°ÂûãÁªìÂêà‰∫ÜËßÜËßâÊåá‰ª§Ë∞É‰ºòÔºåËÉΩÂ§üÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇLLaDA-VÈááÁî®‰∫ÜÊé©ËîΩÊâ©Êï£Ê®°ÂûãÔºåÁ™ÅÁ†¥‰∫ÜÂΩìÂâçÂ§öÊ®°ÊÄÅÊñπÊ≥ï‰∏≠‰∏ªÊµÅÁöÑËá™ÂõûÂΩíËåÉÂºè„ÄÇÂ∞ΩÁÆ°Âú®Á∫ØÊñáÊú¨‰ªªÂä°‰∏äË°®Áé∞‰∏çÂ¶Ç‰∏Ä‰∫õÁé∞ÊúâÊ®°ÂûãÔºå‰ΩÜÂú®Â§öÊ®°ÊÄÅ‰ªªÂä°‰∏≠ÔºåLLaDA-V‰∏éÂÖ∂‰ªñÊ®°ÂûãÁõ∏ÊØîÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÊï∞ÊçÆÂèØÊâ©Â±ïÊÄßÂíåÁ´û‰∫âÂäõ„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂü∫‰∫éÊâ©Êï£ÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅÁêÜËß£ÊñπÈù¢ÂÖ∑ÊúâÂæàÂ§ßÁöÑÊΩúÂäõÔºåÂÄºÂæóËøõ‰∏ÄÊ≠•Á†îÁ©∂„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15270",
            "title": "Scaling Diffusion Transformers Efficiently via ŒºP",
            "url": "https://huggingface.co/papers/2505.15270",
            "abstract": "Maximal Update Parametrization (ŒºP) is extended to diffusion Transformers, demonstrating efficient hyperparameter transferability and reduced tuning costs across various models and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers have emerged as the foundation for vision generative models, but their scalability is limited by the high cost of hyperparameter (HP) tuning at large scales. Recently, Maximal Update Parametrization (muP) was proposed for vanilla Transformers, which enables stable HP transfer from small to large language models, and dramatically reduces tuning costs. However, it remains unclear whether muP of vanilla Transformers extends to diffusion Transformers, which differ architecturally and objectively. In this work, we generalize standard muP to diffusion Transformers and validate its effectiveness through large-scale experiments. First, we rigorously prove that muP of mainstream diffusion Transformers, including DiT, U-ViT, PixArt-alpha, and MMDiT, aligns with that of the vanilla Transformer, enabling the direct application of existing muP methodologies. Leveraging this result, we systematically demonstrate that DiT-muP enjoys robust HP transferability. Notably, DiT-XL-2-muP with transferred learning rate achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we validate the effectiveness of muP on text-to-image generation by scaling PixArt-alpha from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases, models under muP outperform their respective baselines while requiring small tuning cost, only 5.5% of one training run for PixArt-alpha and 3% of consumption by human experts for MMDiT-18B. These results establish muP as a principled and efficient framework for scaling diffusion Transformers.",
            "score": 21,
            "issue_id": 3914,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "6c13e0c5ef8ee4a2",
            "authors": [
                "Chenyu Zheng",
                "Xinyu Zhang",
                "Rongzhen Wang",
                "Wei Huang",
                "Zhi Tian",
                "Weilin Huang",
                "Jun Zhu",
                "Chongxuan Li"
            ],
            "affiliations": [
                "Beijing Key Laboratory of Research on Large Models and Intelligent Governance",
                "ByteDance Seed",
                "Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE",
                "Gaoling School of AI, Renmin University of China",
                "RIKEN AIP",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15270.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#transfer_learning",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "üî¨",
                "ru": {
                    "title": "ŒºP: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ Maximal Update Parametrization (ŒºP) –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã –¥–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ŒºP –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, —Ç–∞–∫–∏–º –∫–∞–∫ DiT, U-ViT, PixArt-alpha –∏ MMDiT. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ŒºP –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –∏—Ö –Ω–∞—Å—Ç—Ä–æ–π–∫—É. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ ŒºP —è–≤–ª—è–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Efficient Hyperparameter Transfer for Diffusion Transformers with ŒºP",
                    "desc": "This paper extends the Maximal Update Parametrization (ŒºP) technique to diffusion Transformers, which are crucial for generative vision models. The authors demonstrate that ŒºP allows for effective hyperparameter transfer from smaller to larger models, significantly reducing the costs associated with hyperparameter tuning. Through extensive experiments, they show that diffusion Transformers like DiT and PixArt-alpha benefit from ŒºP, achieving faster convergence and better performance with minimal tuning effort. Overall, this work establishes ŒºP as a valuable method for enhancing the scalability and efficiency of diffusion Transformers in various tasks."
                },
                "zh": {
                    "title": "ÊúÄÂ§ßÊõ¥Êñ∞ÂèÇÊï∞ÂåñÔºöÊâ©Êï£ÂèòÊç¢Âô®ÁöÑÈ´òÊïàË∞É‰ºòÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ÊñáÊâ©Â±ï‰∫ÜÊúÄÂ§ßÊõ¥Êñ∞ÂèÇÊï∞ÂåñÔºàŒºPÔºâÂà∞Êâ©Êï£ÂèòÊç¢Âô®ÔºåÂ±ïÁ§∫‰∫ÜÈ´òÊïàÁöÑË∂ÖÂèÇÊï∞ÂèØËΩ¨ÁßªÊÄßÂíåÈôç‰ΩéÁöÑË∞É‰ºòÊàêÊú¨„ÄÇÊâ©Êï£ÂèòÊç¢Âô®Âú®ËßÜËßâÁîüÊàêÊ®°Âûã‰∏≠ÂèëÊå•‰∫ÜÂü∫Á°Ä‰ΩúÁî®Ôºå‰ΩÜÂú®Â§ßËßÑÊ®°Â∫îÁî®‰∏≠Ë∂ÖÂèÇÊï∞Ë∞É‰ºòÁöÑÈ´òÊàêÊú¨ÈôêÂà∂‰∫ÜÂÖ∂ÂèØÊâ©Â±ïÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåŒºPÂèØ‰ª•ÊúâÊïàÂú∞‰ªéÂ∞èÂûãÊ®°ÂûãËΩ¨ÁßªÂà∞Â§ßÂûãÊâ©Êï£ÂèòÊç¢Âô®ÔºåÂπ∂Âú®Â§ßËßÑÊ®°ÂÆûÈ™å‰∏≠È™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇÈÄöËøáÁ≥ªÁªüÊÄßÂÆûÈ™åÔºåÁªìÊûúÊòæÁ§∫Âú®Ë∞É‰ºòÊàêÊú¨ËæÉ‰ΩéÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ΩøÁî®ŒºPÁöÑÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏ä‰ºò‰∫éÂü∫Á∫øÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16925",
            "title": "Risk-Averse Reinforcement Learning with Itakura-Saito Loss",
            "url": "https://huggingface.co/papers/2505.16925",
            "abstract": "Risk-averse reinforcement learning finds application in various high-stakes fields. Unlike classical reinforcement learning, which aims to maximize expected returns, risk-averse agents choose policies that minimize risk, occasionally sacrificing expected value. These preferences can be framed through utility theory. We focus on the specific case of the exponential utility function, where we can derive the Bellman equations and employ various reinforcement learning algorithms with few modifications. However, these methods suffer from numerical instability due to the need for exponent computation throughout the process. To address this, we introduce a numerically stable and mathematically sound loss function based on the Itakura-Saito divergence for learning state-value and action-value functions. We evaluate our proposed loss function against established alternatives, both theoretically and empirically. In the experimental section, we explore multiple financial scenarios, some with known analytical solutions, and show that our loss function outperforms the alternatives.",
            "score": 20,
            "issue_id": 3920,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "1233ea3eef980e02",
            "authors": [
                "Igor Udovichenko",
                "Olivier Croissant",
                "Anita Toleutaeva",
                "Evgeny Burnaev",
                "Alexander Korotin"
            ],
            "affiliations": [
                "Artificial Intelligence Research Institute",
                "Natixis Foundation",
                "Skolkovo Institute of Science and Technology",
                "Vega Institute Foundation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16925.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rlhf",
                    "#math",
                    "#rl",
                    "#training",
                    "#games"
                ],
                "emoji": "üìä",
                "ru": {
                    "title": "–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∏—Å–∫–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ —Ä–∏—Å–∫-–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–æ–µ –Ω–∞—Ö–æ–¥–∏—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ —Å–ª—É—á–∞–µ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –∏ –≤—ã–≤–æ–¥—è—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —É—Ä–∞–≤–Ω–µ–Ω–∏—è –ë–µ–ª–ª–º–∞–Ω–∞. –î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —á–∏—Å–ª–µ–Ω–Ω–æ–π –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –ò—Ç–∞–∫—É—Ä—ã-–°–∞–∏—Ç–æ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞–º–∏."
                },
                "en": {
                    "title": "Minimizing Risk in Reinforcement Learning with Stability",
                    "desc": "This paper discusses risk-averse reinforcement learning, which is important in high-stakes situations where minimizing risk is crucial. Unlike traditional reinforcement learning that focuses on maximizing expected returns, risk-averse agents prioritize safer policies, sometimes at the cost of expected value. The authors specifically examine the exponential utility function to derive Bellman equations and adapt reinforcement learning algorithms accordingly. They propose a new loss function based on the Itakura-Saito divergence to improve numerical stability during training, demonstrating its effectiveness through theoretical analysis and empirical tests in financial scenarios."
                },
                "zh": {
                    "title": "È£éÈô©ÂéåÊÅ∂Âº∫ÂåñÂ≠¶‰π†ÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "È£éÈô©ÂéåÊÅ∂Âº∫ÂåñÂ≠¶‰π†Âú®È´òÈ£éÈô©È¢ÜÂüüÊúâÂπøÊ≥õÂ∫îÁî®„ÄÇ‰∏é‰º†ÁªüÁöÑÂº∫ÂåñÂ≠¶‰π†‰∏çÂêåÔºåÈ£éÈô©ÂéåÊÅ∂‰ª£ÁêÜÈÄâÊã©ÁöÑÁ≠ñÁï•ÊòØÊúÄÂ∞èÂåñÈ£éÈô©ÔºåÂèØËÉΩ‰ºöÁâ∫Áâ≤È¢ÑÊúüÊî∂Áõä„ÄÇÊàë‰ª¨‰∏ìÊ≥®‰∫éÊåáÊï∞ÊïàÁî®ÂáΩÊï∞ÁöÑÁâπÂÆöÊÉÖÂÜµÔºåÊé®ÂØºÂá∫Ë¥ùÂ∞îÊõºÊñπÁ®ãÔºåÂπ∂Âú®Ê≠§Âü∫Á°Ä‰∏äÂØπÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïËøõË°åÂ∞ëÈáè‰øÆÊîπ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÊñπÊ≥ïÂú®ËÆ°ÁÆóËøáÁ®ã‰∏≠Áî±‰∫éÈúÄË¶ÅËøõË°åÊåáÊï∞ËøêÁÆóËÄåÂØºËá¥Êï∞ÂÄº‰∏çÁ®≥ÂÆöÔºåÂõ†Ê≠§Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éItakura-SaitoÊï£Â∫¶ÁöÑÊï∞ÂÄºÁ®≥ÂÆö‰∏îÊï∞Â≠¶‰∏äÂêàÁêÜÁöÑÊçüÂ§±ÂáΩÊï∞ÔºåÁî®‰∫éÂ≠¶‰π†Áä∂ÊÄÅÂÄºÂíåÂä®‰ΩúÂÄºÂáΩÊï∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16181",
            "title": "Understanding Generative AI Capabilities in Everyday Image Editing Tasks",
            "url": "https://huggingface.co/papers/2505.16181",
            "abstract": "Analysis of 83k image editing requests reveals that AI editors, including GPT-4o, struggle with low-creativity tasks and precise editing, while performing better on open-ended tasks, and human and VLM judges differ in their preferences for AI versus human edits.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative AI (GenAI) holds significant promise for automating everyday image editing tasks, especially following the recent release of GPT-4o on March 25, 2025. However, what subjects do people most often want edited? What kinds of editing actions do they want to perform (e.g., removing or stylizing the subject)? Do people prefer precise edits with predictable outcomes or highly creative ones? By understanding the characteristics of real-world requests and the corresponding edits made by freelance photo-editing wizards, can we draw lessons for improving AI-based editors and determine which types of requests can currently be handled successfully by AI editors? In this paper, we present a unique study addressing these questions by analyzing 83k requests from the past 12 years (2013-2025) on the Reddit community, which collected 305k PSR-wizard edits. According to human ratings, approximately only 33% of requests can be fulfilled by the best AI editors (including GPT-4o, Gemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on low-creativity requests that require precise editing than on more open-ended tasks. They often struggle to preserve the identity of people and animals, and frequently make non-requested touch-ups. On the other side of the table, VLM judges (e.g., o1) perform differently from human judges and may prefer AI edits more than human edits. Code and qualitative examples are available at: https://psrdataset.github.io",
            "score": 20,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "1d67f723fb5261aa",
            "authors": [
                "Mohammad Reza Taesiri",
                "Brandon Collins",
                "Logan Bolton",
                "Viet Dac Lai",
                "Franck Dernoncourt",
                "Trung Bui",
                "Anh Totti Nguyen"
            ],
            "affiliations": [
                "Adobe Research",
                "Auburn University",
                "University of Alberta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16181.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#games",
                    "#dataset",
                    "#cv",
                    "#optimization",
                    "#interpretability"
                ],
                "emoji": "üñºÔ∏è",
                "ru": {
                    "title": "–ò–ò –≤ —Ñ–æ—Ç–æ—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å vs —Ç–æ—á–Ω–æ—Å—Ç—å",
                    "desc": "–ê–Ω–∞–ª–∏–∑ 83 —Ç—ã—Å—è—á –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ò–ò-—Ä–µ–¥–∞–∫—Ç–æ—Ä—ã, –≤–∫–ª—é—á–∞—è GPT-4o, –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –∑–∞–¥–∞—á–∞–º–∏ –Ω–∏–∑–∫–æ–π –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω—ã–º —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –Ω–æ –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏. –¢–æ–ª—å–∫–æ 33% –∑–∞–ø—Ä–æ—Å–æ–≤ –º–æ–≥—É—Ç –±—ã—Ç—å —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã –ª—É—á—à–∏–º–∏ –ò–ò-—Ä–µ–¥–∞–∫—Ç–æ—Ä–∞–º–∏ –ø–æ –æ—Ü–µ–Ω–∫–∞–º –ª—é–¥–µ–π. –ò–ò —á–∞—Å—Ç–æ –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å –ª—é–¥–µ–π –∏ –∂–∏–≤–æ—Ç–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ –¥–µ–ª–∞–µ—Ç –Ω–µ–∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç –æ—Ü–µ–Ω–æ–∫ –ª—é–¥–µ–π –∏ –º–æ–≥—É—Ç –æ—Ç–¥–∞–≤–∞—Ç—å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –ò–ò."
                },
                "en": {
                    "title": "AI Editors: Better at Creativity, Struggling with Precision",
                    "desc": "This paper analyzes 83,000 image editing requests to evaluate the performance of AI editors like GPT-4o. It finds that AI struggles with low-creativity tasks that require precise edits, achieving success in only about 33% of requests. In contrast, AI performs better on open-ended editing tasks, but often fails to maintain the identity of subjects and makes unwanted changes. Additionally, the preferences of visual language model (VLM) judges differ from human judges, with VLM judges showing a greater inclination towards AI-generated edits."
                },
                "zh": {
                    "title": "AIÁºñËæëÂô®Âú®ÂàõÈÄ†ÊÄß‰ªªÂä°‰∏≠ÁöÑ‰ºòÂäø‰∏éÊåëÊàò",
                    "desc": "Êú¨Á†îÁ©∂ÂàÜÊûê‰∫Ü83000‰∏™ÂõæÂÉèÁºñËæëËØ∑Ê±ÇÔºåÂèëÁé∞AIÁºñËæëÂô®ÔºàÂ¶ÇGPT-4oÔºâÂú®‰ΩéÂàõÈÄ†ÊÄß‰ªªÂä°ÂíåÁ≤æÁ°ÆÁºñËæëÊñπÈù¢Ë°®Áé∞‰∏ç‰Ω≥Ôºå‰ΩÜÂú®ÂºÄÊîæÊÄß‰ªªÂä°‰∏≠Ë°®Áé∞Êõ¥Â•Ω„ÄÇ‰∫∫Á±ªËØÑÂÆ°ÂíåËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâËØÑÂÆ°Âú®ÂØπAI‰∏é‰∫∫Á±ªÁºñËæëÁöÑÂÅèÂ•Ω‰∏äÂ≠òÂú®Â∑ÆÂºÇ„ÄÇÂ§ßÁ∫¶Âè™Êúâ33%ÁöÑËØ∑Ê±ÇËÉΩÂ§üË¢´ÊúÄÂ•ΩÁöÑAIÁºñËæëÂô®Êª°Ë∂≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÁ≤æÁ°ÆÁºñËæëÁöÑÊÉÖÂÜµ‰∏ãÔºåAIÁºñËæëÂô®Â∏∏Â∏∏Êó†Ê≥ï‰øùÊåÅ‰∫∫Áâ©ÂíåÂä®Áâ©ÁöÑË∫´‰ªΩ„ÄÇÈÄöËøáÂØπËøô‰∫õËØ∑Ê±ÇÁöÑÂàÜÊûêÔºåÊàë‰ª¨ÂèØ‰ª•‰∏∫ÊîπËøõAIÁºñËæëÂô®Êèê‰æõÈáçË¶ÅÁöÑËßÅËß£„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16400",
            "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.16400",
            "abstract": "Large-scale reinforcement learning enhances reasoning capabilities in small and mid-sized models more effectively than distillation, achieving superior results in both math and code benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress in large-scale reinforcement learning (RL) for reasoning, the training recipe for building high-performing reasoning models remains elusive. Key implementation details of frontier models, such as DeepSeek-R1, including data curation strategies and RL training recipe, are often omitted. Moreover, recent research indicates distillation remains more effective than RL for smaller models. In this work, we demonstrate that large-scale RL can significantly enhance the reasoning capabilities of strong, small- and mid-sized models, achieving results that surpass those of state-of-the-art distillation-based models. We systematically study the RL training process through extensive ablations and propose a simple yet effective approach: first training on math-only prompts, then on code-only prompts. Notably, we find that math-only RL not only significantly enhances the performance of strong distilled models on math benchmarks (e.g., +14.6% / +17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks (e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition, extended code-only RL iterations further improve performance on code benchmarks with minimal or no degradation in math results. We develop a robust data curation pipeline to collect challenging prompts with high-quality, verifiable answers and test cases to enable verification-based RL across both domains. Finally, we identify key experimental insights, including curriculum learning with progressively increasing response lengths and the stabilizing effect of on-policy parameter updates. We find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), but also pushes the limits of the model's reasoning ability, enabling it to solve problems that were previously unsolvable.",
            "score": 18,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "5a3a63b67bb5b62a",
            "authors": [
                "Yang Chen",
                "Zhuolin Yang",
                "Zihan Liu",
                "Chankyu Lee",
                "Peng Xu",
                "Mohammad Shoeybi",
                "Bryan Catanzaro",
                "Wei Ping"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16400.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#rl",
                    "#optimization",
                    "#training"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Å–∫—Ä—ã—Ç—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –º–∞–ª—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é —É –Ω–µ–±–æ–ª—å—à–∏—Ö –∏ —Å—Ä–µ–¥–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –∑–Ω–∞–Ω–∏–π –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –∫–æ–¥–æ–≤—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥: —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, –∑–∞—Ç–µ–º –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã –≤–∫–ª—é—á–∞—é—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ curriculum learning —Å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–º —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–æ–≤."
                },
                "en": {
                    "title": "Boosting Reasoning with Large-Scale Reinforcement Learning",
                    "desc": "This paper explores how large-scale reinforcement learning (RL) can improve the reasoning abilities of small and mid-sized models more effectively than traditional distillation methods. The authors present a systematic study of the RL training process, highlighting a two-phase approach that first focuses on math prompts and then on code prompts. They demonstrate significant performance gains on both math and code benchmarks, showing that RL enhances the foundational reasoning capabilities of models while also enabling them to tackle previously unsolvable problems. Key insights include the importance of data curation and curriculum learning in optimizing the RL training process."
                },
                "zh": {
                    "title": "Â§ßËßÑÊ®°Âº∫ÂåñÂ≠¶‰π†ÊèêÂçáÊé®ÁêÜËÉΩÂäõÁöÑÊúâÊïàÊÄß",
                    "desc": "Êú¨Á†îÁ©∂Ë°®ÊòéÔºåÂ§ßËßÑÊ®°Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂú®ÊèêÂçáÂ∞èÂûãÂíå‰∏≠ÂûãÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÊñπÈù¢ÔºåÊØîËí∏È¶èÊñπÊ≥ïÊõ¥‰∏∫ÊúâÊïà„ÄÇÊàë‰ª¨ÈÄöËøáÁ≥ªÁªüÁöÑÂÆûÈ™åÁ†îÁ©∂ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÊúâÊïàÁöÑËÆ≠ÁªÉÊñπÊ≥ïÔºöÂÖàÂú®Êï∞Â≠¶ÊèêÁ§∫‰∏äËÆ≠ÁªÉÔºåÂÜçÂú®‰ª£Á†ÅÊèêÁ§∫‰∏äËÆ≠ÁªÉ„ÄÇÁªìÊûúÊòæÁ§∫ÔºåÊï∞Â≠¶Âº∫ÂåñÂ≠¶‰π†ÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÂú®Êï∞Â≠¶Âü∫ÂáÜÊµãËØïÂíå‰ª£Á†ÅÊé®ÁêÜ‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ËøòÂºÄÂèë‰∫Ü‰∏Ä‰∏™Âº∫Â§ßÁöÑÊï∞ÊçÆÊï¥ÁêÜÊµÅÁ®ãÔºå‰ª•Êî∂ÈõÜÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÊèêÁ§∫ÂíåÈ´òË¥®ÈáèÁöÑÁ≠îÊ°àÔºå‰ªéËÄåÊîØÊåÅË∑®È¢ÜÂüüÁöÑÈ™åËØÅÂü∫Á°ÄÂº∫ÂåñÂ≠¶‰π†„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14684",
            "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
            "url": "https://huggingface.co/papers/2505.14684",
            "abstract": "A model for detecting and generating missing intermediate steps in mathematical Chain-of-Thought reasoning improves performance and generalization on mathematical and logical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have achieved remarkable progress on mathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits.",
            "score": 18,
            "issue_id": 3918,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 –º–∞—è",
                "en": "May 20",
                "zh": "5Êúà20Êó•"
            },
            "hash": "bcaa1385d7242117",
            "authors": [
                "Haolei Xu",
                "Yuchen Yan",
                "Yongliang Shen",
                "Wenqi Zhang",
                "Guiyang Hou",
                "Shengpei Jiang",
                "Kaitao Song",
                "Weiming Lu",
                "Jun Xiao",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14684.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#dataset",
                    "#reasoning",
                    "#optimization",
                    "#data"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ú–æ—Å—Ç —á–µ—Ä–µ–∑ –ø—Ä–æ–ø–∞—Å—Ç—å –≤ —Ü–µ–ø–æ—á–∫–µ –º—ã—Å–ª–µ–π: —É–ª—É—á—à–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–∞ —Ü–µ–ø–æ—á–∫–∏ –º—ã—Å–ª–µ–π (Chain-of-Thought, CoT). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∑–∞–¥–∞—á—É CoT Thought Leap Bridge –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ScaleQM+ –∏ –æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å CoT-Bridge –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ–ª–Ω–æ—Ç—ã –∏ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª–∏, –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —É–ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Bridging Thought Leaps for Enhanced Reasoning in AI",
                    "desc": "This paper introduces a model designed to detect and fill in missing steps in Chain-of-Thought (CoT) reasoning for mathematical tasks. The authors identify a problem where existing datasets have gaps, known as Thought Leaps, which hinder the learning process of large language models (LLMs). To address this, they propose the CoT Thought Leap Bridge Task and create a new training dataset called ScaleQM+ to help models learn to generate the missing reasoning steps. Their experiments show that models trained with this approach significantly outperform those trained on incomplete datasets, leading to better performance in both mathematical and logical reasoning tasks."
                },
                "zh": {
                    "title": "Â°´Ë°•ÊÄùÁª¥Ë∑≥Ë∑ÉÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõ",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊ®°ÂûãÔºåÁî®‰∫éÊ£ÄÊµãÂíåÁîüÊàêÊï∞Â≠¶Êé®ÁêÜ‰∏≠ÁöÑÁº∫Â§±‰∏≠Èó¥Ê≠•È™§Ôºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÂú®Êï∞Â≠¶ÂíåÈÄªËæëÊé®ÁêÜ‰ªªÂä°‰∏äÁöÑË°®Áé∞ÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇÁé∞ÊúâÁöÑÊï∞Â≠¶ÈìæÂºèÊé®ÁêÜÊï∞ÊçÆÈõÜÂ∏∏Â∏∏Âõ†‰∏∫‰∏ìÂÆ∂ÁúÅÁï•‰∏≠Èó¥Ê≠•È™§ËÄåÂØºËá¥ÊÄùÁª¥Ë∑≥Ë∑ÉÔºåËøôÂØπÊ®°ÂûãÁöÑÂ≠¶‰π†ÂíåÊ≥õÂåñ‰∫ßÁîüË¥üÈù¢ÂΩ±Âìç„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜÈìæÂºèÊé®ÁêÜÊÄùÁª¥Ë∑≥Ë∑ÉÊ°•Êé•‰ªªÂä°ÔºåÊó®Âú®Ëá™Âä®Ê£ÄÊµãÊÄùÁª¥Ë∑≥Ë∑ÉÂπ∂ÁîüÊàêÁº∫Â§±ÁöÑ‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§Ôºå‰ª•ÊÅ¢Â§çÊé®ÁêÜÁöÑÂÆåÊï¥ÊÄßÂíåËøûË¥ØÊÄß„ÄÇÈÄöËøáÊûÑÂª∫‰∏ìÈó®ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜScaleQM+Âπ∂ËøõË°åÂÆûÈ™åÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜÂú®Ê°•Êé•Êï∞ÊçÆÈõÜ‰∏äÂæÆË∞ÉÁöÑÊ®°ÂûãÂú®Êï∞Â≠¶Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éÂéüÂßãÊï∞ÊçÆÈõÜÔºå‰∏îÂú®ÈÄªËæëÊé®ÁêÜ‰ªªÂä°‰∏ä‰πüÊòæÁ§∫Âá∫Êõ¥Â•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14604",
            "title": "Let LLMs Break Free from Overthinking via Self-Braking Tuning",
            "url": "https://huggingface.co/papers/2505.14604",
            "abstract": "A novel Self-Braking Tuning framework reduces overthinking and unnecessary computational overhead in large reasoning models by enabling the model to self-regulate its reasoning process.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have significantly enhanced their reasoning capabilities by generating longer chains of thought, demonstrating outstanding performance across a variety of tasks. However, this performance gain comes at the cost of a substantial increase in redundant reasoning during the generation process, leading to high computational overhead and exacerbating the issue of overthinking. Although numerous existing approaches aim to address the problem of overthinking, they often rely on external interventions. In this paper, we propose a novel framework, Self-Braking Tuning (SBT), which tackles overthinking from the perspective of allowing the model to regulate its own reasoning process, thus eliminating the reliance on external control mechanisms. We construct a set of overthinking identification metrics based on standard answers and design a systematic method to detect redundant reasoning. This method accurately identifies unnecessary steps within the reasoning trajectory and generates training signals for learning self-regulation behaviors. Building on this foundation, we develop a complete strategy for constructing data with adaptive reasoning lengths and introduce an innovative braking prompt mechanism that enables the model to naturally learn when to terminate reasoning at an appropriate point. Experiments across mathematical benchmarks (AIME, AMC, MATH500, GSM8K) demonstrate that our method reduces token consumption by up to 60% while maintaining comparable accuracy to unconstrained models.",
            "score": 18,
            "issue_id": 3917,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 –º–∞—è",
                "en": "May 20",
                "zh": "5Êúà20Êó•"
            },
            "hash": "20e4c9c407be15de",
            "authors": [
                "Haoran Zhao",
                "Yuchen Yan",
                "Yongliang Shen",
                "Haolei Xu",
                "Wenqi Zhang",
                "Kaitao Song",
                "Jian Shao",
                "Weiming Lu",
                "Jun Xiao",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "Tianjin University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14604.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–°–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—å –ò–ò: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Ä–∞–∑–¥—É–º–∏–π",
                    "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ Self-Braking Tuning –¥–ª—è –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –∏–º —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –∏–∑–±–µ–≥–∞—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —á—Ä–µ–∑–º–µ—Ä–Ω–æ–≥–æ –æ–±–¥—É–º—ã–≤–∞–Ω–∏—è –∏ –º–µ—Ç–æ–¥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ –≤ —Ü–µ–ø–æ—á–∫–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –¥–ª–∏–Ω–æ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –º–µ—Ö–∞–Ω–∏–∑–º '—Ç–æ—Ä–º–æ–∑—è—â–∏—Ö' –ø–æ–¥—Å–∫–∞–∑–æ–∫. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ 60% –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Empowering Models to Self-Regulate Reasoning for Efficiency",
                    "desc": "The paper introduces a Self-Braking Tuning (SBT) framework designed to enhance large reasoning models (LRMs) by allowing them to self-regulate their reasoning processes. This approach addresses the issue of overthinking, which leads to unnecessary computational overhead and redundant reasoning steps. By developing metrics to identify overthinking and creating a braking prompt mechanism, the model learns to determine when to stop reasoning effectively. Experimental results show that SBT can reduce token usage by up to 60% while preserving accuracy, making it a more efficient solution for reasoning tasks."
                },
                "zh": {
                    "title": "Ëá™ÊàëË∞ÉËäÇÔºå‰ºòÂåñÊé®ÁêÜÊïàÁéá",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËá™ÊàëÂà∂Âä®Ë∞É‰ºòÊ°ÜÊû∂ÔºàSelf-Braking Tuning, SBTÔºâÔºåÊó®Âú®ÂáèÂ∞ëÂ§ßÂûãÊé®ÁêÜÊ®°Âûã‰∏≠ÁöÑËøáÂ∫¶ÊÄùËÄÉÂíå‰∏çÂøÖË¶ÅÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇÈÄöËøáÂÖÅËÆ∏Ê®°ÂûãËá™ÊàëË∞ÉËäÇÊé®ÁêÜËøáÁ®ãÔºåSBTÊ∂àÈô§‰∫ÜÂØπÂ§ñÈÉ®ÊéßÂà∂Êú∫Âà∂ÁöÑ‰æùËµñ„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏ÄÂ•óÂü∫‰∫éÊ†áÂáÜÁ≠îÊ°àÁöÑËøáÂ∫¶ÊÄùËÄÉËØÜÂà´ÊåáÊ†áÔºåÂπ∂ËÆæËÆ°‰∫Ü‰∏ÄÁßçÁ≥ªÁªüÁöÑÊñπÊ≥ïÊù•Ê£ÄÊµãÂÜó‰ΩôÊé®ÁêÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®‰øùÊåÅÁõ∏‰ººÂáÜÁ°ÆÁéáÁöÑÂêåÊó∂ÔºåËÉΩÂ§üÂ∞Ü‰ª§ÁâåÊ∂àËÄóÂáèÂ∞ëÂ§öËææ60%„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15952",
            "title": "VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game\n  Quality Assurance",
            "url": "https://huggingface.co/papers/2505.15952",
            "abstract": "A benchmark called VideoGameQA-Bench is introduced to assess Vision-Language Models in video game quality assurance tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t With video games now generating the highest revenues in the entertainment industry, optimizing game development workflows has become essential for the sector's sustained growth. Recent advancements in Vision-Language Models (VLMs) offer considerable potential to automate and enhance various aspects of game development, particularly Quality Assurance (QA), which remains one of the industry's most labor-intensive processes with limited automation options. To accurately evaluate the performance of VLMs in video game QA tasks and determine their effectiveness in handling real-world scenarios, there is a clear need for standardized benchmarks, as existing benchmarks are insufficient to address the specific requirements of this domain. To bridge this gap, we introduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array of game QA activities, including visual unit testing, visual regression testing, needle-in-a-haystack tasks, glitch detection, and bug report generation for both images and videos of various games. Code and data are available at: https://asgaardlab.github.io/videogameqa-bench/",
            "score": 17,
            "issue_id": 3915,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "f9307168ac97ad24",
            "authors": [
                "Mohammad Reza Taesiri",
                "Abhijay Ghildyal",
                "Saman Zadtootaghaj",
                "Nabajeet Barman",
                "Cor-Paul Bezemer"
            ],
            "affiliations": [
                "Sony Interactive Entertainment, Aliso Viejo, US",
                "Sony Interactive Entertainment, Berlin, Germany",
                "Sony Interactive Entertainment, London, UK",
                "University of Alberta, CA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15952.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#cv",
                    "#video",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "üéÆ",
                "ru": {
                    "title": "VideoGameQA-Bench: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ–∏–≥—Ä",
                    "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ VideoGameQA-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ–∏–≥—Ä. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∑–∞–¥–∞—á, –≤–∫–ª—é—á–∞—è –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –º–æ–¥—É–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ä–µ–≥—Ä–µ—Å—Å–∏–π, –ø–æ–∏—Å–∫ –∏–≥–ª –≤ —Å—Ç–æ–≥–µ —Å–µ–Ω–∞, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≥–ª–∏—Ç—á–µ–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç—á–µ—Ç–æ–≤ –æ–± –æ—à–∏–±–∫–∞—Ö. –û–Ω —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –¥–ª—è —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏–≥—Ä. VideoGameQA-Bench –ø—Ä–∏–∑–≤–∞–Ω –∑–∞–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ–±–µ–ª –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –∫ —Å–ø–µ—Ü–∏—Ñ–∏–∫–µ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ –≤–∏–¥–µ–æ–∏–≥—Ä."
                },
                "en": {
                    "title": "Revolutionizing Game QA with VideoGameQA-Bench",
                    "desc": "The paper presents VideoGameQA-Bench, a new benchmark designed to evaluate Vision-Language Models (VLMs) specifically for video game quality assurance tasks. It addresses the need for standardized assessments in a field where existing benchmarks do not meet the unique challenges of game development. By focusing on various QA activities such as visual unit testing and glitch detection, this benchmark aims to enhance the automation of quality assurance processes in the gaming industry. The introduction of VideoGameQA-Bench is a significant step towards improving the efficiency and effectiveness of game development workflows."
                },
                "zh": {
                    "title": "ÊèêÂçáÊ∏∏ÊàèÂºÄÂèëË¥®ÈáèÁöÑÊô∫ËÉΩËØÑ‰º∞Â∑•ÂÖ∑",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫VideoGameQA-BenchÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®ËßÜÈ¢ëÊ∏∏ÊàèË¥®Èáè‰øùËØÅ‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÈöèÁùÄËßÜÈ¢ëÊ∏∏ÊàèÂú®Â®±‰πêË°å‰∏ö‰∏≠‰∫ßÁîüÁöÑÊî∂ÂÖ•ÊúÄÈ´òÔºå‰ºòÂåñÊ∏∏ÊàèÂºÄÂèëÊµÅÁ®ãÂèòÂæóËá≥ÂÖ≥ÈáçË¶Å„ÄÇËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊúÄÊñ∞ËøõÂ±ï‰∏∫Ëá™Âä®ÂåñÂíåÊèêÂçáÊ∏∏ÊàèÂºÄÂèëÁöÑÂêÑ‰∏™ÊñπÈù¢Êèê‰æõ‰∫ÜÂ∑®Â§ßÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®Âä≥Âä®ÂØÜÈõÜÂûãÁöÑË¥®Èáè‰øùËØÅÁéØËäÇ„ÄÇ‰∏∫‰∫ÜÂáÜÁ°ÆËØÑ‰º∞Ëøô‰∫õÊ®°ÂûãÂú®ÂÆûÈôÖÂú∫ÊôØ‰∏≠ÁöÑÊúâÊïàÊÄßÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂü∫ÂáÜÔºåÊ∂µÁõñ‰∫ÜÂ§öÁßçÊ∏∏ÊàèQAÊ¥ªÂä®ÔºåÂåÖÊã¨ËßÜËßâÂçïÂÖÉÊµãËØï„ÄÅËßÜËßâÂõûÂΩíÊµãËØï„ÄÅÊïÖÈöúÊ£ÄÊµãÁ≠â„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16990",
            "title": "Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel\n  Decoding",
            "url": "https://huggingface.co/papers/2505.16990",
            "abstract": "Dimple, a Discrete Diffusion Multimodal Large Language Model, achieves performance comparable to autoregressive models through a hybrid training approach and enhances inference efficiency with confident decoding and structure priors.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we propose Dimple, the first Discrete Diffusion Multimodal Large Language Model (DMLLM). We observe that training with a purely discrete diffusion approach leads to significant training instability, suboptimal performance, and severe length bias issues. To address these challenges, we design a novel training paradigm that combines an initial autoregressive phase with a subsequent diffusion phase. This approach yields the Dimple-7B model, trained on the same dataset and using a similar training pipeline as LLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%, demonstrating that DMLLM can achieve performance comparable to that of autoregressive models. To improve inference efficiency, we propose a decoding strategy termed confident decoding, which dynamically adjusts the number of tokens generated at each step, significantly reducing the number of generation iterations. In autoregressive models, the number of forward iterations during generation equals the response length. With confident decoding, however, the number of iterations needed by Dimple is even only text{response length}{3}. We also re-implement the prefilling technique in autoregressive models and demonstrate that it does not significantly impact performance on most benchmark evaluations, while offering a speedup of 1.5x to 7x. Additionally, we explore Dimple's capability to precisely control its response using structure priors. These priors enable structured responses in a manner distinct from instruction-based or chain-of-thought prompting, and allow fine-grained control over response format and length, which is difficult to achieve in autoregressive models. Overall, this work validates the feasibility and advantages of DMLLM and enhances its inference efficiency and controllability. Code and models are available at https://github.com/yu-rp/Dimple.",
            "score": 14,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "5c0636fbc17936b5",
            "authors": [
                "Runpeng Yu",
                "Xinyin Ma",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16990.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#inference",
                    "#diffusion",
                    "#optimization",
                    "#open_source",
                    "#architecture",
                    "#training"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "Dimple: –î–∏—Å–∫—Ä–µ—Ç–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "Dimple - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –¥–∏—Å–∫—Ä–µ—Ç–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (DMLLM). –û–Ω–∞ —Å–æ—á–µ—Ç–∞–µ—Ç –∞–≤—Ç–æ—Ä–µ√£—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å—Ä–∞–≤–Ω–∏–º–æ–π —Å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã–≤–æ–¥–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∞—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ú–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –æ—Ç–≤–µ—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –ø—Ä–∏–æ—Ä–æ–≤."
                },
                "en": {
                    "title": "Dimple: A New Era in Efficient Language Modeling",
                    "desc": "Dimple is a new type of language model called a Discrete Diffusion Multimodal Large Language Model (DMLLM) that combines two training methods to improve performance and stability. It starts with an autoregressive training phase, which helps to stabilize the learning process, followed by a diffusion phase that enhances the model's capabilities. Dimple-7B outperforms existing models like LLaVA-NEXT by 3.9%, showing that it can compete with traditional autoregressive models. Additionally, it introduces a confident decoding strategy that reduces the number of iterations needed for generating responses, making it faster and more efficient while allowing for better control over the output format and length."
                },
                "zh": {
                    "title": "DimpleÔºöÈ´òÊïàÁöÑÁ¶ªÊï£Êâ©Êï£Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°Âûã",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫ÜDimpleÔºå‰∏Ä‰∏™Á¶ªÊï£Êâ©Êï£Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàDMLLMÔºâÔºåÈÄöËøáÊ∑∑ÂêàËÆ≠ÁªÉÊñπÊ≥ïÂÆûÁé∞‰∫Ü‰∏éËá™ÂõûÂΩíÊ®°ÂûãÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÂçïÁ∫ØÁöÑÁ¶ªÊï£Êâ©Êï£ËÆ≠ÁªÉÊñπÊ≥ï‰ºöÂØºËá¥ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆö„ÄÅÊÄßËÉΩ‰∏ç‰Ω≥Âíå‰∏•ÈáçÁöÑÈïøÂ∫¶ÂÅèÂ∑Æ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËÆ≠ÁªÉËåÉÂºèÔºåÁªìÂêà‰∫ÜÂàùÂßãÁöÑËá™ÂõûÂΩíÈò∂ÊÆµÂíåÂêéÁª≠ÁöÑÊâ©Êï£Èò∂ÊÆµ„ÄÇDimpleÊ®°ÂûãÂú®Êé®ÁêÜÊïàÁéá‰∏ä‰πüÊúâÊâÄÊèêÂçáÔºåÈááÁî®‰∫ÜÂä®ÊÄÅË∞ÉÊï¥ÁîüÊàê‰ª§ÁâåÊï∞ÈáèÁöÑËá™‰ø°Ëß£Á†ÅÁ≠ñÁï•ÔºåÊòæËëóÂáèÂ∞ë‰∫ÜÁîüÊàêËø≠‰ª£Ê¨°Êï∞„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16916",
            "title": "Backdoor Cleaning without External Guidance in MLLM Fine-tuning",
            "url": "https://huggingface.co/papers/2505.16916",
            "abstract": "Multimodal Large Language Models (MLLMs) are increasingly deployed in fine-tuning-as-a-service (FTaaS) settings, where user-submitted datasets adapt general-purpose models to downstream tasks. This flexibility, however, introduces serious security risks, as malicious fine-tuning can implant backdoors into MLLMs with minimal effort. In this paper, we observe that backdoor triggers systematically disrupt cross-modal processing by causing abnormal attention concentration on non-semantic regions--a phenomenon we term attention collapse. Based on this insight, we propose Believe Your Eyes (BYE), a data filtering framework that leverages attention entropy patterns as self-supervised signals to identify and filter backdoor samples. BYE operates via a three-stage pipeline: (1) extracting attention maps using the fine-tuned model, (2) computing entropy scores and profiling sensitive layers via bimodal separation, and (3) performing unsupervised clustering to remove suspicious samples. Unlike prior defenses, BYE equires no clean supervision, auxiliary labels, or model modifications. Extensive experiments across various datasets, models, and diverse trigger types validate BYE's effectiveness: it achieves near-zero attack success rates while maintaining clean-task performance, offering a robust and generalizable solution against backdoor threats in MLLMs.",
            "score": 14,
            "issue_id": 3914,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "36dbc47d484f0a53",
            "authors": [
                "Xuankun Rong",
                "Wenke Huang",
                "Jian Liang",
                "Jinhe Bi",
                "Xun Xiao",
                "Yiming Li",
                "Bo Du",
                "Mang Ye"
            ],
            "affiliations": [
                "Huawei Technologies",
                "Munich Research Center",
                "Nanyang Technological University",
                "School of Computer Science, Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16916.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#data",
                    "#training",
                    "#dataset",
                    "#security"
                ],
                "emoji": "üõ°Ô∏è",
                "ru": {
                    "title": "–ó–∞—â–∏—Ç–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò –æ—Ç —Å–∫—Ä—ã—Ç—ã—Ö —É–≥—Ä–æ–∑: –¥–æ–≤–µ—Ä—è–π —Å–≤–æ–∏–º –≥–ª–∞–∑–∞–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã –≤—ã–∑—ã–≤–∞—é—Ç –∞–Ω–æ–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –Ω–µ—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –Ω–∞–∑–≤–∞–≤ —ç—Ç–æ —è–≤–ª–µ–Ω–∏–µ '–∫–æ–ª–ª–∞–ø—Å–æ–º –≤–Ω–∏–º–∞–Ω–∏—è'. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –æ–Ω–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ BYE –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ backdoor-–æ–±—Ä–∞–∑—Ü–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –ø–∞—Ç—Ç–µ—Ä–Ω—ã —ç–Ω—Ç—Ä–æ–ø–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è. BYE —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ç—Ä–∏ —ç—Ç–∞–ø–∞ –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —á–∏—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –∑–∞—â–∏—Ç—É –æ—Ç backdoor-–∞—Ç–∞–∫ –Ω–∞ MLLM."
                },
                "en": {
                    "title": "Defending MLLMs: Believe Your Eyes Against Backdoors!",
                    "desc": "This paper addresses the security risks associated with Multimodal Large Language Models (MLLMs) in fine-tuning-as-a-service (FTaaS) environments, where malicious fine-tuning can introduce backdoors. The authors identify a phenomenon called 'attention collapse', where backdoor triggers cause abnormal attention focus on irrelevant areas, disrupting cross-modal processing. To combat this, they propose a framework called Believe Your Eyes (BYE), which uses attention entropy patterns to filter out backdoor samples without needing clean supervision or model changes. BYE demonstrates strong effectiveness in various scenarios, achieving low attack success rates while preserving performance on clean tasks."
                },
                "zh": {
                    "title": "ÊäµÂæ°ÂêéÈó®Â®ÅËÉÅÁöÑÂàõÊñ∞Ëß£ÂÜ≥ÊñπÊ°à",
                    "desc": "Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÂæÆË∞ÉÊúçÂä°‰∏≠Ë∂äÊù•Ë∂äÂ∏∏ËßÅÔºå‰ΩÜËøô‰πüÂ∏¶Êù•‰∫ÜÂÆâÂÖ®È£éÈô©ÔºåÊÅ∂ÊÑèÂæÆË∞ÉÂèØËÉΩ‰ºöÂú®Ê®°Âûã‰∏≠Ê§çÂÖ•ÂêéÈó®„ÄÇÊú¨ÊñáËßÇÂØüÂà∞ÔºåÂêéÈó®Ëß¶ÂèëÂô®‰ºöÂØºËá¥Ë∑®Ê®°ÊÄÅÂ§ÑÁêÜÁöÑÂºÇÂ∏∏Ê≥®ÊÑèÂäõÈõÜ‰∏≠ÔºåÂΩ¢ÊàêÊàë‰ª¨Áß∞‰πã‰∏∫Ê≥®ÊÑèÂäõÂ¥©Ê∫ÉÁöÑÁé∞Ë±°„ÄÇÂü∫‰∫éËøô‰∏ÄÂèëÁé∞ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü\"Áõ∏‰ø°‰Ω†ÁöÑÁúºÁùõ\"ÔºàBYEÔºâÊï∞ÊçÆËøáÊª§Ê°ÜÊû∂ÔºåÈÄöËøáÊ≥®ÊÑèÂäõÁÜµÊ®°Âºè‰Ωú‰∏∫Ëá™ÁõëÁù£‰ø°Âè∑Êù•ËØÜÂà´ÂíåËøáÊª§ÂêéÈó®Ê†∑Êú¨„ÄÇBYEÈÄöËøá‰∏â‰∏™Èò∂ÊÆµÁöÑÊµÅÁ®ãÊìç‰ΩúÔºåËÉΩÂ§üÂú®‰∏çÈúÄË¶ÅÂπ≤ÂáÄÁõëÁù£ÊàñÊ®°Âûã‰øÆÊîπÁöÑÊÉÖÂÜµ‰∏ãÔºåÊúâÊïàÂú∞ÊäµÂæ°MLLMs‰∏≠ÁöÑÂêéÈó®Â®ÅËÉÅ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17018",
            "title": "SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward",
            "url": "https://huggingface.co/papers/2505.17018",
            "abstract": "An enhanced multimodal language model incorporates thinking process rewards to improve reasoning and generalization, achieving superior performance on benchmarks compared to larger models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances have shown success in eliciting strong reasoning abilities in multimodal large language models (MLLMs) through rule-based reinforcement learning (RL) with outcome rewards. However, this paradigm typically lacks supervision over the thinking process leading to the final outcome.As a result, the model may learn sub-optimal reasoning strategies, which can hinder its generalization ability. In light of this, we propose SophiaVL-R1, as an attempt to add reward signals for the thinking process in this paradigm. To achieve this, we first train a thinking reward model that evaluates the quality of the entire thinking process. Given that the thinking reward may be unreliable for certain samples due to reward hacking, we propose the Trust-GRPO method, which assigns a trustworthiness weight to the thinking reward during training. This weight is computed based on the thinking reward comparison of responses leading to correct answers versus incorrect answers, helping to mitigate the impact of potentially unreliable thinking rewards. Moreover, we design an annealing training strategy that gradually reduces the thinking reward over time, allowing the model to rely more on the accurate rule-based outcome reward in later training stages. Experiments show that our SophiaVL-R1 surpasses a series of reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU), demonstrating strong reasoning and generalization capabilities. Notably, our SophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite the latter having 10 times more parameters. All code, models, and datasets are made publicly available at https://github.com/kxfan2002/SophiaVL-R1.",
            "score": 12,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "a49e142829760753",
            "authors": [
                "Kaixuan Fan",
                "Kaituo Feng",
                "Haoming Lyu",
                "Dongzhan Zhou",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "MMLab, The Chinese University of Hong Kong",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17018.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#rlhf",
                    "#rl",
                    "#optimization",
                    "#benchmark",
                    "#open_source",
                    "#training"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–£–ª—É—á—à–µ–Ω–∏–µ –º—ã—à–ª–µ–Ω–∏—è –ò–ò —á–µ—Ä–µ–∑ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —É–ª—É—á—à–µ–Ω–Ω—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å SophiaVL-R1, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∑–∞ –ø—Ä–æ—Ü–µ—Å—Å –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –æ–±–æ–±—â–µ–Ω–∏—è. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ Trust-GRPO, –∫–æ—Ç–æ—Ä—ã–π –≤–∑–≤–µ—à–∏–≤–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –∑–∞ –º—ã—à–ª–µ–Ω–∏–µ. –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ—Ç–∂–∏–≥–∞, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–º–µ–Ω—å—à–∞—é—â–∞—è –≤–ª–∏—è–Ω–∏–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –∑–∞ –º—ã—à–ª–µ–Ω–∏–µ –≤ –ø–æ–ª—å–∑—É –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –∑–∞ –∫–æ–Ω–µ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. SophiaVL-R1 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —Å–∏–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –æ–±–æ–±—â–µ–Ω–∏—é."
                },
                "en": {
                    "title": "Empowering Reasoning with Thinking Process Rewards",
                    "desc": "This paper introduces SophiaVL-R1, a multimodal language model that enhances reasoning and generalization by incorporating rewards for the thinking process. Traditional reinforcement learning methods often overlook the quality of reasoning, leading to suboptimal strategies. To address this, the authors develop a thinking reward model that evaluates reasoning quality and implement a Trust-GRPO method to ensure reliable reward signals. Their experiments demonstrate that SophiaVL-R1 outperforms larger models on various benchmarks, showcasing its superior reasoning capabilities."
                },
                "zh": {
                    "title": "ÂºïÂÖ•ÊÄùÁª¥Â•ñÂä±ÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÁöÑÂ§öÊ®°ÊÄÅÊ®°Âûã",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ¢ûÂº∫ÁöÑÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãSophiaVL-R1ÔºåÊó®Âú®ÈÄöËøáÂºïÂÖ•ÊÄùÁª¥ËøáÁ®ãÂ•ñÂä±Êù•ÊîπÂñÑÊé®ÁêÜÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ‰º†ÁªüÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®Êé®ÁêÜÊó∂Áº∫‰πèÂØπÊÄùÁª¥ËøáÁ®ãÁöÑÁõëÁù£ÔºåÂèØËÉΩÂØºËá¥Â≠¶‰π†Âà∞Ê¨°‰ºòÁöÑÊé®ÁêÜÁ≠ñÁï•„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÁ†îÁ©∂ËÄÖ‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™ÊÄùÁª¥Â•ñÂä±Ê®°ÂûãÔºåÂπ∂ÊèêÂá∫‰∫ÜTrust-GRPOÊñπÊ≥ïÊù•ËØÑ‰º∞ÊÄùÁª¥Â•ñÂä±ÁöÑÂèØ‰ø°Â∫¶„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSophiaVL-R1Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éËÆ∏Â§öÂ§ßÂûãÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âº∫Â§ßÁöÑÊé®ÁêÜÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16967",
            "title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard\n  Negatives for Robust Information Retrieval",
            "url": "https://huggingface.co/papers/2505.16967",
            "abstract": "Using cascading LLM prompts to identify and relabel false negatives in datasets improves retrieval and reranking models' performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Training robust retrieval and reranker models typically relies on large-scale retrieval datasets; for example, the BGE collection contains 1.6 million query-passage pairs sourced from various data sources. However, we find that certain datasets can negatively impact model effectiveness -- pruning 8 out of 15 datasets from the BGE collection reduces the training set size by 2.35times and increases nDCG@10 on BEIR by 1.0 point. This motivates a deeper examination of training data quality, with a particular focus on \"false negatives\", where relevant passages are incorrectly labeled as irrelevant. We propose a simple, cost-effective approach using cascading LLM prompts to identify and relabel hard negatives. Experimental results show that relabeling false negatives with true positives improves both E5 (base) and Qwen2.5-7B retrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot AIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on the relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the cascading design is further supported by human annotation results, where we find judgment by GPT-4o shows much higher agreement with humans than GPT-4o-mini.",
            "score": 12,
            "issue_id": 3931,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "d40b350aefa1642b",
            "authors": [
                "Nandan Thakur",
                "Crystina Zhang",
                "Xueguang Ma",
                "Jimmy Lin"
            ],
            "affiliations": [
                "David R. Cheriton School of Computer Science, University of Waterloo, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16967.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training",
                    "#dataset"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–£–ª—É—á—à–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ —É–º–Ω—É—é –ø–µ—Ä–µ—Ä–∞–∑–º–µ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É—è –∫–∞—Å–∫–∞–¥–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –∞–≤—Ç–æ—Ä—ã –≤—ã—è–≤–ª—è—é—Ç –∏ –ø–µ—Ä–µ—Ä–∞–∑–º–µ—á–∞—é—Ç –ª–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –≤ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–∞–∫ –º–æ–¥–µ–ª–µ–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞ (E5, Qwen2.5-7B), —Ç–∞–∫ –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫–∞ (Qwen2.5-3B) –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö BEIR –∏ AIR-Bench. –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç—Å—è –≤—ã—Å–æ–∫–∏–º —Å–æ–≥–ª–∞—Å–∏–µ–º —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π."
                },
                "en": {
                    "title": "Enhancing Model Performance by Relabeling False Negatives",
                    "desc": "This paper discusses improving the performance of retrieval and reranking models by addressing the issue of false negatives in training datasets. The authors demonstrate that by pruning ineffective datasets and focusing on relabeling incorrectly labeled relevant passages, they can enhance model effectiveness significantly. They introduce a method using cascading LLM prompts to identify and correct these false negatives, leading to measurable improvements in nDCG scores on benchmark evaluations. The results indicate that better training data quality directly correlates with enhanced model performance, showcasing the importance of accurate labeling in machine learning."
                },
                "zh": {
                    "title": "ÊèêÂçáÊ®°ÂûãÊÄßËÉΩÁöÑÂÖ≥ÈîÆÔºöÈáçÊñ∞Ê†áËÆ∞ÈîôËØØË¥üÊ†∑Êú¨",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰ΩøÁî®Á∫ßËÅîÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊèêÁ§∫Êù•ËØÜÂà´ÂíåÈáçÊñ∞Ê†áËÆ∞Êï∞ÊçÆÈõÜ‰∏≠ÈîôËØØÊ†áËÆ∞ÁöÑË¥üÊ†∑Êú¨Ôºå‰ªéËÄåÊèêÈ´òÊ£ÄÁ¥¢ÂíåÈáçÊéíÂ∫èÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊüê‰∫õÊï∞ÊçÆÈõÜ‰ºöÂØπÊ®°ÂûãÊïàÊûú‰∫ßÁîüË¥üÈù¢ÂΩ±ÂìçÔºåÂéªÈô§‰∏çÂøÖË¶ÅÁöÑÊï∞ÊçÆÈõÜÂèØ‰ª•ÊòæËëóÊèêÈ´òÊ®°ÂûãÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçï‰∏îÁªèÊµéÁöÑÊñπÊ≥ïÔºåÈÄöËøáÁ∫ßËÅîÊèêÁ§∫Êù•ËØÜÂà´ÂíåÈáçÊñ∞Ê†áËÆ∞Ëøô‰∫õÈöæ‰ª•ËØÜÂà´ÁöÑË¥üÊ†∑Êú¨„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÈáçÊñ∞Ê†áËÆ∞ÂêéÁöÑÊï∞ÊçÆÊòæËëóÊèêÂçá‰∫ÜÊ£ÄÁ¥¢Ê®°ÂûãÁöÑÊïàÊûúÔºåÈ™åËØÅ‰∫ÜÊï∞ÊçÆË¥®ÈáèÂØπÊ®°ÂûãËÆ≠ÁªÉÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16864",
            "title": "Training-Free Efficient Video Generation via Dynamic Token Carving",
            "url": "https://huggingface.co/papers/2505.16864",
            "abstract": "Jenga, a novel inference pipeline for video Diffusion Transformer models, combines dynamic attention carving and progressive resolution generation to significantly speed up video generation while maintaining high quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the remarkable generation quality of video Diffusion Transformer (DiT) models, their practical deployment is severely hindered by extensive computational requirements. This inefficiency stems from two key challenges: the quadratic complexity of self-attention with respect to token length and the multi-step nature of diffusion models. To address these limitations, we present Jenga, a novel inference pipeline that combines dynamic attention carving with progressive resolution generation. Our approach leverages two key insights: (1) early denoising steps do not require high-resolution latents, and (2) later steps do not require dense attention. Jenga introduces a block-wise attention mechanism that dynamically selects relevant token interactions using 3D space-filling curves, alongside a progressive resolution strategy that gradually increases latent resolution during generation. Experimental results demonstrate that Jenga achieves substantial speedups across multiple state-of-the-art video diffusion models while maintaining comparable generation quality (8.83times speedup with 0.01\\% performance drop on VBench). As a plug-and-play solution, Jenga enables practical, high-quality video generation on modern hardware by reducing inference time from minutes to seconds -- without requiring model retraining. Code: https://github.com/dvlab-research/Jenga",
            "score": 12,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "ec67c352f5ae81d7",
            "authors": [
                "Yuechen Zhang",
                "Jinbo Xing",
                "Bin Xia",
                "Shaoteng Liu",
                "Bohao Peng",
                "Xin Tao",
                "Pengfei Wan",
                "Eric Lo",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "Kuaishou Technology",
                "SmartMore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16864.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#inference",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "üß©",
                "ru": {
                    "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞",
                    "desc": "Jenga - —ç—Ç–æ –Ω–æ–≤—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –≤—ã–≤–æ–¥–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—á–µ—Ç–∞–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–µ–∑–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤–∏–¥–µ–æ, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ. Jenga –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–ª–æ—á–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Ç–æ–∫–µ–Ω–æ–≤ —Å –ø–æ–º–æ—â—å—é 3D-–∫—Ä–∏–≤—ã—Ö, –∑–∞–ø–æ–ª–Ω—è—é—â–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Jenga –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "Jenga: Speeding Up Video Generation with Smart Attention!",
                    "desc": "The paper introduces Jenga, an innovative inference pipeline designed for video Diffusion Transformer models that enhances video generation speed while preserving quality. It tackles the computational inefficiencies caused by the quadratic complexity of self-attention and the multi-step nature of diffusion processes. Jenga employs dynamic attention carving to selectively focus on relevant token interactions and utilizes progressive resolution generation to optimize the use of high-resolution latents. Experimental results show that Jenga can significantly accelerate video generation, achieving up to 8.83 times faster performance with minimal quality loss, making it a practical solution for real-time applications."
                },
                "zh": {
                    "title": "JengaÔºöÂä†ÈÄüËßÜÈ¢ëÁîüÊàêÁöÑÂàõÊñ∞Êé®ÁêÜÁÆ°ÈÅì",
                    "desc": "JengaÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑËßÜÈ¢ëÊâ©Êï£ÂèòÊç¢Âô®Ê®°ÂûãÊé®ÁêÜÁÆ°ÈÅìÔºåÁªìÂêà‰∫ÜÂä®ÊÄÅÊ≥®ÊÑèÂäõÂàáÂâ≤ÂíåÊ∏êËøõÂàÜËæ®ÁéáÁîüÊàêÔºåÊòæËëóÂä†Âø´‰∫ÜËßÜÈ¢ëÁîüÊàêÈÄüÂ∫¶ÔºåÂêåÊó∂‰øùÊåÅÈ´òË¥®Èáè„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜËá™Ê≥®ÊÑèÂäõÁöÑÂπ≥ÊñπÂ§çÊùÇÂ∫¶ÂíåÊâ©Êï£Ê®°ÂûãÁöÑÂ§öÊ≠•È™§ÁâπÊÄßÂ∏¶Êù•ÁöÑËÆ°ÁÆóÊïàÁéáÈóÆÈ¢ò„ÄÇJengaÈÄöËøáÂä®ÊÄÅÈÄâÊã©Áõ∏ÂÖ≥ÁöÑÊ†áËÆ∞‰∫§‰∫íÂíåÈÄêÊ≠•ÊèêÈ´òÊΩúÂú®ÂàÜËæ®ÁéáÔºå‰ºòÂåñ‰∫ÜÁîüÊàêËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåJengaÂú®Â§ö‰∏™ÊúÄÂÖàËøõÁöÑËßÜÈ¢ëÊâ©Êï£Ê®°Âûã‰∏äÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÂä†ÈÄüÔºåÂêåÊó∂ÁîüÊàêË¥®Èáè‰øùÊåÅÁõ∏ÂΩì„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17012",
            "title": "SpatialScore: Towards Unified Evaluation for Multimodal Spatial\n  Understanding",
            "url": "https://huggingface.co/papers/2505.17012",
            "abstract": "SpatialScore benchmarks multimodal large language models for 3D spatial understanding, revealing challenges and showcasing the effectiveness of SpatialAgent with specialized tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have achieved impressive success in question-answering tasks, yet their capabilities for spatial understanding are less explored. This work investigates a critical question: do existing MLLMs possess 3D spatial perception and understanding abilities? Concretely, we make the following contributions in this paper: (i) we introduce VGBench, a benchmark specifically designed to assess MLLMs for visual geometry perception, e.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most comprehensive and diverse multimodal spatial understanding benchmark to date, integrating VGBench with relevant data from the other 11 existing datasets. This benchmark comprises 28K samples across various spatial understanding tasks, modalities, and QA formats, along with a carefully curated challenging subset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent system incorporating 9 specialized tools for spatial understanding, supporting both Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive evaluations to reveal persistent challenges in spatial reasoning while demonstrating the effectiveness of SpatialAgent. We believe SpatialScore will offer valuable insights and serve as a rigorous benchmark for the next evolution of MLLMs.",
            "score": 10,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "51519403c5b92e25",
            "authors": [
                "Haoning Wu",
                "Xiao Huang",
                "Yaohui Chen",
                "Ya Zhang",
                "Yanfeng Wang",
                "Weidi Xie"
            ],
            "affiliations": [
                "School of Artificial Intelligence, Shanghai Jiao Tong University",
                "Shanghai AI Laboratory",
                "Tianjin University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17012.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#survey",
                    "#3d",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "SpatialScore: –Ω–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ò–ò",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SpatialScore - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (MLLM). –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ VGBench –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏, –∞ —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–ª–∏ SpatialAgent - –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–µ—Å—è –ø—Ä–æ–±–ª–µ–º—ã –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö MLLM, –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ SpatialAgent. –ë–µ–Ω—á–º–∞—Ä–∫ SpatialScore –ø—Ä–∏–∑–≤–∞–Ω —Å—Ç–∞—Ç—å –≤–∞–∂–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è MLLM –≤ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è."
                },
                "en": {
                    "title": "Enhancing 3D Spatial Understanding in Multimodal Models",
                    "desc": "This paper introduces SpatialScore, a benchmark designed to evaluate multimodal large language models (MLLMs) in their ability to understand 3D spatial concepts. It highlights the development of VGBench, which focuses on visual geometry perception tasks like camera pose estimation. The benchmark includes 28,000 samples from various spatial understanding tasks and features a challenging subset called SpatialScore-Hard. Additionally, the paper presents SpatialAgent, a multi-agent system equipped with specialized tools to enhance spatial reasoning capabilities in MLLMs."
                },
                "zh": {
                    "title": "Á©∫Èó¥ÁêÜËß£ÁöÑÊñ∞Âü∫ÂáÜ‰∏éÂ∑•ÂÖ∑",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®‰∏âÁª¥Á©∫Èó¥ÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜVGBenchÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫éËØÑ‰º∞ËßÜËßâÂá†‰ΩïÊÑüÁü•ÁöÑÂü∫ÂáÜÔºåÊ∂µÁõñÁõ∏Êú∫ÂßøÊÄÅÂíåËøêÂä®‰º∞ËÆ°Á≠â‰ªªÂä°„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜSpatialScoreÔºåËøôÊòØËøÑ‰ªä‰∏∫Ê≠¢ÊúÄÂÖ®Èù¢ÁöÑÂ§öÊ®°ÊÄÅÁ©∫Èó¥ÁêÜËß£Âü∫ÂáÜÔºåÊï¥Âêà‰∫ÜÊù•Ëá™11‰∏™Áé∞ÊúâÊï∞ÊçÆÈõÜÁöÑÊï∞ÊçÆÔºåÂåÖÂê´28K‰∏™Ê†∑Êú¨Âíå‰∏Ä‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂ≠êÈõÜSpatialScore-Hard„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÂºÄÂèë‰∫ÜSpatialAgentÔºå‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÔºåÁªìÂêà‰∫Ü9‰∏™‰∏ìÈó®ÁöÑÂ∑•ÂÖ∑Ôºå‰ª•ÊîØÊåÅÁ©∫Èó¥ÁêÜËß£ÁöÑÊé®ÁêÜ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16839",
            "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
            "url": "https://huggingface.co/papers/2505.16839",
            "abstract": "LaViDa, a family of vision-language models built on discrete diffusion models, offers competitive performance on multimodal benchmarks with advantages in speed, controllability, and bidirectional reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Vision-Language Models (VLMs) can solve a wide range of tasks requiring visual reasoning. In real-world scenarios, desirable properties for VLMs include fast inference and controllable generation (e.g., constraining outputs to adhere to a desired format). However, existing autoregressive (AR) VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs) offer a promising alternative, enabling parallel decoding for faster inference and bidirectional context for controllable generation through text-infilling. While effective in language-only settings, DMs' potential for multimodal tasks is underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build LaViDa by equipping DMs with a vision encoder and jointly fine-tune the combined parts for multimodal instruction following. To address challenges encountered, LaViDa incorporates novel techniques such as complementary masking for effective training, prefix KV cache for efficient inference, and timestep shifting for high-quality sampling. Experiments show that LaViDa achieves competitive or superior performance to AR VLMs on multi-modal benchmarks such as MMMU, while offering unique advantages of DMs, including flexible speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO captioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x speedup. On bidirectional tasks, it achieves +59% improvement on Constrained Poem Completion. These results demonstrate LaViDa as a strong alternative to AR VLMs. Code and models will be released in the camera-ready version.",
            "score": 10,
            "issue_id": 3914,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "9e2202389256fc59",
            "authors": [
                "Shufan Li",
                "Konstantinos Kallidromitis",
                "Hritik Bansal",
                "Akash Gokul",
                "Yusuke Kato",
                "Kazuki Kozuka",
                "Jason Kuen",
                "Zhe Lin",
                "Kai-Wei Chang",
                "Aditya Grover"
            ],
            "affiliations": [
                "Adobe Research",
                "Panasonic AI Research",
                "Salesforce Research",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16839.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#architecture",
                    "#training",
                    "#games",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "LaViDa: –ë—ã—Å—Ç—Ä—ã–µ –∏ –≥–∏–±–∫–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏",
                    "desc": "LaViDa - —ç—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –ø—Ä–∏ —ç—Ç–æ–º –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤ —Å–∫–æ—Ä–æ—Å—Ç–∏, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ—Å—Ç–∏ –∏ –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, LaViDa –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–≥–æ –≤—ã–≤–æ–¥–∞ –∏ –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LaViDa –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã—Ö –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–º–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö."
                },
                "en": {
                    "title": "LaViDa: Fast and Controllable Vision-Language Models",
                    "desc": "LaViDa is a new family of vision-language models that utilize discrete diffusion models to enhance performance on multimodal tasks. It addresses the limitations of traditional autoregressive models by providing faster inference and better control over output generation. By integrating a vision encoder and employing techniques like complementary masking and prefix KV cache, LaViDa achieves high-quality results while maintaining efficiency. Experimental results show that LaViDa outperforms existing models in various benchmarks, demonstrating its potential as a robust alternative in the field of vision-language processing."
                },
                "zh": {
                    "title": "LaViDaÔºöÈ´òÊïàÂèØÊéßÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã",
                    "desc": "LaViDaÊòØ‰∏ÄÁßçÂü∫‰∫éÁ¶ªÊï£Êâ©Êï£Ê®°ÂûãÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂÆ∂ÊóèÔºåËÉΩÂ§üÂú®Â§öÊ®°ÊÄÅÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ‰∏éÁé∞ÊúâÁöÑËá™ÂõûÂΩíËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁõ∏ÊØîÔºåLaViDaÂú®Êé®ÁêÜÈÄüÂ∫¶„ÄÅÂèØÊéßÊÄßÂíåÂèåÂêëÊé®ÁêÜÊñπÈù¢ÂÖ∑ÊúâÊòéÊòæ‰ºòÂäø„ÄÇËØ•Ê®°ÂûãÈÄöËøáÁªìÂêàËßÜËßâÁºñÁ†ÅÂô®ÂíåËÅîÂêàÂæÆË∞ÉÊäÄÊúØÔºåÊèêÂçá‰∫ÜÂ§öÊ®°ÊÄÅ‰ªªÂä°ÁöÑÂ§ÑÁêÜËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLaViDaÂú®Â§öÊ®°ÊÄÅÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÊ®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂‰Ωú‰∏∫Ëá™ÂõûÂΩíÊ®°ÂûãÂº∫ÊúâÂäõÊõø‰ª£ÂìÅÁöÑÊΩúÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14625",
            "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning",
            "url": "https://huggingface.co/papers/2505.14625",
            "abstract": "Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RL's success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problem--false negatives--where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at https://github.com/uw-nsl/TinyV.",
            "score": 10,
            "issue_id": 3914,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 –º–∞—è",
                "en": "May 20",
                "zh": "5Êúà20Êó•"
            },
            "hash": "0bcb0b140b7623c3",
            "authors": [
                "Zhangchen Xu",
                "Yuetai Li",
                "Fengqing Jiang",
                "Bhaskar Ramasubramanian",
                "Luyao Niu",
                "Bill Yuchen Lin",
                "Radha Poovendran"
            ],
            "affiliations": [
                "University of Washington",
                "Western Washington University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14625.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–ë–æ—Ä—å–±–∞ —Å –ª–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è RL-–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –ª–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –±–æ–ª–µ–µ 38% –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π –Ω–µ–≤–µ—Ä–Ω–æ –æ—Ç–∫–ª–æ–Ω—è—é—Ç—Å—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Ö—É–¥—à–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä tinyV –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ø–æ–ª–Ω—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –∏ –≤—ã—è–≤–ª—è–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ª–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è tinyV –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ 10% –∏ —É—Å–∫–æ—Ä—è–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ LLM —Å –ø–æ–º–æ—â—å—é RL –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö."
                },
                "en": {
                    "title": "Enhancing RL Training by Tackling Verifier False Negatives with TinyV",
                    "desc": "This paper discusses the challenges of using Reinforcement Learning (RL) to improve large language models (LLMs) due to the issue of false negatives from verifiers. False negatives occur when verifiers incorrectly reject correct outputs from the model, which can hinder the RL training process by limiting the feedback the model receives. The authors analyze a dataset and find that a significant portion of model responses are misclassified, leading to slower learning and convergence. To address this, they introduce TinyV, a new verifier that enhances existing methods by identifying and correcting these false negatives, resulting in improved performance on math-reasoning tasks."
                },
                "zh": {
                    "title": "Ëß£ÂÜ≥ÂÅáÈò¥ÊÄßÔºåÊèêÂçáÂº∫ÂåñÂ≠¶‰π†ÊïàÊûúÔºÅ",
                    "desc": "Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÊòØ‰∏ÄÁßçÈÄöËøáÂ•ñÂä±‰ø°Âè∑‰ºòÂåñÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁ≠ñÁï•ÁöÑÂº∫Â§ßÂ∑•ÂÖ∑„ÄÇÁÑ∂ËÄåÔºåRLÁöÑÊàêÂäü‰æùËµñ‰∫éÈ™åËØÅËÄÖÊèê‰æõÁöÑÂèØÈù†Â•ñÂä±ÔºåËÄåÊàë‰ª¨ÂèëÁé∞‰∏Ä‰∏™ÊôÆÈÅçÂ≠òÂú®ÁöÑÈóÆÈ¢ò‚Äî‚ÄîÂÅáÈò¥ÊÄßÔºåÂç≥È™åËØÅËÄÖÈîôËØØÂú∞ÊãíÁªùÊ≠£Á°ÆÁöÑÊ®°ÂûãËæìÂá∫„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåË∂ÖËøá38%ÁöÑÊ®°ÂûãÁîüÊàêÁöÑÂìçÂ∫îÂèóÂà∞ÂÅáÈò¥ÊÄßÁöÑÂΩ±ÂìçÔºåËøô‰∏•ÈáçÊçüÂÆ≥‰∫ÜRLËÆ≠ÁªÉÁöÑÊïàÊûú„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜtinyVÔºå‰∏Ä‰∏™ËΩªÈáèÁ∫ßÁöÑLLMÂü∫Á°ÄÈ™åËØÅÂô®ÔºåÂèØ‰ª•Âä®ÊÄÅËØÜÂà´ÊΩúÂú®ÁöÑÂÅáÈò¥ÊÄßÔºå‰ªéËÄåÊèêÈ´òÂ•ñÂä±‰º∞ËÆ°ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16854",
            "title": "Think or Not? Selective Reasoning via Reinforcement Learning for\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2505.16854",
            "abstract": "TON, a two-stage training strategy combining supervised fine-tuning with thought dropout and Group Relative Policy Optimization, reduces unnecessary reasoning steps in vision-language models without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in vision-language models (VLMs). Group Relative Policy Optimization (GRPO) is a recent prominent method that encourages models to generate complete reasoning traces before answering, leading to increased token usage and computational cost. Inspired by the human-like thinking process-where people skip reasoning for easy questions but think carefully when needed-we explore how to enable VLMs to first decide when reasoning is necessary. To realize this, we propose TON, a two-stage training strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective 'thought dropout' operation, where reasoning traces are randomly replaced with empty thoughts. This introduces a think-or-not format that serves as a cold start for selective reasoning; (ii) a GRPO stage that enables the model to freely explore when to think or not, while maximizing task-aware outcome rewards. Experimental results show that TON can reduce the completion length by up to 90% compared to vanilla GRPO, without sacrificing performance or even improving it. Further evaluations across diverse vision-language tasks-covering a range of reasoning difficulties under both 3B and 7B models-consistently reveal that the model progressively learns to bypass unnecessary reasoning steps as training advances. These findings shed light on the path toward human-like reasoning patterns in reinforcement learning approaches. Our code is available at https://github.com/kokolerk/TON.",
            "score": 7,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "af930383188983e1",
            "authors": [
                "Jiaqi Wang",
                "Kevin Qinghong Lin",
                "James Cheng",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16854.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ò–ò —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è TON –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. TON —Å–æ—á–µ—Ç–∞–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –º–µ—Ç–æ–¥–æ–º '–æ—Ç—Å–µ–≤–∞ –º—ã—Å–ª–µ–π' –∏ –≥—Ä—É–ø–ø–æ–≤–æ–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏ (GRPO). –≠—Ç–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –∏–∑–±–µ–≥–∞—Ç—å –Ω–µ–Ω—É–∂–Ω—ã—Ö —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Å–æ–∫—Ä–∞—â–∞—è –¥–ª–∏–Ω—É –≤—ã–≤–æ–¥–∞ –Ω–∞ 90% –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É—á–∏—Ç—Å—è –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å –ª–∏—à–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –ø—Ä–∏–±–ª–∏–∂–∞—è—Å—å –∫ —á–µ–ª–æ–≤–µ–∫–æ–ø–æ–¥–æ–±–Ω–æ–º—É –ø–∞—Ç—Ç–µ—Ä–Ω—É –º—ã—à–ª–µ–Ω–∏—è."
                },
                "en": {
                    "title": "TON: Streamlining Reasoning in Vision-Language Models",
                    "desc": "The paper introduces TON, a two-stage training strategy designed to enhance vision-language models (VLMs) by reducing unnecessary reasoning steps. It combines supervised fine-tuning with a technique called 'thought dropout' to help models decide when reasoning is necessary, mimicking human-like thinking processes. The second stage employs Group Relative Policy Optimization (GRPO) to optimize the model's reasoning efficiency while maximizing task performance. Experimental results demonstrate that TON can significantly decrease reasoning length without compromising, and often improving, the model's performance across various tasks."
                },
                "zh": {
                    "title": "TONÔºö‰ºòÂåñËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÁöÑÂàõÊñ∞Á≠ñÁï•",
                    "desc": "TONÊòØ‰∏ÄÁßç‰∏§Èò∂ÊÆµÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÔºåÁªìÂêà‰∫ÜÁõëÁù£ÂæÆË∞ÉÂíåÊÄùÁª¥‰∏¢ÂºÉÔºåÊó®Âú®ÂáèÂ∞ëËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑ‰∏çÂøÖË¶ÅÊé®ÁêÜÊ≠•È™§ÔºåÂêåÊó∂‰øùÊåÅÊÄßËÉΩ„ÄÇÁ¨¨‰∏ÄÈò∂ÊÆµÈÄöËøáÁÆÄÂçïÊúâÊïàÁöÑ‚ÄúÊÄùÁª¥‰∏¢ÂºÉ‚ÄùÊìç‰ΩúÔºåÈöèÊú∫Áî®Á©∫ÊÄùÁª¥ÊõøÊç¢Êé®ÁêÜËΩ®ËøπÔºåÂºïÂÖ•‰∫ÜÈÄâÊã©ÊÄßÊé®ÁêÜÁöÑÊÄùËÄÉÊ†ºÂºè„ÄÇÁ¨¨‰∫åÈò∂ÊÆµÈááÁî®Áæ§‰ΩìÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºàGRPOÔºâÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üËá™Áî±ÂÜ≥ÂÆö‰ΩïÊó∂ËøõË°åÊé®ÁêÜÔºå‰ªéËÄåÊúÄÂ§ßÂåñ‰ªªÂä°Áõ∏ÂÖ≥ÁöÑÁªìÊûúÂ•ñÂä±„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTONÂèØ‰ª•Â∞ÜÂÆåÊàêÈïøÂ∫¶ÂáèÂ∞ëÂ§öËææ90%ÔºåËÄå‰∏çÁâ∫Áâ≤ÊÄßËÉΩÔºåÁîöËá≥Âú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÊèêÈ´ò‰∫ÜÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16421",
            "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2505.16421",
            "abstract": "WebAgent-R1 is an RL framework for training web agents in multi-turn interactions, achieving high success rates compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents.",
            "score": 7,
            "issue_id": 3928,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "9b6b6b9f01bfb9c0",
            "authors": [
                "Zhepei Wei",
                "Wenlin Yao",
                "Yao Liu",
                "Weizhi Zhang",
                "Qin Lu",
                "Liang Qiu",
                "Changlong Yu",
                "Puyang Xu",
                "Chao Zhang",
                "Bing Yin",
                "Hyokun Yun",
                "Lihong Li"
            ],
            "affiliations": [
                "Amazon",
                "Georgia Institute of Technology",
                "University of Virginia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16421.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#optimization",
                    "#rl",
                    "#benchmark",
                    "#games",
                    "#training"
                ],
                "emoji": "üï∏Ô∏è",
                "ru": {
                    "title": "WebAgent-R1: –ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–≥–æ RL",
                    "desc": "WebAgent-R1 - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö. –û–Ω –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –æ–Ω–ª–∞–π–Ω-–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —Å –≤–µ–±-—Å—Ä–µ–¥–∞–º–∏, –≥–µ–Ω–µ—Ä–∏—Ä—É—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤—É—è—Å—å –±–∏–Ω–∞—Ä–Ω—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Å–ø–µ—Ö–∞ –∑–∞–¥–∞—á–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ –≤—ã—è–≤–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –≤–µ–±-–∑–∞–¥–∞—á."
                },
                "en": {
                    "title": "Empowering Web Agents with Multi-Turn Reinforcement Learning",
                    "desc": "WebAgent-R1 is a reinforcement learning (RL) framework designed to train web agents for multi-turn interactions, which are more complex than single-turn tasks. It operates by learning from real-time interactions with web environments, using binary rewards to guide the training process based on task success. The framework has shown significant improvements in task success rates for large language models, outperforming existing methods and proprietary models. Additionally, it explores different initialization strategies to enhance the training process, emphasizing the role of behavior cloning and chain-of-thought reasoning in achieving better performance."
                },
                "zh": {
                    "title": "WebAgent-R1ÔºöÂ§öËΩÆ‰∫§‰∫íÁöÑÂº∫ÂåñÂ≠¶‰π†Êñ∞Ê°ÜÊû∂",
                    "desc": "WebAgent-R1ÊòØ‰∏Ä‰∏™Áî®‰∫éËÆ≠ÁªÉÁΩëÁªú‰ª£ÁêÜÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂Ôºå‰∏ìÊ≥®‰∫éÂ§öËΩÆ‰∫§‰∫íÔºåÊàêÂäüÁéáÊòæËëóÈ´ò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ‰∏é‰º†ÁªüÁöÑÂçïËΩÆ‰ªªÂä°‰∏çÂêåÔºåËØ•Ê°ÜÊû∂ËÉΩÂ§üÂ§ÑÁêÜÂ§çÊùÇÁöÑÈïøÊúüÂÜ≥Á≠ñÈóÆÈ¢òÔºåÈÄöËøáÂú®Á∫ø‰∫§‰∫íÂ≠¶‰π†Âπ∂ÁîüÊàêÂ§öÊ†∑ÂåñÁöÑËΩ®Ëøπ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWebAgent-R1Âú®WebArena-LiteÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫Ü‰ªªÂä°ÊàêÂäüÁéáÔºåË∂ÖË∂ä‰∫ÜÂΩìÂâçÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇËØ•Á†îÁ©∂ËøòÊé¢ËÆ®‰∫Ü‰∏çÂêåÁöÑÂº∫ÂåñÂ≠¶‰π†ÂàùÂßãÂåñÁ≠ñÁï•ÔºåÂº∫Ë∞É‰∫ÜÁÉ≠Ë∫´ËÆ≠ÁªÉÈò∂ÊÆµÁöÑÈáçË¶ÅÊÄßÔºåÂπ∂Êèê‰æõ‰∫ÜÂú®ÁΩëÁªú‰ª£ÁêÜ‰∏≠ËûçÂÖ•ÈïøÈìæÊÄùÁª¥Êé®ÁêÜÁöÑËßÅËß£„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16151",
            "title": "Training-Free Reasoning and Reflection in MLLMs",
            "url": "https://huggingface.co/papers/2505.16151",
            "abstract": "The FRANK Model enhances multimodal LLMs with reasoning and reflection abilities without retraining, using a hierarchical weight merging approach that merges visual-pretrained and reasoning-specialized models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Reasoning LLMs (e.g., DeepSeek-R1 and OpenAI-o1) have showcased impressive reasoning capabilities via reinforcement learning. However, extending these capabilities to Multimodal LLMs (MLLMs) is hampered by the prohibitive costs of retraining and the scarcity of high-quality, verifiable multimodal reasoning datasets. This paper introduces FRANK Model, a training-FRee ANd r1-liKe MLLM that imbues off-the-shelf MLLMs with reasoning and reflection abilities, without any gradient updates or extra supervision. Our key insight is to decouple perception and reasoning across MLLM decoder layers. Specifically, we observe that compared to the deeper decoder layers, the shallow decoder layers allocate more attention to visual tokens, while the deeper decoder layers concentrate on textual semantics. This observation motivates a hierarchical weight merging approach that combines a visual-pretrained MLLM with a reasoning-specialized LLM. To this end, we propose a layer-wise, Taylor-derived closed-form fusion mechanism that integrates reasoning capacity into deep decoder layers while preserving visual grounding in shallow decoder layers. Extensive experiments on challenging multimodal reasoning benchmarks demonstrate the effectiveness of our approach. On the MMMU benchmark, our model FRANK-38B achieves an accuracy of 69.2, outperforming the strongest baseline InternVL2.5-38B by +5.3, and even surpasses the proprietary GPT-4o model. Our project homepage is at: http://iip.whu.edu.cn/frank/index.html",
            "score": 7,
            "issue_id": 3914,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "59af0e553fdc317e",
            "authors": [
                "Hongchen Wei",
                "Zhenzhong Chen"
            ],
            "affiliations": [
                "School of Remote Sensing and Information Engineering, Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16151.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "FRANK: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è",
                    "desc": "–ú–æ–¥–µ–ª—å FRANK —É–ª—É—á—à–∞–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (MLLM), –¥–æ–±–∞–≤–ª—è—è –∏–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—é –≤–µ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—á–µ—Ç–∞–µ—Ç –º–æ–¥–µ–ª—å, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å –º–æ–¥–µ–ª—å—é, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—Å–ª–æ–π–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è –¢–µ–π–ª–æ—Ä–∞, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ –≥–ª—É–±–æ–∫–∏–µ —Å–ª–æ–∏ –¥–µ–∫–æ–¥–µ—Ä–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –≤–∏–∑—É–∞–ª—å–Ω—É—é –ø—Ä–∏–≤—è–∑–∫—É –≤ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã—Ö —Å–ª–æ—è—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å–∏–ª—å–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π."
                },
                "en": {
                    "title": "FRANK Model: Enhancing MLLMs with Reasoning Without Retraining",
                    "desc": "The FRANK Model enhances multimodal large language models (MLLMs) by integrating reasoning and reflection capabilities without the need for retraining. It employs a hierarchical weight merging technique that combines visual-pretrained models with reasoning-specialized models, allowing for effective reasoning in MLLMs. The model strategically decouples perception and reasoning across different layers of the decoder, leveraging shallow layers for visual attention and deeper layers for textual semantics. Experimental results show that FRANK-38B significantly outperforms existing models on multimodal reasoning tasks, achieving a notable accuracy increase on the MMMU benchmark."
                },
                "zh": {
                    "title": "FRANKÊ®°ÂûãÔºöÊó†ÈúÄÈáçËÆ≠ÁöÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÂ¢ûÂº∫",
                    "desc": "FRANKÊ®°ÂûãÈÄöËøáÂàÜÂ±ÇÊùÉÈáçÂêàÂπ∂ÁöÑÊñπÊ≥ïÔºåÂ¢ûÂº∫‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÁöÑÊé®ÁêÜÂíåÂèçÊÄùËÉΩÂäõÔºåËÄåÊó†ÈúÄÈáçÊñ∞ËÆ≠ÁªÉ„ÄÇËØ•Ê®°ÂûãÂ∞ÜËßÜËßâÈ¢ÑËÆ≠ÁªÉÁöÑMLLM‰∏é‰∏ìÊ≥®‰∫éÊé®ÁêÜÁöÑLLMÁªìÂêàÔºåÈÅøÂÖç‰∫ÜÈ´òÊòÇÁöÑÈáçÊñ∞ËÆ≠ÁªÉÊàêÊú¨„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÊµÖÂ±ÇËß£Á†ÅÂô®Â±ÇÂØπËßÜËßâ‰ø°ÊÅØÁöÑÂÖ≥Ê≥®Â∫¶Êõ¥È´òÔºåËÄåÊ∑±Â±ÇËß£Á†ÅÂô®Â±ÇÂàôÊõ¥Ê≥®ÈáçÊñáÊú¨ËØ≠‰πâÔºåËøô‰∏ÄËßÇÂØü‰øÉÊàê‰∫ÜÂàÜÂ±ÇÊùÉÈáçÂêàÂπ∂ÁöÑÊñπÊ≥ï„ÄÇÈÄöËøáÂú®Ê∑±Â±ÇËß£Á†ÅÂô®‰∏≠Êï¥ÂêàÊé®ÁêÜËÉΩÂäõÔºåÂêåÊó∂‰øùÊåÅÊµÖÂ±ÇËß£Á†ÅÂô®ÁöÑËßÜËßâÂü∫Á°ÄÔºåFRANKÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅÊé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂáÜÁ°ÆÁéáË∂ÖËøá‰∫ÜÂ§ö‰∏™Âº∫Âü∫Á∫øÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15879",
            "title": "GRIT: Teaching MLLMs to Think with Images",
            "url": "https://huggingface.co/papers/2505.15879",
            "abstract": "Recent studies have demonstrated the efficacy of using Reinforcement Learning (RL) in building reasoning models that articulate chains of thoughts prior to producing final answers. However, despite ongoing advances that aim at enabling reasoning for vision-language tasks, existing open-source visual reasoning models typically generate reasoning content with pure natural language, lacking explicit integration of visual information. This limits their ability to produce clearly articulated and visually grounded reasoning chains. To this end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method for training MLLMs to think with images. GRIT introduces a grounded reasoning paradigm, in which models generate reasoning chains that interleave natural language and explicit bounding box coordinates. These coordinates point to regions of the input image that the model consults during its reasoning process. Additionally, GRIT is equipped with a reinforcement learning approach, GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused on the final answer accuracy and format of the grounded reasoning output, which eliminates the need for data with reasoning chain annotations or explicit bounding box labels. As a result, GRIT achieves exceptional data efficiency, requiring as few as 20 image-question-answer triplets from existing datasets. Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to produce coherent and visually grounded reasoning chains, showing a successful unification of reasoning and grounding abilities.",
            "score": 7,
            "issue_id": 3915,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "05b66f8da1752a05",
            "authors": [
                "Yue Fan",
                "Xuehai He",
                "Diji Yang",
                "Kaizhi Zheng",
                "Ching-Chen Kuo",
                "Yuting Zheng",
                "Sravana Jyothi Narayanaraju",
                "Xinze Guan",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "UC Santa Cruz",
                "eBay"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15879.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#rl",
                    "#open_source",
                    "#training"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "GRIT: –í–∏–∑—É–∞–ª—å–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º GRIT. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Å–æ—á–µ—Ç–∞—é—â–∏–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫ –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏—Ö —Ä–∞–º–æ–∫ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. GRIT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–¥—Ö–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º GRPO-GR, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–µ GRPO, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏–ª–∏ —è–≤–Ω—ã—Ö –º–µ—Ç–æ–∫ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏—Ö —Ä–∞–º–æ–∫. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ GRIT —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—É—á–∞–µ—Ç MLLM –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —É—Å–ø–µ—à–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–µ."
                },
                "en": {
                    "title": "Grounded Reasoning: Merging Vision and Language for Better Understanding",
                    "desc": "This paper introduces Grounded Reasoning with Images and Texts (GRIT), a new method that enhances visual reasoning in machine learning models. GRIT allows models to generate reasoning chains that combine natural language with specific visual information, represented by bounding box coordinates. By using a reinforcement learning approach called GRPO-GR, GRIT focuses on improving the accuracy of final answers without needing extensive annotated data. The results show that GRIT can efficiently train models to produce clear and visually supported reasoning, bridging the gap between reasoning and visual understanding."
                },
                "zh": {
                    "title": "ÂõæÂÉè‰∏éÊñáÊú¨ÁöÑÂü∫Á°ÄÊé®ÁêÜÔºöËÆ©Ê®°ÂûãÊõ¥Â•ΩÂú∞ÊÄùËÄÉ",
                    "desc": "ÊúÄËøëÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂú®ÊûÑÂª∫Êé®ÁêÜÊ®°ÂûãÊñπÈù¢ÈùûÂ∏∏ÊúâÊïàÔºåËøô‰∫õÊ®°ÂûãÂú®ÁªôÂá∫ÊúÄÁªàÁ≠îÊ°à‰πãÂâç‰ºöË°®ËææÊÄùÁª¥Èìæ„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂºÄÊ∫êËßÜËßâÊé®ÁêÜÊ®°ÂûãÈÄöÂ∏∏Âè™Áî®Ëá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêÊé®ÁêÜÂÜÖÂÆπÔºåÁº∫‰πè‰∏éËßÜËßâ‰ø°ÊÅØÁöÑÊòéÁ°ÆÁªìÂêàÔºåËøôÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÁîüÊàêÊ∏ÖÊô∞‰∏î‰∏éËßÜËßâÁõ∏ÂÖ≥ÁöÑÊé®ÁêÜÈìæÁöÑËÉΩÂäõ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ï‚Äî‚ÄîÂõæÂÉè‰∏éÊñáÊú¨ÁöÑÂü∫Á°ÄÊé®ÁêÜÔºàGRITÔºâÔºåËØ•ÊñπÊ≥ïËÆ≠ÁªÉÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâ‰∏éÂõæÂÉèÂÖ±ÂêåÊÄùËÄÉ„ÄÇGRITÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂü∫Á°ÄÊé®ÁêÜËåÉÂºèÔºåÊ®°ÂûãÁîüÊàêÁöÑÊé®ÁêÜÈìæ‰∫§ÊõøÂåÖÂê´Ëá™ÁÑ∂ËØ≠Ë®ÄÂíåÊòéÁ°ÆÁöÑËæπÁïåÊ°ÜÂùêÊ†áÔºåËøô‰∫õÂùêÊ†áÊåáÂêëËæìÂÖ•ÂõæÂÉè‰∏≠Ê®°ÂûãÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÂèÇËÄÉÁöÑÂå∫Âüü„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16944",
            "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in\n  Agentic Scenarios",
            "url": "https://huggingface.co/papers/2505.16944",
            "abstract": "A new benchmark, AgentIF, evaluates Large Language Models' ability to follow complex instructions in realistic agentic scenarios, revealing performance limitations in handling constraints and tool specifications.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated advanced capabilities in real-world agentic applications. Growing research efforts aim to develop LLM-based agents to address practical demands, introducing a new challenge: agentic scenarios often involve lengthy instructions with complex constraints, such as extended system prompts and detailed tool specifications. While adherence to such instructions is crucial for agentic applications, whether LLMs can reliably follow them remains underexplored. In this paper, we introduce AgentIF, the first benchmark for systematically evaluating LLM instruction following ability in agentic scenarios. AgentIF features three key characteristics: (1) Realistic, constructed from 50 real-world agentic applications. (2) Long, averaging 1,723 words with a maximum of 15,630 words. (3) Complex, averaging 11.9 constraints per instruction, covering diverse constraint types, such as tool specifications and condition constraints. To construct AgentIF, we collect 707 human-annotated instructions across 50 agentic tasks from industrial application agents and open-source agentic systems. For each instruction, we annotate the associated constraints and corresponding evaluation metrics, including code-based evaluation, LLM-based evaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically evaluate existing advanced LLMs. We observe that current models generally perform poorly, especially in handling complex constraint structures and tool specifications. We further conduct error analysis and analytical experiments on instruction length and meta constraints, providing some findings about the failure modes of existing LLMs. We have released the code and data to facilitate future research.",
            "score": 6,
            "issue_id": 3920,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "1699a99ec316293a",
            "authors": [
                "Yunjia Qi",
                "Hao Peng",
                "Xiaozhi Wang",
                "Amy Xin",
                "Youfeng Liu",
                "Bin Xu",
                "Lei Hou",
                "Juanzi Li"
            ],
            "affiliations": [
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16944.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#agents",
                    "#long_context",
                    "#agi"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "AgentIF: –Ω–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–æ–ª–∏ –∞–≥–µ–Ω—Ç–æ–≤",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ AgentIF –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å–ª–µ–¥–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –∞–≥–µ–Ω—Ç–æ–≤. AgentIF –≤–∫–ª—é—á–∞–µ—Ç 707 –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏–∑ 50 –∞–≥–µ–Ω—Ç–Ω—ã—Ö –∑–∞–¥–∞—á, —Å–æ —Å—Ä–µ–¥–Ω–µ–π –¥–ª–∏–Ω–æ–π 1723 —Å–ª–æ–≤–∞ –∏ 11.9 –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é. –û—Ü–µ–Ω–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö LLM —Å –ø–æ–º–æ—â—å—é AgentIF –ø–æ–∫–∞–∑–∞–ª–∞ –∏—Ö –Ω–∏–∑–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –∏ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, –≤—ã—è–≤–∏–≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∂–∏–º—ã –æ—Ç–∫–∞–∑–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö LLM."
                },
                "en": {
                    "title": "AgentIF: Benchmarking LLMs in Complex Instruction Following",
                    "desc": "This paper introduces AgentIF, a new benchmark designed to assess how well Large Language Models (LLMs) can follow complex instructions in realistic scenarios where they act as agents. The benchmark is based on 50 real-world applications and includes long instructions with an average of 1,723 words and multiple constraints, such as tool specifications. The study reveals that current LLMs struggle significantly with these complex instructions, particularly in managing constraints and tool requirements. By providing detailed evaluations and error analyses, the authors aim to highlight the limitations of existing models and encourage further research in this area."
                },
                "zh": {
                    "title": "ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊåá‰ª§ÈÅµÂæ™ËÉΩÂäõ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊµãËØïAgentIFÔºåÁî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§çÊùÇÊåá‰ª§‰∏ãÁöÑË°®Áé∞ÔºåÁâπÂà´ÊòØÂú®ÁúüÂÆûÁöÑ‰ª£ÁêÜÂú∫ÊôØ‰∏≠„ÄÇAgentIFÁöÑÁâπÁÇπÂåÖÊã¨ÔºöÂü∫‰∫é50‰∏™ÁúüÂÆû‰∏ñÁïåÁöÑ‰ª£ÁêÜÂ∫îÁî®ÊûÑÂª∫ÔºåÊåá‰ª§Âπ≥ÂùáÈïøÂ∫¶‰∏∫1723‰∏™ÂçïËØçÔºå‰∏îÊØèÊù°Êåá‰ª§Âπ≥ÂùáÂåÖÂê´11.9‰∏™Á∫¶ÊùüÊù°‰ª∂„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁé∞ÊúâÁöÑLLMsÂú®Â§ÑÁêÜÂ§çÊùÇÁ∫¶ÊùüÂíåÂ∑•ÂÖ∑ËßÑËåÉÊó∂Ë°®Áé∞‰∏ç‰Ω≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈÅµÂæ™ÈïøÊåá‰ª§ÊñπÈù¢„ÄÇÈÄöËøáÈîôËØØÂàÜÊûêÂíåÂÆûÈ™åÔºåÊú¨ÊñáÊè≠Á§∫‰∫ÜÁé∞ÊúâÊ®°ÂûãÁöÑÂ±ÄÈôêÊÄßÔºåÂπ∂Êèê‰æõ‰∫Ü‰ª£Á†ÅÂíåÊï∞ÊçÆ‰ª•ÊîØÊåÅÊú™Êù•ÁöÑÁ†îÁ©∂„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16192",
            "title": "VLM-R^3: Region Recognition, Reasoning, and Refinement for Enhanced\n  Multimodal Chain-of-Thought",
            "url": "https://huggingface.co/papers/2505.16192",
            "abstract": "Recently, reasoning-based MLLMs have achieved a degree of success in generating long-form textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on and revisiting of visual regions to achieve precise grounding of textual reasoning in visual evidence. We introduce VLM-R^3 (Visual Language Model with Region Recognition and Reasoning), a framework that equips an MLLM with the ability to (i) decide when additional visual evidence is needed, (ii) determine where to ground within the image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved chain-of-thought. The core of our method is Region-Conditioned Reinforcement Policy Optimization (R-GRPO), a training paradigm that rewards the model for selecting informative regions, formulating appropriate transformations (e.g.\\ crop, zoom), and integrating the resulting visual context into subsequent reasoning steps. To bootstrap this policy, we compile a modest but carefully curated Visuo-Lingual Interleaved Rationale (VLIR) corpus that provides step-level supervision on region selection and textual justification. Extensive experiments on MathVista, ScienceQA, and other benchmarks show that VLM-R^3 sets a new state of the art in zero-shot and few-shot settings, with the largest gains appearing on questions demanding subtle spatial reasoning or fine-grained visual cue extraction.",
            "score": 6,
            "issue_id": 3920,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "ffe915867d50a220",
            "authors": [
                "Chaoya Jiang",
                "Yongrui Heng",
                "Wei Ye",
                "Han Yang",
                "Haiyang Xu",
                "Ming Yan",
                "Ji Zhang",
                "Fei Huang",
                "Shikun Zhang"
            ],
            "affiliations": [
                "Alibaba Group",
                "National Engineering Research Center for Software Engineering, Peking University",
                "ZEEKR Intelligent Technology Holding Limited"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16192.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#cv",
                    "#dataset",
                    "#multimodal",
                    "#benchmark",
                    "#rl",
                    "#training"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–£–º–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –ò–ò: —Ç–æ—á–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VLM-R^3 - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ö–ª—é—á–µ–≤–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç - –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è R-GRPO, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç –º–æ–¥–µ–ª—å –≤—ã–±–∏—Ä–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Ä–µ–≥–∏–æ–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –≤ –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –î–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç VLIR —Å –ø–æ—à–∞–≥–æ–≤–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π –≤—ã–±–æ—Ä–∞ —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ VLM-R^3 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–æ–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä—è–¥–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ç–æ–Ω–∫–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning in Language Models with VLM-R^3",
                    "desc": "This paper presents VLM-R^3, a new framework that enhances reasoning in multi-modal language models (MLLMs) by integrating visual evidence more effectively. It allows the model to identify when it needs more visual information, where to focus within an image, and how to incorporate this visual context into its reasoning process. The training method, Region-Conditioned Reinforcement Policy Optimization (R-GRPO), incentivizes the model to select informative image regions and apply transformations like cropping or zooming. The results demonstrate that VLM-R^3 outperforms previous models on tasks requiring detailed spatial reasoning and visual cue extraction, achieving state-of-the-art performance in various benchmarks."
                },
                "zh": {
                    "title": "ËßÜËßâ‰∏éËØ≠Ë®ÄÁöÑÊ∑±Â∫¶ËûçÂêà",
                    "desc": "ÊúÄËøëÔºåÂü∫‰∫éÊé®ÁêÜÁöÑÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÁîüÊàêÈïøÊñáÊú¨Êé®ÁêÜÈìæÊñπÈù¢ÂèñÂæó‰∫Ü‰∏ÄÂÆöÊàêÂäü„ÄÇÁÑ∂ËÄåÔºåÂÆÉ‰ª¨Âú®Â§ÑÁêÜÂ§çÊùÇ‰ªªÂä°Êó∂‰ªçÁÑ∂Èù¢‰∏¥ÊåëÊàòÔºåËøô‰∫õ‰ªªÂä°ÈúÄË¶ÅÂä®ÊÄÅÂíåËø≠‰ª£Âú∞ÂÖ≥Ê≥®ÂíåÈáçÊñ∞ÂÆ°ËßÜËßÜËßâÂå∫ÂüüÔºå‰ª•ÂÆûÁé∞ÊñáÊú¨Êé®ÁêÜ‰∏éËßÜËßâËØÅÊçÆÁöÑÁ≤æÁ°ÆÁªìÂêà„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜVLM-R^3ÔºàÂ∏¶ÊúâÂå∫ÂüüËØÜÂà´ÂíåÊé®ÁêÜÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºâÔºåËØ•Ê°ÜÊû∂‰ΩøMLLMÂÖ∑Â§á‰∫ÜÂÜ≥ÂÆö‰ΩïÊó∂ÈúÄË¶ÅÈ¢ùÂ§ñËßÜËßâËØÅÊçÆ„ÄÅÁ°ÆÂÆöÂõæÂÉè‰∏≠Â∫îËÅöÁÑ¶ÁöÑ‰ΩçÁΩÆ‰ª•ÂèäÂ∞ÜÁõ∏ÂÖ≥Â≠êÂõæÂÉèÂÜÖÂÆπÊó†ÁºùËûçÂÖ•‰∫§ÈîôÊÄùÁª¥ÈìæÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÁöÑÊ†∏ÂøÉÊñπÊ≥ïÊòØÂå∫ÂüüÊù°‰ª∂Âº∫ÂåñÁ≠ñÁï•‰ºòÂåñÔºàR-GRPOÔºâÔºåÈÄöËøáÂ•ñÂä±Ê®°ÂûãÈÄâÊã©‰ø°ÊÅØ‰∏∞ÂØåÁöÑÂå∫Âüü„ÄÅÂà∂ÂÆöÈÄÇÂΩìÁöÑÂèòÊç¢ÔºàÂ¶ÇË£ÅÂâ™„ÄÅÁº©ÊîæÔºâÂπ∂Â∞ÜÁªìÊûúËßÜËßâ‰∏ä‰∏ãÊñáÊï¥ÂêàÂà∞ÂêéÁª≠Êé®ÁêÜÊ≠•È™§‰∏≠ÔºåÊù•ËÆ≠ÁªÉËØ•Á≠ñÁï•„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15963",
            "title": "OViP: Online Vision-Language Preference Learning",
            "url": "https://huggingface.co/papers/2505.15963",
            "abstract": "Large vision-language models (LVLMs) remain vulnerable to hallucination, often generating content misaligned with visual inputs. While recent approaches advance multi-modal Direct Preference Optimization (DPO) to mitigate hallucination, they typically rely on predefined or randomly edited negative samples that fail to reflect actual model errors, limiting training efficacy. In this work, we propose an Online Vision-language Preference Learning (OViP) framework that dynamically constructs contrastive training data based on the model's own hallucinated outputs. By identifying semantic differences between sampled response pairs and synthesizing negative images using a diffusion model, OViP generates more relevant supervision signals in real time. This failure-driven training enables adaptive alignment of both textual and visual preferences. Moreover, we refine existing evaluation protocols to better capture the trade-off between hallucination suppression and expressiveness. Experiments on hallucination and general benchmarks demonstrate that OViP effectively reduces hallucinations while preserving core multi-modal capabilities.",
            "score": 6,
            "issue_id": 3914,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "4476380071b2e936",
            "authors": [
                "Shujun Liu",
                "Siyuan Wang",
                "Zejun Li",
                "Jianxiang Wang",
                "Cheng Zeng",
                "Zhongyu Wei"
            ],
            "affiliations": [
                "ByteDance",
                "Fudan University",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15963.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#hallucinations",
                    "#benchmark",
                    "#diffusion",
                    "#rag",
                    "#alignment"
                ],
                "emoji": "üîÆ",
                "ru": {
                    "title": "OViP: –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π OViP (Online Vision-language Preference Learning). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ —É–º–µ–Ω—å—à–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LVLM) –ø—É—Ç–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ—á–Ω—ã—Ö –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏. OViP –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –æ–±—É—á–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ OViP —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –∫–ª—é—á–µ–≤—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Dynamic Learning to Combat Hallucination in Vision-Language Models",
                    "desc": "This paper addresses the issue of hallucination in large vision-language models (LVLMs), where the models generate content that does not match the visual inputs. The authors introduce a new framework called Online Vision-language Preference Learning (OViP), which creates training data based on the model's own incorrect outputs, rather than relying on static negative samples. By using a diffusion model to synthesize negative images, OViP provides more relevant feedback for the model to learn from. The results show that this approach not only reduces hallucinations but also maintains the model's ability to express multi-modal information effectively."
                },
                "zh": {
                    "title": "Âä®ÊÄÅÊûÑÂª∫ÂØπÊØîÊï∞ÊçÆÔºåÂáèÂ∞ëÂπªËßâÔºÅ",
                    "desc": "Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÂú®ÁîüÊàêÂÜÖÂÆπÊó∂ÂÆπÊòìÂá∫Áé∞ÂπªËßâÔºåÂ∏∏Â∏∏‰∏éËßÜËßâËæìÂÖ•‰∏ç‰∏ÄËá¥„ÄÇËôΩÁÑ∂ÊúÄËøëÁöÑÁ†îÁ©∂ÈÄöËøáÂ§öÊ®°ÊÄÅÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÔºàDPOÔºâÊù•ÂáèËΩªÂπªËßâÈóÆÈ¢òÔºå‰ΩÜÈÄöÂ∏∏‰æùËµñ‰∫éÈ¢ÑÂÆö‰πâÊàñÈöèÊú∫ÁºñËæëÁöÑË¥üÊ†∑Êú¨ÔºåËøô‰∫õÊ†∑Êú¨Êó†Ê≥ïÁúüÂÆûÂèçÊò†Ê®°ÂûãÁöÑÈîôËØØÔºåÈôêÂà∂‰∫ÜËÆ≠ÁªÉÊïàÊûú„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂú®Á∫øËßÜËßâËØ≠Ë®ÄÂÅèÂ•ΩÂ≠¶‰π†ÔºàOViPÔºâÊ°ÜÊû∂ÔºåÂä®ÊÄÅÊûÑÂª∫ÂØπÊØîËÆ≠ÁªÉÊï∞ÊçÆÔºåÂü∫‰∫éÊ®°ÂûãËá™Ë∫´ÁöÑÂπªËßâËæìÂá∫„ÄÇÈÄöËøáËØÜÂà´ÂìçÂ∫îÂØπ‰πãÈó¥ÁöÑËØ≠‰πâÂ∑ÆÂºÇÂπ∂‰ΩøÁî®Êâ©Êï£Ê®°ÂûãÂêàÊàêË¥üÂõæÂÉèÔºåOViPÂÆûÊó∂ÁîüÊàêÊõ¥Áõ∏ÂÖ≥ÁöÑÁõëÁù£‰ø°Âè∑ÔºåÊúâÊïàÂáèÂ∞ëÂπªËßâÔºåÂêåÊó∂‰øùÊåÅÂ§öÊ®°ÊÄÅËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15960",
            "title": "Training Step-Level Reasoning Verifiers with Formal Verification Tools",
            "url": "https://huggingface.co/papers/2505.15960",
            "abstract": "FoVer is a method for automatically annotating step-level error labels using formal verification tools to train Process Reward Models, which significantly improves cross-task generalization and outperforms human-annotated methods in various reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs), which provide step-by-step feedback on the reasoning generated by Large Language Models (LLMs), are receiving increasing attention. However, two key research gaps remain: collecting accurate step-level error labels for training typically requires costly human annotation, and existing PRMs are limited to math reasoning problems. In response to these gaps, this paper aims to address the challenges of automatic dataset creation and the generalization of PRMs to diverse reasoning tasks. To achieve this goal, we propose FoVer, an approach for training PRMs on step-level error labels automatically annotated by formal verification tools, such as Z3 for formal logic and Isabelle for theorem proof, which provide automatic and accurate verification for symbolic tasks. Using this approach, we synthesize a training dataset with error labels on LLM responses for formal logic and theorem proof tasks without human annotation. Although this data synthesis is feasible only for tasks compatible with formal verification, we observe that LLM-based PRMs trained on our dataset exhibit cross-task generalization, improving verification across diverse reasoning tasks. Specifically, PRMs trained with FoVer significantly outperform baseline PRMs based on the original LLMs and achieve competitive or superior results compared to state-of-the-art PRMs trained on labels annotated by humans or stronger models, as measured by step-level verification on ProcessBench and Best-of-K performance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU, and BBH. The datasets, models, and code are provided at https://github.com/psunlpgroup/FoVer.",
            "score": 6,
            "issue_id": 3926,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "fd1009a733cb10f6",
            "authors": [
                "Ryo Kamoi",
                "Yusen Zhang",
                "Nan Zhang",
                "Sarkar Snigdha Sarathi Das",
                "Rui Zhang"
            ],
            "affiliations": [
                "Penn State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15960.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#data",
                    "#optimization",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò",
                    "desc": "FoVer - —ç—Ç–æ –º–µ—Ç–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –æ—à–∏–±–æ–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (Process Reward Models). –î–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–µ–Ω–∏–µ –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–µ—Ç–æ–¥—ã —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –æ—Ç –ª—é–¥–µ–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ. FoVer —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –æ–±—É—á–∞—é—â–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –º–µ—Ç–∫–∞–º–∏ –æ—à–∏–±–æ–∫ –¥–ª—è –∑–∞–¥–∞—á —Ñ–æ—Ä–º–∞–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–∏ –∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —Ç–µ–æ—Ä–µ–º –±–µ–∑ —É—á–∞—Å—Ç–∏—è —á–µ–ª–æ–≤–µ–∫–∞. –ú–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é FoVer, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –¥–æ—Å—Ç–∏–≥–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã—Ö –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ–±—É—á–µ–Ω–Ω—ã–º–∏ –Ω–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö —á–µ–ª–æ–≤–µ–∫–∞."
                },
                "en": {
                    "title": "Automating Error Annotation for Enhanced Reasoning Models",
                    "desc": "FoVer is a novel method that automates the annotation of step-level error labels using formal verification tools, which helps in training Process Reward Models (PRMs). This approach addresses the challenges of costly human annotation and expands the applicability of PRMs beyond just math reasoning tasks. By synthesizing a training dataset with accurate error labels for formal logic and theorem proof tasks, FoVer enables PRMs to generalize across various reasoning tasks effectively. The results show that PRMs trained with FoVer outperform traditional methods and achieve competitive performance on multiple reasoning benchmarks."
                },
                "zh": {
                    "title": "FoVerÔºöËá™Âä®Ê†áÊ≥®ÊèêÂçáÊé®ÁêÜÊ®°ÂûãÁöÑË∑®‰ªªÂä°ËÉΩÂäõ",
                    "desc": "FoVerÊòØ‰∏ÄÁßçÂà©Áî®ÂΩ¢ÂºèÈ™åËØÅÂ∑•ÂÖ∑Ëá™Âä®Ê†áÊ≥®Ê≠•È™§Á∫ßÈîôËØØÊ†áÁ≠æÁöÑÊñπÊ≥ïÔºåÁî®‰∫éËÆ≠ÁªÉËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàPRMsÔºâ„ÄÇËØ•ÊñπÊ≥ïÊòæËëóÊèêÈ´ò‰∫ÜË∑®‰ªªÂä°ÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ∂Âú®Â§ö‰∏™Êé®ÁêÜÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫Ü‰∫∫Â∑•Ê†áÊ≥®ÁöÑÊñπÊ≥ï„ÄÇFoVerÈÄöËøáËá™Âä®ÁîüÊàêÁöÑÈîôËØØÊ†áÁ≠æÔºåËß£ÂÜ≥‰∫ÜËÆ≠ÁªÉ‰∏≠ÈúÄË¶ÅÊòÇË¥µ‰∫∫Â∑•Ê†áÊ≥®ÁöÑÈóÆÈ¢òÔºåÂπ∂Êâ©Â±ï‰∫ÜPRMsÂú®Â§öÁßçÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®FoVerËÆ≠ÁªÉÁöÑPRMsÂú®Ê≠•È™§Á∫ßÈ™åËØÅ‰∏äË°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÂü∫‰∫éÂéüÂßãÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂü∫Á∫øPRMs„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16186",
            "title": "SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning",
            "url": "https://huggingface.co/papers/2505.16186",
            "abstract": "SafeKey enhances the safety of large reasoning models by focusing on activating a safety aha moment in the key sentence through dual-path safety head and query-mask modeling, thereby improving generalization to harmful prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) introduce a new generation paradigm of explicitly reasoning before answering, leading to remarkable improvements in complex tasks. However, they pose great safety risks against harmful queries and adversarial attacks. While recent mainstream safety efforts on LRMs, supervised fine-tuning (SFT), improve safety performance, we find that SFT-aligned models struggle to generalize to unseen jailbreak prompts. After thorough investigation of LRMs' generation, we identify a safety aha moment that can activate safety reasoning and lead to a safe response. This aha moment typically appears in the `key sentence', which follows models' query understanding process and can indicate whether the model will proceed safely. Based on these insights, we propose SafeKey, including two complementary objectives to better activate the safety aha moment in the key sentence: (1) a Dual-Path Safety Head to enhance the safety signal in the model's internal representations before the key sentence, and (2) a Query-Mask Modeling objective to improve the models' attention on its query understanding, which has important safety hints. Experiments across multiple safety benchmarks demonstrate that our methods significantly improve safety generalization to a wide range of jailbreak attacks and out-of-distribution harmful prompts, lowering the average harmfulness rate by 9.6\\%, while maintaining general abilities. Our analysis reveals how SafeKey enhances safety by reshaping internal attention and improving the quality of hidden representations.",
            "score": 5,
            "issue_id": 3918,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "a47a9eb4edda38b3",
            "authors": [
                "Kaiwen Zhou",
                "Xuandong Zhao",
                "Gaowen Liu",
                "Jayanth Srinivasa",
                "Aosong Feng",
                "Dawn Song",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "Cisco Research",
                "UC Berkeley",
                "UC Santa Cruz",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16186.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#security",
                    "#training",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "üõ°Ô∏è",
                "ru": {
                    "title": "SafeKey: –ê–∫—Ç–∏–≤–∞—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ –∫–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ò–ò",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ SafeKey –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM). –ú–µ—Ç–æ–¥ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ '–º–æ–º–µ–Ω—Ç–∞ –æ–∑–∞—Ä–µ–Ω–∏—è' –≤ –∫–ª—é—á–µ–≤–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ —Å –ø–æ–º–æ—â—å—é –¥–≤—É—Ö–ø—É—Ç–µ–≤–æ–π –≥–æ–ª–æ–≤–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–∞—Å–∫–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞. SafeKey —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –∞—Ç–∞–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ–±—â–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏."
                },
                "en": {
                    "title": "Activating Safety Moments in Large Reasoning Models",
                    "desc": "SafeKey is a method designed to improve the safety of large reasoning models (LRMs) by focusing on a critical moment in the model's reasoning process, known as the safety aha moment. This moment occurs in the key sentence, which helps the model determine whether it can respond safely to a query. SafeKey employs a Dual-Path Safety Head to strengthen the safety signals in the model's internal representations and a Query-Mask Modeling approach to enhance the model's attention to safety-related aspects of the query. Through experiments, SafeKey has shown to significantly reduce the risk of harmful responses while preserving the model's overall performance."
                },
                "zh": {
                    "title": "SafeKeyÔºöÊøÄÊ¥ªÂÆâÂÖ®Êó∂ÂàªÔºåÊèêÂçáÊé®ÁêÜÊ®°ÂûãÂÆâÂÖ®ÊÄß",
                    "desc": "SafeKeyÈÄöËøáÊøÄÊ¥ªÂÖ≥ÈîÆÂè•‰∏≠ÁöÑÂÆâÂÖ®Êó∂ÂàªÊù•Â¢ûÂº∫Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄß„ÄÇÂÆÉÈááÁî®ÂèåË∑ØÂæÑÂÆâÂÖ®Â§¥ÂíåÊü•ËØ¢Êé©Á†ÅÂª∫Ê®°ÔºåÊó®Âú®ÊèêÈ´òÊ®°ÂûãÂØπÊúâÂÆ≥ÊèêÁ§∫ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂÖ≥ÈîÆÂè•‰∏≠ÁöÑÂÆâÂÖ®Êó∂ÂàªËÉΩÂ§üÂºïÂØºÊ®°ÂûãÂÅöÂá∫ÂÆâÂÖ®ÁöÑÂìçÂ∫î„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSafeKeyÊòæËëóÈôç‰Ωé‰∫ÜÊ®°ÂûãÂú®ÂêÑÁßçÊîªÂáª‰∏ãÁöÑÊúâÂÆ≥ÊÄßÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÂÖ∂‰∏ÄËà¨ËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11711",
            "title": "Reinforcement Learning Finetunes Small Subnetworks in Large Language\n  Models",
            "url": "https://huggingface.co/papers/2505.11711",
            "abstract": "Reinforcement learning improves large language models with minimal parameter updates, affecting only a small subnetwork without explicit sparsity techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) yields substantial improvements in large language models (LLMs) downstream task performance and alignment with human values. Surprisingly, such large gains result from updating only a small subnetwork comprising just 5 percent to 30 percent of the parameters, with the rest effectively unchanged. We refer to this phenomenon as parameter update sparsity induced by RL. It is observed across all 7 widely used RL algorithms (e.g., PPO, GRPO, DPO) and all 10 LLMs from different families in our experiments. This sparsity is intrinsic and occurs without any explicit sparsity promoting regularizations or architectural constraints. Finetuning the subnetwork alone recovers the test accuracy, and, remarkably, produces a model nearly identical to the one obtained via full finetuning. The subnetworks from different random seeds, training data, and even RL algorithms show substantially greater overlap than expected by chance. Our analysis suggests that this sparsity is not due to updating only a subset of layers, instead, nearly all parameter matrices receive similarly sparse updates. Moreover, the updates to almost all parameter matrices are nearly full-rank, suggesting RL updates a small subset of parameters that nevertheless span almost the full subspaces that the parameter matrices can represent. We conjecture that the this update sparsity can be primarily attributed to training on data that is near the policy distribution, techniques that encourage the policy to remain close to the pretrained model, such as the KL regularization and gradient clipping, have limited impact.",
            "score": 5,
            "issue_id": 3914,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 –º–∞—è",
                "en": "May 16",
                "zh": "5Êúà16Êó•"
            },
            "hash": "e7296e89ef67015f",
            "authors": [
                "Sagnik Mukherjee",
                "Lifan Yuan",
                "Dilek Hakkani-Tur",
                "Hao Peng"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11711.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#rl",
                    "#optimization",
                    "#alignment"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç",
                    "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—é—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á –∏ –∏—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Ü–µ–Ω–Ω–æ—Å—Ç—è–º. –£–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ, –Ω–æ —Ç–∞–∫–∏–µ —É–ª—É—á—à–µ–Ω–∏—è –¥–æ—Å—Ç–∏–≥–∞—é—Ç—Å—è –ø—É—Ç–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –Ω–µ–±–æ–ª—å—à–æ–π –ø–æ–¥—Å–µ—Ç–∏, —Å–æ—Å—Ç–∞–≤–ª—è—é—â–µ–π 5-30% –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏. –≠—Ç–æ—Ç —Ñ–µ–Ω–æ–º–µ–Ω, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π '—Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å—é –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤', –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ RL –∏ —Å–µ–º–µ–π—Å—Ç–≤ LLM –±–µ–∑ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —è–≤–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∑–∞—Ç—Ä–∞–≥–∏–≤–∞—é—Ç –ø–æ—á—Ç–∏ –≤—Å–µ –º–∞—Ç—Ä–∏—Ü—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–æ –æ—Å—Ç–∞—é—Ç—Å—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º–∏ –∏ –ø–æ–ª–Ω–æ—Ä–∞–Ω–≥–æ–≤—ã–º–∏."
                },
                "en": {
                    "title": "Efficient Reinforcement Learning: Small Updates, Big Gains!",
                    "desc": "This paper explores how reinforcement learning (RL) can enhance the performance of large language models (LLMs) by making minimal updates to a small subnetwork of parameters. Remarkably, only 5 to 30 percent of the model's parameters are adjusted, while the majority remain unchanged, a phenomenon termed 'parameter update sparsity.' This sparsity occurs across various RL algorithms and LLMs, indicating a consistent pattern in how RL influences model training. The findings suggest that even with limited updates, the subnetwork can achieve performance comparable to full finetuning, highlighting the efficiency of RL in optimizing LLMs."
                },
                "zh": {
                    "title": "Âº∫ÂåñÂ≠¶‰π†ÔºöÂ∞èÊõ¥Êñ∞ÔºåÂ§ßÊèêÂçá",
                    "desc": "Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏≠ÈÄöËøáÊúÄÂ∞èÁöÑÂèÇÊï∞Êõ¥Êñ∞ÊòæËëóÊèêÂçá‰∫Ü‰∏ãÊ∏∏‰ªªÂä°ÁöÑË°®Áé∞Âíå‰∏é‰∫∫Á±ª‰ª∑ÂÄºËßÇÁöÑÂØπÈΩê„ÄÇ‰ª§‰∫∫ÊÉäËÆ∂ÁöÑÊòØÔºåËøôÁßçÊòæËëóÁöÑÊèêÂçá‰ªÖÈÄöËøáÊõ¥Êñ∞Âç†ÂèÇÊï∞5%Âà∞30%ÁöÑÂ∞èÂ≠êÁΩëÁªúÂÆûÁé∞ÔºåÂÖ∂‰ΩôÂèÇÊï∞Âü∫Êú¨‰øùÊåÅ‰∏çÂèò„ÄÇÊàë‰ª¨Áß∞ËøôÁßçÁé∞Ë±°‰∏∫Áî±RLÂºïËµ∑ÁöÑÂèÇÊï∞Êõ¥Êñ∞Á®ÄÁñèÊÄß„ÄÇÂÆûÈ™åË°®ÊòéÔºåËøôÁßçÁ®ÄÁñèÊÄßÂú®‰∏ÉÁßçÂπøÊ≥õ‰ΩøÁî®ÁöÑRLÁÆóÊ≥ïÂíåÂçÅÁßç‰∏çÂêåÂÆ∂ÊóèÁöÑLLM‰∏≠ÊôÆÈÅçÂ≠òÂú®Ôºå‰∏î‰∏çÈúÄË¶Å‰ªª‰ΩïÊòæÂºèÁöÑÁ®ÄÁñè‰øÉËøõÊ≠£ÂàôÂåñÊàñÊû∂ÊûÑÁ∫¶Êùü„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16265",
            "title": "Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models",
            "url": "https://huggingface.co/papers/2505.16265",
            "abstract": "Think-RM is a framework that enhances generative reward models with long-horizon reasoning and a novel pairwise RLHF pipeline to improve end-policy performance in aligning large language models with human preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning from human feedback (RLHF) has become a powerful post-training paradigm for aligning large language models with human preferences. A core challenge in RLHF is constructing accurate reward signals, where the conventional Bradley-Terry reward models (BT RMs) often suffer from sensitivity to data size and coverage, as well as vulnerability to reward hacking. Generative reward models (GenRMs) offer a more robust alternative by generating chain-of-thought (CoT) rationales followed by a final reward. However, existing GenRMs rely on shallow, vertically scaled reasoning, limiting their capacity to handle nuanced or complex (e.g., reasoning-intensive) tasks. Moreover, their pairwise preference outputs are incompatible with standard RLHF algorithms that require pointwise reward signals. In this work, we introduce Think-RM, a training framework that enables long-horizon reasoning in GenRMs by modeling an internal thinking process. Rather than producing structured, externally provided rationales, Think-RM generates flexible, self-guided reasoning traces that support advanced capabilities such as self-reflection, hypothetical reasoning, and divergent reasoning. To elicit these reasoning abilities, we first warm-up the models by supervised fine-tuning (SFT) over long CoT data. We then further improve the model's long-horizon abilities by rule-based reinforcement learning (RL). In addition, we propose a novel pairwise RLHF pipeline that directly optimizes policies using pairwise preference rewards, eliminating the need for pointwise reward conversion and enabling more effective use of Think-RM outputs. Experiments show that Think-RM achieves state-of-the-art results on RM-Bench, outperforming both BT RM and vertically scaled GenRM by 8%. When combined with our pairwise RLHF pipeline, it demonstrates superior end-policy performance compared to traditional approaches.",
            "score": 4,
            "issue_id": 3929,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "3bfe544ceb0449d3",
            "authors": [
                "Ilgee Hong",
                "Changlong Yu",
                "Liang Qiu",
                "Weixiang Yan",
                "Zhenghao Xu",
                "Haoming Jiang",
                "Qingru Zhang",
                "Qin Lu",
                "Xin Liu",
                "Chao Zhang",
                "Tuo Zhao"
            ],
            "affiliations": [
                "Amazon",
                "Georgia Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16265.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#reasoning",
                    "#rlhf",
                    "#training",
                    "#optimization"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–î—É–º–∞–π –≥–ª—É–±–∂–µ: —É–ª—É—á—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è",
                    "desc": "Think-RM - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –Ω–æ–≤–æ–π –ø–æ–ø–∞—Ä–Ω–æ–π —Å—Ö–µ–º—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ (RLHF). –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≥–∏–±–∫–∏–µ, —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–µ —Ç–∞–∫–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏, –∫–∞–∫ —Å–∞–º–æ–∞–Ω–∞–ª–∏–∑ –∏ –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. Think-RM –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ RM-Bench, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥—Ä—É–≥–∏–µ –ø–æ–¥—Ö–æ–¥—ã –Ω–∞ 8%. –í —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å –Ω–æ–≤–æ–π –ø–æ–ø–∞—Ä–Ω–æ–π —Å—Ö–µ–º–æ–π RLHF –æ–Ω –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–æ–Ω–µ—á–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏."
                },
                "en": {
                    "title": "Empowering Generative Models with Long-Horizon Reasoning",
                    "desc": "Think-RM is a new framework designed to improve generative reward models (GenRMs) by enabling long-horizon reasoning, which helps align large language models with human preferences more effectively. It addresses the limitations of traditional Bradley-Terry reward models (BT RMs) that struggle with data sensitivity and reward hacking. By generating self-guided reasoning traces, Think-RM enhances the model's ability to perform complex reasoning tasks. Additionally, it introduces a novel pairwise reinforcement learning from human feedback (RLHF) pipeline that optimizes policies directly with pairwise preferences, leading to better overall performance."
                },
                "zh": {
                    "title": "Think-RMÔºöÊèêÂçáÁîüÊàêÂ•ñÂä±Ê®°ÂûãÁöÑÈïøÊó∂Èó¥Êé®ÁêÜËÉΩÂäõ",
                    "desc": "Think-RMÊòØ‰∏Ä‰∏™Â¢ûÂº∫ÁîüÊàêÂ•ñÂä±Ê®°ÂûãÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÈïøÊó∂Èó¥Êé®ÁêÜÂíåÊñ∞È¢ñÁöÑÊàêÂØπ‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†ÔºàRLHFÔºâÊµÅÁ®ãÊù•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏é‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂØπÈΩêÊÄßËÉΩ„ÄÇ‰º†ÁªüÁöÑÂ•ñÂä±Ê®°ÂûãÂú®Êï∞ÊçÆËßÑÊ®°ÂíåË¶ÜÁõñÂ∫¶‰∏äÊïèÊÑüÔºåÂÆπÊòìÂèóÂà∞Â•ñÂä±ÊìçÊéßÁöÑÂΩ±ÂìçÔºåËÄåÁîüÊàêÂ•ñÂä±Ê®°ÂûãÂàôÈÄöËøáÁîüÊàêÊÄùÁª¥ÈìæÔºàCoTÔºâÊé®ÁêÜÊù•Êèê‰æõÊõ¥Á®≥ÂÅ•ÁöÑÊõø‰ª£ÊñπÊ°à„ÄÇThink-RMÈÄöËøáÂª∫Ê®°ÂÜÖÈÉ®ÊÄùÁª¥ËøáÁ®ãÔºåÁîüÊàêÁÅµÊ¥ªÁöÑËá™ÊàëÂºïÂØºÊé®ÁêÜËΩ®ËøπÔºåÊîØÊåÅËá™ÊàëÂèçÊÄùÂíåÂÅáËÆæÊé®ÁêÜÁ≠âÈ´òÁ∫ßËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåThink-RMÂú®RM-Bench‰∏äÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåË∂ÖË∂ä‰∫Ü‰º†ÁªüÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17019",
            "title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication\n  Understanding and Reasoning Framework",
            "url": "https://huggingface.co/papers/2505.17019",
            "abstract": "LAD, a three-stage framework using GPT-4o-mini, achieves state-of-the-art performance in image implication understanding and reasoning tasks across different languages and question types.  \t\t\t\t\tAI-generated summary \t\t\t\t Metaphorical comprehension in images remains a critical challenge for AI systems, as existing models struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. While multimodal large language models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they struggle with a fundamental limitation on image implication tasks: contextual gaps that obscure the relationships between different visual elements and their abstract meanings. Inspired by the human cognitive process, we propose Let Androids Dream (LAD), a novel framework for image implication understanding and reasoning. LAD addresses contextual missing through the three-stage framework: (1) Perception: converting visual information into rich and multi-level textual representations, (2) Search: iteratively searching and integrating cross-domain knowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment image implication via explicit reasoning. Our framework with the lightweight GPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English image implication benchmark and a huge improvement on Chinese benchmark, performing comparable with the GPT-4o model on Multiple-Choice Question (MCQ) and outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work provides new insights into how AI can more effectively interpret image implications, advancing the field of vision-language reasoning and human-AI interaction. Our project is publicly available at https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.",
            "score": 3,
            "issue_id": 3922,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "9284570e4cba3821",
            "authors": [
                "Chenhao Zhang",
                "Yazhe Niu"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17019.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#science",
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "LAD: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –ø–æ–¥—Ç–µ–∫—Å—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ LAD –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –æ –ø–æ–¥—Ç–µ–∫—Å—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª—å GPT-4o-mini. LAD –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ç—Ä–µ—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –≤–∫–ª—é—á–∞—é—â–∏–π –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ, –ø–æ–∏—Å–∫ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–æ–¥—Ç–µ–∫—Å—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –∏ —Ç–∏–ø–∞—Ö –≤–æ–ø—Ä–æ—Å–æ–≤. LAD –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏."
                },
                "en": {
                    "title": "Unlocking Image Meanings with LAD Framework",
                    "desc": "The paper introduces Let Androids Dream (LAD), a three-stage framework designed to enhance image implication understanding and reasoning using the lightweight GPT-4o-mini model. It addresses the limitations of existing multimodal large language models (MLLMs) in grasping the nuanced meanings of images by implementing a structured approach: Perception, Search, and Reasoning. This framework converts visual data into detailed textual representations, integrates cross-domain knowledge to clarify ambiguities, and generates contextually aligned implications through reasoning. LAD demonstrates state-of-the-art performance on various benchmarks, significantly improving image implication tasks in both English and Chinese, and offers valuable insights for advancing vision-language reasoning."
                },
                "zh": {
                    "title": "ËÆ©ÂÆâÂçìÊ¢¶ËßÅÁîµÁæäÔºöÂõæÂÉèÈöêÂê´ÁêÜËß£ÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "LADÊòØ‰∏Ä‰∏™‰∏âÈò∂ÊÆµÊ°ÜÊû∂ÔºåÂà©Áî®GPT-4o-miniÊ®°ÂûãÔºåÂú®ÂõæÂÉèÈöêÂê´ÁêÜËß£ÂíåÊé®ÁêÜ‰ªªÂä°‰∏≠ÂèñÂæó‰∫ÜÊúÄÂÖàËøõÁöÑË°®Áé∞„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÊÑüÁü•„ÄÅÊêúÁ¥¢ÂíåÊé®ÁêÜ‰∏â‰∏™Ê≠•È™§ÔºåËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊ®°ÂûãÂú®ÁêÜËß£ÂõæÂÉèÈöêÂê´ÊÑè‰πâÊó∂ÁöÑ‰∏ä‰∏ãÊñáÁº∫Â§±ÈóÆÈ¢ò„ÄÇLADËÉΩÂ§üÂ∞ÜËßÜËßâ‰ø°ÊÅØËΩ¨Âåñ‰∏∫‰∏∞ÂØåÁöÑÊñáÊú¨Ë°®Á§∫ÔºåÂπ∂ÈÄöËøáË∑®È¢ÜÂüüÁü•ËØÜÁöÑÊï¥ÂêàÊù•Ê∂àÈô§Ê≠ß‰πâ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂‰∏∫AIÊõ¥ÊúâÊïàÂú∞Ëß£ËØªÂõæÂÉèÈöêÂê´ÊÑè‰πâÊèê‰æõ‰∫ÜÊñ∞ËßÅËß£ÔºåÊé®Âä®‰∫ÜËßÜËßâ-ËØ≠Ë®ÄÊé®ÁêÜÂíå‰∫∫Êú∫‰∫§‰∫íÈ¢ÜÂüüÁöÑÂèëÂ±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17015",
            "title": "Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal\n  Large Language Models",
            "url": "https://huggingface.co/papers/2505.17015",
            "abstract": "Multi-modal large language models (MLLMs) have rapidly advanced in visual tasks, yet their spatial understanding remains limited to single images, leaving them ill-suited for robotics and other real-world applications that require multi-frame reasoning. In this paper, we propose a framework to equip MLLMs with robust multi-frame spatial understanding by integrating depth perception, visual correspondence, and dynamic perception. Central to our approach is the MultiSPA dataset, a novel, large-scale collection of more than 27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we introduce a comprehensive benchmark that tests a wide spectrum of spatial tasks under uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves significant gains over baselines and proprietary systems, demonstrating scalable, generalizable multi-frame reasoning. We further observe multi-task benefits and early indications of emergent capabilities in challenging scenarios, and showcase how our model can serve as a multi-frame reward annotator for robotics.",
            "score": 3,
            "issue_id": 3923,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "b15dc44bbeff2152",
            "authors": [
                "Runsen Xu",
                "Weiyao Wang",
                "Hao Tang",
                "Xingyu Chen",
                "Xiaodong Wang",
                "Fu-Jen Chu",
                "Dahua Lin",
                "Matt Feiszli",
                "Kevin J. Liang"
            ],
            "affiliations": [
                "FAIR, Meta",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17015.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#games",
                    "#3d",
                    "#robotics",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–ú—É–ª—å—Ç–∏–∫–∞–¥—Ä–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞–¥—Ä–∞—Ö. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç MultiSPA –∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Multi-SpatialMLLM –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –º—É–ª—å—Ç–∏–∫–∞–¥—Ä–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–∞ –Ω–∞–≥—Ä–∞–¥."
                },
                "en": {
                    "title": "Empowering MLLMs with Multi-Frame Spatial Intelligence",
                    "desc": "This paper addresses the limitations of multi-modal large language models (MLLMs) in understanding spatial information across multiple frames, which is crucial for applications like robotics. The authors introduce a new framework that enhances MLLMs by incorporating depth perception, visual correspondence, and dynamic perception. They present the MultiSPA dataset, a large-scale collection of over 27 million samples that covers a variety of 3D and 4D scenes, to train and evaluate their model. The proposed Multi-SpatialMLLM shows improved performance in multi-frame reasoning tasks and demonstrates potential for multi-task learning and emergent capabilities in complex scenarios."
                },
                "zh": {
                    "title": "ÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÂ§öÂ∏ßÁ©∫Èó¥ÁêÜËß£ËÉΩÂäõ",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂Ôºå‰ª•Â¢ûÂº∫Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Â§öÂ∏ßÁ©∫Èó¥ÁêÜËß£ÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇÊàë‰ª¨ÈÄöËøáÊï¥ÂêàÊ∑±Â∫¶ÊÑüÁü•„ÄÅËßÜËßâÂØπÂ∫îÂíåÂä®ÊÄÅÊÑüÁü•ÔºåÊù•Ëß£ÂÜ≥Áé∞ÊúâÊ®°ÂûãÂú®Êú∫Âô®‰∫∫ÂíåÁé∞ÂÆûÂ∫îÁî®‰∏≠ÁöÑÂ±ÄÈôêÊÄß„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜMultiSPAÊï∞ÊçÆÈõÜÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´Ë∂ÖËøá2700‰∏áÊ†∑Êú¨ÁöÑÂÖ®Êñ∞Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÊ∂µÁõñÂ§öÁßç3DÂíå4DÂú∫ÊôØ„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãMulti-SpatialMLLMÂú®Â§öÂ∏ßÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊèêÂçáÔºåÂπ∂Â±ïÁ§∫‰∫ÜÂú®Â§çÊùÇÂú∫ÊôØ‰∏≠ÁöÑÂ§ö‰ªªÂä°‰ºòÂäøÂíåÊñ∞ÂÖ¥ËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15517",
            "title": "Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot\n  Manipulation Datasets",
            "url": "https://huggingface.co/papers/2505.15517",
            "abstract": "Robo2VLM, a framework for generating Visual Question Answering datasets using robot trajectory data, enhances and evaluates Vision-Language Models by leveraging sensory modalities and 3D property understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) acquire real-world knowledge and general reasoning ability through Internet-scale image-text corpora. They can augment robotic systems with scene understanding and task planning, and assist visuomotor policies that are trained on robot trajectory data. We explore the reverse paradigm - using rich, real, multi-modal robot trajectory data to enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual Question Answering (VQA) dataset generation framework for VLMs. Given a human tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual and non-descriptive sensory modalities, such as end-effector pose, gripper aperture, and force sensing. Based on these modalities, it segments the robot trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses scene and interaction understanding to identify 3D properties of the robot, task goal, and the target object. The properties are used to generate representative VQA queries - images with textural multiple-choice questions - based on spatial, goal-conditioned, and interaction reasoning question templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710 questions covering 463 distinct scenes and 3,396 robotic manipulation tasks from 176k real robot trajectories. Results suggest that Robo2VLM-1 can benchmark and improve VLM capabilities in spatial and interaction reasoning.",
            "score": 3,
            "issue_id": 3917,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "7c3b47e3a7b062f1",
            "authors": [
                "Kaiyuan Chen",
                "Shuangyu Xie",
                "Zehan Ma",
                "Ken Goldberg"
            ],
            "affiliations": [
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15517.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#games",
                    "#dataset",
                    "#3d",
                    "#reasoning"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "–†–æ–±–æ—Ç—ã —É—á–∞—Ç –ò–ò –≤–∏–¥–µ—Ç—å –∏ –ø–æ–Ω–∏–º–∞—Ç—å –º–∏—Ä",
                    "desc": "Robo2VLM - —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–æ–±–æ—Ç–∞. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–Ω—Å–æ—Ä–Ω—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ 3D-—Å–≤–æ–π—Å—Ç–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLM). –§—Ä–µ–π–º–≤–æ—Ä–∫ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é —Ä–æ–±–æ—Ç–∞ –Ω–∞ —Ñ–∞–∑—ã –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö, —Ü–µ–ª–µ–≤—ã—Ö –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö Robo2VLM-1, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π 463 —Å—Ü–µ–Ω—ã –∏ 3396 –∑–∞–¥–∞—á —Ä–æ–±–æ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π –∏–∑ 176 —Ç—ã—Å—è—á —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–æ–±–æ—Ç–æ–≤."
                },
                "en": {
                    "title": "Enhancing VLMs with Robot Trajectory Insights",
                    "desc": "Robo2VLM is a framework designed to create Visual Question Answering (VQA) datasets by utilizing data from robot trajectories. It enhances Vision-Language Models (VLMs) by integrating sensory information and understanding 3D properties related to robotic tasks. The framework segments robot movements into phases and generates questions based on the robot's interactions with its environment. The resulting dataset, Robo2VLM-1, includes a vast number of questions that help evaluate and improve the reasoning abilities of VLMs in spatial and interaction contexts."
                },
                "zh": {
                    "title": "Âà©Áî®Êú∫Âô®‰∫∫ËΩ®ËøπÊï∞ÊçÆÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑËÉΩÂäõ",
                    "desc": "Robo2VLMÊòØ‰∏Ä‰∏™Áî®‰∫éÁîüÊàêËßÜËßâÈóÆÁ≠îÊï∞ÊçÆÈõÜÁöÑÊ°ÜÊû∂ÔºåÂà©Áî®Êú∫Âô®‰∫∫ËΩ®ËøπÊï∞ÊçÆÊù•Â¢ûÂº∫ÂíåËØÑ‰º∞ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂàÜÊûêÊú∫Âô®‰∫∫ÁöÑ‰º†ÊÑüÂô®Êï∞ÊçÆÂíå3DÂ±ûÊÄßÁêÜËß£ÔºåÁîüÊàê‰∏éÊú∫Âô®‰∫∫Êìç‰ΩúÁõ∏ÂÖ≥ÁöÑÈóÆÁ≠îÊï∞ÊçÆ„ÄÇRobo2VLMÂ∞ÜÊú∫Âô®‰∫∫ËΩ®ËøπÂàÜ‰∏∫Â§ö‰∏™Êìç‰ΩúÈò∂ÊÆµÔºåÂπ∂Âú®ÊØè‰∏™Èò∂ÊÆµËØÜÂà´‰ªªÂä°ÁõÆÊ†áÂíåÁõÆÊ†áÁâ©‰ΩìÁöÑ3DÂ±ûÊÄß„ÄÇÊúÄÁªàÔºåRobo2VLM-1Êï∞ÊçÆÈõÜÂåÖÂê´684,710‰∏™ÈóÆÈ¢òÔºåÊ∂µÁõñ463‰∏™‰∏çÂêåÂú∫ÊôØÂíå3,396‰∏™Êú∫Âô®‰∫∫Êìç‰Ωú‰ªªÂä°ÔºåËÉΩÂ§üÊúâÊïàËØÑ‰º∞ÂíåÊèêÂçáVLMÂú®Á©∫Èó¥Âíå‰∫§‰∫íÊé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16612",
            "title": "Steering Large Language Models for Machine Translation Personalization",
            "url": "https://huggingface.co/papers/2505.16612",
            "abstract": "Strategies including prompting and contrastive frameworks using latent concepts from sparse autoencoders effectively personalize LLM translations in low-resource settings while maintaining quality.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality machine translation systems based on large language models (LLMs) have simplified the production of personalized translations reflecting specific stylistic constraints. However, these systems still struggle in settings where stylistic requirements are less explicit and might be harder to convey via prompting. We explore various strategies for personalizing LLM-generated translations in low-resource settings, focusing on the challenging literary translation domain. We explore prompting strategies and inference-time interventions for steering model generations towards a personalized style, and propose a contrastive framework exploiting latent concepts extracted from sparse autoencoders to identify salient personalization properties. Our results show that steering achieves strong personalization while preserving translation quality. We further examine the impact of steering on LLM representations, finding model layers with a relevant impact for personalization are impacted similarly by multi-shot prompting and our steering method, suggesting similar mechanism at play.",
            "score": 2,
            "issue_id": 3922,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "5695106d35c6955a",
            "authors": [
                "Daniel Scalena",
                "Gabriele Sarti",
                "Arianna Bisazza",
                "Elisabetta Fersini",
                "Malvina Nissim"
            ],
            "affiliations": [
                "CLCG, University of Groningen",
                "University of Milano-Bicocca"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16612.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#training",
                    "#multimodal",
                    "#machine_translation"
                ],
                "emoji": "üé≠",
                "ru": {
                    "title": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–æ–≤ LLM: –æ—Ç –ø—Ä–æ–º–ø—Ç–æ–≤ –∫ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–º –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º",
                    "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–µ—Ä–µ–≤–æ–¥–æ–≤, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM), –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –æ–±–ª–∞—Å—Ç–∏ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã –∏–∑—É—á–∞—é—Ç –º–µ—Ç–æ–¥—ã –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞ –∏ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –¥–ª—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –º–æ–¥–µ–ª–∏ –∫ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Å—Ç–∏–ª—é. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö —Å–≤–æ–π—Å—Ç–≤ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–∏–ª—å–Ω–æ–π –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞."
                },
                "en": {
                    "title": "Personalized Translations Made Easy with LLMs!",
                    "desc": "This paper discusses methods to improve personalized translations using large language models (LLMs) in situations where resources are limited. It highlights the use of prompting strategies and a contrastive framework that leverages latent concepts from sparse autoencoders to enhance stylistic personalization. The authors demonstrate that these techniques can effectively guide the model's output while maintaining high translation quality. Additionally, they analyze how these personalization methods influence the internal representations of the LLM, indicating that similar mechanisms are at work in both prompting and steering approaches."
                },
                "zh": {
                    "title": "‰∏™ÊÄßÂåñÁøªËØëÁöÑÊñ∞Á≠ñÁï•",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂú®‰ΩéËµÑÊ∫êÁéØÂ¢É‰∏ãÔºåÂ¶Ç‰ΩïÈÄöËøáÊèêÁ§∫ÂíåÂØπÊØîÊ°ÜÊû∂Êù•‰∏™ÊÄßÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÁøªËØë„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂà©Áî®Á®ÄÁñèËá™ÁºñÁ†ÅÂô®ÊèêÂèñÁöÑÊΩúÂú®Ê¶ÇÂøµÔºåÂèØ‰ª•ÊúâÊïàËØÜÂà´‰∏™ÊÄßÂåñÁâπÂæÅÔºå‰ªéËÄåÊîπÂñÑÁøªËØëË¥®Èáè„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑÁ≠ñÁï•Âú®ÊñáÂ≠¶ÁøªËØëÈ¢ÜÂüüË°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§üÂú®‰øùÊåÅÁøªËØëË¥®ÈáèÁöÑÂêåÊó∂ÔºåÂÆûÁé∞Âº∫ÁÉàÁöÑ‰∏™ÊÄßÂåñÊïàÊûú„ÄÇÊ≠§Â§ñÔºåÁ†îÁ©∂ËøòÂèëÁé∞Ôºå‰∏™ÊÄßÂåñÁõ∏ÂÖ≥ÁöÑÊ®°ÂûãÂ±ÇÂú®Â§öÊ¨°ÊèêÁ§∫ÂíåÊàë‰ª¨ÁöÑÂºïÂØºÊñπÊ≥ï‰∏ãÂèóÂà∞ÁöÑÂΩ±ÂìçÁõ∏‰ººÔºåË°®Êòé‰∏§ËÄÖÂèØËÉΩÂ≠òÂú®Áõ∏‰ººÁöÑÊú∫Âà∂„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16170",
            "title": "When Do LLMs Admit Their Mistakes? Understanding the Role of Model\n  Belief in Retraction",
            "url": "https://huggingface.co/papers/2505.16170",
            "abstract": "LLMs rarely retract incorrect answers they believe to be factually correct, but supervised fine-tuning can improve their retraction performance by refining their internal beliefs.  \t\t\t\t\tAI-generated summary \t\t\t\t Can large language models (LLMs) admit their mistakes when they should know better? In this work, we define the behavior of acknowledging errors in previously generated answers as \"retraction\" and aim to understand when and why LLMs choose to retract. We first construct model-specific datasets to evaluate whether a model will retract an incorrect answer that contradicts its own parametric knowledge. While LLMs are capable of retraction, they do so only infrequently. We demonstrate that retraction is closely tied to previously identified indicators of models' internal belief: models fail to retract wrong answers that they \"believe\" to be factually correct. Steering experiments further demonstrate that internal belief causally influences model retraction. In particular, when the model does not believe its answer, this not only encourages the model to attempt to verify the answer, but also alters attention behavior during self-verification. Finally, we demonstrate that simple supervised fine-tuning significantly improves retraction performance by helping the model learn more accurate internal beliefs. Code and datasets are available on https://github.com/ayyyq/llm-retraction.",
            "score": 2,
            "issue_id": 3919,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "2f89bb0f5c59846a",
            "authors": [
                "Yuqing Yang",
                "Robin Jia"
            ],
            "affiliations": [
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16170.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#hallucinations",
                    "#dataset"
                ],
                "emoji": "ü§î",
                "ru": {
                    "title": "–£—á–∏–º –ò–ò –ø—Ä–∏–∑–Ω–∞–≤–∞—Ç—å —Å–≤–æ–∏ –æ—à–∏–±–∫–∏",
                    "desc": "–≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø—Ä–∏–∑–Ω–∞–≤–∞—Ç—å —Å–≤–æ–∏ –æ—à–∏–±–∫–∏, —á—Ç–æ –∞–≤—Ç–æ—Ä—ã –Ω–∞–∑—ã–≤–∞—é—Ç '—Ä–µ—Ç—Ä–∞–∫—Ü–∏–µ–π'. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ LLM —Ä–µ–¥–∫–æ –æ—Ç–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –æ—Ç –Ω–µ–≤–µ—Ä–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω–∏ —Å—á–∏—Ç–∞—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Ä–µ—Ç—Ä–∞–∫—Ü–∏—è —Ç–µ—Å–Ω–æ —Å–≤—è–∑–∞–Ω–∞ —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º–∏ —É–±–µ–∂–¥–µ–Ω–∏—è–º–∏ –º–æ–¥–µ–ª–∏. –ü—Ä–æ—Å—Ç–∞—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ —Ä–µ—Ç—Ä–∞–∫—Ü–∏–∏, –ø–æ–º–æ–≥–∞—è –∏–º —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —É–±–µ–∂–¥–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Enhancing LLMs: Fine-Tuning for Better Error Retraction",
                    "desc": "This paper investigates how large language models (LLMs) handle the acknowledgment of their mistakes, a behavior termed 'retraction'. It finds that LLMs rarely retract incorrect answers, especially when they are confident in their incorrect beliefs. The study shows that the ability to retract is influenced by the model's internal beliefs, where models are less likely to retract answers they consider factually correct. Additionally, the authors demonstrate that supervised fine-tuning can enhance the retraction capabilities of LLMs by refining their internal belief systems."
                },
                "zh": {
                    "title": "ÊèêÂçáÊ®°ÂûãÁöÑÈîôËØØÊí§ÂõûËÉΩÂäõ",
                    "desc": "Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Èù¢ÂØπÈîôËØØÊó∂ÁöÑËá™Êàë‰øÆÊ≠£ËÉΩÂäõÔºåÁß∞‰πã‰∏∫‚ÄúÊí§Âõû‚Äù„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåLLMsÂú®ËÆ§‰∏∫Ëá™Â∑±ÁöÑÁ≠îÊ°àÊòØÊ≠£Á°ÆÊó∂ÔºåÂæÄÂæÄ‰∏ç‰ºöÊí§ÂõûÈîôËØØÁöÑÂõûÁ≠î„ÄÇÈÄöËøáÊûÑÂª∫ÁâπÂÆöÁöÑÊï∞ÊçÆÈõÜÔºåÊàë‰ª¨ËØÑ‰º∞‰∫ÜÊ®°ÂûãÂú®‰ΩïÁßçÊÉÖÂÜµ‰∏ã‰ºöËøõË°åÊí§ÂõûÔºåÂπ∂ÂèëÁé∞ÂÜÖÈÉ®‰ø°ÂøµÂØπÊí§ÂõûË°å‰∏∫ÊúâÈáçË¶ÅÂΩ±Âìç„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ÈÄöËøáÁõëÁù£ÂæÆË∞ÉÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊí§ÂõûÊÄßËÉΩÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂ≠¶‰π†Êõ¥ÂáÜÁ°ÆÁöÑÂÜÖÈÉ®‰ø°Âøµ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16088",
            "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal\n  Reasoning",
            "url": "https://huggingface.co/papers/2505.16088",
            "abstract": "Modern BPE tokenizers often split calendar dates into meaningless fragments, e.g., 20250312 rightarrow 202, 503, 12, inflating token counts and obscuring the inherent structure needed for robust temporal reasoning. In this work, we (1) introduce a simple yet interpretable metric, termed date fragmentation ratio, that measures how faithfully a tokenizer preserves multi-digit date components; (2) release DateAugBench, a suite of 6500 examples spanning three temporal reasoning tasks: context-based date resolution, format-invariance puzzles, and date arithmetic across historical, contemporary, and future regimes; and (3) through layer-wise probing and causal attention-hop analyses, uncover an emergent date-abstraction mechanism whereby large language models stitch together the fragments of month, day, and year components for temporal reasoning. Our experiments show that excessive fragmentation correlates with accuracy drops of up to 10 points on uncommon dates like historical and futuristic dates. Further, we find that the larger the model, the faster the emergent date abstraction that heals date fragments is accomplished. Lastly, we observe a reasoning path that LLMs follow to assemble date fragments, typically differing from human interpretation (year rightarrow month rightarrow day).",
            "score": 2,
            "issue_id": 3920,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 –º–∞—è",
                "en": "May 22",
                "zh": "5Êúà22Êó•"
            },
            "hash": "e95fb5fdc583b428",
            "authors": [
                "Gagan Bhatia",
                "Maxime Peyrard",
                "Wei Zhao"
            ],
            "affiliations": [
                "University of Aberdeen",
                "Universit√© Grenoble Alpes & CNRS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16088.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#data",
                    "#dataset",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "üóìÔ∏è",
                "ru": {
                    "title": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞—Ç –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –º–µ—Ç—Ä–∏–∫—É '–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞—Ç' –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö DateAugBench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏ –¥–∞—Ç –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–π –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–∞—Ç. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —á—Ä–µ–∑–º–µ—Ä–Ω–∞—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è –º–æ–∂–µ—Ç —Å–Ω–∏–∂–∞—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–µ–¥–∫–∏—Ö –¥–∞—Ç–∞—Ö, –∞ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –±—ã—Å—Ç—Ä–µ–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–µ–π –¥–∞—Ç."
                },
                "en": {
                    "title": "Preserving Date Integrity for Better Temporal Reasoning",
                    "desc": "This paper addresses the issue of how modern Byte Pair Encoding (BPE) tokenizers break down calendar dates into smaller parts, which can hinder effective temporal reasoning in language models. The authors propose a new metric called the date fragmentation ratio to evaluate how well tokenizers maintain the integrity of multi-digit date components. They also introduce DateAugBench, a dataset designed for testing temporal reasoning across various tasks involving dates. The findings reveal that excessive fragmentation can lead to significant drops in accuracy, especially for less common dates, and that larger models are better at reconstructing these fragmented dates for reasoning tasks."
                },
                "zh": {
                    "title": "ÊèêÂçáÊó•ÊúüÊé®ÁêÜÁöÑÂàÜËØçÂô®ËÆæËÆ°",
                    "desc": "Áé∞‰ª£ÁöÑBPEÂàÜËØçÂô®Â∏∏Â∏∏Â∞ÜÊó•ÊúüÂàÜÂâ≤ÊàêÊó†ÊÑè‰πâÁöÑÁ¢éÁâáÔºå‰æãÂ¶ÇÂ∞Ü20250312ÂàÜÂâ≤‰∏∫202„ÄÅ503Âíå12ÔºåËøôÊ†∑‰ºöÂ¢ûÂä†Ê†áËÆ∞Êï∞ÈáèÂπ∂Êé©ÁõñËøõË°åÊó∂Èó¥Êé®ÁêÜÊâÄÈúÄÁöÑÂÜÖÂú®ÁªìÊûÑ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçï‰∏îÂèØËß£ÈáäÁöÑÂ∫¶ÈáèÊ†áÂáÜÔºåÁß∞‰∏∫Êó•ÊúüÁ¢éÁâáÂåñÊØîÁéáÔºåÁî®‰∫éË°°ÈáèÂàÜËØçÂô®‰øùÁïôÂ§ö‰ΩçÊï∞Êó•ÊúüÁªÑ‰ª∂ÁöÑÂø†ÂÆûÂ∫¶„ÄÇÊàë‰ª¨ËøòÂèëÂ∏É‰∫ÜDateAugBenchÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´6500‰∏™Á§∫‰æãÁöÑÂ•ó‰ª∂ÔºåÊ∂µÁõñ‰∫Ü‰∏âÁßçÊó∂Èó¥Êé®ÁêÜ‰ªªÂä°ÔºöÂü∫‰∫é‰∏ä‰∏ãÊñáÁöÑÊó•ÊúüËß£Êûê„ÄÅÊ†ºÂºè‰∏çÂèòÊÄßÈöæÈ¢òÂíåÂéÜÂè≤„ÄÅÂΩì‰ª£ÂèäÊú™Êù•ÁöÑÊó•ÊúüÁÆóÊúØ„ÄÇÂÆûÈ™åË°®ÊòéÔºåËøáÂ∫¶ÁöÑÁ¢éÁâáÂåñ‰∏é‰∏çÂ∏∏ËßÅÊó•ÊúüÔºàÂ¶ÇÂéÜÂè≤ÂíåÊú™Êù•Êó•ÊúüÔºâÁöÑÂáÜÁ°ÆÊÄß‰∏ãÈôçÈ´òËææ10‰∏™ÁôæÂàÜÁÇπÁõ∏ÂÖ≥Ôºå‰∏îÊ®°ÂûãË∂äÂ§ßÔºå‰øÆÂ§çÊó•ÊúüÁ¢éÁâáÁöÑÊäΩË±°Êú∫Âà∂Ë∂äÂø´„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15865",
            "title": "How Do Large Vision-Language Models See Text in Image? Unveiling the\n  Distinctive Role of OCR Heads",
            "url": "https://huggingface.co/papers/2505.15865",
            "abstract": "The study identifies and analyzes OCR Heads within Large Vision Language Models, revealing their unique activation patterns and roles in interpreting text within images.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite significant advancements in Large Vision Language Models (LVLMs), a gap remains, particularly regarding their interpretability and how they locate and interpret textual information within images. In this paper, we explore various LVLMs to identify the specific heads responsible for recognizing text from images, which we term the Optical Character Recognition Head (OCR Head). Our findings regarding these heads are as follows: (1) Less Sparse: Unlike previous retrieval heads, a large number of heads are activated to extract textual information from images. (2) Qualitatively Distinct: OCR heads possess properties that differ significantly from general retrieval heads, exhibiting low similarity in their characteristics. (3) Statically Activated: The frequency of activation for these heads closely aligns with their OCR scores. We validate our findings in downstream tasks by applying Chain-of-Thought (CoT) to both OCR and conventional retrieval heads and by masking these heads. We also demonstrate that redistributing sink-token values within the OCR heads improves performance. These insights provide a deeper understanding of the internal mechanisms LVLMs employ in processing embedded textual information in images.",
            "score": 2,
            "issue_id": 3919,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "4cf7128eaead82ce",
            "authors": [
                "Ingeol Baek",
                "Hwan Chang",
                "Sunghyun Ryu",
                "Hwanhee Lee"
            ],
            "affiliations": [
                "Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea",
                "Department of Computer Engineering, Sejong University, Seoul, Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15865.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#interpretability",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–µ–∫—Ä–µ—Ç–æ–≤ OCR –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –∞–Ω–∞–ª–∏–∑—É OCR-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LVLM). –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —ç—Ç–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ OCR-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –º–µ–Ω–µ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω—ã, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç –æ–±—ã—á–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –∏–º–µ—é—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫—É—é –∞–∫—Ç–∏–≤–∞—Ü–∏—é. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (CoT) –∏ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ OCR-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ø–æ–∑–≤–æ–ª–∏–ª–æ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö."
                },
                "en": {
                    "title": "Unlocking Text Recognition in Images with OCR Heads",
                    "desc": "This paper investigates the Optical Character Recognition (OCR) Heads in Large Vision Language Models (LVLMs) to understand how they process text in images. It reveals that these heads are less sparse, meaning many of them activate simultaneously to extract text, unlike traditional retrieval heads. The study also finds that OCR heads have distinct properties, showing low similarity to general retrieval heads, and their activation frequency correlates with their OCR performance. By applying techniques like Chain-of-Thought and redistributing values within these heads, the research enhances the models' ability to interpret text in images, shedding light on their internal workings."
                },
                "zh": {
                    "title": "Êè≠Á§∫Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑOCRÂ§¥",
                    "desc": "Êú¨Á†îÁ©∂ÂàÜÊûê‰∫ÜÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂÖâÂ≠¶Â≠óÁ¨¶ËØÜÂà´Â§¥ÔºàOCRÂ§¥ÔºâÔºåÊè≠Á§∫‰∫ÜÂÆÉ‰ª¨Âú®ÂõæÂÉè‰∏≠ÊñáÊú¨Ëß£ËØª‰∏≠ÁöÑÁã¨ÁâπÊøÄÊ¥ªÊ®°ÂºèÂíåËßíËâ≤„ÄÇÂ∞ΩÁÆ°Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂú®ÂèØËß£ÈáäÊÄßÊñπÈù¢‰ªçÂ≠òÂú®Â∑ÆË∑ùÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂÆö‰ΩçÂíåËß£ËØªÂõæÂÉè‰∏≠ÁöÑÊñáÊú¨‰ø°ÊÅØÊó∂„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåOCRÂ§¥ÁöÑÊøÄÊ¥ªÊñπÂºè‰∏é‰º†ÁªüÁöÑÊ£ÄÁ¥¢Â§¥ÊòæËëó‰∏çÂêåÔºå‰∏îÂú®ÊèêÂèñÊñáÊú¨‰ø°ÊÅØÊó∂ÊøÄÊ¥ªÁöÑÂ§¥Êï∞ÈáèËæÉÂ§ö„ÄÇÈÄöËøáÂØπËøô‰∫õÂ§¥ÁöÑÁ†îÁ©∂ÔºåÊàë‰ª¨Âä†Ê∑±‰∫ÜÂØπLVLMsÂú®Â§ÑÁêÜÂõæÂÉè‰∏≠ÂµåÂÖ•ÊñáÊú¨‰ø°ÊÅØÊó∂ÂÜÖÈÉ®Êú∫Âà∂ÁöÑÁêÜËß£„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14462",
            "title": "RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture\n  Understanding",
            "url": "https://huggingface.co/papers/2505.14462",
            "abstract": "RAVENEA, a retrieval-augmented benchmark, enhances visual culture understanding in VLMs through culture-focused tasks and outperforms non-augmented models across various metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t As vision-language models (VLMs) become increasingly integrated into daily life, the need for accurate visual culture understanding is becoming critical. Yet, these models frequently fall short in interpreting cultural nuances effectively. Prior work has demonstrated the effectiveness of retrieval-augmented generation (RAG) in enhancing cultural understanding in text-only settings, while its application in multimodal scenarios remains underexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented Visual culturE uNdErstAnding), a new benchmark designed to advance visual culture understanding through retrieval, focusing on two tasks: culture-focused visual question answering (cVQA) and culture-informed image captioning (cIC). RAVENEA extends existing datasets by integrating over 10,000 Wikipedia documents curated and ranked by human annotators. With RAVENEA, we train and evaluate seven multimodal retrievers for each image query, and measure the downstream impact of retrieval-augmented inputs across fourteen state-of-the-art VLMs. Our results show that lightweight VLMs, when augmented with culture-aware retrieval, outperform their non-augmented counterparts (by at least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the value of retrieval-augmented methods and culturally inclusive benchmarks for multimodal understanding.",
            "score": 2,
            "issue_id": 3926,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 –º–∞—è",
                "en": "May 20",
                "zh": "5Êúà20Êó•"
            },
            "hash": "49a96a5bbddf2965",
            "authors": [
                "Jiaang Li",
                "Yifei Yuan",
                "Wenyan Li",
                "Mohammad Aliannejadi",
                "Daniel Hershcovich",
                "Anders S√∏gaard",
                "Ivan Vuliƒá",
                "Wenxuan Zhang",
                "Paul Pu Liang",
                "Yang Deng",
                "Serge Belongie"
            ],
            "affiliations": [
                "ETH Z√ºrich",
                "Massachusetts Institute of Technology",
                "Singapore Management University",
                "Singapore University of Technology and Design",
                "University of Amsterdam",
                "University of Cambridge",
                "University of Copenhagen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14462.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#transfer_learning",
                    "#interpretability",
                    "#multimodal",
                    "#rag",
                    "#games",
                    "#benchmark"
                ],
                "emoji": "üåç",
                "ru": {
                    "title": "–ö—É–ª—å—Ç—É—Ä–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ò–ò",
                    "desc": "RAVENEA - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∫—É–ª—å—Ç—É—Ä—ã –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –∑–∞–¥–∞—á–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –∫—É–ª—å—Ç—É—Ä–µ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º (cVQA) –∏ —Å–æ–∑–¥–∞–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —É—á–µ—Ç–æ–º –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (cIC). –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 10 000 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏, –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–∞–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∫—É–ª—å—Ç—É—Ä–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –æ–±—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ 3.2% –≤ cVQA –∏ 6.2% –≤ cIC."
                },
                "en": {
                    "title": "Enhancing Visual Culture Understanding with RAVENEA",
                    "desc": "RAVENEA is a new benchmark designed to improve visual culture understanding in vision-language models (VLMs) through retrieval-augmented techniques. It focuses on two main tasks: culture-focused visual question answering (cVQA) and culture-informed image captioning (cIC). By integrating over 10,000 curated Wikipedia documents, RAVENEA enhances the training and evaluation of multimodal retrievers for image queries. The results show that VLMs using this retrieval-augmented approach significantly outperform those that do not, demonstrating the importance of culturally aware data in machine learning."
                },
                "zh": {
                    "title": "RAVENEAÔºöÊèêÂçáËßÜËßâÊñáÂåñÁêÜËß£ÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫Âü∫ÂáÜ",
                    "desc": "RAVENEAÊòØ‰∏Ä‰∏™Â¢ûÂº∫ËßÜËßâÊñáÂåñÁêÜËß£ÁöÑÂü∫ÂáÜÔºå‰∏ìÊ≥®‰∫éÊñáÂåñÁõ∏ÂÖ≥ÁöÑ‰ªªÂä°ÔºåÊèêÂçá‰∫ÜËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÁöÑË°®Áé∞„ÄÇËØ•Âü∫ÂáÜÈÄöËøáÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÊñπÊ≥ïÔºåËß£ÂÜ≥‰∫ÜÊ®°ÂûãÂú®ÁêÜËß£ÊñáÂåñÁªÜÂæÆÂ∑ÆÂà´ÊñπÈù¢ÁöÑ‰∏çË∂≥„ÄÇRAVENEAÂåÖÂê´Ë∂ÖËøá10,000‰∏™ÁªèËøá‰∫∫Â∑•Ê†áÊ≥®ÂíåÊéíÂêçÁöÑÁª¥Âü∫ÁôæÁßëÊñáÊ°£ÔºåÊîØÊåÅÊñáÂåñËÅöÁÑ¶ÁöÑËßÜËßâÈóÆÁ≠îÂíåÂõæÂÉèÊèèËø∞‰ªªÂä°„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÁªèËøáÊñáÂåñÊÑèËØÜÊ£ÄÁ¥¢Â¢ûÂº∫ÁöÑËΩªÈáèÁ∫ßVLMsÂú®Â§ö‰∏™ÊåáÊ†á‰∏ä‰ºò‰∫éÊú™Â¢ûÂº∫ÁöÑÊ®°ÂûãÔºåÊòæÁ§∫‰∫ÜÊ£ÄÁ¥¢Â¢ûÂº∫ÊñπÊ≥ïÂú®Â§öÊ®°ÊÄÅÁêÜËß£‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14395",
            "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation\n  Capabilities in Any Language",
            "url": "https://huggingface.co/papers/2505.14395",
            "abstract": "MUG-Eval assesses LLMs' multilingual generation by transforming benchmarks into conversational tasks, offering a language-independent and NLP tool-free method that correlates well with established benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating text generation capabilities of large language models (LLMs) is challenging, particularly for low-resource languages where methods for direct assessment are scarce. We propose MUG-Eval, a novel framework that evaluates LLMs' multilingual generation capabilities by transforming existing benchmarks into conversational tasks and measuring the LLMs' accuracies on those tasks. We specifically designed these conversational tasks to require effective communication in the target language. Then, we simply use task success rate as a proxy of successful conversation generation. Our approach offers two key advantages: it is independent of language-specific NLP tools or annotated datasets, which are limited for most languages, and it does not rely on LLMs-as-judges, whose evaluation quality degrades outside a few high-resource languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and low-resource categories, and we find that MUG-Eval correlates strongly with established benchmarks (r > 0.75) while enabling standardized comparisons across languages and models. Our framework provides a robust and resource-efficient solution for evaluating multilingual generation that can be extended to thousands of languages.",
            "score": 2,
            "issue_id": 3921,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 –º–∞—è",
                "en": "May 20",
                "zh": "5Êúà20Êó•"
            },
            "hash": "88fb6eaee50e6cb2",
            "authors": [
                "Seyoung Song",
                "Seogyeong Jeong",
                "Eunsu Kim",
                "Jiho Jin",
                "Dongkwan Kim",
                "Jay Shin",
                "Alice Oh"
            ],
            "affiliations": [
                "KAIST",
                "Trillion Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14395.jpg",
            "data": {
                "categories": [
                    "#machine_translation",
                    "#benchmark",
                    "#low_resource",
                    "#multilingual"
                ],
                "emoji": "üåê",
                "ru": {
                    "title": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö LLM –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤",
                    "desc": "MUG-Eval - —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –∫ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –û–Ω —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ—Å—Ç—ã –≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏ –∏–∑–º–µ—Ä—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å LLM –ø—Ä–∏ –∏—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —è–∑—ã–∫–æ—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –∏ –Ω–µ –ø–æ–ª–∞–≥–∞–µ—Ç—Å—è –Ω–∞ –æ—Ü–µ–Ω–∫—É –¥—Ä—É–≥–∏–º–∏ LLM. MUG-Eval –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∏–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ —ç—Ç–∞–ª–æ–Ω–∞–º–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏ –∏ –º–æ–¥–µ–ª—è–º–∏."
                },
                "en": {
                    "title": "MUG-Eval: A Language-Independent Tool for Multilingual LLM Assessment",
                    "desc": "MUG-Eval is a new framework designed to evaluate the multilingual generation capabilities of large language models (LLMs). It transforms existing benchmarks into conversational tasks, allowing for a language-independent assessment of LLM performance. By measuring the success rate of these tasks, MUG-Eval provides a proxy for effective conversation generation without relying on specific NLP tools or annotated datasets. The framework has been tested on 8 LLMs across 30 languages and shows strong correlation with established benchmarks, making it a valuable tool for evaluating LLMs in low-resource languages."
                },
                "zh": {
                    "title": "MUG-EvalÔºöÂ§öËØ≠Ë®ÄÁîüÊàêÁöÑËØÑ‰º∞Êñ∞ÊñπÊ≥ï",
                    "desc": "MUG-EvalÊòØ‰∏Ä‰∏™ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂ§öËØ≠Ë®ÄÁîüÊàêËÉΩÂäõÁöÑÊñ∞Ê°ÜÊû∂„ÄÇÂÆÉÈÄöËøáÂ∞ÜÁé∞ÊúâÂü∫ÂáÜËΩ¨Âåñ‰∏∫ÂØπËØù‰ªªÂä°ÔºåÊù•ÊµãÈáèLLMsÂú®Ëøô‰∫õ‰ªªÂä°‰∏äÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ï‰∏ç‰æùËµñ‰∫éÁâπÂÆöËØ≠Ë®ÄÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÊàñÊ†áÊ≥®Êï∞ÊçÆÈõÜÔºåÈÄÇÁî®‰∫éÂ§öÁßçËØ≠Ë®Ä„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåMUG-Eval‰∏éÂ∑≤ÊúâÂü∫ÂáÜÁöÑÁõ∏ÂÖ≥ÊÄßÂæàÂº∫ÔºåËÉΩÂ§ü‰∏∫Â§öËØ≠Ë®ÄÁîüÊàêÊèê‰æõÊ†áÂáÜÂåñÁöÑÊØîËæÉ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13344",
            "title": "RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE\n  Optimization on Diffusion Transformers",
            "url": "https://huggingface.co/papers/2505.13344",
            "abstract": "RoPECraft is a training-free method that modifies rotary positional embeddings in diffusion transformers to transfer motion from reference videos, enhancing text-guided video generation and reducing artifacts.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose RoPECraft, a training-free video motion transfer method for diffusion transformers that operates solely by modifying their rotary positional embeddings (RoPE). We first extract dense optical flow from a reference video, and utilize the resulting motion offsets to warp the complex-exponential tensors of RoPE, effectively encoding motion into the generation process. These embeddings are then further optimized during denoising time steps via trajectory alignment between the predicted and target velocities using a flow-matching objective. To keep the output faithful to the text prompt and prevent duplicate generations, we incorporate a regularization term based on the phase components of the reference video's Fourier transform, projecting the phase angles onto a smooth manifold to suppress high-frequency artifacts. Experiments on benchmarks reveal that RoPECraft outperforms all recently published methods, both qualitatively and quantitatively.",
            "score": 2,
            "issue_id": 3932,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 –º–∞—è",
                "en": "May 19",
                "zh": "5Êúà19Êó•"
            },
            "hash": "e47accfd208862f5",
            "authors": [
                "Ahmet Berke Gokmen",
                "Yigit Ekin",
                "Bahri Batuhan Bilecen",
                "Aysegul Dundar"
            ],
            "affiliations": [
                "Bilkent University, Department of Computer Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13344.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#diffusion",
                    "#multimodal",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "–ü–µ—Ä–µ–Ω–æ—Å –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è",
                    "desc": "RoPECraft - —ç—Ç–æ –º–µ—Ç–æ–¥ –ø–µ—Ä–µ–Ω–æ—Å–∞ –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ–æ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—É—Ç–µ–º –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ RoPE –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –∏–∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –≤–∏–¥–µ–æ. –ú–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤–æ –≤—Ä–µ–º—è —à–∞–≥–æ–≤ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∞–∑–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –§—É—Ä—å–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ RoPECraft –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –∫–∞–∫ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ, —Ç–∞–∫ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ."
                },
                "en": {
                    "title": "Seamless Motion Transfer in Video Generation with RoPECraft",
                    "desc": "RoPECraft is a novel method that enhances video generation by transferring motion from reference videos without requiring additional training. It modifies rotary positional embeddings (RoPE) in diffusion transformers to incorporate motion information extracted from dense optical flow. The method aligns predicted and target velocities during the denoising process, ensuring that the generated video matches the intended motion. Additionally, it uses a regularization technique based on Fourier transform phase components to maintain fidelity to text prompts and minimize artifacts in the output."
                },
                "zh": {
                    "title": "RoPECraftÔºöÊó†ËÆ≠ÁªÉÁöÑËßÜÈ¢ëËøêÂä®ËΩ¨ÁßªÊñ∞ÊñπÊ≥ï",
                    "desc": "RoPECraftÊòØ‰∏ÄÁßçÊó†ÈúÄËÆ≠ÁªÉÁöÑÊñπÊ≥ïÔºåÈÄöËøá‰øÆÊîπÊâ©Êï£ÂèòÊç¢Âô®‰∏≠ÁöÑÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÊù•ÂÆûÁé∞ËßÜÈ¢ëËøêÂä®ËΩ¨Áßª„ÄÇËØ•ÊñπÊ≥ïÈ¶ñÂÖà‰ªéÂèÇËÄÉËßÜÈ¢ë‰∏≠ÊèêÂèñÂØÜÈõÜÂÖâÊµÅÔºåÂπ∂Âà©Áî®ËøêÂä®ÂÅèÁßªÈáèÊù•Êâ≠Êõ≤RoPEÁöÑÂ§çÊåáÊï∞Âº†ÈáèÔºå‰ªéËÄåÂ∞ÜËøêÂä®‰ø°ÊÅØÁºñÁ†ÅÂà∞ÁîüÊàêËøáÁ®ã‰∏≠„ÄÇÂú®ÂéªÂô™Êó∂Èó¥Ê≠•È™§‰∏≠ÔºåÈÄöËøá‰ΩøÁî®ÊµÅÂåπÈÖçÁõÆÊ†áÂØπÈ¢ÑÊµãÈÄüÂ∫¶ÂíåÁõÆÊ†áÈÄüÂ∫¶ËøõË°åËΩ®ËøπÂØπÈΩêÔºåËøõ‰∏ÄÊ≠•‰ºòÂåñËøô‰∫õÂµåÂÖ•„ÄÇ‰∏∫‰∫Ü‰øùÊåÅËæìÂá∫‰∏éÊñáÊú¨ÊèêÁ§∫ÁöÑ‰∏ÄËá¥ÊÄßÂπ∂Èò≤Ê≠¢ÈáçÂ§çÁîüÊàêÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÂü∫‰∫éÂèÇËÄÉËßÜÈ¢ëÂÇÖÈáåÂè∂ÂèòÊç¢Áõ∏‰ΩçÂàÜÈáèÁöÑÊ≠£ÂàôÂåñÈ°πÔºå‰ª•ÊäëÂà∂È´òÈ¢ë‰º™ÂΩ±„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16048",
            "title": "SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution",
            "url": "https://huggingface.co/papers/2505.16048",
            "abstract": "A dataset benchmarks spatial and physical reasoning of LLMs using topology optimization tasks without simulation tools.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a novel dataset designed to benchmark the physical and spatial reasoning capabilities of Large Language Models (LLM) based on topology optimization, a method for computing optimal material distributions within a design space under prescribed loads and supports. In this dataset, LLMs are provided with conditions such as 2D boundary, applied forces and supports, and must reason about the resulting optimal material distribution. The dataset includes a variety of tasks, ranging from filling in masked regions within partial structures to predicting complete material distributions. Solving these tasks requires understanding the flow of forces and the required material distribution under given constraints, without access to simulation tools or explicit physical models, challenging models to reason about structural stability and spatial organization. Our dataset targets the evaluation of spatial and physical reasoning abilities in 2D settings, offering a complementary perspective to traditional language and logic benchmarks.",
            "score": 1,
            "issue_id": 3920,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "cf44ff3c901f7498",
            "authors": [
                "Philipp D. Siedler"
            ],
            "affiliations": [
                "Aleph Alpha Research, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16048.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "üèóÔ∏è",
                "ru": {
                    "title": "–ò—Å–ø—ã—Ç–∞–Ω–∏–µ –ò–ò –Ω–∞ –ø—Ä–æ—á–Ω–æ—Å—Ç—å: —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–µ–∑ —Å–∏–º—É–ª—è—Ü–∏–π",
                    "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞—á —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ú–æ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω—ã –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –≤ 2D-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–∏ –∑–∞–¥–∞–Ω–Ω—ã—Ö –Ω–∞–≥—Ä—É–∑–∫–∞—Ö –∏ –æ–ø–æ—Ä–∞—Ö. –ó–∞–¥–∞—á–∏ –≤–∫–ª—é—á–∞—é—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –≤ —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–∞—Ö –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª–∞. –≠—Ç–æ—Ç –¥–∞—Ç–∞—Å–µ—Ç –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Å–∏–º—É–ª—è—Ü–∏–∏."
                },
                "en": {
                    "title": "Benchmarking LLMs in Spatial and Physical Reasoning with Topology Optimization",
                    "desc": "This paper presents a new dataset aimed at evaluating the spatial and physical reasoning skills of Large Language Models (LLMs) through topology optimization tasks. The tasks require LLMs to analyze conditions like 2D boundaries and applied forces to determine optimal material distributions without using simulation tools. By challenging models to predict material layouts and understand force flows, the dataset assesses their ability to reason about structural stability and spatial organization. This approach provides a unique perspective on LLM performance, complementing existing benchmarks focused on language and logic."
                },
                "zh": {
                    "title": "ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ©∫Èó¥‰∏éÁâ©ÁêÜÊé®ÁêÜËÉΩÂäõ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Êñ∞Êï∞ÊçÆÈõÜÔºåÊó®Âú®ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ÊãìÊâë‰ºòÂåñ‰ªªÂä°‰∏≠ÁöÑÁ©∫Èó¥ÂíåÁâ©ÁêÜÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•Êï∞ÊçÆÈõÜÊèê‰æõ‰∫Ü2DËæπÁïå„ÄÅÊñΩÂä†ÁöÑÂäõÂíåÊîØÊíëÊù°‰ª∂ÔºåLLMÈúÄË¶ÅÊé®ÁêÜÂá∫ÊúÄ‰Ω≥ÁöÑÊùêÊñôÂàÜÂ∏É„ÄÇ‰ªªÂä°ÂåÖÊã¨Â°´Ë°•ÈÉ®ÂàÜÁªìÊûÑ‰∏≠ÁöÑÁº∫Â§±Âå∫ÂüüÂíåÈ¢ÑÊµãÂÆåÊï¥ÁöÑÊùêÊñôÂàÜÂ∏ÉÔºåË¶ÅÊ±ÇÊ®°ÂûãÁêÜËß£Âú®ÁªôÂÆöÁ∫¶Êùü‰∏ãÁöÑÂäõÊµÅÂíåÊùêÊñôÂàÜÂ∏É„ÄÇËøô‰∏™Êï∞ÊçÆÈõÜ‰∏∫ËØÑ‰º∞2DÁéØÂ¢É‰∏≠ÁöÑÁ©∫Èó¥ÂíåÁâ©ÁêÜÊé®ÁêÜËÉΩÂäõÊèê‰æõ‰∫ÜÊñ∞ÁöÑËßÜËßíÔºåË°•ÂÖÖ‰∫Ü‰º†ÁªüÁöÑËØ≠Ë®ÄÂíåÈÄªËæëÂü∫ÂáÜ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15263",
            "title": "gen2seg: Generative Models Enable Generalizable Instance Segmentation",
            "url": "https://huggingface.co/papers/2505.15263",
            "abstract": "Generative models fine-tuned for instance segmentation demonstrate strong zero-shot performance on unseen objects and styles, surpassing discriminatively pretrained models.  \t\t\t\t\tAI-generated summary \t\t\t\t By pretraining to synthesize coherent images from perturbed inputs, generative models inherently learn to understand object boundaries and scene compositions. How can we repurpose these generative representations for general-purpose perceptual organization? We finetune Stable Diffusion and MAE (encoder+decoder) for category-agnostic instance segmentation using our instance coloring loss exclusively on a narrow set of object types (indoor furnishings and cars). Surprisingly, our models exhibit strong zero-shot generalization, accurately segmenting objects of types and styles unseen in finetuning (and in many cases, MAE's ImageNet-1K pretraining too). Our best-performing models closely approach the heavily supervised SAM when evaluated on unseen object types and styles, and outperform it when segmenting fine structures and ambiguous boundaries. In contrast, existing promptable segmentation architectures or discriminatively pretrained models fail to generalize. This suggests that generative models learn an inherent grouping mechanism that transfers across categories and domains, even without internet-scale pretraining. Code, pretrained models, and demos are available on our website.",
            "score": 1,
            "issue_id": 3936,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 –º–∞—è",
                "en": "May 21",
                "zh": "5Êúà21Êó•"
            },
            "hash": "01de6ec5d8c92cb4",
            "authors": [
                "Om Khangaonkar",
                "Hamed Pirsiavash"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.15263.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#transfer_learning",
                    "#cv",
                    "#synthetic"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–ì–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç —Å–∫—Ä—ã—Ç—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤",
                    "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏, –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ —Ç–∏–ø–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Å–∏–ª—å–Ω—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø—Ä–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Å—Ç–∏–ª–µ–π. –¢–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –ø—Ä–∏–±–ª–∏–∂–∞—é—Ç—Å—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∫ –ø–æ–ª–Ω–æ—Å—Ç—å—é supervised –º–æ–¥–µ–ª—è–º –≤—Ä–æ–¥–µ SAM. –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –Ω–µ—è–≤–Ω–æ –∏–∑—É—á–∞—é—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –æ–±—ä–µ–∫—Ç–æ–≤, –ø—Ä–∏–º–µ–Ω–∏–º—ã–µ –∫ —Ä–∞–∑–Ω—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ –¥–æ–º–µ–Ω–∞–º."
                },
                "en": {
                    "title": "Generative Models Excel in Zero-Shot Instance Segmentation",
                    "desc": "This paper explores how generative models, specifically those fine-tuned for instance segmentation, can effectively identify and segment objects that were not seen during training. By leveraging a technique called instance coloring loss, the authors adapt models like Stable Diffusion and MAE to work with a limited set of object types, such as indoor furniture and cars. Remarkably, these models demonstrate strong zero-shot performance, meaning they can accurately segment new object types and styles that they have never encountered before. The findings suggest that generative models possess a robust understanding of object boundaries and scene composition, allowing them to generalize better than traditional discriminative models."
                },
                "zh": {
                    "title": "ÁîüÊàêÊ®°ÂûãÂú®ÂÆû‰æãÂàÜÂâ≤‰∏≠ÁöÑÂº∫Â§ßÊ≥õÂåñËÉΩÂäõ",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÁîüÊàêÊ®°ÂûãÂú®ÂÆû‰æãÂàÜÂâ≤‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®ÔºåÁâπÂà´ÊòØÂú®Êú™ËßÅÁâ©‰ΩìÂíåÈ£éÊ†º‰∏äÁöÑÈõ∂-shotÊÄßËÉΩË°®Áé∞„ÄÇÈÄöËøáÂØπÁîüÊàêÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºåÁ†îÁ©∂ËÄÖ‰ª¨ÂèëÁé∞Ëøô‰∫õÊ®°ÂûãËÉΩÂ§üÊúâÊïàÁêÜËß£Áâ©‰ΩìËæπÁïåÂíåÂú∫ÊôØÊûÑÊàê„ÄÇËÆ∫Êñá‰∏≠‰ΩøÁî®‰∫ÜÂÆû‰æãÁùÄËâ≤ÊçüÂ§±Ôºå‰∏ìÊ≥®‰∫éÂÆ§ÂÜÖÂÆ∂ÂÖ∑ÂíåÊ±ΩËΩ¶Á≠âÁâπÂÆöÁâ©‰ΩìÁ±ªÂûãÔºåÁªìÊûúÊòæÁ§∫Ê®°ÂûãÂú®Êú™ËßÅÁâ©‰Ωì‰∏ä‰πüËÉΩÂáÜÁ°ÆÂàÜÂâ≤„ÄÇ‰∏é‰º†ÁªüÁöÑÂà§Âà´ÂºèÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁõ∏ÊØîÔºåÁîüÊàêÊ®°ÂûãÂ±ïÁé∞Âá∫Êõ¥Âº∫ÁöÑË∑®Á±ªÂà´ÂíåÈ¢ÜÂüüÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13237",
            "title": "SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based\n  on Speech and Audio Information",
            "url": "https://huggingface.co/papers/2505.13237",
            "abstract": "SAKURA is introduced to evaluate the multi-hop reasoning abilities of large audio-language models, revealing their struggles in integrating speech/audio representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large audio-language models (LALMs) extend the large language models with multimodal understanding in speech, audio, etc. While their performances on speech and audio-processing tasks are extensively studied, their reasoning abilities remain underexplored. Particularly, their multi-hop reasoning, the ability to recall and integrate multiple facts, lacks systematic evaluation. Existing benchmarks focus on general speech and audio-processing tasks, conversational abilities, and fairness but overlook this aspect. To bridge this gap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning based on speech and audio information. Results show that LALMs struggle to integrate speech/audio representations for multi-hop reasoning, even when they extract the relevant information correctly, highlighting a fundamental challenge in multimodal reasoning. Our findings expose a critical limitation in LALMs, offering insights and resources for future research.",
            "score": 0,
            "issue_id": 3928,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 –º–∞—è",
                "en": "May 19",
                "zh": "5Êúà19Êó•"
            },
            "hash": "657c71fe76ca3155",
            "authors": [
                "Chih-Kai Yang",
                "Neo Ho",
                "Yen-Ting Piao",
                "Hung-yi Lee"
            ],
            "affiliations": [
                "National Taiwan University, Taiwan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13237.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "üéôÔ∏è",
                "ru": {
                    "title": "SAKURA: –≤—ã—è–≤–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ª–æ–≥–∏–∫–∏ –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "SAKURA - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LALM) –∫ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ LALM –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π —Ä–µ—á–µ–≤—ã—Ö –∏ –∞—É–¥–∏–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≤—ã–≤–æ–¥–æ–≤, –¥–∞–∂–µ –∫–æ–≥–¥–∞ –æ–Ω–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞—é—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –≠—Ç–æ –≤—ã—è–≤–∏–ª–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö —Ç–∞–∫–∏—Ö –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –≤–∞–∂–Ω—ã–µ insights –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–æ–∫ –≤ –æ–±–ª–∞—Å—Ç–∏ –∞—É–¥–∏–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "SAKURA: Unveiling the Reasoning Gaps in Audio-Language Models",
                    "desc": "The paper introduces SAKURA, a benchmark designed to evaluate the multi-hop reasoning capabilities of large audio-language models (LALMs). It highlights that while LALMs perform well in speech and audio tasks, their ability to integrate multiple pieces of information for reasoning is not well understood. The study reveals that LALMs face significant challenges in combining speech and audio representations for effective multi-hop reasoning. This research identifies a key limitation in LALMs and provides valuable insights for future advancements in multimodal reasoning."
                },
                "zh": {
                    "title": "SAKURAÔºöËØÑ‰º∞Èü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ§öË∑≥Êé®ÁêÜËÉΩÂäõ",
                    "desc": "SAKURAÊòØ‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Â§ßÂûãÈü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÂú®Â§öË∑≥Êé®ÁêÜÊñπÈù¢ÁöÑËÉΩÂäõ„ÄÇËøô‰∫õÊ®°ÂûãÂú®Â§ÑÁêÜËØ≠Èü≥ÂíåÈü≥È¢ë‰ªªÂä°Êó∂Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®Êï¥ÂêàÂ§öÊù°‰ø°ÊÅØËøõË°åÊé®ÁêÜÊó∂Âç¥Â≠òÂú®Âõ∞Èöæ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂç≥‰ΩøÂÆÉ‰ª¨ËÉΩÂ§üÊ≠£Á°ÆÊèêÂèñÁõ∏ÂÖ≥‰ø°ÊÅØÔºå‰ªçÁÑ∂Èöæ‰ª•Â∞ÜËØ≠Èü≥ÂíåÈü≥È¢ëË°®Á§∫ÁªìÂêàËµ∑Êù•ËøõË°åÂ§öË∑≥Êé®ÁêÜ„ÄÇËøô‰∏ÄÂèëÁé∞Êè≠Á§∫‰∫ÜÂ§öÊ®°ÊÄÅÊé®ÁêÜ‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÊåëÊàòÔºå‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂Êèê‰æõ‰∫ÜÊúâ‰ª∑ÂÄºÁöÑËßÅËß£ÂíåËµÑÊ∫ê„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-05-22.html",
    "link_next": "2025-05-26.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "22.05",
        "en": "05/22",
        "zh": "5Êúà22Êó•"
    },
    "short_date_next": {
        "ru": "26.05",
        "en": "05/26",
        "zh": "5Êúà26Êó•"
    },
    "categories": {
        "#dataset": 15,
        "#data": 5,
        "#benchmark": 24,
        "#agents": 4,
        "#cv": 9,
        "#rl": 12,
        "#rlhf": 5,
        "#rag": 2,
        "#plp": 0,
        "#inference": 3,
        "#3d": 3,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 23,
        "#math": 4,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 1,
        "#training": 27,
        "#robotics": 1,
        "#agi": 1,
        "#games": 8,
        "#interpretability": 5,
        "#reasoning": 25,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 23,
        "#survey": 1,
        "#diffusion": 7,
        "#alignment": 5,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 2,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 2
    },
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∫∫Â∑•Êô∫ËÉΩÔºàAIÔºâÂ¶Ç‰ΩïÂä†ÈÄüÁßëÁ†îËåÉÂºèÁöÑËΩ¨ÂèòÔºåÊèêÈ´òÁ†îÁ©∂ÊïàÁéáÂπ∂Êé®Âä®ÂàõÊñ∞„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫ÜNovelSeekÔºå‰∏Ä‰∏™Áªü‰∏ÄÁöÑÈó≠ÁéØÂ§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåÁî®‰∫éÂú®ÂêÑÁßçÁßëÁ†îÈ¢ÜÂüü‰∏≠ËøõË°åËá™‰∏ªÁßëÂ≠¶Á†îÁ©∂ÔºàASRÔºâ„ÄÇNovelSeekÊúâ‰∏âÂ§ß‰ºòÂäøÔºöÂèØÊâ©Â±ïÊÄß„ÄÅ‰∫íÂä®ÊÄßÂíåÈ´òÊïàÊÄß„ÄÇ‰æãÂ¶ÇÔºåÂú®ÂèçÂ∫î‰∫ßÁéáÈ¢ÑÊµã‰∏≠ÔºåÂÆÉÂú®12Â∞èÊó∂ÂÜÖÂ∞ÜÂáÜÁ°ÆÁéá‰ªé27.6%ÊèêÈ´òÂà∞35.4%ÔºõÂú®Â¢ûÂº∫Ê¥ªÊÄßÈ¢ÑÊµã‰∏≠ÔºåÂáÜÁ°ÆÁéáÂú®4Â∞èÊó∂ÂÜÖ‰ªé0.52ÊèêÈ´òÂà∞0.79ÔºõÂú®2DËØ≠‰πâÂàÜÂâ≤‰∏≠ÔºåÁ≤æÂ∫¶Âú®30Â∞èÊó∂ÂÜÖ‰ªé78.8%ÊèêÈ´òÂà∞81.0%„ÄÇ",
        "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop\n  System from Hypothesis to Verification",
        "pinyin": "Zh√® piƒÅn w√©nzhƒÅng ji√®sh√†o le r√©ng≈çng zh√¨n√©ng (AI) r√∫h√© jiƒÅs√π kƒìy√°n f√†nsh√¨ de zhu«énbi√†n, t√≠gƒÅo y√°nji≈´ xi√†ol«ú b√¨ng tuƒ´d√≤ng chu√†ngxƒ´n. Zu√≤zhƒõ t√≠ch≈´ le NovelSeek, yƒ´g√® t«íngyƒ´ de b√¨hu√°n du≈ç zh√¨n√©ngt«ê kuƒÅngji√†, y√≤ngy√∫ z√†i g√®zh«íng kƒìy√°n l«êngy√π zh≈çng j√¨nx√≠ng z√¨zh«î kƒìxu√© y√°nji≈´ (ASR). NovelSeek y«íu sƒÅn d√† y«íush√¨: kƒõ ku√≤zh√†nx√¨ng, h√πd√≤ngx√¨ng h√© gƒÅoxi√†ox√¨ng. L√¨r√∫, z√†i f«ény√¨ng ch«énl«ú y√πc√® zh≈çng, tƒÅ z√†i 12 xi«éosh√≠ n√®i jiƒÅng zh«înqu√®l«ú c√≥ng 27.6% t√≠gƒÅo d√†o 35.4%; z√†i zƒìngqi√°ng hu√≥x√¨ng y√πc√® zh≈çng, zh«înqu√®l«ú z√†i 4 xi«éosh√≠ n√®i c√≥ng 0.52 t√≠gƒÅo d√†o 0.79; z√†i 2D y«îy√¨ fƒìngƒì zh≈çng, jƒ´ngd√π z√†i 30 xi«éosh√≠ n√®i c√≥ng 78.8% t√≠gƒÅo d√†o 81.0%.",
        "vocab": "[{'word': '‰∫∫Â∑•Êô∫ËÉΩ', 'pinyin': 'r√©ng≈çng zh√¨n√©ng', 'trans': 'artificial intelligence'},\n{'word': 'Âä†ÈÄü', 'pinyin': 'jiƒÅs√π', 'trans': 'accelerate'},\n{'word': 'ÁßëÁ†î', 'pinyin': 'kƒìy√°n', 'trans': 'scientific research'},\n{'word': 'ËåÉÂºè', 'pinyin': 'f√†nsh√¨', 'trans': 'paradigm'},\n{'word': 'ËΩ¨Âèò', 'pinyin': 'zhu«énbi√†n', 'trans': 'transformation'},\n{'word': 'ÊèêÈ´ò', 'pinyin': 't√≠gƒÅo', 'trans': 'improve'},\n{'word': 'ÊïàÁéá', 'pinyin': 'xi√†ol«ú', 'trans': 'efficiency'},\n{'word': 'Êé®Âä®', 'pinyin': 'tuƒ´d√≤ng', 'trans': 'promote'},\n{'word': 'ÂàõÊñ∞', 'pinyin': 'chu√†ngxƒ´n', 'trans': 'innovation'},\n{'word': 'ÊèêÂá∫', 'pinyin': 't√≠ch≈´', 'trans': 'propose'},\n{'word': 'Áªü‰∏Ä', 'pinyin': 't«íngyƒ´', 'trans': 'unified'},\n{'word': 'Èó≠ÁéØ', 'pinyin': 'b√¨hu√°n', 'trans': 'closed-loop'},\n{'word': 'Â§öÊô∫ËÉΩ‰Ωì', 'pinyin': 'du≈ç zh√¨n√©ngt«ê', 'trans': 'multi-agent'},\n{'word': 'Ê°ÜÊû∂', 'pinyin': 'ku√†ngji√†', 'trans': 'framework'},\n{'word': 'Ëá™‰∏ª', 'pinyin': 'z√¨zh«î', 'trans': 'autonomous'},\n{'word': 'ÁßëÂ≠¶Á†îÁ©∂', 'pinyin': 'kƒìxu√© y√°nji≈´', 'trans': 'scientific research'},\n{'word': 'È¢ÜÂüü', 'pinyin': 'l«êngy√π', 'trans': 'field'},\n{'word': '‰ºòÂäø', 'pinyin': 'y≈çush√¨', 'trans': 'advantage'},\n{'word': 'ÂèØÊâ©Â±ïÊÄß', 'pinyin': 'kƒõ ku√≤zh√†n x√¨ng', 'trans': 'scalability'},\n{'word': '‰∫íÂä®ÊÄß', 'pinyin': 'h√πd√≤ng x√¨ng', 'trans': 'interactivity'},\n{'word': 'È´òÊïàÊÄß', 'pinyin': 'gƒÅoxi√†o x√¨ng', 'trans': 'efficiency'},\n{'word': '‰æãÂ¶Ç', 'pinyin': 'l√¨r√∫', 'trans': 'for example'},\n{'word': 'ÂèçÂ∫î', 'pinyin': 'f«ény√¨ng', 'trans': 'reaction'},\n{'word': '‰∫ßÁéá', 'pinyin': 'ch«énl«ú', 'trans': 'yield'},\n{'word': 'È¢ÑÊµã', 'pinyin': 'y√πc√®', 'trans': 'prediction'},\n{'word': 'ÂáÜÁ°ÆÁéá', 'pinyin': 'zh«înqu√®l«ú', 'trans': 'accuracy'},\n{'word': 'Â¢ûÂº∫', 'pinyin': 'zƒìngqi√°ng', 'trans': 'enhance'},\n{'word': 'Ê¥ªÊÄß', 'pinyin': 'hu√≥x√¨ng', 'trans': 'activity'},\n{'word': 'Á≤æÂ∫¶', 'pinyin': 'jƒ´ngd√π', 'trans': 'precision'},\n{'word': 'ËØ≠‰πâ', 'pinyin': 'y«îy√¨', 'trans': 'semantic'},\n{'word': 'ÂàÜÂâ≤', 'pinyin': 'fƒìngƒì', 'trans': 'segmentation'}]",
        "trans": "This article discusses how artificial intelligence (AI) is accelerating the transformation of scientific research paradigms, enhancing research efficiency, and driving innovation. The author introduces NovelSeek, a unified closed-loop multi-agent framework for autonomous scientific research (ASR) across various scientific domains. NovelSeek offers three key advantages: scalability, interactivity, and efficiency. For instance, in reaction yield prediction, it improved accuracy from 27.6% to 35.4% within 12 hours; in enhanced activity prediction, accuracy increased from 0.52 to 0.79 within 4 hours; and in 2D semantic segmentation, precision improved from 78.8% to 81.0% within 30 hours.",
        "update_ts": "2025-05-23 09:12"
    }
}