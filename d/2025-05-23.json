{
    "date": {
        "ru": "23 Ğ¼Ğ°Ñ",
        "en": "May 23",
        "zh": "5æœˆ23æ—¥"
    },
    "time_utc": "2025-05-24 01:53",
    "weekday": 4,
    "issue_id": 3936,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2505.16938",
            "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop\n  System from Hypothesis to Verification",
            "url": "https://huggingface.co/papers/2505.16938",
            "abstract": "Artificial Intelligence (AI) is accelerating the transformation of scientific research paradigms, not only enhancing research efficiency but also driving innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework to conduct Autonomous Scientific Research (ASR) across various scientific research fields, enabling researchers to tackle complicated problems in these fields with unprecedented speed and precision. NovelSeek highlights three key advantages: 1) Scalability: NovelSeek has demonstrated its versatility across 12 scientific research tasks, capable of generating innovative ideas to enhance the performance of baseline code. 2) Interactivity: NovelSeek provides an interface for human expert feedback and multi-agent interaction in automated end-to-end processes, allowing for the seamless integration of domain expert knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in several scientific fields with significantly less time cost compared to human efforts. For instance, in reaction yield prediction, it increased from 27.6% to 35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from 0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation, precision advanced from 78.8% to 81.0% in a mere 30 hours.",
            "score": 86,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "39a42fc40deae7a6",
            "authors": [
                "NovelSeek Team",
                "Bo Zhang",
                "Shiyang Feng",
                "Xiangchao Yan",
                "Jiakang Yuan",
                "Zhiyin Yu",
                "Xiaohan He",
                "Songtao Huang",
                "Shaowei Hou",
                "Zheng Nie",
                "Zhilong Wang",
                "Jinyao Liu",
                "Runmin Ma",
                "Tianshuo Peng",
                "Peng Ye",
                "Dongzhan Zhou",
                "Shufei Zhang",
                "Xiaosong Wang",
                "Yilan Zhang",
                "Meng Li",
                "Zhongying Tu",
                "Xiangyu Yue",
                "Wangli Ouyang",
                "Bowen Zhou",
                "Lei Bai"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16938.jpg",
            "data": {
                "categories": [
                    "#healthcare",
                    "#multimodal",
                    "#agents",
                    "#science"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "NovelSeek: Ğ˜Ğ˜-ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹",
                    "desc": "NovelSeek - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ½Ğ° Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ, Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² 12 Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. NovelSeek Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ. ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ° Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ€Ğ¾ÑĞ»Ğ° Ñ 27.6% Ğ´Ğ¾ 35.4% Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 12 Ñ‡Ğ°ÑĞ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹."
                },
                "en": {
                    "title": "Revolutionizing Research with Autonomous AI Frameworks",
                    "desc": "This paper presents NovelSeek, a multi-agent framework designed for Autonomous Scientific Research (ASR) that enhances the efficiency and innovation in scientific studies. It showcases three main benefits: scalability across various research tasks, interactivity with human experts for feedback, and improved efficiency in achieving results faster than traditional methods. NovelSeek has been tested on multiple scientific tasks, demonstrating significant performance improvements in areas like reaction yield prediction and semantic segmentation. By integrating AI with human expertise, NovelSeek aims to revolutionize how scientific research is conducted."
                },
                "zh": {
                    "title": "NovelSeekï¼šåŠ é€Ÿç§‘å­¦ç ”ç©¶çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºNovelSeekçš„ç»Ÿä¸€é—­ç¯å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°è‡ªä¸»ç§‘å­¦ç ”ç©¶ï¼ˆASRï¼‰ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªç§‘å­¦ç ”ç©¶é¢†åŸŸä¸­å±•ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§ã€äº¤äº’æ€§å’Œæ•ˆç‡ï¼Œèƒ½å¤Ÿä»¥ç©ºå‰çš„é€Ÿåº¦å’Œç²¾åº¦è§£å†³å¤æ‚é—®é¢˜ã€‚NovelSeeké€šè¿‡ä¸äººç±»ä¸“å®¶çš„åé¦ˆå’Œå¤šæ™ºèƒ½ä½“çš„äº’åŠ¨ï¼Œä¿ƒè¿›äº†é¢†åŸŸä¸“å®¶çŸ¥è¯†çš„æ— ç¼æ•´åˆã€‚ç ”ç©¶è¡¨æ˜ï¼ŒNovelSeekåœ¨ååº”äº§ç‡é¢„æµ‹ã€å¢å¼ºå­æ´»æ€§é¢„æµ‹å’Œ2Dè¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼ŒèŠ‚çœäº†å¤§é‡æ—¶é—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14810",
            "title": "Scaling Reasoning, Losing Control: Evaluating Instruction Following in\n  Large Reasoning Models",
            "url": "https://huggingface.co/papers/2505.14810",
            "abstract": "An empirical analysis of MathIF identifies a tension between enhancing reasoning capacity and maintaining instruction adherence in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-following is essential for aligning large language models (LLMs) with user intent. While recent reasoning-oriented models exhibit impressive performance on complex mathematical problems, their ability to adhere to natural language instructions remains underexplored. In this work, we introduce MathIF, a dedicated benchmark for evaluating instruction-following in mathematical reasoning tasks. Our empirical analysis reveals a consistent tension between scaling up reasoning capacity and maintaining controllability, as models that reason more effectively often struggle to comply with user directives. We find that models tuned on distilled long chains-of-thought or trained with reasoning-oriented reinforcement learning often degrade in instruction adherence, especially when generation length increases. Furthermore, we show that even simple interventions can partially recover obedience, though at the cost of reasoning performance. These findings highlight a fundamental tension in current LLM training paradigms and motivate the need for more instruction-aware reasoning models. We release the code and data at https://github.com/TingchenFu/MathIF.",
            "score": 49,
            "issue_id": 3914,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "97ed7c1fde734d7e",
            "authors": [
                "Tingchen Fu",
                "Jiawei Gu",
                "Yafu Li",
                "Xiaoye Qu",
                "Yu Cheng"
            ],
            "affiliations": [
                "Renmin University of China",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14810.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#training",
                    "#benchmark",
                    "#optimization",
                    "#alignment"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ¾Ğ¼ Ğ¸ Ğ¿Ğ¾ÑĞ»ÑƒÑˆĞ°Ğ½Ğ¸ĞµĞ¼ Ğ² Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ MathIF Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ğ°ÑÑ‚Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞŸÑ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ÑƒÑˆĞ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ¾ Ğ·Ğ° ÑÑ‡ĞµÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğµ Ğ² Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ğ¸ Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Balancing Reasoning and Instruction in Language Models",
                    "desc": "This paper presents MathIF, a benchmark designed to evaluate how well large language models (LLMs) follow instructions while solving mathematical problems. The authors find a conflict between improving reasoning abilities and maintaining adherence to user instructions, as models that excel in reasoning often fail to follow directives accurately. They observe that training methods like reinforcement learning can enhance reasoning but may reduce the model's ability to comply with instructions, especially as the complexity of tasks increases. The study suggests that addressing this tension is crucial for developing more effective instruction-aware reasoning models."
                },
                "zh": {
                    "title": "å¹³è¡¡æ¨ç†èƒ½åŠ›ä¸æŒ‡ä»¤éµå¾ªçš„æŒ‘æˆ˜",
                    "desc": "æœ¬ç ”ç©¶åˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ä¸æ¨ç†èƒ½åŠ›ä¹‹é—´çš„çŸ›ç›¾ã€‚æˆ‘ä»¬æå‡ºäº†MathIFåŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä¸­çš„æŒ‡ä»¤éµå¾ªè¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨ç†èƒ½åŠ›æ›´å¼ºçš„æ¨¡å‹å¾€å¾€åœ¨éµå¾ªç”¨æˆ·æŒ‡ä»¤æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆå†…å®¹è¾ƒé•¿æ—¶ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„è®­ç»ƒæ–¹æ³•éœ€è¦æ›´å¤šå…³æ³¨æŒ‡ä»¤æ„è¯†ï¼Œä»¥å¹³è¡¡æ¨ç†èƒ½åŠ›å’ŒæŒ‡ä»¤éµå¾ªã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16410",
            "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2505.16410",
            "abstract": "Tool-Star, an RL-based framework, enables LLMs to autonomously use multiple tools for stepwise reasoning, leveraging data synthesis and hierarchical reward design.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, large language models (LLMs) have shown remarkable reasoning capabilities via large-scale reinforcement learning (RL). However, leveraging the RL algorithm to empower effective multi-tool collaborative reasoning in LLMs remains an open challenge. In this paper, we introduce Tool-Star, an RL-based framework designed to empower LLMs to autonomously invoke multiple external tools during stepwise reasoning. Tool-Star integrates six types of tools and incorporates systematic designs in both data synthesis and training. To address the scarcity of tool-use data, we propose a general tool-integrated reasoning data synthesis pipeline, which combines tool-integrated prompting with hint-based sampling to automatically and scalably generate tool-use trajectories. A subsequent quality normalization and difficulty-aware classification process filters out low-quality samples and organizes the dataset from easy to hard. Furthermore, we propose a two-stage training framework to enhance multi-tool collaborative reasoning by: (1) cold-start fine-tuning, which guides LLMs to explore reasoning patterns via tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with hierarchical reward design, which reinforces reward understanding and promotes effective tool collaboration. Experimental analyses on over 10 challenging reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star. The code is available at https://github.com/dongguanting/Tool-Star.",
            "score": 43,
            "issue_id": 3914,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "acbe5c0b965cb7af",
            "authors": [
                "Guanting Dong",
                "Yifei Chen",
                "Xiaoxi Li",
                "Jiajie Jin",
                "Hongjin Qian",
                "Yutao Zhu",
                "Hangyu Mao",
                "Guorui Zhou",
                "Zhicheng Dou",
                "Ji-Rong Wen"
            ],
            "affiliations": [
                "BAAI",
                "Kuaishou Technology",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16410.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "Tool-Star: ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Tool-Star - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ÑˆĞµÑÑ‚ÑŒ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñ‹ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 10 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Tool-Star."
                },
                "en": {
                    "title": "Empowering LLMs with Multi-Tool Collaborative Reasoning",
                    "desc": "Tool-Star is a reinforcement learning (RL) framework that enhances large language models (LLMs) by enabling them to autonomously utilize multiple external tools for stepwise reasoning. It addresses the challenge of effective multi-tool collaboration by integrating a systematic approach to data synthesis and hierarchical reward design. The framework includes a novel data synthesis pipeline that generates tool-use trajectories and organizes them by difficulty, ensuring high-quality training data. Tool-Star's two-stage training process improves LLMs' reasoning capabilities through fine-tuning and a self-critic RL algorithm, leading to significant performance gains on various reasoning tasks."
                },
                "zh": {
                    "title": "Tool-Starï¼šèµ‹èƒ½LLMçš„å¤šå·¥å…·åä½œæ¨ç†",
                    "desc": "Tool-Staræ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿè‡ªä¸»ä½¿ç”¨å¤šä¸ªå·¥å…·è¿›è¡Œé€æ­¥æ¨ç†ã€‚è¯¥æ¡†æ¶æ•´åˆäº†å…­ç§å·¥å…·ï¼Œå¹¶åœ¨æ•°æ®åˆæˆå’Œè®­ç»ƒæ–¹é¢è¿›è¡Œäº†ç³»ç»Ÿè®¾è®¡ï¼Œä»¥è§£å†³å·¥å…·ä½¿ç”¨æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚é€šè¿‡ç»“åˆå·¥å…·é›†æˆæç¤ºå’ŒåŸºäºæç¤ºçš„é‡‡æ ·ï¼ŒTool-Starèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆå·¥å…·ä½¿ç”¨è½¨è¿¹ï¼Œå¹¶é€šè¿‡è´¨é‡æ ‡å‡†åŒ–å’Œéš¾åº¦æ„ŸçŸ¥åˆ†ç±»æ¥è¿‡æ»¤ä½è´¨é‡æ ·æœ¬ã€‚æœ€åï¼ŒTool-Staré‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œå¢å¼ºå¤šå·¥å…·åä½œæ¨ç†èƒ½åŠ›ï¼Œæå‡äº†æ¨¡å‹çš„æ¨ç†æ•ˆæœå’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15966",
            "title": "Pixel Reasoner: Incentivizing Pixel-Space Reasoning with\n  Curiosity-Driven Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.15966",
            "abstract": "Introducing pixel-space reasoning in Vision-Language Models (VLMs) through visual operations like zoom-in and select-frame enhances their performance on visual tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-thought reasoning has significantly improved the performance of Large Language Models (LLMs) across various domains. However, this reasoning process has been confined exclusively to textual space, limiting its effectiveness in visually intensive tasks. To address this limitation, we introduce the concept of reasoning in the pixel-space. Within this novel framework, Vision-Language Models (VLMs) are equipped with a suite of visual reasoning operations, such as zoom-in and select-frame. These operations enable VLMs to directly inspect, interrogate, and infer from visual evidences, thereby enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space reasoning capabilities in VLMs presents notable challenges, including the model's initially imbalanced competence and its reluctance to adopt the newly introduced pixel-space operations. We address these challenges through a two-phase training approach. The first phase employs instruction tuning on synthesized reasoning traces to familiarize the model with the novel visual operations. Following this, a reinforcement learning (RL) phase leverages a curiosity-driven reward scheme to balance exploration between pixel-space reasoning and textual reasoning. With these visual operations, VLMs can interact with complex visual inputs, such as information-rich images or videos to proactively gather necessary information. We demonstrate that this approach significantly improves VLM performance across diverse visual reasoning benchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on TallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy achieved by any open-source model to date. These results highlight the importance of pixel-space reasoning and the effectiveness of our framework.",
            "score": 37,
            "issue_id": 3915,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "a1134b1247c14719",
            "authors": [
                "Alex Su",
                "Haozhe Wang",
                "Weimin Ren",
                "Fangzhen Lin",
                "Wenhu Chen"
            ],
            "affiliations": [
                "HKUST",
                "USTC",
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15966.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#cv",
                    "#rl",
                    "#optimization",
                    "#benchmark",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑÑ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ĞºĞ°Ğ´Ñ€Ğ°, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³Ğ»Ğ¸ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ğ¼ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing VLMs with Pixel-Space Reasoning",
                    "desc": "This paper introduces a new way for Vision-Language Models (VLMs) to reason about images by using visual operations like zooming in and selecting frames. Traditionally, reasoning in these models has been limited to text, which makes it hard for them to handle visual tasks effectively. The authors propose a two-phase training method that first teaches the model to use these new visual operations and then encourages it to explore both visual and textual reasoning through reinforcement learning. Their approach leads to significant improvements in VLM performance on various visual reasoning tasks, achieving record accuracy on several benchmarks."
                },
                "zh": {
                    "title": "åƒç´ ç©ºé—´æ¨ç†æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„è¡¨ç°",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­å¼•å…¥åƒç´ ç©ºé—´æ¨ç†çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡è§†è§‰æ“ä½œå¦‚æ”¾å¤§å’Œé€‰æ‹©å¸§æ¥æå‡å…¶åœ¨è§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ç©ºé—´çš„æ¨ç†èƒ½åŠ›æ˜¾è‘—æé«˜ï¼Œä½†åœ¨è§†è§‰å¯†é›†ä»»åŠ¡ä¸­æ•ˆæœæœ‰é™ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒæ–¹æ³•è§£å†³äº†æ¨¡å‹åœ¨åƒç´ ç©ºé—´æ¨ç†ä¸­çš„æŒ‘æˆ˜ï¼Œé¦–å…ˆé€šè¿‡æŒ‡ä»¤è°ƒä¼˜è®©æ¨¡å‹ç†Ÿæ‚‰æ–°è§†è§‰æ“ä½œï¼Œç„¶ååˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¹³è¡¡åƒç´ ç©ºé—´å’Œæ–‡æœ¬ç©ºé—´çš„æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªè§†è§‰æ¨ç†åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå±•ç¤ºäº†åƒç´ ç©ºé—´æ¨ç†çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16707",
            "title": "KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models",
            "url": "https://huggingface.co/papers/2505.16707",
            "abstract": "Recent advances in multi-modal generative models have enabled significant progress in instruction-based image editing. However, while these models produce visually plausible outputs, their capacity for knowledge-based reasoning editing tasks remains under-explored. In this paper, we introduce KRIS-Bench (Knowledge-based Reasoning in Image-editing Systems Benchmark), a diagnostic benchmark designed to assess models through a cognitively informed lens. Drawing from educational theory, KRIS-Bench categorizes editing tasks across three foundational knowledge types: Factual, Conceptual, and Procedural. Based on this taxonomy, we design 22 representative tasks spanning 7 reasoning dimensions and release 1,267 high-quality annotated editing instances. To support fine-grained evaluation, we propose a comprehensive protocol that incorporates a novel Knowledge Plausibility metric, enhanced by knowledge hints and calibrated through human studies. Empirical results on 10 state-of-the-art models reveal significant gaps in reasoning performance, highlighting the need for knowledge-centric benchmarks to advance the development of intelligent image editing systems.",
            "score": 36,
            "issue_id": 3914,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "fa1902dc37796b16",
            "authors": [
                "Yongliang Wu",
                "Zonghui Li",
                "Xinting Hu",
                "Xinyu Ye",
                "Xianfang Zeng",
                "Gang Yu",
                "Wenbo Zhu",
                "Bernt Schiele",
                "Ming-Hsuan Yang",
                "Xu Yang"
            ],
            "affiliations": [
                "Max Planck Institute for Informatics",
                "Shanghai Jiao Tong University",
                "Southeast University",
                "StepFun",
                "University of California, Berkeley",
                "University of California, Merced"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16707.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "KRIS-Bench: ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ KRIS-Bench - Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹: Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼, ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ 22 Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ 7 Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ 1267 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Advancing Image Editing with Knowledge-Based Reasoning",
                    "desc": "This paper presents KRIS-Bench, a new benchmark for evaluating multi-modal generative models in the context of instruction-based image editing. It focuses on assessing the models' ability to perform knowledge-based reasoning tasks, which has not been thoroughly investigated before. The benchmark categorizes editing tasks into three knowledge types: Factual, Conceptual, and Procedural, and includes 22 tasks with 1,267 annotated instances. The study reveals that current state-of-the-art models struggle with reasoning tasks, indicating a need for more knowledge-centric evaluation methods in image editing systems."
                },
                "zh": {
                    "title": "çŸ¥è¯†é©±åŠ¨çš„å›¾åƒç¼–è¾‘è¯„ä¼°æ–°åŸºå‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†KRIS-Benchï¼ˆçŸ¥è¯†åŸºç¡€æ¨ç†åœ¨å›¾åƒç¼–è¾‘ç³»ç»ŸåŸºå‡†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹åœ¨çŸ¥è¯†æ¨ç†ç¼–è¾‘ä»»åŠ¡ä¸­çš„èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚KRIS-Benchæ ¹æ®æ•™è‚²ç†è®ºå°†ç¼–è¾‘ä»»åŠ¡åˆ†ä¸ºä¸‰ç§åŸºç¡€çŸ¥è¯†ç±»å‹ï¼šäº‹å®æ€§ã€æ¦‚å¿µæ€§å’Œç¨‹åºæ€§ï¼Œå¹¶è®¾è®¡äº†22ä¸ªä»£è¡¨æ€§ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»¼åˆè¯„ä¼°åè®®ï¼ŒåŒ…å«æ–°çš„çŸ¥è¯†åˆç†æ€§æŒ‡æ ‡ï¼Œå¹¶é€šè¿‡äººç±»ç ”ç©¶è¿›è¡Œæ ¡å‡†ã€‚å®è¯ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨æ¨ç†æ€§èƒ½ä¸Šå­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¼ºè°ƒäº†ä»¥çŸ¥è¯†ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•åœ¨æ™ºèƒ½å›¾åƒç¼–è¾‘ç³»ç»Ÿå‘å±•ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16175",
            "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design",
            "url": "https://huggingface.co/papers/2505.16175",
            "abstract": "QuickVideo accelerates long-video understanding by combining a parallelized video decoder, memory-efficient prefilling, and overlapping video decoding with inference, enabling real-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-video understanding has emerged as a crucial capability in real-world applications such as video surveillance, meeting summarization, educational lecture analysis, and sports broadcasting. However, it remains computationally prohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential video decoding, the process of converting the raw bit stream to RGB frames can take up to a minute for hour-long video inputs, and 2) costly prefilling of up to several million tokens for LLM inference, resulting in high latency and memory use. To address these challenges, we propose QuickVideo, a system-algorithm co-design that substantially accelerates long-video understanding to support real-time downstream applications. It comprises three key innovations: QuickDecoder, a parallelized CPU-based video decoder that achieves 2-3 times speedup by splitting videos into keyframe-aligned intervals processed concurrently; QuickPrefill, a memory-efficient prefilling method using KV-cache pruning to support more frames with less GPU memory; and an overlapping scheme that overlaps CPU video decoding with GPU inference. Together, these components infernece time reduce by a minute on long video inputs, enabling scalable, high-quality video understanding even on limited hardware. Experiments show that QuickVideo generalizes across durations and sampling rates, making long video processing feasible in practice.",
            "score": 30,
            "issue_id": 3914,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "0bcb5c833bad2340",
            "authors": [
                "Benjamin Schneider",
                "Dongfu Jiang",
                "Chao Du",
                "Tianyu Pang",
                "Wenhu Chen"
            ],
            "affiliations": [
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16175.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#video",
                    "#long_context"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ QuickVideo",
                    "desc": "QuickVideo - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑƒÑĞºĞ¾Ñ€ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼. QuickVideo Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ QuickDecoder Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° CPU, QuickPrefill Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ¸ ÑÑ…ĞµĞ¼Ñƒ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸Ñ CPU-Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ GPU-Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Accelerating Long-Video Understanding for Real-Time Applications",
                    "desc": "QuickVideo is a novel system designed to enhance the understanding of long videos in real-time applications. It addresses two major challenges: the slow sequential video decoding and the high memory requirements for token prefilling in large language models (LLMs). By introducing a parallelized video decoder, a memory-efficient prefilling method, and an overlapping decoding scheme, QuickVideo significantly reduces inference time. This allows for efficient processing of long videos, making advanced video analysis accessible even on limited hardware."
                },
                "zh": {
                    "title": "QuickVideoï¼šå®æ—¶é•¿è§†é¢‘ç†è§£çš„åŠ é€Ÿåˆ©å™¨",
                    "desc": "QuickVideo æ˜¯ä¸€ç§åŠ é€Ÿé•¿è§†é¢‘ç†è§£çš„ç³»ç»Ÿï¼Œç»“åˆäº†å¹¶è¡Œè§†é¢‘è§£ç ã€å†…å­˜é«˜æ•ˆçš„é¢„å¡«å……å’Œé‡å è§£ç ä¸æ¨ç†ã€‚å®ƒé€šè¿‡å¿«é€Ÿè§£ç å™¨å°†è§†é¢‘åˆ†å‰²æˆå…³é”®å¸§å¯¹é½çš„é—´éš”ï¼Œå¹¶åŒæ—¶å¤„ç†ï¼Œä»è€Œå®ç°äº† 2-3 å€çš„é€Ÿåº¦æå‡ã€‚QuickPrefill æ–¹æ³•é€šè¿‡ KV-cache å‰ªæå‡å°‘äº†å¯¹ GPU å†…å­˜çš„éœ€æ±‚ï¼Œä½¿å¾—å¯ä»¥å¤„ç†æ›´å¤šå¸§ã€‚è¯¥ç³»ç»Ÿæ˜¾è‘—é™ä½äº†é•¿è§†é¢‘è¾“å…¥çš„æ¨ç†æ—¶é—´ï¼Œä½¿å¾—åœ¨æœ‰é™ç¡¬ä»¶ä¸Šä¹Ÿèƒ½å®ç°é«˜è´¨é‡çš„è§†é¢‘ç†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17022",
            "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation\n  with Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.17022",
            "abstract": "GoT-R1 enhances visual generation by using reinforcement learning to improve semantic-spatial reasoning, outperforming existing models on complex compositional tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual generation models have made remarkable progress in creating realistic images from text prompts, yet struggle with complex prompts that specify multiple objects with precise spatial relationships and attributes. Effective handling of such prompts requires explicit reasoning about the semantic content and spatial layout. We present GoT-R1, a framework that applies reinforcement learning to enhance semantic-spatial reasoning in visual generation. Building upon the Generation Chain-of-Thought approach, GoT-R1 enables models to autonomously discover effective reasoning strategies beyond predefined templates through carefully designed reinforcement learning. To achieve this, we propose a dual-stage multi-dimensional reward framework that leverages MLLMs to evaluate both the reasoning process and final output, enabling effective supervision across the entire generation pipeline. The reward system assesses semantic alignment, spatial accuracy, and visual quality in a unified approach. Experimental results demonstrate significant improvements on T2I-CompBench benchmark, particularly in compositional tasks involving precise spatial relationships and attribute binding. GoT-R1 advances the state-of-the-art in image generation by successfully transferring sophisticated reasoning capabilities to the visual generation domain. To facilitate future research, we make our code and pretrained models publicly available at https://github.com/gogoduan/GoT-R1.",
            "score": 23,
            "issue_id": 3917,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "c3983abb24fa0ce4",
            "authors": [
                "Chengqi Duan",
                "Rongyao Fang",
                "Yuqing Wang",
                "Kun Wang",
                "Linjiang Huang",
                "Xingyu Zeng",
                "Hongsheng Li",
                "Xihui Liu"
            ],
            "affiliations": [
                "Beihang University",
                "CUHK MMLab",
                "HKU MMLab",
                "Sensetime"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17022.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#rl",
                    "#multimodal",
                    "#open_source",
                    "#cv",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "GoT-R1 - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ Generation Chain-of-Thought Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. GoT-R1 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Reinforcing Reasoning for Better Visual Generation",
                    "desc": "GoT-R1 is a new framework that improves visual generation by using reinforcement learning to enhance how models understand and create images based on complex text prompts. It focuses on semantic-spatial reasoning, which means it helps models better grasp the meaning of words and how objects should be arranged in space. The framework uses a dual-stage reward system to evaluate both the reasoning process and the final image quality, ensuring that the generated images are accurate and visually appealing. Experimental results show that GoT-R1 outperforms existing models, especially in tasks that require detailed understanding of object relationships and attributes."
                },
                "zh": {
                    "title": "GoT-R1ï¼šé€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰ç”Ÿæˆèƒ½åŠ›",
                    "desc": "GoT-R1 æ˜¯ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ å¢å¼ºè¯­ä¹‰-ç©ºé—´æ¨ç†çš„è§†è§‰ç”Ÿæˆæ¡†æ¶ã€‚å®ƒèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„æ–‡æœ¬æç¤ºï¼Œå°¤å…¶æ˜¯æ¶‰åŠå¤šä¸ªå¯¹è±¡åŠå…¶ç²¾ç¡®ç©ºé—´å…³ç³»çš„ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒé˜¶æ®µå¤šç»´å¥–åŠ±æœºåˆ¶ï¼Œåˆ©ç”¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯„ä¼°æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆè¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGoT-R1 åœ¨ T2I-CompBench åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»„åˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16933",
            "title": "LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning",
            "url": "https://huggingface.co/papers/2505.16933",
            "abstract": "A diffusion-based Multimodal Large Language Model (LLaDA-V) with integrated visual instruction tuning performs competitively on multimodal tasks and outperforms existing models in multimodal understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large Language Model (MLLM) that integrates visual instruction tuning with masked diffusion models, representing a departure from the autoregressive paradigms dominant in current multimodal approaches. Built upon LLaDA, a representative large language diffusion model, LLaDA-V incorporates a vision encoder and MLP connector that projects visual features into the language embedding space, enabling effective multimodal alignment. Our empirical investigation reveals several intriguing results: First, LLaDA-V demonstrates promising multimodal performance despite its language model being weaker on purely textual tasks than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal tasks with better data scalability. It also narrows the performance gap to Qwen2-VL, suggesting the effectiveness of its architecture for multimodal tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal understanding compared to existing hybrid autoregressive-diffusion and purely diffusion-based MLLMs. Our findings suggest that large language diffusion models show promise in multimodal contexts and warrant further investigation in future research. Project page and codes: https://ml-gsai.github.io/LLaDA-V-demo/.",
            "score": 22,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "9ae7746da2ef9a2b",
            "authors": [
                "Zebin You",
                "Shen Nie",
                "Xiaolu Zhang",
                "Jun Hu",
                "Jun Zhou",
                "Zhiwu Lu",
                "Ji-Rong Wen",
                "Chongxuan Li"
            ],
            "affiliations": [
                "Ant Group",
                "Beijing Key Laboratory of Research on Large Models and Intelligent Governance",
                "Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE",
                "Gaoling School of AI, Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16933.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LLaDA-V - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸. LLaDA-V Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ°Ğ±ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ‡Ğ¸ÑÑ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "LLaDA-V: Bridging Text and Vision with Diffusion Power!",
                    "desc": "LLaDA-V is a new type of Multimodal Large Language Model that uses a diffusion-based approach combined with visual instruction tuning. This model integrates visual features into the language processing space, allowing it to understand and generate responses that involve both text and images. Despite being less effective on purely text tasks compared to some existing models, LLaDA-V performs well in multimodal scenarios and shows strong scalability with data. The results indicate that diffusion models can be highly effective for tasks that require understanding multiple types of information, paving the way for future research in this area."
                },
                "zh": {
                    "title": "æ‰©æ•£æ¨¡å‹å¼•é¢†å¤šæ¨¡æ€ç†è§£æ–°æ½®æµ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹LLaDA-Vï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è§†è§‰æŒ‡ä»¤è°ƒä¼˜ï¼Œèƒ½å¤Ÿåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚LLaDA-Vé‡‡ç”¨äº†æ©è”½æ‰©æ•£æ¨¡å‹ï¼Œçªç ´äº†å½“å‰å¤šæ¨¡æ€æ–¹æ³•ä¸­ä¸»æµçš„è‡ªå›å½’èŒƒå¼ã€‚å°½ç®¡åœ¨çº¯æ–‡æœ¬ä»»åŠ¡ä¸Šè¡¨ç°ä¸å¦‚ä¸€äº›ç°æœ‰æ¨¡å‹ï¼Œä½†åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼ŒLLaDA-Vä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”å…·æœ‰æ›´å¥½çš„æ•°æ®å¯æ‰©å±•æ€§å’Œç«äº‰åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºæ‰©æ•£çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢å…·æœ‰å¾ˆå¤§çš„æ½œåŠ›ï¼Œå€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15270",
            "title": "Scaling Diffusion Transformers Efficiently via Î¼P",
            "url": "https://huggingface.co/papers/2505.15270",
            "abstract": "Maximal Update Parametrization (Î¼P) is extended to diffusion Transformers, demonstrating efficient hyperparameter transferability and reduced tuning costs across various models and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers have emerged as the foundation for vision generative models, but their scalability is limited by the high cost of hyperparameter (HP) tuning at large scales. Recently, Maximal Update Parametrization (muP) was proposed for vanilla Transformers, which enables stable HP transfer from small to large language models, and dramatically reduces tuning costs. However, it remains unclear whether muP of vanilla Transformers extends to diffusion Transformers, which differ architecturally and objectively. In this work, we generalize standard muP to diffusion Transformers and validate its effectiveness through large-scale experiments. First, we rigorously prove that muP of mainstream diffusion Transformers, including DiT, U-ViT, PixArt-alpha, and MMDiT, aligns with that of the vanilla Transformer, enabling the direct application of existing muP methodologies. Leveraging this result, we systematically demonstrate that DiT-muP enjoys robust HP transferability. Notably, DiT-XL-2-muP with transferred learning rate achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we validate the effectiveness of muP on text-to-image generation by scaling PixArt-alpha from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases, models under muP outperform their respective baselines while requiring small tuning cost, only 5.5% of one training run for PixArt-alpha and 3% of consumption by human experts for MMDiT-18B. These results establish muP as a principled and efficient framework for scaling diffusion Transformers.",
            "score": 21,
            "issue_id": 3914,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "6c13e0c5ef8ee4a2",
            "authors": [
                "Chenyu Zheng",
                "Xinyu Zhang",
                "Rongzhen Wang",
                "Wei Huang",
                "Zhi Tian",
                "Weilin Huang",
                "Jun Zhu",
                "Chongxuan Li"
            ],
            "affiliations": [
                "Beijing Key Laboratory of Research on Large Models and Intelligent Governance",
                "ByteDance Seed",
                "Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE",
                "Gaoling School of AI, Renmin University of China",
                "RIKEN AIP",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15270.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#transfer_learning",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Î¼P: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Maximal Update Parametrization (Î¼P) Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Î¼P Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº DiT, U-ViT, PixArt-alpha Ğ¸ MMDiT. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Î¼P Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¸Ñ… Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Î¼P ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Efficient Hyperparameter Transfer for Diffusion Transformers with Î¼P",
                    "desc": "This paper extends the Maximal Update Parametrization (Î¼P) technique to diffusion Transformers, which are crucial for generative vision models. The authors demonstrate that Î¼P allows for effective hyperparameter transfer from smaller to larger models, significantly reducing the costs associated with hyperparameter tuning. Through extensive experiments, they show that diffusion Transformers like DiT and PixArt-alpha benefit from Î¼P, achieving faster convergence and better performance with minimal tuning effort. Overall, this work establishes Î¼P as a valuable method for enhancing the scalability and efficiency of diffusion Transformers in various tasks."
                },
                "zh": {
                    "title": "æœ€å¤§æ›´æ–°å‚æ•°åŒ–ï¼šæ‰©æ•£å˜æ¢å™¨çš„é«˜æ•ˆè°ƒä¼˜æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æ‰©å±•äº†æœ€å¤§æ›´æ–°å‚æ•°åŒ–ï¼ˆÎ¼Pï¼‰åˆ°æ‰©æ•£å˜æ¢å™¨ï¼Œå±•ç¤ºäº†é«˜æ•ˆçš„è¶…å‚æ•°å¯è½¬ç§»æ€§å’Œé™ä½çš„è°ƒä¼˜æˆæœ¬ã€‚æ‰©æ•£å˜æ¢å™¨åœ¨è§†è§‰ç”Ÿæˆæ¨¡å‹ä¸­å‘æŒ¥äº†åŸºç¡€ä½œç”¨ï¼Œä½†åœ¨å¤§è§„æ¨¡åº”ç”¨ä¸­è¶…å‚æ•°è°ƒä¼˜çš„é«˜æˆæœ¬é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒÎ¼På¯ä»¥æœ‰æ•ˆåœ°ä»å°å‹æ¨¡å‹è½¬ç§»åˆ°å¤§å‹æ‰©æ•£å˜æ¢å™¨ï¼Œå¹¶åœ¨å¤§è§„æ¨¡å®éªŒä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚é€šè¿‡ç³»ç»Ÿæ€§å®éªŒï¼Œç»“æœæ˜¾ç¤ºåœ¨è°ƒä¼˜æˆæœ¬è¾ƒä½çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨Î¼Pçš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºåŸºçº¿æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16925",
            "title": "Risk-Averse Reinforcement Learning with Itakura-Saito Loss",
            "url": "https://huggingface.co/papers/2505.16925",
            "abstract": "Risk-averse reinforcement learning finds application in various high-stakes fields. Unlike classical reinforcement learning, which aims to maximize expected returns, risk-averse agents choose policies that minimize risk, occasionally sacrificing expected value. These preferences can be framed through utility theory. We focus on the specific case of the exponential utility function, where we can derive the Bellman equations and employ various reinforcement learning algorithms with few modifications. However, these methods suffer from numerical instability due to the need for exponent computation throughout the process. To address this, we introduce a numerically stable and mathematically sound loss function based on the Itakura-Saito divergence for learning state-value and action-value functions. We evaluate our proposed loss function against established alternatives, both theoretically and empirically. In the experimental section, we explore multiple financial scenarios, some with known analytical solutions, and show that our loss function outperforms the alternatives.",
            "score": 20,
            "issue_id": 3920,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "1233ea3eef980e02",
            "authors": [
                "Igor Udovichenko",
                "Olivier Croissant",
                "Anita Toleutaeva",
                "Evgeny Burnaev",
                "Alexander Korotin"
            ],
            "affiliations": [
                "Artificial Intelligence Research Institute",
                "Natixis Foundation",
                "Skolkovo Institute of Science and Technology",
                "Vega Institute Foundation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16925.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#rlhf",
                    "#math",
                    "#rl",
                    "#training",
                    "#games"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¸ÑĞºĞ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ€Ğ¸ÑĞº-Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğµ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ‘ĞµĞ»Ğ»Ğ¼Ğ°Ğ½Ğ°. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ˜Ñ‚Ğ°ĞºÑƒÑ€Ñ‹-Ğ¡Ğ°Ğ¸Ñ‚Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Minimizing Risk in Reinforcement Learning with Stability",
                    "desc": "This paper discusses risk-averse reinforcement learning, which is important in high-stakes situations where minimizing risk is crucial. Unlike traditional reinforcement learning that focuses on maximizing expected returns, risk-averse agents prioritize safer policies, sometimes at the cost of expected value. The authors specifically examine the exponential utility function to derive Bellman equations and adapt reinforcement learning algorithms accordingly. They propose a new loss function based on the Itakura-Saito divergence to improve numerical stability during training, demonstrating its effectiveness through theoretical analysis and empirical tests in financial scenarios."
                },
                "zh": {
                    "title": "é£é™©åŒæ¶å¼ºåŒ–å­¦ä¹ çš„æ–°æ–¹æ³•",
                    "desc": "é£é™©åŒæ¶å¼ºåŒ–å­¦ä¹ åœ¨é«˜é£é™©é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ä¸åŒï¼Œé£é™©åŒæ¶ä»£ç†é€‰æ‹©çš„ç­–ç•¥æ˜¯æœ€å°åŒ–é£é™©ï¼Œå¯èƒ½ä¼šç‰ºç‰²é¢„æœŸæ”¶ç›Šã€‚æˆ‘ä»¬ä¸“æ³¨äºæŒ‡æ•°æ•ˆç”¨å‡½æ•°çš„ç‰¹å®šæƒ…å†µï¼Œæ¨å¯¼å‡ºè´å°”æ›¼æ–¹ç¨‹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¯¹å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œå°‘é‡ä¿®æ”¹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ç”±äºéœ€è¦è¿›è¡ŒæŒ‡æ•°è¿ç®—è€Œå¯¼è‡´æ•°å€¼ä¸ç¨³å®šï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºItakura-Saitoæ•£åº¦çš„æ•°å€¼ç¨³å®šä¸”æ•°å­¦ä¸Šåˆç†çš„æŸå¤±å‡½æ•°ï¼Œç”¨äºå­¦ä¹ çŠ¶æ€å€¼å’ŒåŠ¨ä½œå€¼å‡½æ•°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16181",
            "title": "Understanding Generative AI Capabilities in Everyday Image Editing Tasks",
            "url": "https://huggingface.co/papers/2505.16181",
            "abstract": "Analysis of 83k image editing requests reveals that AI editors, including GPT-4o, struggle with low-creativity tasks and precise editing, while performing better on open-ended tasks, and human and VLM judges differ in their preferences for AI versus human edits.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative AI (GenAI) holds significant promise for automating everyday image editing tasks, especially following the recent release of GPT-4o on March 25, 2025. However, what subjects do people most often want edited? What kinds of editing actions do they want to perform (e.g., removing or stylizing the subject)? Do people prefer precise edits with predictable outcomes or highly creative ones? By understanding the characteristics of real-world requests and the corresponding edits made by freelance photo-editing wizards, can we draw lessons for improving AI-based editors and determine which types of requests can currently be handled successfully by AI editors? In this paper, we present a unique study addressing these questions by analyzing 83k requests from the past 12 years (2013-2025) on the Reddit community, which collected 305k PSR-wizard edits. According to human ratings, approximately only 33% of requests can be fulfilled by the best AI editors (including GPT-4o, Gemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on low-creativity requests that require precise editing than on more open-ended tasks. They often struggle to preserve the identity of people and animals, and frequently make non-requested touch-ups. On the other side of the table, VLM judges (e.g., o1) perform differently from human judges and may prefer AI edits more than human edits. Code and qualitative examples are available at: https://psrdataset.github.io",
            "score": 20,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "1d67f723fb5261aa",
            "authors": [
                "Mohammad Reza Taesiri",
                "Brandon Collins",
                "Logan Bolton",
                "Viet Dac Lai",
                "Franck Dernoncourt",
                "Trung Bui",
                "Anh Totti Nguyen"
            ],
            "affiliations": [
                "Adobe Research",
                "Auburn University",
                "University of Alberta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16181.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#games",
                    "#dataset",
                    "#cv",
                    "#optimization",
                    "#interpretability"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ˜Ğ˜ Ğ² Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸: ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ vs Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "ĞĞ½Ğ°Ğ»Ğ¸Ğ· 83 Ñ‚Ñ‹ÑÑÑ‡ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ˜Ğ˜-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4o, Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ½Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ 33% Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ñ‹ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ˜Ğ˜-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ»ÑĞ´ĞµĞ¹. Ğ˜Ğ˜ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ñ‹Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ½ĞµĞ·Ğ°Ğ¿Ñ€Ğ¾ÑˆĞµĞ½Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ñ‚Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "AI Editors: Better at Creativity, Struggling with Precision",
                    "desc": "This paper analyzes 83,000 image editing requests to evaluate the performance of AI editors like GPT-4o. It finds that AI struggles with low-creativity tasks that require precise edits, achieving success in only about 33% of requests. In contrast, AI performs better on open-ended editing tasks, but often fails to maintain the identity of subjects and makes unwanted changes. Additionally, the preferences of visual language model (VLM) judges differ from human judges, with VLM judges showing a greater inclination towards AI-generated edits."
                },
                "zh": {
                    "title": "AIç¼–è¾‘å™¨åœ¨åˆ›é€ æ€§ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ä¸æŒ‘æˆ˜",
                    "desc": "æœ¬ç ”ç©¶åˆ†æäº†83000ä¸ªå›¾åƒç¼–è¾‘è¯·æ±‚ï¼Œå‘ç°AIç¼–è¾‘å™¨ï¼ˆå¦‚GPT-4oï¼‰åœ¨ä½åˆ›é€ æ€§ä»»åŠ¡å’Œç²¾ç¡®ç¼–è¾‘æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä½†åœ¨å¼€æ”¾æ€§ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ã€‚äººç±»è¯„å®¡å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¯„å®¡åœ¨å¯¹AIä¸äººç±»ç¼–è¾‘çš„åå¥½ä¸Šå­˜åœ¨å·®å¼‚ã€‚å¤§çº¦åªæœ‰33%çš„è¯·æ±‚èƒ½å¤Ÿè¢«æœ€å¥½çš„AIç¼–è¾‘å™¨æ»¡è¶³ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç¡®ç¼–è¾‘çš„æƒ…å†µä¸‹ï¼ŒAIç¼–è¾‘å™¨å¸¸å¸¸æ— æ³•ä¿æŒäººç‰©å’ŒåŠ¨ç‰©çš„èº«ä»½ã€‚é€šè¿‡å¯¹è¿™äº›è¯·æ±‚çš„åˆ†æï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæ”¹è¿›AIç¼–è¾‘å™¨æä¾›é‡è¦çš„è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16400",
            "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2505.16400",
            "abstract": "Large-scale reinforcement learning enhances reasoning capabilities in small and mid-sized models more effectively than distillation, achieving superior results in both math and code benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress in large-scale reinforcement learning (RL) for reasoning, the training recipe for building high-performing reasoning models remains elusive. Key implementation details of frontier models, such as DeepSeek-R1, including data curation strategies and RL training recipe, are often omitted. Moreover, recent research indicates distillation remains more effective than RL for smaller models. In this work, we demonstrate that large-scale RL can significantly enhance the reasoning capabilities of strong, small- and mid-sized models, achieving results that surpass those of state-of-the-art distillation-based models. We systematically study the RL training process through extensive ablations and propose a simple yet effective approach: first training on math-only prompts, then on code-only prompts. Notably, we find that math-only RL not only significantly enhances the performance of strong distilled models on math benchmarks (e.g., +14.6% / +17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks (e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition, extended code-only RL iterations further improve performance on code benchmarks with minimal or no degradation in math results. We develop a robust data curation pipeline to collect challenging prompts with high-quality, verifiable answers and test cases to enable verification-based RL across both domains. Finally, we identify key experimental insights, including curriculum learning with progressively increasing response lengths and the stabilizing effect of on-policy parameter updates. We find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), but also pushes the limits of the model's reasoning ability, enabling it to solve problems that were previously unsolvable.",
            "score": 18,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "5a3a63b67bb5b62a",
            "authors": [
                "Yang Chen",
                "Zhuolin Yang",
                "Zihan Liu",
                "Chankyu Lee",
                "Peng Xu",
                "Mohammad Shoeybi",
                "Bryan Catanzaro",
                "Wei Ping"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16400.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#rl",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸ ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ curriculum learning Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ñ‹Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Boosting Reasoning with Large-Scale Reinforcement Learning",
                    "desc": "This paper explores how large-scale reinforcement learning (RL) can improve the reasoning abilities of small and mid-sized models more effectively than traditional distillation methods. The authors present a systematic study of the RL training process, highlighting a two-phase approach that first focuses on math prompts and then on code prompts. They demonstrate significant performance gains on both math and code benchmarks, showing that RL enhances the foundational reasoning capabilities of models while also enabling them to tackle previously unsolvable problems. Key insights include the importance of data curation and curriculum learning in optimizing the RL training process."
                },
                "zh": {
                    "title": "å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ æå‡æ¨ç†èƒ½åŠ›çš„æœ‰æ•ˆæ€§",
                    "desc": "æœ¬ç ”ç©¶è¡¨æ˜ï¼Œå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å°å‹å’Œä¸­å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢ï¼Œæ¯”è’¸é¦æ–¹æ³•æ›´ä¸ºæœ‰æ•ˆã€‚æˆ‘ä»¬é€šè¿‡ç³»ç»Ÿçš„å®éªŒç ”ç©¶ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ³•ï¼šå…ˆåœ¨æ•°å­¦æç¤ºä¸Šè®­ç»ƒï¼Œå†åœ¨ä»£ç æç¤ºä¸Šè®­ç»ƒã€‚ç»“æœæ˜¾ç¤ºï¼Œæ•°å­¦å¼ºåŒ–å­¦ä¹ æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•å’Œä»£ç æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªå¼ºå¤§çš„æ•°æ®æ•´ç†æµç¨‹ï¼Œä»¥æ”¶é›†å…·æœ‰æŒ‘æˆ˜æ€§çš„æç¤ºå’Œé«˜è´¨é‡çš„ç­”æ¡ˆï¼Œä»è€Œæ”¯æŒè·¨é¢†åŸŸçš„éªŒè¯åŸºç¡€å¼ºåŒ–å­¦ä¹ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14684",
            "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
            "url": "https://huggingface.co/papers/2505.14684",
            "abstract": "A model for detecting and generating missing intermediate steps in mathematical Chain-of-Thought reasoning improves performance and generalization on mathematical and logical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have achieved remarkable progress on mathematical tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits.",
            "score": 18,
            "issue_id": 3918,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "bcaa1385d7242117",
            "authors": [
                "Haolei Xu",
                "Yuchen Yan",
                "Yongliang Shen",
                "Wenqi Zhang",
                "Guiyang Hou",
                "Shengpei Jiang",
                "Kaitao Song",
                "Weiming Lu",
                "Jun Xiao",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "The Chinese University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14684.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#dataset",
                    "#reasoning",
                    "#optimization",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ¾ÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÑÑ‚ÑŒ Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (Chain-of-Thought, CoT). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ CoT Thought Leap Bridge Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ScaleQM+ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CoT-Bridge Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñ‹ Ğ¸ ÑĞ²ÑĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Bridging Thought Leaps for Enhanced Reasoning in AI",
                    "desc": "This paper introduces a model designed to detect and fill in missing steps in Chain-of-Thought (CoT) reasoning for mathematical tasks. The authors identify a problem where existing datasets have gaps, known as Thought Leaps, which hinder the learning process of large language models (LLMs). To address this, they propose the CoT Thought Leap Bridge Task and create a new training dataset called ScaleQM+ to help models learn to generate the missing reasoning steps. Their experiments show that models trained with this approach significantly outperform those trained on incomplete datasets, leading to better performance in both mathematical and logical reasoning tasks."
                },
                "zh": {
                    "title": "å¡«è¡¥æ€ç»´è·³è·ƒï¼Œæå‡æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ¨¡å‹ï¼Œç”¨äºæ£€æµ‹å’Œç”Ÿæˆæ•°å­¦æ¨ç†ä¸­çš„ç¼ºå¤±ä¸­é—´æ­¥éª¤ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç°æœ‰çš„æ•°å­¦é“¾å¼æ¨ç†æ•°æ®é›†å¸¸å¸¸å› ä¸ºä¸“å®¶çœç•¥ä¸­é—´æ­¥éª¤è€Œå¯¼è‡´æ€ç»´è·³è·ƒï¼Œè¿™å¯¹æ¨¡å‹çš„å­¦ä¹ å’Œæ³›åŒ–äº§ç”Ÿè´Ÿé¢å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†é“¾å¼æ¨ç†æ€ç»´è·³è·ƒæ¡¥æ¥ä»»åŠ¡ï¼Œæ—¨åœ¨è‡ªåŠ¨æ£€æµ‹æ€ç»´è·³è·ƒå¹¶ç”Ÿæˆç¼ºå¤±çš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä»¥æ¢å¤æ¨ç†çš„å®Œæ•´æ€§å’Œè¿è´¯æ€§ã€‚é€šè¿‡æ„å»ºä¸“é—¨çš„è®­ç»ƒæ•°æ®é›†ScaleQM+å¹¶è¿›è¡Œå®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨æ¡¥æ¥æ•°æ®é›†ä¸Šå¾®è°ƒçš„æ¨¡å‹åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºåŸå§‹æ•°æ®é›†ï¼Œä¸”åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šä¹Ÿæ˜¾ç¤ºå‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14604",
            "title": "Let LLMs Break Free from Overthinking via Self-Braking Tuning",
            "url": "https://huggingface.co/papers/2505.14604",
            "abstract": "A novel Self-Braking Tuning framework reduces overthinking and unnecessary computational overhead in large reasoning models by enabling the model to self-regulate its reasoning process.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have significantly enhanced their reasoning capabilities by generating longer chains of thought, demonstrating outstanding performance across a variety of tasks. However, this performance gain comes at the cost of a substantial increase in redundant reasoning during the generation process, leading to high computational overhead and exacerbating the issue of overthinking. Although numerous existing approaches aim to address the problem of overthinking, they often rely on external interventions. In this paper, we propose a novel framework, Self-Braking Tuning (SBT), which tackles overthinking from the perspective of allowing the model to regulate its own reasoning process, thus eliminating the reliance on external control mechanisms. We construct a set of overthinking identification metrics based on standard answers and design a systematic method to detect redundant reasoning. This method accurately identifies unnecessary steps within the reasoning trajectory and generates training signals for learning self-regulation behaviors. Building on this foundation, we develop a complete strategy for constructing data with adaptive reasoning lengths and introduce an innovative braking prompt mechanism that enables the model to naturally learn when to terminate reasoning at an appropriate point. Experiments across mathematical benchmarks (AIME, AMC, MATH500, GSM8K) demonstrate that our method reduces token consumption by up to 60% while maintaining comparable accuracy to unconstrained models.",
            "score": 18,
            "issue_id": 3917,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "20e4c9c407be15de",
            "authors": [
                "Haoran Zhao",
                "Yuchen Yan",
                "Yongliang Shen",
                "Haolei Xu",
                "Wenqi Zhang",
                "Kaitao Song",
                "Jian Shao",
                "Weiming Lu",
                "Jun Xiao",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Microsoft Research Asia",
                "Tianjin University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14604.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#optimization",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ˜Ğ˜: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ´ÑƒĞ¼Ğ¸Ğ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Self-Braking Tuning Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ¸Ğ¼ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ 'Ñ‚Ğ¾Ñ€Ğ¼Ğ¾Ğ·ÑÑ‰Ğ¸Ñ…' Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ¾ 60% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Empowering Models to Self-Regulate Reasoning for Efficiency",
                    "desc": "The paper introduces a Self-Braking Tuning (SBT) framework designed to enhance large reasoning models (LRMs) by allowing them to self-regulate their reasoning processes. This approach addresses the issue of overthinking, which leads to unnecessary computational overhead and redundant reasoning steps. By developing metrics to identify overthinking and creating a braking prompt mechanism, the model learns to determine when to stop reasoning effectively. Experimental results show that SBT can reduce token usage by up to 60% while preserving accuracy, making it a more efficient solution for reasoning tasks."
                },
                "zh": {
                    "title": "è‡ªæˆ‘è°ƒèŠ‚ï¼Œä¼˜åŒ–æ¨ç†æ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªæˆ‘åˆ¶åŠ¨è°ƒä¼˜æ¡†æ¶ï¼ˆSelf-Braking Tuning, SBTï¼‰ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„è¿‡åº¦æ€è€ƒå’Œä¸å¿…è¦çš„è®¡ç®—å¼€é”€ã€‚é€šè¿‡å…è®¸æ¨¡å‹è‡ªæˆ‘è°ƒèŠ‚æ¨ç†è¿‡ç¨‹ï¼ŒSBTæ¶ˆé™¤äº†å¯¹å¤–éƒ¨æ§åˆ¶æœºåˆ¶çš„ä¾èµ–ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€å¥—åŸºäºæ ‡å‡†ç­”æ¡ˆçš„è¿‡åº¦æ€è€ƒè¯†åˆ«æŒ‡æ ‡ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ç³»ç»Ÿçš„æ–¹æ³•æ¥æ£€æµ‹å†—ä½™æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒç›¸ä¼¼å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œèƒ½å¤Ÿå°†ä»¤ç‰Œæ¶ˆè€—å‡å°‘å¤šè¾¾60%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15952",
            "title": "VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game\n  Quality Assurance",
            "url": "https://huggingface.co/papers/2505.15952",
            "abstract": "A benchmark called VideoGameQA-Bench is introduced to assess Vision-Language Models in video game quality assurance tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t With video games now generating the highest revenues in the entertainment industry, optimizing game development workflows has become essential for the sector's sustained growth. Recent advancements in Vision-Language Models (VLMs) offer considerable potential to automate and enhance various aspects of game development, particularly Quality Assurance (QA), which remains one of the industry's most labor-intensive processes with limited automation options. To accurately evaluate the performance of VLMs in video game QA tasks and determine their effectiveness in handling real-world scenarios, there is a clear need for standardized benchmarks, as existing benchmarks are insufficient to address the specific requirements of this domain. To bridge this gap, we introduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array of game QA activities, including visual unit testing, visual regression testing, needle-in-a-haystack tasks, glitch detection, and bug report generation for both images and videos of various games. Code and data are available at: https://asgaardlab.github.io/videogameqa-bench/",
            "score": 17,
            "issue_id": 3915,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "f9307168ac97ad24",
            "authors": [
                "Mohammad Reza Taesiri",
                "Abhijay Ghildyal",
                "Saman Zadtootaghaj",
                "Nabajeet Barman",
                "Cor-Paul Bezemer"
            ],
            "affiliations": [
                "Sony Interactive Entertainment, Aliso Viejo, US",
                "Sony Interactive Entertainment, Berlin, Germany",
                "Sony Interactive Entertainment, London, UK",
                "University of Alberta, CA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15952.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#cv",
                    "#video",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "VideoGameQA-Bench: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VideoGameQA-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¹, Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ³Ğ» Ğ² ÑÑ‚Ğ¾Ğ³Ğµ ÑĞµĞ½Ğ°, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ³Ğ»Ğ¸Ñ‚Ñ‡ĞµĞ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ² Ğ¾Ğ± Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…. ĞĞ½ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ³Ñ€. VideoGameQA-Bench Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğº ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞµ Ğ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€."
                },
                "en": {
                    "title": "Revolutionizing Game QA with VideoGameQA-Bench",
                    "desc": "The paper presents VideoGameQA-Bench, a new benchmark designed to evaluate Vision-Language Models (VLMs) specifically for video game quality assurance tasks. It addresses the need for standardized assessments in a field where existing benchmarks do not meet the unique challenges of game development. By focusing on various QA activities such as visual unit testing and glitch detection, this benchmark aims to enhance the automation of quality assurance processes in the gaming industry. The introduction of VideoGameQA-Bench is a significant step towards improving the efficiency and effectiveness of game development workflows."
                },
                "zh": {
                    "title": "æå‡æ¸¸æˆå¼€å‘è´¨é‡çš„æ™ºèƒ½è¯„ä¼°å·¥å…·",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºVideoGameQA-Benchçš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘æ¸¸æˆè´¨é‡ä¿è¯ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚éšç€è§†é¢‘æ¸¸æˆåœ¨å¨±ä¹è¡Œä¸šä¸­äº§ç”Ÿçš„æ”¶å…¥æœ€é«˜ï¼Œä¼˜åŒ–æ¸¸æˆå¼€å‘æµç¨‹å˜å¾—è‡³å…³é‡è¦ã€‚è§†è§‰è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•ä¸ºè‡ªåŠ¨åŒ–å’Œæå‡æ¸¸æˆå¼€å‘çš„å„ä¸ªæ–¹é¢æä¾›äº†å·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨åŠ³åŠ¨å¯†é›†å‹çš„è´¨é‡ä¿è¯ç¯èŠ‚ã€‚ä¸ºäº†å‡†ç¡®è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œæ¶µç›–äº†å¤šç§æ¸¸æˆQAæ´»åŠ¨ï¼ŒåŒ…æ‹¬è§†è§‰å•å…ƒæµ‹è¯•ã€è§†è§‰å›å½’æµ‹è¯•ã€æ•…éšœæ£€æµ‹ç­‰ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16990",
            "title": "Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel\n  Decoding",
            "url": "https://huggingface.co/papers/2505.16990",
            "abstract": "Dimple, a Discrete Diffusion Multimodal Large Language Model, achieves performance comparable to autoregressive models through a hybrid training approach and enhances inference efficiency with confident decoding and structure priors.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we propose Dimple, the first Discrete Diffusion Multimodal Large Language Model (DMLLM). We observe that training with a purely discrete diffusion approach leads to significant training instability, suboptimal performance, and severe length bias issues. To address these challenges, we design a novel training paradigm that combines an initial autoregressive phase with a subsequent diffusion phase. This approach yields the Dimple-7B model, trained on the same dataset and using a similar training pipeline as LLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%, demonstrating that DMLLM can achieve performance comparable to that of autoregressive models. To improve inference efficiency, we propose a decoding strategy termed confident decoding, which dynamically adjusts the number of tokens generated at each step, significantly reducing the number of generation iterations. In autoregressive models, the number of forward iterations during generation equals the response length. With confident decoding, however, the number of iterations needed by Dimple is even only text{response length}{3}. We also re-implement the prefilling technique in autoregressive models and demonstrate that it does not significantly impact performance on most benchmark evaluations, while offering a speedup of 1.5x to 7x. Additionally, we explore Dimple's capability to precisely control its response using structure priors. These priors enable structured responses in a manner distinct from instruction-based or chain-of-thought prompting, and allow fine-grained control over response format and length, which is difficult to achieve in autoregressive models. Overall, this work validates the feasibility and advantages of DMLLM and enhances its inference efficiency and controllability. Code and models are available at https://github.com/yu-rp/Dimple.",
            "score": 14,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "5c0636fbc17936b5",
            "authors": [
                "Runpeng Yu",
                "Xinyin Ma",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16990.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#inference",
                    "#diffusion",
                    "#optimization",
                    "#open_source",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Dimple: Ğ”Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Dimple - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (DMLLM). ĞĞ½Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµÃ£Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Dimple: A New Era in Efficient Language Modeling",
                    "desc": "Dimple is a new type of language model called a Discrete Diffusion Multimodal Large Language Model (DMLLM) that combines two training methods to improve performance and stability. It starts with an autoregressive training phase, which helps to stabilize the learning process, followed by a diffusion phase that enhances the model's capabilities. Dimple-7B outperforms existing models like LLaVA-NEXT by 3.9%, showing that it can compete with traditional autoregressive models. Additionally, it introduces a confident decoding strategy that reduces the number of iterations needed for generating responses, making it faster and more efficient while allowing for better control over the output format and length."
                },
                "zh": {
                    "title": "Dimpleï¼šé«˜æ•ˆçš„ç¦»æ•£æ‰©æ•£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†Dimpleï¼Œä¸€ä¸ªç¦»æ•£æ‰©æ•£å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆDMLLMï¼‰ï¼Œé€šè¿‡æ··åˆè®­ç»ƒæ–¹æ³•å®ç°äº†ä¸è‡ªå›å½’æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°ï¼Œå•çº¯çš„ç¦»æ•£æ‰©æ•£è®­ç»ƒæ–¹æ³•ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€æ€§èƒ½ä¸ä½³å’Œä¸¥é‡çš„é•¿åº¦åå·®ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒèŒƒå¼ï¼Œç»“åˆäº†åˆå§‹çš„è‡ªå›å½’é˜¶æ®µå’Œåç»­çš„æ‰©æ•£é˜¶æ®µã€‚Dimpleæ¨¡å‹åœ¨æ¨ç†æ•ˆç‡ä¸Šä¹Ÿæœ‰æ‰€æå‡ï¼Œé‡‡ç”¨äº†åŠ¨æ€è°ƒæ•´ç”Ÿæˆä»¤ç‰Œæ•°é‡çš„è‡ªä¿¡è§£ç ç­–ç•¥ï¼Œæ˜¾è‘—å‡å°‘äº†ç”Ÿæˆè¿­ä»£æ¬¡æ•°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16916",
            "title": "Backdoor Cleaning without External Guidance in MLLM Fine-tuning",
            "url": "https://huggingface.co/papers/2505.16916",
            "abstract": "Multimodal Large Language Models (MLLMs) are increasingly deployed in fine-tuning-as-a-service (FTaaS) settings, where user-submitted datasets adapt general-purpose models to downstream tasks. This flexibility, however, introduces serious security risks, as malicious fine-tuning can implant backdoors into MLLMs with minimal effort. In this paper, we observe that backdoor triggers systematically disrupt cross-modal processing by causing abnormal attention concentration on non-semantic regions--a phenomenon we term attention collapse. Based on this insight, we propose Believe Your Eyes (BYE), a data filtering framework that leverages attention entropy patterns as self-supervised signals to identify and filter backdoor samples. BYE operates via a three-stage pipeline: (1) extracting attention maps using the fine-tuned model, (2) computing entropy scores and profiling sensitive layers via bimodal separation, and (3) performing unsupervised clustering to remove suspicious samples. Unlike prior defenses, BYE equires no clean supervision, auxiliary labels, or model modifications. Extensive experiments across various datasets, models, and diverse trigger types validate BYE's effectiveness: it achieves near-zero attack success rates while maintaining clean-task performance, offering a robust and generalizable solution against backdoor threats in MLLMs.",
            "score": 14,
            "issue_id": 3914,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "36dbc47d484f0a53",
            "authors": [
                "Xuankun Rong",
                "Wenke Huang",
                "Jian Liang",
                "Jinhe Bi",
                "Xun Xiao",
                "Yiming Li",
                "Bo Du",
                "Mang Ye"
            ],
            "affiliations": [
                "Huawei Technologies",
                "Munich Research Center",
                "Nanyang Technological University",
                "School of Computer Science, Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16916.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#data",
                    "#training",
                    "#dataset",
                    "#security"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜ Ğ¾Ñ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑƒĞ³Ñ€Ğ¾Ğ·: Ğ´Ğ¾Ğ²ĞµÑ€ÑĞ¹ ÑĞ²Ğ¾Ğ¸Ğ¼ Ğ³Ğ»Ğ°Ğ·Ğ°Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€Ñ‹ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ² ÑÑ‚Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ 'ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ'. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº BYE Ğ´Ğ»Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ backdoor-Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. BYE Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ‡Ğ¸ÑÑ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñƒ Ğ¾Ñ‚ backdoor-Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° MLLM."
                },
                "en": {
                    "title": "Defending MLLMs: Believe Your Eyes Against Backdoors!",
                    "desc": "This paper addresses the security risks associated with Multimodal Large Language Models (MLLMs) in fine-tuning-as-a-service (FTaaS) environments, where malicious fine-tuning can introduce backdoors. The authors identify a phenomenon called 'attention collapse', where backdoor triggers cause abnormal attention focus on irrelevant areas, disrupting cross-modal processing. To combat this, they propose a framework called Believe Your Eyes (BYE), which uses attention entropy patterns to filter out backdoor samples without needing clean supervision or model changes. BYE demonstrates strong effectiveness in various scenarios, achieving low attack success rates while preserving performance on clean tasks."
                },
                "zh": {
                    "title": "æŠµå¾¡åé—¨å¨èƒçš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¾®è°ƒæœåŠ¡ä¸­è¶Šæ¥è¶Šå¸¸è§ï¼Œä½†è¿™ä¹Ÿå¸¦æ¥äº†å®‰å…¨é£é™©ï¼Œæ¶æ„å¾®è°ƒå¯èƒ½ä¼šåœ¨æ¨¡å‹ä¸­æ¤å…¥åé—¨ã€‚æœ¬æ–‡è§‚å¯Ÿåˆ°ï¼Œåé—¨è§¦å‘å™¨ä¼šå¯¼è‡´è·¨æ¨¡æ€å¤„ç†çš„å¼‚å¸¸æ³¨æ„åŠ›é›†ä¸­ï¼Œå½¢æˆæˆ‘ä»¬ç§°ä¹‹ä¸ºæ³¨æ„åŠ›å´©æºƒçš„ç°è±¡ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†\"ç›¸ä¿¡ä½ çš„çœ¼ç›\"ï¼ˆBYEï¼‰æ•°æ®è¿‡æ»¤æ¡†æ¶ï¼Œé€šè¿‡æ³¨æ„åŠ›ç†µæ¨¡å¼ä½œä¸ºè‡ªç›‘ç£ä¿¡å·æ¥è¯†åˆ«å’Œè¿‡æ»¤åé—¨æ ·æœ¬ã€‚BYEé€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„æµç¨‹æ“ä½œï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦å¹²å‡€ç›‘ç£æˆ–æ¨¡å‹ä¿®æ”¹çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåœ°æŠµå¾¡MLLMsä¸­çš„åé—¨å¨èƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17018",
            "title": "SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward",
            "url": "https://huggingface.co/papers/2505.17018",
            "abstract": "An enhanced multimodal language model incorporates thinking process rewards to improve reasoning and generalization, achieving superior performance on benchmarks compared to larger models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances have shown success in eliciting strong reasoning abilities in multimodal large language models (MLLMs) through rule-based reinforcement learning (RL) with outcome rewards. However, this paradigm typically lacks supervision over the thinking process leading to the final outcome.As a result, the model may learn sub-optimal reasoning strategies, which can hinder its generalization ability. In light of this, we propose SophiaVL-R1, as an attempt to add reward signals for the thinking process in this paradigm. To achieve this, we first train a thinking reward model that evaluates the quality of the entire thinking process. Given that the thinking reward may be unreliable for certain samples due to reward hacking, we propose the Trust-GRPO method, which assigns a trustworthiness weight to the thinking reward during training. This weight is computed based on the thinking reward comparison of responses leading to correct answers versus incorrect answers, helping to mitigate the impact of potentially unreliable thinking rewards. Moreover, we design an annealing training strategy that gradually reduces the thinking reward over time, allowing the model to rely more on the accurate rule-based outcome reward in later training stages. Experiments show that our SophiaVL-R1 surpasses a series of reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU), demonstrating strong reasoning and generalization capabilities. Notably, our SophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite the latter having 10 times more parameters. All code, models, and datasets are made publicly available at https://github.com/kxfan2002/SophiaVL-R1.",
            "score": 12,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "a49e142829760753",
            "authors": [
                "Kaixuan Fan",
                "Kaituo Feng",
                "Haoming Lyu",
                "Dongzhan Zhou",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "MMLab, The Chinese University of Hong Kong",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17018.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#rlhf",
                    "#rl",
                    "#optimization",
                    "#benchmark",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SophiaVL-R1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Trust-GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ñ‚Ğ¶Ğ¸Ğ³Ğ°, Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‰Ğ°Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚. SophiaVL-R1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Empowering Reasoning with Thinking Process Rewards",
                    "desc": "This paper introduces SophiaVL-R1, a multimodal language model that enhances reasoning and generalization by incorporating rewards for the thinking process. Traditional reinforcement learning methods often overlook the quality of reasoning, leading to suboptimal strategies. To address this, the authors develop a thinking reward model that evaluates reasoning quality and implement a Trust-GRPO method to ensure reliable reward signals. Their experiments demonstrate that SophiaVL-R1 outperforms larger models on various benchmarks, showcasing its superior reasoning capabilities."
                },
                "zh": {
                    "title": "å¼•å…¥æ€ç»´å¥–åŠ±ï¼Œæå‡æ¨ç†èƒ½åŠ›çš„å¤šæ¨¡æ€æ¨¡å‹",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¢å¼ºçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹SophiaVL-R1ï¼Œæ—¨åœ¨é€šè¿‡å¼•å…¥æ€ç»´è¿‡ç¨‹å¥–åŠ±æ¥æ”¹å–„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä¼ ç»Ÿçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶ç¼ºä¹å¯¹æ€ç»´è¿‡ç¨‹çš„ç›‘ç£ï¼Œå¯èƒ½å¯¼è‡´å­¦ä¹ åˆ°æ¬¡ä¼˜çš„æ¨ç†ç­–ç•¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…ä»¬è®¾è®¡äº†ä¸€ä¸ªæ€ç»´å¥–åŠ±æ¨¡å‹ï¼Œå¹¶æå‡ºäº†Trust-GRPOæ–¹æ³•æ¥è¯„ä¼°æ€ç»´å¥–åŠ±çš„å¯ä¿¡åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSophiaVL-R1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºè®¸å¤šå¤§å‹æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16967",
            "title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard\n  Negatives for Robust Information Retrieval",
            "url": "https://huggingface.co/papers/2505.16967",
            "abstract": "Using cascading LLM prompts to identify and relabel false negatives in datasets improves retrieval and reranking models' performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Training robust retrieval and reranker models typically relies on large-scale retrieval datasets; for example, the BGE collection contains 1.6 million query-passage pairs sourced from various data sources. However, we find that certain datasets can negatively impact model effectiveness -- pruning 8 out of 15 datasets from the BGE collection reduces the training set size by 2.35times and increases nDCG@10 on BEIR by 1.0 point. This motivates a deeper examination of training data quality, with a particular focus on \"false negatives\", where relevant passages are incorrectly labeled as irrelevant. We propose a simple, cost-effective approach using cascading LLM prompts to identify and relabel hard negatives. Experimental results show that relabeling false negatives with true positives improves both E5 (base) and Qwen2.5-7B retrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot AIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on the relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the cascading design is further supported by human annotation results, where we find judgment by GPT-4o shows much higher agreement with humans than GPT-4o-mini.",
            "score": 12,
            "issue_id": 3931,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "d40b350aefa1642b",
            "authors": [
                "Nandan Thakur",
                "Crystina Zhang",
                "Xueguang Ma",
                "Jimmy Lin"
            ],
            "affiliations": [
                "David R. Cheriton School of Computer Science, University of Waterloo, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16967.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#training",
                    "#dataset"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ¸ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡Ğ°ÑÑ‚ Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ğ° (E5, Qwen2.5-7B), Ñ‚Ğ°Ğº Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° (Qwen2.5-3B) Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… BEIR Ğ¸ AIR-Bench. ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¸ĞµĞ¼ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹."
                },
                "en": {
                    "title": "Enhancing Model Performance by Relabeling False Negatives",
                    "desc": "This paper discusses improving the performance of retrieval and reranking models by addressing the issue of false negatives in training datasets. The authors demonstrate that by pruning ineffective datasets and focusing on relabeling incorrectly labeled relevant passages, they can enhance model effectiveness significantly. They introduce a method using cascading LLM prompts to identify and correct these false negatives, leading to measurable improvements in nDCG scores on benchmark evaluations. The results indicate that better training data quality directly correlates with enhanced model performance, showcasing the importance of accurate labeling in machine learning."
                },
                "zh": {
                    "title": "æå‡æ¨¡å‹æ€§èƒ½çš„å…³é”®ï¼šé‡æ–°æ ‡è®°é”™è¯¯è´Ÿæ ·æœ¬",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨çº§è”å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æç¤ºæ¥è¯†åˆ«å’Œé‡æ–°æ ‡è®°æ•°æ®é›†ä¸­é”™è¯¯æ ‡è®°çš„è´Ÿæ ·æœ¬ï¼Œä»è€Œæé«˜æ£€ç´¢å’Œé‡æ’åºæ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼ŒæŸäº›æ•°æ®é›†ä¼šå¯¹æ¨¡å‹æ•ˆæœäº§ç”Ÿè´Ÿé¢å½±å“ï¼Œå»é™¤ä¸å¿…è¦çš„æ•°æ®é›†å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•ä¸”ç»æµçš„æ–¹æ³•ï¼Œé€šè¿‡çº§è”æç¤ºæ¥è¯†åˆ«å’Œé‡æ–°æ ‡è®°è¿™äº›éš¾ä»¥è¯†åˆ«çš„è´Ÿæ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡æ–°æ ‡è®°åçš„æ•°æ®æ˜¾è‘—æå‡äº†æ£€ç´¢æ¨¡å‹çš„æ•ˆæœï¼ŒéªŒè¯äº†æ•°æ®è´¨é‡å¯¹æ¨¡å‹è®­ç»ƒçš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16864",
            "title": "Training-Free Efficient Video Generation via Dynamic Token Carving",
            "url": "https://huggingface.co/papers/2505.16864",
            "abstract": "Jenga, a novel inference pipeline for video Diffusion Transformer models, combines dynamic attention carving and progressive resolution generation to significantly speed up video generation while maintaining high quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the remarkable generation quality of video Diffusion Transformer (DiT) models, their practical deployment is severely hindered by extensive computational requirements. This inefficiency stems from two key challenges: the quadratic complexity of self-attention with respect to token length and the multi-step nature of diffusion models. To address these limitations, we present Jenga, a novel inference pipeline that combines dynamic attention carving with progressive resolution generation. Our approach leverages two key insights: (1) early denoising steps do not require high-resolution latents, and (2) later steps do not require dense attention. Jenga introduces a block-wise attention mechanism that dynamically selects relevant token interactions using 3D space-filling curves, alongside a progressive resolution strategy that gradually increases latent resolution during generation. Experimental results demonstrate that Jenga achieves substantial speedups across multiple state-of-the-art video diffusion models while maintaining comparable generation quality (8.83times speedup with 0.01\\% performance drop on VBench). As a plug-and-play solution, Jenga enables practical, high-quality video generation on modern hardware by reducing inference time from minutes to seconds -- without requiring model retraining. Code: https://github.com/dvlab-research/Jenga",
            "score": 12,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "ec67c352f5ae81d7",
            "authors": [
                "Yuechen Zhang",
                "Jinbo Xing",
                "Bin Xia",
                "Shaoteng Liu",
                "Bohao Peng",
                "Xin Tao",
                "Pengfei Wan",
                "Eric Lo",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "Kuaishou Technology",
                "SmartMore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16864.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#inference",
                    "#diffusion",
                    "#optimization"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Jenga - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾. Jenga Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 3D-ĞºÑ€Ğ¸Ğ²Ñ‹Ñ…, Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Jenga Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Jenga: Speeding Up Video Generation with Smart Attention!",
                    "desc": "The paper introduces Jenga, an innovative inference pipeline designed for video Diffusion Transformer models that enhances video generation speed while preserving quality. It tackles the computational inefficiencies caused by the quadratic complexity of self-attention and the multi-step nature of diffusion processes. Jenga employs dynamic attention carving to selectively focus on relevant token interactions and utilizes progressive resolution generation to optimize the use of high-resolution latents. Experimental results show that Jenga can significantly accelerate video generation, achieving up to 8.83 times faster performance with minimal quality loss, making it a practical solution for real-time applications."
                },
                "zh": {
                    "title": "Jengaï¼šåŠ é€Ÿè§†é¢‘ç”Ÿæˆçš„åˆ›æ–°æ¨ç†ç®¡é“",
                    "desc": "Jengaæ˜¯ä¸€ç§æ–°é¢–çš„è§†é¢‘æ‰©æ•£å˜æ¢å™¨æ¨¡å‹æ¨ç†ç®¡é“ï¼Œç»“åˆäº†åŠ¨æ€æ³¨æ„åŠ›åˆ‡å‰²å’Œæ¸è¿›åˆ†è¾¨ç‡ç”Ÿæˆï¼Œæ˜¾è‘—åŠ å¿«äº†è§†é¢‘ç”Ÿæˆé€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡ã€‚è¯¥æ–¹æ³•è§£å†³äº†è‡ªæ³¨æ„åŠ›çš„å¹³æ–¹å¤æ‚åº¦å’Œæ‰©æ•£æ¨¡å‹çš„å¤šæ­¥éª¤ç‰¹æ€§å¸¦æ¥çš„è®¡ç®—æ•ˆç‡é—®é¢˜ã€‚Jengaé€šè¿‡åŠ¨æ€é€‰æ‹©ç›¸å…³çš„æ ‡è®°äº¤äº’å’Œé€æ­¥æé«˜æ½œåœ¨åˆ†è¾¨ç‡ï¼Œä¼˜åŒ–äº†ç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒJengaåœ¨å¤šä¸ªæœ€å…ˆè¿›çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿï¼ŒåŒæ—¶ç”Ÿæˆè´¨é‡ä¿æŒç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17012",
            "title": "SpatialScore: Towards Unified Evaluation for Multimodal Spatial\n  Understanding",
            "url": "https://huggingface.co/papers/2505.17012",
            "abstract": "SpatialScore benchmarks multimodal large language models for 3D spatial understanding, revealing challenges and showcasing the effectiveness of SpatialAgent with specialized tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have achieved impressive success in question-answering tasks, yet their capabilities for spatial understanding are less explored. This work investigates a critical question: do existing MLLMs possess 3D spatial perception and understanding abilities? Concretely, we make the following contributions in this paper: (i) we introduce VGBench, a benchmark specifically designed to assess MLLMs for visual geometry perception, e.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most comprehensive and diverse multimodal spatial understanding benchmark to date, integrating VGBench with relevant data from the other 11 existing datasets. This benchmark comprises 28K samples across various spatial understanding tasks, modalities, and QA formats, along with a carefully curated challenging subset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent system incorporating 9 specialized tools for spatial understanding, supporting both Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive evaluations to reveal persistent challenges in spatial reasoning while demonstrating the effectiveness of SpatialAgent. We believe SpatialScore will offer valuable insights and serve as a rigorous benchmark for the next evolution of MLLMs.",
            "score": 10,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "51519403c5b92e25",
            "authors": [
                "Haoning Wu",
                "Xiao Huang",
                "Yaohui Chen",
                "Ya Zhang",
                "Yanfeng Wang",
                "Weidi Xie"
            ],
            "affiliations": [
                "School of Artificial Intelligence, Shanghai Jiao Tong University",
                "Shanghai AI Laboratory",
                "Tianjin University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17012.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#survey",
                    "#3d",
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "SpatialScore: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SpatialScore - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ VGBench Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ SpatialAgent - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸ĞµÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… MLLM, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ SpatialAgent. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SpatialScore Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ°Ñ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ MLLM Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing 3D Spatial Understanding in Multimodal Models",
                    "desc": "This paper introduces SpatialScore, a benchmark designed to evaluate multimodal large language models (MLLMs) in their ability to understand 3D spatial concepts. It highlights the development of VGBench, which focuses on visual geometry perception tasks like camera pose estimation. The benchmark includes 28,000 samples from various spatial understanding tasks and features a challenging subset called SpatialScore-Hard. Additionally, the paper presents SpatialAgent, a multi-agent system equipped with specialized tools to enhance spatial reasoning capabilities in MLLMs."
                },
                "zh": {
                    "title": "ç©ºé—´ç†è§£çš„æ–°åŸºå‡†ä¸å·¥å…·",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸‰ç»´ç©ºé—´ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†VGBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°è§†è§‰å‡ ä½•æ„ŸçŸ¥çš„åŸºå‡†ï¼Œæ¶µç›–ç›¸æœºå§¿æ€å’Œè¿åŠ¨ä¼°è®¡ç­‰ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†SpatialScoreï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å…¨é¢çš„å¤šæ¨¡æ€ç©ºé—´ç†è§£åŸºå‡†ï¼Œæ•´åˆäº†æ¥è‡ª11ä¸ªç°æœ‰æ•°æ®é›†çš„æ•°æ®ï¼ŒåŒ…å«28Kä¸ªæ ·æœ¬å’Œä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å­é›†SpatialScore-Hardã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†SpatialAgentï¼Œä¸€ä¸ªæ–°é¢–çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç»“åˆäº†9ä¸ªä¸“é—¨çš„å·¥å…·ï¼Œä»¥æ”¯æŒç©ºé—´ç†è§£çš„æ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16839",
            "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
            "url": "https://huggingface.co/papers/2505.16839",
            "abstract": "LaViDa, a family of vision-language models built on discrete diffusion models, offers competitive performance on multimodal benchmarks with advantages in speed, controllability, and bidirectional reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Vision-Language Models (VLMs) can solve a wide range of tasks requiring visual reasoning. In real-world scenarios, desirable properties for VLMs include fast inference and controllable generation (e.g., constraining outputs to adhere to a desired format). However, existing autoregressive (AR) VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs) offer a promising alternative, enabling parallel decoding for faster inference and bidirectional context for controllable generation through text-infilling. While effective in language-only settings, DMs' potential for multimodal tasks is underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build LaViDa by equipping DMs with a vision encoder and jointly fine-tune the combined parts for multimodal instruction following. To address challenges encountered, LaViDa incorporates novel techniques such as complementary masking for effective training, prefix KV cache for efficient inference, and timestep shifting for high-quality sampling. Experiments show that LaViDa achieves competitive or superior performance to AR VLMs on multi-modal benchmarks such as MMMU, while offering unique advantages of DMs, including flexible speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO captioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x speedup. On bidirectional tasks, it achieves +59% improvement on Constrained Poem Completion. These results demonstrate LaViDa as a strong alternative to AR VLMs. Code and models will be released in the camera-ready version.",
            "score": 10,
            "issue_id": 3914,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "9e2202389256fc59",
            "authors": [
                "Shufan Li",
                "Konstantinos Kallidromitis",
                "Hritik Bansal",
                "Akash Gokul",
                "Yusuke Kato",
                "Kazuki Kozuka",
                "Jason Kuen",
                "Zhe Lin",
                "Kai-Wei Chang",
                "Aditya Grover"
            ],
            "affiliations": [
                "Adobe Research",
                "Panasonic AI Research",
                "Salesforce Research",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16839.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#architecture",
                    "#training",
                    "#games",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LaViDa: Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸",
                    "desc": "LaViDa - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ² ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, LaViDa Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LaViDa Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "LaViDa: Fast and Controllable Vision-Language Models",
                    "desc": "LaViDa is a new family of vision-language models that utilize discrete diffusion models to enhance performance on multimodal tasks. It addresses the limitations of traditional autoregressive models by providing faster inference and better control over output generation. By integrating a vision encoder and employing techniques like complementary masking and prefix KV cache, LaViDa achieves high-quality results while maintaining efficiency. Experimental results show that LaViDa outperforms existing models in various benchmarks, demonstrating its potential as a robust alternative in the field of vision-language processing."
                },
                "zh": {
                    "title": "LaViDaï¼šé«˜æ•ˆå¯æ§çš„è§†è§‰-è¯­è¨€æ¨¡å‹",
                    "desc": "LaViDaæ˜¯ä¸€ç§åŸºäºç¦»æ•£æ‰©æ•£æ¨¡å‹çš„è§†è§‰-è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œèƒ½å¤Ÿåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚ä¸ç°æœ‰çš„è‡ªå›å½’è§†è§‰-è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒLaViDaåœ¨æ¨ç†é€Ÿåº¦ã€å¯æ§æ€§å’ŒåŒå‘æ¨ç†æ–¹é¢å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»“åˆè§†è§‰ç¼–ç å™¨å’Œè”åˆå¾®è°ƒæŠ€æœ¯ï¼Œæå‡äº†å¤šæ¨¡æ€ä»»åŠ¡çš„å¤„ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaViDaåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºè‡ªå›å½’æ¨¡å‹å¼ºæœ‰åŠ›æ›¿ä»£å“çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14625",
            "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning",
            "url": "https://huggingface.co/papers/2505.14625",
            "abstract": "Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RL's success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problem--false negatives--where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at https://github.com/uw-nsl/TinyV.",
            "score": 10,
            "issue_id": 3914,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "0bcb0b140b7623c3",
            "authors": [
                "Zhangchen Xu",
                "Yuetai Li",
                "Fengqing Jiang",
                "Bhaskar Ramasubramanian",
                "Luyao Niu",
                "Bill Yuchen Lin",
                "Radha Poovendran"
            ],
            "affiliations": [
                "University of Washington",
                "Western Washington University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14625.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#reasoning",
                    "#training",
                    "#dataset",
                    "#rl",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ 38% Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½ĞµĞ²ĞµÑ€Ğ½Ğ¾ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ÑÑÑ‚ÑÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ tinyV Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ tinyV Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 10% Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RL Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing RL Training by Tackling Verifier False Negatives with TinyV",
                    "desc": "This paper discusses the challenges of using Reinforcement Learning (RL) to improve large language models (LLMs) due to the issue of false negatives from verifiers. False negatives occur when verifiers incorrectly reject correct outputs from the model, which can hinder the RL training process by limiting the feedback the model receives. The authors analyze a dataset and find that a significant portion of model responses are misclassified, leading to slower learning and convergence. To address this, they introduce TinyV, a new verifier that enhances existing methods by identifying and correcting these false negatives, resulting in improved performance on math-reasoning tasks."
                },
                "zh": {
                    "title": "è§£å†³å‡é˜´æ€§ï¼Œæå‡å¼ºåŒ–å­¦ä¹ æ•ˆæœï¼",
                    "desc": "å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯ä¸€ç§é€šè¿‡å¥–åŠ±ä¿¡å·ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç­–ç•¥çš„å¼ºå¤§å·¥å…·ã€‚ç„¶è€Œï¼ŒRLçš„æˆåŠŸä¾èµ–äºéªŒè¯è€…æä¾›çš„å¯é å¥–åŠ±ï¼Œè€Œæˆ‘ä»¬å‘ç°ä¸€ä¸ªæ™®éå­˜åœ¨çš„é—®é¢˜â€”â€”å‡é˜´æ€§ï¼Œå³éªŒè¯è€…é”™è¯¯åœ°æ‹’ç»æ­£ç¡®çš„æ¨¡å‹è¾“å‡ºã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè¶…è¿‡38%çš„æ¨¡å‹ç”Ÿæˆçš„å“åº”å—åˆ°å‡é˜´æ€§çš„å½±å“ï¼Œè¿™ä¸¥é‡æŸå®³äº†RLè®­ç»ƒçš„æ•ˆæœã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†tinyVï¼Œä¸€ä¸ªè½»é‡çº§çš„LLMåŸºç¡€éªŒè¯å™¨ï¼Œå¯ä»¥åŠ¨æ€è¯†åˆ«æ½œåœ¨çš„å‡é˜´æ€§ï¼Œä»è€Œæé«˜å¥–åŠ±ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16854",
            "title": "Think or Not? Selective Reasoning via Reinforcement Learning for\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2505.16854",
            "abstract": "TON, a two-stage training strategy combining supervised fine-tuning with thought dropout and Group Relative Policy Optimization, reduces unnecessary reasoning steps in vision-language models without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in vision-language models (VLMs). Group Relative Policy Optimization (GRPO) is a recent prominent method that encourages models to generate complete reasoning traces before answering, leading to increased token usage and computational cost. Inspired by the human-like thinking process-where people skip reasoning for easy questions but think carefully when needed-we explore how to enable VLMs to first decide when reasoning is necessary. To realize this, we propose TON, a two-stage training strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective 'thought dropout' operation, where reasoning traces are randomly replaced with empty thoughts. This introduces a think-or-not format that serves as a cold start for selective reasoning; (ii) a GRPO stage that enables the model to freely explore when to think or not, while maximizing task-aware outcome rewards. Experimental results show that TON can reduce the completion length by up to 90% compared to vanilla GRPO, without sacrificing performance or even improving it. Further evaluations across diverse vision-language tasks-covering a range of reasoning difficulties under both 3B and 7B models-consistently reveal that the model progressively learns to bypass unnecessary reasoning steps as training advances. These findings shed light on the path toward human-like reasoning patterns in reinforcement learning approaches. Our code is available at https://github.com/kokolerk/TON.",
            "score": 7,
            "issue_id": 3915,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "af930383188983e1",
            "authors": [
                "Jiaqi Wang",
                "Kevin Qinghong Lin",
                "James Cheng",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16854.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ TON Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. TON ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ 'Ğ¾Ñ‚ÑĞµĞ²Ğ° Ğ¼Ñ‹ÑĞ»ĞµĞ¹' Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (GRPO). Ğ­Ñ‚Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ‚ÑŒ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° 90% Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ÑÑÑŒ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "TON: Streamlining Reasoning in Vision-Language Models",
                    "desc": "The paper introduces TON, a two-stage training strategy designed to enhance vision-language models (VLMs) by reducing unnecessary reasoning steps. It combines supervised fine-tuning with a technique called 'thought dropout' to help models decide when reasoning is necessary, mimicking human-like thinking processes. The second stage employs Group Relative Policy Optimization (GRPO) to optimize the model's reasoning efficiency while maximizing task performance. Experimental results demonstrate that TON can significantly decrease reasoning length without compromising, and often improving, the model's performance across various tasks."
                },
                "zh": {
                    "title": "TONï¼šä¼˜åŒ–è§†è§‰-è¯­è¨€æ¨¡å‹æ¨ç†çš„åˆ›æ–°ç­–ç•¥",
                    "desc": "TONæ˜¯ä¸€ç§ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥ï¼Œç»“åˆäº†ç›‘ç£å¾®è°ƒå’Œæ€ç»´ä¸¢å¼ƒï¼Œæ—¨åœ¨å‡å°‘è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„ä¸å¿…è¦æ¨ç†æ­¥éª¤ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡ç®€å•æœ‰æ•ˆçš„â€œæ€ç»´ä¸¢å¼ƒâ€æ“ä½œï¼Œéšæœºç”¨ç©ºæ€ç»´æ›¿æ¢æ¨ç†è½¨è¿¹ï¼Œå¼•å…¥äº†é€‰æ‹©æ€§æ¨ç†çš„æ€è€ƒæ ¼å¼ã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªç”±å†³å®šä½•æ—¶è¿›è¡Œæ¨ç†ï¼Œä»è€Œæœ€å¤§åŒ–ä»»åŠ¡ç›¸å…³çš„ç»“æœå¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTONå¯ä»¥å°†å®Œæˆé•¿åº¦å‡å°‘å¤šè¾¾90%ï¼Œè€Œä¸ç‰ºç‰²æ€§èƒ½ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹æé«˜äº†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16421",
            "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2505.16421",
            "abstract": "WebAgent-R1 is an RL framework for training web agents in multi-turn interactions, achieving high success rates compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents.",
            "score": 7,
            "issue_id": 3928,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "9b6b6b9f01bfb9c0",
            "authors": [
                "Zhepei Wei",
                "Wenlin Yao",
                "Yao Liu",
                "Weizhi Zhang",
                "Qin Lu",
                "Liang Qiu",
                "Changlong Yu",
                "Puyang Xu",
                "Chao Zhang",
                "Bing Yin",
                "Hyokun Yun",
                "Lihong Li"
            ],
            "affiliations": [
                "Amazon",
                "Georgia Institute of Technology",
                "University of Virginia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16421.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agents",
                    "#optimization",
                    "#rl",
                    "#benchmark",
                    "#games",
                    "#training"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "WebAgent-R1: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ RL",
                    "desc": "WebAgent-R1 - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…. ĞĞ½ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ğ²ĞµĞ±-ÑÑ€ĞµĞ´Ğ°Ğ¼Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ÑƒÑÑÑŒ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑƒÑĞ¿ĞµÑ…Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Empowering Web Agents with Multi-Turn Reinforcement Learning",
                    "desc": "WebAgent-R1 is a reinforcement learning (RL) framework designed to train web agents for multi-turn interactions, which are more complex than single-turn tasks. It operates by learning from real-time interactions with web environments, using binary rewards to guide the training process based on task success. The framework has shown significant improvements in task success rates for large language models, outperforming existing methods and proprietary models. Additionally, it explores different initialization strategies to enhance the training process, emphasizing the role of behavior cloning and chain-of-thought reasoning in achieving better performance."
                },
                "zh": {
                    "title": "WebAgent-R1ï¼šå¤šè½®äº¤äº’çš„å¼ºåŒ–å­¦ä¹ æ–°æ¡†æ¶",
                    "desc": "WebAgent-R1æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒç½‘ç»œä»£ç†çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä¸“æ³¨äºå¤šè½®äº¤äº’ï¼ŒæˆåŠŸç‡æ˜¾è‘—é«˜äºç°æœ‰æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„å•è½®ä»»åŠ¡ä¸åŒï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå¤„ç†å¤æ‚çš„é•¿æœŸå†³ç­–é—®é¢˜ï¼Œé€šè¿‡åœ¨çº¿äº¤äº’å­¦ä¹ å¹¶ç”Ÿæˆå¤šæ ·åŒ–çš„è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWebAgent-R1åœ¨WebArena-LiteåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†ä»»åŠ¡æˆåŠŸç‡ï¼Œè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¯¥ç ”ç©¶è¿˜æ¢è®¨äº†ä¸åŒçš„å¼ºåŒ–å­¦ä¹ åˆå§‹åŒ–ç­–ç•¥ï¼Œå¼ºè°ƒäº†çƒ­èº«è®­ç»ƒé˜¶æ®µçš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†åœ¨ç½‘ç»œä»£ç†ä¸­èå…¥é•¿é“¾æ€ç»´æ¨ç†çš„è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16151",
            "title": "Training-Free Reasoning and Reflection in MLLMs",
            "url": "https://huggingface.co/papers/2505.16151",
            "abstract": "The FRANK Model enhances multimodal LLMs with reasoning and reflection abilities without retraining, using a hierarchical weight merging approach that merges visual-pretrained and reasoning-specialized models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Reasoning LLMs (e.g., DeepSeek-R1 and OpenAI-o1) have showcased impressive reasoning capabilities via reinforcement learning. However, extending these capabilities to Multimodal LLMs (MLLMs) is hampered by the prohibitive costs of retraining and the scarcity of high-quality, verifiable multimodal reasoning datasets. This paper introduces FRANK Model, a training-FRee ANd r1-liKe MLLM that imbues off-the-shelf MLLMs with reasoning and reflection abilities, without any gradient updates or extra supervision. Our key insight is to decouple perception and reasoning across MLLM decoder layers. Specifically, we observe that compared to the deeper decoder layers, the shallow decoder layers allocate more attention to visual tokens, while the deeper decoder layers concentrate on textual semantics. This observation motivates a hierarchical weight merging approach that combines a visual-pretrained MLLM with a reasoning-specialized LLM. To this end, we propose a layer-wise, Taylor-derived closed-form fusion mechanism that integrates reasoning capacity into deep decoder layers while preserving visual grounding in shallow decoder layers. Extensive experiments on challenging multimodal reasoning benchmarks demonstrate the effectiveness of our approach. On the MMMU benchmark, our model FRANK-38B achieves an accuracy of 69.2, outperforming the strongest baseline InternVL2.5-38B by +5.3, and even surpasses the proprietary GPT-4o model. Our project homepage is at: http://iip.whu.edu.cn/frank/index.html",
            "score": 7,
            "issue_id": 3914,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "59af0e553fdc317e",
            "authors": [
                "Hongchen Wei",
                "Zhenzhong Chen"
            ],
            "affiliations": [
                "School of Remote Sensing and Information Engineering, Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16151.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "FRANK: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "ĞœĞ¾Ğ´ĞµĞ»ÑŒ FRANK ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM), Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¢ĞµĞ¹Ğ»Ğ¾Ñ€Ğ°, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ ÑĞ»Ğ¾Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğ² Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "FRANK Model: Enhancing MLLMs with Reasoning Without Retraining",
                    "desc": "The FRANK Model enhances multimodal large language models (MLLMs) by integrating reasoning and reflection capabilities without the need for retraining. It employs a hierarchical weight merging technique that combines visual-pretrained models with reasoning-specialized models, allowing for effective reasoning in MLLMs. The model strategically decouples perception and reasoning across different layers of the decoder, leveraging shallow layers for visual attention and deeper layers for textual semantics. Experimental results show that FRANK-38B significantly outperforms existing models on multimodal reasoning tasks, achieving a notable accuracy increase on the MMMU benchmark."
                },
                "zh": {
                    "title": "FRANKæ¨¡å‹ï¼šæ— éœ€é‡è®­çš„å¤šæ¨¡æ€æ¨ç†å¢å¼º",
                    "desc": "FRANKæ¨¡å‹é€šè¿‡åˆ†å±‚æƒé‡åˆå¹¶çš„æ–¹æ³•ï¼Œå¢å¼ºäº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¨ç†å’Œåæ€èƒ½åŠ›ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¯¥æ¨¡å‹å°†è§†è§‰é¢„è®­ç»ƒçš„MLLMä¸ä¸“æ³¨äºæ¨ç†çš„LLMç»“åˆï¼Œé¿å…äº†é«˜æ˜‚çš„é‡æ–°è®­ç»ƒæˆæœ¬ã€‚ç ”ç©¶å‘ç°ï¼Œæµ…å±‚è§£ç å™¨å±‚å¯¹è§†è§‰ä¿¡æ¯çš„å…³æ³¨åº¦æ›´é«˜ï¼Œè€Œæ·±å±‚è§£ç å™¨å±‚åˆ™æ›´æ³¨é‡æ–‡æœ¬è¯­ä¹‰ï¼Œè¿™ä¸€è§‚å¯Ÿä¿ƒæˆäº†åˆ†å±‚æƒé‡åˆå¹¶çš„æ–¹æ³•ã€‚é€šè¿‡åœ¨æ·±å±‚è§£ç å™¨ä¸­æ•´åˆæ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒæµ…å±‚è§£ç å™¨çš„è§†è§‰åŸºç¡€ï¼ŒFRANKæ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡è¶…è¿‡äº†å¤šä¸ªå¼ºåŸºçº¿æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15879",
            "title": "GRIT: Teaching MLLMs to Think with Images",
            "url": "https://huggingface.co/papers/2505.15879",
            "abstract": "Recent studies have demonstrated the efficacy of using Reinforcement Learning (RL) in building reasoning models that articulate chains of thoughts prior to producing final answers. However, despite ongoing advances that aim at enabling reasoning for vision-language tasks, existing open-source visual reasoning models typically generate reasoning content with pure natural language, lacking explicit integration of visual information. This limits their ability to produce clearly articulated and visually grounded reasoning chains. To this end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method for training MLLMs to think with images. GRIT introduces a grounded reasoning paradigm, in which models generate reasoning chains that interleave natural language and explicit bounding box coordinates. These coordinates point to regions of the input image that the model consults during its reasoning process. Additionally, GRIT is equipped with a reinforcement learning approach, GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused on the final answer accuracy and format of the grounded reasoning output, which eliminates the need for data with reasoning chain annotations or explicit bounding box labels. As a result, GRIT achieves exceptional data efficiency, requiring as few as 20 image-question-answer triplets from existing datasets. Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to produce coherent and visually grounded reasoning chains, showing a successful unification of reasoning and grounding abilities.",
            "score": 7,
            "issue_id": 3915,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "05b66f8da1752a05",
            "authors": [
                "Yue Fan",
                "Xuehai He",
                "Diji Yang",
                "Kaizhi Zheng",
                "Ching-Chen Kuo",
                "Yuting Zheng",
                "Sravana Jyothi Narayanaraju",
                "Xinze Guan",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "UC Santa Cruz",
                "eBay"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15879.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#rl",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "GRIT: Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GRIT. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. GRIT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ GRPO-GR, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğµ GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ ÑĞ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GRIT ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ MLLM Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞµ."
                },
                "en": {
                    "title": "Grounded Reasoning: Merging Vision and Language for Better Understanding",
                    "desc": "This paper introduces Grounded Reasoning with Images and Texts (GRIT), a new method that enhances visual reasoning in machine learning models. GRIT allows models to generate reasoning chains that combine natural language with specific visual information, represented by bounding box coordinates. By using a reinforcement learning approach called GRPO-GR, GRIT focuses on improving the accuracy of final answers without needing extensive annotated data. The results show that GRIT can efficiently train models to produce clear and visually supported reasoning, bridging the gap between reasoning and visual understanding."
                },
                "zh": {
                    "title": "å›¾åƒä¸æ–‡æœ¬çš„åŸºç¡€æ¨ç†ï¼šè®©æ¨¡å‹æ›´å¥½åœ°æ€è€ƒ",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ„å»ºæ¨ç†æ¨¡å‹æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œè¿™äº›æ¨¡å‹åœ¨ç»™å‡ºæœ€ç»ˆç­”æ¡ˆä¹‹å‰ä¼šè¡¨è¾¾æ€ç»´é“¾ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼€æºè§†è§‰æ¨ç†æ¨¡å‹é€šå¸¸åªç”¨è‡ªç„¶è¯­è¨€ç”Ÿæˆæ¨ç†å†…å®¹ï¼Œç¼ºä¹ä¸è§†è§‰ä¿¡æ¯çš„æ˜ç¡®ç»“åˆï¼Œè¿™é™åˆ¶äº†å®ƒä»¬ç”Ÿæˆæ¸…æ™°ä¸”ä¸è§†è§‰ç›¸å…³çš„æ¨ç†é“¾çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•â€”â€”å›¾åƒä¸æ–‡æœ¬çš„åŸºç¡€æ¨ç†ï¼ˆGRITï¼‰ï¼Œè¯¥æ–¹æ³•è®­ç»ƒå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸å›¾åƒå…±åŒæ€è€ƒã€‚GRITå¼•å…¥äº†ä¸€ç§åŸºç¡€æ¨ç†èŒƒå¼ï¼Œæ¨¡å‹ç”Ÿæˆçš„æ¨ç†é“¾äº¤æ›¿åŒ…å«è‡ªç„¶è¯­è¨€å’Œæ˜ç¡®çš„è¾¹ç•Œæ¡†åæ ‡ï¼Œè¿™äº›åæ ‡æŒ‡å‘è¾“å…¥å›¾åƒä¸­æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å‚è€ƒçš„åŒºåŸŸã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16944",
            "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in\n  Agentic Scenarios",
            "url": "https://huggingface.co/papers/2505.16944",
            "abstract": "A new benchmark, AgentIF, evaluates Large Language Models' ability to follow complex instructions in realistic agentic scenarios, revealing performance limitations in handling constraints and tool specifications.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated advanced capabilities in real-world agentic applications. Growing research efforts aim to develop LLM-based agents to address practical demands, introducing a new challenge: agentic scenarios often involve lengthy instructions with complex constraints, such as extended system prompts and detailed tool specifications. While adherence to such instructions is crucial for agentic applications, whether LLMs can reliably follow them remains underexplored. In this paper, we introduce AgentIF, the first benchmark for systematically evaluating LLM instruction following ability in agentic scenarios. AgentIF features three key characteristics: (1) Realistic, constructed from 50 real-world agentic applications. (2) Long, averaging 1,723 words with a maximum of 15,630 words. (3) Complex, averaging 11.9 constraints per instruction, covering diverse constraint types, such as tool specifications and condition constraints. To construct AgentIF, we collect 707 human-annotated instructions across 50 agentic tasks from industrial application agents and open-source agentic systems. For each instruction, we annotate the associated constraints and corresponding evaluation metrics, including code-based evaluation, LLM-based evaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically evaluate existing advanced LLMs. We observe that current models generally perform poorly, especially in handling complex constraint structures and tool specifications. We further conduct error analysis and analytical experiments on instruction length and meta constraints, providing some findings about the failure modes of existing LLMs. We have released the code and data to facilitate future research.",
            "score": 6,
            "issue_id": 3920,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "1699a99ec316293a",
            "authors": [
                "Yunjia Qi",
                "Hao Peng",
                "Xiaozhi Wang",
                "Amy Xin",
                "Youfeng Liu",
                "Bin Xu",
                "Lei Hou",
                "Juanzi Li"
            ],
            "affiliations": [
                "Tsinghua University",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16944.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#agents",
                    "#long_context",
                    "#agi"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "AgentIF: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AgentIF Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². AgentIF Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 707 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸Ğ· 50 Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ 1723 ÑĞ»Ğ¾Ğ²Ğ° Ğ¸ 11.9 Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ. ĞÑ†ĞµĞ½ĞºĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ AgentIF Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¸Ñ… Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñ‹ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… LLM."
                },
                "en": {
                    "title": "AgentIF: Benchmarking LLMs in Complex Instruction Following",
                    "desc": "This paper introduces AgentIF, a new benchmark designed to assess how well Large Language Models (LLMs) can follow complex instructions in realistic scenarios where they act as agents. The benchmark is based on 50 real-world applications and includes long instructions with an average of 1,723 words and multiple constraints, such as tool specifications. The study reveals that current LLMs struggle significantly with these complex instructions, particularly in managing constraints and tool requirements. By providing detailed evaluations and error analyses, the authors aim to highlight the limitations of existing models and encourage further research in this area."
                },
                "zh": {
                    "title": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•AgentIFï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æŒ‡ä»¤ä¸‹çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨çœŸå®çš„ä»£ç†åœºæ™¯ä¸­ã€‚AgentIFçš„ç‰¹ç‚¹åŒ…æ‹¬ï¼šåŸºäº50ä¸ªçœŸå®ä¸–ç•Œçš„ä»£ç†åº”ç”¨æ„å»ºï¼ŒæŒ‡ä»¤å¹³å‡é•¿åº¦ä¸º1723ä¸ªå•è¯ï¼Œä¸”æ¯æ¡æŒ‡ä»¤å¹³å‡åŒ…å«11.9ä¸ªçº¦æŸæ¡ä»¶ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„LLMsåœ¨å¤„ç†å¤æ‚çº¦æŸå’Œå·¥å…·è§„èŒƒæ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨éµå¾ªé•¿æŒ‡ä»¤æ–¹é¢ã€‚é€šè¿‡é”™è¯¯åˆ†æå’Œå®éªŒï¼Œæœ¬æ–‡æ­ç¤ºäº†ç°æœ‰æ¨¡å‹çš„å±€é™æ€§ï¼Œå¹¶æä¾›äº†ä»£ç å’Œæ•°æ®ä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16192",
            "title": "VLM-R^3: Region Recognition, Reasoning, and Refinement for Enhanced\n  Multimodal Chain-of-Thought",
            "url": "https://huggingface.co/papers/2505.16192",
            "abstract": "Recently, reasoning-based MLLMs have achieved a degree of success in generating long-form textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on and revisiting of visual regions to achieve precise grounding of textual reasoning in visual evidence. We introduce VLM-R^3 (Visual Language Model with Region Recognition and Reasoning), a framework that equips an MLLM with the ability to (i) decide when additional visual evidence is needed, (ii) determine where to ground within the image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved chain-of-thought. The core of our method is Region-Conditioned Reinforcement Policy Optimization (R-GRPO), a training paradigm that rewards the model for selecting informative regions, formulating appropriate transformations (e.g.\\ crop, zoom), and integrating the resulting visual context into subsequent reasoning steps. To bootstrap this policy, we compile a modest but carefully curated Visuo-Lingual Interleaved Rationale (VLIR) corpus that provides step-level supervision on region selection and textual justification. Extensive experiments on MathVista, ScienceQA, and other benchmarks show that VLM-R^3 sets a new state of the art in zero-shot and few-shot settings, with the largest gains appearing on questions demanding subtle spatial reasoning or fine-grained visual cue extraction.",
            "score": 6,
            "issue_id": 3920,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "ffe915867d50a220",
            "authors": [
                "Chaoya Jiang",
                "Yongrui Heng",
                "Wei Ye",
                "Han Yang",
                "Haiyang Xu",
                "Ming Yan",
                "Ji Zhang",
                "Fei Huang",
                "Shikun Zhang"
            ],
            "affiliations": [
                "Alibaba Group",
                "National Engineering Research Center for Software Engineering, Peking University",
                "ZEEKR Intelligent Technology Holding Limited"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16192.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#cv",
                    "#dataset",
                    "#multimodal",
                    "#benchmark",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜: Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VLM-R^3 - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ R-GRPO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VLIR Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VLM-R^3 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning in Language Models with VLM-R^3",
                    "desc": "This paper presents VLM-R^3, a new framework that enhances reasoning in multi-modal language models (MLLMs) by integrating visual evidence more effectively. It allows the model to identify when it needs more visual information, where to focus within an image, and how to incorporate this visual context into its reasoning process. The training method, Region-Conditioned Reinforcement Policy Optimization (R-GRPO), incentivizes the model to select informative image regions and apply transformations like cropping or zooming. The results demonstrate that VLM-R^3 outperforms previous models on tasks requiring detailed spatial reasoning and visual cue extraction, achieving state-of-the-art performance in various benchmarks."
                },
                "zh": {
                    "title": "è§†è§‰ä¸è¯­è¨€çš„æ·±åº¦èåˆ",
                    "desc": "æœ€è¿‘ï¼ŒåŸºäºæ¨ç†çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç”Ÿæˆé•¿æ–‡æœ¬æ¨ç†é“¾æ–¹é¢å–å¾—äº†ä¸€å®šæˆåŠŸã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦åŠ¨æ€å’Œè¿­ä»£åœ°å…³æ³¨å’Œé‡æ–°å®¡è§†è§†è§‰åŒºåŸŸï¼Œä»¥å®ç°æ–‡æœ¬æ¨ç†ä¸è§†è§‰è¯æ®çš„ç²¾ç¡®ç»“åˆã€‚æˆ‘ä»¬æå‡ºäº†VLM-R^3ï¼ˆå¸¦æœ‰åŒºåŸŸè¯†åˆ«å’Œæ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼‰ï¼Œè¯¥æ¡†æ¶ä½¿MLLMå…·å¤‡äº†å†³å®šä½•æ—¶éœ€è¦é¢å¤–è§†è§‰è¯æ®ã€ç¡®å®šå›¾åƒä¸­åº”èšç„¦çš„ä½ç½®ä»¥åŠå°†ç›¸å…³å­å›¾åƒå†…å®¹æ— ç¼èå…¥äº¤é”™æ€ç»´é“¾çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ–¹æ³•æ˜¯åŒºåŸŸæ¡ä»¶å¼ºåŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆR-GRPOï¼‰ï¼Œé€šè¿‡å¥–åŠ±æ¨¡å‹é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸã€åˆ¶å®šé€‚å½“çš„å˜æ¢ï¼ˆå¦‚è£å‰ªã€ç¼©æ”¾ï¼‰å¹¶å°†ç»“æœè§†è§‰ä¸Šä¸‹æ–‡æ•´åˆåˆ°åç»­æ¨ç†æ­¥éª¤ä¸­ï¼Œæ¥è®­ç»ƒè¯¥ç­–ç•¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15963",
            "title": "OViP: Online Vision-Language Preference Learning",
            "url": "https://huggingface.co/papers/2505.15963",
            "abstract": "Large vision-language models (LVLMs) remain vulnerable to hallucination, often generating content misaligned with visual inputs. While recent approaches advance multi-modal Direct Preference Optimization (DPO) to mitigate hallucination, they typically rely on predefined or randomly edited negative samples that fail to reflect actual model errors, limiting training efficacy. In this work, we propose an Online Vision-language Preference Learning (OViP) framework that dynamically constructs contrastive training data based on the model's own hallucinated outputs. By identifying semantic differences between sampled response pairs and synthesizing negative images using a diffusion model, OViP generates more relevant supervision signals in real time. This failure-driven training enables adaptive alignment of both textual and visual preferences. Moreover, we refine existing evaluation protocols to better capture the trade-off between hallucination suppression and expressiveness. Experiments on hallucination and general benchmarks demonstrate that OViP effectively reduces hallucinations while preserving core multi-modal capabilities.",
            "score": 6,
            "issue_id": 3914,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "4476380071b2e936",
            "authors": [
                "Shujun Liu",
                "Siyuan Wang",
                "Zejun Li",
                "Jianxiang Wang",
                "Cheng Zeng",
                "Zhongyu Wei"
            ],
            "affiliations": [
                "ByteDance",
                "Fudan University",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15963.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#training",
                    "#hallucinations",
                    "#benchmark",
                    "#diffusion",
                    "#rag",
                    "#alignment"
                ],
                "emoji": "ğŸ”®",
                "ru": {
                    "title": "OViP: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ OViP (Online Vision-language Preference Learning). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LVLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. OViP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ OViP ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Dynamic Learning to Combat Hallucination in Vision-Language Models",
                    "desc": "This paper addresses the issue of hallucination in large vision-language models (LVLMs), where the models generate content that does not match the visual inputs. The authors introduce a new framework called Online Vision-language Preference Learning (OViP), which creates training data based on the model's own incorrect outputs, rather than relying on static negative samples. By using a diffusion model to synthesize negative images, OViP provides more relevant feedback for the model to learn from. The results show that this approach not only reduces hallucinations but also maintains the model's ability to express multi-modal information effectively."
                },
                "zh": {
                    "title": "åŠ¨æ€æ„å»ºå¯¹æ¯”æ•°æ®ï¼Œå‡å°‘å¹»è§‰ï¼",
                    "desc": "å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨ç”Ÿæˆå†…å®¹æ—¶å®¹æ˜“å‡ºç°å¹»è§‰ï¼Œå¸¸å¸¸ä¸è§†è§‰è¾“å…¥ä¸ä¸€è‡´ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶é€šè¿‡å¤šæ¨¡æ€ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æ¥å‡è½»å¹»è§‰é—®é¢˜ï¼Œä½†é€šå¸¸ä¾èµ–äºé¢„å®šä¹‰æˆ–éšæœºç¼–è¾‘çš„è´Ÿæ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬æ— æ³•çœŸå®åæ˜ æ¨¡å‹çš„é”™è¯¯ï¼Œé™åˆ¶äº†è®­ç»ƒæ•ˆæœã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨çº¿è§†è§‰è¯­è¨€åå¥½å­¦ä¹ ï¼ˆOViPï¼‰æ¡†æ¶ï¼ŒåŠ¨æ€æ„å»ºå¯¹æ¯”è®­ç»ƒæ•°æ®ï¼ŒåŸºäºæ¨¡å‹è‡ªèº«çš„å¹»è§‰è¾“å‡ºã€‚é€šè¿‡è¯†åˆ«å“åº”å¯¹ä¹‹é—´çš„è¯­ä¹‰å·®å¼‚å¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹åˆæˆè´Ÿå›¾åƒï¼ŒOViPå®æ—¶ç”Ÿæˆæ›´ç›¸å…³çš„ç›‘ç£ä¿¡å·ï¼Œæœ‰æ•ˆå‡å°‘å¹»è§‰ï¼ŒåŒæ—¶ä¿æŒå¤šæ¨¡æ€èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15960",
            "title": "Training Step-Level Reasoning Verifiers with Formal Verification Tools",
            "url": "https://huggingface.co/papers/2505.15960",
            "abstract": "FoVer is a method for automatically annotating step-level error labels using formal verification tools to train Process Reward Models, which significantly improves cross-task generalization and outperforms human-annotated methods in various reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs), which provide step-by-step feedback on the reasoning generated by Large Language Models (LLMs), are receiving increasing attention. However, two key research gaps remain: collecting accurate step-level error labels for training typically requires costly human annotation, and existing PRMs are limited to math reasoning problems. In response to these gaps, this paper aims to address the challenges of automatic dataset creation and the generalization of PRMs to diverse reasoning tasks. To achieve this goal, we propose FoVer, an approach for training PRMs on step-level error labels automatically annotated by formal verification tools, such as Z3 for formal logic and Isabelle for theorem proof, which provide automatic and accurate verification for symbolic tasks. Using this approach, we synthesize a training dataset with error labels on LLM responses for formal logic and theorem proof tasks without human annotation. Although this data synthesis is feasible only for tasks compatible with formal verification, we observe that LLM-based PRMs trained on our dataset exhibit cross-task generalization, improving verification across diverse reasoning tasks. Specifically, PRMs trained with FoVer significantly outperform baseline PRMs based on the original LLMs and achieve competitive or superior results compared to state-of-the-art PRMs trained on labels annotated by humans or stronger models, as measured by step-level verification on ProcessBench and Best-of-K performance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU, and BBH. The datasets, models, and code are provided at https://github.com/psunlpgroup/FoVer.",
            "score": 6,
            "issue_id": 3926,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "fd1009a733cb10f6",
            "authors": [
                "Ryo Kamoi",
                "Yusen Zhang",
                "Nan Zhang",
                "Sarkar Snigdha Sarathi Das",
                "Rui Zhang"
            ],
            "affiliations": [
                "Penn State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15960.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#data",
                    "#optimization",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "FoVer - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² (Process Reward Models). Ğ”Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. FoVer ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ FoVer, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑÑ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Automating Error Annotation for Enhanced Reasoning Models",
                    "desc": "FoVer is a novel method that automates the annotation of step-level error labels using formal verification tools, which helps in training Process Reward Models (PRMs). This approach addresses the challenges of costly human annotation and expands the applicability of PRMs beyond just math reasoning tasks. By synthesizing a training dataset with accurate error labels for formal logic and theorem proof tasks, FoVer enables PRMs to generalize across various reasoning tasks effectively. The results show that PRMs trained with FoVer outperform traditional methods and achieve competitive performance on multiple reasoning benchmarks."
                },
                "zh": {
                    "title": "FoVerï¼šè‡ªåŠ¨æ ‡æ³¨æå‡æ¨ç†æ¨¡å‹çš„è·¨ä»»åŠ¡èƒ½åŠ›",
                    "desc": "FoVeræ˜¯ä¸€ç§åˆ©ç”¨å½¢å¼éªŒè¯å·¥å…·è‡ªåŠ¨æ ‡æ³¨æ­¥éª¤çº§é”™è¯¯æ ‡ç­¾çš„æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰ã€‚è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†è·¨ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†äººå·¥æ ‡æ³¨çš„æ–¹æ³•ã€‚FoVeré€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„é”™è¯¯æ ‡ç­¾ï¼Œè§£å†³äº†è®­ç»ƒä¸­éœ€è¦æ˜‚è´µäººå·¥æ ‡æ³¨çš„é—®é¢˜ï¼Œå¹¶æ‰©å±•äº†PRMsåœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨FoVerè®­ç»ƒçš„PRMsåœ¨æ­¥éª¤çº§éªŒè¯ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†åŸºäºåŸå§‹å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºçº¿PRMsã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16186",
            "title": "SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning",
            "url": "https://huggingface.co/papers/2505.16186",
            "abstract": "SafeKey enhances the safety of large reasoning models by focusing on activating a safety aha moment in the key sentence through dual-path safety head and query-mask modeling, thereby improving generalization to harmful prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) introduce a new generation paradigm of explicitly reasoning before answering, leading to remarkable improvements in complex tasks. However, they pose great safety risks against harmful queries and adversarial attacks. While recent mainstream safety efforts on LRMs, supervised fine-tuning (SFT), improve safety performance, we find that SFT-aligned models struggle to generalize to unseen jailbreak prompts. After thorough investigation of LRMs' generation, we identify a safety aha moment that can activate safety reasoning and lead to a safe response. This aha moment typically appears in the `key sentence', which follows models' query understanding process and can indicate whether the model will proceed safely. Based on these insights, we propose SafeKey, including two complementary objectives to better activate the safety aha moment in the key sentence: (1) a Dual-Path Safety Head to enhance the safety signal in the model's internal representations before the key sentence, and (2) a Query-Mask Modeling objective to improve the models' attention on its query understanding, which has important safety hints. Experiments across multiple safety benchmarks demonstrate that our methods significantly improve safety generalization to a wide range of jailbreak attacks and out-of-distribution harmful prompts, lowering the average harmfulness rate by 9.6\\%, while maintaining general abilities. Our analysis reveals how SafeKey enhances safety by reshaping internal attention and improving the quality of hidden representations.",
            "score": 5,
            "issue_id": 3918,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "a47a9eb4edda38b3",
            "authors": [
                "Kaiwen Zhou",
                "Xuandong Zhao",
                "Gaowen Liu",
                "Jayanth Srinivasa",
                "Aosong Feng",
                "Dawn Song",
                "Xin Eric Wang"
            ],
            "affiliations": [
                "Cisco Research",
                "UC Berkeley",
                "UC Santa Cruz",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16186.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#security",
                    "#training",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "SafeKey: ĞĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SafeKey Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM). ĞœĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ 'Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ° Ğ¾Ğ·Ğ°Ñ€ĞµĞ½Ğ¸Ñ' Ğ² ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²ÑƒÑ…Ğ¿ÑƒÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. SafeKey ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Activating Safety Moments in Large Reasoning Models",
                    "desc": "SafeKey is a method designed to improve the safety of large reasoning models (LRMs) by focusing on a critical moment in the model's reasoning process, known as the safety aha moment. This moment occurs in the key sentence, which helps the model determine whether it can respond safely to a query. SafeKey employs a Dual-Path Safety Head to strengthen the safety signals in the model's internal representations and a Query-Mask Modeling approach to enhance the model's attention to safety-related aspects of the query. Through experiments, SafeKey has shown to significantly reduce the risk of harmful responses while preserving the model's overall performance."
                },
                "zh": {
                    "title": "SafeKeyï¼šæ¿€æ´»å®‰å…¨æ—¶åˆ»ï¼Œæå‡æ¨ç†æ¨¡å‹å®‰å…¨æ€§",
                    "desc": "SafeKeyé€šè¿‡æ¿€æ´»å…³é”®å¥ä¸­çš„å®‰å…¨æ—¶åˆ»æ¥å¢å¼ºå¤§å‹æ¨ç†æ¨¡å‹çš„å®‰å…¨æ€§ã€‚å®ƒé‡‡ç”¨åŒè·¯å¾„å®‰å…¨å¤´å’ŒæŸ¥è¯¢æ©ç å»ºæ¨¡ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹å¯¹æœ‰å®³æç¤ºçš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå…³é”®å¥ä¸­çš„å®‰å…¨æ—¶åˆ»èƒ½å¤Ÿå¼•å¯¼æ¨¡å‹åšå‡ºå®‰å…¨çš„å“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSafeKeyæ˜¾è‘—é™ä½äº†æ¨¡å‹åœ¨å„ç§æ”»å‡»ä¸‹çš„æœ‰å®³æ€§ï¼ŒåŒæ—¶ä¿æŒäº†å…¶ä¸€èˆ¬èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.11711",
            "title": "Reinforcement Learning Finetunes Small Subnetworks in Large Language\n  Models",
            "url": "https://huggingface.co/papers/2505.11711",
            "abstract": "Reinforcement learning improves large language models with minimal parameter updates, affecting only a small subnetwork without explicit sparsity techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) yields substantial improvements in large language models (LLMs) downstream task performance and alignment with human values. Surprisingly, such large gains result from updating only a small subnetwork comprising just 5 percent to 30 percent of the parameters, with the rest effectively unchanged. We refer to this phenomenon as parameter update sparsity induced by RL. It is observed across all 7 widely used RL algorithms (e.g., PPO, GRPO, DPO) and all 10 LLMs from different families in our experiments. This sparsity is intrinsic and occurs without any explicit sparsity promoting regularizations or architectural constraints. Finetuning the subnetwork alone recovers the test accuracy, and, remarkably, produces a model nearly identical to the one obtained via full finetuning. The subnetworks from different random seeds, training data, and even RL algorithms show substantially greater overlap than expected by chance. Our analysis suggests that this sparsity is not due to updating only a subset of layers, instead, nearly all parameter matrices receive similarly sparse updates. Moreover, the updates to almost all parameter matrices are nearly full-rank, suggesting RL updates a small subset of parameters that nevertheless span almost the full subspaces that the parameter matrices can represent. We conjecture that the this update sparsity can be primarily attributed to training on data that is near the policy distribution, techniques that encourage the policy to remain close to the pretrained model, such as the KL regularization and gradient clipping, have limited impact.",
            "score": 5,
            "issue_id": 3914,
            "pub_date": "2025-05-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ",
                "en": "May 16",
                "zh": "5æœˆ16æ—¥"
            },
            "hash": "e7296e89ef67015f",
            "authors": [
                "Sagnik Mukherjee",
                "Lifan Yuan",
                "Dilek Hakkani-Tur",
                "Hao Peng"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.11711.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#rl",
                    "#optimization",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¸Ñ… ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼. Ğ£Ğ´Ğ¸Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ÑÑ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿Ğ¾Ğ´ÑĞµÑ‚Ğ¸, ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ĞµĞ¹ 5-30% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ 'Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²', Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² RL Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ² LLM Ğ±ĞµĞ· Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑĞ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ğ³Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ²ÑĞµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ½Ğ¾ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Efficient Reinforcement Learning: Small Updates, Big Gains!",
                    "desc": "This paper explores how reinforcement learning (RL) can enhance the performance of large language models (LLMs) by making minimal updates to a small subnetwork of parameters. Remarkably, only 5 to 30 percent of the model's parameters are adjusted, while the majority remain unchanged, a phenomenon termed 'parameter update sparsity.' This sparsity occurs across various RL algorithms and LLMs, indicating a consistent pattern in how RL influences model training. The findings suggest that even with limited updates, the subnetwork can achieve performance comparable to full finetuning, highlighting the efficiency of RL in optimizing LLMs."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ ï¼šå°æ›´æ–°ï¼Œå¤§æå‡",
                    "desc": "å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­é€šè¿‡æœ€å°çš„å‚æ•°æ›´æ–°æ˜¾è‘—æå‡äº†ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°å’Œä¸äººç±»ä»·å€¼è§‚çš„å¯¹é½ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè¿™ç§æ˜¾è‘—çš„æå‡ä»…é€šè¿‡æ›´æ–°å å‚æ•°5%åˆ°30%çš„å°å­ç½‘ç»œå®ç°ï¼Œå…¶ä½™å‚æ•°åŸºæœ¬ä¿æŒä¸å˜ã€‚æˆ‘ä»¬ç§°è¿™ç§ç°è±¡ä¸ºç”±RLå¼•èµ·çš„å‚æ•°æ›´æ–°ç¨€ç–æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§ç¨€ç–æ€§åœ¨ä¸ƒç§å¹¿æ³›ä½¿ç”¨çš„RLç®—æ³•å’Œåç§ä¸åŒå®¶æ—çš„LLMä¸­æ™®éå­˜åœ¨ï¼Œä¸”ä¸éœ€è¦ä»»ä½•æ˜¾å¼çš„ç¨€ç–ä¿ƒè¿›æ­£åˆ™åŒ–æˆ–æ¶æ„çº¦æŸã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16265",
            "title": "Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models",
            "url": "https://huggingface.co/papers/2505.16265",
            "abstract": "Think-RM is a framework that enhances generative reward models with long-horizon reasoning and a novel pairwise RLHF pipeline to improve end-policy performance in aligning large language models with human preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning from human feedback (RLHF) has become a powerful post-training paradigm for aligning large language models with human preferences. A core challenge in RLHF is constructing accurate reward signals, where the conventional Bradley-Terry reward models (BT RMs) often suffer from sensitivity to data size and coverage, as well as vulnerability to reward hacking. Generative reward models (GenRMs) offer a more robust alternative by generating chain-of-thought (CoT) rationales followed by a final reward. However, existing GenRMs rely on shallow, vertically scaled reasoning, limiting their capacity to handle nuanced or complex (e.g., reasoning-intensive) tasks. Moreover, their pairwise preference outputs are incompatible with standard RLHF algorithms that require pointwise reward signals. In this work, we introduce Think-RM, a training framework that enables long-horizon reasoning in GenRMs by modeling an internal thinking process. Rather than producing structured, externally provided rationales, Think-RM generates flexible, self-guided reasoning traces that support advanced capabilities such as self-reflection, hypothetical reasoning, and divergent reasoning. To elicit these reasoning abilities, we first warm-up the models by supervised fine-tuning (SFT) over long CoT data. We then further improve the model's long-horizon abilities by rule-based reinforcement learning (RL). In addition, we propose a novel pairwise RLHF pipeline that directly optimizes policies using pairwise preference rewards, eliminating the need for pointwise reward conversion and enabling more effective use of Think-RM outputs. Experiments show that Think-RM achieves state-of-the-art results on RM-Bench, outperforming both BT RM and vertically scaled GenRM by 8%. When combined with our pairwise RLHF pipeline, it demonstrates superior end-policy performance compared to traditional approaches.",
            "score": 4,
            "issue_id": 3929,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "3bfe544ceb0449d3",
            "authors": [
                "Ilgee Hong",
                "Changlong Yu",
                "Liang Qiu",
                "Weixiang Yan",
                "Zhenghao Xu",
                "Haoming Jiang",
                "Qingru Zhang",
                "Qin Lu",
                "Xin Liu",
                "Chao Zhang",
                "Tuo Zhao"
            ],
            "affiliations": [
                "Amazon",
                "Georgia Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16265.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#reasoning",
                    "#rlhf",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ”ÑƒĞ¼Ğ°Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¶Ğµ: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Think-RM - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (RLHF). ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ¸Ğ±ĞºĞ¸Ğµ, ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ°Ğº ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Think-RM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ RM-Bench, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° 8%. Ğ’ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ğ¾Ğ¹ RLHF Ğ¾Ğ½ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering Generative Models with Long-Horizon Reasoning",
                    "desc": "Think-RM is a new framework designed to improve generative reward models (GenRMs) by enabling long-horizon reasoning, which helps align large language models with human preferences more effectively. It addresses the limitations of traditional Bradley-Terry reward models (BT RMs) that struggle with data sensitivity and reward hacking. By generating self-guided reasoning traces, Think-RM enhances the model's ability to perform complex reasoning tasks. Additionally, it introduces a novel pairwise reinforcement learning from human feedback (RLHF) pipeline that optimizes policies directly with pairwise preferences, leading to better overall performance."
                },
                "zh": {
                    "title": "Think-RMï¼šæå‡ç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„é•¿æ—¶é—´æ¨ç†èƒ½åŠ›",
                    "desc": "Think-RMæ˜¯ä¸€ä¸ªå¢å¼ºç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é•¿æ—¶é—´æ¨ç†å’Œæ–°é¢–çš„æˆå¯¹äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æµç¨‹æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½æ€§èƒ½ã€‚ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹åœ¨æ•°æ®è§„æ¨¡å’Œè¦†ç›–åº¦ä¸Šæ•æ„Ÿï¼Œå®¹æ˜“å—åˆ°å¥–åŠ±æ“æ§çš„å½±å“ï¼Œè€Œç”Ÿæˆå¥–åŠ±æ¨¡å‹åˆ™é€šè¿‡ç”Ÿæˆæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ¥æä¾›æ›´ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆã€‚Think-RMé€šè¿‡å»ºæ¨¡å†…éƒ¨æ€ç»´è¿‡ç¨‹ï¼Œç”Ÿæˆçµæ´»çš„è‡ªæˆ‘å¼•å¯¼æ¨ç†è½¨è¿¹ï¼Œæ”¯æŒè‡ªæˆ‘åæ€å’Œå‡è®¾æ¨ç†ç­‰é«˜çº§èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒThink-RMåœ¨RM-Benchä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œè¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17019",
            "title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication\n  Understanding and Reasoning Framework",
            "url": "https://huggingface.co/papers/2505.17019",
            "abstract": "LAD, a three-stage framework using GPT-4o-mini, achieves state-of-the-art performance in image implication understanding and reasoning tasks across different languages and question types.  \t\t\t\t\tAI-generated summary \t\t\t\t Metaphorical comprehension in images remains a critical challenge for AI systems, as existing models struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. While multimodal large language models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they struggle with a fundamental limitation on image implication tasks: contextual gaps that obscure the relationships between different visual elements and their abstract meanings. Inspired by the human cognitive process, we propose Let Androids Dream (LAD), a novel framework for image implication understanding and reasoning. LAD addresses contextual missing through the three-stage framework: (1) Perception: converting visual information into rich and multi-level textual representations, (2) Search: iteratively searching and integrating cross-domain knowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment image implication via explicit reasoning. Our framework with the lightweight GPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English image implication benchmark and a huge improvement on Chinese benchmark, performing comparable with the GPT-4o model on Multiple-Choice Question (MCQ) and outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work provides new insights into how AI can more effectively interpret image implications, advancing the field of vision-language reasoning and human-AI interaction. Our project is publicly available at https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.",
            "score": 3,
            "issue_id": 3922,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "9284570e4cba3821",
            "authors": [
                "Chenhao Zhang",
                "Yazhe Niu"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17019.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#science",
                    "#multimodal",
                    "#cv",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "LAD: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº LAD Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GPT-4o-mini. LAD Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². LAD Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Unlocking Image Meanings with LAD Framework",
                    "desc": "The paper introduces Let Androids Dream (LAD), a three-stage framework designed to enhance image implication understanding and reasoning using the lightweight GPT-4o-mini model. It addresses the limitations of existing multimodal large language models (MLLMs) in grasping the nuanced meanings of images by implementing a structured approach: Perception, Search, and Reasoning. This framework converts visual data into detailed textual representations, integrates cross-domain knowledge to clarify ambiguities, and generates contextually aligned implications through reasoning. LAD demonstrates state-of-the-art performance on various benchmarks, significantly improving image implication tasks in both English and Chinese, and offers valuable insights for advancing vision-language reasoning."
                },
                "zh": {
                    "title": "è®©å®‰å“æ¢¦è§ç”µç¾Šï¼šå›¾åƒéšå«ç†è§£çš„æ–°çªç ´",
                    "desc": "LADæ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µæ¡†æ¶ï¼Œåˆ©ç”¨GPT-4o-miniæ¨¡å‹ï¼Œåœ¨å›¾åƒéšå«ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„ŸçŸ¥ã€æœç´¢å’Œæ¨ç†ä¸‰ä¸ªæ­¥éª¤ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨ç†è§£å›¾åƒéšå«æ„ä¹‰æ—¶çš„ä¸Šä¸‹æ–‡ç¼ºå¤±é—®é¢˜ã€‚LADèƒ½å¤Ÿå°†è§†è§‰ä¿¡æ¯è½¬åŒ–ä¸ºä¸°å¯Œçš„æ–‡æœ¬è¡¨ç¤ºï¼Œå¹¶é€šè¿‡è·¨é¢†åŸŸçŸ¥è¯†çš„æ•´åˆæ¥æ¶ˆé™¤æ­§ä¹‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºAIæ›´æœ‰æ•ˆåœ°è§£è¯»å›¾åƒéšå«æ„ä¹‰æä¾›äº†æ–°è§è§£ï¼Œæ¨åŠ¨äº†è§†è§‰-è¯­è¨€æ¨ç†å’Œäººæœºäº¤äº’é¢†åŸŸçš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.17015",
            "title": "Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal\n  Large Language Models",
            "url": "https://huggingface.co/papers/2505.17015",
            "abstract": "Multi-modal large language models (MLLMs) have rapidly advanced in visual tasks, yet their spatial understanding remains limited to single images, leaving them ill-suited for robotics and other real-world applications that require multi-frame reasoning. In this paper, we propose a framework to equip MLLMs with robust multi-frame spatial understanding by integrating depth perception, visual correspondence, and dynamic perception. Central to our approach is the MultiSPA dataset, a novel, large-scale collection of more than 27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we introduce a comprehensive benchmark that tests a wide spectrum of spatial tasks under uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves significant gains over baselines and proprietary systems, demonstrating scalable, generalizable multi-frame reasoning. We further observe multi-task benefits and early indications of emergent capabilities in challenging scenarios, and showcase how our model can serve as a multi-frame reward annotator for robotics.",
            "score": 3,
            "issue_id": 3923,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "b15dc44bbeff2152",
            "authors": [
                "Runsen Xu",
                "Weiyao Wang",
                "Hao Tang",
                "Xingyu Chen",
                "Xiaodong Wang",
                "Fu-Jen Chu",
                "Dahua Lin",
                "Matt Feiszli",
                "Kevin J. Liang"
            ],
            "affiliations": [
                "FAIR, Meta",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.17015.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#games",
                    "#3d",
                    "#robotics",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MultiSPA Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Multi-SpatialMLLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´."
                },
                "en": {
                    "title": "Empowering MLLMs with Multi-Frame Spatial Intelligence",
                    "desc": "This paper addresses the limitations of multi-modal large language models (MLLMs) in understanding spatial information across multiple frames, which is crucial for applications like robotics. The authors introduce a new framework that enhances MLLMs by incorporating depth perception, visual correspondence, and dynamic perception. They present the MultiSPA dataset, a large-scale collection of over 27 million samples that covers a variety of 3D and 4D scenes, to train and evaluate their model. The proposed Multi-SpatialMLLM shows improved performance in multi-frame reasoning tasks and demonstrates potential for multi-task learning and emergent capabilities in complex scenarios."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹çš„å¤šå¸§ç©ºé—´ç†è§£èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œä»¥å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šå¸§ç©ºé—´ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡æ•´åˆæ·±åº¦æ„ŸçŸ¥ã€è§†è§‰å¯¹åº”å’ŒåŠ¨æ€æ„ŸçŸ¥ï¼Œæ¥è§£å†³ç°æœ‰æ¨¡å‹åœ¨æœºå™¨äººå’Œç°å®åº”ç”¨ä¸­çš„å±€é™æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†MultiSPAæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡2700ä¸‡æ ·æœ¬çš„å…¨æ–°å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§3Då’Œ4Dåœºæ™¯ã€‚æˆ‘ä»¬çš„æ¨¡å‹Multi-SpatialMLLMåœ¨å¤šå¸§æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ï¼Œå¹¶å±•ç¤ºäº†åœ¨å¤æ‚åœºæ™¯ä¸­çš„å¤šä»»åŠ¡ä¼˜åŠ¿å’Œæ–°å…´èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15517",
            "title": "Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot\n  Manipulation Datasets",
            "url": "https://huggingface.co/papers/2505.15517",
            "abstract": "Robo2VLM, a framework for generating Visual Question Answering datasets using robot trajectory data, enhances and evaluates Vision-Language Models by leveraging sensory modalities and 3D property understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) acquire real-world knowledge and general reasoning ability through Internet-scale image-text corpora. They can augment robotic systems with scene understanding and task planning, and assist visuomotor policies that are trained on robot trajectory data. We explore the reverse paradigm - using rich, real, multi-modal robot trajectory data to enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual Question Answering (VQA) dataset generation framework for VLMs. Given a human tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual and non-descriptive sensory modalities, such as end-effector pose, gripper aperture, and force sensing. Based on these modalities, it segments the robot trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses scene and interaction understanding to identify 3D properties of the robot, task goal, and the target object. The properties are used to generate representative VQA queries - images with textural multiple-choice questions - based on spatial, goal-conditioned, and interaction reasoning question templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710 questions covering 463 distinct scenes and 3,396 robotic manipulation tasks from 176k real robot trajectories. Results suggest that Robo2VLM-1 can benchmark and improve VLM capabilities in spatial and interaction reasoning.",
            "score": 3,
            "issue_id": 3917,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "7c3b47e3a7b062f1",
            "authors": [
                "Kaiyuan Chen",
                "Shuangyu Xie",
                "Zehan Ma",
                "Ken Goldberg"
            ],
            "affiliations": [
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15517.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#games",
                    "#dataset",
                    "#3d",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ Ğ¾Ğ±Ğ¾Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ Ğ˜Ğ˜ Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¼Ğ¸Ñ€",
                    "desc": "Robo2VLM - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ 3D-ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM). Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ½Ğ° Ñ„Ğ°Ğ·Ñ‹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ…, Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Robo2VLM-1, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 463 ÑÑ†ĞµĞ½Ñ‹ Ğ¸ 3396 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ¸Ğ· 176 Ñ‚Ñ‹ÑÑÑ‡ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing VLMs with Robot Trajectory Insights",
                    "desc": "Robo2VLM is a framework designed to create Visual Question Answering (VQA) datasets by utilizing data from robot trajectories. It enhances Vision-Language Models (VLMs) by integrating sensory information and understanding 3D properties related to robotic tasks. The framework segments robot movements into phases and generates questions based on the robot's interactions with its environment. The resulting dataset, Robo2VLM-1, includes a vast number of questions that help evaluate and improve the reasoning abilities of VLMs in spatial and interaction contexts."
                },
                "zh": {
                    "title": "åˆ©ç”¨æœºå™¨äººè½¨è¿¹æ•°æ®æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›",
                    "desc": "Robo2VLMæ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆè§†è§‰é—®ç­”æ•°æ®é›†çš„æ¡†æ¶ï¼Œåˆ©ç”¨æœºå™¨äººè½¨è¿¹æ•°æ®æ¥å¢å¼ºå’Œè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†ææœºå™¨äººçš„ä¼ æ„Ÿå™¨æ•°æ®å’Œ3Då±æ€§ç†è§£ï¼Œç”Ÿæˆä¸æœºå™¨äººæ“ä½œç›¸å…³çš„é—®ç­”æ•°æ®ã€‚Robo2VLMå°†æœºå™¨äººè½¨è¿¹åˆ†ä¸ºå¤šä¸ªæ“ä½œé˜¶æ®µï¼Œå¹¶åœ¨æ¯ä¸ªé˜¶æ®µè¯†åˆ«ä»»åŠ¡ç›®æ ‡å’Œç›®æ ‡ç‰©ä½“çš„3Då±æ€§ã€‚æœ€ç»ˆï¼ŒRobo2VLM-1æ•°æ®é›†åŒ…å«684,710ä¸ªé—®é¢˜ï¼Œæ¶µç›–463ä¸ªä¸åŒåœºæ™¯å’Œ3,396ä¸ªæœºå™¨äººæ“ä½œä»»åŠ¡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯„ä¼°å’Œæå‡VLMåœ¨ç©ºé—´å’Œäº¤äº’æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16612",
            "title": "Steering Large Language Models for Machine Translation Personalization",
            "url": "https://huggingface.co/papers/2505.16612",
            "abstract": "Strategies including prompting and contrastive frameworks using latent concepts from sparse autoencoders effectively personalize LLM translations in low-resource settings while maintaining quality.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality machine translation systems based on large language models (LLMs) have simplified the production of personalized translations reflecting specific stylistic constraints. However, these systems still struggle in settings where stylistic requirements are less explicit and might be harder to convey via prompting. We explore various strategies for personalizing LLM-generated translations in low-resource settings, focusing on the challenging literary translation domain. We explore prompting strategies and inference-time interventions for steering model generations towards a personalized style, and propose a contrastive framework exploiting latent concepts extracted from sparse autoencoders to identify salient personalization properties. Our results show that steering achieves strong personalization while preserving translation quality. We further examine the impact of steering on LLM representations, finding model layers with a relevant impact for personalization are impacted similarly by multi-shot prompting and our steering method, suggesting similar mechanism at play.",
            "score": 2,
            "issue_id": 3922,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "5695106d35c6955a",
            "authors": [
                "Daniel Scalena",
                "Gabriele Sarti",
                "Arianna Bisazza",
                "Elisabetta Fersini",
                "Malvina Nissim"
            ],
            "affiliations": [
                "CLCG, University of Groningen",
                "University of Milano-Bicocca"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16612.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#training",
                    "#multimodal",
                    "#machine_translation"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ² LLM: Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğº Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM), Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ ÑÑ‚Ğ¸Ğ»Ñ. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Personalized Translations Made Easy with LLMs!",
                    "desc": "This paper discusses methods to improve personalized translations using large language models (LLMs) in situations where resources are limited. It highlights the use of prompting strategies and a contrastive framework that leverages latent concepts from sparse autoencoders to enhance stylistic personalization. The authors demonstrate that these techniques can effectively guide the model's output while maintaining high translation quality. Additionally, they analyze how these personalization methods influence the internal representations of the LLM, indicating that similar mechanisms are at work in both prompting and steering approaches."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–ç¿»è¯‘çš„æ–°ç­–ç•¥",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†åœ¨ä½èµ„æºç¯å¢ƒä¸‹ï¼Œå¦‚ä½•é€šè¿‡æç¤ºå’Œå¯¹æ¯”æ¡†æ¶æ¥ä¸ªæ€§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¿»è¯‘ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨æå–çš„æ½œåœ¨æ¦‚å¿µï¼Œå¯ä»¥æœ‰æ•ˆè¯†åˆ«ä¸ªæ€§åŒ–ç‰¹å¾ï¼Œä»è€Œæ”¹å–„ç¿»è¯‘è´¨é‡ã€‚æˆ‘ä»¬æå‡ºçš„ç­–ç•¥åœ¨æ–‡å­¦ç¿»è¯‘é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒç¿»è¯‘è´¨é‡çš„åŒæ—¶ï¼Œå®ç°å¼ºçƒˆçš„ä¸ªæ€§åŒ–æ•ˆæœã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ï¼Œä¸ªæ€§åŒ–ç›¸å…³çš„æ¨¡å‹å±‚åœ¨å¤šæ¬¡æç¤ºå’Œæˆ‘ä»¬çš„å¼•å¯¼æ–¹æ³•ä¸‹å—åˆ°çš„å½±å“ç›¸ä¼¼ï¼Œè¡¨æ˜ä¸¤è€…å¯èƒ½å­˜åœ¨ç›¸ä¼¼çš„æœºåˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16170",
            "title": "When Do LLMs Admit Their Mistakes? Understanding the Role of Model\n  Belief in Retraction",
            "url": "https://huggingface.co/papers/2505.16170",
            "abstract": "LLMs rarely retract incorrect answers they believe to be factually correct, but supervised fine-tuning can improve their retraction performance by refining their internal beliefs.  \t\t\t\t\tAI-generated summary \t\t\t\t Can large language models (LLMs) admit their mistakes when they should know better? In this work, we define the behavior of acknowledging errors in previously generated answers as \"retraction\" and aim to understand when and why LLMs choose to retract. We first construct model-specific datasets to evaluate whether a model will retract an incorrect answer that contradicts its own parametric knowledge. While LLMs are capable of retraction, they do so only infrequently. We demonstrate that retraction is closely tied to previously identified indicators of models' internal belief: models fail to retract wrong answers that they \"believe\" to be factually correct. Steering experiments further demonstrate that internal belief causally influences model retraction. In particular, when the model does not believe its answer, this not only encourages the model to attempt to verify the answer, but also alters attention behavior during self-verification. Finally, we demonstrate that simple supervised fine-tuning significantly improves retraction performance by helping the model learn more accurate internal beliefs. Code and datasets are available on https://github.com/ayyyq/llm-retraction.",
            "score": 2,
            "issue_id": 3919,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "2f89bb0f5c59846a",
            "authors": [
                "Yuqing Yang",
                "Robin Jia"
            ],
            "affiliations": [
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16170.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#hallucinations",
                    "#dataset"
                ],
                "emoji": "ğŸ¤”",
                "ru": {
                    "title": "Ğ£Ñ‡Ğ¸Ğ¼ Ğ˜Ğ˜ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ 'Ñ€ĞµÑ‚Ñ€Ğ°ĞºÑ†Ğ¸ĞµĞ¹'. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM Ñ€ĞµĞ´ĞºĞ¾ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ½Ğ¸ ÑÑ‡Ğ¸Ñ‚Ğ°ÑÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ñ Ñ‚ĞµÑĞ½Ğ¾ ÑĞ²ÑĞ·Ğ°Ğ½Ğ° Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€ĞµÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ Ğ¸Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing LLMs: Fine-Tuning for Better Error Retraction",
                    "desc": "This paper investigates how large language models (LLMs) handle the acknowledgment of their mistakes, a behavior termed 'retraction'. It finds that LLMs rarely retract incorrect answers, especially when they are confident in their incorrect beliefs. The study shows that the ability to retract is influenced by the model's internal beliefs, where models are less likely to retract answers they consider factually correct. Additionally, the authors demonstrate that supervised fine-tuning can enhance the retraction capabilities of LLMs by refining their internal belief systems."
                },
                "zh": {
                    "title": "æå‡æ¨¡å‹çš„é”™è¯¯æ’¤å›èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢å¯¹é”™è¯¯æ—¶çš„è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›ï¼Œç§°ä¹‹ä¸ºâ€œæ’¤å›â€ã€‚æˆ‘ä»¬å‘ç°ï¼ŒLLMsåœ¨è®¤ä¸ºè‡ªå·±çš„ç­”æ¡ˆæ˜¯æ­£ç¡®æ—¶ï¼Œå¾€å¾€ä¸ä¼šæ’¤å›é”™è¯¯çš„å›ç­”ã€‚é€šè¿‡æ„å»ºç‰¹å®šçš„æ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æ¨¡å‹åœ¨ä½•ç§æƒ…å†µä¸‹ä¼šè¿›è¡Œæ’¤å›ï¼Œå¹¶å‘ç°å†…éƒ¨ä¿¡å¿µå¯¹æ’¤å›è¡Œä¸ºæœ‰é‡è¦å½±å“ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ç›‘ç£å¾®è°ƒæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ’¤å›æ€§èƒ½ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ æ›´å‡†ç¡®çš„å†…éƒ¨ä¿¡å¿µã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16088",
            "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal\n  Reasoning",
            "url": "https://huggingface.co/papers/2505.16088",
            "abstract": "Modern BPE tokenizers often split calendar dates into meaningless fragments, e.g., 20250312 rightarrow 202, 503, 12, inflating token counts and obscuring the inherent structure needed for robust temporal reasoning. In this work, we (1) introduce a simple yet interpretable metric, termed date fragmentation ratio, that measures how faithfully a tokenizer preserves multi-digit date components; (2) release DateAugBench, a suite of 6500 examples spanning three temporal reasoning tasks: context-based date resolution, format-invariance puzzles, and date arithmetic across historical, contemporary, and future regimes; and (3) through layer-wise probing and causal attention-hop analyses, uncover an emergent date-abstraction mechanism whereby large language models stitch together the fragments of month, day, and year components for temporal reasoning. Our experiments show that excessive fragmentation correlates with accuracy drops of up to 10 points on uncommon dates like historical and futuristic dates. Further, we find that the larger the model, the faster the emergent date abstraction that heals date fragments is accomplished. Lastly, we observe a reasoning path that LLMs follow to assemble date fragments, typically differing from human interpretation (year rightarrow month rightarrow day).",
            "score": 2,
            "issue_id": 3920,
            "pub_date": "2025-05-22",
            "pub_date_card": {
                "ru": "22 Ğ¼Ğ°Ñ",
                "en": "May 22",
                "zh": "5æœˆ22æ—¥"
            },
            "hash": "e95fb5fdc583b428",
            "authors": [
                "Gagan Bhatia",
                "Maxime Peyrard",
                "Wei Zhao"
            ],
            "affiliations": [
                "University of Aberdeen",
                "UniversitÃ© Grenoble Alpes & CNRS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16088.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#data",
                    "#dataset",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "ğŸ—“ï¸",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ñ‚ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ 'ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ñ‚' Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DateAugBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ğ´Ğ°Ñ‚ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ°Ñ‚. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ½Ğ¸Ğ¶Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°Ñ…, Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸ĞµĞ¹ Ğ´Ğ°Ñ‚."
                },
                "en": {
                    "title": "Preserving Date Integrity for Better Temporal Reasoning",
                    "desc": "This paper addresses the issue of how modern Byte Pair Encoding (BPE) tokenizers break down calendar dates into smaller parts, which can hinder effective temporal reasoning in language models. The authors propose a new metric called the date fragmentation ratio to evaluate how well tokenizers maintain the integrity of multi-digit date components. They also introduce DateAugBench, a dataset designed for testing temporal reasoning across various tasks involving dates. The findings reveal that excessive fragmentation can lead to significant drops in accuracy, especially for less common dates, and that larger models are better at reconstructing these fragmented dates for reasoning tasks."
                },
                "zh": {
                    "title": "æå‡æ—¥æœŸæ¨ç†çš„åˆ†è¯å™¨è®¾è®¡",
                    "desc": "ç°ä»£çš„BPEåˆ†è¯å™¨å¸¸å¸¸å°†æ—¥æœŸåˆ†å‰²æˆæ— æ„ä¹‰çš„ç¢ç‰‡ï¼Œä¾‹å¦‚å°†20250312åˆ†å‰²ä¸º202ã€503å’Œ12ï¼Œè¿™æ ·ä¼šå¢åŠ æ ‡è®°æ•°é‡å¹¶æ©ç›–è¿›è¡Œæ—¶é—´æ¨ç†æ‰€éœ€çš„å†…åœ¨ç»“æ„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•ä¸”å¯è§£é‡Šçš„åº¦é‡æ ‡å‡†ï¼Œç§°ä¸ºæ—¥æœŸç¢ç‰‡åŒ–æ¯”ç‡ï¼Œç”¨äºè¡¡é‡åˆ†è¯å™¨ä¿ç•™å¤šä½æ•°æ—¥æœŸç»„ä»¶çš„å¿ å®åº¦ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†DateAugBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«6500ä¸ªç¤ºä¾‹çš„å¥—ä»¶ï¼Œæ¶µç›–äº†ä¸‰ç§æ—¶é—´æ¨ç†ä»»åŠ¡ï¼šåŸºäºä¸Šä¸‹æ–‡çš„æ—¥æœŸè§£æã€æ ¼å¼ä¸å˜æ€§éš¾é¢˜å’Œå†å²ã€å½“ä»£åŠæœªæ¥çš„æ—¥æœŸç®—æœ¯ã€‚å®éªŒè¡¨æ˜ï¼Œè¿‡åº¦çš„ç¢ç‰‡åŒ–ä¸ä¸å¸¸è§æ—¥æœŸï¼ˆå¦‚å†å²å’Œæœªæ¥æ—¥æœŸï¼‰çš„å‡†ç¡®æ€§ä¸‹é™é«˜è¾¾10ä¸ªç™¾åˆ†ç‚¹ç›¸å…³ï¼Œä¸”æ¨¡å‹è¶Šå¤§ï¼Œä¿®å¤æ—¥æœŸç¢ç‰‡çš„æŠ½è±¡æœºåˆ¶è¶Šå¿«ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15865",
            "title": "How Do Large Vision-Language Models See Text in Image? Unveiling the\n  Distinctive Role of OCR Heads",
            "url": "https://huggingface.co/papers/2505.15865",
            "abstract": "The study identifies and analyzes OCR Heads within Large Vision Language Models, revealing their unique activation patterns and roles in interpreting text within images.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite significant advancements in Large Vision Language Models (LVLMs), a gap remains, particularly regarding their interpretability and how they locate and interpret textual information within images. In this paper, we explore various LVLMs to identify the specific heads responsible for recognizing text from images, which we term the Optical Character Recognition Head (OCR Head). Our findings regarding these heads are as follows: (1) Less Sparse: Unlike previous retrieval heads, a large number of heads are activated to extract textual information from images. (2) Qualitatively Distinct: OCR heads possess properties that differ significantly from general retrieval heads, exhibiting low similarity in their characteristics. (3) Statically Activated: The frequency of activation for these heads closely aligns with their OCR scores. We validate our findings in downstream tasks by applying Chain-of-Thought (CoT) to both OCR and conventional retrieval heads and by masking these heads. We also demonstrate that redistributing sink-token values within the OCR heads improves performance. These insights provide a deeper understanding of the internal mechanisms LVLMs employ in processing embedded textual information in images.",
            "score": 2,
            "issue_id": 3919,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "4cf7128eaead82ce",
            "authors": [
                "Ingeol Baek",
                "Hwan Chang",
                "Sunghyun Ryu",
                "Hwanhee Lee"
            ],
            "affiliations": [
                "Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea",
                "Department of Computer Engineering, Sejong University, Seoul, Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.15865.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#interpretability",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞµĞºÑ€ĞµÑ‚Ğ¾Ğ² OCR Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ OCR-ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LVLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ OCR-ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¼ĞµĞ½ĞµĞµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ñ‹, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT) Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ OCR-ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Unlocking Text Recognition in Images with OCR Heads",
                    "desc": "This paper investigates the Optical Character Recognition (OCR) Heads in Large Vision Language Models (LVLMs) to understand how they process text in images. It reveals that these heads are less sparse, meaning many of them activate simultaneously to extract text, unlike traditional retrieval heads. The study also finds that OCR heads have distinct properties, showing low similarity to general retrieval heads, and their activation frequency correlates with their OCR performance. By applying techniques like Chain-of-Thought and redistributing values within these heads, the research enhances the models' ability to interpret text in images, shedding light on their internal workings."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„OCRå¤´",
                    "desc": "æœ¬ç ”ç©¶åˆ†æäº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å…‰å­¦å­—ç¬¦è¯†åˆ«å¤´ï¼ˆOCRå¤´ï¼‰ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨å›¾åƒä¸­æ–‡æœ¬è§£è¯»ä¸­çš„ç‹¬ç‰¹æ¿€æ´»æ¨¡å¼å’Œè§’è‰²ã€‚å°½ç®¡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¯è§£é‡Šæ€§æ–¹é¢ä»å­˜åœ¨å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨å®šä½å’Œè§£è¯»å›¾åƒä¸­çš„æ–‡æœ¬ä¿¡æ¯æ—¶ã€‚æˆ‘ä»¬å‘ç°ï¼ŒOCRå¤´çš„æ¿€æ´»æ–¹å¼ä¸ä¼ ç»Ÿçš„æ£€ç´¢å¤´æ˜¾è‘—ä¸åŒï¼Œä¸”åœ¨æå–æ–‡æœ¬ä¿¡æ¯æ—¶æ¿€æ´»çš„å¤´æ•°é‡è¾ƒå¤šã€‚é€šè¿‡å¯¹è¿™äº›å¤´çš„ç ”ç©¶ï¼Œæˆ‘ä»¬åŠ æ·±äº†å¯¹LVLMsåœ¨å¤„ç†å›¾åƒä¸­åµŒå…¥æ–‡æœ¬ä¿¡æ¯æ—¶å†…éƒ¨æœºåˆ¶çš„ç†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14462",
            "title": "RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture\n  Understanding",
            "url": "https://huggingface.co/papers/2505.14462",
            "abstract": "RAVENEA, a retrieval-augmented benchmark, enhances visual culture understanding in VLMs through culture-focused tasks and outperforms non-augmented models across various metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t As vision-language models (VLMs) become increasingly integrated into daily life, the need for accurate visual culture understanding is becoming critical. Yet, these models frequently fall short in interpreting cultural nuances effectively. Prior work has demonstrated the effectiveness of retrieval-augmented generation (RAG) in enhancing cultural understanding in text-only settings, while its application in multimodal scenarios remains underexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented Visual culturE uNdErstAnding), a new benchmark designed to advance visual culture understanding through retrieval, focusing on two tasks: culture-focused visual question answering (cVQA) and culture-informed image captioning (cIC). RAVENEA extends existing datasets by integrating over 10,000 Wikipedia documents curated and ranked by human annotators. With RAVENEA, we train and evaluate seven multimodal retrievers for each image query, and measure the downstream impact of retrieval-augmented inputs across fourteen state-of-the-art VLMs. Our results show that lightweight VLMs, when augmented with culture-aware retrieval, outperform their non-augmented counterparts (by at least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the value of retrieval-augmented methods and culturally inclusive benchmarks for multimodal understanding.",
            "score": 2,
            "issue_id": 3926,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "49a96a5bbddf2965",
            "authors": [
                "Jiaang Li",
                "Yifei Yuan",
                "Wenyan Li",
                "Mohammad Aliannejadi",
                "Daniel Hershcovich",
                "Anders SÃ¸gaard",
                "Ivan VuliÄ‡",
                "Wenxuan Zhang",
                "Paul Pu Liang",
                "Yang Deng",
                "Serge Belongie"
            ],
            "affiliations": [
                "ETH ZÃ¼rich",
                "Massachusetts Institute of Technology",
                "Singapore Management University",
                "Singapore University of Technology and Design",
                "University of Amsterdam",
                "University of Cambridge",
                "University of Copenhagen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14462.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#transfer_learning",
                    "#interpretability",
                    "#multimodal",
                    "#rag",
                    "#games",
                    "#benchmark"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞšÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "RAVENEA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ñ‹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğµ Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ (cVQA) Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° (cIC). Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 10 000 Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸, Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 3.2% Ğ² cVQA Ğ¸ 6.2% Ğ² cIC."
                },
                "en": {
                    "title": "Enhancing Visual Culture Understanding with RAVENEA",
                    "desc": "RAVENEA is a new benchmark designed to improve visual culture understanding in vision-language models (VLMs) through retrieval-augmented techniques. It focuses on two main tasks: culture-focused visual question answering (cVQA) and culture-informed image captioning (cIC). By integrating over 10,000 curated Wikipedia documents, RAVENEA enhances the training and evaluation of multimodal retrievers for image queries. The results show that VLMs using this retrieval-augmented approach significantly outperform those that do not, demonstrating the importance of culturally aware data in machine learning."
                },
                "zh": {
                    "title": "RAVENEAï¼šæå‡è§†è§‰æ–‡åŒ–ç†è§£çš„æ£€ç´¢å¢å¼ºåŸºå‡†",
                    "desc": "RAVENEAæ˜¯ä¸€ä¸ªå¢å¼ºè§†è§‰æ–‡åŒ–ç†è§£çš„åŸºå‡†ï¼Œä¸“æ³¨äºæ–‡åŒ–ç›¸å…³çš„ä»»åŠ¡ï¼Œæå‡äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¡¨ç°ã€‚è¯¥åŸºå‡†é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œè§£å†³äº†æ¨¡å‹åœ¨ç†è§£æ–‡åŒ–ç»†å¾®å·®åˆ«æ–¹é¢çš„ä¸è¶³ã€‚RAVENEAåŒ…å«è¶…è¿‡10,000ä¸ªç»è¿‡äººå·¥æ ‡æ³¨å’Œæ’åçš„ç»´åŸºç™¾ç§‘æ–‡æ¡£ï¼Œæ”¯æŒæ–‡åŒ–èšç„¦çš„è§†è§‰é—®ç­”å’Œå›¾åƒæè¿°ä»»åŠ¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡æ–‡åŒ–æ„è¯†æ£€ç´¢å¢å¼ºçš„è½»é‡çº§VLMsåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºæœªå¢å¼ºçš„æ¨¡å‹ï¼Œæ˜¾ç¤ºäº†æ£€ç´¢å¢å¼ºæ–¹æ³•åœ¨å¤šæ¨¡æ€ç†è§£ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.14395",
            "title": "MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation\n  Capabilities in Any Language",
            "url": "https://huggingface.co/papers/2505.14395",
            "abstract": "MUG-Eval assesses LLMs' multilingual generation by transforming benchmarks into conversational tasks, offering a language-independent and NLP tool-free method that correlates well with established benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating text generation capabilities of large language models (LLMs) is challenging, particularly for low-resource languages where methods for direct assessment are scarce. We propose MUG-Eval, a novel framework that evaluates LLMs' multilingual generation capabilities by transforming existing benchmarks into conversational tasks and measuring the LLMs' accuracies on those tasks. We specifically designed these conversational tasks to require effective communication in the target language. Then, we simply use task success rate as a proxy of successful conversation generation. Our approach offers two key advantages: it is independent of language-specific NLP tools or annotated datasets, which are limited for most languages, and it does not rely on LLMs-as-judges, whose evaluation quality degrades outside a few high-resource languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and low-resource categories, and we find that MUG-Eval correlates strongly with established benchmarks (r > 0.75) while enabling standardized comparisons across languages and models. Our framework provides a robust and resource-efficient solution for evaluating multilingual generation that can be extended to thousands of languages.",
            "score": 2,
            "issue_id": 3921,
            "pub_date": "2025-05-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ",
                "en": "May 20",
                "zh": "5æœˆ20æ—¥"
            },
            "hash": "88fb6eaee50e6cb2",
            "authors": [
                "Seyoung Song",
                "Seogyeong Jeong",
                "Eunsu Kim",
                "Jiho Jin",
                "Dongkwan Kim",
                "Jay Shin",
                "Alice Oh"
            ],
            "affiliations": [
                "KAIST",
                "Trillion Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.14395.jpg",
            "data": {
                "categories": [
                    "#machine_translation",
                    "#benchmark",
                    "#low_resource",
                    "#multilingual"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… LLM Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "MUG-Eval - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ğ¿Ñ€Ğ¸ Ğ¸Ñ… Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ÑĞ·Ñ‹ĞºĞ¾ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ LLM. MUG-Eval Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "MUG-Eval: A Language-Independent Tool for Multilingual LLM Assessment",
                    "desc": "MUG-Eval is a new framework designed to evaluate the multilingual generation capabilities of large language models (LLMs). It transforms existing benchmarks into conversational tasks, allowing for a language-independent assessment of LLM performance. By measuring the success rate of these tasks, MUG-Eval provides a proxy for effective conversation generation without relying on specific NLP tools or annotated datasets. The framework has been tested on 8 LLMs across 30 languages and shows strong correlation with established benchmarks, making it a valuable tool for evaluating LLMs in low-resource languages."
                },
                "zh": {
                    "title": "MUG-Evalï¼šå¤šè¯­è¨€ç”Ÿæˆçš„è¯„ä¼°æ–°æ–¹æ³•",
                    "desc": "MUG-Evalæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤šè¯­è¨€ç”Ÿæˆèƒ½åŠ›çš„æ–°æ¡†æ¶ã€‚å®ƒé€šè¿‡å°†ç°æœ‰åŸºå‡†è½¬åŒ–ä¸ºå¯¹è¯ä»»åŠ¡ï¼Œæ¥æµ‹é‡LLMsåœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºç‰¹å®šè¯­è¨€çš„è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·æˆ–æ ‡æ³¨æ•°æ®é›†ï¼Œé€‚ç”¨äºå¤šç§è¯­è¨€ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒMUG-Evalä¸å·²æœ‰åŸºå‡†çš„ç›¸å…³æ€§å¾ˆå¼ºï¼Œèƒ½å¤Ÿä¸ºå¤šè¯­è¨€ç”Ÿæˆæä¾›æ ‡å‡†åŒ–çš„æ¯”è¾ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13344",
            "title": "RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE\n  Optimization on Diffusion Transformers",
            "url": "https://huggingface.co/papers/2505.13344",
            "abstract": "RoPECraft is a training-free method that modifies rotary positional embeddings in diffusion transformers to transfer motion from reference videos, enhancing text-guided video generation and reducing artifacts.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose RoPECraft, a training-free video motion transfer method for diffusion transformers that operates solely by modifying their rotary positional embeddings (RoPE). We first extract dense optical flow from a reference video, and utilize the resulting motion offsets to warp the complex-exponential tensors of RoPE, effectively encoding motion into the generation process. These embeddings are then further optimized during denoising time steps via trajectory alignment between the predicted and target velocities using a flow-matching objective. To keep the output faithful to the text prompt and prevent duplicate generations, we incorporate a regularization term based on the phase components of the reference video's Fourier transform, projecting the phase angles onto a smooth manifold to suppress high-frequency artifacts. Experiments on benchmarks reveal that RoPECraft outperforms all recently published methods, both qualitatively and quantitatively.",
            "score": 2,
            "issue_id": 3932,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "e47accfd208862f5",
            "authors": [
                "Ahmet Berke Gokmen",
                "Yigit Ekin",
                "Bahri Batuhan Bilecen",
                "Aysegul Dundar"
            ],
            "affiliations": [
                "Bilkent University, Department of Computer Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13344.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#optimization",
                    "#diffusion",
                    "#multimodal",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞŸĞµÑ€ĞµĞ½Ğ¾Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "RoPECraft - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² RoPE Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸Ğ· Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑˆĞ°Ğ³Ğ¾Ğ² ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¤ÑƒÑ€ÑŒĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RoPECraft Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾, Ñ‚Ğ°Ğº Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾."
                },
                "en": {
                    "title": "Seamless Motion Transfer in Video Generation with RoPECraft",
                    "desc": "RoPECraft is a novel method that enhances video generation by transferring motion from reference videos without requiring additional training. It modifies rotary positional embeddings (RoPE) in diffusion transformers to incorporate motion information extracted from dense optical flow. The method aligns predicted and target velocities during the denoising process, ensuring that the generated video matches the intended motion. Additionally, it uses a regularization technique based on Fourier transform phase components to maintain fidelity to text prompts and minimize artifacts in the output."
                },
                "zh": {
                    "title": "RoPECraftï¼šæ— è®­ç»ƒçš„è§†é¢‘è¿åŠ¨è½¬ç§»æ–°æ–¹æ³•",
                    "desc": "RoPECraftæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡ä¿®æ”¹æ‰©æ•£å˜æ¢å™¨ä¸­çš„æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰æ¥å®ç°è§†é¢‘è¿åŠ¨è½¬ç§»ã€‚è¯¥æ–¹æ³•é¦–å…ˆä»å‚è€ƒè§†é¢‘ä¸­æå–å¯†é›†å…‰æµï¼Œå¹¶åˆ©ç”¨è¿åŠ¨åç§»é‡æ¥æ‰­æ›²RoPEçš„å¤æŒ‡æ•°å¼ é‡ï¼Œä»è€Œå°†è¿åŠ¨ä¿¡æ¯ç¼–ç åˆ°ç”Ÿæˆè¿‡ç¨‹ä¸­ã€‚åœ¨å»å™ªæ—¶é—´æ­¥éª¤ä¸­ï¼Œé€šè¿‡ä½¿ç”¨æµåŒ¹é…ç›®æ ‡å¯¹é¢„æµ‹é€Ÿåº¦å’Œç›®æ ‡é€Ÿåº¦è¿›è¡Œè½¨è¿¹å¯¹é½ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–è¿™äº›åµŒå…¥ã€‚ä¸ºäº†ä¿æŒè¾“å‡ºä¸æ–‡æœ¬æç¤ºçš„ä¸€è‡´æ€§å¹¶é˜²æ­¢é‡å¤ç”Ÿæˆï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå‚è€ƒè§†é¢‘å‚…é‡Œå¶å˜æ¢ç›¸ä½åˆ†é‡çš„æ­£åˆ™åŒ–é¡¹ï¼Œä»¥æŠ‘åˆ¶é«˜é¢‘ä¼ªå½±ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.16048",
            "title": "SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution",
            "url": "https://huggingface.co/papers/2505.16048",
            "abstract": "A dataset benchmarks spatial and physical reasoning of LLMs using topology optimization tasks without simulation tools.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a novel dataset designed to benchmark the physical and spatial reasoning capabilities of Large Language Models (LLM) based on topology optimization, a method for computing optimal material distributions within a design space under prescribed loads and supports. In this dataset, LLMs are provided with conditions such as 2D boundary, applied forces and supports, and must reason about the resulting optimal material distribution. The dataset includes a variety of tasks, ranging from filling in masked regions within partial structures to predicting complete material distributions. Solving these tasks requires understanding the flow of forces and the required material distribution under given constraints, without access to simulation tools or explicit physical models, challenging models to reason about structural stability and spatial organization. Our dataset targets the evaluation of spatial and physical reasoning abilities in 2D settings, offering a complementary perspective to traditional language and logic benchmarks.",
            "score": 1,
            "issue_id": 3920,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "cf44ff3c901f7498",
            "authors": [
                "Philipp D. Siedler"
            ],
            "affiliations": [
                "Aleph Alpha Research, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.16048.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ—ï¸",
                "ru": {
                    "title": "Ğ˜ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ° Ğ² 2D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ°Ñ… Ğ¸ Ğ¾Ğ¿Ğ¾Ñ€Ğ°Ñ…. Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ² Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Benchmarking LLMs in Spatial and Physical Reasoning with Topology Optimization",
                    "desc": "This paper presents a new dataset aimed at evaluating the spatial and physical reasoning skills of Large Language Models (LLMs) through topology optimization tasks. The tasks require LLMs to analyze conditions like 2D boundaries and applied forces to determine optimal material distributions without using simulation tools. By challenging models to predict material layouts and understand force flows, the dataset assesses their ability to reason about structural stability and spatial organization. This approach provides a unique perspective on LLM performance, complementing existing benchmarks focused on language and logic."
                },
                "zh": {
                    "title": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„ç©ºé—´ä¸ç‰©ç†æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ‹“æ‰‘ä¼˜åŒ–ä»»åŠ¡ä¸­çš„ç©ºé—´å’Œç‰©ç†æ¨ç†èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†æä¾›äº†2Dè¾¹ç•Œã€æ–½åŠ çš„åŠ›å’Œæ”¯æ’‘æ¡ä»¶ï¼ŒLLMéœ€è¦æ¨ç†å‡ºæœ€ä½³çš„ææ–™åˆ†å¸ƒã€‚ä»»åŠ¡åŒ…æ‹¬å¡«è¡¥éƒ¨åˆ†ç»“æ„ä¸­çš„ç¼ºå¤±åŒºåŸŸå’Œé¢„æµ‹å®Œæ•´çš„ææ–™åˆ†å¸ƒï¼Œè¦æ±‚æ¨¡å‹ç†è§£åœ¨ç»™å®šçº¦æŸä¸‹çš„åŠ›æµå’Œææ–™åˆ†å¸ƒã€‚è¿™ä¸ªæ•°æ®é›†ä¸ºè¯„ä¼°2Dç¯å¢ƒä¸­çš„ç©ºé—´å’Œç‰©ç†æ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„è§†è§’ï¼Œè¡¥å……äº†ä¼ ç»Ÿçš„è¯­è¨€å’Œé€»è¾‘åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.15263",
            "title": "gen2seg: Generative Models Enable Generalizable Instance Segmentation",
            "url": "https://huggingface.co/papers/2505.15263",
            "abstract": "Generative models fine-tuned for instance segmentation demonstrate strong zero-shot performance on unseen objects and styles, surpassing discriminatively pretrained models.  \t\t\t\t\tAI-generated summary \t\t\t\t By pretraining to synthesize coherent images from perturbed inputs, generative models inherently learn to understand object boundaries and scene compositions. How can we repurpose these generative representations for general-purpose perceptual organization? We finetune Stable Diffusion and MAE (encoder+decoder) for category-agnostic instance segmentation using our instance coloring loss exclusively on a narrow set of object types (indoor furnishings and cars). Surprisingly, our models exhibit strong zero-shot generalization, accurately segmenting objects of types and styles unseen in finetuning (and in many cases, MAE's ImageNet-1K pretraining too). Our best-performing models closely approach the heavily supervised SAM when evaluated on unseen object types and styles, and outperform it when segmenting fine structures and ambiguous boundaries. In contrast, existing promptable segmentation architectures or discriminatively pretrained models fail to generalize. This suggests that generative models learn an inherent grouping mechanism that transfers across categories and domains, even without internet-scale pretraining. Code, pretrained models, and demos are available on our website.",
            "score": 1,
            "issue_id": 3936,
            "pub_date": "2025-05-21",
            "pub_date_card": {
                "ru": "21 Ğ¼Ğ°Ñ",
                "en": "May 21",
                "zh": "5æœˆ21æ—¥"
            },
            "hash": "01de6ec5d8c92cb4",
            "authors": [
                "Om Khangaonkar",
                "Hamed Pirsiavash"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2505.15263.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#transfer_learning",
                    "#cv",
                    "#synthetic"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ¹. Ğ¢Ğ°ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ supervised Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ²Ñ€Ğ¾Ğ´Ğµ SAM. Ğ­Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞ²Ğ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹Ğµ Ğº Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼."
                },
                "en": {
                    "title": "Generative Models Excel in Zero-Shot Instance Segmentation",
                    "desc": "This paper explores how generative models, specifically those fine-tuned for instance segmentation, can effectively identify and segment objects that were not seen during training. By leveraging a technique called instance coloring loss, the authors adapt models like Stable Diffusion and MAE to work with a limited set of object types, such as indoor furniture and cars. Remarkably, these models demonstrate strong zero-shot performance, meaning they can accurately segment new object types and styles that they have never encountered before. The findings suggest that generative models possess a robust understanding of object boundaries and scene composition, allowing them to generalize better than traditional discriminative models."
                },
                "zh": {
                    "title": "ç”Ÿæˆæ¨¡å‹åœ¨å®ä¾‹åˆ†å‰²ä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç”Ÿæˆæ¨¡å‹åœ¨å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§ç‰©ä½“å’Œé£æ ¼ä¸Šçš„é›¶-shotæ€§èƒ½è¡¨ç°ã€‚é€šè¿‡å¯¹ç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç ”ç©¶è€…ä»¬å‘ç°è¿™äº›æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆç†è§£ç‰©ä½“è¾¹ç•Œå’Œåœºæ™¯æ„æˆã€‚è®ºæ–‡ä¸­ä½¿ç”¨äº†å®ä¾‹ç€è‰²æŸå¤±ï¼Œä¸“æ³¨äºå®¤å†…å®¶å…·å’Œæ±½è½¦ç­‰ç‰¹å®šç‰©ä½“ç±»å‹ï¼Œç»“æœæ˜¾ç¤ºæ¨¡å‹åœ¨æœªè§ç‰©ä½“ä¸Šä¹Ÿèƒ½å‡†ç¡®åˆ†å‰²ã€‚ä¸ä¼ ç»Ÿçš„åˆ¤åˆ«å¼é¢„è®­ç»ƒæ¨¡å‹ç›¸æ¯”ï¼Œç”Ÿæˆæ¨¡å‹å±•ç°å‡ºæ›´å¼ºçš„è·¨ç±»åˆ«å’Œé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2505.13237",
            "title": "SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based\n  on Speech and Audio Information",
            "url": "https://huggingface.co/papers/2505.13237",
            "abstract": "SAKURA is introduced to evaluate the multi-hop reasoning abilities of large audio-language models, revealing their struggles in integrating speech/audio representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large audio-language models (LALMs) extend the large language models with multimodal understanding in speech, audio, etc. While their performances on speech and audio-processing tasks are extensively studied, their reasoning abilities remain underexplored. Particularly, their multi-hop reasoning, the ability to recall and integrate multiple facts, lacks systematic evaluation. Existing benchmarks focus on general speech and audio-processing tasks, conversational abilities, and fairness but overlook this aspect. To bridge this gap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning based on speech and audio information. Results show that LALMs struggle to integrate speech/audio representations for multi-hop reasoning, even when they extract the relevant information correctly, highlighting a fundamental challenge in multimodal reasoning. Our findings expose a critical limitation in LALMs, offering insights and resources for future research.",
            "score": 0,
            "issue_id": 3928,
            "pub_date": "2025-05-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ",
                "en": "May 19",
                "zh": "5æœˆ19æ—¥"
            },
            "hash": "657c71fe76ca3155",
            "authors": [
                "Chih-Kai Yang",
                "Neo Ho",
                "Yen-Ting Piao",
                "Hung-yi Lee"
            ],
            "affiliations": [
                "National Taiwan University, Taiwan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2505.13237.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#multimodal",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "SAKURA: Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "SAKURA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LALM) Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ LALM Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ², Ğ´Ğ°Ğ¶Ğµ ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾Ğ½Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ñ‚Ğ°ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ insights Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğº Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "SAKURA: Unveiling the Reasoning Gaps in Audio-Language Models",
                    "desc": "The paper introduces SAKURA, a benchmark designed to evaluate the multi-hop reasoning capabilities of large audio-language models (LALMs). It highlights that while LALMs perform well in speech and audio tasks, their ability to integrate multiple pieces of information for reasoning is not well understood. The study reveals that LALMs face significant challenges in combining speech and audio representations for effective multi-hop reasoning. This research identifies a key limitation in LALMs and provides valuable insights for future advancements in multimodal reasoning."
                },
                "zh": {
                    "title": "SAKURAï¼šè¯„ä¼°éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„å¤šè·³æ¨ç†èƒ½åŠ›",
                    "desc": "SAKURAæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨å¤šè·³æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹åœ¨å¤„ç†è¯­éŸ³å’ŒéŸ³é¢‘ä»»åŠ¡æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ•´åˆå¤šæ¡ä¿¡æ¯è¿›è¡Œæ¨ç†æ—¶å´å­˜åœ¨å›°éš¾ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿å®ƒä»¬èƒ½å¤Ÿæ­£ç¡®æå–ç›¸å…³ä¿¡æ¯ï¼Œä»ç„¶éš¾ä»¥å°†è¯­éŸ³å’ŒéŸ³é¢‘è¡¨ç¤ºç»“åˆèµ·æ¥è¿›è¡Œå¤šè·³æ¨ç†ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†å¤šæ¨¡æ€æ¨ç†ä¸­çš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼çš„è§è§£å’Œèµ„æºã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-05-22.html",
    "link_next": "2025-05-26.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "22.05",
        "en": "05/22",
        "zh": "5æœˆ22æ—¥"
    },
    "short_date_next": {
        "ru": "26.05",
        "en": "05/26",
        "zh": "5æœˆ26æ—¥"
    },
    "categories": {
        "#dataset": 15,
        "#data": 5,
        "#benchmark": 24,
        "#agents": 4,
        "#cv": 9,
        "#rl": 12,
        "#rlhf": 5,
        "#rag": 2,
        "#plp": 0,
        "#inference": 3,
        "#3d": 3,
        "#audio": 1,
        "#video": 4,
        "#multimodal": 23,
        "#math": 4,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 1,
        "#training": 27,
        "#robotics": 1,
        "#agi": 1,
        "#games": 8,
        "#interpretability": 5,
        "#reasoning": 25,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 23,
        "#survey": 1,
        "#diffusion": 7,
        "#alignment": 5,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 2,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 2
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å¦‚ä½•åŠ é€Ÿç§‘ç ”èŒƒå¼çš„è½¬å˜ï¼Œæé«˜ç ”ç©¶æ•ˆç‡å¹¶æ¨åŠ¨åˆ›æ–°ã€‚ä½œè€…æå‡ºäº†NovelSeekï¼Œä¸€ä¸ªç»Ÿä¸€çš„é—­ç¯å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºåœ¨å„ç§ç§‘ç ”é¢†åŸŸä¸­è¿›è¡Œè‡ªä¸»ç§‘å­¦ç ”ç©¶ï¼ˆASRï¼‰ã€‚NovelSeekæœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼šå¯æ‰©å±•æ€§ã€äº’åŠ¨æ€§å’Œé«˜æ•ˆæ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨ååº”äº§ç‡é¢„æµ‹ä¸­ï¼Œå®ƒåœ¨12å°æ—¶å†…å°†å‡†ç¡®ç‡ä»27.6%æé«˜åˆ°35.4%ï¼›åœ¨å¢å¼ºæ´»æ€§é¢„æµ‹ä¸­ï¼Œå‡†ç¡®ç‡åœ¨4å°æ—¶å†…ä»0.52æé«˜åˆ°0.79ï¼›åœ¨2Dè¯­ä¹‰åˆ†å‰²ä¸­ï¼Œç²¾åº¦åœ¨30å°æ—¶å†…ä»78.8%æé«˜åˆ°81.0%ã€‚",
        "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop\n  System from Hypothesis to Verification",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng jiÃ¨shÃ o le rÃ©ngÅng zhÃ¬nÃ©ng (AI) rÃºhÃ© jiÄsÃ¹ kÄ“yÃ¡n fÃ nshÃ¬ de zhuÇnbiÃ n, tÃ­gÄo yÃ¡njiÅ« xiÃ olÇœ bÃ¬ng tuÄ«dÃ²ng chuÃ ngxÄ«n. ZuÃ²zhÄ› tÃ­chÅ« le NovelSeek, yÄ«gÃ¨ tÇ’ngyÄ« de bÃ¬huÃ¡n duÅ zhÃ¬nÃ©ngtÇ kuÄngjiÃ , yÃ²ngyÃº zÃ i gÃ¨zhÇ’ng kÄ“yÃ¡n lÇngyÃ¹ zhÅng jÃ¬nxÃ­ng zÃ¬zhÇ” kÄ“xuÃ© yÃ¡njiÅ« (ASR). NovelSeek yÇ’u sÄn dÃ  yÇ’ushÃ¬: kÄ› kuÃ²zhÃ nxÃ¬ng, hÃ¹dÃ²ngxÃ¬ng hÃ© gÄoxiÃ oxÃ¬ng. LÃ¬rÃº, zÃ i fÇnyÃ¬ng chÇnlÇœ yÃ¹cÃ¨ zhÅng, tÄ zÃ i 12 xiÇoshÃ­ nÃ¨i jiÄng zhÇ”nquÃ¨lÇœ cÃ³ng 27.6% tÃ­gÄo dÃ o 35.4%; zÃ i zÄ“ngqiÃ¡ng huÃ³xÃ¬ng yÃ¹cÃ¨ zhÅng, zhÇ”nquÃ¨lÇœ zÃ i 4 xiÇoshÃ­ nÃ¨i cÃ³ng 0.52 tÃ­gÄo dÃ o 0.79; zÃ i 2D yÇ”yÃ¬ fÄ“ngÄ“ zhÅng, jÄ«ngdÃ¹ zÃ i 30 xiÇoshÃ­ nÃ¨i cÃ³ng 78.8% tÃ­gÄo dÃ o 81.0%.",
        "vocab": "[{'word': 'äººå·¥æ™ºèƒ½', 'pinyin': 'rÃ©ngÅng zhÃ¬nÃ©ng', 'trans': 'artificial intelligence'},\n{'word': 'åŠ é€Ÿ', 'pinyin': 'jiÄsÃ¹', 'trans': 'accelerate'},\n{'word': 'ç§‘ç ”', 'pinyin': 'kÄ“yÃ¡n', 'trans': 'scientific research'},\n{'word': 'èŒƒå¼', 'pinyin': 'fÃ nshÃ¬', 'trans': 'paradigm'},\n{'word': 'è½¬å˜', 'pinyin': 'zhuÇnbiÃ n', 'trans': 'transformation'},\n{'word': 'æé«˜', 'pinyin': 'tÃ­gÄo', 'trans': 'improve'},\n{'word': 'æ•ˆç‡', 'pinyin': 'xiÃ olÇœ', 'trans': 'efficiency'},\n{'word': 'æ¨åŠ¨', 'pinyin': 'tuÄ«dÃ²ng', 'trans': 'promote'},\n{'word': 'åˆ›æ–°', 'pinyin': 'chuÃ ngxÄ«n', 'trans': 'innovation'},\n{'word': 'æå‡º', 'pinyin': 'tÃ­chÅ«', 'trans': 'propose'},\n{'word': 'ç»Ÿä¸€', 'pinyin': 'tÇ’ngyÄ«', 'trans': 'unified'},\n{'word': 'é—­ç¯', 'pinyin': 'bÃ¬huÃ¡n', 'trans': 'closed-loop'},\n{'word': 'å¤šæ™ºèƒ½ä½“', 'pinyin': 'duÅ zhÃ¬nÃ©ngtÇ', 'trans': 'multi-agent'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ngjiÃ ', 'trans': 'framework'},\n{'word': 'è‡ªä¸»', 'pinyin': 'zÃ¬zhÇ”', 'trans': 'autonomous'},\n{'word': 'ç§‘å­¦ç ”ç©¶', 'pinyin': 'kÄ“xuÃ© yÃ¡njiÅ«', 'trans': 'scientific research'},\n{'word': 'é¢†åŸŸ', 'pinyin': 'lÇngyÃ¹', 'trans': 'field'},\n{'word': 'ä¼˜åŠ¿', 'pinyin': 'yÅushÃ¬', 'trans': 'advantage'},\n{'word': 'å¯æ‰©å±•æ€§', 'pinyin': 'kÄ› kuÃ²zhÃ n xÃ¬ng', 'trans': 'scalability'},\n{'word': 'äº’åŠ¨æ€§', 'pinyin': 'hÃ¹dÃ²ng xÃ¬ng', 'trans': 'interactivity'},\n{'word': 'é«˜æ•ˆæ€§', 'pinyin': 'gÄoxiÃ o xÃ¬ng', 'trans': 'efficiency'},\n{'word': 'ä¾‹å¦‚', 'pinyin': 'lÃ¬rÃº', 'trans': 'for example'},\n{'word': 'ååº”', 'pinyin': 'fÇnyÃ¬ng', 'trans': 'reaction'},\n{'word': 'äº§ç‡', 'pinyin': 'chÇnlÇœ', 'trans': 'yield'},\n{'word': 'é¢„æµ‹', 'pinyin': 'yÃ¹cÃ¨', 'trans': 'prediction'},\n{'word': 'å‡†ç¡®ç‡', 'pinyin': 'zhÇ”nquÃ¨lÇœ', 'trans': 'accuracy'},\n{'word': 'å¢å¼º', 'pinyin': 'zÄ“ngqiÃ¡ng', 'trans': 'enhance'},\n{'word': 'æ´»æ€§', 'pinyin': 'huÃ³xÃ¬ng', 'trans': 'activity'},\n{'word': 'ç²¾åº¦', 'pinyin': 'jÄ«ngdÃ¹', 'trans': 'precision'},\n{'word': 'è¯­ä¹‰', 'pinyin': 'yÇ”yÃ¬', 'trans': 'semantic'},\n{'word': 'åˆ†å‰²', 'pinyin': 'fÄ“ngÄ“', 'trans': 'segmentation'}]",
        "trans": "This article discusses how artificial intelligence (AI) is accelerating the transformation of scientific research paradigms, enhancing research efficiency, and driving innovation. The author introduces NovelSeek, a unified closed-loop multi-agent framework for autonomous scientific research (ASR) across various scientific domains. NovelSeek offers three key advantages: scalability, interactivity, and efficiency. For instance, in reaction yield prediction, it improved accuracy from 27.6% to 35.4% within 12 hours; in enhanced activity prediction, accuracy increased from 0.52 to 0.79 within 4 hours; and in 2D semantic segmentation, precision improved from 78.8% to 81.0% within 30 hours.",
        "update_ts": "2025-05-23 09:12"
    }
}