{
    "date": {
        "ru": "20 ноября",
        "en": "November 20",
        "zh": "11月20日"
    },
    "time_utc": "2024-11-20 16:12",
    "weekday": 2,
    "issue_id": 687,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.12372",
            "title": "RedPajama: an Open Dataset for Training Large Language Models",
            "url": "https://huggingface.co/papers/2411.12372",
            "abstract": "Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top-performing models lack transparency in their dataset curation and model development processes, posing an obstacle to the development of fully open language models. In this paper, we identify three core data-related challenges that must be addressed to advance open-source language models. These include (1) transparency in model development, including the data curation process, (2) access to large quantities of high-quality data, and (3) availability of artifacts and metadata for dataset curation and analysis. To address these challenges, we release RedPajama-V1, an open reproduction of the LLaMA training dataset. In addition, we release RedPajama-V2, a massive web-only dataset consisting of raw, unfiltered text data together with quality signals and metadata. Together, the RedPajama datasets comprise over 100 trillion tokens spanning multiple domains and with their quality signals facilitate the filtering of data, aiming to inspire the development of numerous new datasets. To date, these datasets have already been used in the training of strong language models used in production, such as Snowflake Arctic, Salesforce's XGen and AI2's OLMo. To provide insight into the quality of RedPajama, we present a series of analyses and ablation studies with decoder-only language models with up to 1.6B parameters. Our findings demonstrate how quality signals for web data can be effectively leveraged to curate high-quality subsets of the dataset, underscoring the potential of RedPajama to advance the development of transparent and high-performing language models at scale.",
            "score": 11,
            "issue_id": 682,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 ноября",
                "en": "November 19",
                "zh": "11月19日"
            },
            "hash": "9393337102332466",
            "authors": [
                "Maurice Weber",
                "Daniel Fu",
                "Quentin Anthony",
                "Yonatan Oren",
                "Shane Adams",
                "Anton Alexandrov",
                "Xiaozhong Lyu",
                "Huu Nguyen",
                "Xiaozhe Yao",
                "Virginia Adams",
                "Ben Athiwaratkun",
                "Rahul Chalamala",
                "Kezhen Chen",
                "Max Ryabinin",
                "Tri Dao",
                "Percy Liang",
                "Christopher Ré",
                "Irina Rish",
                "Ce Zhang"
            ],
            "affiliations": [
                "Caltech",
                "ETH Zurich",
                "EleutherAI",
                "Mila, Montréal, Canada",
                "Ohio State University",
                "Ontocord.ai",
                "Princeton University",
                "Stanford University",
                "Together AI",
                "University of Chicago",
                "Université de Montréal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12372.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#data",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "RedPajama: открытый путь к прозрачным и мощным языковым моделям",
                    "desc": "Статья посвящена проблемам создания открытых языковых моделей и представляет набор данных RedPajama для их обучения. Авторы выделяют три ключевых вызова: прозрачность разработки, доступ к качественным данным и наличие метаданных для курации датасетов. RedPajama-V1 воспроизводит датасет LLaMA, а RedPajama-V2 содержит необработанные веб-данные с сигналами качества. Исследования показывают, как эти сигналы могут быть использованы для создания высококачественных подмножеств данных."
                },
                "en": {
                    "title": "Advancing Open-Source Language Models with RedPajama Datasets",
                    "desc": "This paper discusses the importance of transparency and quality in the datasets used for training large language models. It identifies three main challenges: the need for clear data curation processes, access to high-quality data, and the availability of metadata for better dataset analysis. To tackle these issues, the authors introduce the RedPajama datasets, which include a comprehensive reproduction of the LLaMA training dataset and a large web-only dataset with quality signals. The findings highlight how these datasets can improve the development of open-source language models by providing high-quality data and insights into effective data curation practices."
                },
                "zh": {
                    "title": "推动开放语言模型的透明与高效",
                    "desc": "本论文探讨了大型语言模型在数据集构建和过滤方面的挑战，强调了透明度、数据质量和元数据可用性的重要性。我们发布了RedPajama-V1和RedPajama-V2数据集，旨在解决这些问题，提供高质量的开放数据。RedPajama数据集包含超过100万亿个标记，涵盖多个领域，并提供质量信号以帮助数据过滤。我们的研究表明，利用网络数据的质量信号可以有效地构建高质量的数据子集，推动透明且高效的语言模型的发展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.11925",
            "title": "Continuous Speculative Decoding for Autoregressive Image Generation",
            "url": "https://huggingface.co/papers/2411.11925",
            "abstract": "Continuous-valued Autoregressive (AR) image generation models have demonstrated notable superiority over their discrete-token counterparts, showcasing considerable reconstruction quality and higher generation fidelity. However, the computational demands of the autoregressive framework result in significant inference overhead. While speculative decoding has proven effective in accelerating Large Language Models (LLMs), their adaptation to continuous-valued visual autoregressive models remains unexplored. This work generalizes the speculative decoding algorithm from discrete tokens to continuous space. By analyzing the intrinsic properties of output distribution, we establish a tailored acceptance criterion for the diffusion distributions prevalent in such models. To overcome the inconsistency that occurred in speculative decoding output distributions, we introduce denoising trajectory alignment and token pre-filling methods. Additionally, we identify the hard-to-sample distribution in the rejection phase. To mitigate this issue, we propose a meticulous acceptance-rejection sampling method with a proper upper bound, thereby circumventing complex integration. Experimental results show that our continuous speculative decoding achieves a remarkable 2.33times speed-up on off-the-shelf models while maintaining the output distribution. Codes will be available at https://github.com/MarkXCloud/CSpD",
            "score": 10,
            "issue_id": 674,
            "pub_date": "2024-11-18",
            "pub_date_card": {
                "ru": "18 ноября",
                "en": "November 18",
                "zh": "11月18日"
            },
            "hash": "17049106ecc06192",
            "authors": [
                "Zili Wang",
                "Robert Zhang",
                "Kun Ding",
                "Qi Yang",
                "Fei Li",
                "Shiming Xiang"
            ],
            "affiliations": [
                "China Tower Corporation Limited",
                "Institute of Automation, Chinese Academy of Sciences, China",
                "University of Chinese Academy of Sciences, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11925.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Ускорение генерации изображений: от дискретного к непрерывному",
                    "desc": "Статья представляет новый метод ускорения генерации изображений с помощью авторегрессионных моделей с непрерывными значениями. Авторы адаптируют алгоритм спекулятивного декодирования, ранее применявшийся для ускорения больших языковых моделей, к непрерывному пространству. Они вводят специальный критерий принятия для диффузионных распределений и предлагают методы выравнивания траектории шумоподавления и предварительного заполнения токенов. Экспериментальные результаты показывают 2.33-кратное ускорение без ухудшения качества выходных данных."
                },
                "en": {
                    "title": "Speeding Up Image Generation with Continuous Speculative Decoding",
                    "desc": "This paper presents a new approach to improve the speed of continuous-valued autoregressive image generation models. It adapts speculative decoding, a technique previously used in large language models, to work with continuous data. The authors introduce methods to align denoising trajectories and pre-fill tokens to enhance the output quality during the decoding process. Their experiments demonstrate that this new method can significantly speed up the generation process by over two times while preserving the quality of the generated images."
                },
                "zh": {
                    "title": "加速连续值自回归图像生成的推测解码",
                    "desc": "本文提出了一种针对连续值自回归图像生成模型的推测解码算法，旨在提高生成速度。通过分析输出分布的内在特性，建立了适合扩散分布的接受标准。为了解决推测解码输出分布的不一致性，本文引入了去噪轨迹对齐和令牌预填充方法。实验结果表明，该方法在保持输出分布的同时，实现了2.33倍的速度提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12044",
            "title": "ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text, and Architectural Enhancements",
            "url": "https://huggingface.co/papers/2411.12044",
            "abstract": "Recent advances in foundational Vision Language Models (VLMs) have reshaped the evaluation paradigm in computer vision tasks. These foundational models, especially CLIP, have accelerated research in open-vocabulary computer vision tasks, including Open-Vocabulary Semantic Segmentation (OVSS). Although the initial results are promising, the dense prediction capabilities of VLMs still require further improvement. In this study, we enhance the semantic segmentation performance of CLIP by introducing new modules and modifications: 1) architectural changes in the last layer of ViT and the incorporation of attention maps from the middle layers with the last layer, 2) Image Engineering: applying data augmentations to enrich input image representations, and 3) using Large Language Models (LLMs) to generate definitions and synonyms for each class name to leverage CLIP's open-vocabulary capabilities. Our training-free method, ITACLIP, outperforms current state-of-the-art approaches on segmentation benchmarks such as COCO-Stuff, COCO-Object, Pascal Context, and Pascal VOC. Our code is available at https://github.com/m-arda-aydn/ITACLIP.",
            "score": 7,
            "issue_id": 684,
            "pub_date": "2024-11-18",
            "pub_date_card": {
                "ru": "18 ноября",
                "en": "November 18",
                "zh": "11月18日"
            },
            "hash": "d123699ae0dacdaa",
            "authors": [
                "M. Arda Aydın",
                "Efe Mert Çırpar",
                "Elvin Abdinli",
                "Gozde Unal",
                "Yusuf H. Sahin"
            ],
            "affiliations": [
                "Bilkent University",
                "Istanbul Technical University",
                "RWTH Aachen University",
                "Technical University of Munich"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12044.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "ITACLIP: Улучшение семантической сегментации без дополнительного обучения",
                    "desc": "Эта статья представляет новый метод улучшения семантической сегментации изображений с использованием модели CLIP. Авторы предлагают архитектурные изменения в последнем слое ViT и включение карт внимания из средних слоев. Они также применяют аугментацию данных для обогащения представлений входных изображений. Кроме того, исследователи используют большие языковые модели для генерации определений и синонимов каждого класса, чтобы лучше использовать возможности CLIP по работе с открытым словарем."
                },
                "en": {
                    "title": "Enhancing CLIP for Open-Vocabulary Semantic Segmentation",
                    "desc": "This paper discusses improvements to Vision Language Models (VLMs), particularly CLIP, for better performance in Open-Vocabulary Semantic Segmentation (OVSS). The authors propose enhancements through architectural modifications, including changes to the Vision Transformer (ViT) layers and the integration of attention maps. They also introduce data augmentation techniques to improve image representation and utilize Large Language Models (LLMs) to generate class definitions and synonyms, enhancing CLIP's open-vocabulary capabilities. The proposed method, ITACLIP, shows superior performance on various segmentation benchmarks compared to existing methods."
                },
                "zh": {
                    "title": "提升CLIP的语义分割性能",
                    "desc": "本研究针对基础视觉语言模型（VLMs）在计算机视觉任务中的应用进行了改进，特别是CLIP模型在开放词汇语义分割（OVSS）中的表现。我们通过引入新的模块和修改，提升了CLIP的语义分割性能，包括对ViT最后一层的架构调整和中间层注意力图的结合。我们还通过图像工程技术增强输入图像的表示，并利用大型语言模型（LLMs）生成每个类别名称的定义和同义词，以充分利用CLIP的开放词汇能力。我们的无训练方法ITACLIP在COCO-Stuff、COCO-Object、Pascal Context和Pascal VOC等分割基准上超越了当前的最先进方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12734",
            "title": "Soft Robotic Dynamic In-Hand Pen Spinning",
            "url": "https://huggingface.co/papers/2411.12734",
            "abstract": "Dynamic in-hand manipulation remains a challenging task for soft robotic systems that have demonstrated advantages in safe compliant interactions but struggle with high-speed dynamic tasks. In this work, we present SWIFT, a system for learning dynamic tasks using a soft and compliant robotic hand. Unlike previous works that rely on simulation, quasi-static actions and precise object models, the proposed system learns to spin a pen through trial-and-error using only real-world data without requiring explicit prior knowledge of the pen's physical attributes. With self-labeled trials sampled from the real world, the system discovers the set of pen grasping and spinning primitive parameters that enables a soft hand to spin a pen robustly and reliably. After 130 sampled actions per object, SWIFT achieves 100% success rate across three pens with different weights and weight distributions, demonstrating the system's generalizability and robustness to changes in object properties. The results highlight the potential for soft robotic end-effectors to perform dynamic tasks including rapid in-hand manipulation. We also demonstrate that SWIFT generalizes to spinning items with different shapes and weights such as a brush and a screwdriver which we spin with 10/10 and 5/10 success rates respectively. Videos, data, and code are available at https://soft-spin.github.io.",
            "score": 7,
            "issue_id": 676,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 ноября",
                "en": "November 19",
                "zh": "11月19日"
            },
            "hash": "f75a2283a0a8a06f",
            "authors": [
                "Yunchao Yao",
                "Uksang Yoo",
                "Jean Oh",
                "Christopher G. Atkeson",
                "Jeffrey Ichnowski"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.12734.jpg",
            "data": {
                "categories": [
                    "#robotics"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Мягкая роботизированная рука осваивает динамичные манипуляции",
                    "desc": "Статья представляет систему SWIFT для обучения динамическим задачам с использованием мягкой роботизированной руки. В отличие от предыдущих подходов, система учится вращать ручку методом проб и ошибок, используя только данные реального мира без предварительных знаний о физических свойствах объекта. После 130 пробных действий SWIFT достигает 100% успеха при вращении трех ручек с разным весом и распределением массы. Система также демонстрирует обобщающую способность на предметах другой формы и веса."
                },
                "en": {
                    "title": "SWIFT: Mastering Dynamic Manipulation with Soft Robotics",
                    "desc": "This paper introduces SWIFT, a novel system designed for dynamic in-hand manipulation using a soft robotic hand. Unlike traditional methods that depend on simulations or precise object models, SWIFT learns to perform tasks like spinning a pen through real-world trial-and-error. The system effectively identifies optimal grasping and spinning parameters without needing prior knowledge of the object's characteristics. SWIFT demonstrates impressive generalizability, achieving a 100% success rate in spinning various pens and also successfully manipulating other objects like brushes and screwdrivers."
                },
                "zh": {
                    "title": "软机器人动态操作的新突破",
                    "desc": "本研究提出了一种名为SWIFT的系统，用于学习动态任务，特别是在软机器人手中进行快速的物体操作。与以往依赖于模拟和精确物体模型的方法不同，SWIFT通过真实世界的数据进行试错学习，能够在没有物体物理属性先验知识的情况下，成功地旋转笔。经过130次采样操作，SWIFT在三种不同重量和分布的笔上实现了100%的成功率，展示了其在物体属性变化下的通用性和鲁棒性。该系统还能够推广到其他形状和重量的物体，如刷子和螺丝刀，分别实现了10/10和5/10的成功率，显示了软机器人在动态任务中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10161",
            "title": "SEAGULL: No-reference Image Quality Assessment for Regions of Interest via Vision-Language Instruction Tuning",
            "url": "https://huggingface.co/papers/2411.10161",
            "abstract": "Existing Image Quality Assessment (IQA) methods achieve remarkable success in analyzing quality for overall image, but few works explore quality analysis for Regions of Interest (ROIs). The quality analysis of ROIs can provide fine-grained guidance for image quality improvement and is crucial for scenarios focusing on region-level quality. This paper proposes a novel network, SEAGULL, which can SEe and Assess ROIs quality with GUidance from a Large vision-Language model. SEAGULL incorporates a vision-language model (VLM), masks generated by Segment Anything Model (SAM) to specify ROIs, and a meticulously designed Mask-based Feature Extractor (MFE) to extract global and local tokens for specified ROIs, enabling accurate fine-grained IQA for ROIs. Moreover, this paper constructs two ROI-based IQA datasets, SEAGULL-100w and SEAGULL-3k, for training and evaluating ROI-based IQA. SEAGULL-100w comprises about 100w synthetic distortion images with 33 million ROIs for pre-training to improve the model's ability of regional quality perception, and SEAGULL-3k contains about 3k authentic distortion ROIs to enhance the model's ability to perceive real world distortions. After pre-training on SEAGULL-100w and fine-tuning on SEAGULL-3k, SEAGULL shows remarkable performance on fine-grained ROI quality assessment. Code and datasets are publicly available at the https://github.com/chencn2020/Seagull.",
            "score": 4,
            "issue_id": 684,
            "pub_date": "2024-11-15",
            "pub_date_card": {
                "ru": "15 ноября",
                "en": "November 15",
                "zh": "11月15日"
            },
            "hash": "f2475fbc98477ad9",
            "authors": [
                "Zewen Chen",
                "Juan Wang",
                "Wen Wang",
                "Sunhan Xu",
                "Hang Xiong",
                "Yun Zeng",
                "Jian Guo",
                "Shuxun Wang",
                "Chunfeng Yuan",
                "Bing Li",
                "Weiming Hu"
            ],
            "affiliations": [
                "Beijing Jiaotong University",
                "Beijing Union University",
                "China University of Petroleum",
                "PeopleAI Inc.",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "School of Information Science and Technology, ShanghaiTech University",
                "State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10161.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#cv",
                    "#synthetic"
                ],
                "emoji": "🦅",
                "ru": {
                    "title": "SEAGULL: Точная оценка качества регионов интереса в изображениях",
                    "desc": "Статья представляет SEAGULL - новую нейронную сеть для оценки качества изображений в регионах интереса (ROI). SEAGULL использует модель машинного зрения и языка, маски из Segment Anything Model и специально разработанный экстрактор признаков на основе масок для точной оценки качества ROI. Авторы также создали два набора данных для обучения и оценки: SEAGULL-100w с синтетическими искажениями и SEAGULL-3k с реальными искажениями. После предобучения и тонкой настройки SEAGULL показывает выдающиеся результаты в детальной оценке качества ROI."
                },
                "en": {
                    "title": "SEAGULL: Fine-Grained Quality Assessment for Image Regions",
                    "desc": "This paper introduces SEAGULL, a novel network designed for assessing the quality of Regions of Interest (ROIs) in images, which is often overlooked by traditional Image Quality Assessment (IQA) methods. By leveraging a vision-language model and masks from the Segment Anything Model, SEAGULL effectively extracts both global and local features to provide detailed quality evaluations for specified ROIs. The authors also present two new datasets, SEAGULL-100w and SEAGULL-3k, which are used to train and evaluate the model, enhancing its ability to perceive both synthetic and real-world distortions. The results demonstrate that SEAGULL significantly improves fine-grained ROI quality assessment, making it a valuable tool for image quality enhancement."
                },
                "zh": {
                    "title": "精细化区域质量评估的新方法",
                    "desc": "现有的图像质量评估方法在整体图像质量分析上取得了显著成功，但对感兴趣区域（ROI）的质量分析研究较少。本文提出了一种新颖的网络SEAGULL，能够通过大型视觉-语言模型的指导来评估ROI的质量。SEAGULL结合了视觉-语言模型（VLM）、由Segment Anything Model（SAM）生成的掩码来指定ROI，以及精心设计的基于掩码的特征提取器（MFE），实现了对ROI的精细质量评估。此外，本文构建了两个基于ROI的质量评估数据集SEAGULL-100w和SEAGULL-3k，用于训练和评估ROI的质量评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12275",
            "title": "Building Trust: Foundations of Security, Safety and Transparency in AI",
            "url": "https://huggingface.co/papers/2411.12275",
            "abstract": "This paper explores the rapidly evolving ecosystem of publicly available AI models, and their potential implications on the security and safety landscape. As AI models become increasingly prevalent, understanding their potential risks and vulnerabilities is crucial. We review the current security and safety scenarios while highlighting challenges such as tracking issues, remediation, and the apparent absence of AI model lifecycle and ownership processes. Comprehensive strategies to enhance security and safety for both model developers and end-users are proposed. This paper aims to provide some of the foundational pieces for more standardized security, safety, and transparency in the development and operation of AI models and the larger open ecosystems and communities forming around them.",
            "score": 4,
            "issue_id": 682,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 ноября",
                "en": "November 19",
                "zh": "11月19日"
            },
            "hash": "628941b4647bf155",
            "authors": [
                "Huzaifa Sidhpurwala",
                "Garth Mollett",
                "Emily Fox",
                "Mark Bestavros",
                "Huamin Chen"
            ],
            "affiliations": [
                "Red Hat"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12275.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#security"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Безопасность публичных моделей ИИ: вызовы и решения",
                    "desc": "Статья исследует экосистему публично доступных моделей искусственного интеллекта и их потенциальное влияние на безопасность. Авторы рассматривают текущие сценарии безопасности, выделяя проблемы отслеживания, устранения уязвимостей и отсутствия процессов жизненного цикла моделей ИИ. Предлагаются комплексные стратегии повышения безопасности как для разработчиков моделей, так и для конечных пользователей. Цель работы - заложить основу для стандартизации безопасности и прозрачности в разработке и эксплуатации моделей ИИ."
                },
                "en": {
                    "title": "Securing the Future of Open AI Models",
                    "desc": "This paper examines the growing availability of AI models and their impact on security and safety. It emphasizes the importance of identifying risks and vulnerabilities associated with these models as they become more common. The authors discuss challenges like tracking model usage, addressing security issues, and the lack of clear ownership and lifecycle management for AI models. They propose strategies to improve security and safety for developers and users, aiming to establish standards for transparency in AI model development and operation."
                },
                "zh": {
                    "title": "提升人工智能模型的安全与透明性",
                    "desc": "本文探讨了公开可用的人工智能模型快速发展的生态系统及其对安全和安全性影响的潜在含义。随着人工智能模型的普及，理解其潜在风险和脆弱性变得至关重要。我们回顾了当前的安全和安全场景，并强调了跟踪问题、修复措施以及人工智能模型生命周期和所有权流程缺失等挑战。本文提出了增强模型开发者和最终用户安全与安全性的综合策略，旨在为人工智能模型及其周围开放生态系统和社区的发展提供更标准化的安全性、透明性和安全性的基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.10818",
            "title": "FlipSketch: Flipping Static Drawings to Text-Guided Sketch Animations",
            "url": "https://huggingface.co/papers/2411.10818",
            "abstract": "Sketch animations offer a powerful medium for visual storytelling, from simple flip-book doodles to professional studio productions. While traditional animation requires teams of skilled artists to draw key frames and in-between frames, existing automation attempts still demand significant artistic effort through precise motion paths or keyframe specification. We present FlipSketch, a system that brings back the magic of flip-book animation -- just draw your idea and describe how you want it to move! Our approach harnesses motion priors from text-to-video diffusion models, adapting them to generate sketch animations through three key innovations: (i) fine-tuning for sketch-style frame generation, (ii) a reference frame mechanism that preserves visual integrity of input sketch through noise refinement, and (iii) a dual-attention composition that enables fluid motion without losing visual consistency. Unlike constrained vector animations, our raster frames support dynamic sketch transformations, capturing the expressive freedom of traditional animation. The result is an intuitive system that makes sketch animation as simple as doodling and describing, while maintaining the artistic essence of hand-drawn animation.",
            "score": 3,
            "issue_id": 687,
            "pub_date": "2024-11-16",
            "pub_date_card": {
                "ru": "16 ноября",
                "en": "November 16",
                "zh": "11月16日"
            },
            "hash": "2aee0f98e4694b74",
            "authors": [
                "Hmrishav Bandyopadhyay",
                "Yi-Zhe Song"
            ],
            "affiliations": [
                "SketchX, CVSSP, University of Surrey, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.10818.jpg",
            "data": {
                "categories": [
                    "#story_generation",
                    "#cv",
                    "#video",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "✏️",
                "ru": {
                    "title": "Оживляем скетчи силой слова и ИИ",
                    "desc": "FlipSketch - это система, которая автоматизирует создание анимированных скетчей. Она использует диффузионные модели для генерации последовательности кадров на основе исходного рисунка и текстового описания движения. Ключевые инновации включают дообучение модели на скетчах, механизм сохранения визуальной целостности и двойное внимание для плавности движения. Система позволяет создавать выразительные анимации так же просто, как рисовать и описывать идею."
                },
                "en": {
                    "title": "Doodle Your Dreams: Effortless Sketch Animation with FlipSketch!",
                    "desc": "FlipSketch is a novel system that simplifies the process of creating sketch animations by allowing users to draw their ideas and describe desired movements. It leverages motion priors from text-to-video diffusion models, which are fine-tuned for generating sketch-style frames. The system incorporates a reference frame mechanism to ensure the visual integrity of the sketches and employs dual-attention composition for smooth motion while preserving consistency. This approach enables dynamic transformations in raster frames, making sketch animation accessible and intuitive, akin to traditional hand-drawn techniques."
                },
                "zh": {
                    "title": "让草图动画创作变得简单如涂鸦",
                    "desc": "FlipSketch 是一个新系统，旨在简化草图动画的制作过程。用户只需绘制草图并描述动画的运动方式，无需复杂的关键帧设置。该系统利用文本到视频扩散模型的运动先验，通过三项创新来生成草图动画。最终，FlipSketch 使得草图动画的创作变得像涂鸦一样简单，同时保留了手绘动画的艺术性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12240",
            "title": "Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages",
            "url": "https://huggingface.co/papers/2411.12240",
            "abstract": "Large Language Models (LLMs) based on transformer architectures have revolutionized a variety of domains, with tokenization playing a pivotal role in their pre-processing and fine-tuning stages. In multilingual models, particularly those tailored for Indic languages, effective tokenization is crucial for optimizing performance. This paper presents a comprehensive evaluation of tokenizers used by 12 LLMs across all 22 official languages of India, with a focus on comparing the efficiency of their tokenization processes. We employed the Normalized Sequence Length (NSL) as a key metric in our analysis. Our findings reveal that the SUTRA tokenizer outperforms all other models, including several Indic-specific models, excelling in 14 languages. Notable insights include the SUTRA tokenizer's superior handling of Indic languages, GPT-4o's advancement over its predecessor GPT-4 in processing Indian languages, and the limited performance of Project Indus in certain languages. This study underscores the critical importance of developing targeted tokenization strategies for multilingual and Indic-centric models, laying the groundwork for future improvements in tokenizer design to enhance linguistic coverage and model efficiency.",
            "score": 2,
            "issue_id": 675,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 ноября",
                "en": "November 19",
                "zh": "11月19日"
            },
            "hash": "aee934b73b340b71",
            "authors": [
                "S. Tamang",
                "D. J. Bora"
            ],
            "affiliations": [
                "Department of IT The Assam Kaziranga University Jorhat, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12240.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#multilingual",
                    "#data",
                    "#dataset"
                ],
                "emoji": "🇮🇳",
                "ru": {
                    "title": "SUTRA: Лидер токенизации для индийских языков в больших языковых моделях",
                    "desc": "В статье представлен анализ токенизаторов, используемых в 12 больших языковых моделях (LLM) для 22 официальных языков Индии. Исследователи использовали метрику нормализованной длины последовательности (NSL) для оценки эффективности токенизации. Результаты показали, что токенизатор SUTRA превзошел другие модели, включая специализированные для индийских языков, показав лучшие результаты для 14 языков. Исследование подчеркивает важность разработки целевых стратегий токенизации для многоязычных моделей и моделей, ориентированных на индийские языки."
                },
                "en": {
                    "title": "Optimizing Tokenization for Multilingual Mastery",
                    "desc": "This paper evaluates the effectiveness of tokenizers used in Large Language Models (LLMs) for all 22 official languages of India, emphasizing the importance of tokenization in multilingual contexts. The study introduces the Normalized Sequence Length (NSL) as a metric to assess the efficiency of different tokenizers. Results indicate that the SUTRA tokenizer significantly outperforms other models, particularly in handling Indic languages. The findings highlight the need for specialized tokenization strategies to improve the performance of LLMs in diverse linguistic settings."
                },
                "zh": {
                    "title": "优化多语言模型的分词策略",
                    "desc": "本论文探讨了基于变换器架构的大型语言模型（LLMs）中的分词技术，特别是在印度官方语言中的应用。我们对12种LLMs使用的分词器进行了全面评估，重点比较了它们的分词效率。研究结果显示，SUTRA分词器在14种语言中表现优于其他模型，尤其是在处理印度语言方面。该研究强调了为多语言和以印度语言为中心的模型开发针对性分词策略的重要性，以提高语言覆盖率和模型效率。"
                }
            }
        }
    ],
    "link_prev": "2024-11-19.html",
    "link_next": "2024-11-21.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "19.11",
        "en": "11/19",
        "zh": "11月19日"
    },
    "short_date_next": {
        "ru": "21.11",
        "en": "11/21",
        "zh": "11月21日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 1,
        "#agents": 0,
        "#cv": 4,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "连续值自回归（AR）图像生成模型展示了比离散符号模型更高的重建质量和生成保真度。然而，自回归框架的计算需求导致显著的推理开销。虽然推测解码已被证明有效加速大型语言模型（LLMs），但其应用于连续值视觉自回归模型仍未被探索。本文将推测解码算法从离散符号推广到连续空间。通过分析输出分布的内在特性，我们为这些模型中常见的扩散分布建立了定制的接受标准。实验结果显示，我们的连续推测解码在维持输出分布的同时，实现了显著的2.33倍加速。代码将在https://github.com/MarkXCloud/CSpD上提供。",
        "title": "Continuous Speculative Decoding for Autoregressive Image Generation",
        "pinyin": "连续值自回归（AR）图像生成模型展示了比离散符号模型更高的重建质量和生成保真度。然而，自回归框架的计算需求导致显著的推理开销。虽然推测解码已被证明有效加速大型语言模型（LLMs），但其应用于连续值视觉自回归模型仍未被探索。本文将推测解码算法从离散符号推广到连续空间。通过分析输出分布的内在特性，我们为这些模型中常见的扩散分布建立了定制的接受标准。实验结果显示，我们的连续推测解码在维持输出分布的同时，实现了显著的2.33倍加速。代码将在https://github.com/MarkXCloud/CSpD上提供。\n\nlián xù zhí zì huí guī (AR) tú xiàng shēng chéng mó xíng zhàn shì le bǐ lí sàn fú hào mó xíng gèng gāo de chóng jiàn zhì liàng hé shēng chéng bǎo zhēn dù. rán ér, zì huí guī kuàng jià de jì suǎn xū qiú dǎo zhì xiǎn zhù de tuī lǐ kāi xiāo. suī rán, tuī cè jiě mǎ yǐ bèi zhèng míng yǒu xiào jí sù dà xíng yǔ yán mó xíng (LLMs), dàn qí yìng yòng yú lián xù zhí shì jù shí zì huí guī mó xíng réng wèi bèi tuàn suǒ. běn wén jiāng tuī cè jiě mǎ suàn fǎ cóng lí sàn fú hào tuī guǎng dào lián xù kōng jiān. tōng guò fēn xī chū zhì fēn bù de nèi zài tè xìng, wǒ men wèi zhè xiē mó xíng zhōng cháng jiàn de kuò sàn fēn bù jiàn lì le dìng zhì de jiē shòu biāo zhǔn. shí yàn jié guǒ xiǎn shì, wǒ men de lián xù tuī cè jiě mǎ zài wéi chí chū zhì fēn bù de tóng shí, shí xiàn le xiǎn zhù de 2.33 bèi jí sù. dài mǎ jiāng zài https://github.com/MarkXCloud/CSpD shàng tí gōng.",
        "vocab": "[{'word': '连续值', 'pinyin': 'lián xù zhí', 'trans': 'continuous value'},\n{'word': '自回归', 'pinyin': 'zì huí guī', 'trans': 'autoregressive'},\n{'word': '展示', 'pinyin': 'zhǎn shì', 'trans': 'demonstrate'},\n{'word': '离散', 'pinyin': 'lí sàn', 'trans': 'discrete'},\n{'word': '符号', 'pinyin': 'fú hào', 'trans': 'symbol'},\n{'word': '重建', 'pinyin': 'chóng jiàn', 'trans': 'reconstruction'},\n{'word': '质量', 'pinyin': 'zhì liàng', 'trans': 'quality'},\n{'word': '生成', 'pinyin': 'shēng chéng', 'trans': 'generation'},\n{'word': '保真度', 'pinyin': 'bǎo zhēn dù', 'trans': 'fidelity'},\n{'word': '框架', 'pinyin': 'kuàng jià', 'trans': 'framework'},\n{'word': '计算', 'pinyin': 'jì suàn', 'trans': 'computation'},\n{'word': '需求', 'pinyin': 'xū qiú', 'trans': 'demand'},\n{'word': '导致', 'pinyin': 'dǎo zhì', 'trans': 'result in'},\n{'word': '显著', 'pinyin': 'xiǎn zhù', 'trans': 'significant'},\n{'word': '推理', 'pinyin': 'tuī lǐ', 'trans': 'inference'},\n{'word': '开销', 'pinyin': 'kāi xiāo', 'trans': 'cost'},\n{'word': '推测', 'pinyin': 'tuī cè', 'trans': 'speculative'},\n{'word': '解码', 'pinyin': 'jiě mǎ', 'trans': 'decoding'},\n{'word': '加速', 'pinyin': 'jiā sù', 'trans': 'accelerate'},\n{'word': '语言', 'pinyin': 'yǔ yán', 'trans': 'language'},\n{'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'},\n{'word': '应用', 'pinyin': 'yìng yòng', 'trans': 'application'},\n{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'visual'},\n{'word': '推广', 'pinyin': 'tuī guǎng', 'trans': 'extend'},\n{'word': '空间', 'pinyin': 'kōng jiān', 'trans': 'space'},\n{'word': '分析', 'pinyin': 'fēn xī', 'trans': 'analysis'},\n{'word': '输出', 'pinyin': 'shū chū', 'trans': 'output'},\n{'word': '分布', 'pinyin': 'fēn bù', 'trans': 'distribution'},\n{'word': '内在', 'pinyin': 'nèi zài', 'trans': 'intrinsic'},\n{'word': '特性', 'pinyin': 'tè xìng', 'trans': 'characteristic'},\n{'word': '扩散', 'pinyin': 'kuò sàn', 'trans': 'diffusion'},\n{'word': '建立', 'pinyin': 'jiàn lì', 'trans': 'establish'},\n{'word': '定制', 'pinyin': 'dìng zhì', 'trans': 'custom'},\n{'word': '接受', 'pinyin': 'jiē shòu', 'trans': 'acceptance'},\n{'word': '标准', 'pinyin': 'biāo zhǔn', 'trans': 'standard'},\n{'word': '实验', 'pinyin': 'shí yàn', 'trans': 'experiment'},\n{'word': '结果', 'pinyin': 'jié guǒ', 'trans': 'result'},\n{'word': '维持', 'pinyin': 'wéi chí', 'trans': 'maintain'},\n{'word': '实现', 'pinyin': 'shí xiàn', 'trans': 'achieve'},\n{'word': '代码', 'pinyin': 'dài mǎ', 'trans': 'code'},\n{'word': '提供', 'pinyin': 'tí gōng', 'trans': 'provide'}]",
        "trans": "Continuous-valued autoregressive (AR) image generation models have demonstrated higher reconstruction quality and generative fidelity compared to discrete symbol models. However, the computational requirements of the autoregressive framework result in significant inference overhead. While speculative decoding has been proven effective in accelerating large language models (LLMs), its application to continuous-valued visual autoregressive models has not been explored. This paper extends the speculative decoding algorithm from discrete symbols to continuous space. By analyzing the intrinsic properties of the output distribution, we establish custom acceptance criteria for the diffusion distributions commonly found in these models. Experimental results show that our continuous speculative decoding achieves a significant 2.33-fold speedup while maintaining the output distribution. The code will be available at https://github.com/MarkXCloud/CSpD.",
        "update_ts": "2024-11-20 09:11"
    }
}