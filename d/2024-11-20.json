{
    "date": {
        "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 20",
        "zh": "11æœˆ20æ—¥"
    },
    "time_utc": "2024-11-20 10:11",
    "weekday": 2,
    "issue_id": 681,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.11925",
            "title": "Continuous Speculative Decoding for Autoregressive Image Generation",
            "url": "https://huggingface.co/papers/2411.11925",
            "abstract": "Continuous-valued Autoregressive (AR) image generation models have demonstrated notable superiority over their discrete-token counterparts, showcasing considerable reconstruction quality and higher generation fidelity. However, the computational demands of the autoregressive framework result in significant inference overhead. While speculative decoding has proven effective in accelerating Large Language Models (LLMs), their adaptation to continuous-valued visual autoregressive models remains unexplored. This work generalizes the speculative decoding algorithm from discrete tokens to continuous space. By analyzing the intrinsic properties of output distribution, we establish a tailored acceptance criterion for the diffusion distributions prevalent in such models. To overcome the inconsistency that occurred in speculative decoding output distributions, we introduce denoising trajectory alignment and token pre-filling methods. Additionally, we identify the hard-to-sample distribution in the rejection phase. To mitigate this issue, we propose a meticulous acceptance-rejection sampling method with a proper upper bound, thereby circumventing complex integration. Experimental results show that our continuous speculative decoding achieves a remarkable 2.33times speed-up on off-the-shelf models while maintaining the output distribution. Codes will be available at https://github.com/MarkXCloud/CSpD",
            "score": 4,
            "issue_id": 674,
            "pub_date": "2024-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "17049106ecc06192",
            "authors": [
                "Zili Wang",
                "Robert Zhang",
                "Kun Ding",
                "Qi Yang",
                "Fei Li",
                "Shiming Xiang"
            ],
            "affiliations": [
                "China Tower Corporation Limited",
                "Institute of Automation, Chinese Academy of Sciences, China",
                "University of Chinese Academy of Sciences, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.11925.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¾Ñ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°Ğ½ĞµĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞ²ÑˆĞ¸Ğ¹ÑÑ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ñƒ. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ 2.33-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Speeding Up Image Generation with Continuous Speculative Decoding",
                    "desc": "This paper presents a new approach to improve the speed of continuous-valued autoregressive image generation models. It adapts speculative decoding, a technique previously used in large language models, to work with continuous data. The authors introduce methods to align denoising trajectories and pre-fill tokens to enhance the output quality during the decoding process. Their experiments demonstrate that this new method can significantly speed up the generation process by over two times while preserving the quality of the generated images."
                },
                "zh": {
                    "title": "åŠ é€Ÿè¿ç»­å€¼è‡ªå›å½’å›¾åƒç”Ÿæˆçš„æ¨æµ‹è§£ç ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹è¿ç»­å€¼è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¨æµ‹è§£ç ç®—æ³•ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆé€Ÿåº¦ã€‚é€šè¿‡åˆ†æè¾“å‡ºåˆ†å¸ƒçš„å†…åœ¨ç‰¹æ€§ï¼Œå»ºç«‹äº†é€‚åˆæ‰©æ•£åˆ†å¸ƒçš„æ¥å—æ ‡å‡†ã€‚ä¸ºäº†è§£å†³æ¨æµ‹è§£ç è¾“å‡ºåˆ†å¸ƒçš„ä¸ä¸€è‡´æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†å»å™ªè½¨è¿¹å¯¹é½å’Œä»¤ç‰Œé¢„å¡«å……æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè¾“å‡ºåˆ†å¸ƒçš„åŒæ—¶ï¼Œå®ç°äº†2.33å€çš„é€Ÿåº¦æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12734",
            "title": "Soft Robotic Dynamic In-Hand Pen Spinning",
            "url": "https://huggingface.co/papers/2411.12734",
            "abstract": "Dynamic in-hand manipulation remains a challenging task for soft robotic systems that have demonstrated advantages in safe compliant interactions but struggle with high-speed dynamic tasks. In this work, we present SWIFT, a system for learning dynamic tasks using a soft and compliant robotic hand. Unlike previous works that rely on simulation, quasi-static actions and precise object models, the proposed system learns to spin a pen through trial-and-error using only real-world data without requiring explicit prior knowledge of the pen's physical attributes. With self-labeled trials sampled from the real world, the system discovers the set of pen grasping and spinning primitive parameters that enables a soft hand to spin a pen robustly and reliably. After 130 sampled actions per object, SWIFT achieves 100% success rate across three pens with different weights and weight distributions, demonstrating the system's generalizability and robustness to changes in object properties. The results highlight the potential for soft robotic end-effectors to perform dynamic tasks including rapid in-hand manipulation. We also demonstrate that SWIFT generalizes to spinning items with different shapes and weights such as a brush and a screwdriver which we spin with 10/10 and 5/10 success rates respectively. Videos, data, and code are available at https://soft-spin.github.io.",
            "score": 3,
            "issue_id": 676,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "f75a2283a0a8a06f",
            "authors": [
                "Yunchao Yao",
                "Uksang Yoo",
                "Jean Oh",
                "Christopher G. Atkeson",
                "Jeffrey Ichnowski"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2411.12734.jpg",
            "data": {
                "categories": [
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœÑĞ³ĞºĞ°Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ€ÑƒĞºĞ° Ğ¾ÑĞ²Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ SWIFT Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑĞ³ĞºĞ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ÑƒĞºĞ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ Ñ€ÑƒÑ‡ĞºÑƒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ± Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞŸĞ¾ÑĞ»Ğµ 130 Ğ¿Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ SWIFT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 100% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ¿Ñ€Ğ¸ Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ… Ñ€ÑƒÑ‡ĞµĞº Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ğ²ĞµÑĞ¾Ğ¼ Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑÑÑ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ°Ñ… Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ğ²ĞµÑĞ°."
                },
                "en": {
                    "title": "SWIFT: Mastering Dynamic Manipulation with Soft Robotics",
                    "desc": "This paper introduces SWIFT, a novel system designed for dynamic in-hand manipulation using a soft robotic hand. Unlike traditional methods that depend on simulations or precise object models, SWIFT learns to perform tasks like spinning a pen through real-world trial-and-error. The system effectively identifies optimal grasping and spinning parameters without needing prior knowledge of the object's characteristics. SWIFT demonstrates impressive generalizability, achieving a 100% success rate in spinning various pens and also successfully manipulating other objects like brushes and screwdrivers."
                },
                "zh": {
                    "title": "è½¯æœºå™¨äººåŠ¨æ€æ“ä½œçš„æ–°çªç ´",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºSWIFTçš„ç³»ç»Ÿï¼Œç”¨äºå­¦ä¹ åŠ¨æ€ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨è½¯æœºå™¨äººæ‰‹ä¸­è¿›è¡Œå¿«é€Ÿçš„ç‰©ä½“æ“ä½œã€‚ä¸ä»¥å¾€ä¾èµ–äºæ¨¡æ‹Ÿå’Œç²¾ç¡®ç‰©ä½“æ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼ŒSWIFTé€šè¿‡çœŸå®ä¸–ç•Œçš„æ•°æ®è¿›è¡Œè¯•é”™å­¦ä¹ ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰ç‰©ä½“ç‰©ç†å±æ€§å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸåœ°æ—‹è½¬ç¬”ã€‚ç»è¿‡130æ¬¡é‡‡æ ·æ“ä½œï¼ŒSWIFTåœ¨ä¸‰ç§ä¸åŒé‡é‡å’Œåˆ†å¸ƒçš„ç¬”ä¸Šå®ç°äº†100%çš„æˆåŠŸç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨ç‰©ä½“å±æ€§å˜åŒ–ä¸‹çš„é€šç”¨æ€§å’Œé²æ£’æ€§ã€‚è¯¥ç³»ç»Ÿè¿˜èƒ½å¤Ÿæ¨å¹¿åˆ°å…¶ä»–å½¢çŠ¶å’Œé‡é‡çš„ç‰©ä½“ï¼Œå¦‚åˆ·å­å’Œèºä¸åˆ€ï¼Œåˆ†åˆ«å®ç°äº†10/10å’Œ5/10çš„æˆåŠŸç‡ï¼Œæ˜¾ç¤ºäº†è½¯æœºå™¨äººåœ¨åŠ¨æ€ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.12240",
            "title": "Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages",
            "url": "https://huggingface.co/papers/2411.12240",
            "abstract": "Large Language Models (LLMs) based on transformer architectures have revolutionized a variety of domains, with tokenization playing a pivotal role in their pre-processing and fine-tuning stages. In multilingual models, particularly those tailored for Indic languages, effective tokenization is crucial for optimizing performance. This paper presents a comprehensive evaluation of tokenizers used by 12 LLMs across all 22 official languages of India, with a focus on comparing the efficiency of their tokenization processes. We employed the Normalized Sequence Length (NSL) as a key metric in our analysis. Our findings reveal that the SUTRA tokenizer outperforms all other models, including several Indic-specific models, excelling in 14 languages. Notable insights include the SUTRA tokenizer's superior handling of Indic languages, GPT-4o's advancement over its predecessor GPT-4 in processing Indian languages, and the limited performance of Project Indus in certain languages. This study underscores the critical importance of developing targeted tokenization strategies for multilingual and Indic-centric models, laying the groundwork for future improvements in tokenizer design to enhance linguistic coverage and model efficiency.",
            "score": 1,
            "issue_id": 675,
            "pub_date": "2024-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "aee934b73b340b71",
            "authors": [
                "S. Tamang",
                "D. J. Bora"
            ],
            "affiliations": [
                "Department of IT The Assam Kaziranga University Jorhat, India"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.12240.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#multilingual",
                    "#data",
                    "#dataset"
                ],
                "emoji": "ğŸ‡®ğŸ‡³",
                "ru": {
                    "title": "SUTRA: Ğ›Ğ¸Ğ´ĞµÑ€ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ² 12 Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ´Ğ»Ñ 22 Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ˜Ğ½Ğ´Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ (NSL) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ SUTRA Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ 14 ÑĞ·Ñ‹ĞºĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¸."
                },
                "en": {
                    "title": "Optimizing Tokenization for Multilingual Mastery",
                    "desc": "This paper evaluates the effectiveness of tokenizers used in Large Language Models (LLMs) for all 22 official languages of India, emphasizing the importance of tokenization in multilingual contexts. The study introduces the Normalized Sequence Length (NSL) as a metric to assess the efficiency of different tokenizers. Results indicate that the SUTRA tokenizer significantly outperforms other models, particularly in handling Indic languages. The findings highlight the need for specialized tokenization strategies to improve the performance of LLMs in diverse linguistic settings."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å¤šè¯­è¨€æ¨¡å‹çš„åˆ†è¯ç­–ç•¥",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åŸºäºå˜æ¢å™¨æ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åˆ†è¯æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å°åº¦å®˜æ–¹è¯­è¨€ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬å¯¹12ç§LLMsä½¿ç”¨çš„åˆ†è¯å™¨è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œé‡ç‚¹æ¯”è¾ƒäº†å®ƒä»¬çš„åˆ†è¯æ•ˆç‡ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒSUTRAåˆ†è¯å™¨åœ¨14ç§è¯­è¨€ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å°åº¦è¯­è¨€æ–¹é¢ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ä¸ºå¤šè¯­è¨€å’Œä»¥å°åº¦è¯­è¨€ä¸ºä¸­å¿ƒçš„æ¨¡å‹å¼€å‘é’ˆå¯¹æ€§åˆ†è¯ç­–ç•¥çš„é‡è¦æ€§ï¼Œä»¥æé«˜è¯­è¨€è¦†ç›–ç‡å’Œæ¨¡å‹æ•ˆç‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-11-19.html",
    "link_next": "2024-11-21.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "19.11",
        "en": "11/19",
        "zh": "11æœˆ19æ—¥"
    },
    "short_date_next": {
        "ru": "21.11",
        "en": "11/21",
        "zh": "11æœˆ21æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 1,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 0,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 0,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿ç»­å€¼è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆæ¨¡å‹å±•ç¤ºäº†æ¯”ç¦»æ•£ç¬¦å·æ¨¡å‹æ›´é«˜çš„é‡å»ºè´¨é‡å’Œç”Ÿæˆä¿çœŸåº¦ã€‚ç„¶è€Œï¼Œè‡ªå›å½’æ¡†æ¶çš„è®¡ç®—éœ€æ±‚å¯¼è‡´æ˜¾è‘—çš„æ¨ç†å¼€é”€ã€‚è™½ç„¶æ¨æµ‹è§£ç å·²è¢«è¯æ˜æœ‰æ•ˆåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä½†å…¶åº”ç”¨äºè¿ç»­å€¼è§†è§‰è‡ªå›å½’æ¨¡å‹ä»æœªè¢«æ¢ç´¢ã€‚æœ¬æ–‡å°†æ¨æµ‹è§£ç ç®—æ³•ä»ç¦»æ•£ç¬¦å·æ¨å¹¿åˆ°è¿ç»­ç©ºé—´ã€‚é€šè¿‡åˆ†æè¾“å‡ºåˆ†å¸ƒçš„å†…åœ¨ç‰¹æ€§ï¼Œæˆ‘ä»¬ä¸ºè¿™äº›æ¨¡å‹ä¸­å¸¸è§çš„æ‰©æ•£åˆ†å¸ƒå»ºç«‹äº†å®šåˆ¶çš„æ¥å—æ ‡å‡†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„è¿ç»­æ¨æµ‹è§£ç åœ¨ç»´æŒè¾“å‡ºåˆ†å¸ƒçš„åŒæ—¶ï¼Œå®ç°äº†æ˜¾è‘—çš„2.33å€åŠ é€Ÿã€‚ä»£ç å°†åœ¨https://github.com/MarkXCloud/CSpDä¸Šæä¾›ã€‚",
        "title": "Continuous Speculative Decoding for Autoregressive Image Generation",
        "pinyin": "è¿ç»­å€¼è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆæ¨¡å‹å±•ç¤ºäº†æ¯”ç¦»æ•£ç¬¦å·æ¨¡å‹æ›´é«˜çš„é‡å»ºè´¨é‡å’Œç”Ÿæˆä¿çœŸåº¦ã€‚ç„¶è€Œï¼Œè‡ªå›å½’æ¡†æ¶çš„è®¡ç®—éœ€æ±‚å¯¼è‡´æ˜¾è‘—çš„æ¨ç†å¼€é”€ã€‚è™½ç„¶æ¨æµ‹è§£ç å·²è¢«è¯æ˜æœ‰æ•ˆåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä½†å…¶åº”ç”¨äºè¿ç»­å€¼è§†è§‰è‡ªå›å½’æ¨¡å‹ä»æœªè¢«æ¢ç´¢ã€‚æœ¬æ–‡å°†æ¨æµ‹è§£ç ç®—æ³•ä»ç¦»æ•£ç¬¦å·æ¨å¹¿åˆ°è¿ç»­ç©ºé—´ã€‚é€šè¿‡åˆ†æè¾“å‡ºåˆ†å¸ƒçš„å†…åœ¨ç‰¹æ€§ï¼Œæˆ‘ä»¬ä¸ºè¿™äº›æ¨¡å‹ä¸­å¸¸è§çš„æ‰©æ•£åˆ†å¸ƒå»ºç«‹äº†å®šåˆ¶çš„æ¥å—æ ‡å‡†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„è¿ç»­æ¨æµ‹è§£ç åœ¨ç»´æŒè¾“å‡ºåˆ†å¸ƒçš„åŒæ—¶ï¼Œå®ç°äº†æ˜¾è‘—çš„2.33å€åŠ é€Ÿã€‚ä»£ç å°†åœ¨https://github.com/MarkXCloud/CSpDä¸Šæä¾›ã€‚\n\nliÃ¡n xÃ¹ zhÃ­ zÃ¬ huÃ­ guÄ« (AR) tÃº xiÃ ng shÄ“ng chÃ©ng mÃ³ xÃ­ng zhÃ n shÃ¬ le bÇ lÃ­ sÃ n fÃº hÃ o mÃ³ xÃ­ng gÃ¨ng gÄo de chÃ³ng jiÃ n zhÃ¬ liÃ ng hÃ© shÄ“ng chÃ©ng bÇo zhÄ“n dÃ¹. rÃ¡n Ã©r, zÃ¬ huÃ­ guÄ« kuÃ ng jiÃ  de jÃ¬ suÇn xÅ« qiÃº dÇo zhÃ¬ xiÇn zhÃ¹ de tuÄ« lÇ kÄi xiÄo. suÄ« rÃ¡n, tuÄ« cÃ¨ jiÄ› mÇ yÇ bÃ¨i zhÃ¨ng mÃ­ng yÇ’u xiÃ o jÃ­ sÃ¹ dÃ  xÃ­ng yÇ” yÃ¡n mÃ³ xÃ­ng (LLMs), dÃ n qÃ­ yÃ¬ng yÃ²ng yÃº liÃ¡n xÃ¹ zhÃ­ shÃ¬ jÃ¹ shÃ­ zÃ¬ huÃ­ guÄ« mÃ³ xÃ­ng rÃ©ng wÃ¨i bÃ¨i tuÃ n suÇ’. bÄ›n wÃ©n jiÄng tuÄ« cÃ¨ jiÄ› mÇ suÃ n fÇ cÃ³ng lÃ­ sÃ n fÃº hÃ o tuÄ« guÇng dÃ o liÃ¡n xÃ¹ kÅng jiÄn. tÅng guÃ² fÄ“n xÄ« chÅ« zhÃ¬ fÄ“n bÃ¹ de nÃ¨i zÃ i tÃ¨ xÃ¬ng, wÇ’ men wÃ¨i zhÃ¨ xiÄ“ mÃ³ xÃ­ng zhÅng chÃ¡ng jiÃ n de kuÃ² sÃ n fÄ“n bÃ¹ jiÃ n lÃ¬ le dÃ¬ng zhÃ¬ de jiÄ“ shÃ²u biÄo zhÇ”n. shÃ­ yÃ n jiÃ© guÇ’ xiÇn shÃ¬, wÇ’ men de liÃ¡n xÃ¹ tuÄ« cÃ¨ jiÄ› mÇ zÃ i wÃ©i chÃ­ chÅ« zhÃ¬ fÄ“n bÃ¹ de tÃ³ng shÃ­, shÃ­ xiÃ n le xiÇn zhÃ¹ de 2.33 bÃ¨i jÃ­ sÃ¹. dÃ i mÇ jiÄng zÃ i https://github.com/MarkXCloud/CSpD shÃ ng tÃ­ gÅng.",
        "vocab": "[{'word': 'è¿ç»­å€¼', 'pinyin': 'liÃ¡n xÃ¹ zhÃ­', 'trans': 'continuous value'},\n{'word': 'è‡ªå›å½’', 'pinyin': 'zÃ¬ huÃ­ guÄ«', 'trans': 'autoregressive'},\n{'word': 'å±•ç¤º', 'pinyin': 'zhÇn shÃ¬', 'trans': 'demonstrate'},\n{'word': 'ç¦»æ•£', 'pinyin': 'lÃ­ sÃ n', 'trans': 'discrete'},\n{'word': 'ç¬¦å·', 'pinyin': 'fÃº hÃ o', 'trans': 'symbol'},\n{'word': 'é‡å»º', 'pinyin': 'chÃ³ng jiÃ n', 'trans': 'reconstruction'},\n{'word': 'è´¨é‡', 'pinyin': 'zhÃ¬ liÃ ng', 'trans': 'quality'},\n{'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ng chÃ©ng', 'trans': 'generation'},\n{'word': 'ä¿çœŸåº¦', 'pinyin': 'bÇo zhÄ“n dÃ¹', 'trans': 'fidelity'},\n{'word': 'æ¡†æ¶', 'pinyin': 'kuÃ ng jiÃ ', 'trans': 'framework'},\n{'word': 'è®¡ç®—', 'pinyin': 'jÃ¬ suÃ n', 'trans': 'computation'},\n{'word': 'éœ€æ±‚', 'pinyin': 'xÅ« qiÃº', 'trans': 'demand'},\n{'word': 'å¯¼è‡´', 'pinyin': 'dÇo zhÃ¬', 'trans': 'result in'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇn zhÃ¹', 'trans': 'significant'},\n{'word': 'æ¨ç†', 'pinyin': 'tuÄ« lÇ', 'trans': 'inference'},\n{'word': 'å¼€é”€', 'pinyin': 'kÄi xiÄo', 'trans': 'cost'},\n{'word': 'æ¨æµ‹', 'pinyin': 'tuÄ« cÃ¨', 'trans': 'speculative'},\n{'word': 'è§£ç ', 'pinyin': 'jiÄ› mÇ', 'trans': 'decoding'},\n{'word': 'åŠ é€Ÿ', 'pinyin': 'jiÄ sÃ¹', 'trans': 'accelerate'},\n{'word': 'è¯­è¨€', 'pinyin': 'yÇ” yÃ¡n', 'trans': 'language'},\n{'word': 'æ¨¡å‹', 'pinyin': 'mÃ³ xÃ­ng', 'trans': 'model'},\n{'word': 'åº”ç”¨', 'pinyin': 'yÃ¬ng yÃ²ng', 'trans': 'application'},\n{'word': 'è§†è§‰', 'pinyin': 'shÃ¬ juÃ©', 'trans': 'visual'},\n{'word': 'æ¨å¹¿', 'pinyin': 'tuÄ« guÇng', 'trans': 'extend'},\n{'word': 'ç©ºé—´', 'pinyin': 'kÅng jiÄn', 'trans': 'space'},\n{'word': 'åˆ†æ', 'pinyin': 'fÄ“n xÄ«', 'trans': 'analysis'},\n{'word': 'è¾“å‡º', 'pinyin': 'shÅ« chÅ«', 'trans': 'output'},\n{'word': 'åˆ†å¸ƒ', 'pinyin': 'fÄ“n bÃ¹', 'trans': 'distribution'},\n{'word': 'å†…åœ¨', 'pinyin': 'nÃ¨i zÃ i', 'trans': 'intrinsic'},\n{'word': 'ç‰¹æ€§', 'pinyin': 'tÃ¨ xÃ¬ng', 'trans': 'characteristic'},\n{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ² sÃ n', 'trans': 'diffusion'},\n{'word': 'å»ºç«‹', 'pinyin': 'jiÃ n lÃ¬', 'trans': 'establish'},\n{'word': 'å®šåˆ¶', 'pinyin': 'dÃ¬ng zhÃ¬', 'trans': 'custom'},\n{'word': 'æ¥å—', 'pinyin': 'jiÄ“ shÃ²u', 'trans': 'acceptance'},\n{'word': 'æ ‡å‡†', 'pinyin': 'biÄo zhÇ”n', 'trans': 'standard'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­ yÃ n', 'trans': 'experiment'},\n{'word': 'ç»“æœ', 'pinyin': 'jiÃ© guÇ’', 'trans': 'result'},\n{'word': 'ç»´æŒ', 'pinyin': 'wÃ©i chÃ­', 'trans': 'maintain'},\n{'word': 'å®ç°', 'pinyin': 'shÃ­ xiÃ n', 'trans': 'achieve'},\n{'word': 'ä»£ç ', 'pinyin': 'dÃ i mÇ', 'trans': 'code'},\n{'word': 'æä¾›', 'pinyin': 'tÃ­ gÅng', 'trans': 'provide'}]",
        "trans": "Continuous-valued autoregressive (AR) image generation models have demonstrated higher reconstruction quality and generative fidelity compared to discrete symbol models. However, the computational requirements of the autoregressive framework result in significant inference overhead. While speculative decoding has been proven effective in accelerating large language models (LLMs), its application to continuous-valued visual autoregressive models has not been explored. This paper extends the speculative decoding algorithm from discrete symbols to continuous space. By analyzing the intrinsic properties of the output distribution, we establish custom acceptance criteria for the diffusion distributions commonly found in these models. Experimental results show that our continuous speculative decoding achieves a significant 2.33-fold speedup while maintaining the output distribution. The code will be available at https://github.com/MarkXCloud/CSpD.",
        "update_ts": "2024-11-20 09:11"
    }
}