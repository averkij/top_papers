{
    "date": {
        "ru": "30 сентября",
        "en": "September 30",
        "zh": "9月30日"
    },
    "time_utc": "2025-09-30 02:16",
    "weekday": 1,
    "issue_id": 6152,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.23371",
            "title": "Alignment through Meta-Weighted Online Sampling: Bridging the Gap\n  between Data Generation and Preference Optimization",
            "url": "https://huggingface.co/papers/2509.23371",
            "abstract": "Meta-Weighted Adaptive Preference Optimization (MetaAPO) dynamically balances online and offline data to align large language models with human preferences, outperforming existing methods and reducing annotation costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Preference optimization is crucial for aligning large language models (LLMs) with human values and intentions. A significant challenge in this process is the distribution mismatch between pre-collected offline preference data and the evolving model policy. Existing methods attempt to reduce this gap using static heuristics or decoupled online sampling strategies, but they often fail to adapt to the model's dynamic learning state. To bridge this gap, we propose Meta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework that dynamically couples data generation with model training. MetaAPO employs a lightweight meta-learner, as an \"alignment gap estimator\", to evaluate the potential benefits of on-policy sampling in relation to offline data. This guides targeted online generation and assigns sample-wise meta-weights to the optimization objective, dynamically balancing the quality and distribution of online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench demonstrate that MetaAPO consistently outperforms existing preference optimization approaches across various settings, while reducing 42% in online annotation costs.",
            "score": 1,
            "issue_id": 6152,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 сентября",
                "en": "September 27",
                "zh": "9月27日"
            },
            "hash": "e5432419a1266b60",
            "authors": [
                "Junming Yang",
                "Ning Xu",
                "Biao Liu",
                "Shiqi Qiao",
                "Xin Geng"
            ],
            "affiliations": [
                "School of Computer Science and Engineering Southeast University Nanjing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23371.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rlhf",
                    "#alignment"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Умная балансировка данных для выравнивания LLM с человеческими предпочтениями",
                    "desc": "В статье представлен MetaAPO - новый подход для выравнивания больших языковых моделей с человеческими предпочтениями. Метод динамически балансирует онлайн и офлайн данные обучения, используя легковесную мета-модель для оценки потенциальной пользы от генерации новых примеров. Система назначает веса каждому образцу в зависимости от его качества и релевантности для текущего состояния модели. Эксперименты показывают превосходство над существующими методами при сокращении затрат на аннотацию на 42%."
                },
                "en": {
                    "title": "Dynamic Data Balancing for Better AI Alignment",
                    "desc": "MetaAPO is a new method designed to improve how large language models (LLMs) align with human preferences by effectively managing both online and offline data. It addresses the issue of distribution mismatch between previously collected offline preference data and the model's current learning state. By using a meta-learner to assess the value of on-policy sampling, MetaAPO dynamically adjusts the optimization process, ensuring that the model learns from the most relevant data. This approach not only enhances performance compared to existing methods but also significantly cuts down on the costs associated with online data annotation."
                },
                "zh": {
                    "title": "动态平衡在线与离线数据的偏好优化",
                    "desc": "Meta加权自适应偏好优化（MetaAPO）是一种新颖的框架，旨在动态平衡在线和离线数据，以使大型语言模型与人类偏好对齐。该方法通过使用轻量级的元学习器来评估在线采样的潜在好处，从而解决了预先收集的离线偏好数据与模型政策之间的分布不匹配问题。MetaAPO通过动态调整在线和离线数据的质量和分布，优化了样本的加权目标。实验结果表明，MetaAPO在多个设置中均优于现有的偏好优化方法，并且在线标注成本降低了42%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23285",
            "title": "Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference\n  Learning",
            "url": "https://huggingface.co/papers/2509.23285",
            "abstract": "Tool-Light framework improves large language models' tool-integrated reasoning efficiency and accuracy by leveraging information entropy and a two-stage fine-tuning process.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to improve their internal reasoning ability by integrating external tools. However, models employing TIR often display suboptimal behaviors, such as insufficient or excessive tool usage and overthinking after tool calls. The challenge of incentivizing LLMs to perform TIR efficiently and accurately, while stabilizing the reasoning process, remains an open question. In this paper, we start by exploring the impact of tool calls on model reasoning from the perspective of information entropy. Our findings indicate that tool call results lead to a distinct change in the information entropy of subsequent reasoning, with the overall entropy of the reasoning chain varying based on the number of tool calls. Building on these insights, we propose Tool-Light, a framework designed to encourage LLMs to perform TIR efficiently and accurately. Our framework includes dataset construction and multi-stage fine-tuning. For dataset construction, we employ continuous self-evolved sampling using the fine-tuned model, integrating both vanilla sampling and entropy-guided sampling. Besides, we establish strict criteria for selecting positive-negative pairs during sampling. The training process involves a two-stage approach, comprising Supervised Fine-Tuning (SFT) and Self-Evolved Direct Preference Optimization (DPO). Experimental results on 10 datasets demonstrate the effectiveness of Tool-Light, significantly improving the model's efficiency in executing TIR tasks.",
            "score": 1,
            "issue_id": 6152,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 сентября",
                "en": "September 27",
                "zh": "9月27日"
            },
            "hash": "a2cf471319463564",
            "authors": [
                "Yifei Chen",
                "Guanting Dong",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23285.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "🔧",
                "ru": {
                    "title": "Умное использование инструментов через энтропию рассуждений",
                    "desc": "Исследователи предлагают фреймворк Tool-Light для улучшения интеграции внешних инструментов в рассуждения больших языковых моделей. Анализируя информационную энтропию, они обнаружили, что вызовы инструментов влияют на качество последующих рассуждений модели. Фреймворк включает двухэтапное обучение с использованием supervised fine-tuning и direct preference optimization с самоэволюционной выборкой данных. Эксперименты на 10 датасетах показали значительное улучшение эффективности и точности использования инструментов в задачах рассуждения."
                },
                "en": {
                    "title": "Enhancing Tool-Integrated Reasoning with Tool-Light Framework",
                    "desc": "The Tool-Light framework enhances the efficiency and accuracy of large language models (LLMs) in tool-integrated reasoning (TIR) by utilizing information entropy and a two-stage fine-tuning process. It addresses common issues in TIR, such as improper tool usage and overthinking, by analyzing how tool calls affect the reasoning process. The framework includes a novel dataset construction method that combines vanilla and entropy-guided sampling, along with strict criteria for selecting training pairs. Experimental results show that Tool-Light significantly improves LLM performance across multiple datasets, making TIR more effective."
                },
                "zh": {
                    "title": "Tool-Light框架：提升推理效率与准确性",
                    "desc": "本文提出了一种名为Tool-Light的框架，旨在提高大型语言模型在工具集成推理（TIR）中的效率和准确性。通过信息熵的视角，研究了工具调用对模型推理的影响，发现工具调用结果会显著改变后续推理的信息熵。Tool-Light框架包括数据集构建和多阶段微调，采用自我演化采样和严格的正负样本选择标准。实验结果表明，该框架在10个数据集上显著提升了模型执行TIR任务的效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21953",
            "title": "MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially\n  Disentangled Attention and Identity-Aware Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.21953",
            "abstract": "MultiCrafter framework improves multi-subject image generation by addressing attribute leakage through explicit positional supervision, utilizing a Mixture-of-Experts architecture, and aligning with human preferences via online reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-subject image generation aims to synthesize user-provided subjects in a single image while preserving subject fidelity, ensuring prompt consistency, and aligning with human aesthetic preferences. However, existing methods, particularly those built on the In-Context-Learning paradigm, are limited by their reliance on simple reconstruction-based objectives, leading to both severe attribute leakage that compromises subject fidelity and failing to align with nuanced human preferences. To address this, we propose MultiCrafter, a framework that ensures high-fidelity, preference-aligned generation. First, we find that the root cause of attribute leakage is a significant entanglement of attention between different subjects during the generation process. Therefore, we introduce explicit positional supervision to explicitly separate attention regions for each subject, effectively mitigating attribute leakage. To enable the model to accurately plan the attention region of different subjects in diverse scenarios, we employ a Mixture-of-Experts architecture to enhance the model's capacity, allowing different experts to focus on different scenarios. Finally, we design a novel online reinforcement learning framework to align the model with human preferences, featuring a scoring mechanism to accurately assess multi-subject fidelity and a more stable training strategy tailored for the MoE architecture. Experiments validate that our framework significantly improves subject fidelity while aligning with human preferences better.",
            "score": 1,
            "issue_id": 6152,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 сентября",
                "en": "September 26",
                "zh": "9月26日"
            },
            "hash": "0c3656cedf425566",
            "authors": [
                "Tao Wu",
                "Yibo Jiang",
                "Yehao Lu",
                "Zhizhong Wang",
                "Zeyi Huang",
                "Zequn Qin",
                "Xi Li"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Zhejiang University",
                "Huawei Technologies Ltd",
                "School of Software Technology, Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21953.jpg",
            "data": {
                "categories": [
                    "#leakage",
                    "#alignment",
                    "#multimodal",
                    "#training",
                    "#rl",
                    "#architecture",
                    "#synthetic"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Точная генерация изображений с множественными объектами через разделение внимания",
                    "desc": "Исследователи представили MultiCrafter - фреймворк для генерации изображений с несколькими объектами, который решает проблему смешивания атрибутов разных объектов. Основная проблема заключается в том, что механизм внимания (attention) запутывается между различными объектами в процессе генерации. Для решения этого авторы вводят явный позиционный контроль, используют архитектуру Mixture-of-Experts для обработки разных сценариев, и применяют онлайн обучение с подкреплением для лучшего соответствия человеческим предпочтениям. Эксперименты показывают значительное улучшение точности воспроизведения объектов и соответствие эстетическим предпочтениям людей."
                },
                "en": {
                    "title": "MultiCrafter: Enhancing Multi-Subject Image Generation with Precision and Preference",
                    "desc": "The MultiCrafter framework enhances the generation of images containing multiple subjects by tackling the issue of attribute leakage through the use of explicit positional supervision. This approach helps to clearly define attention regions for each subject, preventing confusion during the image creation process. Additionally, the framework employs a Mixture-of-Experts architecture, which allows different model components to specialize in various scenarios, improving overall performance. Finally, an online reinforcement learning strategy is implemented to ensure that the generated images align closely with human aesthetic preferences, resulting in higher fidelity and satisfaction."
                },
                "zh": {
                    "title": "MultiCrafter：提升多主体图像生成的保真度与人类偏好对齐",
                    "desc": "MultiCrafter框架通过显式的位置信息监督来解决多主体图像生成中的属性泄漏问题，从而提高生成的图像质量。该框架采用混合专家架构，使不同的专家能够专注于不同的场景，增强模型的能力。为了更好地符合人类的审美偏好，我们设计了一种新的在线强化学习机制，能够准确评估多主体的保真度。实验结果表明，MultiCrafter显著提高了生成图像的主体保真度，并更好地与人类偏好对齐。"
                }
            }
        }
    ],
    "link_prev": "2025-09-29.html",
    "link_next": "2025-10-01.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "29.09",
        "en": "09/29",
        "zh": "9月29日"
    },
    "short_date_next": {
        "ru": "01.10",
        "en": "10/01",
        "zh": "10月1日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}