{
    "date": {
        "ru": "30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 30",
        "zh": "9æœˆ30æ—¥"
    },
    "time_utc": "2025-09-30 10:12",
    "weekday": 1,
    "issue_id": 6160,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.24006",
            "title": "SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable\n  Sparse-Linear Attention",
            "url": "https://huggingface.co/papers/2509.24006",
            "abstract": "SLA, a trainable attention method combining sparse and linear attention, accelerates Diffusion Transformer models for video generation with minimal quality loss.  \t\t\t\t\tAI-generated summary \t\t\t\t In Diffusion Transformer (DiT) models, particularly for video generation, attention latency is a major bottleneck due to the long sequence length and the quadratic complexity. We find that attention weights can be separated into two parts: a small fraction of large weights with high rank and the remaining weights with very low rank. This naturally suggests applying sparse acceleration to the first part and low-rank acceleration to the second. Based on this finding, we propose SLA (Sparse-Linear Attention), a trainable attention method that fuses sparse and linear attention to accelerate diffusion models. SLA classifies attention weights into critical, marginal, and negligible categories, applying O(N^2) attention to critical weights, O(N) attention to marginal weights, and skipping negligible ones. SLA combines these computations into a single GPU kernel and supports both forward and backward passes. With only a few fine-tuning steps using SLA, DiT models achieve a 20x reduction in attention computation, resulting in significant acceleration without loss of generation quality. Experiments show that SLA reduces attention computation by 95% without degrading end-to-end generation quality, outperforming baseline methods. In addition, we implement an efficient GPU kernel for SLA, which yields a 13.7x speedup in attention computation and a 2.2x end-to-end speedup in video generation on Wan2.1-1.3B.",
            "score": 86,
            "issue_id": 6153,
            "pub_date": "2025-09-28",
            "pub_date_card": {
                "ru": "28 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 28",
                "zh": "9æœˆ28æ—¥"
            },
            "hash": "a49453eb43711107",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#video",
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ SLA (Sparse-Linear Attention), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Diffusion Transformer Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿ÑƒÑ‚ĞµĞ¼ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²ĞµÑĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ´Ğ²Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸: Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ´Ğ¾Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²ĞµÑĞ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ¼ Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ñ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ¼. SLA ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ĞµÑĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, Ğ¼Ğ°Ñ€Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 20-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ 2.2-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Accelerating Video Generation with Sparse-Linear Attention",
                    "desc": "The paper introduces SLA, a novel attention mechanism that enhances the efficiency of Diffusion Transformer models used for video generation. By analyzing attention weights, the authors categorize them into critical, marginal, and negligible, allowing for a tailored application of sparse and linear attention techniques. This approach significantly reduces the computational burden of attention mechanisms, achieving a 20x reduction in computation while maintaining high-quality output. The implementation of SLA on GPU demonstrates impressive speed improvements, making it a valuable advancement in the field of machine learning for video generation."
                },
                "zh": {
                    "title": "ç¨€ç–-çº¿æ€§æ³¨æ„åŠ›ï¼šåŠ é€Ÿè§†é¢‘ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSLAï¼ˆç¨€ç–-çº¿æ€§æ³¨æ„åŠ›ï¼‰çš„å¯è®­ç»ƒæ³¨æ„åŠ›æ–¹æ³•ï¼Œæ—¨åœ¨åŠ é€Ÿæ‰©æ•£å˜æ¢å™¨æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„åº”ç”¨ã€‚é€šè¿‡å°†æ³¨æ„åŠ›æƒé‡åˆ†ä¸ºå…³é”®ã€è¾¹é™…å’Œå¯å¿½ç•¥ä¸‰ç±»ï¼ŒSLAå¯¹å…³é”®æƒé‡ä½¿ç”¨O(N^2)çš„æ³¨æ„åŠ›ï¼Œå¯¹è¾¹é™…æƒé‡ä½¿ç”¨O(N)çš„æ³¨æ„åŠ›ï¼Œè€Œè·³è¿‡å¯å¿½ç•¥çš„æƒé‡ï¼Œä»è€Œæ˜¾è‘—å‡å°‘è®¡ç®—é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒSLAåœ¨ä¸é™ä½ç”Ÿæˆè´¨é‡çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå°†æ³¨æ„åŠ›è®¡ç®—å‡å°‘95%ï¼Œå¹¶å®ç°20å€çš„åŠ é€Ÿã€‚è¯¥æ–¹æ³•è¿˜å®ç°äº†é«˜æ•ˆçš„GPUå†…æ ¸ï¼Œä½¿å¾—æ³¨æ„åŠ›è®¡ç®—é€Ÿåº¦æå‡äº†13.7å€ï¼Œè§†é¢‘ç”Ÿæˆçš„ç«¯åˆ°ç«¯é€Ÿåº¦æå‡äº†2.2å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23102",
            "title": "Multiplayer Nash Preference Optimization",
            "url": "https://huggingface.co/papers/2509.23102",
            "abstract": "Multiplayer Nash Preference Optimization (MNPO) extends Nash learning from human feedback to handle complex, non-transitive human preferences by formulating alignment as an n-player game.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models (LLMs) with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. In this work, we introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an n-player game, where each policy competes against a population of opponents while being regularized toward a reference model. Our framework establishes well-defined Nash equilibria in multiplayer settings and extends the concept of duality gap to quantify approximation quality. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Through comprehensive empirical evaluation, we show that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at https://github.com/smiles724/MNPO.",
            "score": 49,
            "issue_id": 6153,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "f2df8e1f219cb611",
            "authors": [
                "Fang Wu",
                "Xu Huang",
                "Weihao Xuan",
                "Zhiwei Zhang",
                "Yijia Xiao",
                "Guancheng Wan",
                "Xiaomin Li",
                "Bing Hu",
                "Peng Xia",
                "Jure Leskovec",
                "Yejin Choi"
            ],
            "affiliations": [
                "Georgia Institute of Technology",
                "Harvard University",
                "Independent Researcher",
                "Pennsylvania State University",
                "RIKEN AIP",
                "Stanford University",
                "The University of Tokyo",
                "UNC Chapel Hill",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23102.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#alignment",
                    "#optimization",
                    "#rlhf"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ°Ñ Ğ¸Ğ³Ñ€Ğ° Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ»ÑĞ´ĞµĞ¹, Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ² Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Nash learning Ğ´Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¸Ğ³Ñ€Ñ‹. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ RLHF Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Bradley-Terry Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑĞ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒÑÑ Ñ Ğ½ĞµÑ‚Ñ€Ğ°Ğ½Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. Multiplayer Nash Preference Optimization (MNPO) Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº n-Ğ¸Ğ³Ñ€Ğ¾Ğ²ÑƒÑ Ğ¸Ğ³Ñ€Ñƒ, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ†Ğ¸ĞµĞ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ½Ğ¸ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MNPO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Elevating AI Alignment: Multiplayer Nash Preference Optimization",
                    "desc": "Multiplayer Nash Preference Optimization (MNPO) is a new framework that enhances Nash learning from human feedback by addressing complex human preferences in a multiplayer setting. Unlike traditional methods that focus on two-player interactions, MNPO formulates alignment as an n-player game, allowing multiple policies to compete against a diverse set of opponents. This approach not only captures the non-transitive nature of real-world preferences but also establishes well-defined Nash equilibria, improving the robustness of the learning process. Empirical results show that MNPO outperforms existing methods, providing better alignment quality in scenarios with varied human feedback."
                },
                "zh": {
                    "title": "å¤šç©å®¶åšå¼ˆï¼šä¼˜åŒ–äººç±»åå¥½å¯¹é½",
                    "desc": "å¤šç©å®¶çº³ä»€åå¥½ä¼˜åŒ–ï¼ˆMNPOï¼‰æ˜¯ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œå®ƒå°†äººç±»åé¦ˆçš„çº³ä»€å­¦ä¹ æ‰©å±•åˆ°å¤šç©å®¶ç¯å¢ƒä¸­ã€‚è¯¥æ–¹æ³•å°†å¯¹é½é—®é¢˜è§†ä¸ºä¸€ä¸ªnäººåšå¼ˆï¼Œæ¯ä¸ªç­–ç•¥ä¸ä¸€ç»„å¯¹æ‰‹ç«äº‰ï¼ŒåŒæ—¶å‘å‚è€ƒæ¨¡å‹è¿›è¡Œæ­£åˆ™åŒ–ã€‚MNPOåœ¨å¤šç©å®¶è®¾ç½®ä¸­å»ºç«‹äº†æ˜ç¡®çš„çº³ä»€å‡è¡¡ï¼Œå¹¶æ‰©å±•äº†å¯¹å¶é—´éš™çš„æ¦‚å¿µï¼Œä»¥é‡åŒ–è¿‘ä¼¼è´¨é‡ã€‚é€šè¿‡å®è¯è¯„ä¼°ï¼ŒMNPOåœ¨éµå¾ªæŒ‡ä»¤çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„ä¸¤ç©å®¶æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚å’Œéä¼ é€’çš„äººç±»åå¥½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24897",
            "title": "RealUnify: Do Unified Models Truly Benefit from Unification? A\n  Comprehensive Benchmark",
            "url": "https://huggingface.co/papers/2509.24897",
            "abstract": "RealUnify evaluates the bidirectional synergy between understanding and generation in unified multimodal models, revealing that current models lack effective integration despite architectural unification.  \t\t\t\t\tAI-generated summary \t\t\t\t The integration of visual understanding and generation into unified multimodal models represents a significant stride toward general-purpose AI. However, a fundamental question remains unanswered by existing benchmarks: does this architectural unification actually enable synergetic interaction between the constituent capabilities? Existing evaluation paradigms, which primarily assess understanding and generation in isolation, are insufficient for determining whether a unified model can leverage its understanding to enhance its generation, or use generative simulation to facilitate deeper comprehension. To address this critical gap, we introduce RealUnify, a benchmark specifically designed to evaluate bidirectional capability synergy. RealUnify comprises 1,000 meticulously human-annotated instances spanning 10 categories and 32 subtasks. It is structured around two core axes: 1) Understanding Enhances Generation, which requires reasoning (e.g., commonsense, logic) to guide image generation, and 2) Generation Enhances Understanding, which necessitates mental simulation or reconstruction (e.g., of transformed or disordered visual inputs) to solve reasoning tasks. A key contribution is our dual-evaluation protocol, which combines direct end-to-end assessment with a diagnostic stepwise evaluation that decomposes tasks into distinct understanding and generation phases. This protocol allows us to precisely discern whether performance bottlenecks stem from deficiencies in core abilities or from a failure to integrate them. Through large-scale evaluations of 12 leading unified models and 6 specialized baselines, we find that current unified models still struggle to achieve effective synergy, indicating that architectural unification alone is insufficient. These results highlight the need for new training strategies and inductive biases to fully unlock the potential of unified modeling.",
            "score": 38,
            "issue_id": 6153,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "b10849bf0e51cb05",
            "authors": [
                "Yang Shi",
                "Yuhao Dong",
                "Yue Ding",
                "Yuran Wang",
                "Xuanyu Zhu",
                "Sheng Zhou",
                "Wenting Liu",
                "Haochen Tian",
                "Rundong Wang",
                "Huanqian Wang",
                "Zuyan Liu",
                "Bohan Zeng",
                "Ruizhe Chen",
                "Qixun Wang",
                "Zhuoran Zhang",
                "Xinlong Chen",
                "Chengzhuo Tong",
                "Bozhou Li",
                "Chaoyou Fu",
                "Qiang Liu",
                "Haotian Wang",
                "Wenjing Yang",
                "Yuanxing Zhang",
                "Pengfei Wan",
                "Yi-Fan Zhang",
                "Ziwei Liu"
            ],
            "affiliations": [
                "CASIA",
                "Kling Team",
                "NJU",
                "NTU",
                "NUS",
                "PKU",
                "THU",
                "USTC",
                "ZJU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24897.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#agi",
                    "#multimodal",
                    "#benchmark",
                    "#survey",
                    "#architecture"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ±ĞµĞ· ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ğ¸: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ RealUnify - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1000 Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ²Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ³Ğ»ÑƒĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 12 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Synergy in Unified Multimodal Models",
                    "desc": "RealUnify is a benchmark that assesses how well unified multimodal models can integrate visual understanding and generation. It addresses the gap in existing evaluations that only test these capabilities separately, rather than their interaction. The benchmark includes 1,000 human-annotated examples across various tasks, focusing on how understanding can improve generation and vice versa. Findings show that current models do not effectively leverage their integrated capabilities, suggesting a need for improved training methods to enhance synergy between understanding and generation."
                },
                "zh": {
                    "title": "è¯„ä¼°ç†è§£ä¸ç”Ÿæˆçš„åŒå‘ååŒä½œç”¨",
                    "desc": "RealUnify æ˜¯ä¸€ä¸ªè¯„ä¼°ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ä¸­ç†è§£ä¸ç”Ÿæˆä¹‹é—´åŒå‘ååŒä½œç”¨çš„åŸºå‡†ã€‚å°½ç®¡ç°æœ‰æ¨¡å‹åœ¨æ¶æ„ä¸Šå®ç°äº†ç»Ÿä¸€ï¼Œä½†å®ƒä»¬åœ¨æœ‰æ•ˆæ•´åˆç†è§£ä¸ç”Ÿæˆèƒ½åŠ›æ–¹é¢ä»ç„¶å­˜åœ¨ä¸è¶³ã€‚RealUnify åŒ…å«1000ä¸ªç»è¿‡äººå·¥æ ‡æ³¨çš„å®ä¾‹ï¼Œæ—¨åœ¨è¯„ä¼°ç†è§£å¦‚ä½•å¢å¼ºç”Ÿæˆï¼Œä»¥åŠç”Ÿæˆå¦‚ä½•ä¿ƒè¿›ç†è§£ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå½“å‰çš„ç»Ÿä¸€æ¨¡å‹åœ¨å®ç°æœ‰æ•ˆååŒæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†éœ€è¦æ–°çš„è®­ç»ƒç­–ç•¥æ¥å……åˆ†å‘æŒ¥ç»Ÿä¸€å»ºæ¨¡çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24900",
            "title": "OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation\n  and Editing",
            "url": "https://huggingface.co/papers/2509.24900",
            "abstract": "OpenGPT-4o-Image, a large-scale dataset with hierarchical task taxonomy and automated generation, significantly improves performance in image generation and editing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The performance of unified multimodal models for image generation and editing is fundamentally constrained by the quality and comprehensiveness of their training data. While existing datasets have covered basic tasks like style transfer and simple object manipulation, they often lack the systematic structure and challenging scenarios required for real-world applications. To address this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset constructed using a novel methodology that combines hierarchical task taxonomy with automated data generation. Our taxonomy not only includes fundamental capabilities such as text rendering and style control but also introduces highly practical yet challenging categories like scientific imagery for chemistry illustrations and complex instruction editing requiring simultaneous execution of multiple operations. Through an automated pipeline leveraging structured resource pools and GPT-4o, we generate 80k high-quality instruction-image pairs with controlled diversity, covering 11 major domains and 51 subtasks. Extensive experiments show that fine-tuning leading models on our dataset achieves significant performance gains across multiple benchmarks, with improvements of up to 18\\% on editing tasks (UniWorld-V1 on ImgEdit-Bench) and 13% on generation tasks (Harmon on GenEval). Our work demonstrates that systematic data construction is key to advancing multimodal AI capabilities.",
            "score": 34,
            "issue_id": 6154,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "0b2c3d817fd472dc",
            "authors": [
                "Zhihong Chen",
                "Xuehai Bai",
                "Yang Shi",
                "Chaoyou Fu",
                "Huanyu Zhang",
                "Haotian Wang",
                "Xiaoyan Sun",
                "Zhang Zhang",
                "Liang Wang",
                "Yuanxing Zhang",
                "Pengfei Wan",
                "Yi-Fan Zhang"
            ],
            "affiliations": [
                "CASIA",
                "HDU",
                "Kling Team",
                "NJU",
                "PKU",
                "THU",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24900.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#benchmark",
                    "#synthetic",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… â€” ĞºĞ»ÑÑ‡ Ğº Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ñƒ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ AI",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ OpenGPT-4o-Image â€” ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 80 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ°Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 11 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ¸ 51 Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ°ÑÑŒ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT-4o. Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ â€” Ğ´Ğ¾ 18% Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ 13% Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Image Generation with Structured Data",
                    "desc": "OpenGPT-4o-Image is a large-scale dataset designed to enhance image generation and editing tasks by providing a structured and comprehensive training resource. It utilizes a hierarchical task taxonomy to categorize tasks, including both basic operations and complex scenarios like scientific imagery. The dataset consists of 80,000 high-quality instruction-image pairs generated through an automated pipeline, ensuring diversity across 11 domains and 51 subtasks. Experiments show that models fine-tuned on this dataset achieve significant performance improvements, highlighting the importance of systematic data construction in advancing multimodal AI capabilities."
                },
                "zh": {
                    "title": "ç³»ç»ŸåŒ–æ•°æ®æ„å»ºæ¨åŠ¨å¤šæ¨¡æ€AIèƒ½åŠ›æå‡",
                    "desc": "OpenGPT-4o-Imageæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œé‡‡ç”¨åˆ†å±‚ä»»åŠ¡åˆ†ç±»æ³•å’Œè‡ªåŠ¨ç”Ÿæˆæ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡çš„æ€§èƒ½ã€‚ç°æœ‰çš„æ•°æ®é›†è™½ç„¶æ¶µç›–äº†åŸºæœ¬ä»»åŠ¡ï¼Œä½†å¾€å¾€ç¼ºä¹ç³»ç»Ÿç»“æ„å’ŒçœŸå®åœºæ™¯æ‰€éœ€çš„æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬é€šè¿‡è‡ªåŠ¨åŒ–æµç¨‹ç”Ÿæˆäº†8ä¸‡å¯¹é«˜è´¨é‡çš„æŒ‡ä»¤-å›¾åƒé…å¯¹ï¼Œè¦†ç›–äº†11ä¸ªä¸»è¦é¢†åŸŸå’Œ51ä¸ªå­ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šå¾®è°ƒé¢†å…ˆæ¨¡å‹å¯ä»¥åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25190",
            "title": "Visual Jigsaw Post-Training Improves MLLMs",
            "url": "https://huggingface.co/papers/2509.25190",
            "abstract": "Visual Jigsaw, a self-supervised reinforcement learning framework, enhances multimodal large language models' visual understanding through a permutation task without additional annotations or generative components.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning based post-training has recently emerged as a powerful paradigm for enhancing the alignment and reasoning capabilities of multimodal large language models (MLLMs). While vision-centric post-training is crucial for enhancing MLLMs' intrinsic understanding of visual signals, current post-training paradigms are predominantly text-centric, where dense visual inputs are only leveraged to extract sparse cues for text-based reasoning. There exist a few approaches in this direction, however, they often still rely on text as an intermediate mediator or introduce additional visual generative designs. In this work, we introduce Visual Jigsaw, a generic self-supervised post-training framework designed to strengthen visual understanding in MLLMs. Visual Jigsaw is formulated as a general ordering task: visual inputs are partitioned, shuffled, and the model must reconstruct the visual information by producing the correct permutation in natural language. This naturally aligns with reinforcement learning from verifiable rewards (RLVR), requires no additional visual generative components, and derives its supervisory signal automatically without any annotations. We instantiate Visual Jigsaw across three visual modalities, including images, videos, and 3D data. Extensive experiments demonstrate substantial improvements in fine-grained perception, temporal reasoning, and 3D spatial understanding. Our findings highlight the potential of self-supervised vision-centric tasks in post-training MLLMs and aim to inspire further research on vision-centric pretext designs. Project Page: https://penghao-wu.github.io/visual_jigsaw/",
            "score": 33,
            "issue_id": 6153,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "0ffce000f93cc7e9",
            "authors": [
                "Penghao Wu",
                "Yushan Zhang",
                "Haiwen Diao",
                "Bo Li",
                "Lewei Lu",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Linkoping University",
                "S-Lab, Nanyang Technological University",
                "SenseTime Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25190.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#alignment",
                    "#multimodal",
                    "#3d",
                    "#cv"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¿Ğ°Ğ·Ğ» Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Visual Jigsaw - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ÑÑ reinforcement learning Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ñ‹ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸, Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ¸Ğ²Ğ°ÑÑ‚ÑÑ, Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Enhancing Visual Understanding in MLLMs with Visual Jigsaw",
                    "desc": "Visual Jigsaw is a self-supervised reinforcement learning framework that improves the visual understanding of multimodal large language models (MLLMs) by using a permutation task. Instead of relying on text or additional generative components, it focuses on visual inputs by shuffling and requiring the model to reconstruct the correct order. This approach aligns with reinforcement learning from verifiable rewards, allowing the model to learn without needing extra annotations. The framework has been tested on images, videos, and 3D data, showing significant enhancements in perception, reasoning, and spatial understanding."
                },
                "zh": {
                    "title": "è§†è§‰æ‹¼å›¾ï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰ç†è§£",
                    "desc": "Visual Jigsawæ˜¯ä¸€ç§è‡ªç›‘ç£çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ’åˆ—ä»»åŠ¡å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦é¢å¤–çš„æ³¨é‡Šæˆ–ç”Ÿæˆç»„ä»¶ï¼Œè€Œæ˜¯é€šè¿‡å°†è§†è§‰è¾“å…¥åˆ†å‰²å’Œæ‰“ä¹±ï¼Œè¦æ±‚æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€ä¸­é‡å»ºæ­£ç¡®çš„æ’åˆ—ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒVisual Jigsawèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆç›‘ç£ä¿¡å·ï¼Œå¹¶ä¸å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç»†ç²’åº¦æ„ŸçŸ¥ã€æ—¶é—´æ¨ç†å’Œä¸‰ç»´ç©ºé—´ç†è§£æ–¹é¢æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23426",
            "title": "Democratizing AI scientists using ToolUniverse",
            "url": "https://huggingface.co/papers/2509.23426",
            "abstract": "ToolUniverse is an ecosystem that standardizes and integrates tools, models, and data for AI scientists, enabling automated refinement, creation, and composition of workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t AI scientists are emerging computational systems that serve as collaborative partners in discovery. These systems remain difficult to build because they are bespoke, tied to rigid workflows, and lack shared environments that unify tools, data, and analyses into a common ecosystem. In omics, unified ecosystems have transformed research by enabling interoperability, reuse, and community-driven development; AI scientists require comparable infrastructure. We present ToolUniverse, an ecosystem for building AI scientists from any language or reasoning model, whether open or closed. TOOLUNIVERSE standardizes how AI scientists identify and call tools, integrating more than 600 machine learning models, datasets, APIs, and scientific packages for data analysis, knowledge retrieval, and experimental design. It automatically refines tool interfaces for correct use by AI scientists, creates new tools from natural language descriptions, iteratively optimizes tool specifications, and composes tools into agentic workflows. In a case study of hypercholesterolemia, ToolUniverse was used to create an AI scientist to identify a potent analog of a drug with favorable predicted properties. The open-source ToolUniverse is available at https://aiscientist.tools.",
            "score": 29,
            "issue_id": 6153,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "dfc18b241e0932cd",
            "authors": [
                "Shanghua Gao",
                "Richard Zhu",
                "Pengwei Sui",
                "Zhenglun Kong",
                "Sufian Aldogom",
                "Yepeng Huang",
                "Ayush Noori",
                "Reza Shamji",
                "Krishna Parvataneni",
                "Theodoros Tsiligkaridis",
                "Marinka Zitnik"
            ],
            "affiliations": [
                "Broad Institute of MIT and Harvard, Cambridge, MA",
                "Department of Biomedical Informatics, Harvard Medical School, Boston, MA",
                "Harvard College, Harvard University, Cambridge, MA",
                "Harvard Data Science Initiative, Cambridge, MA",
                "Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Cambridge, MA",
                "MIT Lincoln Laboratory, Lexington, MA",
                "Massachusetts Institute of Technology, Cambridge, MA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23426.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#science",
                    "#open_source",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ AI-ÑƒÑ‡ĞµĞ½Ñ‹Ñ…",
                    "desc": "ToolUniverse - ÑÑ‚Ğ¾ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ AI-ÑƒÑ‡ĞµĞ½Ñ‹Ñ…, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºÑƒ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 600 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ², API Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ². ToolUniverse Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ AI-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ’ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ³Ğ¸Ğ¿ĞµÑ€Ñ…Ğ¾Ğ»ĞµÑÑ‚ĞµÑ€Ğ¸Ğ½ĞµĞ¼Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ AI-ÑƒÑ‡ĞµĞ½Ğ¾Ğ³Ğ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ²Ğ° Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Empowering AI Scientists with ToolUniverse: A Unified Ecosystem for Discovery",
                    "desc": "ToolUniverse is a standardized ecosystem designed for AI scientists, facilitating the integration of various tools, models, and datasets. It allows for the automated creation and refinement of workflows, making it easier for AI systems to collaborate in scientific discovery. By supporting over 600 machine learning models and APIs, ToolUniverse enhances interoperability and promotes community-driven development in research. A practical application demonstrated its capability by creating an AI scientist that identified a promising drug analog for hypercholesterolemia."
                },
                "zh": {
                    "title": "ToolUniverseï¼šæ„å»ºAIç§‘å­¦å®¶çš„ç»Ÿä¸€ç”Ÿæ€ç³»ç»Ÿ",
                    "desc": "ToolUniverseæ˜¯ä¸€ä¸ªä¸ºAIç§‘å­¦å®¶æä¾›æ ‡å‡†åŒ–å’Œé›†æˆå·¥å…·ã€æ¨¡å‹å’Œæ•°æ®çš„ç”Ÿæ€ç³»ç»Ÿï¼Œæ—¨åœ¨å®ç°å·¥ä½œæµç¨‹çš„è‡ªåŠ¨åŒ–ä¼˜åŒ–å’Œåˆ›å»ºã€‚è¯¥ç³»ç»Ÿè§£å†³äº†æ„å»ºAIç§‘å­¦å®¶æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚å®šåˆ¶åŒ–ã€åƒµåŒ–çš„å·¥ä½œæµç¨‹å’Œç¼ºä¹ç»Ÿä¸€ç¯å¢ƒçš„é—®é¢˜ã€‚ToolUniverseé›†æˆäº†è¶…è¿‡600ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ã€æ•°æ®é›†ã€APIå’Œç§‘å­¦åŒ…ï¼Œæ”¯æŒæ•°æ®åˆ†æã€çŸ¥è¯†æ£€ç´¢å’Œå®éªŒè®¾è®¡ã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼ŒToolUniverseæˆåŠŸåˆ›å»ºäº†ä¸€ä¸ªAIç§‘å­¦å®¶ï¼Œå¸®åŠ©è¯†åˆ«å…·æœ‰è‰¯å¥½é¢„æµ‹ç‰¹æ€§çš„è¯ç‰©ç±»æ¯”ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24695",
            "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
            "url": "https://huggingface.co/papers/2509.24695",
            "abstract": "SANA-Video, a small diffusion model, efficiently generates high-resolution, high-quality videos with strong text-video alignment using linear attention and a constant-memory KV cache, achieving competitive performance at a lower cost and faster speed.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation.",
            "score": 28,
            "issue_id": 6153,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "95921e0ae91378d2",
            "authors": [
                "Junsong Chen",
                "Yuyang Zhao",
                "Jincheng Yu",
                "Ruihang Chu",
                "Junyu Chen",
                "Shuai Yang",
                "Xianbang Wang",
                "Yicheng Pan",
                "Daquan Zhou",
                "Huan Ling",
                "Haozhe Liu",
                "Hongwei Yi",
                "Hao Zhang",
                "Muyang Li",
                "Yukang Chen",
                "Han Cai",
                "Sanja Fidler",
                "Ping Luo",
                "Song Han",
                "Enze Xie"
            ],
            "affiliations": [
                "HKU",
                "KAUST",
                "MIT",
                "NVIDIA",
                "PKU",
                "THU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24695.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#training",
                    "#inference",
                    "#small_models",
                    "#diffusion"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "SANA-Video - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 720x1280 Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¾ Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ (linear attention) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¹ KV-ĞºÑÑˆ Ñ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¿Ñ€Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² 16 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚Ğ° Ğ½Ğ° RTX 5090 GPU Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 5-ÑĞµĞºÑƒĞ½Ğ´Ğ½Ğ¾Ğ³Ğ¾ 720p Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² 2.4 Ñ€Ğ°Ğ·Ğ°."
                },
                "en": {
                    "title": "Efficient High-Quality Video Generation with SANA-Video",
                    "desc": "SANA-Video is a small diffusion model designed to generate high-resolution videos efficiently. It utilizes linear attention and a constant-memory KV cache to improve speed and reduce costs while maintaining strong text-video alignment. The model can produce videos up to 720x1280 resolution and minute-long duration, achieving competitive performance compared to larger models. With a significant reduction in training time and cost, SANA-Video is optimized for deployment on modern GPUs, making it a practical choice for video generation tasks."
                },
                "zh": {
                    "title": "ä½æˆæœ¬é«˜è´¨é‡è§†é¢‘ç”Ÿæˆçš„é©å‘½æ€§æ¨¡å‹",
                    "desc": "SANA-Videoæ˜¯ä¸€ç§å°å‹æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€é«˜è´¨é‡çš„è§†é¢‘ï¼Œä¸”ä¸æ–‡æœ¬è§†é¢‘çš„å¯¹é½æ€§å¼ºã€‚è¯¥æ¨¡å‹é‡‡ç”¨çº¿æ€§æ³¨æ„åŠ›å’Œå¸¸é‡å†…å­˜KVç¼“å­˜ï¼Œæ˜¾è‘—é™ä½äº†ç”Ÿæˆè§†é¢‘çš„æˆæœ¬å’Œæ—¶é—´ã€‚é€šè¿‡çº¿æ€§DiTå’Œå—çº¿æ€§æ³¨æ„åŠ›çš„è®¾è®¡ï¼ŒSANA-Videoåœ¨ç”Ÿæˆé•¿è§†é¢‘æ—¶ä¿æŒäº†é«˜æ•ˆæ€§å’Œæœ‰æ•ˆæ€§ã€‚ä¸ç°ä»£å°å‹æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼ŒSANA-Videoåœ¨æ€§èƒ½ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶åœ¨ç”Ÿæˆé€Ÿåº¦ä¸Šå¿«äº†16å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25160",
            "title": "GSM8K-V: Can Vision Language Models Solve Grade School Math Word\n  Problems in Visual Contexts",
            "url": "https://huggingface.co/papers/2509.25160",
            "abstract": "GSM8K-V is a new visual multi-image mathematical reasoning benchmark that highlights the limitations of current vision language models in handling visual mathematical problems.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision language models (VLMs) achieve unified modeling of images and text, enabling them to accomplish complex real-world tasks through perception, planning, and reasoning. Among these tasks, reasoning is particularly representative, with mathematical reasoning serving as a prominent example. It highlights the high-level capability of VLMs to comprehend mathematical information in images and to perform sophisticated reasoning. Recently, numerous visual mathematical reasoning benchmarks have been proposed, but they are often restricted to geometry, lack coverage of math word problems, and rarely assess reasoning across multiple images. To address these gaps, we introduce GSM8K-V, a purely visual multi-image mathematical reasoning benchmark. GSM8K-V is built by systematically mapping each sample from the widely used text-based GSM8K into visual form. Through a carefully designed automated image-generation pipeline combined with meticulous human annotation, we curate 1,319 high-quality samples. We evaluate a wide range of open-source and closed-source models on GSM8K-V. Results show that although existing VLMs have nearly saturated performance on text-based GSM8K, there remains substantial room for improvement on GSM8K-V. For example, the best-performing model, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on GSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the limitations of current models as well as potential directions for improvement. GSM8K-V offers a new perspective on visual mathematical reasoning and establishes a benchmark to guide the development of more robust and generalizable VLMs.",
            "score": 24,
            "issue_id": 6154,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "763f54eeeeb77cf4",
            "authors": [
                "Fan Yuan",
                "Yuchen Yan",
                "Yifan Jiang",
                "Haoran Zhao",
                "Tao Feng",
                "Jinyan Chen",
                "Yanwei Lou",
                "Wenqi Zhang",
                "Yongliang Shen",
                "Weiming Lu",
                "Jun Xiao",
                "Yueting Zhuang"
            ],
            "affiliations": [
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25160.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#benchmark",
                    "#reasoning",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞšĞ¾Ğ³Ğ´Ğ° ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞ¸ ÑÑ‚Ğ°Ğ²ÑÑ‚ AI Ğ² Ñ‚ÑƒĞ¿Ğ¸Ğº: Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° ĞºĞ°Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº GSM8K-V Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ 1,319 Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° GSM8K Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… (95% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸), ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¾ÑĞµĞ´Ğ°ÑÑ‚ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµÑ€ÑĞ¸ÑÑ… (Ğ²ÑĞµĞ³Ğ¾ 47% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸). Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… VLM Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "GSM8K-V: Bridging the Gap in Visual Mathematical Reasoning",
                    "desc": "GSM8K-V is a new benchmark designed to test visual multi-image mathematical reasoning, revealing the shortcomings of current vision language models (VLMs) in solving visual math problems. Unlike previous benchmarks that focus mainly on geometry or single-image tasks, GSM8K-V includes a diverse range of math word problems and requires reasoning across multiple images. The benchmark consists of 1,319 high-quality visual samples created through an automated image-generation process and human annotation. Evaluation results indicate that while VLMs perform well on text-based tasks, they struggle significantly with the visual aspects of mathematical reasoning, highlighting the need for further advancements in this area."
                },
                "zh": {
                    "title": "è§†è§‰æ•°å­¦æ¨ç†çš„æ–°åŸºå‡†ï¼šGSM8K-V",
                    "desc": "GSM8K-Væ˜¯ä¸€ä¸ªæ–°çš„è§†è§‰å¤šå›¾åƒæ•°å­¦æ¨ç†åŸºå‡†ï¼Œçªæ˜¾äº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è§†è§‰æ•°å­¦é—®é¢˜æ—¶çš„å±€é™æ€§ã€‚è¯¥åŸºå‡†é€šè¿‡å°†å¹¿æ³›ä½¿ç”¨çš„æ–‡æœ¬åŸºç¡€GSM8Kæ ·æœ¬ç³»ç»Ÿæ€§åœ°æ˜ å°„ä¸ºè§†è§‰å½¢å¼è€Œæ„å»ºï¼ŒåŒ…å«1319ä¸ªé«˜è´¨é‡æ ·æœ¬ã€‚å°½ç®¡ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬åŸºç¡€GSM8Kä¸Šè¡¨ç°æ¥è¿‘é¥±å’Œï¼Œä½†åœ¨GSM8K-Vä¸Šä»æœ‰æ˜¾è‘—çš„æ”¹è¿›ç©ºé—´ã€‚GSM8K-Vä¸ºè§†è§‰æ•°å­¦æ¨ç†æä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶ä¸ºå¼€å‘æ›´å¼ºå¤§å’Œæ›´å…·é€šç”¨æ€§çš„è§†è§‰è¯­è¨€æ¨¡å‹å¥ å®šäº†åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25175",
            "title": "EasySteer: A Unified Framework for High-Performance and Extensible LLM\n  Steering",
            "url": "https://huggingface.co/papers/2509.25175",
            "abstract": "EasySteer is a unified framework for efficient and extensible steering of large language models, offering significant speedups and improved functionality over existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM) steering has emerged as a promising paradigm for controlling model behavior at inference time through targeted manipulation of hidden states, offering a lightweight alternative to expensive retraining. However, existing steering frameworks suffer from critical limitations: computational inefficiency, limited extensibility, and restricted functionality that hinder both research progress and practical deployment. We present EasySteer, a unified framework for high-performance, extensible LLM steering built on vLLM. Our system features modular architecture with pluggable interfaces for both analysis-based and learning-based methods, fine-grained parameter control, pre-computed steering vectors for eight application domains, and an interactive demonstration system. Through deep integration with vLLM's optimized inference engine, EasySteer achieves 5.5-11.4times speedup over existing frameworks. Extensive experiments demonstrate its effectiveness in overthinking mitigation, hallucination reduction, and other key applications. EasySteer transforms steering from research technique to production-ready capability, establishing critical infrastructure for deployable, controllable language models.",
            "score": 23,
            "issue_id": 6153,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "213f3b6e7bcca3af",
            "authors": [
                "Haolei Xu",
                "Xinyu Mei",
                "Yuchen Yan",
                "Rui Zhou",
                "Wenqi Zhang",
                "Weiming Lu",
                "Yueting Zhuang",
                "Yongliang Shen"
            ],
            "affiliations": [
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25175.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#alignment",
                    "#inference",
                    "#optimization",
                    "#hallucinations",
                    "#architecture"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "Ğ’Ñ‹ÑĞ¾ĞºĞ¾ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ LLM Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "EasySteer â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ vLLM Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 5.5-11.4 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. EasySteer Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ñ, Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°Ñ steering Ğ¸Ğ· Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ² Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾Ğµ Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ñƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "EasySteer: Fast and Flexible Steering for Language Models",
                    "desc": "EasySteer is a new framework designed to improve the steering of large language models (LLMs) during inference, allowing for better control over their behavior without the need for costly retraining. It addresses the inefficiencies and limitations of previous steering methods by providing a modular architecture that supports both analysis-based and learning-based approaches. The framework includes pre-computed steering vectors for various applications and offers fine-grained control over model parameters, resulting in significant speed improvements. With its integration into vLLM's optimized inference engine, EasySteer not only enhances performance but also makes steering techniques practical for real-world applications."
                },
                "zh": {
                    "title": "EasySteerï¼šé«˜æ•ˆå¯æ‰©å±•çš„è¯­è¨€æ¨¡å‹å¼•å¯¼æ¡†æ¶",
                    "desc": "EasySteeræ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆå’Œå¯æ‰©å±•çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼•å¯¼ã€‚å®ƒé€šè¿‡å¯¹éšè—çŠ¶æ€çš„æœ‰é’ˆå¯¹æ€§æ“ä½œï¼Œåœ¨æ¨ç†æ—¶æ§åˆ¶æ¨¡å‹è¡Œä¸ºï¼Œæä¾›äº†ä¸€ç§æ¯”æ˜‚è´µçš„å†è®­ç»ƒæ›´è½»é‡çš„æ›¿ä»£æ–¹æ¡ˆã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒEasySteeråœ¨è®¡ç®—æ•ˆç‡ã€å¯æ‰©å±•æ€§å’ŒåŠŸèƒ½æ€§æ–¹é¢æœ‰æ˜¾è‘—æå‡ï¼Œèƒ½å¤Ÿæ”¯æŒå¤šç§åº”ç”¨é¢†åŸŸã€‚é€šè¿‡ä¸vLLMçš„æ·±åº¦é›†æˆï¼ŒEasySteerå®ç°äº†5.5åˆ°11.4å€çš„é€Ÿåº¦æå‡ï¼Œæˆä¸ºå¯éƒ¨ç½²çš„å¯æ§è¯­è¨€æ¨¡å‹çš„é‡è¦åŸºç¡€è®¾æ–½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23909",
            "title": "EditScore: Unlocking Online RL for Image Editing via High-Fidelity\n  Reward Modeling",
            "url": "https://huggingface.co/papers/2509.23909",
            "abstract": "A specialized reward model, EditScore, enables effective reinforcement learning for instruction-guided image editing by providing a high-fidelity reward signal.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-guided image editing has achieved remarkable progress, yet current models still face challenges with complex instructions and often require multiple samples to produce a desired result. Reinforcement Learning (RL) offers a promising solution, but its adoption in image editing has been severely hindered by the lack of a high-fidelity, efficient reward signal. In this work, we present a comprehensive methodology to overcome this barrier, centered on the development of a state-of-the-art, specialized reward model. We first introduce EditReward-Bench, a comprehensive benchmark to systematically evaluate reward models on editing quality. Building on this benchmark, we develop EditScore, a series of reward models (7B-72B) for evaluating the quality of instruction-guided image editing. Through meticulous data curation and filtering, EditScore effectively matches the performance of learning proprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy tailored for the generative nature of EditScore, our largest variant even surpasses GPT-5 in the benchmark. We then demonstrate that a high-fidelity reward model is the key to unlocking online RL for image editing. Our experiments show that, while even the largest open-source VLMs fail to provide an effective learning signal, EditScore enables efficient and robust policy optimization. Applying our framework to a strong base model, OmniGen2, results in a final model that shows a substantial and consistent performance uplift. Overall, this work provides the first systematic path from benchmarking to reward modeling to RL training in image editing, showing that a high-fidelity, domain-specialized reward model is the key to unlocking the full potential of RL in this domain.",
            "score": 23,
            "issue_id": 6153,
            "pub_date": "2025-09-28",
            "pub_date_card": {
                "ru": "28 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 28",
                "zh": "9æœˆ28æ—¥"
            },
            "hash": "a3543c4787af3b5e",
            "authors": [
                "Xin Luo",
                "Jiahao Wang",
                "Chenyuan Wu",
                "Shitao Xiao",
                "Xiyan Jiang",
                "Defu Lian",
                "Jiajun Zhang",
                "Dong Liu",
                "Zheng liu"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence",
                "Institute of Automation, Chinese Academy of Sciences",
                "University of Science and Technology of China",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23909.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#data",
                    "#training",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ’Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ reward Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ - ĞºĞ»ÑÑ‡ Ğº RL Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ reward Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ EditScore Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº EditReward-Bench Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° reward Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ°Ğ¶Ğµ GPT-5. Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ reward Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ½Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ reinforcement learning Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ½ĞµĞµ Ğ±Ñ‹Ğ»Ğ¾ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ OmniGen2 Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Unlocking Image Editing with High-Fidelity Rewards",
                    "desc": "This paper introduces EditScore, a specialized reward model designed to enhance reinforcement learning (RL) for instruction-guided image editing. The authors highlight the challenges faced by current models in interpreting complex instructions and generating desired outputs efficiently. By developing EditReward-Bench, a benchmark for evaluating reward models, they demonstrate that EditScore significantly improves the quality of image editing by providing a high-fidelity reward signal. The results show that EditScore not only matches but can also surpass existing models, enabling effective policy optimization and substantial performance improvements in image editing tasks."
                },
                "zh": {
                    "title": "é«˜ä¿çœŸå¥–åŠ±æ¨¡å‹ï¼Œè§£é”å›¾åƒç¼–è¾‘çš„æ½œåŠ›",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ä¸“é—¨çš„å¥–åŠ±æ¨¡å‹EditScoreï¼Œæ—¨åœ¨æå‡åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ä¸­çš„å¼ºåŒ–å­¦ä¹ æ•ˆæœã€‚å½“å‰çš„å›¾åƒç¼–è¾‘æ¨¡å‹åœ¨å¤„ç†å¤æ‚æŒ‡ä»¤æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä¸”é€šå¸¸éœ€è¦å¤šä¸ªæ ·æœ¬æ‰èƒ½è¾¾åˆ°é¢„æœŸæ•ˆæœã€‚EditScoreé€šè¿‡æä¾›é«˜ä¿çœŸåº¦çš„å¥–åŠ±ä¿¡å·ï¼Œå…‹æœäº†è¿™ä¸€éšœç¢ï¼Œå¹¶åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé«˜ä¿çœŸåº¦çš„å¥–åŠ±æ¨¡å‹æ˜¯å®ç°å›¾åƒç¼–è¾‘åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„å…³é”®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24014",
            "title": "SparseD: Sparse Attention for Diffusion Language Models",
            "url": "https://huggingface.co/papers/2509.24014",
            "abstract": "SparseD is a novel sparse attention method for diffusion language models that addresses the high inference latency by pre-computing head-specific sparse patterns and switching to sparse attention in later denoising steps.  \t\t\t\t\tAI-generated summary \t\t\t\t While diffusion language models (DLMs) offer a promising alternative to autoregressive models (ARs), existing open-source DLMs suffer from high inference latency. This bottleneck is mainly due to the attention's quadratic complexity with respect to context length in computing all query-key pairs. Intuitively, to reduce this complexity, a natural strategy is to restrict attention to sparse patterns that retain only the most relevant connections. Such approaches are well-established in ARs, where attention follows fixed and clearly defined sparse patterns. However, in DLMs, we observe distinct sparsity behaviors: (1) attention patterns vary across heads, (2) attention patterns in each head remain highly similar across denoising steps, and (3) early denoising steps are critical for generation. These findings render sparse attention methods designed for ARs largely incompatible with DLMs, as they fail to capture head-specific structures and risk degrading generation when applied in early denoising steps. To address these challenges, we propose SparseD, a novel sparse attention method for DLMs. Leveraging the observations, SparseD only requires pre-computing head-specific sparse patterns one time, and reuses them across all steps. This prevents recomputing sparse patterns at each denoising step. Meanwhile, SparseD uses full attention in the early steps, then switches to sparse attention later to maintain generation quality. Together, these establish SparseD as a practical and efficient solution for deploying DLMs in long-context applications. Experimental results demonstrate that SparseD achieves lossless acceleration, delivering up to 1.50times speedup over FlashAttention at a 64k context length with 1,024 denoising steps.",
            "score": 21,
            "issue_id": 6153,
            "pub_date": "2025-09-28",
            "pub_date_card": {
                "ru": "28 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 28",
                "zh": "9æœˆ28æ—¥"
            },
            "hash": "de0de0ca5cd9c93d",
            "authors": [
                "Zeqing Wang",
                "Gongfan Fang",
                "Xinyin Ma",
                "Xingyi Yang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore",
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24014.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#long_context",
                    "#optimization",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "SparseD - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ğ¾Ğ·Ğ´Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ°. SparseD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 1.5 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ FlashAttention Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° 64k Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "SparseD: Speeding Up Diffusion Language Models with Smart Sparse Attention",
                    "desc": "SparseD is a new method designed to improve the efficiency of diffusion language models (DLMs) by implementing sparse attention techniques. It addresses the problem of high inference latency caused by the quadratic complexity of traditional attention mechanisms. By pre-computing specific sparse patterns for each attention head, SparseD allows for faster processing during later denoising steps while maintaining high generation quality. This approach not only speeds up the model significantly but also ensures that the unique sparsity behaviors of DLMs are effectively utilized, resulting in a practical solution for long-context applications."
                },
                "zh": {
                    "title": "SparseDï¼šé«˜æ•ˆçš„æ‰©æ•£è¯­è¨€æ¨¡å‹ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•",
                    "desc": "SparseDæ˜¯ä¸€ç§æ–°é¢–çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•ï¼Œä¸“ä¸ºæ‰©æ•£è¯­è¨€æ¨¡å‹è®¾è®¡ï¼Œæ—¨åœ¨é€šè¿‡é¢„è®¡ç®—ç‰¹å®šå¤´çš„ç¨€ç–æ¨¡å¼æ¥è§£å†³é«˜æ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨åç»­çš„å»å™ªæ­¥éª¤ä¸­åˆ‡æ¢åˆ°ç¨€ç–æ³¨æ„åŠ›ï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚SparseDçš„å…³é”®åœ¨äºå®ƒåªéœ€ä¸€æ¬¡æ€§é¢„è®¡ç®—å¤´ç‰¹å®šçš„ç¨€ç–æ¨¡å¼ï¼Œå¹¶åœ¨æ‰€æœ‰æ­¥éª¤ä¸­é‡å¤ä½¿ç”¨ï¼Œé¿å…äº†æ¯ä¸ªå»å™ªæ­¥éª¤éƒ½é‡æ–°è®¡ç®—ç¨€ç–æ¨¡å¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSparseDåœ¨64kä¸Šä¸‹æ–‡é•¿åº¦å’Œ1024ä¸ªå»å™ªæ­¥éª¤ä¸‹ï¼Œèƒ½å¤Ÿå®ç°é«˜è¾¾1.50å€çš„åŠ é€Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22193",
            "title": "When Does Reasoning Matter? A Controlled Study of Reasoning's\n  Contribution to Model Performance",
            "url": "https://huggingface.co/papers/2509.22193",
            "abstract": "Reasoning models enhance performance across various tasks, surpassing instruction fine-tuned models in reasoning-intensive and open-ended tasks, despite higher computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) with reasoning capabilities have achieved state-of-the-art performance on a wide range of tasks. Despite its empirical success, the tasks and model scales at which reasoning becomes effective, as well as its training and inference costs, remain underexplored. In this work, we rely on a synthetic data distillation framework to conduct a large-scale supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models of varying sizes, on a wide range of math-centric and general-purpose tasks, evaluating both multiple-choice and open-ended formats. Our analysis reveals that reasoning consistently improves model performance, often matching or surpassing significantly larger IFT systems. Notably, while IFT remains Pareto-optimal in training and inference costs, reasoning models become increasingly valuable as model size scales, overcoming IFT performance limits on reasoning-intensive and open-ended tasks.",
            "score": 21,
            "issue_id": 6159,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "245d6d1e4d5d0cc1",
            "authors": [
                "Nicolas Boizard",
                "Hippolyte Gisserot-Boukhlef",
                "Kevin El-Haddad",
                "CÃ©line Hudelot",
                "Pierre Colombo"
            ],
            "affiliations": [
                "Artefact Research Center",
                "Diabolocom",
                "Equall",
                "ISIA Lab, University of Mons",
                "MICS, CentraleSupelec, Universite Paris-Saclay"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22193.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#reasoning",
                    "#synthetic",
                    "#data",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€: ĞºĞ°Ğº reasoning Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ IFT ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ… (Instruction Fine-Tuning), Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ IFT Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¥Ğ¾Ñ‚Ñ IFT Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚, reasoning Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ†ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Unlocking Performance: Reasoning Models Outshine Instruction Fine-Tuning",
                    "desc": "This paper investigates the effectiveness of reasoning models in machine learning, particularly in tasks that require complex reasoning and open-ended responses. It compares these models with instruction fine-tuned models, highlighting that reasoning models often outperform larger instruction fine-tuned systems in various tasks. The study uses a synthetic data distillation framework to analyze performance across different model sizes and task types, revealing that reasoning capabilities enhance overall model performance. Although instruction fine-tuning is efficient in terms of training and inference costs, reasoning models show increasing advantages as model sizes grow, especially in challenging reasoning tasks."
                },
                "zh": {
                    "title": "æ¨ç†æ¨¡å‹ï¼šè¶…è¶ŠæŒ‡ä»¤å¾®è°ƒçš„æ€§èƒ½",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†æ¨ç†æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°å…¶åœ¨æ¨ç†å¯†é›†å’Œå¼€æ”¾å¼ä»»åŠ¡ä¸­ä¼˜äºæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ã€‚å°½ç®¡æ¨ç†æ¨¡å‹çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œä½†åœ¨æ•°å­¦å’Œé€šç”¨ä»»åŠ¡ä¸­ï¼Œæ¨ç†æ¨¡å‹çš„è¡¨ç°å¸¸å¸¸ä¸æ›´å¤§è§„æ¨¡çš„æŒ‡ä»¤å¾®è°ƒç³»ç»Ÿç›¸å½“æˆ–æ›´å¥½ã€‚æˆ‘ä»¬ä½¿ç”¨åˆæˆæ•°æ®è’¸é¦æ¡†æ¶è¿›è¡Œå¤§è§„æ¨¡ç›‘ç£ç ”ç©¶ï¼Œæ¯”è¾ƒäº†ä¸åŒè§„æ¨¡çš„æ¨ç†æ¨¡å‹å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œéšç€æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œæ¨ç†æ¨¡å‹åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­çš„ä»·å€¼ä¸æ–­æå‡ï¼Œçªç ´äº†æŒ‡ä»¤å¾®è°ƒçš„æ€§èƒ½é™åˆ¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25106",
            "title": "Towards Personalized Deep Research: Benchmarks and Evaluations",
            "url": "https://huggingface.co/papers/2509.25106",
            "abstract": "A new benchmark, Personalized Deep Research Bench, evaluates the personalization capabilities of Deep Research Agents across diverse tasks and user profiles using the PQR Evaluation Framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep Research Agents (DRAs) can autonomously conduct complex investigations and generate comprehensive reports, demonstrating strong real-world potential. However, existing evaluations mostly rely on close-ended benchmarks, while open-ended deep research benchmarks remain scarce and typically neglect personalized scenarios. To bridge this gap, we introduce Personalized Deep Research Bench, the first benchmark for evaluating personalization in DRAs. It pairs 50 diverse research tasks across 10 domains with 25 authentic user profiles that combine structured persona attributes with dynamic real-world contexts, yielding 250 realistic user-task queries. To assess system performance, we propose the PQR Evaluation Framework, which jointly measures (P) Personalization Alignment, (Q) Content Quality, and (R) Factual Reliability. Our experiments on a range of systems highlight current capabilities and limitations in handling personalized deep research. This work establishes a rigorous foundation for developing and evaluating the next generation of truly personalized AI research assistants.",
            "score": 19,
            "issue_id": 6157,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "771044af63bb2ae2",
            "authors": [
                "Yuan Liang",
                "Jiaxian Li",
                "Yuqing Wang",
                "Piaohong Wang",
                "Motong Tian",
                "Pai Liu",
                "Shuofei Qiao",
                "Runnan Fang",
                "He Zhu",
                "Ge Zhang",
                "Minghao Liu",
                "Yuchen Eleanor Jiang",
                "Ningyu Zhang",
                "Wangchunshu Zhou"
            ],
            "affiliations": [
                "OPPO",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25106.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#personalization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Personalized Deep Research Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (Deep Research Agents) Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 50 Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· 10 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ¸ 25 Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°ĞµÑ‚ 250 Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº PQR, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ·Ğ°ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing AI Research with Personalized Deep Research Bench",
                    "desc": "The paper introduces the Personalized Deep Research Bench, a new benchmark designed to evaluate how well Deep Research Agents (DRAs) can personalize their responses based on different user profiles and tasks. It addresses the limitations of existing evaluations that often focus on closed-ended tasks and overlook personalized scenarios. By combining 50 diverse research tasks with 25 authentic user profiles, the benchmark creates 250 realistic queries to test DRAs. The proposed PQR Evaluation Framework measures Personalization Alignment, Content Quality, and Factual Reliability, providing a comprehensive assessment of DRA performance in personalized research contexts."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–æ·±åº¦ç ”ç©¶çš„æœªæ¥åŸºå‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œåä¸ºä¸ªæ€§åŒ–æ·±åº¦ç ”ç©¶åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ·±åº¦ç ”ç©¶ä»£ç†åœ¨ä¸åŒä»»åŠ¡å’Œç”¨æˆ·æ¡£æ¡ˆä¸‹çš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚è¯¥åŸºå‡†ç»“åˆäº†50ä¸ªå¤šæ ·åŒ–çš„ç ”ç©¶ä»»åŠ¡å’Œ25ä¸ªçœŸå®ç”¨æˆ·æ¡£æ¡ˆï¼Œç”Ÿæˆäº†250ä¸ªç°å®çš„ç”¨æˆ·-ä»»åŠ¡æŸ¥è¯¢ã€‚ä¸ºäº†è¯„ä¼°ç³»ç»Ÿæ€§èƒ½ï¼Œæå‡ºäº†PQRè¯„ä¼°æ¡†æ¶ï¼Œåˆ†åˆ«æµ‹é‡ä¸ªæ€§åŒ–å¯¹é½ã€å†…å®¹è´¨é‡å’Œäº‹å®å¯é æ€§ã€‚é€šè¿‡å®éªŒï¼Œæœ¬æ–‡æ­ç¤ºäº†å½“å‰ç³»ç»Ÿåœ¨å¤„ç†ä¸ªæ€§åŒ–æ·±åº¦ç ”ç©¶æ–¹é¢çš„èƒ½åŠ›å’Œå±€é™æ€§ï¼Œä¸ºä¸‹ä¸€ä»£ä¸ªæ€§åŒ–AIç ”ç©¶åŠ©æ‰‹çš„å¼€å‘å’Œè¯„ä¼°å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24007",
            "title": "Sequential Diffusion Language Models",
            "url": "https://huggingface.co/papers/2509.24007",
            "abstract": "Sequential Diffusion Language Model (SDLM) enhances pre-trained autoregressive language models by adaptively determining generation length and maintaining KV-cache compatibility, achieving high efficiency and throughput.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion language models (DLMs) have strong theoretical efficiency but are limited by fixed-length decoding and incompatibility with key-value (KV) caches. Block diffusion mitigates these issues, yet still enforces a fixed block size and requires expensive training. We introduce Next Sequence Prediction (NSP), which unifies next-token and next-block prediction, enabling the model to adaptively determine the generation length at each step. When the length is fixed to 1, NSP reduces to standard next-token prediction. Building on NSP, we propose Sequential Diffusion Language Model (SDLM), which can retrofit pre-trained autoregressive language models (ALMs) at minimal cost. Specifically, SDLM performs diffusion inference within fixed-size mask blocks, but dynamically decodes consecutive subsequences based on model confidence, thereby preserving KV-cache compatibility and improving robustness to varying uncertainty and semantics across the sequence. Experiments show that SDLM matches or surpasses strong autoregressive baselines using only 3.5M training samples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the SDLM-32B model delivers even more pronounced efficiency gains, demonstrating the strong scalability potential of our modeling paradigm. Project page and codes: https://github.com/OpenGVLab/SDLM",
            "score": 18,
            "issue_id": 6153,
            "pub_date": "2025-09-28",
            "pub_date_card": {
                "ru": "28 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 28",
                "zh": "9æœˆ28æ—¥"
            },
            "hash": "d49df754c8588e2c",
            "authors": [
                "Yangzhou Liu",
                "Yue Cao",
                "Hao Li",
                "Gen Luo",
                "Zhe Chen",
                "Weiyun Wang",
                "Xiaobo Liang",
                "Biqing Qi",
                "Lijun Wu",
                "Changyao Tian",
                "Yanting Zhang",
                "Yuqiang Li",
                "Tong Lu",
                "Yu Qiao",
                "Jifeng Dai",
                "Wenhai Wang"
            ],
            "affiliations": [
                "Donghua University",
                "Fudan University",
                "Nanjing University",
                "Shanghai AI Laboratory",
                "Soochow University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24007.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#diffusion",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Sequential Diffusion Language Model (SDLM) - Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Next Sequence Prediction (NSP), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. SDLM Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°, Ğ½Ğ¾ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ KV-cache. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 3.5M Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ² 2.1 Ñ€Ğ°Ğ·Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Adaptive Text Generation with SDLM: Efficiency Meets Flexibility",
                    "desc": "The Sequential Diffusion Language Model (SDLM) improves pre-trained autoregressive language models by allowing them to adaptively decide how long to generate text while ensuring compatibility with key-value (KV) caches. Traditional diffusion language models face challenges with fixed-length decoding, but SDLM introduces Next Sequence Prediction (NSP) to unify next-token and next-block predictions, enhancing flexibility. This approach enables the model to dynamically decode subsequences based on its confidence, which helps it handle varying levels of uncertainty and semantics. Experiments demonstrate that SDLM achieves high efficiency and throughput, outperforming strong baselines with significantly fewer training samples."
                },
                "zh": {
                    "title": "é¡ºåºæ‰©æ•£è¯­è¨€æ¨¡å‹ï¼šé«˜æ•ˆç”Ÿæˆçš„æœªæ¥",
                    "desc": "é¡ºåºæ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆSDLMï¼‰é€šè¿‡è‡ªé€‚åº”ç¡®å®šç”Ÿæˆé•¿åº¦å’Œä¿æŒKVç¼“å­˜å…¼å®¹æ€§ï¼Œå¢å¼ºäº†é¢„è®­ç»ƒçš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œè¾¾åˆ°äº†é«˜æ•ˆæ€§å’Œååé‡ã€‚æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨ç†è®ºä¸Šå…·æœ‰å¼ºå¤§çš„æ•ˆç‡ï¼Œä½†å—åˆ°å›ºå®šé•¿åº¦è§£ç å’Œä¸KVç¼“å­˜ä¸å…¼å®¹çš„é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºçš„ä¸‹ä¸€åºåˆ—é¢„æµ‹ï¼ˆNSPï¼‰æ–¹æ³•ç»Ÿä¸€äº†ä¸‹ä¸€ä¸ªæ ‡è®°å’Œä¸‹ä¸€ä¸ªå—çš„é¢„æµ‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¯ä¸€æ­¥è‡ªé€‚åº”åœ°ç¡®å®šç”Ÿæˆé•¿åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒSDLMåœ¨ä»…ä½¿ç”¨350ä¸‡è®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤ŸåŒ¹é…æˆ–è¶…è¶Šå¼ºå¤§çš„è‡ªå›å½’åŸºçº¿ï¼ŒåŒæ—¶å®ç°æ¯”Qwen-2.5é«˜å‡º2.1çš„ååé‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24473",
            "title": "Euclid's Gift: Enhancing Spatial Perception and Reasoning in\n  Vision-Language Models via Geometric Surrogate Tasks",
            "url": "https://huggingface.co/papers/2509.24473",
            "abstract": "Geometry-centric fine-tuning using the Euclid30K dataset significantly improves spatial reasoning abilities in multimodal large language models across multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs).To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. To enable the model to acquire and apply Euclidean principles from these geometry problems, we employed Group Relative Policy Optimization (GRPO) to finetune the Qwen2.5VL family and RoboBrain2.0 family, inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy of all evaluated models rose from 34.5% to 40.5%, improving by 5.5 percentage points. Among them, RoboBrain2.0-Euclid-7B achieves 49.6\\% accuracy, surpassing the previous state-of-the-art model, Spatial-MLLM.To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in https://zgca-ai4edu.github.io/Euclids_Gift.",
            "score": 15,
            "issue_id": 6153,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "1cf3d7b11e8af378",
            "authors": [
                "Shijie Lian",
                "Changti Wu",
                "Laurence Tianruo Yang",
                "Hang Yuan",
                "Bin Yu",
                "Lei Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "East China Normal University",
                "Huazhong University of Science and Technology",
                "Zhengzhou University",
                "Zhongguancun Academy",
                "Zhongguancun Institute of Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24473.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#reasoning",
                    "#multimodal",
                    "#benchmark",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ AI",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Euclid30K Ñ 30 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ ÑÑ‚ĞµÑ€ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Group Relative Policy Optimization, Ğ¾Ğ½Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ² Qwen2.5VL Ğ¸ RoboBrain2.0 Ğ½Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ RoboBrain2.0-Euclid-7B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 49.6% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VSI-Bench, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹."
                },
                "en": {
                    "title": "Boosting Spatial Reasoning in MLLMs with Geometry Fine-Tuning",
                    "desc": "This paper presents a method to enhance the spatial reasoning capabilities of Multimodal Large Language Models (MLLMs) by using a specially designed dataset called Euclid30K, which contains geometry problems. The authors employed Group Relative Policy Optimization (GRPO) to fine-tune models like Qwen2.5VL and RoboBrain2.0, enabling them to better understand and apply Euclidean geometry principles. The results showed significant improvements in spatial reasoning performance across multiple benchmarks, with the RoboBrain2.0-Euclid-7B model achieving a new state-of-the-art accuracy. This research highlights the effectiveness of geometry-centric fine-tuning in developing transferable spatial skills in vision-language models."
                },
                "zh": {
                    "title": "å‡ ä½•å¾®è°ƒæå‡ç©ºé—´æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å‡ ä½•ä¸­å¿ƒçš„å¾®è°ƒæ–¹æ³•ï¼Œåˆ©ç”¨Euclid30Kæ•°æ®é›†æ˜¾è‘—æå‡äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«çº¦3ä¸‡é“å¹³é¢å’Œç«‹ä½“å‡ ä½•é—®é¢˜çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä»¥å¸®åŠ©æ¨¡å‹å­¦ä¹ å’Œåº”ç”¨æ¬§å‡ é‡Œå¾—å‡ ä½•åŸç†ã€‚é€šè¿‡ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å½¢çŠ¶ã€è®¡æ•°ã€å…³è”å®ä½“ï¼Œå¹¶è¿›è¡Œå¤šæ­¥æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡è®­ç»ƒåï¼Œæ¨¡å‹åœ¨å¤šä¸ªç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯RoboBrain2.0-Euclid-7Bæ¨¡å‹çš„å‡†ç¡®ç‡è¾¾åˆ°äº†49.6%ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23808",
            "title": "Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach\n  for LLM Reasoning in RLVR",
            "url": "https://huggingface.co/papers/2509.23808",
            "abstract": "Re-examining the exploration-exploitation trade-off in Reinforcement Learning for Verifiable Rewards through hidden-state analysis reveals opportunities for simultaneous enhancement using Effective Rank and its derivatives, leading to improved performance in diverse benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR) interprets recent progress through the lens of an exploration-exploitation trade-off, a perspective largely shaped by token-level metrics. We re-examine this perspective, proposing that this perceived trade-off may not be a fundamental constraint but rather an artifact of the measurement level. To investigate this, we shift the analysis to the semantically rich hidden-state space, adopting Effective Rank (ER) to quantify exploration and proposing its novel first- and second-order derivatives, named Effective Rank Velocity (ERV) and Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our analysis reveals that at the hidden-state level, exploration and exploitation could be decoupled (Sec. 4). This finding reveals an opportunity to enhance both capacities simultaneously. This insight motivates our method, Velocity-Exploiting Rank-Learning (VERL), the first to operationalize the principle of synergistic exploration-exploitation enhancement by directly shaping the RL advantage function. The key innovation is leveraging the theoretically stable ERA as a predictive meta-controller to create a synergistic, dual-channel incentive structure. Instead of forcing a trade-off, VERL prospectively amplifies rewards for exploration to preempt overconfidence and reinforces exploitative gains to consolidate reasoning. Experiments across diverse LLMs and reasoning benchmarks show consistent gains, including up to 21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.",
            "score": 15,
            "issue_id": 6156,
            "pub_date": "2025-09-28",
            "pub_date_card": {
                "ru": "28 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 28",
                "zh": "9æœˆ28æ—¥"
            },
            "hash": "4e729698740f3ee5",
            "authors": [
                "Fanding Huang",
                "Guanbo Huang",
                "Xiao Fan",
                "Yi He",
                "Xiao Liang",
                "Xiao Chen",
                "Qinting Jiang",
                "Faisal Nadeem Khan",
                "Jingyan Jiang",
                "Zhi Wang"
            ],
            "affiliations": [
                "Shenzhen Technology University",
                "Tsinghua Shenzhen International Graduate School, Tsinghua University",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23808.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹: Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² RL",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ (RLVR). Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¾Ğ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Effective Rank Ğ¸ ĞµÑ‘ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. Ğ˜Ñ… Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ VERL Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 21.4% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Exploration and Exploitation in RL with VERL",
                    "desc": "This paper investigates the exploration-exploitation trade-off in Reinforcement Learning for Verifiable Rewards (RLVR) by analyzing hidden states instead of traditional token-level metrics. The authors propose that the trade-off is not a fundamental limitation but a result of how performance is measured. They introduce Effective Rank (ER) and its derivatives, Effective Rank Velocity (ERV) and Effective Rank Acceleration (ERA), to better understand and enhance both exploration and exploitation simultaneously. Their method, Velocity-Exploiting Rank-Learning (VERL), uses these insights to improve reward structures, leading to significant performance gains in various benchmarks, including a notable accuracy increase on the Gaokao 2024 dataset."
                },
                "zh": {
                    "title": "ååŒå¢å¼ºæ¢ç´¢ä¸åˆ©ç”¨çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡é‡æ–°å®¡è§†äº†å¼ºåŒ–å­¦ä¹ ä¸­å¯éªŒè¯å¥–åŠ±çš„æ¢ç´¢ä¸åˆ©ç”¨æƒè¡¡ï¼Œæå‡ºè¿™ä¸€æƒè¡¡å¯èƒ½å¹¶éæ ¹æœ¬é™åˆ¶ï¼Œè€Œæ˜¯æµ‹é‡å±‚é¢çš„ä¼ªå½±ã€‚æˆ‘ä»¬é€šè¿‡åˆ†æéšè—çŠ¶æ€ç©ºé—´ï¼Œé‡‡ç”¨æœ‰æ•ˆç§©ï¼ˆEffective Rankï¼‰æ¥é‡åŒ–æ¢ç´¢ï¼Œå¹¶æå‡ºå…¶ä¸€é˜¶å’ŒäºŒé˜¶å¯¼æ•°ï¼Œåˆ†åˆ«ç§°ä¸ºæœ‰æ•ˆç§©é€Ÿåº¦ï¼ˆEffective Rank Velocityï¼‰å’Œæœ‰æ•ˆç§©åŠ é€Ÿåº¦ï¼ˆEffective Rank Accelerationï¼‰ï¼Œä»¥æ•æ‰åˆ©ç”¨åŠ¨æ€ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨éšè—çŠ¶æ€å±‚é¢ï¼Œæ¢ç´¢ä¸åˆ©ç”¨å¯ä»¥è§£è€¦ï¼Œè¿™ä¸ºåŒæ—¶å¢å¼ºä¸¤è€…æä¾›äº†æœºä¼šã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é€Ÿåº¦åˆ©ç”¨ç§©å­¦ä¹ ï¼ˆVERLï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç›´æ¥å¡‘é€ å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŠ¿å‡½æ•°ï¼Œå®ç°ååŒå¢å¼ºæ¢ç´¢ä¸åˆ©ç”¨çš„åŸåˆ™ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25123",
            "title": "From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL by\n  Composing Old Ones",
            "url": "https://huggingface.co/papers/2509.25123",
            "abstract": "Reinforcement learning enables large language models to acquire new compositional skills by combining existing ones, which transfer to different tasks and improve reasoning behaviors.  \t\t\t\t\tAI-generated summary \t\t\t\t Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.",
            "score": 14,
            "issue_id": 6155,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "6c3606d0185935f9",
            "authors": [
                "Lifan Yuan",
                "Weize Chen",
                "Yuchen Zhang",
                "Ganqu Cui",
                "Hanbin Wang",
                "Ziming You",
                "Ning Ding",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Hao Peng"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai AI Laboratory",
                "Tsinghua University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25123.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#training",
                    "#reasoning",
                    "#rl",
                    "#synthetic",
                    "#rlhf"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "RL ÑƒÑ‡Ğ¸Ñ‚ LLM ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ ĞºĞ°Ğº ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¾Ñ€",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ reinforcement learning Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑƒĞ¶Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ h(x)=g(f(x)) Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ´Ğ°Ğ¶Ğµ ĞµÑĞ»Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ f Ğ¸ g Ğ±Ñ‹Ğ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ñ‹ Ñ€Ğ°Ğ½ĞµĞµ. Ğ­Ñ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ next-token prediction, RL Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼."
                },
                "en": {
                    "title": "Reinforcement Learning: Unlocking New Skills in Language Models",
                    "desc": "This paper explores how reinforcement learning (RL) can help large language models (LLMs) develop new skills by combining existing ones, enhancing their reasoning capabilities. The authors investigate whether RL genuinely teaches LLMs new skills or simply reactivates learned strategies. Through a synthetic framework, they demonstrate that LLMs can learn to compose functions during RL training, allowing them to solve more complex tasks. The findings suggest that RL significantly alters the reasoning behaviors of LLMs, leading to improved performance on tasks without prior specific training."
                },
                "zh": {
                    "title": "å¼ºåŒ–å­¦ä¹ åŠ©åŠ›è¯­è¨€æ¨¡å‹è·å¾—æ–°æŠ€èƒ½",
                    "desc": "å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿé€šè¿‡ç»„åˆç°æœ‰æŠ€èƒ½æ¥è·å¾—æ–°çš„ç»„åˆæŠ€èƒ½ï¼Œè¿™äº›æŠ€èƒ½å¯ä»¥è½¬ç§»åˆ°ä¸åŒçš„ä»»åŠ¡ä¸­å¹¶æ”¹å–„æ¨ç†è¡Œä¸ºã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMåœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­ç¡®å®å¯ä»¥è·å¾—çœŸæ­£çš„æ–°æŠ€èƒ½ï¼Œè€Œä¸ä»…ä»…æ˜¯æ¿€æ´»å·²æœ‰çš„æ¨ç†ç­–ç•¥ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåˆæˆæ¡†æ¶æ¥æ§åˆ¶ä»»åŠ¡å¤æ‚æ€§ï¼Œå¹¶å‘ç°LLMèƒ½å¤Ÿå­¦ä¹ æœªè§çš„å‡½æ•°ç»„åˆï¼Œå¹¶ä¸”è¿™ç§ç»„åˆèƒ½åŠ›å¯ä»¥æ¨å¹¿åˆ°æ›´å¤æ‚çš„é—®é¢˜ä¸Šã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLLMåœ¨æºä»»åŠ¡ä¸Šè·å¾—çš„ç»„åˆæŠ€èƒ½å¯ä»¥è½¬ç§»åˆ°ä¸åŒçš„ç›®æ ‡ä»»åŠ¡ï¼Œå³ä½¿åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šæ²¡æœ‰è¿›è¡Œç»„åˆè®­ç»ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22799",
            "title": "VideoScore2: Think before You Score in Generative Video Evaluation",
            "url": "https://huggingface.co/papers/2509.22799",
            "abstract": "VideoScore2 is a multi-dimensional, interpretable framework for evaluating text-to-video generation, assessing visual quality, alignment, and consistency with detailed rationales.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in text-to-video generation have produced increasingly realistic and diverse content, yet evaluating such videos remains a fundamental challenge due to their multi-faceted nature encompassing visual quality, semantic alignment, and physical consistency. Existing evaluators and reward models are limited to single opaque scores, lack interpretability, or provide only coarse analysis, making them insufficient for capturing the comprehensive nature of video quality assessment. We present VideoScore2, a multi-dimensional, interpretable, and human-aligned framework that explicitly evaluates visual quality, text-to-video alignment, and physical/common-sense consistency while producing detailed chain-of-thought rationales. Our model is trained on a large-scale dataset VideoFeedback2 containing 27,168 human-annotated videos with both scores and reasoning traces across three dimensions, using a two-stage pipeline of supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to enhance analytical robustness. Extensive experiments demonstrate that VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc), while providing interpretable assessments that bridge the gap between evaluation and controllable generation through effective reward modeling for Best-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/",
            "score": 14,
            "issue_id": 6154,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "f70244e03286d72f",
            "authors": [
                "Xuan He",
                "Dongfu Jiang",
                "Ping Nie",
                "Minghao Liu",
                "Zhengxuan Jiang",
                "Mingyi Su",
                "Wentao Ma",
                "Junru Lin",
                "Chun Ye",
                "Yi Lu",
                "Keming Wu",
                "Benjamin Schneider",
                "Quy Duc Do",
                "Zhuofeng Li",
                "Yiming Jia",
                "Yuxuan Zhang",
                "Guo Cheng",
                "Haozhe Wang",
                "Wangchunshu Zhou",
                "Qunshu Lin",
                "Yuanxing Zhang",
                "Ge Zhang",
                "Wenhao Huang",
                "Wenhu Chen"
            ],
            "affiliations": [
                "AI",
                "Abaka AI",
                "Independent",
                "M-A-P",
                "Netmind.AI",
                "University of Illinois Urbana-Champaign",
                "University of Toronto",
                "University of Waterloo",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22799.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#rlhf",
                    "#interpretability",
                    "#alignment"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° AI-Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° VideoScore2 â€” Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ VideoFeedback2 Ñ 27,168 Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "VideoScore2: A Clearer Lens for Evaluating Text-to-Video Generation",
                    "desc": "VideoScore2 is a novel framework designed to evaluate text-to-video generation by focusing on three key aspects: visual quality, alignment with the input text, and physical consistency. Unlike previous models that provide single scores without interpretability, VideoScore2 offers detailed rationales for its evaluations, making it easier to understand the reasoning behind the scores. The framework is trained on a large dataset of human-annotated videos, utilizing a two-stage training process that combines supervised fine-tuning and reinforcement learning to improve its analytical capabilities. Experimental results show that VideoScore2 outperforms existing benchmarks, providing a more comprehensive and interpretable assessment of generated videos."
                },
                "zh": {
                    "title": "VideoScore2ï¼šå¤šç»´åº¦è§†é¢‘ç”Ÿæˆè¯„ä¼°æ–°æ ‡å‡†",
                    "desc": "VideoScore2æ˜¯ä¸€ä¸ªå¤šç»´åº¦ã€å¯è§£é‡Šçš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚å®ƒä¸ä»…è¯„ä¼°è§†è§‰è´¨é‡ï¼Œè¿˜è€ƒè™‘æ–‡æœ¬ä¸è§†é¢‘çš„å¯¹é½æ€§å’Œç‰©ç†ä¸€è‡´æ€§ï¼Œå¹¶æä¾›è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ã€‚ç°æœ‰çš„è¯„ä¼°æ¨¡å‹é€šå¸¸åªèƒ½ç»™å‡ºå•ä¸€çš„åˆ†æ•°ï¼Œç¼ºä¹å¯è§£é‡Šæ€§ï¼Œæ— æ³•å…¨é¢æ•æ‰è§†é¢‘è´¨é‡çš„å¤æ‚æ€§ã€‚é€šè¿‡åœ¨å¤§å‹æ•°æ®é›†VideoFeedback2ä¸Šè®­ç»ƒï¼ŒVideoScore2å±•ç¤ºäº†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†å¯è§£é‡Šçš„è¯„ä¼°ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25161",
            "title": "Rolling Forcing: Autoregressive Long Video Diffusion in Real Time",
            "url": "https://huggingface.co/papers/2509.25161",
            "abstract": "Rolling Forcing is a novel video generation technique that reduces error accumulation in long video streams by using joint denoising, attention sink mechanism, and efficient training with non-overlapping windows.  \t\t\t\t\tAI-generated summary \t\t\t\t Streaming video generation, as one fundamental component in interactive world models and neural game engines, aims to generate high-quality, low-latency, and temporally coherent long video streams. However, most existing work suffers from severe error accumulation that often significantly degrades the generated stream videos over long horizons. We design Rolling Forcing, a novel video generation technique that enables streaming long videos with minimal error accumulation. Rolling Forcing comes with three novel designs. First, instead of iteratively sampling individual frames, which accelerates error propagation, we design a joint denoising scheme that simultaneously denoises multiple frames with progressively increasing noise levels. This design relaxes the strict causality across adjacent frames, effectively suppressing error growth. Second, we introduce the attention sink mechanism into the long-horizon stream video generation task, which allows the model to keep key value states of initial frames as a global context anchor and thereby enhances long-term global consistency. Third, we design an efficient training algorithm that enables few-step distillation over largely extended denoising windows. This algorithm operates on non-overlapping windows and mitigates exposure bias conditioned on self-generated histories. Extensive experiments show that Rolling Forcing enables real-time streaming generation of multi-minute videos on a single GPU, with substantially reduced error accumulation.",
            "score": 12,
            "issue_id": 6154,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "a2272753ec4619e6",
            "authors": [
                "Kunhao Liu",
                "Wenbo Hu",
                "Jiale Xu",
                "Ying Shan",
                "Shijian Lu"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25161.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#optimization",
                    "#games",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº",
                    "desc": "Rolling Forcing - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ»Ğ°Ğ±Ğ»ÑĞµÑ‚ ÑÑ‚Ñ€Ğ¾Ğ³ÑƒÑ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ğ¼Ğ¸ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ attention sink ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² ĞºĞ°Ğº Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¸Ğ½ÑƒÑ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ GPU Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº."
                },
                "en": {
                    "title": "Minimizing Errors in Long Video Generation with Rolling Forcing",
                    "desc": "Rolling Forcing is a new technique for generating long video streams that minimizes error accumulation, which is a common problem in video generation. It uses a joint denoising approach to process multiple frames at once, reducing the propagation of errors that can occur when frames are generated one by one. The method also incorporates an attention sink mechanism to maintain important information from earlier frames, ensuring consistency throughout the video. Additionally, it features an efficient training process that allows for quick learning over extended periods, making it possible to generate high-quality videos in real-time."
                },
                "zh": {
                    "title": "å‡å°‘è§†é¢‘ç”Ÿæˆä¸­çš„é”™è¯¯ç´¯ç§¯",
                    "desc": "Rolling Forcingæ˜¯ä¸€ç§æ–°çš„è§†é¢‘ç”ŸæˆæŠ€æœ¯ï¼Œæ—¨åœ¨å‡å°‘é•¿è§†é¢‘æµä¸­çš„é”™è¯¯ç´¯ç§¯ã€‚å®ƒé€šè¿‡è”åˆå»å™ªã€æ³¨æ„åŠ›æ±‡èšæœºåˆ¶å’Œé«˜æ•ˆçš„éé‡å çª—å£è®­ç»ƒæ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚è¯¥æ–¹æ³•åŒæ—¶å»å™ªå¤šä¸ªå¸§ï¼ŒæŠ‘åˆ¶é”™è¯¯ä¼ æ’­ï¼Œå¹¶ä¿æŒåˆå§‹å¸§çš„å…³é”®çŠ¶æ€ä»¥å¢å¼ºé•¿æœŸä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒRolling Forcingèƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šå®æ—¶ç”Ÿæˆå¤šåˆ†é’Ÿçš„è§†é¢‘ï¼Œæ˜¾è‘—é™ä½é”™è¯¯ç´¯ç§¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25077",
            "title": "BRIDGE - Building Reinforcement-Learning Depth-to-Image Data Generation\n  Engine for Monocular Depth Estimation",
            "url": "https://huggingface.co/papers/2509.25077",
            "abstract": "BRIDGE uses RL-optimized depth-to-image generation to create a large, diverse dataset, enhancing monocular depth estimation robustness and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Monocular Depth Estimation (MDE) is a foundational task for computer vision. Traditional methods are limited by data scarcity and quality, hindering their robustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image (D2I) generation framework that synthesizes over 20M realistic and geometrically accurate RGB images, each intrinsically paired with its ground truth depth, from diverse source depth maps. Then we train our depth estimation model on this dataset, employing a hybrid supervision strategy that integrates teacher pseudo-labels with ground truth depth for comprehensive and robust training. This innovative data generation and training paradigm enables BRIDGE to achieve breakthroughs in scale and domain diversity, consistently outperforming existing state-of-the-art approaches quantitatively and in complex scene detail capture, thereby fostering general and robust depth features. Code and models are available at https://dingning-liu.github.io/bridge.github.io/.",
            "score": 12,
            "issue_id": 6155,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "f1a1fef4161fc727",
            "authors": [
                "Dingning Liu",
                "Haoyu Guo",
                "Jingyi Zhou",
                "Tong He"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25077.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#optimization",
                    "#rl",
                    "#dataset",
                    "#synthetic",
                    "#cv"
                ],
                "emoji": "ğŸŒ‰",
                "ru": {
                    "title": "ĞœĞ¾ÑÑ‚Ğ¸Ğº Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ BRIDGE Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾nocular depth estimation - Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ reinforcement learning Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 20 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… RGB Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¼ĞµÑ‚ĞºĞ¸ Ğ¾Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ñ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ BRIDGE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ state-of-the-art Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼Ğ¸ ÑÑ†ĞµĞ½."
                },
                "en": {
                    "title": "BRIDGE: Revolutionizing Depth Estimation with RL-Optimized Data Generation",
                    "desc": "BRIDGE is a novel framework that utilizes reinforcement learning (RL) to optimize the generation of depth-to-image (D2I) data, creating a vast dataset of over 20 million realistic RGB images paired with their corresponding ground truth depth maps. This approach addresses the limitations of traditional monocular depth estimation (MDE) methods, which often suffer from insufficient and low-quality data. By employing a hybrid supervision strategy that combines teacher pseudo-labels with actual depth data, BRIDGE enhances the training process for depth estimation models. As a result, BRIDGE significantly improves performance and robustness in depth estimation tasks, outperforming existing methods in both quantitative metrics and the ability to capture complex scene details."
                },
                "zh": {
                    "title": "BRIDGEï¼šæ·±åº¦ä¼°è®¡çš„æ–°çªç ´",
                    "desc": "BRIDGEæ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–çš„æ·±åº¦åˆ°å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨åˆ›å»ºä¸€ä¸ªå¤§å‹å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œä»¥å¢å¼ºå•ç›®æ·±åº¦ä¼°è®¡çš„é²æ£’æ€§å’Œæ€§èƒ½ã€‚è¯¥æ¡†æ¶åˆæˆäº†è¶…è¿‡2000ä¸‡å¼ çœŸå®ä¸”å‡ ä½•å‡†ç¡®çš„RGBå›¾åƒï¼Œå¹¶ä¸å…¶çœŸå®æ·±åº¦ä¸€ä¸€å¯¹åº”ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºå’Œè´¨é‡ä¸Šçš„é™åˆ¶ã€‚é€šè¿‡é‡‡ç”¨æ··åˆç›‘ç£ç­–ç•¥ï¼Œå°†æ•™å¸ˆä¼ªæ ‡ç­¾ä¸çœŸå®æ·±åº¦ç»“åˆï¼ŒBRIDGEçš„æ·±åº¦ä¼°è®¡æ¨¡å‹åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå–å¾—äº†æ˜¾è‘—çš„çªç ´ã€‚æœ€ç»ˆï¼ŒBRIDGEåœ¨è§„æ¨¡å’Œé¢†åŸŸå¤šæ ·æ€§ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22824",
            "title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2509.22824",
            "abstract": "Critique Reinforcement Learning (CRL) enhances LLMs by teaching them to generate critiques, leading to improved performance on code generation and logic reasoning tasks compared to standard RL.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning (RL) has emerged as a popular training paradigm, particularly when paired with reasoning models. While effective, it primarily focuses on generating responses and lacks mechanisms to explicitly foster critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT) and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly teaching LLMs how to critique. Motivated by them, we propose Critique Reinforcement Learning (CRL), where the model is tasked with generating a critique for a given (question, solution) pair. The reward is determined solely by whether the final judgment label c in {True, False} of the generated critique aligns with the ground-truth judgment c^*. Building on this point, we introduce Critique-Coder, which is trained on a hybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL data. We fine-tune multiple models (Critique-Coder) and evaluate them on different benchmarks to show their advantages over RL-only models. We show that Critique-Coder consistently outperforms RL-only baselines on all the evaluated benchmarks. Notably, our Critique-Coder-8B can reach over 60\\% on LiveCodeBench (v5), outperforming other reasoning models like DeepCoder-14B and GPT-o1. Beyond code generation, Critique-Coder also demonstrates enhanced general reasoning abilities, as evidenced by its better performance on logic reasoning tasks from the BBEH dataset. This indicates that the application of CRL on coding datasets enhances general reasoning and critique abilities, which are transferable across a broad range of tasks. Hence, we believe that CRL works as a great complement to standard RL for LLM reasoning.",
            "score": 12,
            "issue_id": 6154,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "18f3e09fc9d2fcc9",
            "authors": [
                "Chi Ruan",
                "Dongfu Jiang",
                "Yubo Wang",
                "Wenhu Chen"
            ],
            "affiliations": [
                "University of Waterloo",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22824.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#optimization",
                    "#benchmark",
                    "#rl",
                    "#rlhf",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ»Ğ°ĞµÑ‚ AI ÑƒĞ¼Ğ½ĞµĞµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Critique Reinforcement Learning (CRL), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ LLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Critique-Coder, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ CRL Ğ² Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¸ 80/20. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ RL. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLMs with Critique Reinforcement Learning",
                    "desc": "Critique Reinforcement Learning (CRL) improves large language models (LLMs) by training them to generate critiques for given question-solution pairs. This method enhances their performance in tasks like code generation and logic reasoning, surpassing traditional reinforcement learning (RL) approaches. The model receives rewards based on the accuracy of its critique compared to a ground-truth judgment, fostering better reasoning skills. By integrating CRL with standard RL, the proposed Critique-Coder model shows significant improvements across various benchmarks, demonstrating its effectiveness in both coding and general reasoning tasks."
                },
                "zh": {
                    "title": "æ‰¹è¯„å¼ºåŒ–å­¦ä¹ ï¼šæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…³é”®",
                    "desc": "æ‰¹è¯„å¼ºåŒ–å­¦ä¹ ï¼ˆCRLï¼‰é€šè¿‡æ•™å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆæ‰¹è¯„ï¼Œæå‡äº†å®ƒä»¬åœ¨ä»£ç ç”Ÿæˆå’Œé€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç›¸æ¯”ï¼ŒCRLä¸“æ³¨äºç”Ÿæˆæ‰¹è¯„ï¼Œä»è€Œä¿ƒè¿›æ¨¡å‹çš„åæ€èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºçš„æ‰¹è¯„ç¼–ç å™¨ï¼ˆCritique-Coderï¼‰ç»“åˆäº†RLå’ŒCRLçš„ä¼˜ç‚¹ï¼Œç»è¿‡è®­ç»ƒååœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä»…ä½¿ç”¨RLçš„æ¨¡å‹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒCRLä¸ä»…æé«˜äº†ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œè¿˜å¢å¼ºäº†æ¨¡å‹åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæ˜¾ç¤ºå‡ºå…¶å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22820",
            "title": "MMPB: It's Time for Multi-Modal Personalization",
            "url": "https://huggingface.co/papers/2509.22820",
            "abstract": "MMPB is a benchmark for evaluating the personalization capabilities of Vision-Language Models across various tasks and concepts, revealing significant challenges in maintaining consistency and adapting to user preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual personalization is essential in user-facing AI systems such as smart homes and healthcare, where aligning model behavior with user-centric concepts is critical. However, recent large Vision-Language Models (VLMs), despite their broad applicability, remain underexplored in their ability to adapt to individual users. In this paper, we introduce MMPB, the first extensive benchmark for evaluating VLMs on personalization. MMPB comprises 10k image-query pairs and includes 111 personalizable concepts across four categories: humans, animals, objects, and characters, with the human category enriched with preference-grounded queries. We structure personalization into three main task types, each highlighting a different key property of VLMs. Using 23 widely used VLMs including both open- and closed-source models, we evaluate personalization performance via a three-stage protocol: concept injection, multi-turn dialogue, and personalized querying. Our findings indicate that most VLMs (including some closed-source models) struggle with personalization, particularly in maintaining consistency over dialogue, handling user preferences, and adapting to visual cues. Our analysis reveals that the challenges in VLM personalization (such as refusal behaviors and long-context forgetting) highlight substantial room for improvement. By identifying these limitations and offering a scalable benchmark, MMPB offers valuable insights and a solid foundation for future research toward truly personalized multi-modal AI. Project Page: aidaslab.github.io/MMPB",
            "score": 12,
            "issue_id": 6158,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "7c8dc1b8167577c1",
            "authors": [
                "Jaeik Kim",
                "Woojin Kim",
                "Woohyeon Park",
                "Jaeyoung Do"
            ],
            "affiliations": [
                "AIDAS Laboratory, IPAI & ECE, Seoul National University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22820.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#alignment",
                    "#cv",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ‘¤",
                "ru": {
                    "title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Vision-Language Models: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ benchmark Ğ²Ñ‹ÑĞ²Ğ¸Ğ» ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ MMPB - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¹ benchmark Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Vision-Language Models Ğº Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Benchmark Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 10 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ñ 111 Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹: Ğ»ÑĞ´Ğ¸, Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ñ‹Ğµ, Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ¸. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 23 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… VLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğµ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğº Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ VLM Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ AI."
                },
                "en": {
                    "title": "MMPB: Pioneering Personalization in Vision-Language Models",
                    "desc": "MMPB is a new benchmark designed to assess how well Vision-Language Models (VLMs) can personalize their responses based on user preferences. It includes 10,000 image-query pairs and focuses on 111 concepts across categories like humans and animals, emphasizing the importance of user-centric personalization in AI applications. The study evaluates 23 VLMs through tasks that test their ability to inject concepts, engage in multi-turn dialogue, and respond to personalized queries. Results show that many VLMs struggle with consistency and adapting to user preferences, indicating significant areas for improvement in personalized AI systems."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–è¯„ä¼°æ–°åŸºå‡†ï¼šMMPB",
                    "desc": "MMPBæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹ä¸ªæ€§åŒ–èƒ½åŠ›çš„åŸºå‡†ï¼Œæ¶µç›–äº†å¤šä¸ªä»»åŠ¡å’Œæ¦‚å¿µã€‚è¯¥åŸºå‡†åŒ…å«10,000ä¸ªå›¾åƒ-æŸ¥è¯¢å¯¹ï¼Œæ¶‰åŠäººç±»ã€åŠ¨ç‰©ã€ç‰©ä½“å’Œè§’è‰²å››ä¸ªç±»åˆ«ï¼Œå¹¶ç‰¹åˆ«å…³æ³¨äººç±»ç±»åˆ«çš„ç”¨æˆ·åå¥½æŸ¥è¯¢ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®¸å¤šè§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ä¸ªæ€§åŒ–æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹è¯ä¸€è‡´æ€§ã€ç”¨æˆ·åå¥½å¤„ç†å’Œè§†è§‰çº¿ç´¢é€‚åº”æ–¹é¢ã€‚é€šè¿‡MMPBï¼Œæˆ‘ä»¬ä¸ºæœªæ¥çš„ä¸ªæ€§åŒ–å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç ”ç©¶æä¾›äº†é‡è¦çš„è§è§£å’ŒåŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25191",
            "title": "VGGT-X: When VGGT Meets Dense Novel View Synthesis",
            "url": "https://huggingface.co/papers/2509.25191",
            "abstract": "VGGT-X addresses VRAM and output quality issues in scaling 3D Foundation Models for dense Novel View Synthesis without relying on COLMAP.  \t\t\t\t\tAI-generated summary \t\t\t\t We study the problem of applying 3D Foundation Models (3DFMs) to dense Novel View Synthesis (NVS). Despite significant progress in Novel View Synthesis powered by NeRF and 3DGS, current approaches remain reliant on accurate 3D attributes (e.g., camera poses and point clouds) acquired from Structure-from-Motion (SfM), which is often slow and fragile in low-texture or low-overlap captures. Recent 3DFMs showcase orders of magnitude speedup over the traditional pipeline and great potential for online NVS. But most of the validation and conclusions are confined to sparse-view settings. Our study reveals that naively scaling 3DFMs to dense views encounters two fundamental barriers: dramatically increasing VRAM burden and imperfect outputs that degrade initialization-sensitive 3D training. To address these barriers, we introduce VGGT-X, incorporating a memory-efficient VGGT implementation that scales to 1,000+ images, an adaptive global alignment for VGGT output enhancement, and robust 3DGS training practices. Extensive experiments show that these measures substantially close the fidelity gap with COLMAP-initialized pipelines, achieving state-of-the-art results in dense COLMAP-free NVS and pose estimation. Additionally, we analyze the causes of remaining gaps with COLMAP-initialized rendering, providing insights for the future development of 3D foundation models and dense NVS. Our project page is available at https://dekuliutesla.github.io/vggt-x.github.io/",
            "score": 11,
            "issue_id": 6155,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "3c76ba3e06f19be4",
            "authors": [
                "Yang Liu",
                "Chuanchen Luo",
                "Zimo Tang",
                "Junran Peng",
                "Zhaoxiang Zhang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences",
                "Shandong University",
                "University of Chinese Academy of Sciences",
                "University of Science and Technology Beijing"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25191.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#3d",
                    "#optimization"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞŸĞ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ±ĞµĞ· COLMAP Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 3D Foundation Models",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ 3D Foundation Models Ğ´Ğ»Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² (Novel View Synthesis). Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Structure-from-Motion Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ· Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº. ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 3DFM Ğ½Ğ° Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´Ñ‹ ÑÑ‚Ğ°Ğ»ĞºĞ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ VGGT-X Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ 3DGS."
                },
                "en": {
                    "title": "VGGT-X: Scaling 3D Models for High-Quality View Synthesis Without COLMAP",
                    "desc": "VGGT-X tackles the challenges of VRAM usage and output quality when scaling 3D Foundation Models (3DFMs) for dense Novel View Synthesis (NVS) without using COLMAP. Traditional methods rely on accurate 3D attributes from Structure-from-Motion, which can be slow and unreliable in certain conditions. The paper introduces a memory-efficient implementation and adaptive techniques to enhance output quality, allowing for the processing of over 1,000 images. Experimental results demonstrate that VGGT-X significantly improves fidelity in dense NVS and pose estimation compared to existing methods, while also providing insights for future advancements in 3D modeling."
                },
                "zh": {
                    "title": "VGGT-Xï¼šé«˜æ•ˆè§£å†³å¯†é›†æ–°è§†å›¾åˆæˆä¸­çš„æŒ‘æˆ˜",
                    "desc": "VGGT-X ç ”ç©¶äº†åœ¨ä¸ä¾èµ– COLMAP çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•è§£å†³åœ¨å¯†é›†æ–°è§†å›¾åˆæˆä¸­æ‰©å±• 3D åŸºç¡€æ¨¡å‹æ—¶çš„ VRAM å’Œè¾“å‡ºè´¨é‡é—®é¢˜ã€‚å°½ç®¡ç°æœ‰çš„æ–°è§†å›¾åˆæˆæ–¹æ³•å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»ç„¶ä¾èµ–äºä»è¿åŠ¨ç»“æ„ä¸­è·å–çš„å‡†ç¡® 3D å±æ€§ï¼Œè¿™åœ¨ä½çº¹ç†æˆ–ä½é‡å çš„åœºæ™¯ä¸­å¾€å¾€è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç®€å•åœ°å°† 3D åŸºç¡€æ¨¡å‹æ‰©å±•åˆ°å¯†é›†è§†å›¾ä¼šé¢ä¸´ VRAM è´Ÿæ‹…å¢åŠ å’Œè¾“å‡ºè´¨é‡ä¸‹é™çš„ä¸¤ä¸ªä¸»è¦éšœç¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº† VGGT-Xï¼Œé‡‡ç”¨äº†å†…å­˜é«˜æ•ˆçš„å®ç°å’Œè‡ªé€‚åº”å…¨å±€å¯¹é½æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†å¯†é›†æ–°è§†å›¾åˆæˆçš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24981",
            "title": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable\n  Rewards",
            "url": "https://huggingface.co/papers/2509.24981",
            "abstract": "ROVER, a minimalist RL method, achieves superior performance and diversity in LLM math reasoning by leveraging Q-values from a fixed random policy, bypassing complex policy iteration.  \t\t\t\t\tAI-generated summary \t\t\t\t RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for improving the reasoning abilities of large language models (LLMs). Current methods rely primarily on policy optimization frameworks like PPO and GRPO, which follow generalized policy iteration that alternates between evaluating the current policy's value and improving the policy based on evaluation. While effective, they often suffer from training instability and diversity collapse, requiring complex heuristic tricks and careful tuning. We observe that standard RLVR in math reasoning can be formalized as a specialized finite-horizon Markov Decision Process with deterministic state transitions, tree-structured dynamics, and binary terminal rewards. Though large in scale, the underlying structure is simpler than general-purpose control settings for which popular RL algorithms (e.g., PPO) were developed, suggesting that several sophisticated techniques in existing methods may be reduced or even omitted. Based on this insight, we prove a surprising result: the optimal action can be recovered from the Q-function of a fixed uniformly random policy, thereby bypassing the generalized policy iteration loop and its associated heuristics. We introduce Random Policy Valuation for Diverse Reasoning (ROVER) to translate this principle into a practical and scalable algorithm for LLM math reasoning, a minimalist yet highly effective RL method that samples actions from a softmax over these uniform-policy Q-values. ROVER preserves diversity throughout training, allowing sustained exploration of multiple valid pathways. Across multiple base models and standard math reasoning benchmarks, ROVER demonstrates superior performance in both quality (+8.2 on pass@1, +16.8 on pass@256) and diversity (+17.6\\%), despite its radical simplification compared to strong, complicated existing methods.",
            "score": 11,
            "issue_id": 6156,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "9271e4a6f3547297",
            "authors": [
                "Haoran He",
                "Yuxiao Ye",
                "Qingpeng Cai",
                "Chen Hu",
                "Binxing Jiao",
                "Daxin Jiang",
                "Ling Pan"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Kuaishou Technology",
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24981.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#rlhf",
                    "#optimization"
                ],
                "emoji": "ğŸ²",
                "ru": {
                    "title": "Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ROVER - Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¸Ğ· Q-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº. ĞœĞµÑ‚Ğ¾Ğ´ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· softmax Ğ½Ğ°Ğ´ Q-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ROVER Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ (+8.2 Ğ½Ğ° pass@1, +16.8 Ğ½Ğ° pass@256) Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ (+17.6%) Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Simplifying RL for Superior Math Reasoning in LLMs",
                    "desc": "ROVER is a new reinforcement learning (RL) method designed to enhance the math reasoning capabilities of large language models (LLMs) by simplifying the training process. Instead of using complex policy optimization techniques like PPO, ROVER utilizes Q-values derived from a fixed random policy, which allows it to avoid the instability and diversity issues common in traditional methods. This approach enables ROVER to maintain a diverse set of reasoning pathways while achieving better performance on math reasoning tasks. The results show that ROVER outperforms existing methods in both quality and diversity, proving that simpler techniques can be highly effective in RL applications."
                },
                "zh": {
                    "title": "ROVERï¼šç®€åŒ–çš„å¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ•°å­¦æ¨ç†èƒ½åŠ›",
                    "desc": "ROVERæ˜¯ä¸€ç§ç®€åŒ–çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä¸“æ³¨äºå¤§è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†ã€‚å®ƒé€šè¿‡åˆ©ç”¨å›ºå®šéšæœºç­–ç•¥çš„Qå€¼ï¼Œé¿å…äº†å¤æ‚çš„ç­–ç•¥è¿­ä»£è¿‡ç¨‹ï¼Œä»è€Œæé«˜äº†æ€§èƒ½å’Œå¤šæ ·æ€§ã€‚ä¸ä¼ ç»Ÿçš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒROVERåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒäº†å¤šæ ·æ€§ï¼Œèƒ½å¤ŸæŒç»­æ¢ç´¢å¤šæ¡æœ‰æ•ˆè·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒROVERåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè´¨é‡å’Œå¤šæ ·æ€§å‡æœ‰æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24663",
            "title": "InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long\n  Adaptation",
            "url": "https://huggingface.co/papers/2509.24663",
            "abstract": "A dense-sparse switchable attention framework, InfLLM-V2, enhances long-sequence processing in large language models by efficiently adapting between dense and sparse attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-sequence processing is a critical capability for modern large language models. However, the self-attention mechanism in the standard Transformer architecture faces severe computational and memory bottlenecks when processing long sequences. While trainable sparse attention methods offer a promising solution, existing approaches such as NSA introduce excessive extra parameters and disrupt the conventional pretrain-on-short, finetune-on-long workflow, resulting in slow convergence and difficulty in acceleration. To overcome these limitations, we introduce dense-sparse switchable attention framework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that seamlessly adapts models from short to long sequences. Specifically, InfLLM-V2 reuses dense attention parameters through parameter-free architecture modification, maintaining consistency between short and long sequence processing. Additionally, InfLLM-V2 ensures computational efficiency across all sequence lengths, by using dense attention for short inputs and smoothly transitioning to sparse attention for long sequences. To achieve practical acceleration, we further introduce an efficient implementation of InfLLM-V2 that significantly reduces the computational overhead. Our experiments on long-context understanding and chain-of-thought reasoning demonstrate that InfLLM-V2 is 4times faster than dense attention while retaining 98.1% and 99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we have trained and open-sourced MiniCPM4.1 (https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model, providing a reproducible implementation for the research community.",
            "score": 11,
            "issue_id": 6154,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "bc083d082afe02e9",
            "authors": [
                "Weilin Zhao",
                "Zihan Zhou",
                "Zhou Su",
                "Chaojun Xiao",
                "Yuxuan Li",
                "Yanghao Li",
                "Yudi Zhang",
                "Weilun Zhao",
                "Zhen Li",
                "Yuxiang Huang",
                "Ao Sun",
                "Xu Han",
                "Zhiyuan Liu"
            ],
            "affiliations": [
                "Harbin Institute of Technology",
                "OpenBMB",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24663.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#open_source",
                    "#reasoning",
                    "#training",
                    "#long_context"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ InfLLM-V2 â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒĞ¼ĞµĞ»Ğ¾ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ (dense Ğ¸ sparse attention) Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ° Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² 4 Ñ€Ğ°Ğ·Ğ° Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ 98-99% ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Efficient Long-Sequence Processing with InfLLM-V2",
                    "desc": "The paper presents InfLLM-V2, a novel framework that improves the processing of long sequences in large language models by utilizing a dense-sparse switchable attention mechanism. This approach allows the model to efficiently transition between dense attention for short sequences and sparse attention for longer ones, addressing the computational and memory challenges of traditional self-attention methods. InfLLM-V2 maintains parameter efficiency by reusing dense attention parameters, which helps preserve performance while speeding up processing times. Experimental results show that InfLLM-V2 is significantly faster than standard dense attention, achieving high performance retention in long-context tasks."
                },
                "zh": {
                    "title": "InfLLM-V2ï¼šé«˜æ•ˆå¤„ç†é•¿åºåˆ—çš„æ³¨æ„åŠ›æ¡†æ¶",
                    "desc": "InfLLM-V2æ˜¯ä¸€ç§å¯†é›†-ç¨€ç–å¯åˆ‡æ¢æ³¨æ„åŠ›æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é•¿åºåˆ—çš„å¤„ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨çŸ­åºåˆ—å’Œé•¿åºåˆ—ä¹‹é—´é«˜æ•ˆåˆ‡æ¢å¯†é›†å’Œç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œè§£å†³äº†ä¼ ç»ŸTransformeråœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„è®¡ç®—å’Œå†…å­˜ç“¶é¢ˆã€‚InfLLM-V2é€šè¿‡å‚æ•°æ— å…³çš„æ¶æ„ä¿®æ”¹é‡ç”¨å¯†é›†æ³¨æ„åŠ›å‚æ•°ï¼Œç¡®ä¿çŸ­åºåˆ—å’Œé•¿åºåˆ—å¤„ç†çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶åœ¨æ‰€æœ‰åºåˆ—é•¿åº¦ä¸Šä¿æŒè®¡ç®—æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒInfLLM-V2åœ¨é•¿ä¸Šä¸‹æ–‡ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­æ¯”å¯†é›†æ³¨æ„åŠ›å¿«4å€ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è¾¾98.1%å’Œ99.7%çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23285",
            "title": "Toward Effective Tool-Integrated Reasoning via Self-Evolved Preference\n  Learning",
            "url": "https://huggingface.co/papers/2509.23285",
            "abstract": "Tool-Light framework improves large language models' tool-integrated reasoning efficiency and accuracy by leveraging information entropy and a two-stage fine-tuning process.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to improve their internal reasoning ability by integrating external tools. However, models employing TIR often display suboptimal behaviors, such as insufficient or excessive tool usage and overthinking after tool calls. The challenge of incentivizing LLMs to perform TIR efficiently and accurately, while stabilizing the reasoning process, remains an open question. In this paper, we start by exploring the impact of tool calls on model reasoning from the perspective of information entropy. Our findings indicate that tool call results lead to a distinct change in the information entropy of subsequent reasoning, with the overall entropy of the reasoning chain varying based on the number of tool calls. Building on these insights, we propose Tool-Light, a framework designed to encourage LLMs to perform TIR efficiently and accurately. Our framework includes dataset construction and multi-stage fine-tuning. For dataset construction, we employ continuous self-evolved sampling using the fine-tuned model, integrating both vanilla sampling and entropy-guided sampling. Besides, we establish strict criteria for selecting positive-negative pairs during sampling. The training process involves a two-stage approach, comprising Supervised Fine-Tuning (SFT) and Self-Evolved Direct Preference Optimization (DPO). Experimental results on 10 datasets demonstrate the effectiveness of Tool-Light, significantly improving the model's efficiency in executing TIR tasks.",
            "score": 11,
            "issue_id": 6152,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "a2cf471319463564",
            "authors": [
                "Yifei Chen",
                "Guanting Dong",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23285.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#optimization",
                    "#reasoning",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ”§",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Tool-Light Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ, Ğ¾Ğ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ supervised fine-tuning Ğ¸ direct preference optimization Ñ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 10 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Tool-Integrated Reasoning with Tool-Light Framework",
                    "desc": "The Tool-Light framework enhances the efficiency and accuracy of large language models (LLMs) in tool-integrated reasoning (TIR) by utilizing information entropy and a two-stage fine-tuning process. It addresses common issues in TIR, such as improper tool usage and overthinking, by analyzing how tool calls affect the reasoning process. The framework includes a novel dataset construction method that combines vanilla and entropy-guided sampling, along with strict criteria for selecting training pairs. Experimental results show that Tool-Light significantly improves LLM performance across multiple datasets, making TIR more effective."
                },
                "zh": {
                    "title": "Tool-Lightæ¡†æ¶ï¼šæå‡æ¨ç†æ•ˆç‡ä¸å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTool-Lightçš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰ä¸­çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡ä¿¡æ¯ç†µçš„è§†è§’ï¼Œç ”ç©¶äº†å·¥å…·è°ƒç”¨å¯¹æ¨¡å‹æ¨ç†çš„å½±å“ï¼Œå‘ç°å·¥å…·è°ƒç”¨ç»“æœä¼šæ˜¾è‘—æ”¹å˜åç»­æ¨ç†çš„ä¿¡æ¯ç†µã€‚Tool-Lightæ¡†æ¶åŒ…æ‹¬æ•°æ®é›†æ„å»ºå’Œå¤šé˜¶æ®µå¾®è°ƒï¼Œé‡‡ç”¨è‡ªæˆ‘æ¼”åŒ–é‡‡æ ·å’Œä¸¥æ ¼çš„æ­£è´Ÿæ ·æœ¬é€‰æ‹©æ ‡å‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨10ä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹æ‰§è¡ŒTIRä»»åŠ¡çš„æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22572",
            "title": "Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs\n  at Test Time",
            "url": "https://huggingface.co/papers/2509.22572",
            "abstract": "Dynamic Experts Search (DES) enhances large language models by controlling expert activation during inference, improving accuracy and stability without additional cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocating additional computation during inference. However, existing approaches primarily rely on output-level sampling while overlooking the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we observe that varying the number of activated experts yields complementary solution sets with stable accuracy, revealing a new and underexplored source of diversity. Motivated by this observation, we propose Dynamic Experts Search (DES), a TTS strategy that elevates expert activation into a controllable dimension of the search space. DES integrates two key components: (1) Dynamic MoE, which enables direct control of expert counts during inference to generate diverse reasoning trajectories without additional cost; and (2) Expert Configuration Inheritance, which preserves consistent expert counts within a reasoning path while varying them across runs, thereby balancing stability and diversity throughout the search. Extensive experiments across MoE architectures, verifiers and reasoning benchmarks (i.e., math, code and knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional cost. These results highlight DES as a practical and scalable form of architecture-aware TTS, illustrating how structural flexibility in modern LLMs can advance reasoning.",
            "score": 11,
            "issue_id": 6154,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "9bf9aff7575fc967",
            "authors": [
                "Yixuan Han",
                "Fan Ma",
                "Ruijie Quan",
                "Yi Yang"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22572.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Dynamic Experts Search (DES), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. Ğ’ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‚Ğ¸Ğ¿Ğ° Mixture-of-Experts Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. DES Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Dynamic MoE Ğ´Ğ»Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Expert Configuration Inheritance Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Test-Time Scaling Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚."
                },
                "en": {
                    "title": "Unlocking Diverse Reasoning with Dynamic Experts Search",
                    "desc": "Dynamic Experts Search (DES) is a method that improves large language models by managing which experts are activated during their operation, leading to better accuracy and stability without extra costs. It builds on the idea of Test-Time Scaling (TTS) but focuses on the model's architecture rather than just output sampling. By allowing control over the number of active experts, DES creates diverse reasoning paths while maintaining consistent expert usage within each path. Experiments show that DES outperforms traditional TTS methods, demonstrating its effectiveness in enhancing reasoning capabilities in various tasks."
                },
                "zh": {
                    "title": "åŠ¨æ€ä¸“å®¶æœç´¢ï¼šæå‡æ¨ç†çš„å‡†ç¡®æ€§ä¸ç¨³å®šæ€§",
                    "desc": "åŠ¨æ€ä¸“å®¶æœç´¢ï¼ˆDESï¼‰é€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ§åˆ¶ä¸“å®¶çš„æ¿€æ´»ï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼Œè€Œæ— éœ€é¢å¤–æˆæœ¬ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„çš„çµæ´»æ€§ï¼Œå‘ç°æ¿€æ´»ä¸“å®¶æ•°é‡çš„å˜åŒ–å¯ä»¥äº§ç”Ÿäº’è¡¥çš„è§£å†³æ–¹æ¡ˆé›†ã€‚DESçš„ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†æ˜¯åŠ¨æ€MoEå’Œä¸“å®¶é…ç½®ç»§æ‰¿ï¼Œå‰è€…å…è®¸åœ¨æ¨ç†æ—¶ç›´æ¥æ§åˆ¶ä¸“å®¶æ•°é‡ï¼Œåè€…åˆ™åœ¨æ¨ç†è·¯å¾„ä¸­ä¿æŒä¸€è‡´çš„ä¸“å®¶æ•°é‡ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒDESåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰çš„æµ‹è¯•æ—¶æ‰©å±•æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶ä½œä¸ºä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„æ¶æ„æ„ŸçŸ¥ç­–ç•¥çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25176",
            "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved\n  Compression",
            "url": "https://huggingface.co/papers/2509.25176",
            "abstract": "SIRI, a reinforcement learning approach with interleaved compression and expansion, enhances the efficiency and accuracy of large reasoning models by dynamically adjusting the reasoning budget.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved Compression, a simple yet effective RL approach for Large Reasoning Models (LRMs) that enables more efficient and accurate reasoning. Existing studies have observed repetitive thinking patterns in LRMs, and attempts to reduce them often come at the cost of performance. In this paper, we show that this trade-off can be overcome through a training regime that iteratively alternates between compressing and expanding the reasoning budget, by dynamically adjusting the maximum rollout length during training. The compression phase cuts the rollout length, forcing the model to make precise and valuable decisions within a limited context, which effectively reduces redundant tokens and increases reasoning density. The expansion phase then relaxes the length limit, providing space for the model to explore and plan in long-horizon settings. Remarkably, we find that after each compression-expansion cycle, the model's performance improves even as its output length decreases, steadily pushing it closer to the Pareto frontier in the performance-efficiency trade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves performance on AIME24 by 43.2% while reducing token usage by 46.9% after three iterations, and SIRI-high achieves the highest accuracy compared to all other methods (Figure 1). Our findings shed light on the potential of periodically oscillating the LRM's output truncation length during training to dynamically balance exploration and efficiency in reasoning, converging towards an optimal \"sweet spot\" between the two. Our models are publicly available.",
            "score": 9,
            "issue_id": 6153,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "4aad706cb9d9d40b",
            "authors": [
                "Haoming Wen",
                "Yushi Bai",
                "Juanzi Li",
                "Jie Tang"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25176.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ SIRI Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ reinforcement learning. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ´ÑƒĞµÑ‚ Ñ„Ğ°Ğ·Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒÑ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ’ Ñ„Ğ°Ğ·Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, ÑƒĞ±Ğ¸Ñ€Ğ°Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ° Ğ² Ñ„Ğ°Ğ·Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 43.2% Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 46.9%."
                },
                "en": {
                    "title": "SIRI: Balancing Efficiency and Accuracy in Large Reasoning Models",
                    "desc": "The paper presents SIRI, a novel reinforcement learning method designed to enhance the efficiency and accuracy of large reasoning models (LRMs). It addresses the issue of repetitive thinking patterns by implementing a training strategy that alternates between compressing and expanding the reasoning budget. During the compression phase, the model focuses on making precise decisions with a limited context, which reduces redundancy and increases reasoning density. The expansion phase allows for longer planning horizons, leading to improved performance while decreasing output length, ultimately achieving a better balance between exploration and efficiency in reasoning."
                },
                "zh": {
                    "title": "åŠ¨æ€å¹³è¡¡æ¨ç†æ•ˆç‡ä¸å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSIRIçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå®ƒé€šè¿‡äº¤æ›¿å‹ç¼©å’Œæ‰©å±•æ¨ç†é¢„ç®—ï¼Œæé«˜äº†å¤§å‹æ¨ç†æ¨¡å‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚SIRIé€šè¿‡åŠ¨æ€è°ƒæ•´è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€å¤§å±•å¼€é•¿åº¦ï¼Œå…‹æœäº†ç°æœ‰ç ”ç©¶ä¸­è§‚å¯Ÿåˆ°çš„é‡å¤æ€ç»´æ¨¡å¼æ‰€å¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚å‹ç¼©é˜¶æ®µç¼©çŸ­äº†å±•å¼€é•¿åº¦ï¼Œè¿«ä½¿æ¨¡å‹åœ¨æœ‰é™çš„ä¸Šä¸‹æ–‡ä¸­åšå‡ºç²¾ç¡®å†³ç­–ï¼Œä»è€Œå‡å°‘å†—ä½™æ ‡è®°å¹¶æé«˜æ¨ç†å¯†åº¦ã€‚æ‰©å±•é˜¶æ®µåˆ™æ”¾å®½äº†é•¿åº¦é™åˆ¶ï¼Œä¸ºæ¨¡å‹åœ¨é•¿æ—¶é—´èŒƒå›´å†…æ¢ç´¢å’Œè§„åˆ’æä¾›äº†ç©ºé—´ï¼Œæœ€ç»ˆå®ç°äº†æ€§èƒ½å’Œæ•ˆç‡çš„æœ€ä½³å¹³è¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25084",
            "title": "Scaling Generalist Data-Analytic Agents",
            "url": "https://huggingface.co/papers/2509.25084",
            "abstract": "DataMind addresses challenges in building open-source data-analytic agents through task taxonomy, trajectory sampling, dynamic training objectives, and stable multi-turn rollouts, achieving state-of-the-art performance on data analysis benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Data-analytic agents are emerging as a key catalyst for automated scientific discovery and for the vision of Innovating AI. Current approaches, however, rely heavily on prompt engineering over proprietary models, while open-source models struggle to face diverse-format, large-scale data files and long-horizon, multi-step reasoning that real-world analytics demands. This paper introduces DataMind, a scalable data synthesis and agent training recipe designed to build generalist data-analytic agents. DataMind tackles three key challenges in building open-source data-analytic agents, including insufficient data resources, improper training strategy, and unstable code-based multi-turn rollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a recursive easy-to-hard task composition mechanism to increase the diversity and difficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling strategy followed by model-based and rule-based filtering; 3) a dynamically adjustable training objective combining both SFT and RL losses; 4) a memory-frugal and stable code-based multi-turn rollout framework. Built on DataMind, we curate DataMind-12K, a high-quality trajectory set spanning diverse domains, task categories, and data file formats for data-analytic tasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with an average score of 71.16% on multiple data analysis benchmarks, outperforming the strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B also performs best among all open-source models with a score of 68.10%. We also incorporate some empirical insights gained from our exploratory trials into the analysis experiments, aiming to provide actionable insights about agentic training for the community. We will release DataMind-12K and DataMind-7B,14B for the community's future research.",
            "score": 8,
            "issue_id": 6157,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "500d1fd3b9d6791c",
            "authors": [
                "Shuofei Qiao",
                "Yanqiu Zhao",
                "Zhisong Qiu",
                "Xiaobin Wang",
                "Jintian Zhang",
                "Zhao Bin",
                "Ningyu Zhang",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Huajun Chen"
            ],
            "affiliations": [
                "Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25084.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#data",
                    "#training",
                    "#science",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Open-source Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "DataMind Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ open-source Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ†ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğµ supervised fine-tuning Ğ¸ reinforcement learning. ĞœĞ¾Ğ´ĞµĞ»ÑŒ DataMind-14B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ 71.16% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-5."
                },
                "en": {
                    "title": "Empowering Open-Source Data-Analytic Agents with DataMind",
                    "desc": "DataMind is a framework designed to enhance the capabilities of open-source data-analytic agents by addressing key challenges in their development. It introduces a structured approach to task taxonomy and dynamic training objectives, allowing agents to handle diverse data formats and complex reasoning tasks more effectively. The framework utilizes advanced techniques like knowledge-augmented trajectory sampling and stable multi-turn rollouts to improve training efficiency and performance. As a result, DataMind achieves state-of-the-art results on data analysis benchmarks, outperforming existing proprietary models and providing valuable resources for future research."
                },
                "zh": {
                    "title": "DataMindï¼šå¼€æºæ•°æ®åˆ†æçš„æœªæ¥",
                    "desc": "DataMind æ˜¯ä¸€ä¸ªæ—¨åœ¨æ„å»ºå¼€æºæ•°æ®åˆ†æä»£ç†çš„æ¡†æ¶ï¼Œè§£å†³äº†æ•°æ®èµ„æºä¸è¶³ã€è®­ç»ƒç­–ç•¥ä¸å½“å’Œå¤šè½®å›åˆä¸ç¨³å®šç­‰å…³é”®æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡ç»†åŒ–çš„ä»»åŠ¡åˆ†ç±»å’Œé€’å½’çš„ç®€å•åˆ°å›°éš¾çš„ä»»åŠ¡ç»„åˆæœºåˆ¶ï¼Œå¢åŠ äº†åˆæˆæŸ¥è¯¢çš„å¤šæ ·æ€§å’Œéš¾åº¦ã€‚DataMind è¿˜é‡‡ç”¨äº†çŸ¥è¯†å¢å¼ºçš„è½¨è¿¹é‡‡æ ·ç­–ç•¥å’ŒåŠ¨æ€å¯è°ƒçš„è®­ç»ƒç›®æ ‡ï¼Œç»“åˆäº†ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„æŸå¤±ã€‚ç»è¿‡è®­ç»ƒçš„ DataMind-14B åœ¨å¤šä¸ªæ•°æ®åˆ†æåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºäº†å…¶åœ¨è‡ªåŠ¨åŒ–ç§‘å­¦å‘ç°ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23951",
            "title": "HunyuanImage 3.0 Technical Report",
            "url": "https://huggingface.co/papers/2509.23951",
            "abstract": "HunyuanImage 3.0, a multimodal model with an autoregressive framework, achieves state-of-the-art performance in image generation and text-image alignment using a Mixture-of-Experts architecture with over 80 billion parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t We present HunyuanImage 3.0, a native multimodal model that unifies multimodal understanding and generation within an autoregressive framework, with its image generation module publicly available. The achievement of HunyuanImage 3.0 relies on several key components, including meticulous data curation, advanced architecture design, a native Chain-of-Thoughts schema, progressive model pre-training, aggressive model post-training, and an efficient infrastructure that enables large-scale training and inference. With these advancements, we successfully trained a Mixture-of-Experts (MoE) model comprising over 80 billion parameters in total, with 13 billion parameters activated per token during inference, making it the largest and most powerful open-source image generative model to date. We conducted extensive experiments and the results of automatic and human evaluation of text-image alignment and visual quality demonstrate that HunyuanImage 3.0 rivals previous state-of-the-art models. By releasing the code and weights of HunyuanImage 3.0, we aim to enable the community to explore new ideas with a state-of-the-art foundation model, fostering a dynamic and vibrant multimodal ecosystem. All open source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanImage-3.0",
            "score": 8,
            "issue_id": 6154,
            "pub_date": "2025-09-28",
            "pub_date_card": {
                "ru": "28 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 28",
                "zh": "9æœˆ28æ—¥"
            },
            "hash": "ad3b7b8b923097f2",
            "authors": [
                "Siyu Cao",
                "Hangting Chen",
                "Peng Chen",
                "Yiji Cheng",
                "Yutao Cui",
                "Xinchi Deng",
                "Ying Dong",
                "Kipper Gong",
                "Tianpeng Gu",
                "Xiusen Gu",
                "Tiankai Hang",
                "Duojun Huang",
                "Jie Jiang",
                "Zhengkai Jiang",
                "Weijie Kong",
                "Changlin Li",
                "Donghao Li",
                "Junzhe Li",
                "Xin Li",
                "Yang Li",
                "Zhenxi Li",
                "Zhimin Li",
                "Jiaxin Lin",
                "Linus",
                "Lucaz Liu",
                "Shu Liu",
                "Songtao Liu",
                "Yu Liu",
                "Yuhong Liu",
                "Yanxin Long",
                "Fanbin Lu",
                "Qinglin Lu",
                "Yuyang Peng",
                "Yuanbo Peng",
                "Xiangwei Shen",
                "Yixuan Shi",
                "Jiale Tao",
                "Yangyu Tao",
                "Qi Tian",
                "Pengfei Wan",
                "Chunyu Wang",
                "Kai Wang",
                "Lei Wang",
                "Linqing Wang",
                "Lucas Wang",
                "Qixun Wang",
                "Weiyan Wang",
                "Hao Wen",
                "Bing Wu",
                "Jianbing Wu",
                "Yue Wu",
                "Senhao Xie",
                "Fang Yang",
                "Miles Yang",
                "Xiaofeng Yang",
                "Xuan Yang",
                "Zhantao Yang",
                "Jingmiao Yu",
                "Zheng Yuan",
                "Chao Zhang",
                "Jian-Wei Zhang",
                "Peizhen Zhang",
                "Shi-Xue Zhang",
                "Tao Zhang",
                "Weigang Zhang",
                "Yepeng Zhang",
                "Yingfang Zhang",
                "Zihao Zhang",
                "Zijian Zhang",
                "Penghao Zhao",
                "Zhiyuan Zhao",
                "Xuefei Zhe",
                "Jianchen Zhu",
                "Zhao Zhong"
            ],
            "affiliations": [
                "Tencent Hunyuan Foundation Model Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23951.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#data",
                    "#open_source",
                    "#diffusion",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ³Ğ°Ğ½Ñ‚ÑĞºĞ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ 80 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²",
                    "desc": "HunyuanImage 3.0 - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mixture-of-Experts Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 80 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… 13 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ĞºÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ¼Ñƒ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ñƒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ğµ Chain-of-Thoughts. Ğ­Ñ‚Ğ¾ ÑĞ°Ğ¼Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ğ°Ñ open-source Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞµĞ³Ğ¾Ğ´Ğ½ÑÑˆĞ½Ğ¸Ğ¹ Ğ´ĞµĞ½ÑŒ, ĞºĞ¾Ğ´ Ğ¸ Ğ²ĞµÑĞ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Unleashing the Power of Multimodal AI with HunyuanImage 3.0",
                    "desc": "HunyuanImage 3.0 is a cutting-edge multimodal model designed for image generation and text-image alignment, utilizing an autoregressive framework. It features a Mixture-of-Experts architecture with over 80 billion parameters, allowing for efficient processing and high-quality outputs. The model's success is attributed to careful data curation, innovative architecture, and a robust training process that includes both pre-training and post-training phases. By making this model open-source, the authors encourage further exploration and development within the multimodal AI community."
                },
                "zh": {
                    "title": "HunyuanImage 3.0ï¼šå¼€åˆ›å¤šæ¨¡æ€ç”Ÿæˆçš„æ–°çºªå…ƒ",
                    "desc": "HunyuanImage 3.0 æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹ï¼Œé‡‡ç”¨è‡ªå›å½’æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬-å›¾åƒå¯¹é½æ–¹é¢è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹ä½¿ç”¨äº†è¶…è¿‡800äº¿ä¸ªå‚æ•°çš„ä¸“å®¶æ··åˆæ¶æ„ï¼Œå…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡ç²¾å¿ƒçš„æ•°æ®æ•´ç†ã€å…ˆè¿›çš„æ¶æ„è®¾è®¡å’Œæœ‰æ•ˆçš„è®­ç»ƒåŸºç¡€è®¾æ–½ï¼ŒHunyuanImage 3.0 å®ç°äº†å¤§è§„æ¨¡çš„è®­ç»ƒå’Œæ¨ç†ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡å¼€æºä»£ç å’Œæ¨¡å‹æƒé‡ï¼Œä¿ƒè¿›ç¤¾åŒºæ¢ç´¢æ–°çš„å¤šæ¨¡æ€åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23196",
            "title": "From Harm to Help: Turning Reasoning In-Context Demos into Assets for\n  Reasoning LMs",
            "url": "https://huggingface.co/papers/2509.23196",
            "abstract": "Insight-to-Solve (I2S) and its refined version (I2S+) improve few-shot chain-of-thought performance by converting demonstrations into reusable insights, outperforming direct answering and scaling methods across various models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reasoning LLMs (RLMs), especially those trained with verifier-based reinforcement learning, often perform worse with few-shot CoT than with direct answering. We revisit this paradox using high-quality reasoning traces from DeepSeek-R1 as demonstrations and find that adding more exemplars consistently degrades accuracy, even when demonstrations are optimal. A detailed analysis reveals two mechanisms behind this decline: (i) semantic misguidance, where high textual similarity leads the model to treat the target as the same as the exemplar and to copy intermediate steps verbatim; and (ii) strategy transfer failure, where the model struggles to extract useful reasoning strategies and apply them to target questions. Guided by these, we introduce Insight-to-Solve (I2S), a sequential test-time procedure that turns demonstrations into explicit, reusable insights and derives a target-specific reasoning trace; optionally, the reasoning is self-refined for coherence and correctness (I2S+). Extensive experiments on diverse benchmarks show that I2S and I2S+ consistently outperform both direct answering and test-time scaling baselines across open- and closed-source models. Even for GPT models, our method helps: on AIME'25, GPT-4.1 rises by +14.0%, and o1-mini improves by +2.7% on AIME and +1.7% on GPQA, indicating that in-context demonstrations can be harnessed effectively via insight-refine-solve framework.",
            "score": 8,
            "issue_id": 6153,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "05ad7f2b555ea8c1",
            "authors": [
                "Haonan Wang",
                "Weida Liang",
                "Zihang Fu",
                "Nie Zheng",
                "Yifan Zhang",
                "Yao Tong",
                "Tongyao Zhu",
                "Hao Jiang",
                "Chuang Li",
                "Jiaying Wu",
                "Kenji Kawaguchi"
            ],
            "affiliations": [
                "MiroMind AI",
                "National University of Singapore",
                "University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23196.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğº Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ğ°Ğ¼: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº few-shot Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ reasoning LLM Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ…ÑƒĞ´ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ few-shot chain-of-thought Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ÑĞ¼Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ´Ğ²Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¿Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°, Ğ¸ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Insight-to-Solve (I2S), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ²Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ñ‹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ I2S Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ğ½Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Transforming Demonstrations into Reusable Insights for Better Reasoning",
                    "desc": "The paper introduces Insight-to-Solve (I2S) and its enhanced version I2S+, which improve few-shot chain-of-thought (CoT) performance in reasoning language models (RLMs). It identifies issues with traditional few-shot learning, such as semantic misguidance and strategy transfer failure, which can lead to decreased accuracy when using multiple demonstrations. I2S transforms these demonstrations into reusable insights, allowing models to generate more accurate and coherent reasoning traces for specific tasks. Experimental results show that I2S and I2S+ significantly outperform direct answering methods and scaling techniques across various models and benchmarks."
                },
                "zh": {
                    "title": "æ´å¯Ÿè§£å†³ï¼šæå‡å°‘æ ·æœ¬æ¨ç†çš„æœ‰æ•ˆæ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†Insight-to-Solve (I2S) åŠå…¶æ”¹è¿›ç‰ˆI2S+ï¼Œæ—¨åœ¨æå‡å°‘æ ·æœ¬æ¨ç†é“¾çš„è¡¨ç°ã€‚é€šè¿‡å°†ç¤ºä¾‹è½¬åŒ–ä¸ºå¯é‡ç”¨çš„æ´å¯Ÿï¼ŒI2Så’ŒI2S+åœ¨å¤šç§æ¨¡å‹ä¸Šè¶…è¶Šäº†ç›´æ¥å›ç­”å’Œæ‰©å±•æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œå¢åŠ ç¤ºä¾‹æ•°é‡ä¼šå¯¼è‡´å‡†ç¡®æ€§ä¸‹é™ï¼Œä¸»è¦åŸå› åŒ…æ‹¬è¯­ä¹‰è¯¯å¯¼å’Œç­–ç•¥è½¬ç§»å¤±è´¥ã€‚é€šè¿‡å¼•å¯¼æ¨¡å‹æå–æœ‰æ•ˆçš„æ¨ç†ç­–ç•¥ï¼ŒI2Sèƒ½å¤Ÿç”Ÿæˆé’ˆå¯¹ç‰¹å®šé—®é¢˜çš„æ¨ç†è½¨è¿¹ï¼Œä»è€Œæé«˜æ¨ç†çš„è¿è´¯æ€§å’Œæ­£ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23924",
            "title": "Taming Masked Diffusion Language Models via Consistency Trajectory\n  Reinforcement Learning with Fewer Decoding Step",
            "url": "https://huggingface.co/papers/2509.23924",
            "abstract": "Proposed decoding strategies and reinforcement learning algorithms improve the performance and efficiency of masked diffusion language models during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive (AR) language models, offering properties such as parallel decoding, flexible generation orders, and the potential for fewer inference steps. Despite these advantages, decoding strategies and reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored. A naive approach is to directly transfer techniques well-established for AR models to MDLMs. However, this raises an immediate question: Is such a naive transfer truly optimal? For example, 1) Block-wise and semi-AR decoding strategies are not employed during the training of MDLMs, so why do they outperform full diffusion-style decoding during inference? 2) Applying RL algorithms designed for AR models directly to MDLMs exhibits a training-inference inconsistency, since MDLM decoding are non-causal (parallel). This results in inconsistencies between the rollout trajectory and the optimization trajectory. To address these challenges, we propose EOS Early Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which unlock the potential of MDLMs to perform full diffusion-style decoding, achieving competitive performance with fewer decoding steps. Additionally, we introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) for taming MDLMs, which emphasizes the consistency between rollout trajectory and optimization trajectory, and reduces the optimization errors caused by skip-step optimization. We conduct extensive experiments on reasoning tasks, such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The results demonstrate that the proposed EOSER and ASS mechanisms, together with CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs. Code: https://github.com/yjyddq/EOSER-ASS-RL.",
            "score": 7,
            "issue_id": 6157,
            "pub_date": "2025-09-28",
            "pub_date_card": {
                "ru": "28 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 28",
                "zh": "9æœˆ28æ—¥"
            },
            "hash": "e494b8cf6f813ac2",
            "authors": [
                "Jingyi Yang",
                "Guanxu Chen",
                "Xuhao Hu",
                "Jing Shao"
            ],
            "affiliations": [
                "Fudan University",
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23924.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rl",
                    "#diffusion",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#math",
                    "#inference"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ RL Ğ´Ğ»Ñ masked diffusion ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ masked diffusion language models (MDLM) ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° autoregressive Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ¿Ñ€ÑĞ¼Ñ‹Ğ¼ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¾Ğ¼ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ¾Ñ‚ AR Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº MDLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ inference. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ EOS Early Rejection (EOSER) Ğ¸ Ascending Step-Size (ASS), Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ reinforcement learning CJ-GRPO Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… reasoning Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ MDLM."
                },
                "en": {
                    "title": "Unlocking the Power of Masked Diffusion Language Models",
                    "desc": "This paper focuses on improving masked diffusion language models (MDLMs) by introducing new decoding strategies and reinforcement learning (RL) algorithms. The authors highlight that traditional methods used for autoregressive models are not optimal for MDLMs due to their unique non-causal decoding process. They propose two novel techniques, EOS Early Rejection (EOSER) and Ascending Step-Size (ASS), which enhance the efficiency of MDLMs during inference. Additionally, they introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) to align the training and inference processes, leading to better performance on reasoning tasks with fewer decoding steps."
                },
                "zh": {
                    "title": "æå‡æ©è”½æ‰©æ•£è¯­è¨€æ¨¡å‹çš„è§£ç æ•ˆç‡ä¸æ€§èƒ½",
                    "desc": "æœ¬æ–‡æå‡ºçš„è§£ç ç­–ç•¥å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•æ˜¾è‘—æå‡äº†æ©è”½æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼ˆMDLMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚MDLMsä½œä¸ºè‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆARï¼‰çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå…·å¤‡å¹¶è¡Œè§£ç å’Œçµæ´»ç”Ÿæˆé¡ºåºçš„ä¼˜ç‚¹ï¼Œä½†ç°æœ‰çš„è§£ç ç­–ç•¥å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•å°šæœªå……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºçš„EOSæ—©æœŸæ‹’ç»ï¼ˆEOSERï¼‰å’Œä¸Šå‡æ­¥é•¿ï¼ˆASSï¼‰è§£ç è°ƒåº¦å™¨ï¼Œèƒ½å¤Ÿå……åˆ†å‘æŒ¥MDLMsçš„æ½œåŠ›ï¼Œå®ç°æ›´å°‘çš„è§£ç æ­¥éª¤è€Œä¿æŒç«äº‰åŠ›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒCJ-GRPOç®—æ³•å¼ºè°ƒäº†å›æ»šè½¨è¿¹ä¸ä¼˜åŒ–è½¨è¿¹ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œå‡å°‘äº†ç”±äºè·³æ­¥ä¼˜åŒ–é€ æˆçš„ä¼˜åŒ–è¯¯å·®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23866",
            "title": "Efficient Multi-turn RL for GUI Agents via Decoupled Training and\n  Adaptive Data Curation",
            "url": "https://huggingface.co/papers/2509.23866",
            "abstract": "DART, a decoupled reinforcement learning framework for GUI agents, improves efficiency and learning effectiveness through asynchronous modules and adaptive data curation, achieving high task success rates on the OSWorld benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language model (VLM) based GUI agents show promise for automating complex desktop and mobile tasks, but face significant challenges in applying reinforcement learning (RL): (1) slow multi-turn interactions with GUI environments for policy rollout, and (2) insufficient high-quality agent-environment interactions for policy learning. To address these challenges, we propose DART, a Decoupled Agentic RL Training framework for GUI agents, which coordinates heterogeneous modules in a highly decoupled manner. DART separates the training system into four asynchronous modules: environment cluster, rollout service, data manager, and trainer. This design enables non-blocking communication, asynchronous training, rollout-wise trajectory sampling, and per-worker model synchronization, significantly improving the system efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput, and 5.5* environment utilization. To facilitate effective learning from abundant samples, we introduce an adaptive data curation scheme: (1) pre-collecting successful trajectories for challenging tasks to supplement sparse success in online sampling; (2) dynamically adjusting rollout numbers and trajectory lengths based on task difficulty; (3) training selectively on high-entropy steps to prioritize critical decisions; (4) stabilizing learning via truncated importance sampling for policy mismatch between policy rollout and updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task success rate, a 14.61% absolute gain over the base model, and 7.34% higher than open-source SOTA. We will fully open-source our training framework, data, and model checkpoints via computer-use-agents.github.io/dart-gui, which we believe is a timely contribution to the open-source community of agentic RL training.",
            "score": 7,
            "issue_id": 6154,
            "pub_date": "2025-09-28",
            "pub_date_card": {
                "ru": "28 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 28",
                "zh": "9æœˆ28æ—¥"
            },
            "hash": "5427fac94e859a8b",
            "authors": [
                "Pengxiang Li",
                "Zechen Hu",
                "Zirui Shang",
                "Jingrong Wu",
                "Yang Liu",
                "Hui Liu",
                "Zhi Gao",
                "Chenrui Shi",
                "Bofei Zhang",
                "Zihao Zhang",
                "Xiaochuan Shi",
                "Zedong YU",
                "Yuwei Wu",
                "Xinxiao Wu",
                "Yunde Jia",
                "Liuyu Xiang",
                "Zhaofeng He",
                "Qing Li"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "Beijing University of Posts and Telecommunications",
                "DataCanvas",
                "Shenzhen MSU-BIT University",
                "State Key Laboratory of General Artificial Intelligence, BIGAI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23866.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#optimization",
                    "#benchmark",
                    "#rl",
                    "#open_source",
                    "#training",
                    "#games",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ GUI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ",
                    "desc": "DART Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ´ĞµÑ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ GUI Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑˆĞ°Ğ³Ğ°Ñ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ GPU Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OSWorld Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DART-GUI-7B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 42.13% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° 14.61%."
                },
                "en": {
                    "title": "DART: Decoupling Reinforcement Learning for Efficient GUI Agents",
                    "desc": "DART is a new framework designed to improve reinforcement learning for GUI agents by using a decoupled approach. It organizes the training process into four asynchronous modules, allowing for more efficient communication and training without delays. The framework also includes an adaptive data curation method that enhances learning by focusing on successful interactions and adjusting to task difficulty. As a result, DART significantly boosts task success rates and overall system performance compared to previous models."
                },
                "zh": {
                    "title": "DARTï¼šæå‡GUIä»£ç†å­¦ä¹ æ•ˆç‡çš„è§£è€¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶",
                    "desc": "DARTæ˜¯ä¸€ç§ä¸ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†è®¾è®¡çš„è§£è€¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚å®ƒé€šè¿‡å¼‚æ­¥æ¨¡å—å’Œè‡ªé€‚åº”æ•°æ®ç®¡ç†ï¼Œæé«˜äº†å­¦ä¹ æ•ˆç‡å’Œæ•ˆæœã€‚DARTå°†è®­ç»ƒç³»ç»Ÿåˆ†ä¸ºå››ä¸ªå¼‚æ­¥æ¨¡å—ï¼Œå…è®¸éé˜»å¡é€šä¿¡å’Œé«˜æ•ˆçš„æ¨¡å‹åŒæ­¥ï¼Œä»è€Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿçš„åˆ©ç”¨ç‡ã€‚é€šè¿‡å¼•å…¥è‡ªé€‚åº”æ•°æ®ç­–åˆ’æ–¹æ¡ˆï¼ŒDARTåœ¨OSWorldåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†42.13%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œè¶…è¶Šäº†åŸºç¡€æ¨¡å‹å’Œå¼€æºæœ€ä¼˜æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24335",
            "title": "Hyperspherical Latents Improve Continuous-Token Autoregressive\n  Generation",
            "url": "https://huggingface.co/papers/2509.24335",
            "abstract": "SphereAR, an autoregressive model with hyperspherical constraints, achieves state-of-the-art performance in image generation, surpassing diffusion and masked-generation models at similar parameter scales.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) models are promising for image generation, yet continuous-token AR variants often trail latent diffusion and masked-generation models. The core issue is heterogeneous variance in VAE latents, which is amplified during AR decoding, especially under classifier-free guidance (CFG), and can cause variance collapse. We propose SphereAR to address this issue. Its core design is to constrain all AR inputs and outputs -- including after CFG -- to lie on a fixed-radius hypersphere (constant ell_2 norm), leveraging hyperspherical VAEs. Our theoretical analysis shows that hyperspherical constraint removes the scale component (the primary cause of variance collapse), thereby stabilizing AR decoding. Empirically, on ImageNet generation, SphereAR-H (943M) sets a new state of the art for AR models, achieving FID 1.34. Even at smaller scales, SphereAR-L (479M) reaches FID 1.54 and SphereAR-B (208M) reaches 1.92, matching or surpassing much larger baselines such as MAR-H (943M, 1.55) and VAR-d30 (2B, 1.92). To our knowledge, this is the first time a pure next-token AR image generator with raster order surpasses diffusion and masked-generation models at comparable parameter scales.",
            "score": 6,
            "issue_id": 6153,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "4f3275712f51af77",
            "authors": [
                "Guolin Ke",
                "Hui Xue"
            ],
            "affiliations": [
                "DP Technology",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24335.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#cv",
                    "#architecture",
                    "#diffusion"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ¿ĞµÑ€ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° SphereAR - Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ÑĞµ Ğ²Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ³Ğ¸Ğ¿ĞµÑ€ÑÑ„ĞµÑ€Ğµ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ´Ğ¸ÑƒÑĞ°. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… AR Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… VAE, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑÑƒ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸. Ğ“Ğ¸Ğ¿ĞµÑ€ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸, Ñ‚ĞµĞ¼ ÑĞ°Ğ¼Ñ‹Ğ¼ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. SphereAR Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ° ÑÑ€ĞµĞ´Ğ¸ AR Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ImageNet Ñ FID 1.34, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "SphereAR: Revolutionizing Image Generation with Hyperspherical Constraints",
                    "desc": "SphereAR is an autoregressive model designed for image generation that incorporates hyperspherical constraints to improve performance. By constraining inputs and outputs to a fixed-radius hypersphere, it effectively addresses the issue of variance collapse that often occurs in traditional autoregressive models. This innovative approach allows SphereAR to achieve state-of-the-art results on image generation tasks, outperforming both diffusion and masked-generation models at similar parameter sizes. The model demonstrates significant improvements in FID scores, showcasing its effectiveness in generating high-quality images."
                },
                "zh": {
                    "title": "SphereARï¼šè¶…çƒé¢çº¦æŸä¸‹çš„è‡ªå›å½’å›¾åƒç”Ÿæˆæ–°çªç ´",
                    "desc": "SphereARæ˜¯ä¸€ç§è‡ªå›å½’æ¨¡å‹ï¼Œé‡‡ç”¨è¶…çƒé¢çº¦æŸï¼Œèƒ½å¤Ÿåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†åŒç­‰å‚æ•°è§„æ¨¡çš„æ‰©æ•£æ¨¡å‹å’Œæ©è”½ç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹è§£å†³äº†åœ¨è‡ªå›å½’è§£ç è¿‡ç¨‹ä¸­ï¼Œç”±äºå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æ½œå˜é‡çš„å¼‚è´¨æ–¹å·®å¯¼è‡´çš„æ–¹å·®å´©æºƒé—®é¢˜ã€‚SphereARé€šè¿‡å°†æ‰€æœ‰è‡ªå›å½’è¾“å…¥å’Œè¾“å‡ºé™åˆ¶åœ¨å›ºå®šåŠå¾„çš„è¶…çƒé¢ä¸Šï¼Œç¨³å®šäº†è‡ªå›å½’è§£ç è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSphereARåœ¨ImageNetç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ–°çš„æœ€ä½³ç»“æœï¼Œå±•ç¤ºäº†å…¶åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„å¼ºå¤§èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24193",
            "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced\n  Self-Play",
            "url": "https://huggingface.co/papers/2509.24193",
            "abstract": "AceSearcher, a cooperative self-play framework, enhances a large language model's reasoning ability by alternating between decomposing queries and solving them, outperforming state-of-the-art models with fewer parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9x more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks. Our code will be published at https://github.com/ritaranx/AceSearcher and https://huggingface.co/AceSearcher.",
            "score": 6,
            "issue_id": 6153,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "f817dc5569e19eb9",
            "authors": [
                "Ran Xu",
                "Yuchen Zhuang",
                "Zihan Dong",
                "Jonathan Wang",
                "Yue Yu",
                "Joyce C. Ho",
                "Linjun Zhang",
                "Haoyu Wang",
                "Wenqi Shi",
                "Carl Yang"
            ],
            "affiliations": [
                "Emory University",
                "Georgia Institute of Technology",
                "Rutgers University",
                "SUNY Albany",
                "UT Southwestern Medical Center"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24193.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#optimization",
                    "#small_models"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞšĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ°Ğ¼Ğ¾Ğ¸Ğ³Ñ€Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° AceSearcher â€” framework Ğ´Ğ»Ñ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾Ğ¸Ğ³Ñ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ´Ğ²Ğµ Ñ€Ğ¾Ğ»Ğ¸: Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸, Ğ° Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ supervised fine-tuning Ğ½Ğ° ÑĞ¼ĞµÑĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ reinforcement learning Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². AceSearcher Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 7.6% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² 32B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑĞ¾Ğ¿ĞµÑ€Ğ½Ğ¸Ñ‡Ğ°ĞµÑ‚ Ñ DeepSeek-V3, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ 5% ĞµĞ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "AceSearcher: Efficient Reasoning with Less Complexity",
                    "desc": "AceSearcher is a novel framework that improves the reasoning capabilities of large language models (LLMs) by using a cooperative self-play approach. It alternates between two roles: a decomposer that simplifies complex queries and a solver that uses retrieved information to generate answers. This method combines supervised fine-tuning with reinforcement learning to enhance accuracy without needing extra annotations. The results show that AceSearcher outperforms existing models, achieving better performance with significantly fewer parameters, demonstrating its efficiency in handling complex reasoning tasks."
                },
                "zh": {
                    "title": "AceSearcherï¼šé«˜æ•ˆæ¨ç†çš„æ–°æ–¹æ³•",
                    "desc": "AceSearcheræ˜¯ä¸€ç§åˆä½œè‡ªæˆ‘å¯¹å¼ˆæ¡†æ¶ï¼Œé€šè¿‡äº¤æ›¿åˆ†è§£æŸ¥è¯¢å’Œè§£å†³é—®é¢˜ï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è®­ç»ƒä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åˆ†è§£è€…å’Œæ±‚è§£è€…ä¹‹é—´åˆ‡æ¢ï¼Œåˆ†è§£è€…è´Ÿè´£å°†å¤æ‚æŸ¥è¯¢æ‹†è§£ï¼Œè€Œæ±‚è§£è€…åˆ™æ•´åˆæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆã€‚AceSearcherç»“åˆäº†å¤šç§æœç´¢ã€æ¨ç†å’Œåˆ†è§£ä»»åŠ¡çš„ç›‘ç£å¾®è°ƒä¸é’ˆå¯¹æœ€ç»ˆç­”æ¡ˆå‡†ç¡®æ€§çš„å¼ºåŒ–å¾®è°ƒï¼Œæ¶ˆé™¤äº†å¯¹ä¸­é—´æ³¨é‡Šçš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAceSearcheråœ¨ä¸‰ä¸ªæ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„åŸºçº¿ï¼Œå±•ç°å‡ºå“è¶Šçš„æ•ˆç‡å’Œæ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24786",
            "title": "LOVE-R1: Advancing Long Video Understanding with an Adaptive Zoom-in\n  Mechanism via Multi-Step Reasoning",
            "url": "https://huggingface.co/papers/2509.24786",
            "abstract": "LOVE-R1, a model with adaptive frame sampling, enhances long video understanding by balancing temporal and spatial details through multi-step reasoning and decoupled reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Long video understanding is still challenging for recent Large Video-Language Models (LVLMs) due to the conflict between long-form temporal understanding and detailed spatial perception. LVLMs with a uniform frame sampling mechanism, which samples frames with an equal frame size and fixed sampling rate, inevitably sacrifice either temporal clues or spatial details, resulting in suboptimal solutions. To mitigate this dilemma, we propose LOVE-R1, a model that can adaptively zoom in on a video clip. The model is first provided with densely sampled frames but in a small resolution. If some spatial details are needed, the model can zoom in on a clip of interest with a large frame resolution based on its reasoning until key visual information is obtained. The whole process is implemented as a multi-step reasoning process. To train the reasoning ability, we first finetune the model on our collected 38k high-quality CoT data and enhance it with decoupled reinforcement finetuning. As outcome rewards can not provide fine-grained process supervision, we decouple multi-step reasoning into multiple single-step reasoning and optimize the internal zoom-in ability explicitly. Experiments on long video understanding benchmarks show that our model with the slow-fast adaptive frame sampling mechanism achieves a great trade-off between sampling density and frame resolutions, and LOVE-R1 outperforms our baseline Qwen2.5-VL by an average of 3.1% points across 4 common long video understanding benchmarks.",
            "score": 5,
            "issue_id": 6153,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "60657ccb584048d7",
            "authors": [
                "Shenghao Fu",
                "Qize Yang",
                "Yuan-Ming Li",
                "Xihan Wei",
                "Xiaohua Xie",
                "Wei-Shi Zheng"
            ],
            "affiliations": [
                "Guangdong Province Key Laboratory of Information Security Technology, China",
                "Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China",
                "Pazhou Laboratory (Huangpu), China",
                "Peng Cheng Laboratory, China",
                "School of Computer Science and Engineering, Sun Yat-sen University, China",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24786.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#rl",
                    "#training",
                    "#reasoning",
                    "#long_context",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ LOVE-R1 - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ ĞºĞ°Ğ´Ñ€Ğ¾Ğ²: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑƒÑ‡Ğ°ÑÑ‚ĞºĞ°Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ Ğ½Ğ° 38k Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Chain-of-Thought Ğ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Qwen2.5-VL Ğ½Ğ° 3.1% Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµĞ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "LOVE-R1: Adaptive Sampling for Enhanced Long Video Understanding",
                    "desc": "The paper introduces LOVE-R1, a novel model designed to improve long video understanding by using adaptive frame sampling. This model addresses the challenge faced by Large Video-Language Models (LVLMs) in balancing temporal and spatial information. LOVE-R1 employs a multi-step reasoning process that allows it to dynamically adjust frame resolution based on the importance of spatial details. Through decoupled reinforcement learning and extensive training on high-quality data, LOVE-R1 demonstrates superior performance on long video understanding tasks compared to existing models."
                },
                "zh": {
                    "title": "LOVE-R1ï¼šè‡ªé€‚åº”å¸§é‡‡æ ·æå‡é•¿è§†é¢‘ç†è§£",
                    "desc": "LOVE-R1æ˜¯ä¸€ç§å…·æœ‰è‡ªé€‚åº”å¸§é‡‡æ ·çš„æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜é•¿è§†é¢‘ç†è§£èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤šæ­¥æ¨ç†å’Œå¹³è¡¡æ—¶é—´å’Œç©ºé—´ç»†èŠ‚ï¼Œè§£å†³äº†é•¿è§†é¢‘è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ—¶é—´åºåˆ—æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚LOVE-R1é¦–å…ˆä»¥ä½åˆ†è¾¨ç‡å¯†é›†é‡‡æ ·å¸§ï¼Œç„¶åæ ¹æ®éœ€è¦å¯¹æ„Ÿå…´è¶£çš„ç‰‡æ®µè¿›è¡Œæ”¾å¤§ï¼Œä»¥è·å–å…³é”®è§†è§‰ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLOVE-R1åœ¨é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºé‡‡æ ·å¯†åº¦å’Œå¸§åˆ†è¾¨ç‡ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.21953",
            "title": "MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially\n  Disentangled Attention and Identity-Aware Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.21953",
            "abstract": "MultiCrafter framework improves multi-subject image generation by addressing attribute leakage through explicit positional supervision, utilizing a Mixture-of-Experts architecture, and aligning with human preferences via online reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-subject image generation aims to synthesize user-provided subjects in a single image while preserving subject fidelity, ensuring prompt consistency, and aligning with human aesthetic preferences. However, existing methods, particularly those built on the In-Context-Learning paradigm, are limited by their reliance on simple reconstruction-based objectives, leading to both severe attribute leakage that compromises subject fidelity and failing to align with nuanced human preferences. To address this, we propose MultiCrafter, a framework that ensures high-fidelity, preference-aligned generation. First, we find that the root cause of attribute leakage is a significant entanglement of attention between different subjects during the generation process. Therefore, we introduce explicit positional supervision to explicitly separate attention regions for each subject, effectively mitigating attribute leakage. To enable the model to accurately plan the attention region of different subjects in diverse scenarios, we employ a Mixture-of-Experts architecture to enhance the model's capacity, allowing different experts to focus on different scenarios. Finally, we design a novel online reinforcement learning framework to align the model with human preferences, featuring a scoring mechanism to accurately assess multi-subject fidelity and a more stable training strategy tailored for the MoE architecture. Experiments validate that our framework significantly improves subject fidelity while aligning with human preferences better.",
            "score": 5,
            "issue_id": 6152,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "0c3656cedf425566",
            "authors": [
                "Tao Wu",
                "Yibo Jiang",
                "Yehao Lu",
                "Zhizhong Wang",
                "Zeyi Huang",
                "Zequn Qin",
                "Xi Li"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Zhejiang University",
                "Huawei Technologies Ltd",
                "School of Software Technology, Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.21953.jpg",
            "data": {
                "categories": [
                    "#leakage",
                    "#alignment",
                    "#multimodal",
                    "#training",
                    "#rl",
                    "#architecture",
                    "#synthetic"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MultiCrafter - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (attention) Ğ·Ğ°Ğ¿ÑƒÑ‚Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mixture-of-Experts Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ², Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ»ÑĞ´ĞµĞ¹."
                },
                "en": {
                    "title": "MultiCrafter: Enhancing Multi-Subject Image Generation with Precision and Preference",
                    "desc": "The MultiCrafter framework enhances the generation of images containing multiple subjects by tackling the issue of attribute leakage through the use of explicit positional supervision. This approach helps to clearly define attention regions for each subject, preventing confusion during the image creation process. Additionally, the framework employs a Mixture-of-Experts architecture, which allows different model components to specialize in various scenarios, improving overall performance. Finally, an online reinforcement learning strategy is implemented to ensure that the generated images align closely with human aesthetic preferences, resulting in higher fidelity and satisfaction."
                },
                "zh": {
                    "title": "MultiCrafterï¼šæå‡å¤šä¸»ä½“å›¾åƒç”Ÿæˆçš„ä¿çœŸåº¦ä¸äººç±»åå¥½å¯¹é½",
                    "desc": "MultiCrafteræ¡†æ¶é€šè¿‡æ˜¾å¼çš„ä½ç½®ä¿¡æ¯ç›‘ç£æ¥è§£å†³å¤šä¸»ä½“å›¾åƒç”Ÿæˆä¸­çš„å±æ€§æ³„æ¼é—®é¢˜ï¼Œä»è€Œæé«˜ç”Ÿæˆçš„å›¾åƒè´¨é‡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ··åˆä¸“å®¶æ¶æ„ï¼Œä½¿ä¸åŒçš„ä¸“å®¶èƒ½å¤Ÿä¸“æ³¨äºä¸åŒçš„åœºæ™¯ï¼Œå¢å¼ºæ¨¡å‹çš„èƒ½åŠ›ã€‚ä¸ºäº†æ›´å¥½åœ°ç¬¦åˆäººç±»çš„å®¡ç¾åå¥½ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æœºåˆ¶ï¼Œèƒ½å¤Ÿå‡†ç¡®è¯„ä¼°å¤šä¸»ä½“çš„ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMultiCrafteræ˜¾è‘—æé«˜äº†ç”Ÿæˆå›¾åƒçš„ä¸»ä½“ä¿çœŸåº¦ï¼Œå¹¶æ›´å¥½åœ°ä¸äººç±»åå¥½å¯¹é½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25185",
            "title": "PixelCraft: A Multi-Agent System for High-Fidelity Visual Reasoning on\n  Structured Images",
            "url": "https://huggingface.co/papers/2509.25185",
            "abstract": "PixelCraft, a multi-agent system, enhances visual reasoning in multimodal large language models by integrating high-fidelity image processing and flexible reasoning through a dynamic workflow and image memory.  \t\t\t\t\tAI-generated summary \t\t\t\t Structured images (e.g., charts and geometric diagrams) remain challenging for multimodal large language models (MLLMs), as perceptual slips can cascade into erroneous conclusions. Intermediate visual cues can steer reasoning; however, existing cue-based methods are constrained with low-fidelity image processing and linear, rigid reasoning patterns, limiting their effectiveness on complex structured-image tasks. In this paper, we propose PixelCraft, a novel multi-agent system for high-fidelity image processing and flexible visual reasoning on structured images. The system comprises a dispatcher, a planner, a reasoner, critics, and a set of visual tool agents. To achieve high-fidelity processing, we construct a high-quality corpus and fine-tune an MLLM into a grounding model, whose pixel-level localizations are integrated with traditional computer vision (CV) algorithms in tool agents. Building on this foundation, PixelCraft facilitates flexible visual reasoning through a dynamic three-stage workflow of tool selection, agent discussion, and self-criticism. Moreover, unlike prior linear reasoning patterns that simply append historical images, PixelCraft maintains an image memory to allow the planner to adaptively revisit earlier visual steps, explore alternative reasoning branches, and dynamically adjust the reasoning trajectory during discussion. Extensive experiments on challenging chart and geometry benchmarks demonstrate that PixelCraft significantly improves visual reasoning performance for advanced MLLMs, setting a new standard for structured image reasoning. Our code will be available at https://github.com/microsoft/PixelCraft.",
            "score": 4,
            "issue_id": 6154,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "9cba7a1f097e46cc",
            "authors": [
                "Shuoshuo Zhang",
                "Zijian Li",
                "Yizhen Zhang",
                "Jingjing Fu",
                "Lei Song",
                "Jiang Bian",
                "Jun Zhang",
                "Yujiu Yang",
                "Rui Wang"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Microsoft Research",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25185.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#reasoning",
                    "#interpretability",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "PixelCraft â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ ÑĞ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¸ÑĞ¿ĞµÑ‚Ñ‡ĞµÑ€, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, PixelCraft Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "PixelCraft: Revolutionizing Visual Reasoning in MLLMs",
                    "desc": "PixelCraft is a multi-agent system designed to improve visual reasoning in multimodal large language models (MLLMs) by combining high-quality image processing with flexible reasoning capabilities. It addresses the challenges posed by structured images, such as charts and diagrams, which often lead to errors due to perceptual slips. The system features a dynamic workflow that includes tool selection, agent discussions, and self-criticism, allowing for adaptive reasoning. By integrating pixel-level localizations with traditional computer vision techniques and maintaining an image memory, PixelCraft enhances the reasoning process, leading to significant performance improvements on complex visual tasks."
                },
                "zh": {
                    "title": "PixelCraftï¼šæå‡è§†è§‰æ¨ç†çš„æ–°æ ‡å‡†",
                    "desc": "PixelCraft æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡åŠ¨æ€å·¥ä½œæµç¨‹å’Œå›¾åƒè®°å¿†æ¥å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†é«˜ä¿çœŸå›¾åƒå¤„ç†å’Œçµæ´»çš„æ¨ç†æœºåˆ¶ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†ç»“æ„åŒ–å›¾åƒæ—¶çš„å±€é™æ€§ã€‚é€šè¿‡æ„å»ºé«˜è´¨é‡è¯­æ–™åº“å¹¶å¾®è°ƒæ¨¡å‹ï¼ŒPixelCraft å®ç°äº†åƒç´ çº§çš„æœ¬åœ°åŒ–ï¼Œå¹¶ä¸ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰ç®—æ³•ç›¸ç»“åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPixelCraft åœ¨å¤æ‚çš„å›¾è¡¨å’Œå‡ ä½•ä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†è§†è§‰æ¨ç†æ€§èƒ½ï¼Œæ ‘ç«‹äº†ç»“æ„åŒ–å›¾åƒæ¨ç†çš„æ–°æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25149",
            "title": "Pretraining Large Language Models with NVFP4",
            "url": "https://huggingface.co/papers/2509.25149",
            "abstract": "A novel training approach using NVFP4 format with Random Hadamard transforms, two-dimensional quantization, stochastic rounding, and selective high-precision layers enables stable and accurate training of large language models in 4-bit precision.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) today are powerful problem solvers across many domains, and they continue to get stronger as they scale in model size, training set size, and training set quality, as shown by extensive research and experimentation across the industry. Training a frontier model today requires on the order of tens to hundreds of yottaflops, which is a massive investment of time, compute, and energy. Improving pretraining efficiency is therefore essential to enable the next generation of even more capable LLMs. While 8-bit floating point (FP8) training is now widely adopted, transitioning to even narrower precision, such as 4-bit floating point (FP4), could unlock additional improvements in computational speed and resource utilization. However, quantization at this level poses challenges to training stability, convergence, and implementation, notably for large-scale models trained on long token horizons.   In this study, we introduce a novel approach for stable and accurate training of large language models (LLMs) using the NVFP4 format. Our method integrates Random Hadamard transforms (RHT) to bound block-level outliers, employs a two-dimensional quantization scheme for consistent representations across both the forward and backward passes, utilizes stochastic rounding for unbiased gradient estimation, and incorporates selective high-precision layers. We validate our approach by training a 12-billion-parameter model on 10 trillion tokens -- the longest publicly documented training run in 4-bit precision to date. Our results show that the model trained with our NVFP4-based pretraining technique achieves training loss and downstream task accuracies comparable to an FP8 baseline. These findings highlight that NVFP4, when combined with our training approach, represents a major step forward in narrow-precision LLM training algorithms.",
            "score": 4,
            "issue_id": 6154,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "b4b8ecd5db151816",
            "authors": [
                "NVIDIA",
                "Felix Abecassis",
                "Anjulie Agrusa",
                "Dong Ahn",
                "Jonah Alben",
                "Stefania Alborghetti",
                "Michael Andersch",
                "Sivakumar Arayandi",
                "Alexis Bjorlin",
                "Aaron Blakeman",
                "Evan Briones",
                "Ian Buck",
                "Bryan Catanzaro",
                "Jinhang Choi",
                "Mike Chrzanowski",
                "Eric Chung",
                "Victor Cui",
                "Steve Dai",
                "Bita Darvish Rouhani",
                "Carlo del Mundo",
                "Deena Donia",
                "Burc Eryilmaz",
                "Henry Estela",
                "Abhinav Goel",
                "Oleg Goncharov",
                "Yugi Guvvala",
                "Robert Hesse",
                "Russell Hewett",
                "Herbert Hum",
                "Ujval Kapasi",
                "Brucek Khailany",
                "Mikail Khona",
                "Nick Knight",
                "Alex Kondratenko",
                "Ronny Krashinsky",
                "Ben Lanir",
                "Simon Layton",
                "Michael Lightstone",
                "Daniel Lo",
                "Paulius Micikevicius",
                "Asit Mishra",
                "Tim Moon",
                "Deepak Narayanan",
                "Chao Ni",
                "Abhijit Paithankar",
                "Satish Pasumarthi",
                "Ankit Patel",
                "Mostofa Patwary",
                "Ashwin Poojary",
                "Gargi Prasad",
                "Sweta Priyadarshi",
                "Yigong Qin",
                "Xiaowei Ren",
                "Oleg Rybakov",
                "Charbel Sakr",
                "Sanjeev Satheesh",
                "Stas Sergienko",
                "Pasha Shamis",
                "Kirthi Shankar",
                "Nishant Sharma",
                "Mohammad Shoeybi",
                "Michael Siu",
                "Misha Smelyanskiy",
                "Darko Stosic",
                "Dusan Stosic",
                "Bor-Yiing Su",
                "Frank Sun",
                "Nima Tajbakhsh",
                "Shelby Thomas",
                "Przemek Tredak",
                "Evgeny Tsykunov",
                "Gandhi Vaithilingam",
                "Aditya Vavre",
                "Rangharajan Venkatesan",
                "Roger Waleffe",
                "Qiyu Wan",
                "Hexin Wang",
                "Mengdi Wang",
                "Lizzie Wei",
                "Hao Wu",
                "Evan Wu",
                "Keith Wyss",
                "Ning Xu",
                "Jinze Xue",
                "Charlene Yang",
                "Yujia Zhai",
                "Ruoxi Zhang",
                "Jingyang Zhu",
                "Zhongbo Zhu"
            ],
            "affiliations": [
                "DeepSeek-AI",
                "NVIDIA",
                "Open-Compute-Project"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25149.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#training"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ 4-Ğ±Ğ¸Ñ‚Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ NVFP4 Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ 8-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ FP8. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Random Hadamard transforms Ğ´Ğ»Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ², Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾ĞºÑ€ÑƒĞ³Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° 12 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° 10 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ FP8 baseline. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking 4-bit Precision for Powerful Language Models",
                    "desc": "This paper presents a new training method for large language models (LLMs) using a 4-bit floating point format called NVFP4. The approach incorporates Random Hadamard transforms to manage outliers, a two-dimensional quantization technique for consistent data representation, and stochastic rounding to improve gradient accuracy. By also using selective high-precision layers, the method ensures stable training even with reduced precision. The results demonstrate that models trained with NVFP4 can achieve performance similar to those trained with higher precision formats, marking a significant advancement in efficient LLM training."
                },
                "zh": {
                    "title": "NVFP4ï¼šå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒçš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è®­ç»ƒæ–¹æ³•ï¼Œä½¿ç”¨NVFP4æ ¼å¼ç»“åˆéšæœºå“ˆè¾¾ç›å˜æ¢ã€äºŒç»´é‡åŒ–ã€éšæœºèˆå…¥å’Œé€‰æ‹©æ€§é«˜ç²¾åº¦å±‚ï¼Œä»¥å®ç°å¤§è¯­è¨€æ¨¡å‹åœ¨4ä½ç²¾åº¦ä¸‹çš„ç¨³å®šå’Œå‡†ç¡®è®­ç»ƒã€‚å½“å‰çš„å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„é—®é¢˜è§£å†³èƒ½åŠ›ï¼Œä½†è®­ç»ƒè¿™äº›æ¨¡å‹éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºå’Œæ—¶é—´ã€‚é€šè¿‡é‡‡ç”¨4ä½æµ®ç‚¹æ•°è®­ç»ƒï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨è®¡ç®—é€Ÿåº¦å’Œèµ„æºåˆ©ç”¨ç‡ä¸Šæä¾›äº†é¢å¤–çš„æ”¹è¿›ï¼ŒåŒæ—¶è§£å†³äº†ä½ç²¾åº¦è®­ç»ƒå¸¦æ¥çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨NVFP4çš„é¢„è®­ç»ƒæŠ€æœ¯åœ¨è®­ç»ƒæŸå¤±å’Œä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®æ€§ä¸Šä¸FP8åŸºçº¿ç›¸å½“ï¼Œæ ‡å¿—ç€çª„ç²¾åº¦å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒç®—æ³•çš„é‡å¤§è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25131",
            "title": "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech",
            "url": "https://huggingface.co/papers/2509.25131",
            "abstract": "MGM-Omni is a unified multimodal language model for speech generation and understanding, featuring a dual-track architecture for efficient cross-modal interaction and data-efficient training.  \t\t\t\t\tAI-generated summary \t\t\t\t We present MGM-Omni, a unified Omni LLM for omni-modal understanding and expressive, long-horizon speech generation. Unlike cascaded pipelines that isolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a dual-track, token-based architecture that cleanly decouples multimodal reasoning from real-time speech generation. This design enables efficient cross-modal interaction and low-latency, streaming speech generation. For understanding, a unified training strategy coupled with a dual audio encoder design enables long-form audio perception across diverse acoustic conditions. For generation, a chunk-based parallel decoding scheme narrows the text speech token-rate gap, accelerating inference and supporting streaming zero-shot voice cloning with stable timbre over extended durations. Compared to concurrent work, MGM-Omni achieves these capabilities with markedly data-efficient training. Extensive experiments demonstrate that MGM-Omni outperforms existing open source models in preserving timbre identity across extended sequences, producing natural and context-aware speech, and achieving superior long-form audio and omnimodal understanding. MGM-Omni establishes an efficient, end-to-end paradigm for omnimodal understanding and controllable, personalised long-horizon speech generation.",
            "score": 4,
            "issue_id": 6153,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "e5f4eb55f7f45ab2",
            "authors": [
                "Chengyao Wang",
                "Zhisheng Zhong",
                "Bohao Peng",
                "Senqiao Yang",
                "Yuqi Liu",
                "Haokun Gui",
                "Bin Xia",
                "Jingyao Li",
                "Bei Yu",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "SmartMore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25131.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#games",
                    "#agi",
                    "#long_context",
                    "#multimodal",
                    "#interpretability",
                    "#audio",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¼Ğ¾Ğ·Ğ³-Ñ€Ğ¾Ñ‚",
                    "desc": "MGM-Omni Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ \"Ğ¼Ğ¾Ğ·Ğ³-Ñ€Ğ¾Ñ‚\", ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ĞºĞ°Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑÑ…ĞµĞ¼Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ñ‡Ğ°ÑÑ‚ÑĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğµ ĞºĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞ¼Ğ±Ñ€Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "MGM-Omni: Revolutionizing Speech Generation and Understanding",
                    "desc": "MGM-Omni is a cutting-edge multimodal language model designed for both understanding and generating speech. It features a unique dual-track architecture that separates the processes of multimodal reasoning and real-time speech generation, allowing for efficient interaction between different types of data. The model employs a unified training strategy and advanced audio encoding to enhance its ability to perceive and generate long-form audio across various conditions. With its innovative design, MGM-Omni achieves high-quality, context-aware speech generation while being data-efficient, outperforming existing models in maintaining voice identity and producing natural speech."
                },
                "zh": {
                    "title": "MGM-Omniï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€è¯­éŸ³ç”Ÿæˆä¸ç†è§£",
                    "desc": "MGM-Omniæ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œä¸“æ³¨äºè¯­éŸ³ç”Ÿæˆå’Œç†è§£ã€‚å®ƒé‡‡ç”¨åŒè½¨æ¶æ„ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œè·¨æ¨¡æ€äº¤äº’ï¼Œå¹¶å®ç°æ•°æ®é«˜æ•ˆè®­ç»ƒã€‚è¯¥æ¨¡å‹é€šè¿‡â€œè„‘-å£â€è®¾è®¡ï¼Œå°†å¤šæ¨¡æ€æ¨ç†ä¸å®æ—¶è¯­éŸ³ç”Ÿæˆè§£è€¦ï¼Œæ”¯æŒä½å»¶è¿Ÿçš„æµå¼è¯­éŸ³ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒMGM-Omniåœ¨ä¿æŒéŸ³è‰²ä¸€è‡´æ€§å’Œç”Ÿæˆè‡ªç„¶è¯­éŸ³æ–¹é¢ä¼˜äºç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24285",
            "title": "SCI-Verifier: Scientific Verifier with Thinking",
            "url": "https://huggingface.co/papers/2509.24285",
            "abstract": "A framework combining SCI-VerifyBench and SCI-Verifier addresses challenges in verifying LLM-generated scientific answers through cross-disciplinary benchmarks and reasoning-augmented verification.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) are increasingly applied to scientific reasoning, the complexity of answer formats and the diversity of equivalent expressions make answer verification a critical yet challenging task. Existing verification studies in scientific domains suffer from two major limitations: (a) the absence of systematic evaluation standards and insufficient disciplinary coverage, which hinders their comprehensive assessment; and (b) heavy reliance on cumbersome rule design or prompt engineering, which reduces their effectiveness in complex reasoning scenarios or limits their cross-disciplinary generalization. To address these challenges, we propose solutions at both the data and model levels. On the data side, we construct SCI-VerifyBench, a cross-disciplinary benchmark covering mathematics, physics, biology, chemistry, and general scientific QA. The benchmark is built from real LLM responses and enhanced with domain-specific equivalence transformations that generate challenging and realistic data. Model-based and expert annotations ensure both quality and diversity, enabling rigorous evaluation of verification ability. On the model side, we emphasize the importance of reasoning for verification and introduce SCI-Verifier, a unified reasoning-augmented verifier for scientific domains. Through post-training, SCI-Verifier demonstrates strong logical reasoning and equivalence judgment capabilities while maintaining concise and stable outputs. Together, SCI-VerifyBench and SCI-Verifier provide a principled framework for scientific verification, offering both systematic evaluation and practical pathways to enhance the reliability and applicability of LLMs in scientific domains.",
            "score": 4,
            "issue_id": 6157,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "9d001d0a49a6077c",
            "authors": [
                "Shenghe Zheng",
                "Chenyu Huang",
                "Fangchen Yu",
                "Junchi Yao",
                "Jingqi Ye",
                "Tao Chen",
                "Yun Luo",
                "Ning Ding",
                "LEI BAI",
                "Ganqu Cui",
                "Peng Ye"
            ],
            "affiliations": [
                "CUHK",
                "CUHK-Shenzhen",
                "Fudan University",
                "Harbin Institute of Technology",
                "Shanghai AI Laboratory",
                "Tsinghua University",
                "UESTC",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24285.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#reasoning",
                    "#science",
                    "#multimodal"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² AI Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµĞ¶Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SCI-VerifyBench, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ, Ñ„Ğ¸Ğ·Ğ¸ĞºÑƒ, Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ, Ñ…Ğ¸Ğ¼Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SCI-Verifier, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€Ğ¾ÑÑĞ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Scientific Verification with LLMs through Reasoning and Benchmarks",
                    "desc": "This paper presents a framework that combines SCI-VerifyBench and SCI-Verifier to improve the verification of scientific answers generated by large language models (LLMs). It identifies key challenges in current verification methods, such as lack of standardized evaluation and reliance on complex rule design. To tackle these issues, the authors introduce SCI-VerifyBench, a benchmark that spans multiple scientific disciplines and includes real LLM responses with domain-specific transformations. Additionally, they propose SCI-Verifier, a reasoning-augmented model that enhances verification accuracy through logical reasoning and equivalence judgment, ultimately aiming to increase the reliability of LLMs in scientific contexts."
                },
                "zh": {
                    "title": "ç§‘å­¦éªŒè¯çš„æ–°æ¡†æ¶ï¼šSCI-VerifyBenchä¸SCI-Verifierçš„ç»“åˆ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç»“åˆSCI-VerifyBenchå’ŒSCI-Verifierçš„æ¡†æ¶ï¼Œä»¥è§£å†³éªŒè¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„ç§‘å­¦ç­”æ¡ˆçš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡è·¨å­¦ç§‘åŸºå‡†å’Œå¢å¼ºæ¨ç†çš„éªŒè¯æ–¹æ³•ï¼Œæå‡äº†ç­”æ¡ˆéªŒè¯çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æ„å»ºäº†SCI-VerifyBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–æ•°å­¦ã€ç‰©ç†ã€ç”Ÿç‰©ã€åŒ–å­¦ç­‰é¢†åŸŸçš„è·¨å­¦ç§‘åŸºå‡†ï¼Œæä¾›äº†çœŸå®çš„LLMå“åº”å’Œé¢†åŸŸç‰¹å®šçš„ç­‰ä»·è½¬æ¢ã€‚åŒæ—¶ï¼ŒSCI-Verifierä½œä¸ºä¸€ä¸ªç»Ÿä¸€çš„æ¨ç†å¢å¼ºéªŒè¯å™¨ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„é€»è¾‘æ¨ç†å’Œç­‰ä»·åˆ¤æ–­èƒ½åŠ›ï¼Œç¡®ä¿äº†ç§‘å­¦é¢†åŸŸéªŒè¯çš„ç³»ç»Ÿæ€§å’Œå®ç”¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23371",
            "title": "Alignment through Meta-Weighted Online Sampling: Bridging the Gap\n  between Data Generation and Preference Optimization",
            "url": "https://huggingface.co/papers/2509.23371",
            "abstract": "Meta-Weighted Adaptive Preference Optimization (MetaAPO) dynamically balances online and offline data to align large language models with human preferences, outperforming existing methods and reducing annotation costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Preference optimization is crucial for aligning large language models (LLMs) with human values and intentions. A significant challenge in this process is the distribution mismatch between pre-collected offline preference data and the evolving model policy. Existing methods attempt to reduce this gap using static heuristics or decoupled online sampling strategies, but they often fail to adapt to the model's dynamic learning state. To bridge this gap, we propose Meta-Weighted Adaptive Preference Optimization (MetaAPO), a novel framework that dynamically couples data generation with model training. MetaAPO employs a lightweight meta-learner, as an \"alignment gap estimator\", to evaluate the potential benefits of on-policy sampling in relation to offline data. This guides targeted online generation and assigns sample-wise meta-weights to the optimization objective, dynamically balancing the quality and distribution of online and offline data. Experiments on AlpacaEval 2, Arena-Hard and MT-Bench demonstrate that MetaAPO consistently outperforms existing preference optimization approaches across various settings, while reducing 42% in online annotation costs.",
            "score": 4,
            "issue_id": 6152,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "e5432419a1266b60",
            "authors": [
                "Junming Yang",
                "Ning Xu",
                "Biao Liu",
                "Shiqi Qiao",
                "Xin Geng"
            ],
            "affiliations": [
                "School of Computer Science and Engineering Southeast University Nanjing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23371.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rlhf",
                    "#alignment"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MetaAPO - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½ÑƒÑ Ğ¼ĞµÑ‚Ğ°-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ñ‹ Ğ¾Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ Ğ²ĞµÑĞ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñƒ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ĞµĞ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° 42%."
                },
                "en": {
                    "title": "Dynamic Data Balancing for Better AI Alignment",
                    "desc": "MetaAPO is a new method designed to improve how large language models (LLMs) align with human preferences by effectively managing both online and offline data. It addresses the issue of distribution mismatch between previously collected offline preference data and the model's current learning state. By using a meta-learner to assess the value of on-policy sampling, MetaAPO dynamically adjusts the optimization process, ensuring that the model learns from the most relevant data. This approach not only enhances performance compared to existing methods but also significantly cuts down on the costs associated with online data annotation."
                },
                "zh": {
                    "title": "åŠ¨æ€å¹³è¡¡åœ¨çº¿ä¸ç¦»çº¿æ•°æ®çš„åå¥½ä¼˜åŒ–",
                    "desc": "MetaåŠ æƒè‡ªé€‚åº”åå¥½ä¼˜åŒ–ï¼ˆMetaAPOï¼‰æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨åŠ¨æ€å¹³è¡¡åœ¨çº¿å’Œç¦»çº¿æ•°æ®ï¼Œä»¥ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ã€‚è¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨è½»é‡çº§çš„å…ƒå­¦ä¹ å™¨æ¥è¯„ä¼°åœ¨çº¿é‡‡æ ·çš„æ½œåœ¨å¥½å¤„ï¼Œä»è€Œè§£å†³äº†é¢„å…ˆæ”¶é›†çš„ç¦»çº¿åå¥½æ•°æ®ä¸æ¨¡å‹æ”¿ç­–ä¹‹é—´çš„åˆ†å¸ƒä¸åŒ¹é…é—®é¢˜ã€‚MetaAPOé€šè¿‡åŠ¨æ€è°ƒæ•´åœ¨çº¿å’Œç¦»çº¿æ•°æ®çš„è´¨é‡å’Œåˆ†å¸ƒï¼Œä¼˜åŒ–äº†æ ·æœ¬çš„åŠ æƒç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMetaAPOåœ¨å¤šä¸ªè®¾ç½®ä¸­å‡ä¼˜äºç°æœ‰çš„åå¥½ä¼˜åŒ–æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨çº¿æ ‡æ³¨æˆæœ¬é™ä½äº†42%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23219",
            "title": "WirelessMathLM: Teaching Mathematical Reasoning for LLMs in Wireless\n  Communications with Reinforcement Learning",
            "url": "https://huggingface.co/papers/2509.23219",
            "abstract": "WirelessMathLM, a compact model trained with domain-specific reinforcement learning, achieves high accuracy on wireless mathematics problems and transfers well to general mathematics benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at general mathematical reasoning but fail catastrophically on specialized technical mathematics. In wireless communications, where problems require precise manipulation of information-theoretic bounds, optimization constraints, and signal processing formulations, even state-of-the-art models struggle to achieve competent performance. We present WirelessMathLM, demonstrating that compact models (0.5B-7B parameters) can match or exceed much larger models through domain-specific reinforcement learning with verifiable rewards. Our key insight is that wireless mathematics problems possess a unique property--verifiable correctness--that enables effective reinforcement learning without human feedback. We construct WirelessMathBench-XL, a comprehensive benchmark of 4,027 problems from 970 papers. Using Group Relative Policy Optimization (GRPO) with binary verification rewards, we train models directly from base checkpoints without supervised warm-start. Our 7B model achieves 39.5% accuracy on WirelessMathBench-XL, approaching GPT-4o (40.4%) while using about 100 times fewer parameters than DeepSeek-R1 (671B, 57.4%). Remarkably, GRPO training nearly doubles performance across all model scales (0.5B +11%, 3B +103%, 7B +81%), with positive transfer to general mathematics benchmarks--our models gain +8.4 points on average across MATH, Minerva-Math, OlympiadBench, AMC, and AIME without any training on these tasks.",
            "score": 4,
            "issue_id": 6158,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "ed842d0ce27b84a3",
            "authors": [
                "Xin Li",
                "Mengbing Liu",
                "Yiyang Zhu",
                "Wenhe Zhang",
                "Li Wei",
                "Jiancheng An",
                "Chau Yuen"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2509.23219.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#transfer_learning",
                    "#optimization",
                    "#benchmark",
                    "#small_models"
                ],
                "emoji": "ğŸ“¡",
                "ru": {
                    "title": "ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ² Ğ±ĞµÑĞ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ WirelessMathLM - ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑÑÑ Ğ½Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ±ĞµÑĞ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ WirelessMathBench-XL Ğ¸Ğ· 4027 Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ±ĞµĞ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ 7B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 39.5%, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ñ GPT-4o Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² 100 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ transfer learning Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ² Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 8.4 Ğ¿ÑƒĞ½ĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Compact Models, Big Results: Mastering Wireless Mathematics with Reinforcement Learning",
                    "desc": "WirelessMathLM is a compact machine learning model designed specifically for solving wireless mathematics problems using domain-specific reinforcement learning. It demonstrates that smaller models, with 0.5B to 7B parameters, can achieve high accuracy comparable to larger models by leveraging the unique property of verifiable correctness in wireless mathematics. The model was trained using Group Relative Policy Optimization (GRPO) with binary verification rewards, allowing it to improve performance significantly without the need for human feedback. Additionally, WirelessMathLM shows positive transfer to general mathematics tasks, enhancing its accuracy on various benchmarks without direct training on those problems."
                },
                "zh": {
                    "title": "æ— çº¿æ•°å­¦çš„å¼ºåŒ–å­¦ä¹ æ–°çªç ´",
                    "desc": "WirelessMathLM æ˜¯ä¸€ä¸ªé€šè¿‡é¢†åŸŸç‰¹å®šçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ç´§å‡‘æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ— çº¿æ•°å­¦é—®é¢˜ä¸Šå®ç°é«˜å‡†ç¡®ç‡ï¼Œå¹¶ä¸”åœ¨ä¸€èˆ¬æ•°å­¦åŸºå‡†ä¸Šä¹Ÿè¡¨ç°è‰¯å¥½ã€‚å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸€èˆ¬æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸“ä¸šæŠ€æœ¯æ•°å­¦ä¸Šå´å¸¸å¸¸å¤±è´¥ã€‚æˆ‘ä»¬æå‡ºçš„ WirelessMathLM é€šè¿‡å¯éªŒè¯çš„å¥–åŠ±ï¼Œå±•ç¤ºäº†ç´§å‡‘æ¨¡å‹ï¼ˆå‚æ•°åœ¨0.5Båˆ°7Bä¹‹é—´ï¼‰å¯ä»¥é€šè¿‡é¢†åŸŸç‰¹å®šçš„å¼ºåŒ–å­¦ä¹ è¶…è¶Šæ›´å¤§æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæ— çº¿æ•°å­¦é—®é¢˜å…·æœ‰å¯éªŒè¯çš„æ­£ç¡®æ€§ï¼Œè¿™ä½¿å¾—åœ¨æ²¡æœ‰äººå·¥åé¦ˆçš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ æˆä¸ºå¯èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24269",
            "title": "AdvChain: Adversarial Chain-of-Thought Tuning for Robust Safety\n  Alignment of Large Reasoning Models",
            "url": "https://huggingface.co/papers/2509.24269",
            "abstract": "AdvChain enhances the safety and reliability of large reasoning models by teaching them dynamic self-correction through adversarial chain-of-thought tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in complex problem-solving through Chain-of-Thought (CoT) reasoning. However, the multi-step nature of CoT introduces new safety challenges that extend beyond conventional language model alignment. We identify a failure mode in current safety CoT tuning methods: the snowball effect, where minor reasoning deviations progressively amplify throughout the thought process, leading to either harmful compliance or excessive refusal. This effect stems from models being trained to imitate perfect reasoning scripts without learning to self-correct. To address this limitation, we propose AdvChain, an alignment paradigm that teaches models dynamic self-correction through adversarial CoT tuning. Our method involves constructing a dataset containing Temptation-Correction and Hesitation-Correction samples, where models learn to recover from harmful reasoning drifts and unnecessary cautions. Extensive experiments show that AdvChain significantly enhances robustness against jailbreak attacks and CoT hijacking while substantially reducing over-refusal on benign prompts, achieving a superior safety-utility balance without compromising reasoning capabilities. Our work establishes a new direction for building more robust and reliable reasoning models.",
            "score": 3,
            "issue_id": 6153,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "dd7bc5e3eeee681d",
            "authors": [
                "Zihao Zhu",
                "Xinyu Wu",
                "Gehan Hu",
                "Siwei Lyu",
                "Ke Xu",
                "Baoyuan Wu"
            ],
            "affiliations": [
                "Huawei International, Singapore",
                "State University of New York at Buffalo",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24269.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#reasoning",
                    "#security",
                    "#alignment",
                    "#rlhf"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ AI ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· adversarial Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Â«ÑÑ„Ñ„ĞµĞºÑ‚Ğ° ÑĞ½ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ°Â» Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LRM), Ğ³Ğ´Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ Ğ² Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼ Ğ¸Ğ»Ğ¸ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°Ğ¼. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ AdvChain, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· adversarial Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Chain-of-Thought. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Â«Ğ˜ÑĞºÑƒÑˆĞµĞ½Ğ¸Ğµ-Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµÂ» Ğ¸ Â«ĞšĞ¾Ğ»ĞµĞ±Ğ°Ğ½Ğ¸Ğµ-Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµÂ», Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ AdvChain Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº jailbreak Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹ Ğ½Ğ° Ğ±ĞµĞ·Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ñ…, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒÑ."
                },
                "en": {
                    "title": "Empowering Models with Dynamic Self-Correction for Safer Reasoning",
                    "desc": "AdvChain is a novel approach designed to improve the safety and reliability of Large Reasoning Models (LRMs) by enabling them to self-correct during complex reasoning tasks. It addresses a critical issue known as the snowball effect, where small errors in reasoning can escalate, leading to harmful outcomes or excessive caution. By using adversarial chain-of-thought tuning, AdvChain trains models with specific datasets that help them recover from these reasoning drifts. The results demonstrate that this method enhances the models' robustness against attacks while maintaining their reasoning capabilities, thus achieving a better balance between safety and utility."
                },
                "zh": {
                    "title": "AdvChainï¼šæå‡æ¨ç†æ¨¡å‹çš„å®‰å…¨æ€§ä¸å¯é æ€§",
                    "desc": "AdvChainé€šè¿‡å¯¹æŠ—æ€§æ€ç»´é“¾è°ƒä¼˜ï¼Œå¢å¼ºäº†å¤§å‹æ¨ç†æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚è¯¥æ–¹æ³•è§£å†³äº†å½“å‰æ€ç»´é“¾è°ƒä¼˜ä¸­çš„é›ªå´©æ•ˆåº”é—®é¢˜ï¼Œé¿å…äº†å°çš„æ¨ç†åå·®åœ¨æ€ç»´è¿‡ç¨‹ä¸­é€æ¸æ”¾å¤§çš„æƒ…å†µã€‚é€šè¿‡æ„å»ºåŒ…å«è¯±æƒ‘-çº æ­£å’ŒçŠ¹è±«-çº æ­£æ ·æœ¬çš„æ•°æ®é›†ï¼Œæ¨¡å‹å­¦ä¹ å¦‚ä½•ä»æœ‰å®³çš„æ¨ç†åå·®ä¸­æ¢å¤ã€‚å®éªŒè¡¨æ˜ï¼ŒAdvChainæ˜¾è‘—æé«˜äº†å¯¹è¶Šç‹±æ”»å‡»å’Œæ€ç»´é“¾åŠ«æŒçš„é²æ£’æ€§ï¼ŒåŒæ—¶å‡å°‘äº†å¯¹è‰¯æ€§æç¤ºçš„è¿‡åº¦æ‹’ç»ï¼Œè¾¾åˆ°äº†å®‰å…¨æ€§å’Œæ•ˆç”¨çš„ä¼˜è‰¯å¹³è¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23338",
            "title": "PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation",
            "url": "https://huggingface.co/papers/2509.23338",
            "abstract": "PARROT is a benchmark for evaluating Cross-System SQL Translation across multiple database systems, addressing limitations in existing SQL benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMS) have shown increasing effectiveness in Text-to-SQL tasks. However, another closely related problem, Cross-System SQL Translation (a.k.a., SQL-to-SQL), which adapts a query written for one database system (e.g., MySQL) into its equivalent one for another system (e.g., ClickHouse), is of great practical importance but remains underexplored. Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which (1) focus on a limited set of database systems (often just SQLite) and (2) cannot capture many system-specific SQL dialects (e.g., customized functions, data types, and syntax rules). Thus, in this paper, we introduce PARROT, a Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT comprises 598 translation pairs from 38 open-source benchmarks and real-world business services, specifically prepared to challenge system-specific SQL understanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We also provide multiple benchmark variants, including PARROT-Diverse with 28,003 translations (for extensive syntax testing) and PARROT-Simple with 5,306 representative samples (for focused stress testing), covering 22 production-grade database systems. To promote future research, we release a public leaderboard and source code at: https://code4db.github.io/parrot-bench/.",
            "score": 3,
            "issue_id": 6157,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "933424d8e1e4106a",
            "authors": [
                "Wei Zhou",
                "Guoliang Li",
                "Haoyu Wang",
                "Yuxing Han",
                "Xufei Wu",
                "Fan Wu",
                "Xuanhe Zhou"
            ],
            "affiliations": [
                "ByteDance",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23338.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#survey",
                    "#open_source"
                ],
                "emoji": "ğŸ¦œ",
                "ru": {
                    "title": "PARROT: Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° SQL Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PARROT Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Cross-System SQL Translation - Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ SQL Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‚ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ñ‹ SQL. PARROT Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 598 Ğ¿Ğ°Ñ€ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ğ· 38 Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¸Ğ·Ğ½ĞµÑ-ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ 22 Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ½ĞµĞµ 38.53% Ğ½Ğ° ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° SQL."
                },
                "en": {
                    "title": "PARROT: Advancing Cross-System SQL Translation Evaluation",
                    "desc": "PARROT is a new benchmark designed to evaluate Cross-System SQL Translation, which is the process of converting SQL queries from one database system to another. It addresses the shortcomings of existing benchmarks that often focus on a narrow range of systems and fail to account for unique SQL dialects. The benchmark includes 598 translation pairs from various sources and offers different variants for testing, such as PARROT-Diverse and PARROT-Simple, to assess system-specific SQL understanding. By providing a public leaderboard and source code, PARROT aims to foster further research in this important area of machine learning and database management."
                },
                "zh": {
                    "title": "PARROTï¼šè·¨ç³»ç»ŸSQLç¿»è¯‘çš„å…¨æ–°åŸºå‡†",
                    "desc": "PARROTæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è·¨ç³»ç»ŸSQLç¿»è¯‘çš„åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰SQLåŸºå‡†çš„å±€é™æ€§ã€‚å®ƒåŒ…å«æ¥è‡ª38ä¸ªå¼€æºåŸºå‡†å’ŒçœŸå®ä¸šåŠ¡æœåŠ¡çš„598ä¸ªç¿»è¯‘å¯¹ï¼Œä¸“é—¨è®¾è®¡ç”¨äºæŒ‘æˆ˜ç³»ç»Ÿç‰¹å®šçš„SQLç†è§£ã€‚PARROTæä¾›äº†å¤šä¸ªåŸºå‡†å˜ä½“ï¼ŒåŒ…æ‹¬PARROT-Diverseå’ŒPARROT-Simpleï¼Œè¦†ç›–22ä¸ªç”Ÿäº§çº§æ•°æ®åº“ç³»ç»Ÿã€‚é€šè¿‡è¿™ä¸ªåŸºå‡†ï¼Œç ”ç©¶äººå‘˜å¯ä»¥æ›´å¥½åœ°è¯„ä¼°å’Œæ”¹è¿›è·¨ç³»ç»ŸSQLç¿»è¯‘çš„æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22830",
            "title": "ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents",
            "url": "https://huggingface.co/papers/2509.22830",
            "abstract": "ChatInject, a novel attack exploiting structured chat templates and persuasive multi-turn dialogues, significantly enhances attack success rates on large language model-based agents compared to traditional methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The growing deployment of large language model (LLM) based agents that interact with external environments has created new attack surfaces for adversarial manipulation. One major threat is indirect prompt injection, where attackers embed malicious instructions in external environment output, causing agents to interpret and execute them as if they were legitimate prompts. While previous research has focused primarily on plain-text injection attacks, we find a significant yet underexplored vulnerability: LLMs' dependence on structured chat templates and their susceptibility to contextual manipulation through persuasive multi-turn dialogues. To this end, we introduce ChatInject, an attack that formats malicious payloads to mimic native chat templates, thereby exploiting the model's inherent instruction-following tendencies. Building on this foundation, we develop a persuasion-driven Multi-turn variant that primes the agent across conversational turns to accept and execute otherwise suspicious actions. Through comprehensive experiments across frontier LLMs, we demonstrate three critical findings: (1) ChatInject achieves significantly higher average attack success rates than traditional prompt injection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13% to 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong performance at average 52.33% success rate on InjecAgent, (2) chat-template-based payloads demonstrate strong transferability across models and remain effective even against closed-source LLMs, despite their unknown template structures, and (3) existing prompt-based defenses are largely ineffective against this attack approach, especially against Multi-turn variants. These findings highlight vulnerabilities in current agent systems.",
            "score": 3,
            "issue_id": 6153,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "5a9ccef5c0547575",
            "authors": [
                "Hwan Chang",
                "Yonghyun Jun",
                "Hwanhee Lee"
            ],
            "affiliations": [
                "Department of Artificial Intelligence, Chung-Ang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22830.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#rlhf",
                    "#security"
                ],
                "emoji": "ğŸ’¬",
                "ru": {
                    "title": "ĞĞ±Ğ¼Ğ°Ğ½ Ñ‡ĞµÑ€ĞµĞ· Ñ‡Ğ°Ñ‚-ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹: Ğ½Ğ¾Ğ²Ğ°Ñ ÑƒĞ³Ñ€Ğ¾Ğ·Ğ° Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ChatInject - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚Ğ¸Ğ¿ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ chat-ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞÑ‚Ğ°ĞºĞ° Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ»Ğ¾Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ñ‚Ğ°Ğº, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ½Ğ¸ Ğ²Ñ‹Ğ³Ğ»ÑĞ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ñ‡Ğ°Ñ‚Ğµ, ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ¸Ñ€ÑƒÑ ÑĞºĞ»Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞœĞ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¾Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ChatInject Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ prompt injection, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ¼Ğ°Ğ»Ğ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸."
                },
                "en": {
                    "title": "ChatInject: Elevating Attack Success with Persuasive Dialogues",
                    "desc": "The paper introduces ChatInject, a new type of attack that targets large language model (LLM) agents by using structured chat templates and persuasive multi-turn dialogues. This method significantly increases the success rate of attacks compared to traditional prompt injection techniques. The research reveals that LLMs are particularly vulnerable to indirect prompt injection, where malicious instructions are hidden in seemingly legitimate outputs. Through experiments, the authors demonstrate that ChatInject not only achieves higher success rates but also shows strong transferability across different models, exposing critical weaknesses in current defenses against such attacks."
                },
                "zh": {
                    "title": "åˆ©ç”¨èŠå¤©æ¨¡æ¿çš„æ”»å‡»æ–°æ–¹å¼ï¼šChatInject",
                    "desc": "ChatInjectæ˜¯ä¸€ç§æ–°å‹æ”»å‡»æ–¹æ³•ï¼Œåˆ©ç”¨ç»“æ„åŒ–èŠå¤©æ¨¡æ¿å’Œè¯´æœæ€§çš„å¤šè½®å¯¹è¯ï¼Œæ˜¾è‘—æé«˜äº†å¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†çš„æ”»å‡»æˆåŠŸç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ¶æ„è´Ÿè½½æ ¼å¼åŒ–ä¸ºç±»ä¼¼äºåŸç”ŸèŠå¤©æ¨¡æ¿çš„å½¢å¼ï¼Œåˆ©ç”¨äº†æ¨¡å‹å›ºæœ‰çš„æŒ‡ä»¤éµå¾ªå€¾å‘ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒChatInjectåœ¨å¤šä¸ªå‰æ²¿å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„æ”»å‡»æˆåŠŸç‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤šè½®å¯¹è¯ä¸­ï¼ŒæˆåŠŸç‡å¯è¾¾52.33%ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºäºæç¤ºçš„é˜²å¾¡æªæ–½å¯¹è¿™ç§æ”»å‡»æ–¹æ³•çš„æœ‰æ•ˆæ€§è¾ƒä½ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹å¤šè½®å˜ä½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22570",
            "title": "UniMIC: Token-Based Multimodal Interactive Coding for Human-AI\n  Collaboration",
            "url": "https://huggingface.co/papers/2509.22570",
            "abstract": "UniMIC, a unified token-based framework, enhances multimodal communication by using compact tokenized representations and lightweight Transformer-based entropy models, achieving significant bitrate savings without compromising performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI agents is transforming human-AI collaboration into bidirectional, multimodal interaction. However, existing codecs remain optimized for unimodal, one-way communication, resulting in repeated degradation under conventional compress-transmit-reconstruct pipelines. To address this limitation, we propose UniMIC, a Unified token-based Multimodal Interactive Coding framework that bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or plain text, UniMIC employs compact tokenized representations as the communication medium, enabling efficient low-bitrate transmission while maintaining compatibility with LMMs. To further enhance compression, lightweight Transformer-based entropy models with scenario-specific designs-generic, masked, and text-conditioned-effectively minimize inter-token redundancy. Extensive experiments on text-to-image generation, text-guided inpainting, outpainting, and visual question answering show that UniMIC achieves substantial bitrate savings and remains robust even at ultra-low bitrates (<0.05bpp), without compromising downstream task performance. These results establish UniMIC as a practical and forward-looking paradigm for next-generation multimodal interactive communication.",
            "score": 3,
            "issue_id": 6153,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "0d8cb97b2040b91e",
            "authors": [
                "Qi Mao",
                "Tinghan Yang",
                "Jiahao Li",
                "Bin Li",
                "Libiao Jin",
                "Yan Lu"
            ],
            "affiliations": [
                "Microsoft Research Asia, Beijing 10080, China",
                "State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing 100024, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22570.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#games",
                    "#optimization",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸ“¡",
                "ru": {
                    "title": "Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    "desc": "UniMIC Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ ÑÑ‹Ñ€Ñ‹Ñ… Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ›ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğµ Transformer-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµÑ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ²: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾, Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾-Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°."
                },
                "en": {
                    "title": "UniMIC: Efficient Multimodal Communication with Tokenization",
                    "desc": "UniMIC is a new framework designed to improve communication between different types of data, like text and images, using a token-based approach. It replaces traditional methods that send raw data with compact tokenized representations, which allows for more efficient data transmission. By using lightweight Transformer models to reduce redundancy, UniMIC can save significant amounts of data while still performing well in various tasks. This makes it a promising solution for future interactions between edge devices and cloud-based AI systems."
                },
                "zh": {
                    "title": "UniMICï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€äº¤äº’ç¼–ç æ¡†æ¶",
                    "desc": "UniMICæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŸºäºä»¤ç‰Œçš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€é€šä¿¡ã€‚å®ƒé€šè¿‡ä½¿ç”¨ç´§å‡‘çš„ä»¤ç‰ŒåŒ–è¡¨ç¤ºå’Œè½»é‡çº§çš„åŸºäºTransformerçš„ç†µæ¨¡å‹ï¼Œå®ç°äº†æ˜¾è‘—çš„æ¯”ç‰¹ç‡èŠ‚çœï¼ŒåŒæ—¶ä¸å½±å“æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„å•ä¸€æ¨¡æ€é€šä¿¡ä¸åŒï¼ŒUniMICèƒ½å¤Ÿåœ¨è¾¹ç¼˜è®¾å¤‡å’Œäº‘AIä»£ç†ä¹‹é—´é«˜æ•ˆä¼ è¾“ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniMICåœ¨è¶…ä½æ¯”ç‰¹ç‡ä¸‹ä»ç„¶ä¿æŒå¼ºå¤§çš„æ€§èƒ½ï¼Œé€‚ç”¨äºä¸‹ä¸€ä»£å¤šæ¨¡æ€äº¤äº’é€šä¿¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.25052",
            "title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and\n  Planning",
            "url": "https://huggingface.co/papers/2509.25052",
            "abstract": "CEL, a novel agent architecture using a Large Language Model, learns to master complex environments through explicit reasoning and planning, achieving success in diverse grid-world tasks with sparse rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of artificial agents that can learn to master complex environments has led to remarkable successes, yet prevailing deep reinforcement learning methods often rely on immense experience, encoding their knowledge opaquely within neural network weights. We propose a different paradigm, one in which an agent learns to play by reasoning and planning. We introduce Cogito, ergo ludo (CEL), a novel agent architecture that leverages a Large Language Model (LLM) to build an explicit, language-based understanding of its environment's mechanics and its own strategy. Starting from a tabula rasa state with no prior knowledge (except action set), CEL operates on a cycle of interaction and reflection. After each episode, the agent analyzes its complete trajectory to perform two concurrent learning processes: Rule Induction, where it refines its explicit model of the environment's dynamics, and Strategy and Playbook Summarization, where it distills experiences into an actionable strategic playbook. We evaluate CEL on diverse grid-world tasks (i.e., Minesweeper, Frozen Lake, and Sokoban), and show that the CEL agent successfully learns to master these games by autonomously discovering their rules and developing effective policies from sparse rewards. Ablation studies confirm that the iterative process is critical for sustained learning. Our work demonstrates a path toward more general and interpretable agents that not only act effectively but also build a transparent and improving model of their world through explicit reasoning on raw experience.",
            "score": 2,
            "issue_id": 6154,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "ca5fb75d67ceeb90",
            "authors": [
                "Sai Wang",
                "Yu Wu",
                "Zhongwen Xu"
            ],
            "affiliations": [
                "Tencent",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.25052.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#interpretability",
                    "#reasoning",
                    "#games",
                    "#agents"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ÑŒ: ÑĞ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ° CEL (Cogito, ergo ludo), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ… Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞĞ³ĞµĞ½Ñ‚ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ñ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ†Ğ¸ĞºĞ»Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, CEL ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ ÑĞ²Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» ÑÑ€ĞµĞ´Ñ‹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚Ğ¸Ğ¿Ğ° grid-world Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¸Ğ³Ñ€ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ñ…."
                },
                "en": {
                    "title": "Reasoning and Planning for Mastery in Complex Environments",
                    "desc": "The paper introduces CEL, a new agent architecture that utilizes a Large Language Model to enhance learning in complex environments. Unlike traditional deep reinforcement learning methods that depend heavily on vast amounts of experience, CEL focuses on reasoning and planning to understand its surroundings. The agent learns by analyzing its actions and refining its knowledge of the environment's rules while creating a strategic playbook for decision-making. This approach allows CEL to effectively master various grid-world tasks by autonomously discovering rules and developing policies from limited rewards."
                },
                "zh": {
                    "title": "é€šè¿‡æ¨ç†ä¸è§„åˆ’æŒæ¡å¤æ‚ç¯å¢ƒçš„æ™ºèƒ½ä½“",
                    "desc": "CELæ˜¯ä¸€ç§æ–°å‹çš„æ™ºèƒ½ä½“æ¶æ„ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å­¦ä¹ å¤æ‚ç¯å¢ƒä¸­çš„æ¨ç†å’Œè§„åˆ’ã€‚å®ƒé€šè¿‡æ˜ç¡®çš„è¯­è¨€ç†è§£ç¯å¢ƒçš„æœºåˆ¶å’Œè‡ªèº«ç­–ç•¥ï¼Œä»è€Œåœ¨ç¨€ç–å¥–åŠ±çš„æƒ…å†µä¸‹æˆåŠŸæŒæ¡å¤šç§ç½‘æ ¼ä¸–ç•Œä»»åŠ¡ã€‚CELåœ¨æ¯ä¸ªå›åˆååˆ†æå…¶å®Œæ•´è½¨è¿¹ï¼Œè¿›è¡Œè§„åˆ™å½’çº³å’Œç­–ç•¥æ€»ç»“ï¼Œé€æ­¥ä¼˜åŒ–å…¶ç¯å¢ƒæ¨¡å‹å’Œè¡ŒåŠ¨ç­–ç•¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§è¿­ä»£å­¦ä¹ è¿‡ç¨‹å¯¹äºæŒç»­å­¦ä¹ è‡³å…³é‡è¦ï¼Œå±•ç¤ºäº†æ„å»ºæ›´é€šç”¨å’Œå¯è§£é‡Šæ™ºèƒ½ä½“çš„å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24910",
            "title": "Learning Goal-Oriented Language-Guided Navigation with Self-Improving\n  Demonstrations at Scale",
            "url": "https://huggingface.co/papers/2509.24910",
            "abstract": "SID, a self-improving demonstration approach, enhances exploration and generalization in goal-oriented language-guided navigation tasks, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Goal-oriented language-guided navigation requires robust exploration capabilities for agents to navigate to specified goals in unknown environments without step-by-step instructions. Existing methods tend to exclusively utilize shortest-path trajectories, lacking effective exploration priors for training navigation agents. To address the above challenges, we present SID, a goal-oriented language-guided navigation learning approach with Self-Improving Demonstrations. Specifically, SID learns an initial agent on the shortest-path data sampled from environments and then leverages this agent to generate novel exploration trajectories. The novel rollouts provide demonstrations with stronger exploration strategies to train a better agent, which in turn produces higher-quality agent demonstrations for the next round of training. We show that this iterative self-improving pipeline readily scales to new environments, and the resulting demonstrations can be transferred across a variety of language-guided navigation tasks, elevating the performance ceiling in diverse goal-oriented navigation tasks. Extensive experiments demonstrate that SID significantly boosts the exploration capabilities and generalization of navigation agents. The resulting agent achieves new state-of-the-art performance on goal-oriented language-guided navigation tasks, including REVERIE, SOON, notably achieving a 50.9% success rate on the unseen validation splits of SOON, surpassing the prior leading approaches by a margin of 13.9%.",
            "score": 2,
            "issue_id": 6153,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "6aa993d12be845fa",
            "authors": [
                "Songze Li",
                "Zun Wang",
                "Gengze Zhou",
                "Jialu Li",
                "Xiangyu Zeng",
                "Limin Wang",
                "Yu Qiao",
                "Qi Wu",
                "Mohit Bansal",
                "Yi Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Nanjing University",
                "Shanghai AI Laboratory",
                "The University of Adelaide",
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24910.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#agi",
                    "#optimization",
                    "#transfer_learning",
                    "#agents"
                ],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SID - Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° ĞºÑ€Ğ°Ñ‚Ñ‡Ğ°Ğ¹ÑˆĞ¸Ñ… Ğ¿ÑƒÑ‚ÑÑ…, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ³Ğ¾ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ€ĞµĞ´Ñ‹. Ğ­Ñ‚Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒĞ² Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞºĞ¾Ñ€Ğ´Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° benchmark'Ğ°Ñ… REVERIE Ğ¸ SOON."
                },
                "en": {
                    "title": "SID: Self-Improving Demonstrations for Enhanced Navigation",
                    "desc": "The paper introduces SID, a novel approach for enhancing exploration and generalization in goal-oriented language-guided navigation tasks. It addresses the limitations of existing methods that rely solely on shortest-path trajectories by implementing a self-improving demonstration framework. SID begins with an initial agent trained on basic data and iteratively generates new exploration trajectories that improve the agent's performance. This method not only scales to new environments but also allows for the transfer of learned strategies across various navigation tasks, achieving state-of-the-art results in multiple benchmarks."
                },
                "zh": {
                    "title": "è‡ªæˆ‘æ”¹è¿›æ¼”ç¤ºï¼Œæå‡å¯¼èˆªèƒ½åŠ›ï¼",
                    "desc": "SIDæ˜¯ä¸€ç§è‡ªæˆ‘æ”¹è¿›çš„æ¼”ç¤ºæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºç›®æ ‡å¯¼å‘çš„è¯­è¨€å¼•å¯¼å¯¼èˆªä»»åŠ¡ä¸­çš„æ¢ç´¢å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é¦–å…ˆåœ¨æœ€çŸ­è·¯å¾„æ•°æ®ä¸Šè®­ç»ƒåˆå§‹ä»£ç†ï¼Œç„¶ååˆ©ç”¨è¯¥ä»£ç†ç”Ÿæˆæ–°çš„æ¢ç´¢è½¨è¿¹ã€‚é€šè¿‡è¿™ç§è¿­ä»£çš„è‡ªæˆ‘æ”¹è¿›æµç¨‹ï¼ŒSIDèƒ½å¤Ÿåœ¨æ–°çš„ç¯å¢ƒä¸­æ‰©å±•ï¼Œå¹¶åœ¨å¤šç§è¯­è¨€å¼•å¯¼çš„å¯¼èˆªä»»åŠ¡ä¸­å®ç°æ¼”ç¤ºçš„è¿ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSIDæ˜¾è‘—æé«˜äº†å¯¼èˆªä»£ç†çš„æ¢ç´¢èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23143",
            "title": "MathBode: Frequency-Domain Fingerprints of LLM Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2509.23143",
            "abstract": "MathBode provides a diagnostic for mathematical reasoning in LLMs by analyzing frequency-resolved metrics of model outputs compared to exact solutions, revealing systematic low-pass behavior and phase lag.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents MathBode, a dynamic diagnostic for mathematical reasoning in large language models (LLMs). Instead of one-shot accuracy, MathBode treats each parametric problem as a system: we drive a single parameter sinusoidally and fit first-harmonic responses of model outputs and exact solutions. This yields interpretable, frequency-resolved metrics -- gain (amplitude tracking) and phase (lag) -- that form Bode-style fingerprints. Across five closed-form families (linear solve, ratio/saturation, compound interest, 2x2 linear systems, similar triangles), the diagnostic surfaces systematic low-pass behavior and growing phase lag that accuracy alone obscures. We compare several models against a symbolic baseline that calibrates the instrument (G approx 1, phi approx 0). Results separate frontier from mid-tier models on dynamics, providing a compact, reproducible protocol that complements standard benchmarks with actionable measurements of reasoning fidelity and consistency. We open-source the dataset and code to enable further research and adoption.",
            "score": 2,
            "issue_id": 6154,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "c2f1e8211a73ca99",
            "authors": [
                "Charles L. Wang"
            ],
            "affiliations": [
                "Department of Computer Science, Columbia University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23143.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#reasoning",
                    "#math",
                    "#interpretability",
                    "#dataset"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ§Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² LLM",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MathBode - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ´Ğ²Ğ¸Ğ³Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ LLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ low-pass Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑÑ‚ÑƒÑ‰ÑƒÑ Ñ„Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¸ Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ AI."
                },
                "en": {
                    "title": "MathBode: Unveiling the Dynamics of Mathematical Reasoning in LLMs",
                    "desc": "This paper introduces MathBode, a diagnostic tool designed to evaluate mathematical reasoning in large language models (LLMs) by analyzing their outputs against exact solutions. Instead of focusing solely on accuracy, MathBode examines how models respond to changes in parameters, using frequency-resolved metrics like gain and phase to create Bode-style fingerprints. The findings reveal that many models exhibit systematic low-pass behavior and increasing phase lag, which are not apparent when only considering accuracy. By comparing various models to a symbolic baseline, MathBode provides a new way to assess reasoning fidelity and consistency, and the authors have made the dataset and code available for further research."
                },
                "zh": {
                    "title": "MathBodeï¼šæ­ç¤ºLLMsæ•°å­¦æ¨ç†çš„æ–°å·¥å…·",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†MathBodeï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ•°å­¦æ¨ç†çš„åŠ¨æ€è¯Šæ–­å·¥å…·ã€‚MathBodeé€šè¿‡åˆ†ææ¨¡å‹è¾“å‡ºä¸ç²¾ç¡®è§£çš„é¢‘ç‡åˆ†è¾¨åº¦é‡ï¼Œæ­ç¤ºäº†ç³»ç»Ÿæ€§çš„ä½é€šè¡Œä¸ºå’Œç›¸ä½æ»åã€‚ä¸å•æ¬¡å‡†ç¡®æ€§ä¸åŒï¼ŒMathBodeå°†æ¯ä¸ªå‚æ•°é—®é¢˜è§†ä¸ºä¸€ä¸ªç³»ç»Ÿï¼Œé€šè¿‡é©±åŠ¨å•ä¸ªå‚æ•°çš„æ­£å¼¦æ³¢å¹¶æ‹Ÿåˆæ¨¡å‹è¾“å‡ºå’Œç²¾ç¡®è§£çš„ç¬¬ä¸€è°æ³¢å“åº”ï¼Œç”Ÿæˆå¯è§£é‡Šçš„é¢‘ç‡åˆ†è¾¨åº¦é‡ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒMathBodeèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†ä¸åŒæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶æä¾›å¯é‡å¤çš„åè®®ï¼Œä»¥è¡¥å……æ ‡å‡†åŸºå‡†æµ‹è¯•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23061",
            "title": "Local Success Does Not Compose: Benchmarking Large Language Models for\n  Compositional Formal Verification",
            "url": "https://huggingface.co/papers/2509.23061",
            "abstract": "DafnyCOMP evaluates large language models on generating compositional specifications in Dafny, highlighting their weaknesses in cross-functional reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce DafnyCOMP, a benchmark for evaluating large language models (LLMs) on compositional specification generation in Dafny. Unlike prior benchmarks that focus on single-function tasks, DafnyCOMP targets programs composed of multiple interacting functions with data dependencies, requiring reasoning across component boundaries. The benchmark consists of 300 automatically synthesized multi-function programs. We evaluate several state-of-the-art LLM families and find that, while they perform well on single-function verification, their performance drops sharply on compositional tasks. Analysis reveals systematic failures in cross-functional reasoning, including fragile specifications, misalignment between implementations and proofs, and unstable reasoning. DafnyCOMP thus provides a diagnostic tool for measuring progress toward reliable, verifiable, and compositional code generation with LLMs.",
            "score": 2,
            "issue_id": 6158,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "b9ab7885c301c8a2",
            "authors": [
                "Xu Xu",
                "Xin Li",
                "Xingwei Qu",
                "Jie Fu",
                "Binhang Yuan"
            ],
            "affiliations": [
                "HKUST",
                "NTU",
                "Shanghai AI Lab",
                "UoM"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23061.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ - ÑĞ»Ğ°Ğ±Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ DafnyCOMP - Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞµ Dafny. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑÑ…, DafnyCOMP Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ… Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ñ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹, Ğ½Ğ¾ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ĞºĞ¾ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²Ñ‹ÑĞ²Ğ¸Ğ» ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¼ĞµĞ¶Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ…Ñ€ÑƒĞ¿ĞºĞ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ."
                },
                "en": {
                    "title": "DafnyCOMP: Testing LLMs on Complex Code Generation",
                    "desc": "DafnyCOMP is a new benchmark designed to test large language models (LLMs) on their ability to generate specifications for programs that involve multiple functions working together. Unlike previous tests that only looked at single-function tasks, DafnyCOMP focuses on the complexities of programs where functions depend on each other. The study found that while LLMs excel at verifying single-function tasks, they struggle significantly with tasks that require reasoning across multiple functions. This highlights important weaknesses in their ability to create reliable and coherent specifications for complex code."
                },
                "zh": {
                    "title": "DafnyCOMPï¼šè¯„ä¼°è·¨åŠŸèƒ½æ¨ç†èƒ½åŠ›çš„åŸºå‡†",
                    "desc": "DafnyCOMPæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”ŸæˆDafnyçš„ç»„åˆè§„èŒƒæ–¹é¢çš„åŸºå‡†ã€‚ä¸ä¹‹å‰ä¸“æ³¨äºå•ä¸€åŠŸèƒ½ä»»åŠ¡çš„åŸºå‡†ä¸åŒï¼ŒDafnyCOMPé’ˆå¯¹ç”±å¤šä¸ªç›¸äº’ä½œç”¨çš„å‡½æ•°ç»„æˆçš„ç¨‹åºï¼Œè¿™äº›å‡½æ•°ä¹‹é—´å­˜åœ¨æ•°æ®ä¾èµ–å…³ç³»ï¼Œéœ€è¦è·¨ç»„ä»¶è¾¹ç•Œè¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬è¯„ä¼°äº†å‡ ç§æœ€å…ˆè¿›çš„LLMæ¨¡å‹ï¼Œå‘ç°å®ƒä»¬åœ¨å•ä¸€åŠŸèƒ½éªŒè¯ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç»„åˆä»»åŠ¡ä¸Šçš„è¡¨ç°æ€¥å‰§ä¸‹é™ã€‚åˆ†ææ˜¾ç¤ºåœ¨è·¨åŠŸèƒ½æ¨ç†æ–¹é¢å­˜åœ¨ç³»ç»Ÿæ€§å¤±è´¥ï¼ŒåŒ…æ‹¬è„†å¼±çš„è§„èŒƒã€å®ç°ä¸è¯æ˜ä¹‹é—´çš„ä¸ä¸€è‡´ä»¥åŠä¸ç¨³å®šçš„æ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.22518",
            "title": "REMA: A Unified Reasoning Manifold Framework for Interpreting Large\n  Language Model",
            "url": "https://huggingface.co/papers/2509.22518",
            "abstract": "The Reasoning Manifold framework quantifies and localizes reasoning failures in Large Language Models by analyzing geometric deviations in internal representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding how Large Language Models (LLMs) perform complex reasoning and their failure mechanisms is a challenge in interpretability research. To provide a measurable geometric analysis perspective, we define the concept of the Reasoning Manifold, a latent low-dimensional geometric structure formed by the internal representations corresponding to all correctly reasoned generations. This structure can be conceptualized as the embodiment of the effective thinking paths that the model has learned to successfully solve a given task. Based on this concept, we build REMA, a framework that explains the origins of failures by quantitatively comparing the spatial relationships of internal model representations corresponding to both erroneous and correct reasoning samples. Specifically, REMA first quantifies the geometric deviation of each erroneous representation by calculating its k-nearest neighbors distance to the approximated manifold formed by correct representations, thereby providing a unified failure signal. It then localizes the divergence points where these deviations first become significant by tracking this deviation metric across the model's layers and comparing it against a baseline of internal fluctuations from correct representations, thus identifying where the reasoning chain begins to go off-track. Our extensive experiments on diverse language and multimodal models and tasks demonstrate the low-dimensional nature of the reasoning manifold and the high separability between erroneous and correct reasoning representations. The results also validate the effectiveness of the REMA framework in analyzing the origins of reasoning failures. This research connects abstract reasoning failures to measurable geometric deviations in representations, providing new avenues for in-depth understanding and diagnosis of the internal computational processes of black-box models.",
            "score": 2,
            "issue_id": 6154,
            "pub_date": "2025-09-26",
            "pub_date_card": {
                "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 26",
                "zh": "9æœˆ26æ—¥"
            },
            "hash": "22e35d1e36530f86",
            "authors": [
                "Bo Li",
                "Guanzhi Deng",
                "Ronghao Chen",
                "Junrong Yue",
                "Shuo Zhang",
                "Qinghua Zhao",
                "Linqi Song",
                "Lijie Wen"
            ],
            "affiliations": [
                "Baidu Inc.",
                "Beihang University",
                "Beijing University of Posts and Telecommunications",
                "City University of Hong Kong",
                "Peking University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.22518.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#data",
                    "#interpretability",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: ĞºĞ°Ğº Ğ½Ğ°Ğ¹Ñ‚Ğ¸ ÑĞ±Ğ¾Ğ¸ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ˜Ğ˜",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Reasoning Manifold Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ…. ĞĞ½Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹' ĞºĞ°Ğº Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº REMA ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑĞ»Ğ¾ÑĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² LLM."
                },
                "en": {
                    "title": "Unveiling Reasoning Failures through Geometric Analysis",
                    "desc": "The paper introduces the Reasoning Manifold framework, which helps to identify and measure reasoning failures in Large Language Models (LLMs) by examining geometric changes in their internal representations. It defines a low-dimensional geometric structure, called the Reasoning Manifold, that represents successful reasoning paths learned by the model. The framework, REMA, quantifies how far erroneous representations deviate from this manifold by analyzing their spatial relationships with correct representations. Through experiments, the study shows that these geometric deviations can effectively indicate where reasoning errors occur, enhancing our understanding of LLMs' internal processes."
                },
                "zh": {
                    "title": "æ¨ç†æµå½¢ï¼šæ­ç¤ºè¯­è¨€æ¨¡å‹æ¨ç†å¤±è´¥çš„å‡ ä½•åˆ†æ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºæ¨ç†æµå½¢çš„æ¡†æ¶ï¼Œç”¨äºé‡åŒ–å’Œå®šä½å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†å¤±è´¥ã€‚é€šè¿‡åˆ†æå†…éƒ¨è¡¨ç¤ºçš„å‡ ä½•åå·®ï¼Œç ”ç©¶è€…èƒ½å¤Ÿç†è§£æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä¸­çš„è¡¨ç°åŠå…¶å¤±è´¥æœºåˆ¶ã€‚æ¨ç†æµå½¢æ˜¯ç”±æ­£ç¡®æ¨ç†ç”Ÿæˆçš„å†…éƒ¨è¡¨ç¤ºå½¢æˆçš„ä½ç»´å‡ ä½•ç»“æ„ï¼Œä»£è¡¨äº†æ¨¡å‹æˆåŠŸè§£å†³ä»»åŠ¡çš„æœ‰æ•ˆæ€ç»´è·¯å¾„ã€‚REMAæ¡†æ¶é€šè¿‡æ¯”è¾ƒé”™è¯¯å’Œæ­£ç¡®æ¨ç†æ ·æœ¬çš„ç©ºé—´å…³ç³»ï¼Œå®šé‡åˆ†ææ¨ç†å¤±è´¥çš„æ¥æºï¼Œå¸®åŠ©æˆ‘ä»¬æ›´æ·±å…¥åœ°ç†è§£é»‘ç®±æ¨¡å‹çš„å†…éƒ¨è®¡ç®—è¿‡ç¨‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24908",
            "title": "BOE-XSUM: Extreme Summarization in Clear Language of Spanish Legal\n  Decrees and Notifications",
            "url": "https://huggingface.co/papers/2509.24908",
            "abstract": "BOE-XSUM, a dataset of Spanish legal document summaries, demonstrates that fine-tuned medium-sized LLMs outperform general-purpose models in zero-shot summarization tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability to summarize long documents succinctly is increasingly important in daily life due to information overload, yet there is a notable lack of such summaries for Spanish documents in general, and in the legal domain in particular. In this work, we present BOE-XSUM, a curated dataset comprising 3,648 concise, plain-language summaries of documents sourced from Spain's ``Bolet\\'{\\i}n Oficial del Estado'' (BOE), the State Official Gazette. Each entry in the dataset includes a short summary, the original text, and its document type label. We evaluate the performance of medium-sized large language models (LLMs) fine-tuned on BOE-XSUM, comparing them to general-purpose generative models in a zero-shot setting. Results show that fine-tuned models significantly outperform their non-specialized counterparts. Notably, the best-performing model -- BERTIN GPT-J 6B (32-bit precision) -- achieves a 24\\% performance gain over the top zero-shot model, DeepSeek-R1 (accuracies of 41.6\\% vs.\\ 33.5\\%).",
            "score": 1,
            "issue_id": 6157,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "d3de02497f9aca8f",
            "authors": [
                "AndrÃ©s FernÃ¡ndez GarcÃ­a",
                "Javier de la Rosa",
                "Julio Gonzalo",
                "Roser Morante",
                "Enrique AmigÃ³",
                "Alejandro Benito-Santos",
                "Jorge Carrillo-de-Albornoz",
                "VÃ­ctor Fresno",
                "Adrian Ghajari",
                "Guillermo Marco",
                "Laura Plaza",
                "Eva SÃ¡nchez Salido"
            ],
            "affiliations": [
                "The National Library of Norway, Norway",
                "Universidad Nacional de Educacion Distancia, Spain"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24908.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#low_resource",
                    "#data",
                    "#machine_translation",
                    "#multilingual"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ BOE-XSUM - Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 3,648 ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ¸ÑĞ¿Ğ°Ğ½ÑĞºĞ¸Ñ… ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞ»Ğ»ĞµÑ‚ĞµĞ½Ñ Ğ˜ÑĞ¿Ğ°Ğ½Ğ¸Ğ¸. ĞĞ½Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñƒ LLM, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ, Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸. Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ BERTIN GPT-J 6B Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ½Ğ° 24% Ğ»ÑƒÑ‡ÑˆÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ zero-shot Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ."
                },
                "en": {
                    "title": "Fine-Tuned LLMs Excel in Spanish Legal Summarization",
                    "desc": "The paper introduces BOE-XSUM, a dataset specifically designed for summarizing Spanish legal documents. It contains 3,648 summaries that help address the lack of concise legal summaries in Spanish. The study shows that medium-sized large language models (LLMs) that are fine-tuned on this dataset perform better in zero-shot summarization tasks compared to general-purpose models. The results indicate a significant performance improvement, with the best model achieving a 24% higher accuracy than the leading zero-shot model."
                },
                "zh": {
                    "title": "å¾®è°ƒæ¨¡å‹åœ¨æ³•å¾‹æ‘˜è¦ä¸­çš„ä¼˜åŠ¿",
                    "desc": "BOE-XSUMæ˜¯ä¸€ä¸ªè¥¿ç­ç‰™æ³•å¾‹æ–‡ä»¶æ‘˜è¦çš„æ•°æ®é›†ï¼Œå±•ç¤ºäº†ç»è¿‡å¾®è°ƒçš„ä¸­å‹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬æ‘˜è¦ä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚éšç€ä¿¡æ¯è¿‡è½½çš„åŠ å‰§ï¼Œç®€æ´åœ°æ€»ç»“é•¿æ–‡æ¡£çš„èƒ½åŠ›å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œä½†è¥¿ç­ç‰™æ–‡æ¡£ï¼Œå°¤å…¶æ˜¯æ³•å¾‹é¢†åŸŸçš„æ‘˜è¦ä»ç„¶ç¨€ç¼ºã€‚è¯¥æ•°æ®é›†åŒ…å«3,648ä¸ªæ¥è‡ªè¥¿ç­ç‰™ã€Šå®˜æ–¹å…¬æŠ¥ã€‹çš„ç®€æ˜æ‘˜è¦ï¼Œæ¯ä¸ªæ¡ç›®åŒ…æ‹¬çŸ­æ‘˜è¦ã€åŸå§‹æ–‡æœ¬åŠå…¶æ–‡æ¡£ç±»å‹æ ‡ç­¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡BOE-XSUMå¾®è°ƒçš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºé€šç”¨ç”Ÿæˆæ¨¡å‹ï¼Œæœ€ä½³æ¨¡å‹BERTIN GPT-J 6Bçš„è¡¨ç°æ¯”é¡¶çº§é›¶æ ·æœ¬æ¨¡å‹DeepSeek-R1é«˜å‡º24%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24709",
            "title": "IWR-Bench: Can LVLMs reconstruct interactive webpage from a user\n  interaction video?",
            "url": "https://huggingface.co/papers/2509.24709",
            "abstract": "IWR-Bench evaluates Large Vision-Language Models in reconstructing interactive webpages from video, highlighting challenges in multi-modal reasoning and code generation.  \t\t\t\t\tAI-generated summary \t\t\t\t The webpage-to-code task requires models to understand visual representations of webpages and generate corresponding code. However, existing benchmarks primarily focus on static screenshot-to-code tasks, thereby overlooking the dynamic interactions fundamental to real-world web applications. To address this limitation, this paper introduces IWR-Bench, a novel benchmark for evaluating the capabilities of Large Vision-Language Models (LVLMs) in interactive webpage reconstruction from video. IWR-Bench comprises 113 meticulously curated tasks from 100 real-world websites, with 1,001 actions and featuring diverse interaction complexities (e.g., web games), visual styles, and domains. Aligning with standard web development practices, each task includes not only user interaction videos but also all crawled static assets (e.g., images, videos). This benchmark evaluates models on two fundamental challenges: comprehensive multi-modal reasoning to infer interaction logic from video and assets, and advanced code generation to translate this logic into functional code. An agent-as-a-judge framework with a comprehensive metric system automatically assesses the functional correctness and visual fidelity of generated webpages. Extensive experiments on 28 LVLMs reveal a significant challenge: the best model achieves an overall score of only 36.35%, as functional correctness (24.39% IFS) lags significantly behind visual fidelity (64.25% VFS). These results highlight critical limitations in current models' ability to reason about temporal dynamics and synthesize event-driven logic, establishing IWR-Bench as a challenging frontier for vision-language research. The benchmark and evaluation code will be made publicly available. Code is available at https://github.com/L-O-I/IWR-Bench.",
            "score": 1,
            "issue_id": 6154,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "66300a4a6e51add9",
            "authors": [
                "Yang Chen",
                "Minghao Liu",
                "Yufan Shen",
                "Yunwen Li",
                "Tianyuan Huang",
                "Xinyu Fang",
                "Tianyu Zheng",
                "Wenxuan Huang",
                "Cheng Yang",
                "Daocheng Fu",
                "Jianbiao Mei",
                "Rong Wu",
                "Licheng Wen",
                "Xuemeng Yang",
                "Song Mao",
                "Qunshu Lin",
                "Zhi Yu",
                "Yongliang Shen",
                "Yu Qiao",
                "Botian Shi"
            ],
            "affiliations": [
                "Bai et al.",
                "Caron et al.",
                "Comanici et al.",
                "Deka et al.",
                "Gui et al.",
                "Gupta & Kembhavi",
                "IWR-Bench Team",
                "Jiang et al.",
                "Jimenez et al.",
                "Laurencon et al.",
                "Lee et al.",
                "Li et al.",
                "Luo et al.",
                "OpenAI",
                "Radford et al.",
                "Si et al.",
                "Song et al.",
                "Xiao et al.",
                "Yun et al.",
                "Zhang et al.",
                "Zhuge et al."
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24709.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#benchmark",
                    "#reasoning",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº ĞºĞ¾Ğ´Ñƒ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ AI Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ IWR-Bench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 113 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ°Ğ¹Ñ‚Ğ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ñ… 1001 Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†. Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ»Ğ¸ÑˆÑŒ 36.35% Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹."
                },
                "en": {
                    "title": "IWR-Bench: Advancing Interactive Webpage Reconstruction with LVLMs",
                    "desc": "The paper introduces IWR-Bench, a new benchmark designed to evaluate Large Vision-Language Models (LVLMs) on their ability to reconstruct interactive webpages from video inputs. Unlike previous benchmarks that focus on static screenshots, IWR-Bench emphasizes the dynamic interactions that are essential for real-world web applications. It includes 113 tasks from 100 websites, featuring a variety of interaction complexities and visual styles, and assesses models on their multi-modal reasoning and code generation capabilities. The findings reveal that current models struggle significantly with functional correctness, achieving only 24.39% in this area, while visual fidelity is comparatively higher at 64.25%, indicating a need for improved reasoning about temporal dynamics and event-driven logic."
                },
                "zh": {
                    "title": "IWR-Benchï¼šé‡å»ºäº¤äº’ç½‘é¡µçš„æ–°æŒ‘æˆ˜",
                    "desc": "IWR-Benchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ä»è§†é¢‘é‡å»ºäº¤äº’ç½‘é¡µæ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«æ¥è‡ª100ä¸ªçœŸå®ç½‘ç«™çš„113ä¸ªç²¾å¿ƒç­–åˆ’çš„ä»»åŠ¡ï¼Œæ¶µç›–äº†å¤šç§äº¤äº’å¤æ‚æ€§å’Œè§†è§‰é£æ ¼ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨ç†è§£è§†é¢‘ä¸­çš„äº¤äº’é€»è¾‘å’Œç”ŸæˆåŠŸèƒ½ä»£ç æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ï¼Œæœ€ä½³æ¨¡å‹çš„æ•´ä½“å¾—åˆ†ä»…ä¸º36.35%ã€‚è¿™è¡¨æ˜å½“å‰æ¨¡å‹åœ¨å¤„ç†æ—¶é—´åŠ¨æ€å’Œåˆæˆäº‹ä»¶é©±åŠ¨é€»è¾‘æ–¹é¢å­˜åœ¨å…³é”®é™åˆ¶ï¼ŒIWR-Benchä¸ºè§†è§‰-è¯­è¨€ç ”ç©¶è®¾å®šäº†æ–°çš„æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24592",
            "title": "BPMN Assistant: An LLM-Based Approach to Business Process Modeling",
            "url": "https://huggingface.co/papers/2509.24592",
            "abstract": "BPMN Assistant uses Large Language Models to create and edit BPMN diagrams, evaluating process generation and editing performance using JSON and XML representations.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents BPMN Assistant, a tool that leverages Large Language Models (LLMs) for natural language-based creation and editing of BPMN diagrams. A specialized JSON-based representation is introduced as a structured alternative to the direct handling of XML to enhance the accuracy of process modifications. Process generation quality is evaluated using Graph Edit Distance (GED) and Relative Graph Edit Distance (RGED), while editing performance is evaluated with a binary success metric. Results show that JSON and XML achieve similar similarity scores in generation, but JSON offers greater reliability, faster processing, and significantly higher editing success rates. We discuss key trade-offs, limitations, and future improvements. The implementation is available at https://github.com/jtlicardo/bpmn-assistant.",
            "score": 1,
            "issue_id": 6158,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "b8a8d5cee79d47fb",
            "authors": [
                "Josip Tomo Licardo",
                "Nikola Tankovic",
                "Darko Etinger"
            ],
            "affiliations": [
                "Faculty of Informatics Juraj Dobrila University of Pula ZagrebaÄka 30 52100 Pula, Croatia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24592.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#multimodal",
                    "#graphs",
                    "#optimization",
                    "#benchmark",
                    "#games"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "LLM Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ BPMN Assistant â€” Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ LLM Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ BPMN-Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ JSON-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ XML Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Graph Edit Distance (GED) Ğ¸ Relative Graph Edit Distance (RGED), Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ â€” Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¾Ğ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ JSON Ğ¸ XML Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑÑ…Ğ¾Ğ¶Ğ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğ¾ JSON Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing BPMN with JSON and LLMs",
                    "desc": "This paper introduces BPMN Assistant, a tool that utilizes Large Language Models (LLMs) to facilitate the creation and editing of BPMN diagrams through natural language input. It presents a JSON-based representation as a more structured alternative to XML, aiming to improve the accuracy of process modifications. The evaluation of process generation quality is conducted using metrics like Graph Edit Distance (GED) and Relative Graph Edit Distance (RGED), while editing performance is assessed with a binary success metric. The findings indicate that while JSON and XML yield similar generation scores, JSON demonstrates enhanced reliability, quicker processing times, and significantly improved editing success rates."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå‡BPMNå›¾è¡¨ç¼–è¾‘æ•ˆç‡",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†BPMN Assistantï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡ŒBPMNå›¾è¡¨åˆ›å»ºå’Œç¼–è¾‘çš„å·¥å…·ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºJSONçš„ä¸“ç”¨è¡¨ç¤ºï¼Œä½œä¸ºå¤„ç†XMLçš„ç»“æ„åŒ–æ›¿ä»£æ–¹æ¡ˆï¼Œä»¥æé«˜è¿‡ç¨‹ä¿®æ”¹çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å›¾ç¼–è¾‘è·ç¦»ï¼ˆGEDï¼‰å’Œç›¸å¯¹å›¾ç¼–è¾‘è·ç¦»ï¼ˆRGEDï¼‰è¯„ä¼°ç”Ÿæˆè´¨é‡ï¼Œè€Œç¼–è¾‘æ€§èƒ½åˆ™ä½¿ç”¨äºŒå…ƒæˆåŠŸæŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒJSONå’ŒXMLåœ¨ç”Ÿæˆæ—¶çš„ç›¸ä¼¼æ€§å¾—åˆ†ç›¸ä¼¼ï¼Œä½†JSONæä¾›äº†æ›´é«˜çš„å¯é æ€§ã€æ›´å¿«çš„å¤„ç†é€Ÿåº¦å’Œæ˜¾è‘—æ›´é«˜çš„ç¼–è¾‘æˆåŠŸç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.24200",
            "title": "UniVid: The Open-Source Unified Video Model",
            "url": "https://huggingface.co/papers/2509.24200",
            "abstract": "UniVid combines video understanding and generation using an MLLM with a diffusion decoder, achieving state-of-the-art performance through Temperature Modality Alignment and Pyramid Reflection.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified video modeling that combines generation and understanding capabilities is increasingly important but faces two key challenges: maintaining semantic faithfulness during flow-based generation due to text-visual token imbalance and the limitations of uniform cross-modal attention across the flow trajectory, and efficiently extending image-centric MLLMs to video without costly retraining. We present UniVid, a unified architecture that couples an MLLM with a diffusion decoder through a lightweight adapter, enabling both video understanding and generation. We introduce Temperature Modality Alignment to improve prompt adherence and Pyramid Reflection for efficient temporal reasoning via dynamic keyframe selection. Extensive experiments on standard benchmarks demonstrate state-of-the-art performance, achieving a 2.2% improvement on VBench-Long total score compared to EasyAnimateV5.1, and 1.0% and 3.3% accuracy gains on MSVD-QA and ActivityNet-QA, respectively, compared with the best prior 7B baselines.",
            "score": 1,
            "issue_id": 6160,
            "pub_date": "2025-09-29",
            "pub_date_card": {
                "ru": "29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 29",
                "zh": "9æœˆ29æ—¥"
            },
            "hash": "874ca55ccdd8f888",
            "authors": [
                "Jiabin Luo",
                "Junhui Lin",
                "Zeyu Zhang",
                "Biao Wu",
                "Meng Fang",
                "Ling Chen",
                "Hao Tang"
            ],
            "affiliations": [
                "AI Geeks",
                "Australian Artificial Intelligence Institute",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.24200.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#video",
                    "#diffusion",
                    "#architecture",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "UniVid Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ MLLM (Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ) Ñ diffusion decoder Ñ‡ĞµÑ€ĞµĞ· Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Temperature Modality Alignment. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Pyramid Reflection Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 2.2% Ğ¿Ğ¾ VBench-Long Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ EasyAnimateV5.1 Ğ¸ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 1.0% Ğ¸ 3.3% Ğ½Ğ° MSVD-QA Ğ¸ ActivityNet-QA ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾."
                },
                "en": {
                    "title": "UniVid: Bridging Video Understanding and Generation with State-of-the-Art Performance",
                    "desc": "UniVid is a novel architecture that integrates video understanding and generation by combining a Multi-Modal Language Model (MLLM) with a diffusion decoder. It addresses challenges like semantic consistency during generation and the inefficiencies of cross-modal attention in video processing. The introduction of Temperature Modality Alignment enhances the model's ability to follow prompts accurately, while Pyramid Reflection allows for better temporal reasoning through dynamic keyframe selection. Extensive testing shows that UniVid outperforms previous models, achieving significant accuracy improvements on various benchmarks."
                },
                "zh": {
                    "title": "è§†é¢‘ç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€è§£å†³æ–¹æ¡ˆ",
                    "desc": "UniVidæ˜¯ä¸€ç§ç»“åˆè§†é¢‘ç†è§£å’Œç”Ÿæˆçš„ç»Ÿä¸€æ¶æ„ï¼Œä½¿ç”¨äº†å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å’Œæ‰©æ•£è§£ç å™¨ã€‚å®ƒé€šè¿‡è½»é‡çº§é€‚é…å™¨è¿æ¥è¿™ä¸¤è€…ï¼Œè§£å†³äº†æ–‡æœ¬ä¸è§†è§‰æ ‡è®°ä¸å¹³è¡¡çš„é—®é¢˜ã€‚è®ºæ–‡ä¸­æå‡ºçš„æ¸©åº¦æ¨¡æ€å¯¹é½æŠ€æœ¯æé«˜äº†æç¤ºçš„éµå¾ªæ€§ï¼Œè€Œé‡‘å­—å¡”åå°„åˆ™é€šè¿‡åŠ¨æ€å…³é”®å¸§é€‰æ‹©å®ç°äº†é«˜æ•ˆçš„æ—¶é—´æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniVidåœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23233",
            "title": "Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large\n  Language Models",
            "url": "https://huggingface.co/papers/2509.23233",
            "abstract": "CLAIRE, an agentic system combining LLM reasoning and retrieval, improves Wikipedia accuracy by detecting inconsistencies, with human editors reporting higher confidence and identifying more issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Wikipedia is the largest open knowledge corpus, widely used worldwide and serving as a key resource for training large language models (LLMs) and retrieval-augmented generation (RAG) systems. Ensuring its accuracy is therefore critical. But how accurate is Wikipedia, and how can we improve it?   We focus on inconsistencies, a specific type of factual inaccuracy, and introduce the task of corpus-level inconsistency detection. We present CLAIRE, an agentic system that combines LLM reasoning with retrieval to surface potentially inconsistent claims along with contextual evidence for human review. In a user study with experienced Wikipedia editors, 87.5% reported higher confidence when using CLAIRE, and participants identified 64.7% more inconsistencies in the same amount of time.   Combining CLAIRE with human annotation, we contribute WIKICOLLIDE, the first benchmark of real Wikipedia inconsistencies. Using random sampling with CLAIRE-assisted analysis, we find that at least 3.3% of English Wikipedia facts contradict another fact, with inconsistencies propagating into 7.3% of FEVEROUS and 4.0% of AmbigQA examples. Benchmarking strong baselines on this dataset reveals substantial headroom: the best fully automated system achieves an AUROC of only 75.1%.   Our results show that contradictions are a measurable component of Wikipedia and that LLM-based systems like CLAIRE can provide a practical tool to help editors improve knowledge consistency at scale.",
            "score": 1,
            "issue_id": 6158,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "7034c5edf3a48b1b",
            "authors": [
                "Sina J. Semnani",
                "Jirayu Burapacheep",
                "Arpandeep Khatua",
                "Thanawan Atchariyachanvanit",
                "Zheng Wang",
                "Monica S. Lam"
            ],
            "affiliations": [
                "Computer Science Department Stanford University, Stanford, CA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23233.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#agents",
                    "#rag",
                    "#science",
                    "#reasoning",
                    "#alignment",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "AI-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ¹ Ğ² Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ CLAIRE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ¹ Ğ² Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸. Ğ’ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ 87,5% Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰Ğ¸Ğ»Ğ¸ Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ CLAIRE, Ğ° ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ½Ğ° 64,7% Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ·Ğ° Ñ‚Ğ¾ Ğ¶Ğµ Ğ²Ñ€ĞµĞ¼Ñ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ ĞºĞ°Ğº Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ 3,3% Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ² Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¹ Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ°Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ°Ğ¼, Ğ¸ ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ½Ğ° 7,3% Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ FEVEROUS. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ LLM-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°Ñ…."
                },
                "en": {
                    "title": "CLAIRE: Enhancing Wikipedia Accuracy with LLMs and Retrieval",
                    "desc": "This paper introduces CLAIRE, an agentic system that enhances the accuracy of Wikipedia by detecting inconsistencies in its content. CLAIRE combines large language model (LLM) reasoning with retrieval techniques to identify potentially inaccurate claims and provide contextual evidence for human editors. A user study showed that experienced editors using CLAIRE reported increased confidence and were able to find significantly more inconsistencies. The study also established WIKICOLLIDE, a benchmark for real Wikipedia inconsistencies, highlighting the prevalence of contradictions in the encyclopedia and demonstrating the potential of LLM-based systems to assist in improving knowledge consistency."
                },
                "zh": {
                    "title": "CLAIREï¼šæå‡ç»´åŸºç™¾ç§‘å‡†ç¡®æ€§çš„æ™ºèƒ½åŠ©æ‰‹",
                    "desc": "CLAIREæ˜¯ä¸€ä¸ªç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†å’Œæ£€ç´¢çš„æ™ºèƒ½ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜ç»´åŸºç™¾ç§‘çš„å‡†ç¡®æ€§ã€‚å®ƒé€šè¿‡æ£€æµ‹ä¸ä¸€è‡´æ€§ï¼Œå¸®åŠ©äººç±»ç¼–è¾‘è€…è¯†åˆ«æ›´å¤šé—®é¢˜ï¼Œå¹¶æå‡ä»–ä»¬çš„ä¿¡å¿ƒã€‚ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨CLAIREçš„ç¼–è¾‘è€…åœ¨ç›¸åŒæ—¶é—´å†…å‘ç°çš„ä¸ä¸€è‡´æ€§å¢åŠ äº†64.7%ã€‚æ­¤å¤–ï¼ŒCLAIREè¿˜å¸®åŠ©å»ºç«‹äº†WIKICOLLIDEï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªçœŸå®ç»´åŸºç™¾ç§‘ä¸ä¸€è‡´æ€§çš„åŸºå‡†æ•°æ®é›†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.23115",
            "title": "RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human\n  Mobility",
            "url": "https://huggingface.co/papers/2509.23115",
            "abstract": "RHYTHM uses hierarchical temporal tokenization and large language models to predict human mobility, capturing long-range dependencies and multi-scale periodic behaviors efficiently.  \t\t\t\t\tAI-generated summary \t\t\t\t Predicting human mobility is inherently challenging due to complex long-range dependencies and multi-scale periodic behaviors. To address this, we introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), a unified framework that leverages large language models (LLMs) as general-purpose spatio-temporal predictors and trajectory reasoners. Methodologically, RHYTHM employs temporal tokenization to partition each trajectory into daily segments and encode them as discrete tokens with hierarchical attention that captures both daily and weekly dependencies, thereby significantly reducing the sequence length while preserving cyclical information. Additionally, we enrich token representations by adding pre-computed prompt embeddings for trajectory segments and prediction targets via a frozen LLM, and feeding these combined embeddings back into the LLM backbone to capture complex interdependencies. Computationally, RHYTHM freezes the pretrained LLM's backbone to reduce attention complexity and memory cost. We evaluate our model against state-of-the-art methods using three real-world datasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a 5.0% increase on weekends, and a 24.6% reduction in training time. Code is publicly available at https://github.com/he-h/rhythm.",
            "score": 1,
            "issue_id": 6154,
            "pub_date": "2025-09-27",
            "pub_date_card": {
                "ru": "27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 27",
                "zh": "9æœˆ27æ—¥"
            },
            "hash": "c32db800b68d7fd5",
            "authors": [
                "Haoyu He",
                "Haozheng Luo",
                "Yan Chen",
                "Qi R. Wang"
            ],
            "affiliations": [
                "Northeastern University",
                "Northwestern University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.23115.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#data",
                    "#optimization",
                    "#benchmark",
                    "#open_source",
                    "#reasoning",
                    "#training",
                    "#long_context",
                    "#dataset"
                ],
                "emoji": "ğŸƒ",
                "ru": {
                    "title": "Ğ Ğ¸Ñ‚Ğ¼Ñ‹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ: Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ RHYTHM Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ½Ğ° ĞµĞ¶ĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… ĞºĞ°Ğº Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´Ğ½ĞµĞ¹ Ğ¸ Ğ½ĞµĞ´ĞµĞ»ÑŒ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ backbone LLM Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 2.4% Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 24.6% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "RHYTHM: Smart Predictions for Human Mobility",
                    "desc": "RHYTHM is a novel framework designed to predict human mobility by effectively managing complex long-range dependencies and periodic behaviors. It utilizes hierarchical temporal tokenization to break down mobility trajectories into daily segments, which are then encoded as discrete tokens, allowing the model to focus on both daily and weekly patterns. By incorporating large language models (LLMs) and enriching token representations with prompt embeddings, RHYTHM captures intricate interdependencies while minimizing computational costs. The model demonstrates significant improvements in accuracy and efficiency compared to existing methods, particularly during weekends, showcasing its effectiveness in real-world applications."
                },
                "zh": {
                    "title": "RHYTHMï¼šé«˜æ•ˆé¢„æµ‹äººç±»ç§»åŠ¨çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "RHYTHMæ˜¯ä¸€ç§ç”¨äºé¢„æµ‹äººç±»ç§»åŠ¨çš„æ¡†æ¶ï¼Œåˆ©ç”¨å±‚æ¬¡æ—¶é—´æ ‡è®°åŒ–å’Œå¤§å‹è¯­è¨€æ¨¡å‹æ¥æ•æ‰é•¿è·ç¦»ä¾èµ–å’Œå¤šå°ºåº¦å‘¨æœŸæ€§è¡Œä¸ºã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ¯ä¸ªè½¨è¿¹åˆ†å‰²ä¸ºæ—¥å¸¸ç‰‡æ®µï¼Œå¹¶ä½¿ç”¨å±‚æ¬¡æ³¨æ„åŠ›ç¼–ç è¿™äº›ç‰‡æ®µï¼Œä»è€Œæ˜¾è‘—å‡å°‘åºåˆ—é•¿åº¦ï¼ŒåŒæ—¶ä¿ç•™å‘¨æœŸæ€§ä¿¡æ¯ã€‚RHYTHMè¿˜é€šè¿‡æ·»åŠ é¢„è®¡ç®—çš„æç¤ºåµŒå…¥æ¥ä¸°å¯Œæ ‡è®°è¡¨ç¤ºï¼Œè¿›ä¸€æ­¥æ•æ‰å¤æ‚çš„ç›¸äº’ä¾èµ–å…³ç³»ã€‚ç»è¿‡è¯„ä¼°ï¼ŒRHYTHMåœ¨å‡†ç¡®æ€§å’Œè®­ç»ƒæ—¶é—´ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—çš„æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.19033",
            "title": "Charting a Decade of Computational Linguistics in Italy: The CLiC-it\n  Corpus",
            "url": "https://huggingface.co/papers/2509.19033",
            "abstract": "The study analyzes research trends in Italian Computational Linguistics and Natural Language Processing by compiling and examining the proceedings of the CLiC-it conference from 2014 to 2024.  \t\t\t\t\tAI-generated summary \t\t\t\t Over the past decade, Computational Linguistics (CL) and Natural Language Processing (NLP) have evolved rapidly, especially with the advent of Transformer-based Large Language Models (LLMs). This shift has transformed research goals and priorities, from Lexical and Semantic Resources to Language Modelling and Multimodality. In this study, we track the research trends of the Italian CL and NLP community through an analysis of the contributions to CLiC-it, arguably the leading Italian conference in the field. We compile the proceedings from the first 10 editions of the CLiC-it conference (from 2014 to 2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its metadata, including author provenance, gender, affiliations, and more, as well as the content of the papers themselves, which address various topics. Our goal is to provide the Italian and international research communities with valuable insights into emerging trends and key developments over time, supporting informed decisions and future directions in the field.",
            "score": 0,
            "issue_id": 6159,
            "pub_date": "2025-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "9c9b6802bca9c07c",
            "authors": [
                "Chiara Alzetta",
                "Serena Auriemma",
                "Alessandro Bondielli",
                "Luca Dini",
                "Chiara Fazzone",
                "Alessio Miaschi",
                "Martina Miliani",
                "Marta Sartor"
            ],
            "affiliations": [
                "CoLingLab, Department of Philology, Literature and Linguistics, University of Pisa",
                "Department of Computer Science, University of Pisa",
                "Istituto di Linguistica Computazionale Antonio Zampolli, CNR, Pisa",
                "University of Pisa"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.19033.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#data",
                    "#survey",
                    "#multilingual"
                ],
                "emoji": "ğŸ‡®ğŸ‡¹",
                "ru": {
                    "title": "Ğ”ĞµÑÑÑ‚Ğ¸Ğ»ĞµÑ‚Ğ¸Ğµ Ğ¸Ñ‚Ğ°Ğ»ÑŒÑĞ½ÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸ĞºĞ¸: Ğ¾Ñ‚ Ğ»ĞµĞºÑĞ¸ĞºĞ¸ Ğº LLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹ Ğ² Ğ¸Ñ‚Ğ°Ğ»ÑŒÑĞ½ÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸ĞºĞµ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ·Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 10 Ğ»ĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ CLiC-it Ñ 2014 Ğ¿Ğ¾ 2024 Ğ³Ğ¾Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑ CLiC-it, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²ÑĞµ Ğ´Ğ¾ĞºĞ»Ğ°Ğ´Ñ‹ ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸, Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ĞºĞ°Ğº Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹, Ğ³ĞµĞ½Ğ´ĞµÑ€, Ğ°Ñ„Ñ„Ğ¸Ğ»Ğ¸Ğ°Ñ†Ğ¸Ğ¸), Ñ‚Ğ°Ğº Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Transformer-Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ¸ LLM. Ğ¦ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ - Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¸Ñ‚Ğ°Ğ»ÑŒÑĞ½ÑĞºĞ¾Ğ¼Ñƒ Ğ¸ Ğ¼ĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ñ€ĞµĞ½Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¾ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Tracking Trends in Italian NLP: Insights from CLiC-it",
                    "desc": "This study investigates the evolution of research in Italian Computational Linguistics (CL) and Natural Language Processing (NLP) by analyzing the CLiC-it conference proceedings from 2014 to 2024. It highlights the impact of Transformer-based Large Language Models (LLMs) on shifting research priorities towards Language Modelling and Multimodality. The authors compile the CLiC-it Corpus, which includes metadata on authors and the content of the papers, to provide a detailed overview of the community's contributions. The findings aim to offer insights into emerging trends and developments, guiding future research directions in the field."
                },
                "zh": {
                    "title": "æ­ç¤ºæ„å¤§åˆ©è®¡ç®—è¯­è¨€å­¦ä¸è‡ªç„¶è¯­è¨€å¤„ç†çš„ç ”ç©¶è¶‹åŠ¿",
                    "desc": "æœ¬ç ”ç©¶åˆ†æäº†æ„å¤§åˆ©è®¡ç®—è¯­è¨€å­¦å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„ç ”ç©¶è¶‹åŠ¿ï¼Œé‡ç‚¹å…³æ³¨2014å¹´è‡³2024å¹´CLiC-itä¼šè®®çš„è®ºæ–‡é›†ã€‚éšç€åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ºç°ï¼Œè®¡ç®—è¯­è¨€å­¦å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ç ”ç©¶ç›®æ ‡å’Œä¼˜å…ˆäº‹é¡¹å‘ç”Ÿäº†æ˜¾è‘—å˜åŒ–ã€‚æˆ‘ä»¬é€šè¿‡ç¼–åˆ¶CLiC-itè¯­æ–™åº“ï¼Œåˆ†æäº†ä¼šè®®çš„å‰åå±Šè®ºæ–‡çš„å…ƒæ•°æ®å’Œå†…å®¹ï¼Œæ¶µç›–äº†ä½œè€…èƒŒæ™¯ã€æ€§åˆ«ã€æœºæ„ç­‰ä¿¡æ¯ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸ºæ„å¤§åˆ©åŠå›½é™…ç ”ç©¶ç¤¾åŒºæä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œå¸®åŠ©ä»–ä»¬äº†è§£æ–°å…´è¶‹åŠ¿å’Œå…³é”®å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.16538",
            "title": "Advancing Reference-free Evaluation of Video Captions with Factual\n  Analysis",
            "url": "https://huggingface.co/papers/2509.16538",
            "abstract": "VC-Inspector, a reference-free and factually grounded caption quality evaluator, uses large language models to generate pseudo captions and train a multimodal model, demonstrating superior performance in evaluating video captions across diverse domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Video captions offer concise snapshots of actors, objects, and actions within a video, serving as valuable assets for applications such as question answering and event localization. However, acquiring human annotations for video captions is costly or even impractical, especially when dealing with diverse video domains. Existing models trained on supervised datasets face challenges in evaluating performance across different domains due to the reliance on reference-based evaluation protocols, which necessitate ground truth captions. This assumption is unrealistic for evaluating videos in the wild. To address these limitations, we propose a reference-free evaluation framework that does not require ground truth captions, focusing on factual grounding to ensure accurate assessment of caption quality. We introduce VC-Inspector, a novel caption quality evaluator that is both reference-free and factually grounded. Utilizing large language models, we generate pseudo captions of varying quality based on supervised data, which are subsequently used to train a multimodal model (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior alignment with human judgments on the VATEX-Eval dataset, outperforming existing methods. The performance also generalizes to image caption datasets, Flickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos. Overall, VC-Inspector offers a scalable and generalizable solution for evaluating the factual accuracy of video captions, paving the way for more effective and objective assessment methodologies in diverse video domains.",
            "score": 0,
            "issue_id": 6156,
            "pub_date": "2025-09-20",
            "pub_date_card": {
                "ru": "20 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 20",
                "zh": "9æœˆ20æ—¥"
            },
            "hash": "9a484f605f3326dc",
            "authors": [
                "Shubhashis Roy Dipta",
                "Tz-Ying Wu",
                "Subarna Tripathi"
            ],
            "affiliations": [
                "Intel Labs",
                "University of Maryland, Baltimore County"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.16538.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#video",
                    "#multimodal",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ†ĞµĞ½ĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ ÑÑƒĞ±Ñ‚Ğ¸Ñ‚Ñ€Ğ¾Ğ² Ğ±ĞµĞ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ VC-Inspector - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ ÑÑƒĞ±Ñ‚Ğ¸Ñ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ VATEX-Eval Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… ĞºĞ°Ğº Ğ¾Ğ´Ğ½Ğ¾-ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Revolutionizing Video Caption Evaluation with VC-Inspector",
                    "desc": "VC-Inspector is a novel tool designed to evaluate the quality of video captions without needing reference captions. It uses large language models to create pseudo captions, which helps train a multimodal model for better assessment. This approach addresses the challenges of traditional methods that rely on human-annotated ground truth, making it more practical for diverse video content. The results show that VC-Inspector aligns well with human evaluations and performs effectively across various datasets, enhancing the evaluation of video captions."
                },
                "zh": {
                    "title": "æ— å‚è€ƒçš„å­—å¹•è´¨é‡è¯„ä¼°æ–°æ–¹æ³•",
                    "desc": "VC-Inspectoræ˜¯ä¸€ç§æ— å‚è€ƒä¸”åŸºäºäº‹å®çš„å­—å¹•è´¨é‡è¯„ä¼°å·¥å…·ï¼Œæ—¨åœ¨è§£å†³è§†é¢‘å­—å¹•è¯„ä¼°ä¸­çš„æŒ‘æˆ˜ã€‚å®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¼ªå­—å¹•ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹ï¼Œä»è€Œåœ¨ä¸åŒé¢†åŸŸçš„è§†é¢‘å­—å¹•è¯„ä¼°ä¸­è¡¨ç°å‡ºè‰²ã€‚ä¸ä¼ ç»Ÿä¾èµ–å‚è€ƒçš„è¯„ä¼°æ–¹æ³•ä¸åŒï¼ŒVC-Inspectorä¸éœ€è¦çœŸå®çš„å­—å¹•ï¼Œä¸“æ³¨äºç¡®ä¿å­—å¹•è´¨é‡çš„å‡†ç¡®è¯„ä¼°ã€‚è¯¥æ–¹æ³•åœ¨VATEX-Evalæ•°æ®é›†ä¸Šä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ï¼Œå¹¶ä¸”åœ¨å›¾åƒå­—å¹•æ•°æ®é›†ä¸Šä¹Ÿè¡¨ç°è‰¯å¥½ï¼Œå±•ç¤ºäº†å…¶å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-29.html",
    "link_next": "2025-10-01.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "29.09",
        "en": "09/29",
        "zh": "9æœˆ29æ—¥"
    },
    "short_date_next": {
        "ru": "01.10",
        "en": "10/01",
        "zh": "10æœˆ1æ—¥"
    },
    "categories": {
        "#dataset": 14,
        "#data": 14,
        "#benchmark": 28,
        "#agents": 9,
        "#cv": 8,
        "#rl": 18,
        "#rlhf": 9,
        "#rag": 1,
        "#plp": 0,
        "#inference": 5,
        "#3d": 2,
        "#audio": 1,
        "#video": 7,
        "#multimodal": 17,
        "#math": 3,
        "#multilingual": 2,
        "#architecture": 14,
        "#healthcare": 0,
        "#training": 34,
        "#robotics": 0,
        "#agi": 3,
        "#games": 7,
        "#interpretability": 10,
        "#reasoning": 27,
        "#transfer_learning": 5,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 32,
        "#survey": 4,
        "#diffusion": 8,
        "#alignment": 9,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 6,
        "#synthetic": 5,
        "#machine_translation": 1,
        "#leakage": 1,
        "#open_source": 11,
        "#small_models": 3,
        "#science": 5,
        "#low_resource": 1,
        "#personalization": 1
    }
}