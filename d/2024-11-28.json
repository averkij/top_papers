{
    "date": {
        "ru": "28 ноября",
        "en": "November 28",
        "zh": "11月28日"
    },
    "time_utc": "2024-11-28 04:13",
    "weekday": 3,
    "issue_id": 829,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.17949",
            "title": "ROICtrl: Boosting Instance Control for Visual Generation",
            "url": "https://huggingface.co/papers/2411.17949",
            "abstract": "Natural language often struggles to accurately associate positional and attribute information with multiple instances, which limits current text-based visual generation models to simpler compositions featuring only a few dominant instances. To address this limitation, this work enhances diffusion models by introducing regional instance control, where each instance is governed by a bounding box paired with a free-form caption. Previous methods in this area typically rely on implicit position encoding or explicit attention masks to separate regions of interest (ROIs), resulting in either inaccurate coordinate injection or large computational overhead. Inspired by ROI-Align in object detection, we introduce a complementary operation called ROI-Unpool. Together, ROI-Align and ROI-Unpool enable explicit, efficient, and accurate ROI manipulation on high-resolution feature maps for visual generation. Building on ROI-Unpool, we propose ROICtrl, an adapter for pretrained diffusion models that enables precise regional instance control. ROICtrl is compatible with community-finetuned diffusion models, as well as with existing spatial-based add-ons (\\eg, ControlNet, T2I-Adapter) and embedding-based add-ons (\\eg, IP-Adapter, ED-LoRA), extending their applications to multi-instance generation. Experiments show that ROICtrl achieves superior performance in regional instance control while significantly reducing computational costs.",
            "score": 38,
            "issue_id": 827,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "55ffbda778f2f640",
            "authors": [
                "Yuchao Gu",
                "Yipin Zhou",
                "Yunfan Ye",
                "Yixin Nie",
                "Licheng Yu",
                "Pingchuan Ma",
                "Kevin Qinghong Lin",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "GenAI, Meta",
                "MIT",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17949.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Точный контроль над областями в генеративных моделях изображений",
                    "desc": "Статья представляет новый метод улучшения диффузионных моделей для генерации изображений - ROICtrl. Он позволяет точно контролировать отдельные области изображения с помощью ограничивающих рамок и подписей. ROICtrl использует операции ROI-Align и ROI-Unpool для эффективной работы с высокоразрешающими картами признаков. Метод совместим с существующими дополнениями к диффузионным моделям и значительно снижает вычислительные затраты при генерации сложных многообъектных сцен."
                },
                "en": {
                    "title": "Enhancing Visual Generation with Precise Regional Control",
                    "desc": "This paper presents a new method to improve text-based visual generation models by allowing better control over multiple instances in images. It introduces a technique called ROICtrl, which uses bounding boxes and captions to manage regions of interest (ROIs) more effectively. The method combines ROI-Align and a new operation called ROI-Unpool to manipulate high-resolution feature maps accurately and efficiently. Experiments demonstrate that ROICtrl enhances performance in generating images with multiple instances while lowering computational costs."
                },
                "zh": {
                    "title": "区域实例控制，提升视觉生成精度",
                    "desc": "这篇论文提出了一种改进扩散模型的方法，通过引入区域实例控制来解决自然语言在多实例位置和属性信息关联上的不足。每个实例由一个边界框和一个自由形式的描述配对，从而实现更精确的控制。论文中介绍的ROI-Unpool操作与ROI-Align相结合，使得在高分辨率特征图上进行区域操作变得高效且准确。实验结果表明，ROICtrl在区域实例控制方面表现优越，同时显著降低了计算成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17188",
            "title": "Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment",
            "url": "https://huggingface.co/papers/2411.17188",
            "abstract": "Many real-world user queries (e.g. \"How do to make egg fried rice?\") could benefit from systems capable of generating responses with both textual steps with accompanying images, similar to a cookbook. Models designed to generate interleaved text and images face challenges in ensuring consistency within and across these modalities. To address these challenges, we present ISG, a comprehensive evaluation framework for interleaved text-and-image generation. ISG leverages a scene graph structure to capture relationships between text and image blocks, evaluating responses on four levels of granularity: holistic, structural, block-level, and image-specific. This multi-tiered evaluation allows for a nuanced assessment of consistency, coherence, and accuracy, and provides interpretable question-answer feedback. In conjunction with ISG, we introduce a benchmark, ISG-Bench, encompassing 1,150 samples across 8 categories and 21 subcategories. This benchmark dataset includes complex language-vision dependencies and golden answers to evaluate models effectively on vision-centric tasks such as style transfer, a challenging area for current models. Using ISG-Bench, we demonstrate that recent unified vision-language models perform poorly on generating interleaved content. While compositional approaches that combine separate language and image models show a 111% improvement over unified models at the holistic level, their performance remains suboptimal at both block and image levels. To facilitate future work, we develop ISG-Agent, a baseline agent employing a \"plan-execute-refine\" pipeline to invoke tools, achieving a 122% performance improvement.",
            "score": 11,
            "issue_id": 828,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "dac4ab78d1cc7e28",
            "authors": [
                "Dongping Chen",
                "Ruoxi Chen",
                "Shu Pu",
                "Zhaoyi Liu",
                "Yanru Wu",
                "Caixi Chen",
                "Benlin Liu",
                "Yue Huang",
                "Yao Wan",
                "Pan Zhou",
                "Ranjay Krishna"
            ],
            "affiliations": [
                "HUST",
                "University of Notre Dame",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17188.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#agents",
                    "#games",
                    "#benchmark"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "ISG: Новый подход к оценке генерации мультимодального контента",
                    "desc": "Статья представляет ISG - комплексную систему оценки для генерации чередующегося текста и изображений. ISG использует структуру графа сцены для оценки согласованности и точности на четырех уровнях детализации. Авторы также вводят набор данных ISG-Bench для эффективной оценки моделей в задачах, связанных со зрением. Исследование показывает, что современные унифицированные модели плохо справляются с генерацией чередующегося контента, а композиционные подходы, хотя и лучше, все еще далеки от оптимальных результатов."
                },
                "en": {
                    "title": "Enhancing Text-Image Generation with ISG Framework",
                    "desc": "This paper introduces ISG, a framework designed to evaluate models that generate interleaved text and images, which is useful for tasks like creating step-by-step cooking guides. It uses a scene graph structure to analyze the relationships between text and images, providing a detailed assessment across different levels of granularity. The authors also present ISG-Bench, a benchmark dataset with 1,150 samples that helps evaluate models on complex language-vision tasks. The findings reveal that while compositional models outperform unified models significantly, there is still room for improvement, leading to the development of ISG-Agent, which enhances performance through a structured approach."
                },
                "zh": {
                    "title": "提升文本与图像生成一致性的评估框架",
                    "desc": "本文提出了一种名为ISG的评估框架，用于生成文本和图像交错的响应，旨在解决生成一致性的问题。ISG利用场景图结构捕捉文本与图像块之间的关系，并在整体、结构、块级和图像特定四个层面进行评估。通过ISG-Bench基准数据集，研究表明现有的统一视觉语言模型在生成交错内容方面表现不佳，而组合方法则在整体层面上有显著提升。最后，开发的ISG-Agent通过“计划-执行-优化”流程实现了更高的性能，推动了未来的研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17945",
            "title": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation",
            "url": "https://huggingface.co/papers/2411.17945",
            "abstract": "Generating high-fidelity 3D content from text prompts remains a significant challenge in computer vision due to the limited size, diversity, and annotation depth of the existing datasets. To address this, we introduce MARVEL-40M+, an extensive dataset with 40 million text annotations for over 8.9 million 3D assets aggregated from seven major 3D datasets. Our contribution is a novel multi-stage annotation pipeline that integrates open-source pretrained multi-view VLMs and LLMs to automatically produce multi-level descriptions, ranging from detailed (150-200 words) to concise semantic tags (10-20 words). This structure supports both fine-grained 3D reconstruction and rapid prototyping. Furthermore, we incorporate human metadata from source datasets into our annotation pipeline to add domain-specific information in our annotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D, a two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our annotations and use a pretrained image-to-3D network to generate 3D textured meshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly outperforms existing datasets in annotation quality and linguistic diversity, achieving win rates of 72.41% by GPT-4 and 73.40% by human evaluators.",
            "score": 5,
            "issue_id": 827,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "360ba2514eea161e",
            "authors": [
                "Sankalp Sinha",
                "Mohammad Sadil Khan",
                "Muhammad Usama",
                "Shino Sam",
                "Didier Stricker",
                "Sk Aziz Ali",
                "Muhammad Zeshan Afzal"
            ],
            "affiliations": [
                "BITS Pilani, Hyderabad",
                "DFKI",
                "MindGarage",
                "RPTU Kaiserslautern-Landau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17945.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#diffusion",
                    "#hallucinations",
                    "#dataset"
                ],
                "emoji": "🌟",
                "ru": {
                    "title": "Революция в 3D: от текста к реалистичным объектам",
                    "desc": "Статья представляет MARVEL-40M+, масштабный набор данных с 40 миллионами текстовых аннотаций для более чем 8,9 миллионов 3D-объектов. Авторы разработали многоэтапный конвейер аннотаций, использующий предобученные мультимодальные языковые модели и большие языковые модели для создания многоуровневых описаний. Набор данных значительно превосходит существующие по качеству аннотаций и лингвистическому разнообразию. Также представлен MARVEL-FX3D - двухэтапный конвейер для генерации 3D-контента из текстовых запросов."
                },
                "en": {
                    "title": "Unlocking 3D Creation with MARVEL-40M+: A New Era in Text-to-3D Generation",
                    "desc": "This paper presents MARVEL-40M+, a new dataset designed to improve the generation of 3D content from text prompts. It includes 40 million text annotations for 8.9 million 3D assets, created using a multi-stage annotation pipeline that leverages pretrained vision-language models (VLMs) and large language models (LLMs). The dataset enhances 3D reconstruction and prototyping by providing detailed and concise descriptions, while also incorporating human metadata to reduce inaccuracies in the generated annotations. Additionally, the authors introduce MARVEL-FX3D, a text-to-3D pipeline that efficiently generates 3D meshes, demonstrating superior annotation quality and diversity compared to existing datasets."
                },
                "zh": {
                    "title": "MARVEL-40M+: 高质量3D内容生成的新纪元",
                    "desc": "本文介绍了MARVEL-40M+数据集，该数据集包含4000万条文本注释，涵盖890万件3D资产，旨在解决现有数据集在规模和多样性上的不足。我们提出了一种新颖的多阶段注释流程，结合了开源的多视角视觉语言模型（VLM）和大型语言模型（LLM），自动生成多层次的描述。通过引入人类元数据，我们增强了注释的领域特定信息，减少了VLM的幻觉现象。此外，我们开发了MARVEL-FX3D，一个两阶段的文本到3D生成管道，能够快速生成高质量的3D网格。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18613",
            "title": "CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models",
            "url": "https://huggingface.co/papers/2411.18613",
            "abstract": "We present CAT4D, a method for creating 4D (dynamic 3D) scenes from monocular video. CAT4D leverages a multi-view video diffusion model trained on a diverse combination of datasets to enable novel view synthesis at any specified camera poses and timestamps. Combined with a novel sampling approach, this model can transform a single monocular video into a multi-view video, enabling robust 4D reconstruction via optimization of a deformable 3D Gaussian representation. We demonstrate competitive performance on novel view synthesis and dynamic scene reconstruction benchmarks, and highlight the creative capabilities for 4D scene generation from real or generated videos. See our project page for results and interactive demos: cat-4d.github.io.",
            "score": 3,
            "issue_id": 829,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "736c7f565ac43a96",
            "authors": [
                "Rundi Wu",
                "Ruiqi Gao",
                "Ben Poole",
                "Alex Trevithick",
                "Changxi Zheng",
                "Jonathan T. Barron",
                "Aleksander Holynski"
            ],
            "affiliations": [
                "Columbia University",
                "Google DeepMind",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18613.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#optimization",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Превращение 2D-видео в динамичные 3D-сцены",
                    "desc": "CAT4D - это метод создания динамических 3D-сцен из монокулярного видео с использованием многоракурсной видео-диффузионной модели. Модель обучена на разнообразных наборах данных и позволяет синтезировать новые ракурсы для заданных положений камеры и временных меток. CAT4D преобразует одно монокулярное видео в многоракурсное, что делает возможной надежную 4D-реконструкцию путем оптимизации деформируемого 3D гауссового представления. Метод демонстрирует конкурентоспособные результаты в синтезе новых ракурсов и реконструкции динамических сцен, а также открывает возможности для генерации 4D-сцен из реальных или сгенерированных видео."
                },
                "en": {
                    "title": "Transforming Monocular Videos into Dynamic 4D Scenes",
                    "desc": "CAT4D is a new method that creates dynamic 3D scenes from a single video. It uses a multi-view video diffusion model, which is trained on various datasets, to generate new views from different camera angles and times. The approach includes a unique sampling technique that allows for the transformation of a single video into a multi-view format, facilitating the reconstruction of 4D scenes using a flexible 3D Gaussian model. The results show that CAT4D performs well in generating new views and reconstructing dynamic scenes, showcasing its potential for creative applications."
                },
                "zh": {
                    "title": "CAT4D：从单目视频生成动态3D场景的创新方法",
                    "desc": "CAT4D是一种从单目视频创建4D动态3D场景的方法。它利用多视角视频扩散模型，经过多种数据集的训练，能够在指定的相机位置和时间戳下进行新视角合成。通过一种新颖的采样方法，该模型可以将单个单目视频转换为多视角视频，从而通过优化可变形的3D高斯表示实现稳健的4D重建。我们在新视角合成和动态场景重建基准测试中展示了竞争力的性能，并强调了从真实或生成视频中生成4D场景的创造能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17787",
            "title": "Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient",
            "url": "https://huggingface.co/papers/2411.17787",
            "abstract": "In the rapidly advancing field of image generation, Visual Auto-Regressive (VAR) modeling has garnered considerable attention for its innovative next-scale prediction approach. This paradigm offers substantial improvements in efficiency, scalability, and zero-shot generalization. Yet, the inherently coarse-to-fine nature of VAR introduces a prolonged token sequence, leading to prohibitive memory consumption and computational redundancies. To address these bottlenecks, we propose Collaborative Decoding (CoDe), a novel efficient decoding strategy tailored for the VAR framework. CoDe capitalizes on two critical observations: the substantially reduced parameter demands at larger scales and the exclusive generation patterns across different scales. Based on these insights, we partition the multi-scale inference process into a seamless collaboration between a large model and a small model. The large model serves as the 'drafter', specializing in generating low-frequency content at smaller scales, while the smaller model serves as the 'refiner', solely focusing on predicting high-frequency details at larger scales. This collaboration yields remarkable efficiency with minimal impact on quality: CoDe achieves a 1.7x speedup, slashes memory usage by around 50%, and preserves image quality with only a negligible FID increase from 1.95 to 1.98. When drafting steps are further decreased, CoDe can achieve an impressive 2.9x acceleration ratio, reaching 41 images/s at 256x256 resolution on a single NVIDIA 4090 GPU, while preserving a commendable FID of 2.27. The code is available at https://github.com/czg1225/CoDe",
            "score": 3,
            "issue_id": 828,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "0ac2e8ea4bbd89ad",
            "authors": [
                "Zigeng Chen",
                "Xinyin Ma",
                "Gongfan Fang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17787.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#optimization",
                    "#small_models",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Эффективное сотрудничество моделей для быстрой генерации изображений",
                    "desc": "Статья представляет новую стратегию декодирования для визуального авторегрессионного моделирования (VAR) под названием Collaborative Decoding (CoDe). CoDe разделяет процесс генерации изображения на две части: большая модель генерирует низкочастотный контент, а малая модель уточняет детали. Это позволяет значительно ускорить генерацию и снизить потребление памяти при минимальном влиянии на качество изображений. Авторы демонстрируют ускорение до 2.9 раз при сохранении хорошего значения метрики FID."
                },
                "en": {
                    "title": "Collaborative Decoding: Boosting Efficiency in Image Generation",
                    "desc": "This paper introduces Collaborative Decoding (CoDe), an efficient decoding strategy for Visual Auto-Regressive (VAR) models in image generation. CoDe optimizes the multi-scale inference process by utilizing a large model to generate low-frequency content and a smaller model to refine high-frequency details. This collaboration significantly reduces memory usage by about 50% and increases processing speed by up to 2.9 times, while maintaining image quality with only a slight increase in FID score. The proposed method demonstrates a promising approach to overcoming the computational challenges associated with VAR modeling."
                },
                "zh": {
                    "title": "协作解码：提升图像生成效率的新策略",
                    "desc": "在快速发展的图像生成领域，视觉自回归（VAR）建模因其创新的下一步预测方法而受到广泛关注。VAR的粗到细特性导致了较长的令牌序列，从而造成了高昂的内存消耗和计算冗余。为了解决这些瓶颈，我们提出了一种新的高效解码策略——协作解码（CoDe），它通过大模型和小模型的无缝协作来优化多尺度推理过程。CoDe在保持图像质量的同时，实现了1.7倍的加速和约50%的内存使用减少，展示了其在效率上的显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.18462",
            "title": "Draft Model Knows When to Stop: A Self-Verification Length Policy for Speculative Decoding",
            "url": "https://huggingface.co/papers/2411.18462",
            "abstract": "Speculative Decoding (SD) has become an important technique in accelerating the inference speed of large language models. Conventional SD methods employ a fixed draft length, which ignores the token generation difficulty across tasks. Consequently, in this paper, we address such an issue and introduce SVIP - a difficulty-aware dynamic draft length policy for speculative decoding systems. Based on a theoretical lower bound of draft token acceptance rate and its inference-time approximation, SVIP adaptively determines the lengths of draft sequences based on the entropy of each draft token distribution. Experimental results on mainstream SD benchmarks and frameworks demonstrate the superior performance of SVIP, achieving up to 20\\% walltime speedup on SpecBench over baseline SD methods and 60\\% speedup on MT-Bench for long-form generation of up to 8K tokens. Moreover, SVIP is totally training-free and compatible with any existing SD methods that generate draft tokens autoregressively. Experimental results also show that SVIP yields consistent walltime improvement on top of GliDe & CaPE and EAGLE-2.",
            "score": 2,
            "issue_id": 828,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "b9bc0d66b9a257c9",
            "authors": [
                "Ziyin Zhang",
                "Jiahao Xu",
                "Tian Liang",
                "Xingyu Chen",
                "Zhiwei He",
                "Rui Wang",
                "Zhaopeng Tu"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.18462.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#optimization",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Адаптивное спекулятивное декодирование для ускорения языковых моделей",
                    "desc": "Статья представляет SVIP - новый метод для ускорения вывода больших языковых моделей с помощью спекулятивного декодирования. SVIP адаптивно определяет длину черновых последовательностей токенов на основе энтропии распределения каждого чернового токена. Эксперименты показывают значительное ускорение по сравнению с базовыми методами спекулятивного декодирования - до 20% на SpecBench и до 60% на MT-Bench для генерации длинных текстов. SVIP не требует дополнительного обучения и совместим с существующими методами спекулятивного декодирования."
                },
                "en": {
                    "title": "Dynamic Draft Length for Faster Language Model Inference",
                    "desc": "This paper presents SVIP, a new method for speculative decoding (SD) that improves the efficiency of large language models by adapting the draft length based on task difficulty. Traditional SD approaches use a fixed draft length, which can be inefficient as it does not account for the varying complexity of token generation. SVIP utilizes the entropy of draft token distributions to dynamically adjust the length of draft sequences, leading to faster inference times. Experimental results indicate that SVIP can achieve significant speedups in processing time, making it a valuable enhancement for existing SD frameworks without requiring additional training."
                },
                "zh": {
                    "title": "动态草稿长度，提升推理速度！",
                    "desc": "本论文介绍了一种新的推测解码技术，称为SVIP，旨在提高大型语言模型的推理速度。传统的推测解码方法使用固定的草稿长度，未能考虑不同任务的生成难度。SVIP通过分析每个草稿令牌分布的熵，动态调整草稿序列的长度，从而提高效率。实验结果表明，SVIP在多个基准测试中表现优异，能够显著加快推理速度，且无需额外训练。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17440",
            "title": "Identity-Preserving Text-to-Video Generation by Frequency Decomposition",
            "url": "https://huggingface.co/papers/2411.17440",
            "abstract": "Identity-preserving text-to-video (IPT2V) generation aims to create high-fidelity videos with consistent human identity. It is an important task in video generation but remains an open problem for generative models. This paper pushes the technical frontier of IPT2V in two directions that have not been resolved in literature: (1) A tuning-free pipeline without tedious case-by-case finetuning, and (2) A frequency-aware heuristic identity-preserving DiT-based control scheme. We propose ConsisID, a tuning-free DiT-based controllable IPT2V model to keep human identity consistent in the generated video. Inspired by prior findings in frequency analysis of diffusion transformers, it employs identity-control signals in the frequency domain, where facial features can be decomposed into low-frequency global features and high-frequency intrinsic features. First, from a low-frequency perspective, we introduce a global facial extractor, which encodes reference images and facial key points into a latent space, generating features enriched with low-frequency information. These features are then integrated into shallow layers of the network to alleviate training challenges associated with DiT. Second, from a high-frequency perspective, we design a local facial extractor to capture high-frequency details and inject them into transformer blocks, enhancing the model's ability to preserve fine-grained features. We propose a hierarchical training strategy to leverage frequency information for identity preservation, transforming a vanilla pre-trained video generation model into an IPT2V model. Extensive experiments demonstrate that our frequency-aware heuristic scheme provides an optimal control solution for DiT-based models. Thanks to this scheme, our ConsisID generates high-quality, identity-preserving videos, making strides towards more effective IPT2V.",
            "score": 2,
            "issue_id": 827,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "28823ea0e7fa6b0c",
            "authors": [
                "Shenghai Yuan",
                "Jinfa Huang",
                "Xianyi He",
                "Yunyuan Ge",
                "Yujun Shi",
                "Liuhan Chen",
                "Jiebo Luo",
                "Li Yuan"
            ],
            "affiliations": [
                "National University of Singapore",
                "Peking University",
                "Peng Cheng Laboratory",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17440.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#diffusion",
                    "#optimization",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Сохранение идентичности в видео через частотный анализ",
                    "desc": "Статья представляет ConsisID - модель для генерации видео с сохранением идентичности человека на основе диффузионных трансформеров. Авторы предлагают частотно-ориентированный подход, разделяя facial features на низкочастотные глобальные и высокочастотные локальные компоненты. Модель использует глобальный и локальный экстракторы лиц для сохранения идентичности в разных частотных диапазонах. Предложенная иерархическая стратегия обучения позволяет эффективно использовать частотную информацию для сохранения идентичности в генерируемом видео."
                },
                "en": {
                    "title": "ConsisID: Seamless Identity Preservation in Video Generation",
                    "desc": "This paper presents a novel approach to identity-preserving text-to-video (IPT2V) generation, focusing on maintaining consistent human identity in videos. The authors introduce ConsisID, a controllable model that operates without the need for extensive fine-tuning, simplifying the process of video generation. By utilizing a frequency-aware heuristic, the model effectively separates and processes low-frequency global features and high-frequency intrinsic details of facial characteristics. The proposed hierarchical training strategy enhances the model's performance, allowing it to generate high-fidelity videos while preserving individual identities throughout the video sequence."
                },
                "zh": {
                    "title": "高保真身份保持视频生成的突破",
                    "desc": "身份保持的文本到视频生成（IPT2V）旨在创建具有一致人类身份的高保真视频。本文提出了一种名为ConsisID的模型，能够在不需要繁琐微调的情况下，保持生成视频中的人类身份一致性。该模型利用频率分析的方法，将人脸特征分解为低频和高频信息，从而有效地提取和保留细致的面部特征。通过层次化的训练策略，ConsisID在视频生成任务中实现了更好的身份保持效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15139",
            "title": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
            "url": "https://huggingface.co/papers/2411.15139",
            "abstract": "Recently, the diffusion model has emerged as a powerful generative technique for robotic policy learning, capable of modeling multi-mode action distributions. Leveraging its capability for end-to-end autonomous driving is a promising direction. However, the numerous denoising steps in the robotic diffusion policy and the more dynamic, open-world nature of traffic scenes pose substantial challenges for generating diverse driving actions at a real-time speed. To address these challenges, we propose a novel truncated diffusion policy that incorporates prior multi-mode anchors and truncates the diffusion schedule, enabling the model to learn denoising from anchored Gaussian distribution to the multi-mode driving action distribution. Additionally, we design an efficient cascade diffusion decoder for enhanced interaction with conditional scene context. The proposed model, DiffusionDrive, demonstrates 10times reduction in denoising steps compared to vanilla diffusion policy, delivering superior diversity and quality in just 2 steps. On the planning-oriented NAVSIM dataset, with the aligned ResNet-34 backbone, DiffusionDrive achieves 88.1 PDMS without bells and whistles, setting a new record, while running at a real-time speed of 45 FPS on an NVIDIA 4090. Qualitative results on challenging scenarios further confirm that DiffusionDrive can robustly generate diverse plausible driving actions. Code and model will be available at https://github.com/hustvl/DiffusionDrive.",
            "score": 1,
            "issue_id": 828,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "cef89e5425478782",
            "authors": [
                "Bencheng Liao",
                "Shaoyu Chen",
                "Haoran Yin",
                "Bo Jiang",
                "Cheng Wang",
                "Sixu Yan",
                "Xinbang Zhang",
                "Xiangyu Li",
                "Ying Zhang",
                "Qian Zhang",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Horizon Robotics",
                "Institute of Artificial Intelligence, Huazhong University of Science & Technology",
                "School of EIC, Huazhong University of Science & Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15139.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#optimization",
                    "#robotics",
                    "#dataset",
                    "#diffusion"
                ],
                "emoji": "🚗",
                "ru": {
                    "title": "Ускоренная диффузионная модель для разнообразного и эффективного автономного вождения",
                    "desc": "Статья представляет новую модель DiffusionDrive для автономного вождения, основанную на усеченной диффузионной политике. Авторы предлагают использовать предварительные многомодовые якоря и сокращенное расписание диффузии для ускорения генерации разнообразных действий вождения. Модель включает каскадный диффузионный декодер для улучшенного взаимодействия с контекстом сцены. DiffusionDrive демонстрирует 10-кратное сокращение шагов шумоподавления по сравнению с обычной диффузионной политикой, обеспечивая высокое качество и разнообразие действий всего за 2 шага."
                },
                "en": {
                    "title": "Revolutionizing Robotic Driving with Efficient Diffusion Models",
                    "desc": "This paper introduces DiffusionDrive, a novel approach to robotic policy learning using diffusion models. It addresses the challenges of generating diverse driving actions in dynamic traffic environments by implementing a truncated diffusion policy that utilizes prior multi-mode anchors. The model significantly reduces the number of denoising steps required, achieving high-quality action generation in just 2 steps while maintaining real-time performance. Experimental results show that DiffusionDrive outperforms existing methods in both diversity and quality of driving actions, setting a new benchmark in the field."
                },
                "zh": {
                    "title": "扩散驱动：实时多样化驾驶策略的突破",
                    "desc": "最近，扩散模型作为一种强大的生成技术，在机器人策略学习中得到了广泛应用，能够建模多模态的动作分布。我们提出了一种新颖的截断扩散策略，结合了先前的多模态锚点，并缩短了扩散调度，从而使模型能够从锚定的高斯分布学习去噪声，生成多模态驾驶动作分布。该模型DiffusionDrive在去噪步骤上减少了10倍，能够在仅需2个步骤内提供更高的多样性和质量。实验结果表明，DiffusionDrive在实时速度下表现出色，能够稳健地生成多样化的合理驾驶动作。"
                }
            }
        }
    ],
    "link_prev": "2024-11-27.html",
    "link_next": "2024-11-29.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "27.11",
        "en": "11/27",
        "zh": "11月27日"
    },
    "short_date_next": {
        "ru": "29.11",
        "en": "11/29",
        "zh": "11月29日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 2,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 5,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的图形用户界面（GUI）助手模型，名为ShowUI。它结合了视觉、语言和动作，旨在提高人类工作流的生产力。ShowUI通过UI引导的视觉标记选择、交错的视觉-语言-动作流和小规模高质量的GUI指令跟随数据集来减少计算成本并提高训练效率。该模型在零样本截图定位任务中取得了75.1%的准确率，并在多个环境中表现出色。模型的代码已在GitHub上公开。",
        "title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent",
        "pinyin": "这篇文章介绍了一种新的图形用户界面（GUI）助手模型，名为ShowUI。它结合了视觉、语言和动作，旨在提高人类工作流的生产力。ShowUI通过UI引导的视觉标记选择、交错的视觉-语言-动作流和小规模高质量的GUI指令跟随数据集来减少计算成本并提高训练效率。该模型在零样本截图定位任务中取得了75.1%的准确率，并在多个环境中表现出色。模型的代码已在GitHub上公开。\n\nzhè piān wén zhāng jiè shào le yī zhǒng xīn de tú xíng yòng hù jiē miàn (GUI) zhù shǒu mó xíng, míng wèi ShowUI. tā jié hé le shì jué, yǔ yán hé dòng zuò, zhǐ zài tí gāo rén lèi gōng zuò liú de shēng chǎn lì. ShowUI tōng guò UI yǐn dǎo de shì jué biāo jì xuǎn zé, jiāo cuò de shì jué-yǔ yán-dòng zuò liú hé xiǎo guī mó gāo zhì liàng de GUI zhǐ lìng gēn suí shù jù jiǎng shǎo jì suàn chéng běn bìng tí gāo xùn liàn xiào yì. gǎi mó xíng zài líng yàng bǎn jié dìng wèi rèn wù zhōng qǔ dé le 75.1% de zhǔn què lǜ, bìng zài duō gè huán jìng zhōng biǎo xiàn chū sè. mó xíng de dài mǎ yǐ zài GitHub shàng gōng kāi.",
        "vocab": "[{'word': '图形用户界面', 'pinyin': 'tú xíng yòng hù jiē miàn', 'trans': 'graphical user interface'},\n{'word': '助手', 'pinyin': 'zhù shǒu', 'trans': 'assistant'},\n{'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'},\n{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'vision'},\n{'word': '语言', 'pinyin': 'yǔ yán', 'trans': 'language'},\n{'word': '动作', 'pinyin': 'dòng zuò', 'trans': 'action'},\n{'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'},\n{'word': '生产力', 'pinyin': 'shēng chǎn lì', 'trans': 'productivity'},\n{'word': '引导', 'pinyin': 'yǐn dǎo', 'trans': 'guide'},\n{'word': '标记', 'pinyin': 'biāo jì', 'trans': 'mark'},\n{'word': '选择', 'pinyin': 'xuǎn zé', 'trans': 'selection'},\n{'word': '交错', 'pinyin': 'jiāo cuò', 'trans': 'interleave'},\n{'word': '流', 'pinyin': 'liú', 'trans': 'flow'},\n{'word': '规模', 'pinyin': 'guī mó', 'trans': 'scale'},\n{'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'},\n{'word': '指令', 'pinyin': 'zhǐ lìng', 'trans': 'command'},\n{'word': '跟随', 'pinyin': 'gēn suí', 'trans': 'follow'},\n{'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'},\n{'word': '计算', 'pinyin': 'jì suàn', 'trans': 'computation'},\n{'word': '成本', 'pinyin': 'chéng běn', 'trans': 'cost'},\n{'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'training'},\n{'word': '效率', 'pinyin': 'xiào lǜ', 'trans': 'efficiency'},\n{'word': '零样本', 'pinyin': 'líng yàng běn', 'trans': 'zero-shot'},\n{'word': '截图', 'pinyin': 'jié tú', 'trans': 'screenshot'},\n{'word': '定位', 'pinyin': 'dìng wèi', 'trans': 'localization'},\n{'word': '任务', 'pinyin': 'rèn wu', 'trans': 'task'},\n{'word': '准确率', 'pinyin': 'zhǔn què lǜ', 'trans': 'accuracy'},\n{'word': '环境', 'pinyin': 'huán jìng', 'trans': 'environment'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'},\n{'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'},\n{'word': '公开', 'pinyin': 'gōng kāi', 'trans': 'public'},\n{'word': 'GitHub', 'pinyin': 'GitHub', 'trans': 'GitHub'}]",
        "trans": "This article introduces a new graphical user interface (GUI) assistant model called ShowUI. It combines vision, language, and action to enhance the productivity of human workflows. ShowUI reduces computational costs and improves training efficiency through UI-guided visual mark selection, interleaved vision-language-action flows, and a small-scale, high-quality GUI instruction-following dataset. The model achieved a 75.1% accuracy rate in zero-shot screenshot localization tasks and performed well in multiple environments. The model's code has been made publicly available on GitHub.",
        "update_ts": "2024-11-27 09:11"
    }
}