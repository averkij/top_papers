{
    "date": {
        "ru": "28 ноября",
        "en": "November 28",
        "zh": "11月28日"
    },
    "time_utc": "2024-11-28 00:49",
    "weekday": 3,
    "issue_id": 826,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.17465",
            "title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent",
            "url": "https://huggingface.co/papers/2411.17465",
            "abstract": "Building Graphical User Interface (GUI) assistants holds significant promise for enhancing human workflow productivity. While most agents are language-based, relying on closed-source API with text-rich meta-information (e.g., HTML or accessibility tree), they show limitations in perceiving UI visuals as humans do, highlighting the need for GUI visual agents. In this work, we develop a vision-language-action model in digital world, namely ShowUI, which features the following innovations: (i) UI-Guided Visual Token Selection to reduce computational costs by formulating screenshots as an UI connected graph, adaptively identifying their redundant relationship and serve as the criteria for token selection during self-attention blocks; (ii) Interleaved Vision-Language-Action Streaming that flexibly unifies diverse needs within GUI tasks, enabling effective management of visual-action history in navigation or pairing multi-turn query-action sequences per screenshot to enhance training efficiency; (iii) Small-scale High-quality GUI Instruction-following Datasets by careful data curation and employing a resampling strategy to address significant data type imbalances. With above components, ShowUI, a lightweight 2B model using 256K data, achieves a strong 75.1% accuracy in zero-shot screenshot grounding. Its UI-guided token selection further reduces 33% of redundant visual tokens during training and speeds up the performance by 1.4x. Navigation experiments across web Mind2Web, mobile AITW, and online MiniWob environments further underscore the effectiveness and potential of our model in advancing GUI visual agents. The models are available at https://github.com/showlab/ShowUI.",
            "score": 46,
            "issue_id": 803,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "372a78043d62af12",
            "authors": [
                "Kevin Qinghong Lin",
                "Linjie Li",
                "Difei Gao",
                "Zhengyuan Yang",
                "Shiwei Wu",
                "Zechen Bai",
                "Weixian Lei",
                "Lijuan Wang",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Microsoft",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17465.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data",
                    "#games",
                    "#optimization",
                    "#cv",
                    "#agents",
                    "#graphs",
                    "#dataset"
                ],
                "emoji": "🖥️",
                "ru": {
                    "title": "ShowUI: Революция в создании интеллектуальных графических интерфейсов",
                    "desc": "Статья представляет ShowUI - модель для создания графических пользовательских интерфейсов (GUI) с использованием искусственного интеллекта. Модель использует инновационный подход к выбору визуальных токенов на основе структуры интерфейса, что позволяет снизить вычислительные затраты. ShowUI объединяет зрение, язык и действия в потоковом режиме, что делает ее эффективной для различных задач GUI. Модель достигает точности 75.1% при нулевом обучении в задаче локализации элементов на скриншотах, используя всего 256 тысяч примеров для обучения."
                },
                "en": {
                    "title": "Empowering GUI Assistants with Visual Intelligence",
                    "desc": "This paper introduces ShowUI, a vision-language-action model designed to improve GUI assistant capabilities by better understanding visual elements. It innovates with UI-Guided Visual Token Selection to optimize computational efficiency by treating screenshots as connected graphs, which helps in selecting relevant visual tokens. Additionally, it employs Interleaved Vision-Language-Action Streaming to manage visual-action history effectively, enhancing the model's ability to handle complex GUI tasks. The model demonstrates strong performance with a 75.1% accuracy in zero-shot screenshot grounding and significantly reduces redundant visual tokens during training, showcasing its potential in advancing GUI visual agents."
                },
                "zh": {
                    "title": "提升GUI助手效率的视觉-语言-行动模型",
                    "desc": "本研究提出了一种新的视觉-语言-行动模型，名为ShowUI，旨在提升图形用户界面（GUI）助手的效率。该模型通过UI引导的视觉标记选择，减少了计算成本，并在自注意力模块中优化了标记选择过程。ShowUI还实现了交错的视觉-语言-行动流，灵活处理GUI任务中的多样需求，提高了训练效率。通过精心的数据整理和重采样策略，ShowUI在零样本截图定位中达到了75.1%的准确率，展示了其在GUI视觉代理领域的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17116",
            "title": "Star Attention: Efficient LLM Inference over Long Sequences",
            "url": "https://huggingface.co/papers/2411.17116",
            "abstract": "Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 95-100% of accuracy.",
            "score": 26,
            "issue_id": 802,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "12194d270104d50f",
            "authors": [
                "Shantanu Acharya",
                "Fei Jia",
                "Boris Ginsburg"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17116.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#inference",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "⭐",
                "ru": {
                    "title": "Звездное внимание: ускорение LLM без потери точности",
                    "desc": "Статья представляет метод Star Attention для улучшения эффективности вычислений в трансформерных моделях большого языка (LLM) при работе с длинными последовательностями. Метод использует двухфазное блочно-разреженное приближение, распределяя внимание между несколькими узлами и минимизируя накладные расходы на коммуникацию. Star Attention сочетает локальное блочное внимание на первой фазе и глобальное внимание на второй фазе. Этот подход позволяет снизить требования к памяти и время вывода до 11 раз при сохранении 95-100% точности."
                },
                "en": {
                    "title": "Boosting Efficiency in Long Sequence Processing with Star Attention",
                    "desc": "This paper presents Star Attention, a new method designed to enhance the efficiency of Transformer-based Large Language Models (LLMs) when processing long sequences. It addresses the high computational cost and slow inference times caused by the traditional self-attention mechanism's quadratic complexity. Star Attention operates in two phases: first, it uses blockwise-local attention to process context in parallel across multiple hosts, and then it applies sequence-global attention for query and response tokens. This approach significantly reduces memory usage and inference time by up to 11 times while maintaining a high level of accuracy, making it a valuable advancement in the field of machine learning."
                },
                "zh": {
                    "title": "星际注意力：提升长序列推理效率的创新方法",
                    "desc": "本文介绍了一种名为星际注意力（Star Attention）的新方法，旨在提高基于变换器的大型语言模型（LLM）在长序列上的推理效率。该方法通过在多个主机之间分片注意力，采用两阶段的块稀疏近似，显著降低了计算成本。第一阶段使用块局部注意力并行处理上下文，第二阶段则通过全局注意力处理查询和响应标记。星际注意力与大多数使用全局注意力训练的变换器模型无缝集成，能够将内存需求和推理时间减少多达11倍，同时保持95-100%的准确率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16819",
            "title": "Pathways on the Image Manifold: Image Editing via Video Generation",
            "url": "https://huggingface.co/papers/2411.16819",
            "abstract": "Recent advances in image editing, driven by image diffusion models, have shown remarkable progress. However, significant challenges remain, as these models often struggle to follow complex edit instructions accurately and frequently compromise fidelity by altering key elements of the original image. Simultaneously, video generation has made remarkable strides, with models that effectively function as consistent and continuous world simulators. In this paper, we propose merging these two fields by utilizing image-to-video models for image editing. We reformulate image editing as a temporal process, using pretrained video models to create smooth transitions from the original image to the desired edit. This approach traverses the image manifold continuously, ensuring consistent edits while preserving the original image's key aspects. Our approach achieves state-of-the-art results on text-based image editing, demonstrating significant improvements in both edit accuracy and image preservation.",
            "score": 23,
            "issue_id": 808,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "4c5f3c4f6e2e545b",
            "authors": [
                "Noam Rotstein",
                "Gal Yona",
                "Daniel Silver",
                "Roy Velich",
                "David Bensaïd",
                "Ron Kimmel"
            ],
            "affiliations": [
                "Technion - Israel Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16819.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Редактирование изображений через призму видео: новый взгляд на старую задачу",
                    "desc": "Эта статья предлагает новый подход к редактированию изображений с использованием моделей генерации видео. Авторы переосмысливают процесс редактирования как временной, создавая плавный переход от исходного изображения к отредактированному. Этот метод позволяет сохранить ключевые аспекты оригинального изображения и обеспечивает более точное выполнение сложных инструкций по редактированию. Результаты показывают значительное улучшение как в точности редактирования, так и в сохранении исходного изображения по сравнению с существующими методами."
                },
                "en": {
                    "title": "Seamless Image Editing through Video Model Integration",
                    "desc": "This paper addresses the challenges in image editing using diffusion models, which often fail to accurately follow complex instructions and can distort important image features. The authors propose a novel approach that combines image editing with video generation by treating image editing as a temporal process. By leveraging pretrained video models, they create smooth transitions from the original image to the edited version, ensuring that key elements are preserved. Their method achieves state-of-the-art performance in text-based image editing, significantly enhancing both the accuracy of edits and the fidelity of the original image."
                },
                "zh": {
                    "title": "图像编辑与视频生成的完美结合",
                    "desc": "本文提出了一种将图像编辑与视频生成相结合的方法。我们将图像编辑重新定义为一个时间过程，利用预训练的视频模型实现从原始图像到目标编辑的平滑过渡。通过这种方法，我们能够在保持原始图像关键特征的同时，确保编辑的一致性。实验结果表明，我们的方法在基于文本的图像编辑任务中达到了最先进的效果，显著提高了编辑的准确性和图像的保真度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17686",
            "title": "Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration",
            "url": "https://huggingface.co/papers/2411.17686",
            "abstract": "To accelerate the inference of heavy Multimodal Large Language Models (MLLMs), this study rethinks the current landscape of training-free token reduction research. We regret to find that the critical components of existing methods are tightly intertwined, with their interconnections and effects remaining unclear for comparison, transfer, and expansion. Therefore, we propose a unified ''filter-correlate-compress'' paradigm that decomposes the token reduction into three distinct stages within a pipeline, maintaining consistent design objectives and elements while allowing for unique implementations. We additionally demystify the popular works and subsume them into our paradigm to showcase its universality. Finally, we offer a suite of methods grounded in the paradigm, striking a balance between speed and accuracy throughout different phases of the inference. Experimental results across 10 benchmarks indicate that our methods can achieve up to an 82.4% reduction in FLOPs with a minimal impact on performance, simultaneously surpassing state-of-the-art training-free methods. Our project page is at https://ficoco-accelerate.github.io/.",
            "score": 14,
            "issue_id": 805,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "968244e861534e4a",
            "authors": [
                "Yuhang Han",
                "Xuyang Liu",
                "Pengxiang Ding",
                "Donglin Wang",
                "Honggang Chen",
                "Qingsen Yan",
                "Siteng Huang"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba Group",
                "Northwestern Polytechnical University",
                "Sichuan University",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17686.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#inference",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Ускорение MLLM: новая парадигма сокращения токенов",
                    "desc": "Данная статья представляет новый подход к ускорению вывода мультимодальных больших языковых моделей (MLLM). Авторы предлагают унифицированную парадигму 'фильтрация-корреляция-сжатие' для сокращения токенов, разделяя процесс на три отдельных этапа. Исследование демонстрирует, как существующие методы вписываются в эту парадигму, и предлагает набор новых методов, балансирующих между скоростью и точностью. Экспериментальные результаты показывают сокращение FLOPS до 82.4% при минимальном влиянии на производительность, превосходя современные методы без дополнительного обучения."
                },
                "en": {
                    "title": "Streamlining Inference: The Filter-Correlate-Compress Paradigm for MLLMs",
                    "desc": "This paper introduces a new approach to improve the efficiency of Multimodal Large Language Models (MLLMs) during inference by proposing a 'filter-correlate-compress' paradigm. This framework breaks down the token reduction process into three clear stages, allowing for better understanding and implementation of each component. The authors analyze existing methods and integrate them into their unified approach, demonstrating its broad applicability. Their experiments show that this new method can significantly reduce computational load while maintaining high performance, outperforming current leading techniques."
                },
                "zh": {
                    "title": "加速推理，优化多模态模型的令牌减少",
                    "desc": "本研究旨在加速重型多模态大型语言模型（MLLMs）的推理过程，重新审视无训练的令牌减少研究现状。我们发现现有方法的关键组件紧密相连，其相互关系和效果不明确，难以进行比较和扩展。因此，我们提出了一个统一的“过滤-关联-压缩”范式，将令牌减少分解为三个独立的阶段，保持设计目标的一致性，同时允许独特的实现方式。实验结果表明，我们的方法在不同推理阶段之间实现了速度和准确性的平衡，能够在性能影响最小的情况下，达到高达82.4%的FLOPs减少，超越了现有的无训练方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15296",
            "title": "MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs",
            "url": "https://huggingface.co/papers/2411.15296",
            "abstract": "As a prominent direction of Artificial General Intelligence (AGI), Multimodal Large Language Models (MLLMs) have garnered increased attention from both industry and academia. Building upon pre-trained LLMs, this family of models further develops multimodal perception and reasoning capabilities that are impressive, such as writing code given a flow chart or creating stories based on an image. In the development process, evaluation is critical since it provides intuitive feedback and guidance on improving models. Distinct from the traditional train-eval-test paradigm that only favors a single task like image classification, the versatility of MLLMs has spurred the rise of various new benchmarks and evaluation methods. In this paper, we aim to present a comprehensive survey of MLLM evaluation, discussing four key aspects: 1) the summarised benchmarks types divided by the evaluation capabilities, including foundation capabilities, model self-analysis, and extented applications; 2) the typical process of benchmark counstruction, consisting of data collection, annotation, and precautions; 3) the systematic evaluation manner composed of judge, metric, and toolkit; 4) the outlook for the next benchmark. This work aims to offer researchers an easy grasp of how to effectively evaluate MLLMs according to different needs and to inspire better evaluation methods, thereby driving the progress of MLLM research.",
            "score": 14,
            "issue_id": 804,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "f83b02f1623b605b",
            "authors": [
                "Chaoyou Fu",
                "Yi-Fan Zhang",
                "Shukang Yin",
                "Bo Li",
                "Xinyu Fang",
                "Sirui Zhao",
                "Haodong Duan",
                "Xing Sun",
                "Ziwei Liu",
                "Liang Wang",
                "Caifeng Shan",
                "Ran He"
            ],
            "affiliations": [
                "Nanjing University",
                "Institute of Automation, Chinese Academy of Science",
                "University of Science and Technology of China",
                "Nanyang Technological University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15296.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agi",
                    "#survey",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Комплексный подход к оценке мультимодальных языковых моделей",
                    "desc": "Эта статья представляет собой обзор методов оценки мультимодальных больших языковых моделей (MLLM). Авторы рассматривают четыре ключевых аспекта: типы бенчмарков, процесс их создания, систематический подход к оценке и перспективы будущих бенчмарков. Особое внимание уделяется важности оценки для развития MLLM, которые сочетают возможности обработки естественного языка с восприятием других модальностей. Цель работы - помочь исследователям эффективно оценивать MLLM и стимулировать разработку улучшенных методов оценки."
                },
                "en": {
                    "title": "Evaluating the Future of Multimodal AI",
                    "desc": "This paper surveys the evaluation methods for Multimodal Large Language Models (MLLMs), which are advanced AI systems capable of understanding and generating content across different types of data, like text and images. It categorizes various benchmarks based on their evaluation capabilities, such as foundational skills and application breadth. The authors outline the typical process for creating these benchmarks, including data collection and annotation, and discuss the systematic approach to evaluation involving judges, metrics, and toolkits. The goal is to provide insights that help researchers effectively assess MLLMs and inspire improvements in evaluation techniques, ultimately advancing the field of MLLM research."
                },
                "zh": {
                    "title": "全面评估多模态大型语言模型的未来",
                    "desc": "多模态大型语言模型（MLLMs）是人工通用智能（AGI）的重要方向，近年来受到广泛关注。这些模型在预训练大型语言模型的基础上，进一步发展了多模态感知和推理能力，例如根据流程图编写代码或根据图像创作故事。评估在模型开发过程中至关重要，它提供了直观的反馈和改进指导。本文旨在全面调查MLLM的评估，讨论评估能力、基准构建过程、系统评估方式及未来基准展望等四个关键方面。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14740",
            "title": "TEXGen: a Generative Diffusion Model for Mesh Textures",
            "url": "https://huggingface.co/papers/2411.14740",
            "abstract": "While high-quality texture maps are essential for realistic 3D asset rendering, few studies have explored learning directly in the texture space, especially on large-scale datasets. In this work, we depart from the conventional approach of relying on pre-trained 2D diffusion models for test-time optimization of 3D textures. Instead, we focus on the fundamental problem of learning in the UV texture space itself. For the first time, we train a large diffusion model capable of directly generating high-resolution texture maps in a feed-forward manner. To facilitate efficient learning in high-resolution UV spaces, we propose a scalable network architecture that interleaves convolutions on UV maps with attention layers on point clouds. Leveraging this architectural design, we train a 700 million parameter diffusion model that can generate UV texture maps guided by text prompts and single-view images. Once trained, our model naturally supports various extended applications, including text-guided texture inpainting, sparse-view texture completion, and text-driven texture synthesis. Project page is at http://cvmi-lab.github.io/TEXGen/.",
            "score": 12,
            "issue_id": 803,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "79b09429e72c3c18",
            "authors": [
                "Xin Yu",
                "Ze Yuan",
                "Yuan-Chen Guo",
                "Ying-Tian Liu",
                "JianHui Liu",
                "Yangguang Li",
                "Yan-Pei Cao",
                "Ding Liang",
                "Xiaojuan Qi"
            ],
            "affiliations": [
                "Beihang University, China",
                "The University of Hong Kong, Hong Kong",
                "Tsinghua University, China",
                "VAST, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14740.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#games",
                    "#architecture",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "Революция в генерации 3D-текстур: обучение диффузионной модели прямо в UV-пространстве",
                    "desc": "Статья представляет новый подход к генерации текстурных карт для 3D-объектов. Авторы обучили крупную диффузионную модель, способную напрямую создавать высококачественные текстуры в UV-пространстве. Предложена масштабируемая архитектура нейронной сети, сочетающая свёрточные слои на UV-картах с слоями внимания на облаках точек. Обученная модель может генерировать текстуры на основе текстовых подсказок и изображений с одного ракурса, а также поддерживает задачи инпейнтинга и дополнения текстур."
                },
                "en": {
                    "title": "Revolutionizing 3D Textures: Direct Learning in UV Space",
                    "desc": "This paper presents a novel approach to generating high-quality texture maps for 3D assets by directly learning in the UV texture space. Unlike traditional methods that use pre-trained 2D models, the authors introduce a large diffusion model that generates textures in a feed-forward manner. They propose a scalable network architecture that combines convolutional operations on UV maps with attention mechanisms on point clouds, enabling efficient learning. The resulting model, with 700 million parameters, can create UV texture maps based on text prompts and single-view images, and supports various applications like texture inpainting and synthesis."
                },
                "zh": {
                    "title": "直接在UV纹理空间中生成高质量纹理图",
                    "desc": "本研究提出了一种新的方法，直接在UV纹理空间中学习高质量的纹理图。我们训练了一个大型扩散模型，能够以前馈方式生成高分辨率的纹理图，而不是依赖于预训练的2D扩散模型。该模型具有7亿个参数，能够根据文本提示和单视图图像生成UV纹理图。我们的架构设计结合了卷积和注意力层，使得在高分辨率UV空间中的学习更加高效，并支持多种扩展应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17673",
            "title": "SketchAgent: Language-Driven Sequential Sketch Generation",
            "url": "https://huggingface.co/papers/2411.17673",
            "abstract": "Sketching serves as a versatile tool for externalizing ideas, enabling rapid exploration and visual communication that spans various disciplines. While artificial systems have driven substantial advances in content creation and human-computer interaction, capturing the dynamic and abstract nature of human sketching remains challenging. In this work, we introduce SketchAgent, a language-driven, sequential sketch generation method that enables users to create, modify, and refine sketches through dynamic, conversational interactions. Our approach requires no training or fine-tuning. Instead, we leverage the sequential nature and rich prior knowledge of off-the-shelf multimodal large language models (LLMs). We present an intuitive sketching language, introduced to the model through in-context examples, enabling it to \"draw\" using string-based actions. These are processed into vector graphics and then rendered to create a sketch on a pixel canvas, which can be accessed again for further tasks. By drawing stroke by stroke, our agent captures the evolving, dynamic qualities intrinsic to sketching. We demonstrate that SketchAgent can generate sketches from diverse prompts, engage in dialogue-driven drawing, and collaborate meaningfully with human users.",
            "score": 11,
            "issue_id": 804,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "e1660475c1d9b813",
            "authors": [
                "Yael Vinker",
                "Tamar Rott Shaham",
                "Kristine Zheng",
                "Alex Zhao",
                "Judith E Fan",
                "Antonio Torralba"
            ],
            "affiliations": [
                "MIT",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17673.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "✏️",
                "ru": {
                    "title": "SketchAgent: диалоговое рисование с помощью языковых моделей",
                    "desc": "В статье представлен SketchAgent - метод генерации эскизов, управляемый языком. Он позволяет пользователям создавать и модифицировать эскизы через диалоговое взаимодействие, используя мультимодальные языковые модели без дополнительного обучения. SketchAgent использует интуитивно понятный язык рисования, который переводится в векторную графику, создавая эскиз штрих за штрихом. Система демонстрирует способность генерировать эскизы по различным запросам и эффективно сотрудничать с пользователями."
                },
                "en": {
                    "title": "SketchAgent: Conversational Sketching Made Easy!",
                    "desc": "This paper presents SketchAgent, a novel method for generating sketches using a language-driven approach. It allows users to create and modify sketches through interactive conversations without needing any prior training. The system utilizes large language models to interpret a simple sketching language, converting string-based commands into vector graphics. By drawing incrementally, SketchAgent captures the fluid and dynamic essence of human sketching, enabling effective collaboration between the user and the AI."
                },
                "zh": {
                    "title": "SketchAgent：通过对话生成动态草图",
                    "desc": "本研究介绍了一种名为SketchAgent的草图生成方法，它利用语言驱动的方式，允许用户通过对话互动来创建、修改和完善草图。该方法不需要训练或微调，而是利用现成的多模态大型语言模型的顺序特性和丰富的先验知识。SketchAgent使用一种直观的草图语言，通过上下文示例引入模型，使其能够通过基于字符串的动作进行“绘图”。通过逐笔绘制，我们的代理捕捉了草图固有的动态特性，并能够从多样的提示中生成草图，与用户进行有意义的合作。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17467",
            "title": "Learning 3D Representations from Procedural 3D Programs",
            "url": "https://huggingface.co/papers/2411.17467",
            "abstract": "Self-supervised learning has emerged as a promising approach for acquiring transferable 3D representations from unlabeled 3D point clouds. Unlike 2D images, which are widely accessible, acquiring 3D assets requires specialized expertise or professional 3D scanning equipment, making it difficult to scale and raising copyright concerns. To address these challenges, we propose learning 3D representations from procedural 3D programs that automatically generate 3D shapes using simple primitives and augmentations.   Remarkably, despite lacking semantic content, the 3D representations learned from this synthesized dataset perform on par with state-of-the-art representations learned from semantically recognizable 3D models (e.g., airplanes) across various downstream 3D tasks, including shape classification, part segmentation, and masked point cloud completion. Our analysis further suggests that current self-supervised learning methods primarily capture geometric structures rather than high-level semantics.",
            "score": 8,
            "issue_id": 803,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "de7061420b319a85",
            "authors": [
                "Xuweiyi Chen",
                "Zezhou Cheng"
            ],
            "affiliations": [
                "University of Virginia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17467.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset",
                    "#transfer_learning",
                    "#synthetic"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "Самообучение 3D-представлениям без семантики: геометрия важнее смысла",
                    "desc": "Статья представляет новый подход к самообучению для получения трехмерных представлений из немаркированных облаков точек. Авторы предлагают использовать процедурные 3D-программы для автоматической генерации форм из простых примитивов. Несмотря на отсутствие семантического содержания, полученные представления показывают результаты на уровне современных методов. Исследование также указывает, что текущие методы самообучения в основном фиксируют геометрические структуры, а не высокоуровневую семантику."
                },
                "en": {
                    "title": "Unlocking 3D Learning with Procedural Generation",
                    "desc": "This paper discusses a new method for self-supervised learning to create useful 3D representations from unlabeled 3D point clouds. The authors highlight the difficulty of obtaining 3D data due to the need for specialized equipment and the associated copyright issues. They propose using procedural 3D programs that generate shapes from basic building blocks, allowing for scalable data generation. The results show that these representations, although generated without semantic meaning, perform comparably to those derived from labeled 3D models in various tasks like shape classification and segmentation."
                },
                "zh": {
                    "title": "自监督学习：从程序化生成中获取3D表示",
                    "desc": "自监督学习是一种有前景的方法，可以从未标记的3D点云中获取可转移的3D表示。与2D图像不同，获取3D资产需要专业知识或专业的3D扫描设备，这使得其难以扩展并引发版权问题。为了解决这些挑战，我们提出从程序化3D程序中学习3D表示，这些程序使用简单的原始体和增强技术自动生成3D形状。值得注意的是，尽管缺乏语义内容，但从合成数据集中学习的3D表示在各种下游3D任务中表现与从语义可识别的3D模型（如飞机）学习的最先进表示相当。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17451",
            "title": "VLRewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models",
            "url": "https://huggingface.co/papers/2411.17451",
            "abstract": "Vision-language generative reward models (VL-GenRMs) play a crucial role in aligning and evaluating multimodal AI systems, yet their own evaluation remains under-explored. Current assessment methods primarily rely on AI-annotated preference labels from traditional VL tasks, which can introduce biases and often fail to effectively challenge state-of-the-art models. To address these limitations, we introduce VL-RewardBench, a comprehensive benchmark spanning general multimodal queries, visual hallucination detection, and complex reasoning tasks. Through our AI-assisted annotation pipeline combining sample selection with human verification, we curate 1,250 high-quality examples specifically designed to probe model limitations. Comprehensive evaluation across 16 leading large vision-language models, demonstrates VL-RewardBench's effectiveness as a challenging testbed, where even GPT-4o achieves only 65.4% accuracy, and state-of-the-art open-source models such as Qwen2-VL-72B, struggle to surpass random-guessing. Importantly, performance on VL-RewardBench strongly correlates (Pearson's r > 0.9) with MMMU-Pro accuracy using Best-of-N sampling with VL-GenRMs. Analysis experiments uncover three critical insights for improving VL-GenRMs: (i) models predominantly fail at basic visual perception tasks rather than reasoning tasks; (ii) inference-time scaling benefits vary dramatically by model capacity; and (iii) training VL-GenRMs to learn to judge substantially boosts judgment capability (+14.7% accuracy for a 7B VL-GenRM). We believe VL-RewardBench along with the experimental insights will become a valuable resource for advancing VL-GenRMs.",
            "score": 7,
            "issue_id": 813,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "646be90925e52606",
            "authors": [
                "Lei Li",
                "Yuancheng Wei",
                "Zhihui Xie",
                "Xuqing Yang",
                "Yifan Song",
                "Peiyi Wang",
                "Chenxin An",
                "Tianyu Liu",
                "Sujian Li",
                "Bill Yuchen Lin",
                "Lingpeng Kong",
                "Qi Liu"
            ],
            "affiliations": [
                "AI2",
                "HKU",
                "PKU",
                "SCUT",
                "SJTU",
                "UW"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17451.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#hallucinations",
                    "#reasoning",
                    "#benchmark",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "VL-RewardBench: новый стандарт оценки визуально-языковых моделей вознаграждения",
                    "desc": "Статья представляет VL-RewardBench - новый бенчмарк для оценки генеративных моделей вознаграждения для визуально-языковых задач (VL-GenRMs). Авторы создали набор из 1250 высококачественных примеров, охватывающих различные аспекты мультимодальных запросов и рассуждений. Оценка 16 ведущих крупных визуально-языковых моделей показала эффективность VL-RewardBench как сложного тестового набора. Анализ результатов выявил ключевые проблемы и направления для улучшения VL-GenRMs."
                },
                "en": {
                    "title": "VL-RewardBench: Elevating Evaluation for Vision-Language Models",
                    "desc": "This paper introduces VL-RewardBench, a new benchmark designed to evaluate vision-language generative reward models (VL-GenRMs). The benchmark addresses the limitations of current evaluation methods that rely on biased AI-annotated preference labels. By curating 1,250 high-quality examples, VL-RewardBench effectively challenges state-of-the-art models and reveals their weaknesses in visual perception and reasoning tasks. The findings suggest that improving the training of VL-GenRMs can significantly enhance their judgment capabilities, making this benchmark a valuable tool for future research."
                },
                "zh": {
                    "title": "提升视觉-语言模型评估的全新基准",
                    "desc": "视觉-语言生成奖励模型（VL-GenRMs）在对齐和评估多模态人工智能系统中起着重要作用，但其自身的评估仍然未被充分探索。目前的评估方法主要依赖于传统视觉-语言任务的AI标注偏好标签，这可能引入偏见，并且往往无法有效挑战最先进的模型。为了解决这些局限性，我们引入了VL-RewardBench，这是一个涵盖一般多模态查询、视觉幻觉检测和复杂推理任务的综合基准。通过结合样本选择和人工验证的AI辅助注释流程，我们策划了1250个高质量示例，专门设计用于探测模型的局限性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15411",
            "title": "FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity",
            "url": "https://huggingface.co/papers/2411.15411",
            "abstract": "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal tasks, enabling more sophisticated and accurate reasoning across various applications, including image and video captioning, visual question answering, and cross-modal retrieval. Despite their superior capabilities, VLMs struggle with fine-grained image regional composition information perception. Specifically, they have difficulty accurately aligning the segmentation masks with the corresponding semantics and precisely describing the compositional aspects of the referred regions.   However, compositionality - the ability to understand and generate novel combinations of known visual and textual components - is critical for facilitating coherent reasoning and understanding across modalities by VLMs. To address this issue, we propose FINECAPTION, a novel VLM that can recognize arbitrary masks as referential inputs and process high-resolution images for compositional image captioning at different granularity levels. To support this endeavor, we introduce COMPOSITIONCAP, a new dataset for multi-grained region compositional image captioning, which introduces the task of compositional attribute-aware regional image captioning.   Empirical results demonstrate the effectiveness of our proposed model compared to other state-of-the-art VLMs. Additionally, we analyze the capabilities of current VLMs in recognizing various visual prompts for compositional region image captioning, highlighting areas for improvement in VLM design and training.",
            "score": 6,
            "issue_id": 803,
            "pub_date": "2024-11-23",
            "pub_date_card": {
                "ru": "23 ноября",
                "en": "November 23",
                "zh": "11月23日"
            },
            "hash": "54aae052c8dc586b",
            "authors": [
                "Hang Hua",
                "Qing Liu",
                "Lingzhi Zhang",
                "Jing Shi",
                "Zhifei Zhang",
                "Yilin Wang",
                "Jianming Zhang",
                "Jiebo Luo"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15411.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#games",
                    "#cv",
                    "#dataset",
                    "#reasoning"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Новый подход к композиционному описанию изображений с помощью усовершенствованных мультимодальных языковых моделей",
                    "desc": "В статье представлена новая модель FINECAPTION, способная распознавать произвольные маски и обрабатывать изображения высокого разрешения для композиционного описания изображений на разных уровнях детализации. Авторы также представили новый датасет COMPOSITIONCAP для многоуровневого композиционного описания регионов изображений. Эмпирические результаты демонстрируют эффективность предложенной модели по сравнению с другими современными мультимодальными языковыми моделями. В работе также проанализированы возможности существующих моделей в распознавании различных визуальных подсказок для композиционного описания регионов изображений."
                },
                "en": {
                    "title": "Enhancing Compositional Understanding in Vision-Language Models",
                    "desc": "This paper introduces FINECAPTION, a new Vision-Language Model (VLM) designed to enhance compositional image captioning by accurately recognizing and processing arbitrary segmentation masks. The model addresses the challenge of aligning image regions with their corresponding semantics, which is crucial for generating coherent captions. To facilitate this, the authors present COMPOSITIONCAP, a dataset that focuses on multi-grained region compositional image captioning, allowing for a deeper understanding of visual attributes. Empirical results show that FINECAPTION outperforms existing VLMs, while also identifying areas for further improvement in VLM capabilities."
                },
                "zh": {
                    "title": "提升视觉语言模型的组合能力",
                    "desc": "本文介绍了一种新的视觉语言模型FINECAPTION，旨在提高多模态任务中的图像区域组合信息感知能力。尽管现有的大型视觉语言模型在图像和视频描述等任务中表现出色，但它们在精确对齐分割掩码和语义方面存在困难。FINECAPTION能够识别任意掩码作为参考输入，并处理高分辨率图像，以实现不同粒度级别的组合图像描述。我们还提出了一个新数据集COMPOSITIONCAP，以支持多粒度区域组合图像描述任务，实验结果表明该模型在性能上优于其他先进的视觉语言模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16856",
            "title": "SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE",
            "url": "https://huggingface.co/papers/2411.16856",
            "abstract": "Autoregressive models have demonstrated remarkable success across various fields, from large language models (LLMs) to large multimodal models (LMMs) and 2D content generation, moving closer to artificial general intelligence (AGI). Despite these advances, applying autoregressive approaches to 3D object generation and understanding remains largely unexplored. This paper introduces Scale AutoRegressive 3D (SAR3D), a novel framework that leverages a multi-scale 3D vector-quantized variational autoencoder (VQVAE) to tokenize 3D objects for efficient autoregressive generation and detailed understanding. By predicting the next scale in a multi-scale latent representation instead of the next single token, SAR3D reduces generation time significantly, achieving fast 3D object generation in just 0.82 seconds on an A6000 GPU. Additionally, given the tokens enriched with hierarchical 3D-aware information, we finetune a pretrained LLM on them, enabling multimodal comprehension of 3D content. Our experiments show that SAR3D surpasses current 3D generation methods in both speed and quality and allows LLMs to interpret and caption 3D models comprehensively.",
            "score": 5,
            "issue_id": 804,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "953bc387802b7daf",
            "authors": [
                "Yongwei Chen",
                "Yushi Lan",
                "Shangchen Zhou",
                "Tengfei Wang",
                "XIngang Pan"
            ],
            "affiliations": [
                "S-Lab, Nanyang Technological University",
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16856.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#architecture",
                    "#games",
                    "#agi"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "SAR3D: Быстрая генерация и глубокое понимание 3D объектов",
                    "desc": "Статья представляет новый фреймворк SAR3D для генерации и понимания 3D объектов с использованием авторегрессионного подхода. SAR3D использует многомасштабный 3D векторно-квантованный вариационный автоэнкодер для токенизации 3D объектов. Это позволяет значительно ускорить генерацию 3D объектов до 0.82 секунд на GPU A6000. Кроме того, дообучение предобученной языковой модели на полученных токенах позволяет достичь мультимодального понимания 3D контента."
                },
                "en": {
                    "title": "Revolutionizing 3D Generation with SAR3D!",
                    "desc": "This paper presents Scale AutoRegressive 3D (SAR3D), a new framework designed for generating and understanding 3D objects using autoregressive models. It utilizes a multi-scale 3D vector-quantized variational autoencoder (VQVAE) to tokenize 3D objects, which allows for faster and more efficient generation. By predicting the next scale in a multi-scale latent representation, SAR3D significantly reduces the time required for 3D object generation. The framework also enhances large language models (LLMs) with the ability to comprehend and caption 3D content, outperforming existing methods in both speed and quality."
                },
                "zh": {
                    "title": "快速高效的3D物体生成与理解",
                    "desc": "自回归模型在多个领域取得了显著成功，包括大型语言模型和多模态模型。然而，将自回归方法应用于3D物体生成和理解的研究仍然相对较少。本文提出了一种新框架——规模自回归3D（SAR3D），利用多尺度3D向量量化变分自编码器（VQVAE）对3D物体进行标记，从而实现高效的自回归生成和详细理解。实验表明，SAR3D在速度和质量上均优于现有的3D生成方法，并使大型语言模型能够全面理解和描述3D模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17223",
            "title": "DreamMix: Decoupling Object Attributes for Enhanced Editability in Customized Image Inpainting",
            "url": "https://huggingface.co/papers/2411.17223",
            "abstract": "Subject-driven image inpainting has emerged as a popular task in image editing alongside recent advancements in diffusion models. Previous methods primarily focus on identity preservation but struggle to maintain the editability of inserted objects. In response, this paper introduces DreamMix, a diffusion-based generative model adept at inserting target objects into given scenes at user-specified locations while concurrently enabling arbitrary text-driven modifications to their attributes. In particular, we leverage advanced foundational inpainting models and introduce a disentangled local-global inpainting framework to balance precise local object insertion with effective global visual coherence. Additionally, we propose an Attribute Decoupling Mechanism (ADM) and a Textual Attribute Substitution (TAS) module to improve the diversity and discriminative capability of the text-based attribute guidance, respectively. Extensive experiments demonstrate that DreamMix effectively balances identity preservation and attribute editability across various application scenarios, including object insertion, attribute editing, and small object inpainting. Our code is publicly available at https://github.com/mycfhs/DreamMix.",
            "score": 5,
            "issue_id": 802,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "56eb0ccceda40f61",
            "authors": [
                "Yicheng Yang",
                "Pengxiang Li",
                "Lu Zhang",
                "Liqian Ma",
                "Ping Hu",
                "Siyu Du",
                "Yunzhi Zhuge",
                "Xu Jia",
                "Huchuan Lu"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "University of Electronic Science and Technology of China",
                "ZMO AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17223.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "DreamMix: Вставка и редактирование объектов на изображениях с помощью текста",
                    "desc": "DreamMix - это генеративная модель на основе диффузии для вставки объектов в изображения. Она позволяет не только сохранять идентичность объектов, но и редактировать их атрибуты с помощью текстовых запросов. Модель использует усовершенствованный подход к инпейнтингу, сочетающий локальную и глобальную обработку. Также в DreamMix применяется механизм разделения атрибутов и модуль текстовой подстановки атрибутов для улучшения разнообразия и точности управления свойствами объектов."
                },
                "en": {
                    "title": "DreamMix: Seamless Object Insertion and Attribute Editing in Images",
                    "desc": "This paper presents DreamMix, a novel diffusion-based generative model designed for subject-driven image inpainting. Unlike previous methods that prioritize identity preservation, DreamMix allows users to insert objects into images at specified locations while also enabling modifications to their attributes through text prompts. The model employs a disentangled local-global inpainting framework to ensure that inserted objects blend well with the overall scene. Additionally, it introduces mechanisms for attribute decoupling and textual attribute substitution to enhance the diversity and effectiveness of text-based guidance for editing."
                },
                "zh": {
                    "title": "DreamMix：智能图像修复与属性编辑的完美结合",
                    "desc": "本文介绍了一种名为DreamMix的扩散生成模型，旨在实现目标对象在指定场景中的插入，同时允许用户对其属性进行文本驱动的修改。与以往方法不同，DreamMix不仅关注身份保留，还能保持插入对象的可编辑性。我们采用了先进的基础修复模型，并引入了局部-全局分离修复框架，以平衡精确的局部对象插入和有效的全局视觉一致性。此外，本文还提出了属性解耦机制和文本属性替换模块，以提高基于文本的属性指导的多样性和区分能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17691",
            "title": "Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens",
            "url": "https://huggingface.co/papers/2411.17691",
            "abstract": "We reveal that low-bit quantization favors undertrained large language models (LLMs) by observing that models with larger sizes or fewer training tokens experience less quantization-induced degradation (QiD) when applying low-bit quantization, whereas smaller models with extensive training tokens suffer significant QiD. To gain deeper insights into this trend, we study over 1500 quantized LLM checkpoints of various sizes and at different training levels (undertrained or fully trained) in a controlled setting, deriving scaling laws for understanding the relationship between QiD and factors such as the number of training tokens, model size and bit width.   With the derived scaling laws, we propose a novel perspective that we can use QiD to measure an LLM's training levels and determine the number of training tokens required for fully training LLMs of various sizes. Moreover, we use the scaling laws to predict the quantization performance of different-sized LLMs trained with 100 trillion tokens. Our projection shows that the low-bit quantization performance of future models, which are expected to be trained with over 100 trillion tokens, may NOT be desirable. This poses a potential challenge for low-bit quantization in the future and highlights the need for awareness of a model's training level when evaluating low-bit quantization research. To facilitate future research on this problem, we release all the 1500+ quantized checkpoints used in this work at https://huggingface.co/Xu-Ouyang.",
            "score": 4,
            "issue_id": 804,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "3dfc643e242c0247",
            "authors": [
                "Xu Ouyang",
                "Tao Ge",
                "Thomas Hartvigsen",
                "Zhisong Zhang",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab",
                "University of Virginia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17691.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#dataset",
                    "#open_source",
                    "#inference"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Квантование раскрывает тайны обучения языковых моделей",
                    "desc": "Исследование показывает, что квантование с низким битрейтом менее вредно для недообученных больших языковых моделей (LLM), чем для полностью обученных. Авторы изучили более 1500 квантованных чекпоинтов LLM разных размеров и уровней обучения, выведя законы масштабирования для понимания связи между деградацией от квантования и такими факторами, как количество токенов обучения, размер модели и битовая ширина. На основе этих законов предложен новый подход к измерению уровня обучения LLM и определению необходимого количества токенов для полного обучения моделей разных размеров. Исследование также прогнозирует, что производительность квантования будущих моделей, обученных на более чем 100 триллионах токенов, может оказаться неудовлетворительной."
                },
                "en": {
                    "title": "Understanding Low-Bit Quantization Impact on LLMs",
                    "desc": "This paper investigates how low-bit quantization affects large language models (LLMs) based on their training levels and sizes. It finds that larger models or those with fewer training tokens are less impacted by quantization-induced degradation (QiD), while smaller models with extensive training suffer more. The authors analyze over 1500 quantized LLM checkpoints to derive scaling laws that relate QiD to model size and training tokens. They also predict that future models trained with 100 trillion tokens may face challenges with low-bit quantization performance, emphasizing the importance of understanding a model's training level in quantization research."
                },
                "zh": {
                    "title": "低位量化与模型训练水平的关系",
                    "desc": "本研究揭示了低位量化对未充分训练的大型语言模型（LLMs）的影响。我们发现，较大模型或训练样本较少的模型在应用低位量化时，量化引起的降级（QiD）较小，而小模型即使经过大量训练样本也会遭受显著的QiD。通过分析1500多个不同大小和训练水平的量化LLM检查点，我们推导出了一些规律，以理解QiD与训练样本数量、模型大小和位宽之间的关系。我们的研究表明，未来训练超过100万亿个样本的模型在低位量化性能上可能面临挑战，因此在评估低位量化研究时，需要关注模型的训练水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16173",
            "title": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis",
            "url": "https://huggingface.co/papers/2411.16173",
            "abstract": "Despite advances in Large Multi-modal Models, applying them to long and untrimmed video content remains challenging due to limitations in context length and substantial memory overhead. These constraints often lead to significant information loss and reduced relevance in the model responses. With the exponential growth of video data across web platforms, understanding long-form video is crucial for advancing generalized intelligence. In this paper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel video-LLM framework designed to enhance the comprehension of lengthy video content through targeted retrieval process. We address two main challenges to achieve it: (i) We present the SceneWalk dataset, a high-quality collection of 87.8K long videos, each densely captioned at the segment level to enable models to capture scene continuity and maintain rich descriptive context. (ii) We develop robust architectural designs integrating dynamic routing mechanism and spatio-temporal projector to efficiently retrieve and process relevant video segments based on user queries. Our framework mitigates the limitations of current video-LMMs by allowing for precise identification and retrieval of relevant video segments in response to queries, thereby improving the contextual relevance of the generated responses. Through extensive experiments, SALOVA demonstrates enhanced capability in processing complex long-form videos, showing significant capability to maintain contextual integrity across extended sequences.",
            "score": 3,
            "issue_id": 805,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "a5ec8ffe17a0004a",
            "authors": [
                "Junho Kim",
                "Hyunjun Kim",
                "Hosu Lee",
                "Yong Man Ro"
            ],
            "affiliations": [
                "Integrated Vision and Language Lab, KAIST, South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16173.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#video",
                    "#long_context",
                    "#multimodal",
                    "#dataset"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "SALOVA: умный помощник для анализа длинных видео",
                    "desc": "SALOVA - это новая система для обработки длинных видео с помощью больших мультимодальных моделей. Она решает проблему ограничений контекста и памяти при анализе длинных видео путем целевого извлечения релевантных сегментов. Система использует датасет SceneWalk с 87.8 тысячами длинных видео, размеченных на уровне сегментов. SALOVA применяет механизм динамической маршрутизации и пространственно-временной проектор для эффективной обработки запросов пользователей."
                },
                "en": {
                    "title": "Enhancing Long Video Comprehension with SALOVA",
                    "desc": "This paper presents SALOVA, a new framework designed to improve the understanding of long videos using large multi-modal models. It addresses the challenges of context length and memory overhead that often lead to information loss in video analysis. SALOVA utilizes the SceneWalk dataset, which contains 87.8K long videos with detailed segment-level captions, allowing models to better capture scene continuity. Additionally, it incorporates a dynamic routing mechanism and spatio-temporal projector to efficiently retrieve relevant video segments, enhancing the contextual relevance of model responses."
                },
                "zh": {
                    "title": "提升长视频理解的智能助手",
                    "desc": "尽管大型多模态模型取得了进展，但在处理长视频内容时仍面临挑战，主要是由于上下文长度和内存开销的限制。这些限制常常导致信息丢失和模型响应的相关性降低。为了解决这个问题，本文提出了SALOVA：一种新的视频-大语言模型框架，旨在通过目标检索过程增强对长视频内容的理解。我们引入了SceneWalk数据集和动态路由机制，以提高模型在处理复杂长视频时的能力，确保生成的响应在上下文上更具相关性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16801",
            "title": "Controllable Human Image Generation with Personalized Multi-Garments",
            "url": "https://huggingface.co/papers/2411.16801",
            "abstract": "We present BootComp, a novel framework based on text-to-image diffusion models for controllable human image generation with multiple reference garments. Here, the main bottleneck is data acquisition for training: collecting a large-scale dataset of high-quality reference garment images per human subject is quite challenging, i.e., ideally, one needs to manually gather every single garment photograph worn by each human. To address this, we propose a data generation pipeline to construct a large synthetic dataset, consisting of human and multiple-garment pairs, by introducing a model to extract any reference garment images from each human image. To ensure data quality, we also propose a filtering strategy to remove undesirable generated data based on measuring perceptual similarities between the garment presented in human image and extracted garment. Finally, by utilizing the constructed synthetic dataset, we train a diffusion model having two parallel denoising paths that use multiple garment images as conditions to generate human images while preserving their fine-grained details. We further show the wide-applicability of our framework by adapting it to different types of reference-based generation in the fashion domain, including virtual try-on, and controllable human image generation with other conditions, e.g., pose, face, etc.",
            "score": 2,
            "issue_id": 815,
            "pub_date": "2024-11-25",
            "pub_date_card": {
                "ru": "25 ноября",
                "en": "November 25",
                "zh": "11月25日"
            },
            "hash": "ef40f6dd8c1ccd32",
            "authors": [
                "Yisol Choi",
                "Sangkyung Kwak",
                "Sihyun Yu",
                "Hyungwon Choi",
                "Jinwoo Shin"
            ],
            "affiliations": [
                "KAIST",
                "OMNIOUS.AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16801.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#diffusion",
                    "#synthetic",
                    "#cv",
                    "#dataset"
                ],
                "emoji": "👚",
                "ru": {
                    "title": "BootComp: Генерация изображений людей с контролируемой одеждой без ручного сбора данных",
                    "desc": "В статье представлен BootComp - новый фреймворк для генерации изображений людей с контролируемой одеждой, основанный на диффузионных моделях преобразования текста в изображение. Авторы предлагают конвейер для создания синтетического набора данных, состоящего из пар 'человек-несколько предметов одежды', используя модель для извлечения эталонных изображений одежды из каждого изображения человека. Для обеспечения качества данных применяется стратегия фильтрации, основанная на измерении перцептивного сходства между одеждой на изображении человека и извлеченной одеждой. Обученная на этом наборе данных диффузионная модель имеет два параллельных пути шумоподавления, использующих несколько изображений одежды в качестве условий для генерации изображений людей."
                },
                "en": {
                    "title": "Revolutionizing Fashion with Synthetic Human Image Generation",
                    "desc": "BootComp is a new framework that uses text-to-image diffusion models to generate human images with specific garments. The challenge it addresses is the difficulty of collecting a large dataset of high-quality garment images for each individual. To solve this, BootComp creates a synthetic dataset by extracting garment images from human photos and filtering out low-quality data. The framework allows for the generation of detailed human images based on multiple garment references and can be adapted for various applications in the fashion industry, such as virtual try-ons."
                },
                "zh": {
                    "title": "BootComp：可控的人物图像生成新框架",
                    "desc": "我们提出了BootComp，这是一个基于文本到图像扩散模型的框架，用于可控的人物图像生成，支持多种参考服装。主要的瓶颈在于数据获取，收集每个人体的高质量参考服装图像数据集非常具有挑战性。为了解决这个问题，我们提出了一种数据生成管道，通过引入模型从每个人体图像中提取参考服装图像，构建一个大型合成数据集。最后，我们利用构建的合成数据集训练了一个扩散模型，具有两个并行去噪路径，能够在保留细节的同时生成带有多种服装条件的人物图像。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.14721",
            "title": "MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts",
            "url": "https://huggingface.co/papers/2411.14721",
            "abstract": "Molecule discovery is a pivotal research field, impacting everything from the medicines we take to the materials we use. Recently, Large Language Models (LLMs) have been widely adopted in molecule understanding and generation, yet the alignments between molecules and their corresponding captions remain a significant challenge. Previous endeavours often treat the molecule as a general SMILES string or molecular graph, neglecting the fine-grained alignments between the molecular sub-structures and the descriptive textual phrases, which are crucial for accurate and explainable predictions. In this case, we introduce MolReFlect, a novel teacher-student framework designed to contextually perform the molecule-caption alignments in a fine-grained way. Our approach initially leverages a larger teacher LLM to label the detailed alignments by directly extracting critical phrases from molecule captions or SMILES strings and implying them to corresponding sub-structures or characteristics. To refine these alignments, we propose In-Context Selective Reflection, which retrieves previous extraction results as context examples for teacher LLM to reflect and lets a smaller student LLM select from in-context reflection and previous extraction results. Finally, we enhance the learning process of the student LLM through Chain-of-Thought In-Context Molecule Tuning, integrating the fine-grained alignments and the reasoning processes within the Chain-of-Thought format. Our experimental results demonstrate that MolReFlect enables LLMs like Mistral-7B to significantly outperform the previous baselines, achieving SOTA performance on the ChEBI-20 dataset. This advancement not only enhances the generative capabilities of LLMs in the molecule-caption translation task, but also contributes to a more explainable framework.",
            "score": 2,
            "issue_id": 811,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "7b85c924ec519ef3",
            "authors": [
                "Jiatong Li",
                "Yunqing Liu",
                "Wei Liu",
                "Jingdi Le",
                "Di Zhang",
                "Wenqi Fan",
                "Dongzhan Zhou",
                "Yuqiang Li",
                "Qing Li"
            ],
            "affiliations": [
                "Shanghai AI Lab",
                "Shanghai Jiao Tong University",
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.14721.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#multimodal",
                    "#dataset",
                    "#interpretability",
                    "#reasoning",
                    "#training"
                ],
                "emoji": "🧪",
                "ru": {
                    "title": "MolReFlect: Точное соответствие молекул и текста с помощью LLM",
                    "desc": "Статья представляет MolReFlect - новый метод для улучшения соответствия между молекулами и их текстовыми описаниями с использованием больших языковых моделей (LLM). Авторы предлагают подход учитель-ученик, где большая модель-учитель выделяет ключевые фразы и соотносит их с подструктурами молекул, а меньшая модель-ученик уточняет эти соответствия. Метод включает в себя контекстную селективную рефлексию и обучение с использованием цепочки рассуждений. Эксперименты показывают, что MolReFlect значительно превосходит предыдущие подходы на датасете ChEBI-20, улучшая генеративные способности LLM в задаче перевода между молекулами и их описаниями."
                },
                "en": {
                    "title": "Enhancing Molecule-Caption Alignment with MolReFlect",
                    "desc": "This paper presents MolReFlect, a new framework that improves the alignment between molecules and their descriptive captions using Large Language Models (LLMs). It addresses the challenge of fine-grained alignments by employing a teacher-student model where a larger LLM guides a smaller one in understanding molecular sub-structures and their corresponding textual descriptions. The method includes In-Context Selective Reflection to refine alignments and Chain-of-Thought In-Context Molecule Tuning to enhance the learning process. Experimental results show that MolReFlect achieves state-of-the-art performance on the ChEBI-20 dataset, making LLMs more effective and explainable in molecule-caption tasks."
                },
                "zh": {
                    "title": "MolReFlect：提升分子与描述对齐的智能框架",
                    "desc": "分子发现是一个重要的研究领域，影响着我们所用的药物和材料。本文提出了一种新的教师-学生框架MolReFlect，旨在细致地对分子与其对应的描述进行对齐。我们的方法利用大型语言模型（LLM）来标记详细的对齐信息，并通过上下文选择性反思来优化这些对齐。实验结果表明，MolReFlect显著提升了LLM在分子-描述翻译任务中的表现，提供了更具解释性的框架。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.15241",
            "title": "EfficientViM: Efficient Vision Mamba with Hidden State Mixer based State Space Duality",
            "url": "https://huggingface.co/papers/2411.15241",
            "abstract": "For the deployment of neural networks in resource-constrained environments, prior works have built lightweight architectures with convolution and attention for capturing local and global dependencies, respectively. Recently, the state space model has emerged as an effective global token interaction with its favorable linear computational cost in the number of tokens. Yet, efficient vision backbones built with SSM have been explored less. In this paper, we introduce Efficient Vision Mamba (EfficientViM), a novel architecture built on hidden state mixer-based state space duality (HSM-SSD) that efficiently captures global dependencies with further reduced computational cost. In the HSM-SSD layer, we redesign the previous SSD layer to enable the channel mixing operation within hidden states. Additionally, we propose multi-stage hidden state fusion to further reinforce the representation power of hidden states, and provide the design alleviating the bottleneck caused by the memory-bound operations. As a result, the EfficientViM family achieves a new state-of-the-art speed-accuracy trade-off on ImageNet-1k, offering up to a 0.7% performance improvement over the second-best model SHViT with faster speed. Further, we observe significant improvements in throughput and accuracy compared to prior works, when scaling images or employing distillation training. Code is available at https://github.com/mlvlab/EfficientViM.",
            "score": 1,
            "issue_id": 826,
            "pub_date": "2024-11-22",
            "pub_date_card": {
                "ru": "22 ноября",
                "en": "November 22",
                "zh": "11月22日"
            },
            "hash": "4bc18d66c219c8cb",
            "authors": [
                "Sanghyeok Lee",
                "Joonmyung Choi",
                "Hyunwoo J. Kim"
            ],
            "affiliations": [
                "Department of Computer Science and Engineering, Korea University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.15241.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#training",
                    "#cv"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "EfficientViM: Прорыв в эффективности обработки изображений",
                    "desc": "Статья представляет EfficientViM - новую архитектуру нейронной сети для обработки изображений в условиях ограниченных ресурсов. Она основана на модели пространства состояний с двойственностью (HSM-SSD) для эффективного захвата глобальных зависимостей. Авторы предлагают многоступенчатое слияние скрытых состояний и оптимизацию операций с памятью. EfficientViM достигает нового уровня соотношения скорость-точность на ImageNet-1k, превосходя существующие модели."
                },
                "en": {
                    "title": "EfficientViM: Redefining Speed-Accuracy in Resource-Constrained Vision Tasks",
                    "desc": "This paper presents Efficient Vision Mamba (EfficientViM), a new neural network architecture designed for efficient deployment in environments with limited resources. It utilizes a hidden state mixer-based state space duality (HSM-SSD) to effectively capture global dependencies while minimizing computational costs. The architecture introduces a channel mixing operation within hidden states and employs multi-stage hidden state fusion to enhance representation power. As a result, EfficientViM achieves a state-of-the-art balance between speed and accuracy on the ImageNet-1k dataset, outperforming existing models with improved throughput and accuracy."
                },
                "zh": {
                    "title": "高效视觉架构：速度与准确性的完美平衡",
                    "desc": "本论文提出了一种新颖的高效视觉架构，称为Efficient Vision Mamba（EfficientViM），旨在在资源受限的环境中有效部署神经网络。我们引入了基于隐藏状态混合的状态空间双重性（HSM-SSD），通过重新设计SSD层来实现隐藏状态中的通道混合操作，从而更高效地捕捉全局依赖关系。通过多阶段隐藏状态融合，我们进一步增强了隐藏状态的表示能力，并缓解了内存限制操作带来的瓶颈。实验结果表明，EfficientViM在ImageNet-1k上实现了新的速度-准确性平衡，相较于第二名模型SHViT，性能提升达0.7%，且在图像缩放和蒸馏训练中表现出显著的吞吐量和准确性提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.16754",
            "title": "Visual Counter Turing Test (VCT^2): Discovering the Challenges for AI-Generated Image Detection and Introducing Visual AI Index (V_AI)",
            "url": "https://huggingface.co/papers/2411.16754",
            "abstract": "The proliferation of AI techniques for image generation, coupled with their increasing accessibility, has raised significant concerns about the potential misuse of these images to spread misinformation. Recent AI-generated image detection (AGID) methods include CNNDetection, NPR, DM Image Detection, Fake Image Detection, DIRE, LASTED, GAN Image Detection, AIDE, SSP, DRCT, RINE, OCC-CLIP, De-Fake, and Deep Fake Detection. However, we argue that the current state-of-the-art AGID techniques are inadequate for effectively detecting contemporary AI-generated images and advocate for a comprehensive reevaluation of these methods. We introduce the Visual Counter Turing Test (VCT^2), a benchmark comprising ~130K images generated by contemporary text-to-image models (Stable Diffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALL-E 3, and Midjourney 6). VCT^2 includes two sets of prompts sourced from tweets by the New York Times Twitter account and captions from the MS COCO dataset. We also evaluate the performance of the aforementioned AGID techniques on the VCT^2 benchmark, highlighting their ineffectiveness in detecting AI-generated images. As image-generative AI models continue to evolve, the need for a quantifiable framework to evaluate these models becomes increasingly critical. To meet this need, we propose the Visual AI Index (V_AI), which assesses generated images from various visual perspectives, including texture complexity and object coherence, setting a new standard for evaluating image-generative AI models. To foster research in this domain, we make our https://huggingface.co/datasets/anonymous1233/COCO_AI and https://huggingface.co/datasets/anonymous1233/twitter_AI datasets publicly available.",
            "score": 1,
            "issue_id": 805,
            "pub_date": "2024-11-24",
            "pub_date_card": {
                "ru": "24 ноября",
                "en": "November 24",
                "zh": "11月24日"
            },
            "hash": "cbbd14594d775cab",
            "authors": [
                "Nasrin Imanpour",
                "Shashwat Bajpai",
                "Subhankar Ghosh",
                "Sainath Reddy Sankepally",
                "Abhilekh Borah",
                "Hasnat Md Abdullah",
                "Nishoak Kosaraju",
                "Shreyas Dixit",
                "Ashhar Aziz",
                "Shwetangshu Biswas",
                "Vinija Jain",
                "Aman Chadha",
                "Amit Sheth",
                "Amitava Das"
            ],
            "affiliations": [
                "Amazon AI, USA",
                "Amazon GenAI, USA",
                "BITS Pilani Hyderabad Campus, India",
                "Carnegie Mellon University, USA",
                "IIIT Delhi, India",
                "International Institute of Information Technology, India",
                "Manipal University Jaipur, India",
                "National Institute of Technology Silchar, India",
                "Stanford University, USA",
                "Texas A&M University, USA",
                "University of South Carolina, USA",
                "Vishwakarma Institute of Information Technology, India",
                "Washington State University, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.16754.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#cv",
                    "#benchmark",
                    "#security",
                    "#ethics",
                    "#dataset"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Новый подход к выявлению ИИ-генерированных изображений",
                    "desc": "В статье рассматривается проблема обнаружения изображений, сгенерированных искусственным интеллектом (ИИ), в контексте распространения дезинформации. Авторы представляют новый бенчмарк Visual Counter Turing Test (VCT^2), содержащий около 130 тысяч изображений, созданных современными моделями text-to-image. Исследование показывает неэффективность существующих методов обнаружения ИИ-генерированных изображений на этом бенчмарке. Предлагается новый стандарт оценки моделей генерации изображений - Visual AI Index (V_AI), учитывающий различные визуальные аспекты."
                },
                "en": {
                    "title": "Rethinking AI Image Detection: Introducing VCT² and V_AI",
                    "desc": "This paper discusses the challenges of detecting AI-generated images, which have become more prevalent and accessible. It critiques existing AI-generated image detection (AGID) methods, asserting that they are insufficient for identifying modern images created by advanced models. The authors introduce the Visual Counter Turing Test (VCT^2), a new benchmark with around 130,000 images to evaluate the effectiveness of current AGID techniques. Additionally, they propose the Visual AI Index (V_AI) to provide a comprehensive framework for assessing image quality from multiple visual aspects, aiming to improve the evaluation of generative AI models."
                },
                "zh": {
                    "title": "提升AI图像检测，构建新标准！",
                    "desc": "随着人工智能图像生成技术的普及，滥用这些图像传播虚假信息的风险日益增加。现有的人工智能生成图像检测方法（AGID）如CNNDetection和Deep Fake Detection等，已被证明在检测现代AI生成图像方面效果不佳。为此，本文提出了视觉反图灵测试（VCT^2），该基准包含约13万张由最新文本到图像模型生成的图像，并评估了现有AGID技术在此基准上的表现。我们还提出了视觉人工智能指数（V_AI），为评估图像生成AI模型提供了新的标准，强调了从多个视觉角度评估生成图像的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17383",
            "title": "AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation",
            "url": "https://huggingface.co/papers/2411.17383",
            "abstract": "The automatic generation of anchor-style product promotion videos presents promising opportunities in online commerce, advertising, and consumer engagement. However, this remains a challenging task despite significant advancements in pose-guided human video generation. In addressing this challenge, we identify the integration of human-object interactions (HOI) into pose-guided human video generation as a core issue. To this end, we introduce AnchorCrafter, a novel diffusion-based system designed to generate 2D videos featuring a target human and a customized object, achieving high visual fidelity and controllable interactions. Specifically, we propose two key innovations: the HOI-appearance perception, which enhances object appearance recognition from arbitrary multi-view perspectives and disentangles object and human appearance, and the HOI-motion injection, which enables complex human-object interactions by overcoming challenges in object trajectory conditioning and inter-occlusion management. Additionally, we introduce the HOI-region reweighting loss, a training objective that enhances the learning of object details. Extensive experiments demonstrate that our proposed system outperforms existing methods in preserving object appearance and shape awareness, while simultaneously maintaining consistency in human appearance and motion. Project page: https://cangcz.github.io/Anchor-Crafter/",
            "score": 1,
            "issue_id": 803,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "8deec5510490c901",
            "authors": [
                "Ziyi Xu",
                "Ziyao Huang",
                "Juan Cao",
                "Yong Zhang",
                "Xiaodong Cun",
                "Qing Shuai",
                "Yuchen Wang",
                "Linchao Bao",
                "Jintao Li",
                "Fan Tang"
            ],
            "affiliations": [
                "Great Bay University",
                "Institute of Computing Technology, Chinese Academy of Sciences",
                "Meituan",
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17383.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#cv",
                    "#diffusion"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "AnchorCrafter: ИИ создает реалистичные промо-видео с взаимодействием человека и товара",
                    "desc": "Статья представляет AnchorCrafter - новую систему на основе диффузии для генерации видео с взаимодействием человека и объекта. Система решает проблему интеграции взаимодействий человека с объектами в генерацию видео на основе поз. AnchorCrafter использует инновационные подходы HOI-appearance perception и HOI-motion injection для улучшения распознавания внешнего вида объекта и управления сложными взаимодействиями. Эксперименты показывают, что система превосходит существующие методы в сохранении внешнего вида объекта при поддержании согласованности внешнего вида и движений человека."
                },
                "en": {
                    "title": "Revolutionizing Product Promotion with AnchorCrafter!",
                    "desc": "This paper presents AnchorCrafter, a new system for generating promotional videos that feature a person interacting with a product. It focuses on improving how humans and objects are represented together in videos by using advanced techniques in pose-guided video generation. The system introduces two main innovations: one that improves how objects are recognized from different angles and another that allows for realistic interactions between humans and objects. The results show that AnchorCrafter produces videos with better object appearance and human motion consistency compared to existing methods."
                },
                "zh": {
                    "title": "锚点风格视频生成的新突破",
                    "desc": "本论文提出了一种名为AnchorCrafter的新型扩散系统，旨在自动生成锚点风格的产品推广视频。该系统通过整合人-物交互（HOI）来提升基于姿态的人类视频生成效果，解决了复杂的人-物交互问题。我们提出了HOI-外观感知和HOI-运动注入两个关键创新，前者改善了物体外观的识别，后者则增强了人-物交互的复杂性。实验结果表明，AnchorCrafter在物体外观和形状保持方面优于现有方法，同时保持了人类外观和运动的一致性。"
                }
            }
        }
    ],
    "link_prev": "2024-11-27.html",
    "link_next": "2024-11-29.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "27.11",
        "en": "11/27",
        "zh": "11月27日"
    },
    "short_date_next": {
        "ru": "29.11",
        "en": "11/29",
        "zh": "11月29日"
    },
    "categories": {
        "#dataset": 8,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 2,
        "#cv": 9,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 3,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 10,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 2,
        "#games": 4,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 5,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了一种新的图形用户界面（GUI）助手模型，名为ShowUI。它结合了视觉、语言和动作，旨在提高人类工作流的生产力。ShowUI通过UI引导的视觉标记选择、交错的视觉-语言-动作流和小规模高质量的GUI指令跟随数据集来减少计算成本并提高训练效率。该模型在零样本截图定位任务中取得了75.1%的准确率，并在多个环境中表现出色。模型的代码已在GitHub上公开。",
        "title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent",
        "pinyin": "这篇文章介绍了一种新的图形用户界面（GUI）助手模型，名为ShowUI。它结合了视觉、语言和动作，旨在提高人类工作流的生产力。ShowUI通过UI引导的视觉标记选择、交错的视觉-语言-动作流和小规模高质量的GUI指令跟随数据集来减少计算成本并提高训练效率。该模型在零样本截图定位任务中取得了75.1%的准确率，并在多个环境中表现出色。模型的代码已在GitHub上公开。\n\nzhè piān wén zhāng jiè shào le yī zhǒng xīn de tú xíng yòng hù jiē miàn (GUI) zhù shǒu mó xíng, míng wèi ShowUI. tā jié hé le shì jué, yǔ yán hé dòng zuò, zhǐ zài tí gāo rén lèi gōng zuò liú de shēng chǎn lì. ShowUI tōng guò UI yǐn dǎo de shì jué biāo jì xuǎn zé, jiāo cuò de shì jué-yǔ yán-dòng zuò liú hé xiǎo guī mó gāo zhì liàng de GUI zhǐ lìng gēn suí shù jù jiǎng shǎo jì suàn chéng běn bìng tí gāo xùn liàn xiào yì. gǎi mó xíng zài líng yàng bǎn jié dìng wèi rèn wù zhōng qǔ dé le 75.1% de zhǔn què lǜ, bìng zài duō gè huán jìng zhōng biǎo xiàn chū sè. mó xíng de dài mǎ yǐ zài GitHub shàng gōng kāi.",
        "vocab": "[{'word': '图形用户界面', 'pinyin': 'tú xíng yòng hù jiē miàn', 'trans': 'graphical user interface'},\n{'word': '助手', 'pinyin': 'zhù shǒu', 'trans': 'assistant'},\n{'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'},\n{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'vision'},\n{'word': '语言', 'pinyin': 'yǔ yán', 'trans': 'language'},\n{'word': '动作', 'pinyin': 'dòng zuò', 'trans': 'action'},\n{'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'},\n{'word': '生产力', 'pinyin': 'shēng chǎn lì', 'trans': 'productivity'},\n{'word': '引导', 'pinyin': 'yǐn dǎo', 'trans': 'guide'},\n{'word': '标记', 'pinyin': 'biāo jì', 'trans': 'mark'},\n{'word': '选择', 'pinyin': 'xuǎn zé', 'trans': 'selection'},\n{'word': '交错', 'pinyin': 'jiāo cuò', 'trans': 'interleave'},\n{'word': '流', 'pinyin': 'liú', 'trans': 'flow'},\n{'word': '规模', 'pinyin': 'guī mó', 'trans': 'scale'},\n{'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'},\n{'word': '指令', 'pinyin': 'zhǐ lìng', 'trans': 'command'},\n{'word': '跟随', 'pinyin': 'gēn suí', 'trans': 'follow'},\n{'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'},\n{'word': '计算', 'pinyin': 'jì suàn', 'trans': 'computation'},\n{'word': '成本', 'pinyin': 'chéng běn', 'trans': 'cost'},\n{'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'training'},\n{'word': '效率', 'pinyin': 'xiào lǜ', 'trans': 'efficiency'},\n{'word': '零样本', 'pinyin': 'líng yàng běn', 'trans': 'zero-shot'},\n{'word': '截图', 'pinyin': 'jié tú', 'trans': 'screenshot'},\n{'word': '定位', 'pinyin': 'dìng wèi', 'trans': 'localization'},\n{'word': '任务', 'pinyin': 'rèn wu', 'trans': 'task'},\n{'word': '准确率', 'pinyin': 'zhǔn què lǜ', 'trans': 'accuracy'},\n{'word': '环境', 'pinyin': 'huán jìng', 'trans': 'environment'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'},\n{'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'},\n{'word': '公开', 'pinyin': 'gōng kāi', 'trans': 'public'},\n{'word': 'GitHub', 'pinyin': 'GitHub', 'trans': 'GitHub'}]",
        "trans": "This article introduces a new graphical user interface (GUI) assistant model called ShowUI. It combines vision, language, and action to enhance the productivity of human workflows. ShowUI reduces computational costs and improves training efficiency through UI-guided visual mark selection, interleaved vision-language-action flows, and a small-scale, high-quality GUI instruction-following dataset. The model achieved a 75.1% accuracy rate in zero-shot screenshot localization tasks and performed well in multiple environments. The model's code has been made publicly available on GitHub.",
        "update_ts": "2024-11-27 09:11"
    }
}