{
    "date": {
        "ru": "28 ноября",
        "en": "November 28",
        "zh": "11月28日"
    },
    "time_utc": "2024-11-28 02:21",
    "weekday": 3,
    "issue_id": 827,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.17949",
            "title": "ROICtrl: Boosting Instance Control for Visual Generation",
            "url": "https://huggingface.co/papers/2411.17949",
            "abstract": "Natural language often struggles to accurately associate positional and attribute information with multiple instances, which limits current text-based visual generation models to simpler compositions featuring only a few dominant instances. To address this limitation, this work enhances diffusion models by introducing regional instance control, where each instance is governed by a bounding box paired with a free-form caption. Previous methods in this area typically rely on implicit position encoding or explicit attention masks to separate regions of interest (ROIs), resulting in either inaccurate coordinate injection or large computational overhead. Inspired by ROI-Align in object detection, we introduce a complementary operation called ROI-Unpool. Together, ROI-Align and ROI-Unpool enable explicit, efficient, and accurate ROI manipulation on high-resolution feature maps for visual generation. Building on ROI-Unpool, we propose ROICtrl, an adapter for pretrained diffusion models that enables precise regional instance control. ROICtrl is compatible with community-finetuned diffusion models, as well as with existing spatial-based add-ons (\\eg, ControlNet, T2I-Adapter) and embedding-based add-ons (\\eg, IP-Adapter, ED-LoRA), extending their applications to multi-instance generation. Experiments show that ROICtrl achieves superior performance in regional instance control while significantly reducing computational costs.",
            "score": 2,
            "issue_id": 827,
            "pub_date": "2024-11-27",
            "pub_date_card": {
                "ru": "27 ноября",
                "en": "November 27",
                "zh": "11月27日"
            },
            "hash": "55ffbda778f2f640",
            "authors": [
                "Yuchao Gu",
                "Yipin Zhou",
                "Yunfan Ye",
                "Yixin Nie",
                "Licheng Yu",
                "Pingchuan Ma",
                "Kevin Qinghong Lin",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "GenAI, Meta",
                "MIT",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17949.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Точный контроль над областями в генеративных моделях изображений",
                    "desc": "Статья представляет новый метод улучшения диффузионных моделей для генерации изображений - ROICtrl. Он позволяет точно контролировать отдельные области изображения с помощью ограничивающих рамок и подписей. ROICtrl использует операции ROI-Align и ROI-Unpool для эффективной работы с высокоразрешающими картами признаков. Метод совместим с существующими дополнениями к диффузионным моделям и значительно снижает вычислительные затраты при генерации сложных многообъектных сцен."
                },
                "en": {
                    "title": "Enhancing Visual Generation with Precise Regional Control",
                    "desc": "This paper presents a new method to improve text-based visual generation models by allowing better control over multiple instances in images. It introduces a technique called ROICtrl, which uses bounding boxes and captions to manage regions of interest (ROIs) more effectively. The method combines ROI-Align and a new operation called ROI-Unpool to manipulate high-resolution feature maps accurately and efficiently. Experiments demonstrate that ROICtrl enhances performance in generating images with multiple instances while lowering computational costs."
                },
                "zh": {
                    "title": "区域实例控制，提升视觉生成精度",
                    "desc": "这篇论文提出了一种改进扩散模型的方法，通过引入区域实例控制来解决自然语言在多实例位置和属性信息关联上的不足。每个实例由一个边界框和一个自由形式的描述配对，从而实现更精确的控制。论文中介绍的ROI-Unpool操作与ROI-Align相结合，使得在高分辨率特征图上进行区域操作变得高效且准确。实验结果表明，ROICtrl在区域实例控制方面表现优越，同时显著降低了计算成本。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17440",
            "title": "Identity-Preserving Text-to-Video Generation by Frequency Decomposition",
            "url": "https://huggingface.co/papers/2411.17440",
            "abstract": "Identity-preserving text-to-video (IPT2V) generation aims to create high-fidelity videos with consistent human identity. It is an important task in video generation but remains an open problem for generative models. This paper pushes the technical frontier of IPT2V in two directions that have not been resolved in literature: (1) A tuning-free pipeline without tedious case-by-case finetuning, and (2) A frequency-aware heuristic identity-preserving DiT-based control scheme. We propose ConsisID, a tuning-free DiT-based controllable IPT2V model to keep human identity consistent in the generated video. Inspired by prior findings in frequency analysis of diffusion transformers, it employs identity-control signals in the frequency domain, where facial features can be decomposed into low-frequency global features and high-frequency intrinsic features. First, from a low-frequency perspective, we introduce a global facial extractor, which encodes reference images and facial key points into a latent space, generating features enriched with low-frequency information. These features are then integrated into shallow layers of the network to alleviate training challenges associated with DiT. Second, from a high-frequency perspective, we design a local facial extractor to capture high-frequency details and inject them into transformer blocks, enhancing the model's ability to preserve fine-grained features. We propose a hierarchical training strategy to leverage frequency information for identity preservation, transforming a vanilla pre-trained video generation model into an IPT2V model. Extensive experiments demonstrate that our frequency-aware heuristic scheme provides an optimal control solution for DiT-based models. Thanks to this scheme, our ConsisID generates high-quality, identity-preserving videos, making strides towards more effective IPT2V.",
            "score": 1,
            "issue_id": 827,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "28823ea0e7fa6b0c",
            "authors": [
                "Shenghai Yuan",
                "Jinfa Huang",
                "Xianyi He",
                "Yunyuan Ge",
                "Yujun Shi",
                "Liuhan Chen",
                "Jiebo Luo",
                "Li Yuan"
            ],
            "affiliations": [
                "National University of Singapore",
                "Peking University",
                "Peng Cheng Laboratory",
                "University of Rochester"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17440.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#diffusion",
                    "#optimization",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Сохранение идентичности в видео через частотный анализ",
                    "desc": "Статья представляет ConsisID - модель для генерации видео с сохранением идентичности человека на основе диффузионных трансформеров. Авторы предлагают частотно-ориентированный подход, разделяя facial features на низкочастотные глобальные и высокочастотные локальные компоненты. Модель использует глобальный и локальный экстракторы лиц для сохранения идентичности в разных частотных диапазонах. Предложенная иерархическая стратегия обучения позволяет эффективно использовать частотную информацию для сохранения идентичности в генерируемом видео."
                },
                "en": {
                    "title": "ConsisID: Seamless Identity Preservation in Video Generation",
                    "desc": "This paper presents a novel approach to identity-preserving text-to-video (IPT2V) generation, focusing on maintaining consistent human identity in videos. The authors introduce ConsisID, a controllable model that operates without the need for extensive fine-tuning, simplifying the process of video generation. By utilizing a frequency-aware heuristic, the model effectively separates and processes low-frequency global features and high-frequency intrinsic details of facial characteristics. The proposed hierarchical training strategy enhances the model's performance, allowing it to generate high-fidelity videos while preserving individual identities throughout the video sequence."
                },
                "zh": {
                    "title": "高保真身份保持视频生成的突破",
                    "desc": "身份保持的文本到视频生成（IPT2V）旨在创建具有一致人类身份的高保真视频。本文提出了一种名为ConsisID的模型，能够在不需要繁琐微调的情况下，保持生成视频中的人类身份一致性。该模型利用频率分析的方法，将人脸特征分解为低频和高频信息，从而有效地提取和保留细致的面部特征。通过层次化的训练策略，ConsisID在视频生成任务中实现了更好的身份保持效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.17945",
            "title": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation",
            "url": "https://huggingface.co/papers/2411.17945",
            "abstract": "Generating high-fidelity 3D content from text prompts remains a significant challenge in computer vision due to the limited size, diversity, and annotation depth of the existing datasets. To address this, we introduce MARVEL-40M+, an extensive dataset with 40 million text annotations for over 8.9 million 3D assets aggregated from seven major 3D datasets. Our contribution is a novel multi-stage annotation pipeline that integrates open-source pretrained multi-view VLMs and LLMs to automatically produce multi-level descriptions, ranging from detailed (150-200 words) to concise semantic tags (10-20 words). This structure supports both fine-grained 3D reconstruction and rapid prototyping. Furthermore, we incorporate human metadata from source datasets into our annotation pipeline to add domain-specific information in our annotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D, a two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our annotations and use a pretrained image-to-3D network to generate 3D textured meshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly outperforms existing datasets in annotation quality and linguistic diversity, achieving win rates of 72.41% by GPT-4 and 73.40% by human evaluators.",
            "score": 1,
            "issue_id": 827,
            "pub_date": "2024-11-26",
            "pub_date_card": {
                "ru": "26 ноября",
                "en": "November 26",
                "zh": "11月26日"
            },
            "hash": "360ba2514eea161e",
            "authors": [
                "Sankalp Sinha",
                "Mohammad Sadil Khan",
                "Muhammad Usama",
                "Shino Sam",
                "Didier Stricker",
                "Sk Aziz Ali",
                "Muhammad Zeshan Afzal"
            ],
            "affiliations": [
                "BITS Pilani, Hyderabad",
                "DFKI",
                "MindGarage",
                "RPTU Kaiserslautern-Landau"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.17945.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#open_source",
                    "#diffusion",
                    "#hallucinations",
                    "#dataset"
                ],
                "emoji": "🌟",
                "ru": {
                    "title": "Революция в 3D: от текста к реалистичным объектам",
                    "desc": "Статья представляет MARVEL-40M+, масштабный набор данных с 40 миллионами текстовых аннотаций для более чем 8,9 миллионов 3D-объектов. Авторы разработали многоэтапный конвейер аннотаций, использующий предобученные мультимодальные языковые модели и большие языковые модели для создания многоуровневых описаний. Набор данных значительно превосходит существующие по качеству аннотаций и лингвистическому разнообразию. Также представлен MARVEL-FX3D - двухэтапный конвейер для генерации 3D-контента из текстовых запросов."
                },
                "en": {
                    "title": "Unlocking 3D Creation with MARVEL-40M+: A New Era in Text-to-3D Generation",
                    "desc": "This paper presents MARVEL-40M+, a new dataset designed to improve the generation of 3D content from text prompts. It includes 40 million text annotations for 8.9 million 3D assets, created using a multi-stage annotation pipeline that leverages pretrained vision-language models (VLMs) and large language models (LLMs). The dataset enhances 3D reconstruction and prototyping by providing detailed and concise descriptions, while also incorporating human metadata to reduce inaccuracies in the generated annotations. Additionally, the authors introduce MARVEL-FX3D, a text-to-3D pipeline that efficiently generates 3D meshes, demonstrating superior annotation quality and diversity compared to existing datasets."
                },
                "zh": {
                    "title": "MARVEL-40M+: 高质量3D内容生成的新纪元",
                    "desc": "本文介绍了MARVEL-40M+数据集，该数据集包含4000万条文本注释，涵盖890万件3D资产，旨在解决现有数据集在规模和多样性上的不足。我们提出了一种新颖的多阶段注释流程，结合了开源的多视角视觉语言模型（VLM）和大型语言模型（LLM），自动生成多层次的描述。通过引入人类元数据，我们增强了注释的领域特定信息，减少了VLM的幻觉现象。此外，我们开发了MARVEL-FX3D，一个两阶段的文本到3D生成管道，能够快速生成高质量的3D网格。"
                }
            }
        }
    ],
    "link_prev": "2024-11-27.html",
    "link_next": "2024-11-29.html",
    "link_month": "2024-11.html",
    "short_date_prev": {
        "ru": "27.11",
        "en": "11/27",
        "zh": "11月27日"
    },
    "short_date_next": {
        "ru": "29.11",
        "en": "11/29",
        "zh": "11月29日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 0,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 3,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的图形用户界面（GUI）助手模型，名为ShowUI。它结合了视觉、语言和动作，旨在提高人类工作流的生产力。ShowUI通过UI引导的视觉标记选择、交错的视觉-语言-动作流和小规模高质量的GUI指令跟随数据集来减少计算成本并提高训练效率。该模型在零样本截图定位任务中取得了75.1%的准确率，并在多个环境中表现出色。模型的代码已在GitHub上公开。",
        "title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent",
        "pinyin": "这篇文章介绍了一种新的图形用户界面（GUI）助手模型，名为ShowUI。它结合了视觉、语言和动作，旨在提高人类工作流的生产力。ShowUI通过UI引导的视觉标记选择、交错的视觉-语言-动作流和小规模高质量的GUI指令跟随数据集来减少计算成本并提高训练效率。该模型在零样本截图定位任务中取得了75.1%的准确率，并在多个环境中表现出色。模型的代码已在GitHub上公开。\n\nzhè piān wén zhāng jiè shào le yī zhǒng xīn de tú xíng yòng hù jiē miàn (GUI) zhù shǒu mó xíng, míng wèi ShowUI. tā jié hé le shì jué, yǔ yán hé dòng zuò, zhǐ zài tí gāo rén lèi gōng zuò liú de shēng chǎn lì. ShowUI tōng guò UI yǐn dǎo de shì jué biāo jì xuǎn zé, jiāo cuò de shì jué-yǔ yán-dòng zuò liú hé xiǎo guī mó gāo zhì liàng de GUI zhǐ lìng gēn suí shù jù jiǎng shǎo jì suàn chéng běn bìng tí gāo xùn liàn xiào yì. gǎi mó xíng zài líng yàng bǎn jié dìng wèi rèn wù zhōng qǔ dé le 75.1% de zhǔn què lǜ, bìng zài duō gè huán jìng zhōng biǎo xiàn chū sè. mó xíng de dài mǎ yǐ zài GitHub shàng gōng kāi.",
        "vocab": "[{'word': '图形用户界面', 'pinyin': 'tú xíng yòng hù jiē miàn', 'trans': 'graphical user interface'},\n{'word': '助手', 'pinyin': 'zhù shǒu', 'trans': 'assistant'},\n{'word': '模型', 'pinyin': 'mó xíng', 'trans': 'model'},\n{'word': '视觉', 'pinyin': 'shì jué', 'trans': 'vision'},\n{'word': '语言', 'pinyin': 'yǔ yán', 'trans': 'language'},\n{'word': '动作', 'pinyin': 'dòng zuò', 'trans': 'action'},\n{'word': '旨在', 'pinyin': 'zhǐ zài', 'trans': 'aim to'},\n{'word': '生产力', 'pinyin': 'shēng chǎn lì', 'trans': 'productivity'},\n{'word': '引导', 'pinyin': 'yǐn dǎo', 'trans': 'guide'},\n{'word': '标记', 'pinyin': 'biāo jì', 'trans': 'mark'},\n{'word': '选择', 'pinyin': 'xuǎn zé', 'trans': 'selection'},\n{'word': '交错', 'pinyin': 'jiāo cuò', 'trans': 'interleave'},\n{'word': '流', 'pinyin': 'liú', 'trans': 'flow'},\n{'word': '规模', 'pinyin': 'guī mó', 'trans': 'scale'},\n{'word': '高质量', 'pinyin': 'gāo zhì liàng', 'trans': 'high quality'},\n{'word': '指令', 'pinyin': 'zhǐ lìng', 'trans': 'command'},\n{'word': '跟随', 'pinyin': 'gēn suí', 'trans': 'follow'},\n{'word': '数据集', 'pinyin': 'shù jù jí', 'trans': 'dataset'},\n{'word': '计算', 'pinyin': 'jì suàn', 'trans': 'computation'},\n{'word': '成本', 'pinyin': 'chéng běn', 'trans': 'cost'},\n{'word': '训练', 'pinyin': 'xùn liàn', 'trans': 'training'},\n{'word': '效率', 'pinyin': 'xiào lǜ', 'trans': 'efficiency'},\n{'word': '零样本', 'pinyin': 'líng yàng běn', 'trans': 'zero-shot'},\n{'word': '截图', 'pinyin': 'jié tú', 'trans': 'screenshot'},\n{'word': '定位', 'pinyin': 'dìng wèi', 'trans': 'localization'},\n{'word': '任务', 'pinyin': 'rèn wu', 'trans': 'task'},\n{'word': '准确率', 'pinyin': 'zhǔn què lǜ', 'trans': 'accuracy'},\n{'word': '环境', 'pinyin': 'huán jìng', 'trans': 'environment'},\n{'word': '表现', 'pinyin': 'biǎo xiàn', 'trans': 'performance'},\n{'word': '出色', 'pinyin': 'chū sè', 'trans': 'outstanding'},\n{'word': '公开', 'pinyin': 'gōng kāi', 'trans': 'public'},\n{'word': 'GitHub', 'pinyin': 'GitHub', 'trans': 'GitHub'}]",
        "trans": "This article introduces a new graphical user interface (GUI) assistant model called ShowUI. It combines vision, language, and action to enhance the productivity of human workflows. ShowUI reduces computational costs and improves training efficiency through UI-guided visual mark selection, interleaved vision-language-action flows, and a small-scale, high-quality GUI instruction-following dataset. The model achieved a 75.1% accuracy rate in zero-shot screenshot localization tasks and performed well in multiple environments. The model's code has been made publicly available on GitHub.",
        "update_ts": "2024-11-27 09:11"
    }
}