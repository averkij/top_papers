{
    "date": {
        "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 9",
        "zh": "12æœˆ9æ—¥"
    },
    "time_utc": "2025-12-09 09:00",
    "weekday": 1,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2025-12-09",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.07461",
            "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning",
            "url": "https://huggingface.co/papers/2512.07461",
            "abstract": "NPR, a teacher-free framework, enhances Large Language Models with native parallel reasoning capabilities through self-distilled training, Parallel-Aware Policy Optimization, and a robust NPR Engine, achieving substantial performance and speed improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.",
            "score": 63,
            "issue_id": 1,
            "pub_date": "2025-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "338d051f90e86520",
            "authors": [
                "Tong Wu",
                "Yang Liu",
                "Jun Bai",
                "Zixia Jia",
                "Shuyi Zhang",
                "Ziyong Lin",
                "Yanting Wang",
                "Song-Chun Zhu",
                "Zilong Zheng"
            ],
            "affiliations": [
                "Beijing Institute for General Artificial Intelligence (BIGAI)"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.07461.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#benchmark",
                    "#small_models"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ",
                    "desc": "NPR â€” ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ñ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ° Ğº ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¼ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼, Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Parallel-Aware Policy Optimization Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ³Ñ€Ğ°Ñ„Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. NPR Engine Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 24,5% Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ¾ 4,6x Ğ¿Ñ€Ğ¸ 100% Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Empowering LLMs with Native Parallel Reasoning",
                    "desc": "The Native Parallel Reasoner (NPR) is a framework designed to enhance Large Language Models (LLMs) by enabling them to perform parallel reasoning without the need for a teacher. It introduces a self-distilled training approach that allows models to evolve their reasoning capabilities independently. NPR also features a Parallel-Aware Policy Optimization (PAPO) algorithm that helps the model learn effective strategies for decision-making through experimentation. As a result, NPR achieves significant improvements in both performance and speed, setting a new benchmark for efficient reasoning in AI."
                },
                "zh": {
                    "title": "NPRï¼šå¼€å¯çœŸæ­£çš„å¹¶è¡Œæ¨ç†æ–°æ—¶ä»£",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºNPRçš„æ— æ•™å¸ˆæ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„åŸç”Ÿå¹¶è¡Œæ¨ç†èƒ½åŠ›ã€‚NPRé€šè¿‡è‡ªæˆ‘è’¸é¦è®­ç»ƒã€å¹¶è¡Œæ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–å’Œå¼ºå¤§çš„NPRå¼•æ“ï¼Œå®ç°äº†ä»é¡ºåºæ¨¡æ‹Ÿåˆ°åŸç”Ÿå¹¶è¡Œè®¤çŸ¥çš„è½¬å˜ã€‚è¯¥æ¡†æ¶åœ¨å…«ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºé«˜è¾¾24.5%çš„æ€§èƒ½æå‡å’Œ4.6å€çš„æ¨ç†é€Ÿåº¦åŠ å¿«ã€‚ä¸ä»¥å¾€çš„åŸºçº¿ç›¸æ¯”ï¼ŒNPRå®ç°äº†100%çš„çœŸæ­£å¹¶è¡Œæ‰§è¡Œï¼Œæ ‘ç«‹äº†è‡ªæˆ‘è¿›åŒ–ã€é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ™ºèƒ½æ¨ç†çš„æ–°æ ‡å‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.07525",
            "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs",
            "url": "https://huggingface.co/papers/2512.07525",
            "abstract": "The paper proposes a method to enhance Rotary Position Embeddings by utilizing both the real and imaginary components of the complex-valued dot product, improving long-context modeling in Large Language Models.  \t\t\t\t\tAI-generated summary \t\t\t\t Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.",
            "score": 52,
            "issue_id": 1,
            "pub_date": "2025-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "7571894576164782",
            "authors": [
                "Xiaoran Liu",
                "Yuerong Song",
                "Zhigeng Liu",
                "Zengfeng Huang",
                "Qipeng Guo",
                "Zhaoxiang Liu",
                "Shiguo Lian",
                "Ziwei He",
                "Xipeng Qiu"
            ],
            "affiliations": [
                "China Unicom",
                "Fudan University",
                "Shanghai AI Lab",
                "Shanghai Innovation Institute"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.07525.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'list'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.07469",
            "title": "Unified Video Editing with Temporal Reasoner",
            "url": "https://huggingface.co/papers/2512.07469",
            "abstract": "VideoCoF, a Chain-of-Frames approach, improves video editing precision and instruction-to-region mapping by using reasoning tokens without requiring user-provided masks.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.",
            "score": 41,
            "issue_id": 1,
            "pub_date": "2025-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "a3dca53d4b568421",
            "authors": [
                "Xiangpeng Yang",
                "Ji Xie",
                "Yiyuan Yang",
                "Yan Huang",
                "Min Xu",
                "Qiang Wu"
            ],
            "affiliations": [
                "University of Technology Sydney",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.07469.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#architecture",
                    "#video",
                    "#diffusion",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¦ĞµĞ¿Ğ¾Ñ‡ĞºĞ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ²: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¼Ğ°ÑĞ¾Ğº Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "VideoCoF Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Chain-of-Frames Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğº Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ¼ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°ÑĞ¾Ğº Ğ¾Ñ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ), Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ»ĞµĞ´ÑƒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğµ Â«Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÂ». Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ RoPE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½ĞµĞµ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ° Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 50k Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ°Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "VideoCoF: Precision Video Editing Through Reasoning Tokens",
                    "desc": "VideoCoF is a novel approach to video editing that enhances precision and mapping of instructions to specific regions in a video. It utilizes a Chain-of-Frames method that incorporates reasoning tokens, allowing the model to predict edit regions without needing user-provided masks. This method improves the alignment of instructions to video content by enforcing a structured process of seeing, reasoning, and then editing. Additionally, the RoPE alignment strategy ensures that motion is consistent and can extend beyond the original video length, achieving state-of-the-art results with minimal data requirements."
                },
                "zh": {
                    "title": "VideoCoFï¼šç²¾å‡†è§†é¢‘ç¼–è¾‘çš„æ–°æ–¹æ³•",
                    "desc": "VideoCoFæ˜¯ä¸€ç§é“¾å¸§æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨æ¨ç†æ ‡è®°æ¥æé«˜è§†é¢‘ç¼–è¾‘çš„ç²¾ç¡®åº¦å’ŒæŒ‡ä»¤åˆ°åŒºåŸŸçš„æ˜ å°„ï¼Œè€Œæ— éœ€ç”¨æˆ·æä¾›çš„æ©ç ã€‚ç°æœ‰çš„è§†é¢‘ç¼–è¾‘æ–¹æ³•é¢ä¸´ç€ç²¾åº¦ä¸ç»Ÿä¸€æ€§çš„æƒè¡¡ï¼Œä¸“å®¶æ¨¡å‹ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„å…ˆéªŒçŸ¥è¯†ï¼Œè€Œç»Ÿä¸€çš„å­¦ä¹ æ¨¡å‹åˆ™ç¼ºä¹æ˜ç¡®çš„ç©ºé—´çº¿ç´¢ã€‚VideoCoFé€šè¿‡å¼ºåˆ¶è§†é¢‘æ‰©æ•£æ¨¡å‹é¦–å…ˆé¢„æµ‹æ¨ç†æ ‡è®°ï¼Œç„¶åå†ç”Ÿæˆç›®æ ‡è§†é¢‘æ ‡è®°ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§RoPEå¯¹é½ç­–ç•¥ï¼Œä»¥ç¡®ä¿è¿åŠ¨å¯¹é½å¹¶æ”¯æŒè¶…å‡ºè®­ç»ƒæ—¶é•¿çš„é•¿åº¦å¤–æ¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.07834",
            "title": "Voxify3D: Pixel Art Meets Volumetric Rendering",
            "url": "https://huggingface.co/papers/2512.07834",
            "abstract": "Voxify3D is a two-stage framework that combines 3D mesh optimization with 2D pixel art supervision to generate high-quality voxel art with semantic preservation, pixel-art aesthetics, and discrete color coherence.  \t\t\t\t\tAI-generated summary \t\t\t\t Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/",
            "score": 36,
            "issue_id": 1,
            "pub_date": "2025-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "f119f190cce60858",
            "authors": [
                "Yi-Chuan Huang",
                "Jiewen Chan",
                "Hao-Jen Chien",
                "Yu-Lun Liu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.07834.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.07783",
            "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models",
            "url": "https://huggingface.co/papers/2512.07783",
            "abstract": "A controlled experimental framework isolates and evaluates the contributions of pre-training, mid-training, and reinforcement learning in improving language model reasoning, demonstrating the necessity of each phase and the role of process-level rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.",
            "score": 29,
            "issue_id": 1,
            "pub_date": "2025-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "93a20c7506c2655e",
            "authors": [
                "Charlie Zhang",
                "Graham Neubig",
                "Xiang Yue"
            ],
            "affiliations": [
                "Carnegie Mellon University, Language Technologies Institute"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.07783.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞºĞ»Ğ°Ğ´Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»Ğ°Ğ´Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ÑÑĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ°ĞµÑ‚ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Â«Ğ·Ğ°Ğ¿Ğ°ÑĞ° Ğ¿Ñ€Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸Â» Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ³Ğ´Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ RL Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½Ñ‹ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ¸Ğ³Ñ€Ñ‹ Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking Language Model Reasoning: The Power of Training Phases",
                    "desc": "This paper investigates how different training phasesâ€”pre-training, mid-training, and reinforcement learning (RL)â€”contribute to the reasoning abilities of language models. It introduces a controlled experimental framework to isolate these contributions and evaluates models on their ability to generalize in complex reasoning tasks. The findings reveal that RL enhances capabilities only when pre-training is effective and that mid-training plays a crucial role in improving performance. Additionally, the study highlights the importance of process-level rewards in enhancing reasoning accuracy and reducing reward hacking."
                },
                "zh": {
                    "title": "æ­ç¤ºè¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡çš„å…³é”®é˜¶æ®µ",
                    "desc": "æœ¬ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªå—æ§å®éªŒæ¡†æ¶ï¼Œä»¥è¯„ä¼°é¢„è®­ç»ƒã€ä¸­æœŸè®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ åœ¨æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›ä¸­çš„è´¡çŒ®ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒã€ä¸­æœŸè®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ å„é˜¶æ®µéƒ½æ˜¯å¿…è¦çš„ï¼Œä¸”è¿‡ç¨‹çº§å¥–åŠ±åœ¨å…¶ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚é€šè¿‡åˆæˆæ¨ç†ä»»åŠ¡ï¼Œç ”ç©¶å›¢é˜Ÿèƒ½å¤Ÿç³»ç»Ÿåœ°åˆ†æä¸åŒè®­ç»ƒé˜¶æ®µçš„å› æœå…³ç³»ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸­æœŸè®­ç»ƒæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œè€Œå¼ºåŒ–å­¦ä¹ çš„æœ‰æ•ˆæ€§ä¾èµ–äºé¢„è®­ç»ƒçš„å……åˆ†æ€§å’Œä»»åŠ¡çš„é€‚åº”æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06905",
            "title": "Scaling Zero-Shot Reference-to-Video Generation",
            "url": "https://huggingface.co/papers/2512.06905",
            "abstract": "Saber is a scalable zero-shot framework for reference-to-video generation that uses video-text pairs to learn identity-consistent representations and outperforms models trained with explicit reference data.  \t\t\t\t\tAI-generated summary \t\t\t\t Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.",
            "score": 28,
            "issue_id": 1,
            "pub_date": "2025-12-07",
            "pub_date_card": {
                "ru": "7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 7",
                "zh": "12æœˆ7æ—¥"
            },
            "hash": "e3487af78b5e9625",
            "authors": [
                "Zijian Zhou",
                "Shikun Liu",
                "Haozhe Liu",
                "Haonan Qiu",
                "Zhaochong An",
                "Weiming Ren",
                "Zhiheng Liu",
                "Xiaoke Huang",
                "Kam Woh Ng",
                "Tian Xie",
                "Xiao Han",
                "Yuren Cong",
                "Hang Li",
                "Chuyan Zhu",
                "Aditya Patel",
                "Tao Xiang",
                "Sen He"
            ],
            "affiliations": [
                "Kings College London",
                "Meta AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.06905.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06749",
            "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems",
            "url": "https://huggingface.co/papers/2512.06749",
            "abstract": "DoVer, an intervention-driven debugging framework, enhances reliability in LLM-based multi-agent systems by actively validating failure hypotheses and measuring task progress through targeted interventions.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.",
            "score": 25,
            "issue_id": 1,
            "pub_date": "2025-12-07",
            "pub_date_card": {
                "ru": "7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 7",
                "zh": "12æœˆ7æ—¥"
            },
            "hash": "e0b39bce909efdd4",
            "authors": [
                "Ming Ma",
                "Jue Zhang",
                "Fangkai Yang",
                "Yu Kang",
                "Qingwei Lin",
                "Tianming Yang",
                "Saravan Rajmohan",
                "Dongmei Zhang"
            ],
            "affiliations": [
                "Institute of Neuroscience, State Key Laboratory of Brain Cognition and Brain-inspired Intelligence Technology, Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences",
                "Microsoft",
                "University of Chinese Academy of Sciences, School of Future Technology"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.06749.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06065",
            "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing",
            "url": "https://huggingface.co/papers/2512.06065",
            "abstract": "EgoEdit is a real-time, instruction-following egocentric video editor that addresses challenges in handling egomotion and hand-object interactions, outperforming existing methods on egocentric editing tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit",
            "score": 23,
            "issue_id": 1,
            "pub_date": "2025-12-05",
            "pub_date_card": {
                "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 5",
                "zh": "12æœˆ5æ—¥"
            },
            "hash": "fc437da58a265a3d",
            "authors": [
                "Runjia Li",
                "Moayed Haji-Ali",
                "Ashkan Mirzaei",
                "Chaoyang Wang",
                "Arpit Sahni",
                "Ivan Skorokhodov",
                "Aliaksandr Siarohin",
                "Tomas Jakab",
                "Junlin Han",
                "Sergey Tulyakov",
                "Philip Torr",
                "Willi Menapace"
            ],
            "affiliations": [
                "Rice University",
                "Snap Research",
                "University of Oxford"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.06065.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#inference",
                    "#video",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸",
                    "desc": "EgoEdit â€” ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ‡Ğ°ÑÑ‚Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€ÑƒĞº Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ EgoEditData Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² EgoEditBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ€ÑƒĞº Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Egocentric Video Editing in Real-Time",
                    "desc": "EgoEdit is a novel video editing tool designed specifically for egocentric videos, which are recorded from a first-person perspective. It tackles the unique challenges of egomotion and hand-object interactions that traditional video editors struggle with. By utilizing a specially curated dataset called EgoEditData, it ensures that hand movements and interactions are accurately preserved during editing. The system operates in real-time on a single GPU, providing fast and reliable editing results that outperform existing methods in egocentric scenarios while maintaining competitive performance in general video editing tasks."
                },
                "zh": {
                    "title": "å®æ—¶è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç¼–è¾‘çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ",
                    "desc": "EgoEdit æ˜¯ä¸€ä¸ªå®æ—¶çš„ã€éµå¾ªæŒ‡ä»¤çš„è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç¼–è¾‘å™¨ï¼Œä¸“é—¨è§£å†³è‡ªæˆ‘è¿åŠ¨å’Œæ‰‹ç‰©ä½“äº¤äº’çš„æŒ‘æˆ˜ã€‚å®ƒåœ¨è‡ªæˆ‘ä¸­å¿ƒç¼–è¾‘ä»»åŠ¡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æ–¹æ³•ï¼Œæä¾›äº†æ›´å¥½çš„ç¼–è¾‘æ•ˆæœã€‚æˆ‘ä»¬æ„å»ºäº† EgoEditData æ•°æ®é›†ï¼Œä¸“æ³¨äºä¸°å¯Œçš„æ‰‹ç‰©ä½“äº¤äº’ï¼Œå¹¶ä¿ç•™æ‰‹éƒ¨ä¿¡æ¯ã€‚EgoEdit è¿˜æ”¯æŒåœ¨å•ä¸ª GPU ä¸Šè¿›è¡Œå®æ—¶æ¨ç†ï¼Œç¡®ä¿äº†ä½å»¶è¿Ÿçš„äº¤äº’ä½“éªŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.07833",
            "title": "Relational Visual Similarity",
            "url": "https://huggingface.co/papers/2512.07833",
            "abstract": "Vision-Language models fine-tuned on anonymized image captions can capture relational similarity between images, a capability lacking in current visual similarity metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.",
            "score": 22,
            "issue_id": 1,
            "pub_date": "2025-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "66d5a53806317deb",
            "authors": [
                "Thao Nguyen",
                "Sicheng Mo",
                "Krishna Kumar Singh",
                "Yilin Wang",
                "Jing Shi",
                "Nicholas Kolkin",
                "Eli Shechtman",
                "Yong Jae Lee",
                "Yuheng Li"
            ],
            "affiliations": [
                "Adobe Research",
                "University of California, Los Angeles",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.07833.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.07778",
            "title": "Distribution Matching Variational AutoEncoder",
            "url": "https://huggingface.co/papers/2512.07778",
            "abstract": "DMVAE explicitly aligns the encoder's latent distribution with a reference distribution, improving modeling efficiency and image synthesis fidelity compared to conventional VAEs.  \t\t\t\t\tAI-generated summary \t\t\t\t Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce Distribution-Matching VAE (DMVAE), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.",
            "score": 20,
            "issue_id": 1,
            "pub_date": "2025-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "36921e5514ed4873",
            "authors": [
                "Sen Ye",
                "Jianning Pei",
                "Mengde Xu",
                "Shuyang Gu",
                "Chunyu Wang",
                "Liwei Wang",
                "Han Hu"
            ],
            "affiliations": [
                "Peking University",
                "Tencent",
                "UCAS"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.07778.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¯Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Distribution-Matching VAE (DMVAE) â€” ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ²Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… VAE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½ĞµÑĞ²Ğ½Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ¼, DMVAE Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ²Ğ½Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ· self-supervised Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ° Ğ¸Ğ»Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚, ĞºĞ°ĞºĞ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ»ÑƒÑ‡ÑˆĞµ Ğ²ÑĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‚ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ· self-supervised feature, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ ImageNet Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ gFID = 3.2 Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 64 ÑĞ¿Ğ¾Ñ…Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Aligning Latent Distributions for Better Image Synthesis",
                    "desc": "The paper introduces the Distribution-Matching VAE (DMVAE), which enhances the performance of visual generative models by explicitly aligning the encoder's latent distribution with a chosen reference distribution. This approach allows for greater flexibility compared to traditional VAEs, which typically rely on a fixed Gaussian prior. By using distribution matching constraints, DMVAE can adapt to various distributions, including those derived from self-supervised learning and diffusion processes. The results demonstrate that this method significantly improves image synthesis quality and modeling efficiency, achieving a notable gFID score on ImageNet with minimal training epochs."
                },
                "zh": {
                    "title": "åˆ†å¸ƒåŒ¹é…ï¼Œæå‡å›¾åƒåˆæˆè´¨é‡",
                    "desc": "DMVAEï¼ˆåˆ†å¸ƒåŒ¹é…å˜åˆ†è‡ªç¼–ç å™¨ï¼‰é€šè¿‡å°†ç¼–ç å™¨çš„æ½œåœ¨åˆ†å¸ƒä¸å‚è€ƒåˆ†å¸ƒæ˜¾å¼å¯¹é½ï¼Œæå‡äº†å»ºæ¨¡æ•ˆç‡å’Œå›¾åƒåˆæˆçš„ä¿çœŸåº¦ã€‚ä¸ä¼ ç»Ÿçš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ä¸åŒï¼ŒDMVAEå…è®¸ä¸ä»»æ„å‚è€ƒåˆ†å¸ƒå¯¹é½ï¼Œè€Œä¸ä»…ä»…æ˜¯é«˜æ–¯å…ˆéªŒã€‚è¿™ç§æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿç³»ç»Ÿåœ°ç ”ç©¶å“ªäº›æ½œåœ¨åˆ†å¸ƒæ›´é€‚åˆå»ºæ¨¡ï¼Œå¹¶å‘ç°è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰è¡ç”Ÿçš„åˆ†å¸ƒåœ¨é‡å»ºä¿çœŸåº¦å’Œå»ºæ¨¡æ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†è‰¯å¥½çš„å¹³è¡¡ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œé€‰æ‹©åˆé€‚çš„æ½œåœ¨åˆ†å¸ƒç»“æ„æ˜¯å®ç°é«˜ä¿çœŸå›¾åƒåˆæˆçš„å…³é”®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.07806",
            "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader",
            "url": "https://huggingface.co/papers/2512.07806",
            "abstract": "MVP, a scalable multi-view transformer architecture, efficiently reconstructs large 3D scenes from multiple images using dual hierarchies and achieves state-of-the-art quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.",
            "score": 19,
            "issue_id": 1,
            "pub_date": "2025-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "3b5cc6cb4777ae6f",
            "authors": [
                "Gyeongjin Kang",
                "Seungkwon Yang",
                "Seungtae Nam",
                "Younggeun Lee",
                "Jungwoo Kim",
                "Eunbyung Park"
            ],
            "affiliations": [
                "Sungkyunkwan University",
                "Yonsei University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.07806.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06589",
            "title": "OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation",
            "url": "https://huggingface.co/papers/2512.06589",
            "abstract": "OmniSafeBench-MM is a comprehensive tool for evaluating multi-modal jailbreak attacks and defenses, covering various attack methods, defense strategies, and risk domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM.",
            "score": 16,
            "issue_id": 1,
            "pub_date": "2025-12-06",
            "pub_date_card": {
                "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 6",
                "zh": "12æœˆ6æ—¥"
            },
            "hash": "ad55b75eeb1085ca",
            "authors": [
                "Xiaojun Jia",
                "Jie Liao",
                "Qi Guo",
                "Teng Ma",
                "Simeng Qin",
                "Ranjie Duan",
                "Tianlin Li",
                "Yihao Huang",
                "Zhitao Zeng",
                "Dongxian Wu",
                "Yiming Li",
                "Wenqi Ren",
                "Xiaochun Cao",
                "Yang Liu"
            ],
            "affiliations": [
                "Alibaba, China",
                "BraneMatrix AI, China",
                "ByteDance, China",
                "Chongqing University, China",
                "Nanyang Technological University, Singapore",
                "National University of Singapore, Singapore",
                "Northeastern University, China",
                "Sun Yat-sen University, China",
                "Xian Jiaotong University, China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.06589.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#open_source",
                    "#multimodal",
                    "#benchmark",
                    "#alignment",
                    "#dataset"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "OmniSafeBench-MM Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ĞµÑ€ĞµĞ´ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ 13 Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ°Ñ‚Ğ°Ğº, 15 ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 9 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ñ€Ğ¸ÑĞºĞ° Ñ 50 Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ° Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ ÑˆĞºĞ°Ğ»Ğµ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 10 Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ 8 Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM, ÑĞ¾Ğ·Ğ´Ğ°Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Strengthening Multi-Modal Models Against Jailbreak Attacks",
                    "desc": "OmniSafeBench-MM is a new tool designed to evaluate how well multi-modal large language models (MLLMs) can withstand jailbreak attacks. It includes a wide range of attack methods and defense strategies, making it a comprehensive resource for researchers. The tool assesses vulnerabilities across various risk domains and uses a detailed evaluation protocol to measure harmfulness, intent alignment, and response detail. By providing a standardized and reproducible framework, OmniSafeBench-MM aims to enhance the understanding and safety of MLLMs against potential threats."
                },
                "zh": {
                    "title": "å…¨é¢è¯„ä¼°å¤šæ¨¡æ€è¶Šç‹±æ”»å‡»ä¸é˜²å¾¡çš„å·¥å…·",
                    "desc": "OmniSafeBench-MM æ˜¯ä¸€ä¸ªå…¨é¢çš„å·¥å…·ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€çš„è¶Šç‹±æ”»å‡»å’Œé˜²å¾¡ã€‚å®ƒæ•´åˆäº†13ç§ä»£è¡¨æ€§çš„æ”»å‡»æ–¹æ³•å’Œ15ç§é˜²å¾¡ç­–ç•¥ï¼Œè¦†ç›–äº†9ä¸ªä¸»è¦é£é™©é¢†åŸŸå’Œ50ä¸ªç»†åˆ†ç±»åˆ«ã€‚è¯¥å·¥å…·å»ºç«‹äº†ä¸€ä¸ªä¸‰ç»´è¯„ä¼°åè®®ï¼Œæµ‹é‡æœ‰å®³æ€§ã€æ„å›¾ä¸€è‡´æ€§å’Œå“åº”ç»†èŠ‚æ°´å¹³ï¼Œä»¥ä¾¿è¿›è¡Œç»†è‡´çš„å®‰å…¨æ€§å’Œæ•ˆç”¨åˆ†æã€‚é€šè¿‡ç»Ÿä¸€æ•°æ®ã€æ–¹æ³•å’Œè¯„ä¼°ï¼ŒOmniSafeBench-MM ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.08294",
            "title": "OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation",
            "url": "https://huggingface.co/papers/2512.08294",
            "abstract": "OpenSubject, a large-scale video-derived dataset, enhances subject-driven image generation and manipulation through a four-stage pipeline that maintains identity fidelity and handles complex scenes.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.",
            "score": 15,
            "issue_id": 1,
            "pub_date": "2025-12-09",
            "pub_date_card": {
                "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 9",
                "zh": "12æœˆ9æ—¥"
            },
            "hash": "9849fa13995b20b0",
            "authors": [
                "Yexin Liu",
                "Manyuan Zhang",
                "Yueze Wang",
                "Hongyu Li",
                "Dian Zheng",
                "Weiming Zhang",
                "Changsheng Lu",
                "Xunliang Cai",
                "Yan Feng",
                "Peng Pei",
                "Harry Yang"
            ],
            "affiliations": [
                "HKUST",
                "HKUST(GZ)",
                "Meituan"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.08294.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.07831",
            "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation",
            "url": "https://huggingface.co/papers/2512.07831",
            "abstract": "UnityVideo, a unified framework, enhances video generation by integrating multiple modalities and paradigms, leading to improved quality and alignment with real-world constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo",
            "score": 15,
            "issue_id": 1,
            "pub_date": "2025-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "ea4843b9a1b01894",
            "authors": [
                "Jiehui Huang",
                "Yuechen Zhang",
                "Xu He",
                "Yuan Gao",
                "Zhi Cen",
                "Bin Xia",
                "Yan Zhou",
                "Xin Tao",
                "Pengfei Wan",
                "Jiaya Jia"
            ],
            "affiliations": [
                "CUHK",
                "HKUST",
                "Kling Team, Kuaishou Technology",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.07831.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.07584",
            "title": "LongCat-Image Technical Report",
            "url": "https://huggingface.co/papers/2512.07584",
            "abstract": "LongCat-Image is a bilingual open-source foundation model for image generation that addresses multilingual text rendering, photorealism, and deployment efficiency through rigorous data curation, compact design, and comprehensive open-source support.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.",
            "score": 15,
            "issue_id": 1,
            "pub_date": "2025-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "19dd8c92e6e77b12",
            "authors": [
                "Meituan LongCat Team",
                "Hanghang Ma",
                "Haoxian Tan",
                "Jiale Huang",
                "Junqiang Wu",
                "Jun-Yan He",
                "Lishuai Gao",
                "Songlin Xiao",
                "Xiaoming Wei",
                "Xiaoqi Ma",
                "Xunliang Cai",
                "Yayong Guan",
                "Jie Hu"
            ],
            "affiliations": [
                "Meituan"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.07584.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#open_source",
                    "#multilingual",
                    "#training",
                    "#architecture",
                    "#inference",
                    "#small_models",
                    "#diffusion",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "LongCat-Image â€” ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ (ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾-Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ°Ñ) Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ¾Ğ³ÑƒÑ ĞºÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ reward models Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ ÑĞ´Ñ€Ğ¾Ğ¼ Ğ²ÑĞµĞ³Ğ¾ Ğ² 6B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MoE Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ²ĞµÑ€ÑĞ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "LongCat-Image: Bridging Languages with Photorealistic Image Generation",
                    "desc": "LongCat-Image is an innovative bilingual foundation model designed for image generation, focusing on both Chinese and English text rendering. It utilizes advanced data curation techniques throughout its training phases, leading to state-of-the-art performance in photorealism and text accuracy, especially for complex Chinese characters. The model is compact, with only 6 billion parameters, making it efficient for deployment while maintaining high-quality output. Additionally, it fosters community engagement by providing a comprehensive open-source ecosystem, including various model versions and training tools to support developers and researchers."
                },
                "zh": {
                    "title": "é•¿çŒ«å›¾åƒï¼šå¼€åˆ›åŒè¯­å›¾åƒç”Ÿæˆæ–°æ ‡å‡†",
                    "desc": "LongCat-Imageæ˜¯ä¸€ä¸ªå¼€æºçš„åŒè¯­åŸºç¡€æ¨¡å‹ï¼Œä¸“æ³¨äºå›¾åƒç”Ÿæˆï¼Œæ—¨åœ¨è§£å†³å¤šè¯­è¨€æ–‡æœ¬æ¸²æŸ“ã€çœŸå®æ„Ÿå’Œéƒ¨ç½²æ•ˆç‡ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚é€šè¿‡ä¸¥æ ¼çš„æ•°æ®æ•´ç†å’Œç´§å‡‘çš„è®¾è®¡ï¼Œè¯¥æ¨¡å‹åœ¨æ–‡æœ¬æ¸²æŸ“èƒ½åŠ›å’ŒçœŸå®æ„Ÿæ–¹é¢è¾¾åˆ°äº†æ–°çš„è¡Œä¸šæ ‡å‡†ï¼Œå°¤å…¶åœ¨ä¸­æ–‡å­—ç¬¦çš„æ¸²æŸ“ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚å®ƒçš„æ ¸å¿ƒæ‰©æ•£æ¨¡å‹ä»…æœ‰6äº¿å‚æ•°ï¼Œæ˜¾è‘—å°äºè¡Œä¸šå†…å¸¸è§çš„20äº¿å‚æ•°æ¨¡å‹ï¼Œç¡®ä¿äº†ä½æ˜¾å­˜ä½¿ç”¨å’Œå¿«é€Ÿæ¨ç†ã€‚LongCat-Imageè¿˜åœ¨å›¾åƒç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†å…¨é¢çš„å¼€æºç”Ÿæ€ç³»ç»Ÿï¼Œæ”¯æŒå¼€å‘è€…å’Œç ”ç©¶äººå‘˜æ¨åŠ¨è§†è§‰å†…å®¹åˆ›ä½œçš„å‰æ²¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03244",
            "title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning",
            "url": "https://huggingface.co/papers/2512.03244",
            "abstract": "A three-stage framework, SPARK, uses a generator and verifier to create synthetic training data for process reward models, enabling reference-free reinforcement learning that surpasses ground-truth methods in mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2025-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "eced3f0c0df8f415",
            "authors": [
                "Salman Rahman",
                "Sruthi Gorantla",
                "Arpit Gupta",
                "Swastik Roy",
                "Nanyun Peng",
                "Yang Liu"
            ],
            "affiliations": [
                "Amazon AGI",
                "UCLA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03244.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#data",
                    "#training",
                    "#optimization",
                    "#benchmark",
                    "#synthetic",
                    "#rlhf",
                    "#math"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ²: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±ĞµĞ· Ğ½Ğ°Ğ·ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ°Ğ²Ğ´Ñ‹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ SPARK â€” Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ° Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ñ‚Ñ€ĞµÑ‚ÑŒĞµĞ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ ĞºĞ°Ğº Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "SPARK: Revolutionizing Reinforcement Learning with Synthetic Data",
                    "desc": "The paper introduces SPARK, a three-stage framework designed to enhance reinforcement learning by generating synthetic training data for process reward models (PRMs). In the first stage, a generator creates diverse solutions, while a verifier assesses these solutions using both self-consistency and meta-critique methods. The second stage utilizes the verification results to fine-tune PRMs, which then provide reward signals during training. The final stage demonstrates that this approach achieves superior performance in mathematical reasoning tasks compared to traditional ground-truth methods, enabling effective reinforcement learning without the need for expensive annotations."
                },
                "zh": {
                    "title": "SPARKï¼šè¶…è¶ŠçœŸå®æ•°æ®çš„æ— å‚è€ƒå¼ºåŒ–å­¦ä¹ ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSPARKçš„ä¸‰é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®ä»¥æ”¹è¿›è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œç”Ÿæˆå™¨æ¨¡å‹äº§ç”Ÿå¤šæ ·åŒ–çš„è§£å†³æ–¹æ¡ˆï¼ŒéªŒè¯å™¨æ¨¡å‹é€šè¿‡è‡ªä¸€è‡´æ€§å’Œå…ƒæ‰¹è¯„å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚ç¬¬äºŒé˜¶æ®µåˆ©ç”¨è¿™äº›éªŒè¯è¾“å‡ºä½œä¸ºåˆæˆè®­ç»ƒæ•°æ®ï¼Œå¾®è°ƒç”Ÿæˆçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œä»¥åœ¨è®­ç»ƒä¸­æä¾›å¥–åŠ±ä¿¡å·ã€‚æœ€ç»ˆï¼ŒSPARKåœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å®ç°äº†è¶…è¿‡åŸºäºçœŸå®æ•°æ®çš„æ–¹æ³•çš„è¡¨ç°ï¼Œå±•ç¤ºäº†æ— å‚è€ƒå¼ºåŒ–å­¦ä¹ çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.23469",
            "title": "Visual Generation Tuning",
            "url": "https://huggingface.co/papers/2511.23469",
            "abstract": "VGT, a novel paradigm for visual generation tuning, enhances vision language models to achieve high-quality image reconstruction and generation with improved efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Vision Language Models (VLMs) effectively bridge the modality gap through extensive pretraining, acquiring sophisticated visual representations aligned with language. However, it remains underexplored whether these representations, optimized for multimodal understanding tasks, harbor an inherent potential for visual generation. In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models. By performing efficient visual generation tuning on well-pretrained VLMs, we significantly mitigate the alignment costs and accelerate the convergence of autoregressive modeling in the continuous space (20x speedup). Specifically, we dismiss the entangled pixel-level VAEs designed for diffusion transformers and formulate VGT-AE through aligning the semantic encoders from pretrained VLMs with the latent representations of pixel decoders. In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench. Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models. Models and codes are available at https://github.com/hustvl/VGT.",
            "score": 13,
            "issue_id": 1,
            "pub_date": "2025-11-28",
            "pub_date_card": {
                "ru": "28 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 28",
                "zh": "11æœˆ28æ—¥"
            },
            "hash": "0ef0af488bd62ad9",
            "authors": [
                "Jiahao Guo",
                "Sinan Du",
                "Jingfeng Yao",
                "Wenyu Liu",
                "Bo Li",
                "Haoxiang Cao",
                "Kun Gai",
                "Chun Yuan",
                "Kai Wu",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology (HUST)",
                "Kolors Team, Kuaishou Technology",
                "School of Artificial Intelligence, South China Normal University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.23469.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06373",
            "title": "VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning",
            "url": "https://huggingface.co/papers/2512.06373",
            "abstract": "The VG-Refiner framework improves tool-integrated visual reasoning by introducing a two-stage mechanism to handle unreliable tool outputs and enhance accuracy in referring and grounding tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2025-12-06",
            "pub_date_card": {
                "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 6",
                "zh": "12æœˆ6æ—¥"
            },
            "hash": "bdfd82bfcf4f8442",
            "authors": [
                "Yuji Wang",
                "Wenlong Liu",
                "Jingxuan Niu",
                "Haoji Zhang",
                "Yansong Tang"
            ],
            "affiliations": [
                "International Digital Economy Academy (IDEA)",
                "Tsinghua Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.06373.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#multimodal",
                    "#hallucinations",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "ĞÑ‚ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ²ĞµÑ€Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼: ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° VG-Refiner, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ 'Ğ´ÑƒĞ¼Ğ°Ğ¹-Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸'. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ³Ñ€ÑƒĞ½Ğ´Ğ¸Ğ½Ğ³Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñƒ Ğ·Ğ° ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ (refinement reward), Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑÑŒ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Refining Visual Reasoning with VG-Refiner",
                    "desc": "The VG-Refiner framework enhances tool-integrated visual reasoning (TiVR) by implementing a two-stage mechanism that addresses unreliable outputs from visual tools. This framework specifically targets referring and grounding tasks, which often suffer from inaccuracies in tool predictions leading to misleading reasoning. By introducing a think-rethink process, VG-Refiner allows the model to critically evaluate tool feedback and apply corrections effectively. Additionally, it establishes new metrics for evaluating refinement capabilities, resulting in improved accuracy and correction performance while maintaining the pretrained model's general abilities."
                },
                "zh": {
                    "title": "VG-Refinerï¼šæå‡è§†è§‰æ¨ç†çš„å·¥å…·é›†æˆèƒ½åŠ›",
                    "desc": "VG-Refineræ¡†æ¶é€šè¿‡å¼•å…¥ä¸¤é˜¶æ®µæœºåˆ¶æ¥æ”¹å–„å·¥å…·é›†æˆçš„è§†è§‰æ¨ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä¸å¯é çš„å·¥å…·è¾“å‡ºæ—¶ã€‚è¯¥æ¡†æ¶ä¸“æ³¨äºæé«˜å¼•ç”¨å’Œå®šä½ä»»åŠ¡çš„å‡†ç¡®æ€§ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­å¯¹é”™è¯¯å·¥å…·è¾“å‡ºååº”ä¸è¶³çš„é—®é¢˜ã€‚VG-Refineré€šè¿‡åˆ†æå’Œå“åº”å·¥å…·åé¦ˆï¼Œç»“åˆä¿®æ­£å¥–åŠ±æœºåˆ¶ï¼Œé¼“åŠ±æ¨¡å‹æœ‰æ•ˆçº æ­£é”™è¯¯ã€‚é€šè¿‡ä½¿ç”¨å°‘é‡ç‰¹å®šä»»åŠ¡æ•°æ®ï¼ŒVG-Refineråœ¨å¼•ç”¨å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œçº æ­£èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†é¢„è®­ç»ƒæ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03621",
            "title": "ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation",
            "url": "https://huggingface.co/papers/2512.03621",
            "abstract": "ReCamDriving generates camera-controlled novel-trajectory videos using dense 3DGS renderings and a two-stage training approach, achieving state-of-the-art results in controllability and consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "c736727a018311b8",
            "authors": [
                "Yaokun Li",
                "Shuaixian Wang",
                "Mantang Guo",
                "Jiehui Huang",
                "Taojun Ding",
                "Mu Hu",
                "Kaixuan Wang",
                "Shaojie Shen",
                "Guang Tan"
            ],
            "affiliations": [
                "Shenzhen Polytechnic University",
                "Sun Yat-sen University",
                "The Hong Kong University of Science and Technology",
                "ZYT"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03621.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.07829",
            "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation",
            "url": "https://huggingface.co/papers/2512.07829",
            "abstract": "FAE, a framework using a feature auto-encoder and dual decoders, adapts pre-trained visual representations for generative models, achieving high performance in image generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "7fd0877bda27e798",
            "authors": [
                "Yuan Gao",
                "Chen Chen",
                "Tianrong Chen",
                "Jiatao Gu"
            ],
            "affiliations": [
                "Apple"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.07829.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#diffusion",
                    "#cv",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞœĞ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "FAE â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ°Ğ²Ñ‚Ğ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ´Ğ²ÑƒĞ¼Ñ Ğ´ĞµcodĞµÑ€Ñ‹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°: Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. FAE ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ĞµĞ½ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼."
                },
                "en": {
                    "title": "FAE: Bridging Features and Generative Models for Superior Image Generation",
                    "desc": "FAE (Feature Auto-Encoder) is a novel framework designed to adapt pre-trained visual representations for use in generative models, specifically targeting image generation tasks. It effectively bridges the gap between high-dimensional feature spaces and low-dimensional latent spaces, which are essential for generating high-quality images. By utilizing two separate decodersâ€”one for reconstructing the original features and another for generating imagesâ€”FAE maintains crucial information while simplifying the adaptation process. The framework shows impressive results across various benchmarks, achieving state-of-the-art performance in both class-conditional and text-to-image generation tasks."
                },
                "zh": {
                    "title": "FAEï¼šé«˜æ•ˆçš„å›¾åƒç”Ÿæˆæ¡†æ¶",
                    "desc": "FAEï¼ˆç‰¹å¾è‡ªç¼–ç å™¨ï¼‰æ˜¯ä¸€ç§æ¡†æ¶ï¼Œåˆ©ç”¨ç‰¹å¾è‡ªç¼–ç å™¨å’ŒåŒè§£ç å™¨ï¼Œå°†é¢„è®­ç»ƒçš„è§†è§‰è¡¨ç¤ºé€‚åº”äºç”Ÿæˆæ¨¡å‹ï¼Œä»è€Œåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å®ç°é«˜æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†é«˜ç»´ç‰¹å¾æ˜ å°„åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œè§£å†³äº†ç†è§£å¯¼å‘ç‰¹å¾ä¸ç”Ÿæˆå‹å¥½æ½œåœ¨ç©ºé—´ä¹‹é—´çš„åŸºæœ¬ä¸åŒ¹é…é—®é¢˜ã€‚FAEçš„å…³é”®åœ¨äºç»“åˆä¸¤ä¸ªç‹¬ç«‹çš„æ·±åº¦è§£ç å™¨ï¼Œä¸€ä¸ªç”¨äºé‡å»ºåŸå§‹ç‰¹å¾ç©ºé—´ï¼Œå¦ä¸€ä¸ªåˆ™åˆ©ç”¨é‡å»ºçš„ç‰¹å¾è¿›è¡Œå›¾åƒç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFAEåœ¨å¤šç§ç”Ÿæˆæ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œå­¦ä¹ é€Ÿåº¦ä¸Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06533",
            "title": "Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning",
            "url": "https://huggingface.co/papers/2512.06533",
            "abstract": "Reinforcement Learning enhances decoding-based regression by introducing sequence-level rewards, improving precision and generalization over token-level methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-12-06",
            "pub_date_card": {
                "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 6",
                "zh": "12æœˆ6æ—¥"
            },
            "hash": "23079536852c67f2",
            "authors": [
                "Ming Chen",
                "Sheng Tang",
                "Rong-Xi Tan",
                "Ziniu Li",
                "Jiacheng Chen",
                "Ke Xue",
                "Chao Qian"
            ],
            "affiliations": [
                "Department of Computer Science and Engineering, The Chinese University of Hong Kong",
                "National Key Laboratory for Novel Software Technology, Nanjing University",
                "School of Artificial Intelligence, Nanjing University",
                "School of Data Science, The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.06533.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'dict'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.07805",
            "title": "Group Representational Position Encoding",
            "url": "https://huggingface.co/papers/2512.07805",
            "abstract": "GRAPE is a unified positional encoding framework that combines multiplicative rotations and additive logit biases, extending existing methods like RoPE and ALiBi.  \t\t\t\t\tAI-generated summary \t\t\t\t We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in SO(d) and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group GL. In Multiplicative GRAPE, a position n in Z (or t in R) acts as G(n)=exp(n,Ï‰,L) with a rank-2 skew generator L in R^{d times d}, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the d/2 planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at O(d) and O(r d) cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "78d1417c6281ee3d",
            "authors": [
                "Yifan Zhang",
                "Zixiang Chen",
                "Yifeng Liu",
                "Zhen Qin",
                "Huizhuo Yuan",
                "Kangping Xu",
                "Yang Yuan",
                "Quanquan Gu",
                "Andrew Chi-Chih Yao"
            ],
            "affiliations": [
                "IIIS, Tsinghua University",
                "Princeton University",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.07805.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#long_context",
                    "#architecture",
                    "#math"
                ],
                "emoji": "ğŸŒ€",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° GRAPE â€” ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¿Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ SO(d) Ğ¸ Ğ°Ğ´Ğ´Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ² Ğ¸Ğ· ÑƒĞ½Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¿Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ GRAPE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½ÑƒÑ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñƒ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ RoPE ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¼ ÑĞ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑÑ‚Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞĞ´Ğ´Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ GRAPE Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ALiBi Ğ¸ Forgetting Transformer ĞºĞ°Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ñ€Ğ°Ğ½Ğ³Ğ°-1, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº GRAPE Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹."
                },
                "en": {
                    "title": "GRAPE: Unifying Positional Encoding for Enhanced Model Performance",
                    "desc": "GRAPE is a new framework for positional encoding in machine learning that integrates two key methods: multiplicative rotations and additive logit biases. It uses mathematical structures from group theory to create a unified approach that enhances how models understand position in data. The framework allows for efficient representation of complex relationships between features, improving performance in tasks with long contexts. By encompassing existing methods like RoPE and ALiBi, GRAPE provides a flexible and powerful tool for developing advanced models."
                },
                "zh": {
                    "title": "GRAPEï¼šç»Ÿä¸€çš„ä½ç½®ç¼–ç æ¡†æ¶",
                    "desc": "GRAPEæ˜¯ä¸€ç§ç»Ÿä¸€çš„ä½ç½®ç¼–ç æ¡†æ¶ï¼Œç»“åˆäº†ä¹˜æ³•æ—‹è½¬å’ŒåŠ æ³•é€»è¾‘åç½®ï¼Œæ‰©å±•äº†ç°æœ‰çš„æ–¹æ³•å¦‚RoPEå’ŒALiBiã€‚å®ƒå°†ä¸¤ç§æœºåˆ¶ç»“åˆåœ¨ä¸€èµ·ï¼šä¹˜æ³•GRAPEå’ŒåŠ æ³•GRAPEï¼Œåˆ†åˆ«åŸºäºç¾¤ä½“ä½œç”¨ã€‚ä¹˜æ³•GRAPEé€šè¿‡åœ¨SO(d)ä¸­çš„æ—‹è½¬å®ç°ç›¸å¯¹çš„ã€ç»„åˆçš„ã€ä¿æŒèŒƒæ•°çš„æ˜ å°„ï¼Œè€ŒåŠ æ³•GRAPEåˆ™é€šè¿‡ä½ç§©çš„å•å…ƒä½œç”¨ç”ŸæˆåŠ æ³•é€»è¾‘ã€‚GRAPEä¸ºé•¿ä¸Šä¸‹æ–‡æ¨¡å‹ä¸­çš„ä½ç½®å‡ ä½•æä¾›äº†ä¸€ä¸ªæœ‰åŸåˆ™çš„è®¾è®¡ç©ºé—´ï¼Œæ¶µç›–äº†RoPEå’ŒALiBiä½œä¸ºç‰¹ä¾‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06963",
            "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
            "url": "https://huggingface.co/papers/2512.06963",
            "abstract": "VideoVLA uses a multi-modal Diffusion Transformer to predict actions and visual outcomes from language and image inputs, enabling strong generalization in robotic manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-07",
            "pub_date_card": {
                "ru": "7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 7",
                "zh": "12æœˆ7æ—¥"
            },
            "hash": "4bad1a680c044c57",
            "authors": [
                "Yichao Shen",
                "Fangyun Wei",
                "Zhiying Du",
                "Yaobo Liang",
                "Yan Lu",
                "Jiaolong Yang",
                "Nanning Zheng",
                "Baining Guo"
            ],
            "affiliations": [
                "Fudan University",
                "IAIR, Xian Jiaotong University",
                "Microsoft Research Asia"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.06963.jpg",
            "data": {
                "categories": [
                    "#agi",
                    "#multimodal",
                    "#architecture",
                    "#video",
                    "#robotics",
                    "#diffusion"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ²",
                    "desc": "VideoVLA â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Vision-Language-Action, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Diffusion Transformer Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ ÑƒÑĞ¿ĞµÑ…Ğ¾Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Empowering Robots with Visual Imagination for Action Prediction",
                    "desc": "VideoVLA is a novel approach that utilizes a multi-modal Diffusion Transformer to enhance robotic manipulation by predicting actions and visual outcomes based on language and image inputs. This method addresses the limitations of existing Vision-Language-Action (VLA) models, which struggle to generalize to new tasks and environments. By leveraging pre-trained video generation models, VideoVLA effectively combines video, language, and action modalities to forecast both the actions a robot should take and the expected visual results. The findings indicate that this dual-prediction strategy not only improves task success but also enables robots to adapt to unfamiliar objects and skills, marking a significant advancement in robot learning."
                },
                "zh": {
                    "title": "è§†é¢‘ç”Ÿæˆä¸æœºå™¨äººæ“æ§çš„ç»“åˆ",
                    "desc": "VideoVLAæ˜¯ä¸€ç§å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨ï¼Œèƒ½å¤Ÿæ ¹æ®è¯­è¨€å’Œå›¾åƒè¾“å…¥é¢„æµ‹åŠ¨ä½œå’Œè§†è§‰ç»“æœï¼Œä»è€Œåœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å®ç°å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡å°†å¤§å‹è§†é¢‘ç”Ÿæˆæ¨¡å‹è½¬åŒ–ä¸ºæœºå™¨äººVLAæ“æ§å™¨ï¼Œæ¢ç´¢äº†è§†é¢‘ç”Ÿæˆä¸æœºå™¨äººæ“ä½œçš„ç»“åˆã€‚å®éªŒè¡¨æ˜ï¼Œé«˜è´¨é‡çš„æƒ³è±¡æœªæ¥ä¸å¯é çš„åŠ¨ä½œé¢„æµ‹å’Œä»»åŠ¡æˆåŠŸç‡å¯†åˆ‡ç›¸å…³ï¼Œå¼ºè°ƒäº†è§†è§‰æƒ³è±¡åœ¨æ“ä½œä¸­çš„é‡è¦æ€§ã€‚VideoVLAå±•ç¤ºäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¨¡ä»¿å…¶ä»–å®ä¾‹çš„æŠ€èƒ½å’Œå¤„ç†æ–°ç‰©ä½“ï¼Œæ¨åŠ¨äº†æœºå™¨äººå­¦ä¹ çš„æ–°èŒƒå¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06835",
            "title": "Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning",
            "url": "https://huggingface.co/papers/2512.06835",
            "abstract": "DoGe, a dual-decoupling framework, enhances vision-language models by separating context learning from problem solving, using a curriculum learning pipeline to improve reward signals and data diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-07",
            "pub_date_card": {
                "ru": "7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 7",
                "zh": "12æœˆ7æ—¥"
            },
            "hash": "0a8fcf1d1267bd3c",
            "authors": [
                "Tingyu Li",
                "Zheng Sun",
                "Jingxuan Wei",
                "Siyuan Li",
                "Conghui He",
                "Lijun Wu",
                "Cheng Tan"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "Shanghai JiaoTong University",
                "University of Chinese Academy of Sciences",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.06835.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#multimodal",
                    "#benchmark",
                    "#synthetic",
                    "#cv"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ´ĞµĞ»ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹: Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ°Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "DoGe â€” ÑÑ‚Ğ¾ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¾Ñ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ RL Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Â«ĞœÑ‹ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒÂ» Ğ¸ Â«Ğ ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÂ», Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ reward hacking. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ curriculum learning Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¿ÑƒĞ»Ğ¾Ğ¼ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Decoupling Context and Problem Solving for Enhanced Vision-Language Models",
                    "desc": "The paper introduces DoGe, a dual-decoupling framework designed to improve vision-language models (VLMs) by separating the learning of context from the actual problem-solving process. This approach utilizes a curriculum learning pipeline to enhance the quality of reward signals and increase the diversity of training data, addressing challenges in specialized domains. By decoupling the learning into two components, Thinker and Solver, the framework allows for better quantification of rewards and a more effective reinforcement learning (RL) strategy. Experimental results demonstrate that DoGe significantly outperforms existing methods, paving the way for more robust self-evolving large vision-language models (LVLMs)."
                },
                "zh": {
                    "title": "è§£è€¦å­¦ä¹ ï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›",
                    "desc": "DoGeæ˜¯ä¸€ç§åŒé‡è§£è€¦æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚å®ƒé€šè¿‡å°†ä¸Šä¸‹æ–‡å­¦ä¹ ä¸é—®é¢˜è§£å†³åˆ†å¼€ï¼Œåˆ©ç”¨è¯¾ç¨‹å­¦ä¹ æµç¨‹æ¥æ”¹å–„å¥–åŠ±ä¿¡å·å’Œæ•°æ®å¤šæ ·æ€§ã€‚è¯¥æ–¹æ³•é¦–å…ˆå¼•å¯¼æ¨¡å‹ä»ä¸Šä¸‹æ–‡ä¸­å­¦ä¹ ï¼Œè€Œä¸æ˜¯ç›´æ¥è§£å†³é—®é¢˜ï¼Œä»è€Œé¿å…äº†åˆæˆæ•°æ®æ–¹æ³•çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDoGeåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä¸ºå®ç°è‡ªæˆ‘è¿›åŒ–çš„å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹æä¾›äº†å¯æ‰©å±•çš„è·¯å¾„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06421",
            "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation",
            "url": "https://huggingface.co/papers/2512.06421",
            "abstract": "Self-Autoregressive Refinement (SAR) improves the quality of autoregressive generative models by addressing exposure bias through Stagger-Scale Rollout and Contrastive Student-Forcing Loss, leading to consistent improvements with minimal computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-06",
            "pub_date_card": {
                "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 6",
                "zh": "12æœˆ6æ—¥"
            },
            "hash": "4eb23fe19101d4d8",
            "authors": [
                "Gengze Zhou",
                "Chongjian Ge",
                "Hao Tan",
                "Feng Liu",
                "Yicong Hong"
            ],
            "affiliations": [
                "Adobe Research",
                "Australian Institute for Machine Learning, Adelaide University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.06421.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#cv",
                    "#architecture"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Self-Autoregressive Refinement (SAR) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ exposure bias Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ coarse-to-fine. SAR Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Stagger-Scale Rollout, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğµ Ğ°Ğ²Ñ‚Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ train-test Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹, Ğ¸ Contrastive Student-Forcing Loss Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SAR Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ SAR Ğº Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ°Ñ‘Ñ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Autoregressive Models with Self-Autoregressive Refinement",
                    "desc": "Self-Autoregressive Refinement (SAR) enhances autoregressive generative models by tackling exposure bias, which occurs when models generate outputs based on their own previous predictions. It introduces a Stagger-Scale Rollout (SSR) technique that allows models to learn from their intermediate outputs, improving the alignment between training and testing phases. Additionally, the Contrastive Student-Forcing Loss (CSFL) provides better supervision for the model's self-generated contexts, leading to more stable training. Experimental results demonstrate that SAR significantly improves generation quality with minimal computational costs, making it a promising method for visual autoregressive generation."
                },
                "zh": {
                    "title": "è‡ªå›å½’ç²¾ç‚¼ï¼šæå‡ç”Ÿæˆæ¨¡å‹è´¨é‡çš„æœ‰æ•ˆæ–¹æ³•",
                    "desc": "è‡ªå›å½’ç²¾ç‚¼ï¼ˆSARï¼‰é€šè¿‡å¼•å…¥åˆ†å±‚è§„æ¨¡å›æ»šå’Œå¯¹æ¯”å­¦ç”Ÿå¼ºåˆ¶æŸå¤±ï¼Œæ”¹å–„äº†è‡ªå›å½’ç”Ÿæˆæ¨¡å‹çš„è´¨é‡ï¼Œè§£å†³äº†æ›å…‰åå·®é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡è½»é‡çº§çš„è‡ªå›å½’å›æ»šï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ¥è§¦åˆ°è‡ªèº«çš„ä¸­é—´é¢„æµ‹ï¼Œä»è€Œå¯¹é½è®­ç»ƒå’Œæµ‹è¯•æ¨¡å¼ã€‚SARè¿˜æä¾›äº†è¶³å¤Ÿçš„ç›‘ç£ï¼Œä»¥ç¡®ä¿è‡ªç”Ÿæˆä¸Šä¸‹æ–‡çš„ç¨³å®šè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSARåœ¨é¢„è®­ç»ƒçš„è‡ªå›å½’æ¨¡å‹ä¸Šåº”ç”¨åï¼Œç”Ÿæˆè´¨é‡æ˜¾è‘—æé«˜ï¼Œè®¡ç®—å¼€é”€æå°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06558",
            "title": "Embodied Referring Expression Comprehension in Human-Robot Interaction",
            "url": "https://huggingface.co/papers/2512.06558",
            "abstract": "A large-scale dataset and multimodal model improve embodied interaction comprehension in robots by addressing perspective bias and enhancing multimodal signal integration.  \t\t\t\t\tAI-generated summary \t\t\t\t As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-12-06",
            "pub_date_card": {
                "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 6",
                "zh": "12æœˆ6æ—¥"
            },
            "hash": "49b6bc2ca4f8291d",
            "authors": [
                "Md Mofijul Islam",
                "Alexi Gladstone",
                "Sujan Sarker",
                "Ganesh Nanduru",
                "Md Fahim",
                "Keyan Du",
                "Aman Chadha",
                "Tariq Iqbal"
            ],
            "affiliations": [
                "Amazon GenAI",
                "Stanford University",
                "University of Dhaka",
                "University of Virginia"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.06558.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#benchmark",
                    "#robotics",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Refer360, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ½ĞµĞ²ĞµÑ€Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ MuRes â€” Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒĞ·ĞºĞ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ MuRes Ğº ÑÑ‚Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°."
                },
                "en": {
                    "title": "Enhancing Robot Comprehension with Multimodal Learning and Diverse Datasets",
                    "desc": "This paper addresses the challenge of robots understanding human instructions through embodied interactions, which is essential for effective human-robot interaction (HRI). It introduces the Refer360 dataset, a comprehensive collection of verbal and nonverbal interactions captured from multiple perspectives in various environments. The authors also propose MuRes, a multimodal guided residual module that enhances the integration of different types of signals to improve comprehension of referring expressions. Experimental results show that incorporating MuRes significantly boosts the performance of existing multimodal models in understanding embodied interactions."
                },
                "zh": {
                    "title": "æå‡æœºå™¨äººç†è§£èƒ½åŠ›çš„å¤šæ¨¡æ€æ•°æ®é›†ä¸æ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†Refer360æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¹å–„æœºå™¨äººå¯¹äººç±»æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†é€šè¿‡å¤šè§’åº¦æ”¶é›†å®¤å†…å’Œå®¤å¤–çš„è‡ªç„¶äº’åŠ¨ï¼Œè§£å†³äº†ç°æœ‰æ•°æ®é›†ä¸­å­˜åœ¨çš„è§†è§’åå·®å’Œéè¯­è¨€æ‰‹åŠ¿è¦†ç›–ä¸è¶³çš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†MuResæ¨¡å—ï¼Œå®ƒé€šè¿‡æå–ç‰¹å®šæ¨¡æ€çš„å…³é”®ä¿¡æ¯ï¼Œå¢å¼ºäº†æœºå™¨äººå¯¹æŒ‡ä»£è¡¨è¾¾çš„ç†è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆMuResçš„å¤šæ¨¡æ€æ¨¡å‹åœ¨ç†è§£äººç±»äº’åŠ¨æ–¹é¢è¡¨ç°æ›´ä½³ï¼Œå±•ç¤ºäº†å…¶åœ¨æœºå™¨äººä¸äººç±»ç¯å¢ƒä¸­åº”ç”¨çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.07168",
            "title": "JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention",
            "url": "https://huggingface.co/papers/2512.07168",
            "abstract": "A two-stage self-supervised framework combines JEPA with DAAM to learn robust speech representations, using masked prediction, FSQ, mixed-radix packing, and HiFi-GAN for efficient tokenization and waveform reconstruction.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Density Adaptive Attention Mechanism (DAAM) for learning robust speech representations. Stage~1 uses JEPA with DAAM to learn semantic audio features via masked prediction in latent space, fully decoupled from waveform reconstruction. Stage~2 leverages these representations for efficient tokenization using Finite Scalar Quantization (FSQ) and a mixed-radix packing scheme, followed by high-fidelity waveform reconstruction with a HiFi-GAN decoder. By integrating Gaussian mixture-based density-adaptive gating into the JEPA encoder, the model performs adaptive temporal feature selection and discovers hierarchical speech structure at a low frame rate of 2.5~Hz. The resulting tokens (47.5 tokens/sec) provide a reversible, highly compressed, and language-model-friendly representation that is competitive with, and often more efficient than, existing neural audio codecs.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-12-08",
            "pub_date_card": {
                "ru": "8 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 8",
                "zh": "12æœˆ8æ—¥"
            },
            "hash": "1a0d5d64f0ce857b",
            "authors": [
                "Georgios Ioannides",
                "Christos Constantinou",
                "Aman Chadha",
                "Aaron Elkins",
                "Linsey Pang",
                "Ravid Shwartz-Ziv",
                "Yann LeCun"
            ],
            "affiliations": [
                "Amazon GenAI",
                "Carnegie Mellon University",
                "James Silberrad Brown Center for Artificial Intelligence",
                "New York University",
                "Northeastern University",
                "Stanford University",
                "University of Bristol"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.07168.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'dict'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06791",
            "title": "Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games",
            "url": "https://huggingface.co/papers/2512.06791",
            "abstract": "The SGN condition provides a framework for certifying convergence of gradient-based learning in games by constructing a weighted block metric, enabling convergence under conditions where Euclidean geometry fails.  \t\t\t\t\tAI-generated summary \t\t\t\t Classical convergence guarantees for gradient-based learning in games require the pseudo-gradient to be (strongly) monotone in Euclidean geometry as shown by rosen(1965), a condition that often fails even in simple games with strong cross-player couplings. We introduce Small-Gain Nash (SGN), a block small-gain condition in a custom block-weighted geometry. SGN converts local curvature and cross-player Lipschitz coupling bounds into a tractable certificate of contraction. It constructs a weighted block metric in which the pseudo-gradient becomes strongly monotone on any region where these bounds hold, even when it is non-monotone in the Euclidean sense. The continuous flow is exponentially contracting in this designed geometry, and projected Euler and RK4 discretizations converge under explicit step-size bounds derived from the SGN margin and a local Lipschitz constant. Our analysis reveals a certified ``timescale band'', a non-asymptotic, metric-based certificate that plays a TTUR-like role: rather than forcing asymptotic timescale separation via vanishing, unequal step sizes, SGN identifies a finite band of relative metric weights for which a single-step-size dynamics is provably contractive. We validate the framework on quadratic games where Euclidean monotonicity analysis fails to predict convergence, but SGN successfully certifies it, and extend the construction to mirror/Fisher geometries for entropy-regularized policy gradient in Markov games. The result is an offline certification pipeline that estimates curvature, coupling, and Lipschitz parameters on compact regions, optimizes block weights to enlarge the SGN margin, and returns a structural, computable convergence certificate consisting of a metric, contraction rate, and safe step-sizes for non-monotone games.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-12-07",
            "pub_date_card": {
                "ru": "7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 7",
                "zh": "12æœˆ7æ—¥"
            },
            "hash": "31de0f6349469bd7",
            "authors": [
                "Vedansh Sharma"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.06791.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#games",
                    "#math"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ¡ĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ³Ñ€ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ĞµĞ²ĞºĞ»Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° SGN (Small-Gain Nash) Ğ´Ğ»Ñ ÑĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ² ĞµĞ²ĞºĞ»Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ ĞºÑ€Ğ¸Ğ²Ğ¸Ğ·Ğ½Ğ° Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ›Ğ¸Ğ¿ÑˆĞ¸Ñ†Ğ° Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ, Ğ² ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ¶Ğµ Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ…, Ğ³Ğ´Ğµ ÑÑ‚Ğ¾ Ğ½Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ Ğ² ĞµĞ²ĞºĞ»Ğ¸Ğ´Ğ¾Ğ²Ğ¾Ğ¼ ÑĞ¼Ñ‹ÑĞ»Ğµ, Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ ÑˆĞ°Ğ³Ğ°. ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ÑÑ Ğ½Ğ° Ğ·ĞµÑ€ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾-Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ² Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ…."
                },
                "en": {
                    "title": "Certifying Convergence in Non-Monotone Games with SGN",
                    "desc": "The paper introduces the Small-Gain Nash (SGN) condition, which provides a new way to certify convergence in gradient-based learning for games, especially when traditional methods fail. It constructs a weighted block metric that allows the pseudo-gradient to be strongly monotone, even in cases where it is not in Euclidean geometry. This approach translates local curvature and cross-player Lipschitz bounds into a practical certificate of contraction, ensuring that the learning process converges. The authors validate their method on quadratic games and extend it to other geometries, creating a comprehensive certification pipeline for non-monotone games."
                },
                "zh": {
                    "title": "SGNæ¡ä»¶ï¼šåšå¼ˆå­¦ä¹ æ”¶æ•›çš„æ–°æ¡†æ¶",
                    "desc": "SGNæ¡ä»¶ä¸ºåŸºäºæ¢¯åº¦çš„åšå¼ˆå­¦ä¹ æä¾›äº†ä¸€ä¸ªæ”¶æ•›æ€§è¯æ˜æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºåŠ æƒå—åº¦é‡ï¼Œä½¿å¾—åœ¨æ¬§å‡ é‡Œå¾—å‡ ä½•å¤±æ•ˆçš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°æ”¶æ•›ã€‚ä¼ ç»Ÿçš„æ”¶æ•›ä¿è¯è¦æ±‚ä¼ªæ¢¯åº¦åœ¨æ¬§å‡ é‡Œå¾—å‡ ä½•ä¸­æ˜¯ï¼ˆå¼ºï¼‰å•è°ƒçš„ï¼Œä½†åœ¨è®¸å¤šç®€å•åšå¼ˆä¸­è¿™ä¸€æ¡ä»¶å¸¸å¸¸ä¸æˆç«‹ã€‚SGNå°†å±€éƒ¨æ›²ç‡å’Œè·¨ç©å®¶çš„Lipschitzè€¦åˆç•Œé™è½¬åŒ–ä¸ºå¯å¤„ç†çš„æ”¶ç¼©è¯æ˜ï¼Œå¹¶åœ¨è¿™äº›ç•Œé™æˆç«‹çš„åŒºåŸŸå†…ä½¿ä¼ªæ¢¯åº¦åœ¨åŠ æƒå—åº¦é‡ä¸‹å˜å¾—å¼ºå•è°ƒã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸€ä¸ªè®¤è¯çš„â€œæ—¶é—´å°ºåº¦å¸¦â€ï¼Œä¸ºéæ¸è¿‘æ€§ã€åŸºäºåº¦é‡çš„æ”¶æ•›è¯æ˜æä¾›äº†æ”¯æŒï¼Œç¡®ä¿åœ¨ç‰¹å®šçš„åº¦é‡æƒé‡ä¸‹ï¼Œå•æ­¥åŠ¨æ€æ˜¯å¯è¯æ˜çš„æ”¶ç¼©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06609",
            "title": "Vector Quantization using Gaussian Variational Autoencoder",
            "url": "https://huggingface.co/papers/2512.06609",
            "abstract": "Gaussian Quant (GQ) converts Gaussian VAE to VQ-VAE without training, outperforming previous VQ-VAEs and Gaussian VAE discretization methods across different architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-12-07",
            "pub_date_card": {
                "ru": "7 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 7",
                "zh": "12æœˆ7æ—¥"
            },
            "hash": "646946f537b624a7",
            "authors": [
                "Tongda Xu",
                "Wendi Zheng",
                "Jiajun He",
                "Jose Miguel Hernandez-Lobato",
                "Yan Wang",
                "Ya-Qin Zhang",
                "Jie Tang"
            ],
            "affiliations": [
                "Tsinghua University",
                "University of Cambridge",
                "Zhipu AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.06609.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'list'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.05100",
            "title": "Structured Document Translation via Format Reinforcement Learning",
            "url": "https://huggingface.co/papers/2512.05100",
            "abstract": "Format Reinforcement Learning enhances structured text translation by optimizing structure-aware rewards and distinguishing between minor errors and major structural failures.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose Format Reinforcement Learning (FormatRL), which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "c7232cc74496b432",
            "authors": [
                "Haiyue Song",
                "Johannes Eschbach-Dymanus",
                "Hour Kaing",
                "Sumire Honda",
                "Hideki Tanaka",
                "Bianka Buschbeck",
                "Masao Utiyama"
            ],
            "affiliations": [
                "National Institute of Information and Communications Technology, Japan",
                "SAP, Germany"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.05100.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#training",
                    "#optimization",
                    "#benchmark",
                    "#rlhf",
                    "#translation"
                ],
                "emoji": "ğŸ“‹",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾-Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Format Reinforcement Learning Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Group Relative Policy Optimization Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ: TreeSim Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° XML-Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ² Ğ¸ Node-chrF Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑƒĞ·Ğ»Ğ¾Ğ². Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° StrucAUC, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑĞ±Ğ¾Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ SAP Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²ĞºĞ»Ğ°Ğ´ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹."
                },
                "en": {
                    "title": "Optimizing Structured Text Translation with Format Reinforcement Learning",
                    "desc": "This paper introduces Format Reinforcement Learning (FormatRL) to improve structured text translation, particularly for complex document formats like XML and HTML. It utilizes Group Relative Policy Optimization to enhance a supervised model by optimizing structure-aware rewards, specifically TreeSim and Node-chrF, which evaluate structural similarity and translation quality, respectively. The method also incorporates StrucAUC, a metric that differentiates between minor and major errors in structure. Experiments show that FormatRL significantly enhances translation quality and structural accuracy on the SAP software-documentation benchmark."
                },
                "zh": {
                    "title": "æ ¼å¼å¼ºåŒ–å­¦ä¹ ï¼šä¼˜åŒ–ç»“æ„åŒ–æ–‡æœ¬ç¿»è¯‘çš„å…³é”®",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ ¼å¼å¼ºåŒ–å­¦ä¹ ï¼ˆFormatRLï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„ç»“æ„åŒ–æ–‡æœ¬ç¿»è¯‘ï¼Œç‰¹åˆ«æ˜¯å¤„ç†å¤æ‚çš„æ–‡æ¡£çº§XMLæˆ–HTMLç»“æ„ã€‚è¯¥æ–¹æ³•åœ¨ç›‘ç£å¾®è°ƒæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œé‡‡ç”¨äº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œç›´æ¥ä¼˜åŒ–æ–°çš„ç»“æ„æ„ŸçŸ¥å¥–åŠ±ï¼ŒåŒ…æ‹¬æ ‘ç»“æ„ç›¸ä¼¼åº¦å’ŒXMLèŠ‚ç‚¹ç¿»è¯‘è´¨é‡çš„è¯„ä¼°ã€‚é€šè¿‡å¼•å…¥ç»†ç²’åº¦çš„StrucAUCæŒ‡æ ‡ï¼Œèƒ½å¤ŸåŒºåˆ†å°é”™è¯¯å’Œé‡å¤§ç»“æ„å¤±è´¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…­ä¸ªæŒ‡æ ‡ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œè¿›ä¸€æ­¥åˆ†ææ˜¾ç¤ºä¸åŒå¥–åŠ±å‡½æ•°å¯¹ç»“æ„å’Œç¿»è¯‘è´¨é‡çš„æ”¹å–„æœ‰é‡è¦è´¡çŒ®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.03704",
            "title": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue",
            "url": "https://huggingface.co/papers/2512.03704",
            "abstract": "DZ-TDPO framework improves long-context dialogue systems by using dynamic KL constraints and temporal attention bias to resolve user intent conflicts with historical context, achieving high win rates and zero-shot generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a calibrated temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (55.4% on Phi-3.5) while maintaining robust zero-shot generalization. Our scaling analysis reveals a \"Capacity-Stability Trade-off\": while smaller models incur an \"alignment tax\" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves 50.8% win rate with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "b9b417eec9d7ef33",
            "authors": [
                "Yijun Liao"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.03704.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'list'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.06032",
            "title": "The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation",
            "url": "https://huggingface.co/papers/2512.06032",
            "abstract": "The paper contrasts SAM2, a prompt-based segmentation model, with SAM3, a multimodal concept-driven model, highlighting differences in architecture, dataset use, training, and evaluation metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.",
            "score": 0,
            "issue_id": 1,
            "pub_date": "2025-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "367de46bb263f9b3",
            "authors": [
                "Ranjan Sapkota",
                "Konstantinos I. Roumeliotis",
                "Manoj Karkee"
            ],
            "affiliations": [
                "Cornell University",
                "University of the Peloponnese"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.06032.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        }
    ],
    "link_prev": "2025-12-08.html",
    "link_next": "2025-12-10.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "08.12",
        "en": "12/08",
        "zh": "12æœˆ8æ—¥"
    },
    "short_date_next": {
        "ru": "10.12",
        "en": "12/10",
        "zh": "12æœˆ10æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 9,
        "#agents": 0,
        "#cv": 6,
        "#rl": 7,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 3,
        "#multimodal": 6,
        "#math": 3,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 0,
        "#training": 9,
        "#robotics": 2,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 7,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 8,
        "#survey": 0,
        "#diffusion": 5,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 7,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 0,
        "#translation": 1
    }
}