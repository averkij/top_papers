
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 12 papers. September 10.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">10 сентября</span> | <span id="title-articles-count">12 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-09-09.html">⬅️ <span id="prev-date">09.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-09-11.html">➡️ <span id="next-date">11.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-09.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '10 сентября', 'en': 'September 10', 'zh': '9月10日'};
        let feedDateNext = {'ru': '11.09', 'en': '09/11', 'zh': '9月11日'};
        let feedDatePrev = {'ru': '09.09', 'en': '09/09', 'zh': '9月9日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2409.02795', 'title': 'Towards a Unified View of Preference Learning for Large Language Models: A Survey', 'url': 'https://huggingface.co/papers/2409.02795', 'abstract': "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of the crucial factors to achieve success is aligning the LLM's output with human preferences. This alignment process often requires only a small amount of data to efficiently enhance the LLM's performance. While effective, research in this area spans multiple domains, and the methods involved are relatively complex to understand. The relationships between different methods have been under-explored, limiting the development of the preference alignment. In light of this, we break down the existing popular alignment strategies into different components and provide a unified framework to study the current alignment strategies, thereby establishing connections among them. In this survey, we decompose all the strategies in preference learning into four components: model, data, feedback, and algorithm. This unified view offers an in-depth understanding of existing alignment algorithms and also opens up possibilities to synergize the strengths of different strategies. Furthermore, we present detailed working examples of prevalent existing algorithms to facilitate a comprehensive understanding for the readers. Finally, based on our unified perspective, we explore the challenges and future research directions for aligning large language models with human preferences.", 'score': 72, 'issue_id': 1, 'pub_date': '2024-09-04', 'pub_date_card': {'ru': '4 сентября', 'en': 'September 4', 'zh': '9月4日'}, 'hash': '106d90c6a1457d6c', 'data': {'categories': ['#training', '#rlhf', '#survey', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Унификация стратегий настройки LLM под человеческие предпочтения', 'desc': 'Эта статья посвящена методам настройки больших языковых моделей (LLM) в соответствии с человеческими предпочтениями. Авторы предлагают унифицированную структуру для изучения существующих стратегий выравнивания, разбивая их на четыре компонента: модель, данные, обратная связь и алгоритм. Такой подход позволяет глубже понять существующие алгоритмы и открывает возможности для синергии сильных сторон различных стратегий. Статья также рассматривает проблемы и будущие направления исследований в области настройки LLM под человеческие предпочтения.'}, 'en': {'title': 'Unifying Strategies for Aligning LLMs with Human Preferences', 'desc': 'This paper focuses on improving how Large Language Models (LLMs) align their outputs with human preferences. It identifies that this alignment can be achieved with minimal data, but the existing methods are complex and not well connected. The authors propose a unified framework that breaks down alignment strategies into four key components: model, data, feedback, and algorithm. By doing so, they aim to clarify the relationships between different methods and suggest future research directions for enhancing preference alignment in LLMs.'}, 'zh': {'title': '统一视角下的偏好对齐策略', 'desc': '大型语言模型（LLMs）展现出强大的能力，成功的关键在于将模型输出与人类偏好对齐。这个对齐过程通常只需少量数据即可有效提升模型性能。尽管已有有效的方法，但不同方法之间的关系尚未得到充分探索，限制了偏好对齐的发展。我们将现有的对齐策略分解为模型、数据、反馈和算法四个组成部分，提供一个统一的框架，以便深入理解现有的对齐算法，并探讨未来的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2409.05840', 'title': 'MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct', 'url': 'https://huggingface.co/papers/2409.05840', 'abstract': 'The development of Multimodal Large Language Models (MLLMs) has seen significant advancements. However, the quantity and quality of multimodal instruction data have emerged as significant bottlenecks in their progress. Manually creating multimodal instruction data is both time-consuming and inefficient, posing challenges in producing instructions of high complexity. Moreover, distilling instruction data from black-box commercial models (e.g., GPT-4o, GPT-4V) often results in simplistic instruction data, which constrains performance to that of these models. The challenge of curating diverse and complex instruction data remains substantial. We propose MMEvol, a novel multimodal instruction data evolution framework that combines fine-grained perception evolution, cognitive reasoning evolution, and interaction evolution. This iterative approach breaks through data quality bottlenecks to generate a complex and diverse image-text instruction dataset, thereby empowering MLLMs with enhanced capabilities. Beginning with an initial set of instructions, SEED-163K, we utilize MMEvol to systematically broadens the diversity of instruction types, integrates reasoning steps to enhance cognitive capabilities, and extracts detailed information from images to improve visual understanding and robustness. To comprehensively evaluate the effectiveness of our data, we train LLaVA-NeXT using the evolved data and conduct experiments across 13 vision-language tasks. Compared to the baseline trained with seed data, our approach achieves an average accuracy improvement of 3.1 points and reaches state-of-the-art (SOTA) performance on 9 of these tasks.', 'score': 45, 'issue_id': 1, 'pub_date': '2024-09-09', 'pub_date_card': {'ru': '9 сентября', 'en': 'September 9', 'zh': '9月9日'}, 'hash': 'f4e9e0ae3e146a8a', 'data': {'categories': ['#reasoning', '#training', '#data', '#optimization', '#benchmark', '#synthetic', '#multimodal'], 'emoji': '🧬', 'ru': {'title': 'MMEvol: эволюция инструкций для прорыва в мультимодальном ИИ', 'desc': 'Статья представляет MMEvol - новый фреймворк для эволюции мультимодальных инструкций, который решает проблему нехватки качественных данных для обучения мультимодальных больших языковых моделей (MLLM). MMEvol использует итеративный подход, сочетающий эволюцию детального восприятия, когнитивных рассуждений и взаимодействий для создания сложного и разнообразного набора инструкций с изображениями и текстом. Начиная с исходного набора SEED-163K, MMEvol систематически расширяет разнообразие типов инструкций, интегрирует шаги рассуждений и извлекает детальную информацию из изображений. Эксперименты показали, что обучение LLaVA-NeXT на эволюционированных данных привело к улучшению средней точности на 3.1 пункта по 13 задачам компьютерного зрения и обработки естественного языка.'}, 'en': {'title': 'Evolving Instruction Data for Superior Multimodal Learning', 'desc': 'This paper introduces MMEvol, a framework designed to enhance the quality and diversity of multimodal instruction data for Multimodal Large Language Models (MLLMs). The authors identify the limitations of current methods, such as manual data creation and simplistic outputs from existing models, which hinder the development of complex instructions. MMEvol employs an iterative process that evolves instruction data by improving perception, cognitive reasoning, and interaction, resulting in a richer dataset. The effectiveness of this approach is demonstrated through training LLaVA-NeXT, which shows significant performance improvements across various vision-language tasks.'}, 'zh': {'title': '突破多模态指令数据瓶颈，提升模型能力！', 'desc': '多模态大型语言模型（MLLMs）在发展中取得了显著进展，但多模态指令数据的数量和质量成为了主要瓶颈。手动创建这些数据既耗时又低效，导致复杂指令的生成面临挑战。我们提出了MMEvol框架，通过细粒度感知演化、认知推理演化和交互演化，系统性地提升指令数据的多样性和复杂性。通过使用MMEvol，我们的实验显示，训练后的模型在13个视觉语言任务中平均准确率提高了3.1个百分点，达到了9个任务的最新性能。'}}}, {'id': 'https://huggingface.co/papers/2409.05152', 'title': 'OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs', 'url': 'https://huggingface.co/papers/2409.05152', 'abstract': "Despite the recent advancements in Large Language Models (LLMs), which have significantly enhanced the generative capabilities for various NLP tasks, LLMs still face limitations in directly handling retrieval tasks. However, many practical applications demand the seamless integration of both retrieval and generation. This paper introduces a novel and efficient One-pass Generation and retrieval framework (OneGen), designed to improve LLMs' performance on tasks that require both generation and retrieval. The proposed framework bridges the traditionally separate training approaches for generation and retrieval by incorporating retrieval tokens generated autoregressively. This enables a single LLM to handle both tasks simultaneously in a unified forward pass. We conduct experiments on two distinct types of composite tasks, RAG and Entity Linking, to validate the pluggability, effectiveness, and efficiency of OneGen in training and inference. Furthermore, our results show that integrating generation and retrieval within the same context preserves the generative capabilities of LLMs while improving retrieval performance. To the best of our knowledge, OneGen is the first to enable LLMs to conduct vector retrieval during the generation.", 'score': 29, 'issue_id': 1, 'pub_date': '2024-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '64a48fe6a15c45e6', 'data': {'categories': ['#reasoning', '#training', '#rag', '#inference', '#optimization', '#alignment', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'OneGen: Объединение генерации и поиска в LLM за один проход', 'desc': 'Статья представляет новую структуру OneGen для улучшения работы больших языковых моделей (LLM) в задачах, требующих как генерации, так и поиска. OneGen объединяет обучение генерации и поиска, включая токены поиска, генерируемые автореgressивно. Это позволяет одной LLM выполнять обе задачи одновременно за один проход. Эксперименты на задачах RAG и Entity Linking показывают эффективность OneGen в обучении и выводе.'}, 'en': {'title': 'OneGen: Unifying Generation and Retrieval for Enhanced LLM Performance', 'desc': 'This paper presents OneGen, a new framework that combines generation and retrieval tasks for Large Language Models (LLMs). Traditionally, these tasks have been handled separately, but OneGen allows them to be performed together in one step. By using retrieval tokens generated in an autoregressive manner, OneGen enhances the efficiency and effectiveness of LLMs in tasks like RAG and Entity Linking. The results demonstrate that this integration not only maintains the generative strengths of LLMs but also boosts their retrieval capabilities.'}, 'zh': {'title': '一体化生成与检索，提升LLMs性能', 'desc': '尽管大型语言模型（LLMs）在自然语言处理任务中取得了显著进展，但在直接处理检索任务时仍然存在局限性。本文提出了一种新颖高效的一次性生成与检索框架（OneGen），旨在提升LLMs在需要生成和检索的任务中的表现。该框架通过自回归生成检索令牌，打破了生成与检索的传统训练方式，使得单个LLM能够在统一的前向传递中同时处理这两项任务。实验结果表明，在同一上下文中整合生成与检索不仅保留了LLMs的生成能力，还提高了检索性能。'}}}, {'id': 'https://huggingface.co/papers/2409.05591', 'title': 'MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery', 'url': 'https://huggingface.co/papers/2409.05591', 'abstract': "Retrieval-Augmented Generation (RAG) leverages retrieval tools to access external databases, thereby enhancing the generation quality of large language models (LLMs) through optimized context. However, the existing retrieval methods are constrained inherently, as they can only perform relevance matching between explicitly stated queries and well-formed knowledge, but unable to handle tasks involving ambiguous information needs or unstructured knowledge. Consequently, existing RAG systems are primarily effective for straightforward question-answering tasks. In this work, we propose MemoRAG, a novel retrieval-augmented generation paradigm empowered by long-term memory. MemoRAG adopts a dual-system architecture. On the one hand, it employs a light but long-range LLM to form the global memory of database. Once a task is presented, it generates draft answers, cluing the retrieval tools to locate useful information within the database. On the other hand, it leverages an expensive but expressive LLM, which generates the ultimate answer based on the retrieved information. Building on this general framework, we further optimize MemoRAG's performance by enhancing its cluing mechanism and memorization capacity. In our experiment, MemoRAG achieves superior performance across a variety of evaluation tasks, including both complex ones where conventional RAG fails and straightforward ones where RAG is commonly applied.", 'score': 28, 'issue_id': 1, 'pub_date': '2024-09-09', 'pub_date_card': {'ru': '9 сентября', 'en': 'September 9', 'zh': '9月9日'}, 'hash': '1cafdc5e6d8fe4f4', 'data': {'categories': ['#reasoning', '#long_context', '#rag', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'MemoRAG: Революция в генерации текста с долговременной памятью', 'desc': 'Статья представляет MemoRAG - новую парадигму генерации с использованием извлечения информации, усиленную долговременной памятью. MemoRAG использует двухсистемную архитектуру: легкую модель для формирования глобальной памяти базы данных и генерации черновых ответов, и мощную модель для создания окончательного ответа на основе извлеченной информации. Эта система превосходит обычные методы RAG как в сложных задачах, где RAG не справляется, так и в простых, где RAG обычно применяется. MemoRAG оптимизирует механизм подсказок и возможности запоминания, что позволяет ей эффективно работать с неоднозначными запросами и неструктурированными знаниями.'}, 'en': {'title': 'MemoRAG: Enhancing RAG with Long-Term Memory for Complex Queries', 'desc': "This paper introduces MemoRAG, a new approach to Retrieval-Augmented Generation (RAG) that improves the performance of large language models (LLMs) by incorporating long-term memory. MemoRAG uses a dual-system architecture, where a lightweight LLM creates a global memory of the database and generates draft answers to guide retrieval tools. The second, more powerful LLM then refines these drafts into final answers using the retrieved information. This method enhances the system's ability to handle complex queries and unstructured knowledge, outperforming traditional RAG systems in various tasks."}, 'zh': {'title': 'MemoRAG：提升生成质量的新方法', 'desc': '本文提出了一种新的检索增强生成框架，称为MemoRAG，旨在通过长期记忆来提升生成质量。MemoRAG采用双系统架构，一方面使用轻量级的长距离大语言模型（LLM）构建全局数据库记忆，另一方面利用复杂但表达能力强的LLM生成最终答案。该方法不仅能处理简单的问答任务，还能应对模糊信息需求和非结构化知识的挑战。实验结果表明，MemoRAG在多种评估任务中表现优越，超越了传统的检索增强生成系统。'}}}, {'id': 'https://huggingface.co/papers/2409.04828', 'title': 'POINTS: Improving Your Vision-language Model with Affordable Strategies', 'url': 'https://huggingface.co/papers/2409.04828', 'abstract': 'In recent years, vision-language models have made significant strides, excelling in tasks like optical character recognition and geometric problem-solving. However, several critical issues remain: 1) Proprietary models often lack transparency about their architectures, while open-source models need more detailed ablations of their training strategies. 2) Pre-training data in open-source works is under-explored, with datasets added empirically, making the process cumbersome. 3) Fine-tuning often focuses on adding datasets, leading to diminishing returns. To address these issues, we propose the following contributions: 1) We trained a robust baseline model using the latest advancements in vision-language models, introducing effective improvements and conducting comprehensive ablation and validation for each technique. 2) Inspired by recent work on large language models, we filtered pre-training data using perplexity, selecting the lowest perplexity data for training. This approach allowed us to train on a curated 1M dataset, achieving competitive performance. 3) During visual instruction tuning, we used model soup on different datasets when adding more datasets yielded marginal improvements. These innovations resulted in a 9B parameter model that performs competitively with state-of-the-art models. Our strategies are efficient and lightweight, making them easily adoptable by the community.', 'score': 22, 'issue_id': 1, 'pub_date': '2024-09-07', 'pub_date_card': {'ru': '7 сентября', 'en': 'September 7', 'zh': '9月7日'}, 'hash': 'cb8f394c31146255', 'data': {'categories': ['#cv', '#training', '#data', '#optimization', '#open_source', '#vision', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'Эффективное обучение мультимодальных моделей: меньше данных, больше результат', 'desc': "Данная статья представляет новый подход к обучению мультимодальных моделей, работающих с изображениями и текстом. Авторы предлагают эффективную стратегию предобучения на отфильтрованном наборе данных, используя перплексию для отбора. Они также применяют технику 'model soup' при дообучении модели на различных задачах. В результате получена компактная 9-миллиардная модель, показывающая результаты на уровне современных state-of-the-art решений."}, 'en': {'title': 'Enhancing Vision-Language Models with Efficient Training Strategies', 'desc': 'This paper discusses advancements in vision-language models, which are used for tasks like recognizing text and solving geometric problems. It highlights issues with current models, such as lack of transparency and inefficient pre-training data usage. The authors propose a robust baseline model that incorporates effective training techniques and a curated dataset filtered by perplexity. Their approach results in a competitive 9B parameter model that is efficient and accessible for the community.'}, 'zh': {'title': '创新视觉语言模型，提升性能与透明度', 'desc': '近年来，视觉语言模型在光学字符识别和几何问题解决等任务上取得了显著进展。然而，现有模型存在一些关键问题，如专有模型缺乏透明度，开源模型的训练策略缺乏详细的分析。此外，开源工作的预训练数据探索不足，导致数据集的添加过程繁琐。为了解决这些问题，我们提出了一种新的方法，通过最新的视觉语言模型训练出一个强大的基线模型，并在预训练数据选择和视觉指令微调方面进行了创新。'}}}, {'id': 'https://huggingface.co/papers/2409.04593', 'title': 'Paper Copilot: A Self-Evolving and Efficient LLM System for Personalized Academic Assistance', 'url': 'https://huggingface.co/papers/2409.04593', 'abstract': 'As scientific research proliferates, researchers face the daunting task of navigating and reading vast amounts of literature. Existing solutions, such as document QA, fail to provide personalized and up-to-date information efficiently. We present Paper Copilot, a self-evolving, efficient LLM system designed to assist researchers, based on thought-retrieval, user profile and high performance optimization. Specifically, Paper Copilot can offer personalized research services, maintaining a real-time updated database. Quantitative evaluation demonstrates that Paper Copilot saves 69.92\\% of time after efficient deployment. This paper details the design and implementation of Paper Copilot, highlighting its contributions to personalized academic support and its potential to streamline the research process.', 'score': 22, 'issue_id': 1, 'pub_date': '2024-09-06', 'pub_date_card': {'ru': '6 сентября', 'en': 'September 6', 'zh': '9月6日'}, 'hash': '66418c66a9050b62', 'data': {'categories': ['#science', '#training', '#rag', '#inference', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Paper Copilot: ИИ-ассистент для эффективной работы с научной литературой', 'desc': 'Исследователи представляют систему Paper Copilot, самообучающуюся систему на основе больших языковых моделей (LLM), предназначенную для помощи ученым в работе с научной литературой. Paper Copilot использует методы извлечения мыслей, профилирования пользователей и оптимизации производительности для предоставления персонализированных исследовательских услуг. Система поддерживает постоянно обновляемую базу данных и, согласно количественной оценке, позволяет сэкономить 69,92% времени после эффективного развертывания. Авторы подробно описывают дизайн и реализацию Paper Copilot, подчеркивая его вклад в персонализированную академическую поддержку и потенциал для оптимизации исследовательского процесса.'}, 'en': {'title': 'Streamlining Research with Personalized AI Assistance', 'desc': 'This paper introduces Paper Copilot, a machine learning system that helps researchers manage and access academic literature more effectively. It utilizes a self-evolving large language model (LLM) to provide personalized research assistance based on user profiles and thought-retrieval techniques. The system maintains a continuously updated database, ensuring that users receive the most relevant and current information. Quantitative results show that Paper Copilot significantly reduces the time researchers spend on literature review by nearly 70%.'}, 'zh': {'title': 'Paper Copilot：个性化学术支持的未来', 'desc': '随着科学研究的不断增加，研究人员面临着阅读大量文献的挑战。现有的解决方案，如文档问答，无法高效地提供个性化和最新的信息。我们提出了Paper Copilot，这是一种自我进化的高效大语言模型系统，旨在基于思维检索、用户档案和高性能优化来辅助研究人员。Paper Copilot能够提供个性化的研究服务，并维护一个实时更新的数据库，显著节省研究时间。'}}}, {'id': 'https://huggingface.co/papers/2409.05865', 'title': 'Robot Utility Models: General Policies for Zero-Shot Deployment in New Environments', 'url': 'https://huggingface.co/papers/2409.05865', 'abstract': 'Robot models, particularly those trained with large amounts of data, have recently shown a plethora of real-world manipulation and navigation capabilities. Several independent efforts have shown that given sufficient training data in an environment, robot policies can generalize to demonstrated variations in that environment. However, needing to finetune robot models to every new environment stands in stark contrast to models in language or vision that can be deployed zero-shot for open-world problems. In this work, we present Robot Utility Models (RUMs), a framework for training and deploying zero-shot robot policies that can directly generalize to new environments without any finetuning. To create RUMs efficiently, we develop new tools to quickly collect data for mobile manipulation tasks, integrate such data into a policy with multi-modal imitation learning, and deploy policies on-device on Hello Robot Stretch, a cheap commodity robot, with an external mLLM verifier for retrying. We train five such utility models for opening cabinet doors, opening drawers, picking up napkins, picking up paper bags, and reorienting fallen objects. Our system, on average, achieves 90% success rate in unseen, novel environments interacting with unseen objects. Moreover, the utility models can also succeed in different robot and camera set-ups with no further data, training, or fine-tuning. Primary among our lessons are the importance of training data over training algorithm and policy class, guidance about data scaling, necessity for diverse yet high-quality demonstrations, and a recipe for robot introspection and retrying to improve performance on individual environments. Our code, data, models, hardware designs, as well as our experiment and deployment videos are open sourced and can be found on our project website: https://robotutilitymodels.com', 'score': 14, 'issue_id': 1, 'pub_date': '2024-09-09', 'pub_date_card': {'ru': '9 сентября', 'en': 'September 9', 'zh': '9月9日'}, 'hash': 'dcc98c08eb130ff0', 'data': {'categories': ['#dataset', '#training', '#data', '#transfer_learning', '#benchmark', '#games', '#open_source', '#robotics', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Универсальные роботы: обобщение без дополнительного обучения', 'desc': 'Статья представляет Robot Utility Models (RUMs) - фреймворк для обучения и развертывания робототехнических политик с нулевым обучением, способных напрямую обобщаться на новые среды без дополнительной настройки. Авторы разработали инструменты для быстрого сбора данных о задачах мобильной манипуляции и интеграции этих данных в политику с помощью мультимодального имитационного обучения. Они обучили пять моделей полезности для различных задач и достигли 90% успеха в новых средах с незнакомыми объектами. Ключевыми выводами являются важность обучающих данных, необходимость разнообразных демонстраций высокого качества и рецепт самоанализа робота для улучшения производительности.'}, 'en': {'title': 'Zero-Shot Robot Policies for New Environments', 'desc': 'This paper introduces Robot Utility Models (RUMs), a novel framework that enables robots to perform tasks in new environments without the need for finetuning. By leveraging multi-modal imitation learning, RUMs can generalize learned policies to unseen scenarios, achieving a high success rate in various mobile manipulation tasks. The authors emphasize the significance of high-quality training data and diverse demonstrations for effective model performance. Additionally, the framework allows for on-device deployment, making it accessible for practical applications in robotics.'}, 'zh': {'title': '零-shot机器人策略的创新应用', 'desc': '本研究提出了一种名为机器人效用模型（RUMs）的框架，旨在训练和部署零-shot机器人策略，使其能够在新环境中直接泛化，而无需微调。通过快速收集移动操作任务的数据，并利用多模态模仿学习将这些数据整合到策略中，我们在Hello Robot Stretch机器人上实现了这一目标。我们训练了五个效用模型，成功率在未见的新环境中达到90%。此外，这些模型在不同的机器人和摄像头设置下也能成功运行，无需额外的数据、训练或微调。'}}}, {'id': 'https://huggingface.co/papers/2409.05806', 'title': 'Benchmarking Chinese Knowledge Rectification in Large Language Models', 'url': 'https://huggingface.co/papers/2409.05806', 'abstract': 'While Large Language Models (LLMs) exhibit remarkable generative capabilities, they are not without flaws, particularly in the form of hallucinations. This issue is even more pronounced when LLMs are applied to specific languages and domains. For example, LLMs may generate nonsense information when handling Chinese ancient poetry, proverbs, or idioms, owing to the lack of specific knowledge. To this end, this paper introduces a benchmark for rectifying Chinese knowledge in LLMs via knowledge editing. Specifically, we introduce a new Chinese dataset, CKnowEdit, by collecting seven type of knowledge from various sources, including classical texts, idioms, and content from Baidu Tieba Ruozhiba, thereby accounting for the unique polyphony, antithesis, and logical constructs inherent in the Chinese language. Through the analysis of this dataset, we uncover the challenges faced by current LLMs in mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge editing techniques on this dataset unveil the substantial scope for advancement in the rectification of Chinese knowledge. Code and dataset are available at https://github.com/zjunlp/EasyEdit.', 'score': 14, 'issue_id': 1, 'pub_date': '2024-09-09', 'pub_date_card': {'ru': '9 сентября', 'en': 'September 9', 'zh': '9月9日'}, 'hash': '9636706ebd51ea54', 'data': {'categories': ['#dataset', '#hallucinations', '#multilingual', '#benchmark', '#open_source', '#synthetic'], 'emoji': '🀄', 'ru': {'title': 'Улучшение китайских знаний в LLM: новый бенчмарк и датасет', 'desc': 'Эта статья представляет новый бенчмарк для исправления китайских знаний в больших языковых моделях (LLM) с помощью редактирования знаний. Авторы создали датасет CKnowEdit, включающий семь типов знаний из различных источников китайской культуры. Анализ датасета выявил проблемы, с которыми сталкиваются современные LLM при работе с китайским языком. Оценка современных методов редактирования знаний на этом датасете показала значительный потенциал для улучшения в области исправления китайских знаний.'}, 'en': {'title': 'Enhancing Chinese Knowledge in LLMs through Knowledge Editing', 'desc': 'This paper addresses the issue of hallucinations in Large Language Models (LLMs) when they generate content in specific languages, particularly Chinese. It highlights the challenges LLMs face with Chinese ancient poetry, proverbs, and idioms due to their lack of specialized knowledge. To tackle this, the authors introduce a new dataset called CKnowEdit, which includes various types of knowledge from classical texts and online sources. The study evaluates current knowledge editing techniques and identifies significant opportunities for improving the accuracy of LLMs in handling Chinese language content.'}, 'zh': {'title': '修正中文知识，提升语言模型能力', 'desc': '大型语言模型（LLMs）在生成能力上表现出色，但在特定语言和领域中存在幻觉问题，尤其是在处理中文古诗、成语和俗语时。为了解决这一问题，本文提出了一个基准，通过知识编辑来修正LLMs中的中文知识。我们收集了七种类型的知识，创建了新的中文数据集CKnowEdit，涵盖了古典文本、成语和百度贴吧内容，以应对中文的多音性、对立性和逻辑结构。通过对该数据集的分析，我们揭示了当前LLMs在掌握中文时面临的挑战，并评估了最先进的知识编辑技术在该数据集上的应用，显示出在中文知识修正方面的巨大改进空间。'}}}, {'id': 'https://huggingface.co/papers/2409.04269', 'title': 'Open Language Data Initiative: Advancing Low-Resource Machine Translation for Karakalpak', 'url': 'https://huggingface.co/papers/2409.04269', 'abstract': 'This study presents several contributions for the Karakalpak language: a FLORES+ devtest dataset translated to Karakalpak, parallel corpora for Uzbek-Karakalpak, Russian-Karakalpak and English-Karakalpak of 100,000 pairs each and open-sourced fine-tuned neural models for translation across these languages. Our experiments compare different model variants and training approaches, demonstrating improvements over existing baselines. This work, conducted as part of the Open Language Data Initiative (OLDI) shared task, aims to advance machine translation capabilities for Karakalpak and contribute to expanding linguistic diversity in NLP technologies.', 'score': 9, 'issue_id': 1, 'pub_date': '2024-09-06', 'pub_date_card': {'ru': '6 сентября', 'en': 'September 6', 'zh': '9月6日'}, 'hash': '5c09626a0cc4d649', 'data': {'categories': ['#dataset', '#multilingual', '#training', '#machine_translation', '#open_source', '#low_resource'], 'emoji': '🌐', 'ru': {'title': 'Расширение границ машинного перевода для каракалпакского языка', 'desc': 'Исследование представляет несколько вкладов для каракалпакского языка. Создан набор данных FLORES+ devtest, переведенный на каракалпакский, и параллельные корпусы для пар узбекский-каракалпакский, русский-каракалпакский и английский-каракалпакский по 100 000 пар каждый. Разработаны и открыто опубликованы дообученные нейронные модели для перевода между этими языками. Эксперименты сравнивают различные варианты моделей и подходы к обучению, демонстрируя улучшения по сравнению с существующими базовыми моделями.'}, 'en': {'title': 'Empowering Karakalpak: Advancing Machine Translation and Linguistic Diversity', 'desc': 'This paper focuses on enhancing machine translation for the Karakalpak language by introducing a new FLORES+ devtest dataset specifically translated into Karakalpak. It also provides parallel corpora for three language pairs: Uzbek-Karakalpak, Russian-Karakalpak, and English-Karakalpak, each containing 100,000 translation pairs. The authors conduct experiments comparing various model architectures and training methods, showing significant improvements over previous benchmarks. This research is part of the Open Language Data Initiative (OLDI) and aims to promote linguistic diversity in natural language processing technologies.'}, 'zh': {'title': '推动卡拉卡尔帕克语的机器翻译发展', 'desc': '本研究为卡拉卡尔帕克语提供了多个贡献，包括翻译成卡拉卡尔帕克语的FLORES+开发测试数据集，以及乌兹别克语-卡拉卡尔帕克语、俄语-卡拉卡尔帕克语和英语-卡拉卡尔帕克语的平行语料库，每种语言对各有100,000对。我们比较了不同模型变体和训练方法的实验，显示出相较于现有基准的改进。该工作是开放语言数据倡议（OLDI）共享任务的一部分，旨在提升卡拉卡尔帕克语的机器翻译能力，并为自然语言处理技术的语言多样性做出贡献。'}}}, {'id': 'https://huggingface.co/papers/2409.05862', 'title': 'Evaluating Multiview Object Consistency in Humans and Image Models', 'url': 'https://huggingface.co/papers/2409.05862', 'abstract': "We introduce a benchmark to directly evaluate the alignment between human observers and vision models on a 3D shape inference task. We leverage an experimental design from the cognitive sciences which requires zero-shot visual inferences about object shape: given a set of images, participants identify which contain the same/different objects, despite considerable viewpoint variation. We draw from a diverse range of images that include common objects (e.g., chairs) as well as abstract shapes (i.e., procedurally generated `nonsense' objects). After constructing over 2000 unique image sets, we administer these tasks to human participants, collecting 35K trials of behavioral data from over 500 participants. This includes explicit choice behaviors as well as intermediate measures, such as reaction time and gaze data. We then evaluate the performance of common vision models (e.g., DINOv2, MAE, CLIP). We find that humans outperform all models by a wide margin. Using a multi-scale evaluation approach, we identify underlying similarities and differences between models and humans: while human-model performance is correlated, humans allocate more time/processing on challenging trials. All images, data, and code can be accessed via our project page.", 'score': 8, 'issue_id': 1, 'pub_date': '2024-09-09', 'pub_date_card': {'ru': '9 сентября', 'en': 'September 9', 'zh': '9月9日'}, 'hash': '2e274d901f8e84e5', 'data': {'categories': ['#science', '#dataset', '#cv', '#data', '#benchmark', '#alignment', '#open_source', '#3d'], 'emoji': '👁️', 'ru': {'title': 'Человек vs ИИ: кто лучше понимает 3D-формы объектов?', 'desc': 'Исследователи представили новый бенчмарк для оценки соответствия между восприятием человека и моделями компьютерного зрения при выводе трехмерных форм объектов. Эксперимент основан на задаче идентификации одинаковых/разных объектов по набору изображений с разных ракурсов. Было собрано более 35 тысяч результатов испытаний от более чем 500 участников, включая данные о выборе, времени реакции и движении глаз. Сравнение с популярными моделями компьютерного зрения (DINOv2, MAE, CLIP) показало, что люди значительно превосходят все модели в этой задаче.'}, 'en': {'title': 'Benchmarking Human vs. Model Shape Recognition', 'desc': 'This paper presents a benchmark for assessing how well vision models align with human perception in recognizing 3D shapes. The study uses a zero-shot inference task where participants identify whether images depict the same or different objects, despite variations in viewpoint. Over 2000 unique image sets were created, and data from 35,000 trials involving more than 500 participants were collected, including choice behaviors and reaction times. The results show that humans significantly outperform existing vision models, revealing insights into the differences in processing strategies between humans and models.'}, 'zh': {'title': '人类超越模型：3D形状推断的新基准', 'desc': '本文介绍了一个基准测试，用于直接评估人类观察者与视觉模型在3D形状推断任务上的一致性。我们采用了认知科学中的实验设计，要求参与者在没有先前训练的情况下，根据一组图像识别相同或不同的物体。通过构建超过2000个独特的图像集，我们收集了来自500多名参与者的35K次行为数据，包括选择行为、反应时间和注视数据。结果显示，人类在所有模型中表现优异，且在困难试验中，人类花费更多时间进行处理。'}}}, {'id': 'https://huggingface.co/papers/2409.04234', 'title': 'UniDet3D: Multi-dataset Indoor 3D Object Detection', 'url': 'https://huggingface.co/papers/2409.04234', 'abstract': 'Growing customer demand for smart solutions in robotics and augmented reality has attracted considerable attention to 3D object detection from point clouds. Yet, existing indoor datasets taken individually are too small and insufficiently diverse to train a powerful and general 3D object detection model. In the meantime, more general approaches utilizing foundation models are still inferior in quality to those based on supervised training for a specific task. In this work, we propose , a simple yet effective 3D object detection model, which is trained on a mixture of indoor datasets and is capable of working in various indoor environments. By unifying different label spaces,  enables learning a strong representation across multiple datasets through a supervised joint training scheme. The proposed network architecture is built upon a vanilla transformer encoder, making it easy to run, customize and extend the prediction pipeline for practical use. Extensive experiments demonstrate that  obtains significant gains over existing 3D object detection methods in 6 indoor benchmarks: ScanNet (+1.1 mAP50), ARKitScenes (+19.4 mAP25), S3DIS (+9.1 mAP50), MultiScan (+9.3 mAP50), 3RScan (+3.2 mAP50), and ScanNet++ (+2.7 mAP50). Code is available at https://github.com/filapro/unidet3d .', 'score': 7, 'issue_id': 1, 'pub_date': '2024-09-06', 'pub_date_card': {'ru': '6 сентября', 'en': 'September 6', 'zh': '9月6日'}, 'hash': 'fb98f72911410422', 'data': {'categories': ['#dataset', '#training', '#optimization', '#transfer_learning', '#benchmark', '#open_source', '#architecture', '#robotics', '#3d'], 'emoji': '🏠', 'ru': {'title': 'UniDet3D: Единая модель для обнаружения 3D объектов в любых интерьерах', 'desc': 'Статья представляет UniDet3D - модель для обнаружения 3D объектов в облаках точек, обученную на нескольких наборах данных для работы в различных интерьерах. Архитектура основана на трансформере-энкодере и использует унифицированное пространство меток для совместного обучения на разных датасетах. UniDet3D превосходит существующие методы на 6 эталонных наборах данных для помещений, включая ScanNet, ARKitScenes и S3DIS. Модель предлагает простое, но эффективное решение для 3D детекции объектов в помещениях.'}, 'en': {'title': 'Unified 3D Object Detection for Diverse Indoor Environments', 'desc': 'This paper presents a novel 3D object detection model designed to improve performance in indoor environments by leveraging a combination of multiple datasets. The model addresses the limitations of existing datasets, which are often too small and not diverse enough for effective training. By unifying different label spaces and employing a supervised joint training approach, the model learns robust representations that enhance detection accuracy. The architecture is based on a transformer encoder, allowing for easy customization and practical application, and it shows significant improvements over current methods across several benchmarks.'}, 'zh': {'title': '统一多数据集，提升3D物体检测性能', 'desc': '本论文提出了一种新的3D物体检测模型，旨在解决现有室内数据集规模小和多样性不足的问题。该模型通过混合多个室内数据集进行训练，能够在不同的室内环境中有效工作。我们采用了统一标签空间的方法，通过监督联合训练方案来学习强大的表示能力。实验结果表明，该模型在多个室内基准测试中显著优于现有的3D物体检测方法。'}}}, {'id': 'https://huggingface.co/papers/2409.05177', 'title': 'Insights from Benchmarking Frontier Language Models on Web App Code Generation', 'url': 'https://huggingface.co/papers/2409.05177', 'abstract': 'This paper presents insights from evaluating 16 frontier large language models (LLMs) on the WebApp1K benchmark, a test suite designed to assess the ability of LLMs to generate web application code. The results reveal that while all models possess similar underlying knowledge, their performance is differentiated by the frequency of mistakes they make. By analyzing lines of code (LOC) and failure distributions, we find that writing correct code is more complex than generating incorrect code. Furthermore, prompt engineering shows limited efficacy in reducing errors beyond specific cases. These findings suggest that further advancements in coding LLM should emphasize on model reliability and mistake minimization.', 'score': 5, 'issue_id': 1, 'pub_date': '2024-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '0ae54c8b57572607', 'data': {'categories': ['#optimization', '#interpretability', '#plp', '#benchmark'], 'emoji': '🖥️', 'ru': {'title': 'Надёжность превыше всего: ключ к улучшению LLM в программировании', 'desc': 'Статья представляет результаты оценки 16 передовых больших языковых моделей (LLM) на бенчмарке WebApp1K, разработанном для тестирования способности LLM генерировать код веб-приложений. Исследование показывает, что хотя все модели обладают схожими базовыми знаниями, их производительность различается частотой допускаемых ошибок. Анализ распределения строк кода (LOC) и ошибок выявляет, что написание корректного кода сложнее, чем генерация некорректного. Инженерия промптов показала ограниченную эффективность в снижении ошибок, за исключением отдельных случаев.'}, 'en': {'title': 'Enhancing Code Reliability in Large Language Models', 'desc': 'This paper evaluates 16 advanced large language models (LLMs) using the WebApp1K benchmark, which tests their ability to generate web application code. The study finds that although these models share similar knowledge, their performance varies significantly based on the frequency of errors they produce. An analysis of lines of code (LOC) and error distributions indicates that creating correct code is inherently more challenging than producing incorrect code. Additionally, the research highlights that prompt engineering has limited success in reducing errors, suggesting a need for future improvements in LLMs to focus on enhancing reliability and minimizing mistakes.'}, 'zh': {'title': '提升编码模型的可靠性与错误最小化', 'desc': '本文评估了16个前沿大型语言模型（LLMs）在WebApp1K基准测试中的表现，该测试旨在评估LLMs生成网页应用代码的能力。结果显示，尽管所有模型具有相似的基础知识，但它们的表现因错误频率而有所不同。通过分析代码行数（LOC）和失败分布，我们发现编写正确代码比生成错误代码更复杂。此外，提示工程在减少错误方面的效果有限，超出了特定案例。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents', '#agi', '#alignment (3)', '#architecture (4)', '#audio', '#benchmark (6)', '#cv (2)', '#data (4)', '#dataset (5)', '#diffusion', '#ethics', '#games (1)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (2)', '#interpretability (1)', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation (1)', '#math', '#multilingual (2)', '#multimodal (2)', '#open_source (6)', '#optimization (7)', '#plp (1)', '#rag (3)', '#reasoning (3)', '#rl', '#rlhf (1)', '#robotics (2)', '#science (2)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (8)', '#transfer_learning (2)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-09-10 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-09-10 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-09-10 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    