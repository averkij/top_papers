{
    "date": {
        "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 23",
        "zh": "9æœˆ23æ—¥"
    },
    "time_utc": "2024-09-23 09:00",
    "weekday": 0,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-23",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.13346",
            "title": "Imagine yourself: Tuning-Free Personalized Image Generation",
            "url": "https://huggingface.co/papers/2409.13346",
            "abstract": "Diffusion models have demonstrated remarkable efficacy across various image-to-image tasks. In this research, we introduce Imagine yourself, a state-of-the-art model designed for personalized image generation. Unlike conventional tuning-based personalization techniques, Imagine yourself operates as a tuning-free model, enabling all users to leverage a shared framework without individualized adjustments. Moreover, previous work met challenges balancing identity preservation, following complex prompts and preserving good visual quality, resulting in models having strong copy-paste effect of the reference images. Thus, they can hardly generate images following prompts that require significant changes to the reference image, \\eg, changing facial expression, head and body poses, and the diversity of the generated images is low. To address these limitations, our proposed method introduces 1) a new synthetic paired data generation mechanism to encourage image diversity, 2) a fully parallel attention architecture with three text encoders and a fully trainable vision encoder to improve the text faithfulness, and 3) a novel coarse-to-fine multi-stage finetuning methodology that gradually pushes the boundary of visual quality. Our study demonstrates that Imagine yourself surpasses the state-of-the-art personalization model, exhibiting superior capabilities in identity preservation, visual quality, and text alignment. This model establishes a robust foundation for various personalization applications. Human evaluation results validate the model's SOTA superiority across all aspects (identity preservation, text faithfulness, and visual appeal) compared to the previous personalization models.",
            "score": 67,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 20",
                "zh": "9æœˆ20æ—¥"
            },
            "hash": "cd0a322cf520de60",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Imagine yourself Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸, ÑÑ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Imagine yourself Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼."
                },
                "en": {
                    "title": "Personalized Image Generation Without Individual Tuning",
                    "desc": "This paper presents 'Imagine yourself', a cutting-edge diffusion model for personalized image generation that does not require individual tuning. It addresses the limitations of previous models by introducing a synthetic paired data generation mechanism to enhance image diversity and a parallel attention architecture to improve text alignment. The model employs a novel coarse-to-fine multi-stage finetuning approach to enhance visual quality while maintaining identity preservation. Human evaluations confirm that 'Imagine yourself' outperforms existing personalization models in identity preservation, text faithfulness, and overall visual appeal."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º\"Imagine yourself\"çš„å…ˆè¿›æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„è°ƒä¼˜ä¸ªæ€§åŒ–æŠ€æœ¯ä¸åŒï¼Œè¯¥æ¨¡å‹æ— éœ€ä¸ªæ€§åŒ–è°ƒæ•´ï¼Œå…è®¸æ‰€æœ‰ç”¨æˆ·åœ¨å…±äº«æ¡†æ¶ä¸‹ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ–°é¢–çš„åˆæˆé…å¯¹æ•°æ®ç”Ÿæˆæœºåˆ¶ã€å…¨å¹¶è¡Œæ³¨æ„åŠ›æ¶æ„å’Œé€æ­¥ç»†åŒ–çš„å¤šé˜¶æ®µè°ƒä¼˜æ–¹æ³•ï¼Œå…‹æœäº†ä»¥å¾€æ¨¡å‹åœ¨èº«ä»½ä¿ç•™ã€æ–‡æœ¬ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡æ–¹é¢çš„æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13592",
            "title": "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models",
            "url": "https://huggingface.co/papers/2409.13592",
            "abstract": "Understanding satire and humor is a challenging task for even current Vision-Language models. In this paper, we propose the challenging tasks of Satirical Image Detection (detecting whether an image is satirical), Understanding (generating the reason behind the image being satirical), and Completion (given one half of the image, selecting the other half from 2 given options, such that the complete image is satirical) and release a high-quality dataset YesBut, consisting of 2547 images, 1084 satirical and 1463 non-satirical, containing different artistic styles, to evaluate those tasks. Each satirical image in the dataset depicts a normal scenario, along with a conflicting scenario which is funny or ironic. Despite the success of current Vision-Language Models on multimodal tasks such as Visual QA and Image Captioning, our benchmarking experiments show that such models perform poorly on the proposed tasks on the YesBut Dataset in Zero-Shot Settings w.r.t both automated as well as human evaluation. Additionally, we release a dataset of 119 real, satirical photographs for further research. The dataset and code are available at https://github.com/abhi1nandy2/yesbut_dataset.",
            "score": 48,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 20",
                "zh": "9æœˆ20æ—¥"
            },
            "hash": "63915fb63f61f8bf",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑĞ°Ñ‚Ğ¸Ñ€Ñƒ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°: Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ°Ñ‚Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ YesBut, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 2547 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (1084 ÑĞ°Ñ‚Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ 1463 Ğ½ĞµÑĞ°Ñ‚Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…) Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ zero-shot. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» Ğ²Ñ‹Ğ¿ÑƒÑ‰ĞµĞ½ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 119 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ°Ñ‚Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Decoding Satire: A New Challenge for Vision-Language Models",
                    "desc": "This paper addresses the difficulty of understanding satire and humor in images using Vision-Language models. It introduces three tasks: Satirical Image Detection, Understanding the satire, and Completion of satirical images. The authors present a new dataset called YesBut, which includes 2547 images to evaluate these tasks, highlighting the contrast between normal and satirical scenarios. Benchmarking results reveal that existing models struggle with these tasks, indicating a gap in their ability to comprehend humor and irony in visual content."
                },
                "zh": {
                    "title": "æ­ç¤ºè®½åˆºçš„æŒ‘æˆ˜ä¸æœºé‡",
                    "desc": "ç†è§£è®½åˆºå’Œå¹½é»˜å¯¹å½“å‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹æ¥è¯´æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸‰ä¸ªä»»åŠ¡ï¼šè®½åˆºå›¾åƒæ£€æµ‹ã€ç†è§£è®½åˆºåŸå› å’Œå›¾åƒè¡¥å…¨ï¼Œå¹¶å‘å¸ƒäº†ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†YesButï¼ŒåŒ…å«2547å¼ å›¾åƒã€‚å°½ç®¡ç°æœ‰çš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨YesButæ•°æ®é›†çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¿™äº›æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„è¡¨ç°å´å¾ˆå·®ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†119å¼ çœŸå®çš„è®½åˆºç…§ç‰‡æ•°æ®é›†ï¼Œä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13598",
            "title": "Prithvi WxC: Foundation Model for Weather and Climate",
            "url": "https://huggingface.co/papers/2409.13598",
            "abstract": "Triggered by the realization that AI emulators can rival the performance of traditional numerical weather prediction models running on HPC systems, there is now an increasing number of large AI models that address use cases such as forecasting, downscaling, or nowcasting. While the parallel developments in the AI literature focus on foundation models -- models that can be effectively tuned to address multiple, different use cases -- the developments on the weather and climate side largely focus on single-use cases with particular emphasis on mid-range forecasting. We close this gap by introducing Prithvi WxC, a 2.3 billion parameter foundation model developed using 160 variables from the Modern-Era Retrospective Analysis for Research and Applications, Version 2 (MERRA-2). Prithvi WxC employs an encoder-decoder-based architecture, incorporating concepts from various recent transformer models to effectively capture both regional and global dependencies in the input data. The model has been designed to accommodate large token counts to model weather phenomena in different topologies at fine resolutions. Furthermore, it is trained with a mixed objective that combines the paradigms of masked reconstruction with forecasting. We test the model on a set of challenging downstream tasks namely: Autoregressive rollout forecasting, Downscaling, Gravity wave flux parameterization, and Extreme events estimation. The pretrained model with 2.3 billion parameters, along with the associated fine-tuning workflows, has been publicly released as an open-source contribution via Hugging Face.",
            "score": 37,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 20",
                "zh": "9æœˆ20æ—¥"
            },
            "hash": "a0a84f660d5ff945",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸŒ¦ï¸",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹ Ğ¸ ĞºĞ»Ğ¸Ğ¼Ğ°Ñ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Prithvi WxC - Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 2,3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹ Ğ¸ ĞºĞ»Ğ¸Ğ¼Ğ°Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 160 Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ€ĞµĞ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° MERRA-2. Prithvi WxC Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ´Ğ°ÑƒĞ½ÑĞºĞµĞ¹Ğ»Ğ¸Ğ½Ğ³ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ½ĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ Ñ‡ĞµÑ€ĞµĞ· Hugging Face."
                },
                "en": {
                    "title": "Revolutionizing Weather Forecasting with AI Foundation Models",
                    "desc": "This paper introduces Prithvi WxC, a large foundation model designed for weather and climate applications, featuring 2.3 billion parameters. It utilizes an encoder-decoder architecture inspired by transformer models to effectively capture both regional and global dependencies in weather data. The model is trained on a diverse dataset from MERRA-2 and is capable of handling large token counts for fine-resolution weather phenomena modeling. It has been tested on various challenging tasks, including forecasting and downscaling, and is available as an open-source resource for further research."
                },
                "zh": {
                    "title": "åŸºç¡€æ¨¡å‹åŠ©åŠ›å¤©æ°”é¢„æµ‹æ–°çºªå…ƒ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPrithvi WxCçš„åŸºç¡€æ¨¡å‹ï¼Œå…·æœ‰23äº¿ä¸ªå‚æ•°ï¼Œæ—¨åœ¨è§£å†³å¤©æ°”å’Œæ°”å€™é¢„æµ‹é—®é¢˜ã€‚è¯¥æ¨¡å‹ä½¿ç”¨160ä¸ªå˜é‡ï¼ŒåŸºäºç°ä»£æ—¶ä»£å›é¡¾åˆ†æï¼ˆMERRA-2ï¼‰æ•°æ®ï¼Œé‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è¾“å…¥æ•°æ®çš„åŒºåŸŸå’Œå…¨çƒä¾èµ–å…³ç³»ã€‚Prithvi WxCè®¾è®¡ç”¨äºå¤„ç†å¤§è§„æ¨¡çš„æ ‡è®°æ•°é‡ï¼Œä»¥åœ¨ä¸åŒåœ°å½¢ä¸Šä»¥é«˜åˆ†è¾¨ç‡æ¨¡æ‹Ÿå¤©æ°”ç°è±¡ã€‚æ¨¡å‹ç»è¿‡æ··åˆç›®æ ‡è®­ç»ƒï¼Œç»“åˆäº†æ©è”½é‡å»ºå’Œé¢„æµ‹çš„èŒƒå¼ï¼Œå¹¶åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13216",
            "title": "MuCodec: Ultra Low-Bitrate Music Codec",
            "url": "https://huggingface.co/papers/2409.13216",
            "abstract": "Music codecs are a vital aspect of audio codec research, and ultra low-bitrate compression holds significant importance for music transmission and generation. Due to the complexity of music backgrounds and the richness of vocals, solely relying on modeling semantic or acoustic information cannot effectively reconstruct music with both vocals and backgrounds. To address this issue, we propose MuCodec, specifically targeting music compression and reconstruction tasks at ultra low bitrates. MuCodec employs MuEncoder to extract both acoustic and semantic features, discretizes them with RVQ, and obtains Mel-VAE features via flow-matching. The music is then reconstructed using a pre-trained MEL-VAE decoder and HiFi-GAN. MuCodec can reconstruct high-fidelity music at ultra low (0.35kbps) or high bitrates (1.35kbps), achieving the best results to date in both subjective and objective metrics. Code and Demo: https://xuyaoxun.github.io/MuCodec_demo/.",
            "score": 22,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 20",
                "zh": "9æœˆ20æ—¥"
            },
            "hash": "82a6af61f8a6c886",
            "data": {
                "categories": [
                    "#audio"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸: Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ°Ñ…",
                    "desc": "MuCodec - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ¿Ñ€Ğ¸ ÑĞ²ĞµÑ€Ñ…Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ°Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ MuEncoder Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RVQ Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ÑÑ Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Mel-VAE. Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ MEL-VAE Ğ¸ HiFi-GAN. MuCodec Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ±Ğ¸Ñ‚Ñ€ĞµĞ¹Ñ‚Ğ°Ñ… 0.35-1.35 ĞºĞ±Ğ¸Ñ‚/Ñ Ğ¿Ğ¾ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "MuCodec: High-Fidelity Music Compression at Ultra Low Bitrates",
                    "desc": "This paper introduces MuCodec, a novel approach for music compression and reconstruction at ultra low bitrates. It addresses the challenge of effectively reconstructing music that includes both vocals and complex backgrounds by utilizing a combination of acoustic and semantic feature extraction. MuCodec employs a MuEncoder to gather these features, which are then processed using Residual Vector Quantization (RVQ) and flow-matching to obtain Mel-VAE features. The final music reconstruction is achieved through a pre-trained MEL-VAE decoder and HiFi-GAN, demonstrating superior performance in both subjective and objective evaluations at bitrates as low as 0.35kbps."
                },
                "zh": {
                    "title": "MuCodecï¼šè¶…ä½æ¯”ç‰¹ç‡éŸ³ä¹é‡å»ºçš„åˆ›æ–°è§£å†³æ–¹æ¡ˆ",
                    "desc": "éŸ³ä¹ç¼–è§£ç å™¨åœ¨éŸ³é¢‘ç¼–è§£ç ç ”ç©¶ä¸­éå¸¸é‡è¦ï¼Œè¶…ä½æ¯”ç‰¹ç‡å‹ç¼©å¯¹éŸ³ä¹ä¼ è¾“å’Œç”Ÿæˆå…·æœ‰é‡è¦æ„ä¹‰ã€‚ç”±äºéŸ³ä¹èƒŒæ™¯çš„å¤æ‚æ€§å’Œäººå£°çš„ä¸°å¯Œæ€§ï¼Œä»…ä¾é å»ºæ¨¡è¯­ä¹‰æˆ–å£°å­¦ä¿¡æ¯æ— æ³•æœ‰æ•ˆé‡å»ºåŒæ—¶åŒ…å«äººå£°å’ŒèƒŒæ™¯çš„éŸ³ä¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MuCodecï¼Œä¸“é—¨é’ˆå¯¹è¶…ä½æ¯”ç‰¹ç‡ä¸‹çš„éŸ³ä¹å‹ç¼©å’Œé‡å»ºä»»åŠ¡ã€‚MuCodecé€šè¿‡MuEncoderæå–å£°å­¦å’Œè¯­ä¹‰ç‰¹å¾ï¼Œä½¿ç”¨RVQè¿›è¡Œç¦»æ•£åŒ–ï¼Œå¹¶é€šè¿‡æµåŒ¹é…è·å¾—Mel-VAEç‰¹å¾ï¼Œæœ€ç»ˆåˆ©ç”¨é¢„è®­ç»ƒçš„MEL-VAEè§£ç å™¨å’ŒHiFi-GANé‡å»ºé«˜ä¿çœŸéŸ³ä¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.12941",
            "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2409.12941",
            "abstract": "Large Language Models (LLMs) have demonstrated significant performance improvements across various cognitive tasks. An emerging application is using LLMs to enhance retrieval-augmented generation (RAG) capabilities. These systems require LLMs to understand user queries, retrieve relevant information, and synthesize coherent and accurate responses. Given the increasing real-world deployment of such systems, comprehensive evaluation becomes crucial. To this end, we propose FRAMES (Factuality, Retrieval, And reasoning MEasurement Set), a high-quality evaluation dataset designed to test LLMs' ability to provide factual responses, assess retrieval capabilities, and evaluate the reasoning required to generate final answers. While previous work has provided datasets and benchmarks to evaluate these abilities in isolation, FRAMES offers a unified framework that provides a clearer picture of LLM performance in end-to-end RAG scenarios. Our dataset comprises challenging multi-hop questions that require the integration of information from multiple sources. We present baseline results demonstrating that even state-of-the-art LLMs struggle with this task, achieving 0.40 accuracy with no retrieval. The accuracy is significantly improved with our proposed multi-step retrieval pipeline, achieving an accuracy of 0.66 (>50% improvement). We hope our work will help bridge evaluation gaps and assist in developing more robust and capable RAG systems.",
            "score": 20,
            "issue_id": 1,
            "pub_date": "2024-09-19",
            "pub_date_card": {
                "ru": "19 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 19",
                "zh": "9æœˆ19æ—¥"
            },
            "hash": "dc5c06fd6d7625ca",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#rag",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "FRAMES: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… RAG",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FRAMES - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (RAG). FRAMES Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹, Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Enhancing LLMs with FRAMES for Better RAG Performance",
                    "desc": "This paper discusses the use of Large Language Models (LLMs) to improve retrieval-augmented generation (RAG) systems, which combine information retrieval and text generation. The authors introduce FRAMES, a new evaluation dataset that measures LLMs' factual accuracy, retrieval effectiveness, and reasoning skills in generating responses. Unlike previous benchmarks that assessed these abilities separately, FRAMES provides a comprehensive framework for evaluating LLM performance in real-world scenarios. The results show that while current LLMs perform poorly without retrieval, their accuracy significantly improves when using a multi-step retrieval approach."
                },
                "zh": {
                    "title": "æå‡æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿçš„è¯„ä¼°èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§è®¤çŸ¥ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ•°æ®é›†FRAMESï¼Œæ—¨åœ¨æµ‹è¯•LLMsåœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬æä¾›äº‹å®æ€§å›ç­”ã€è¯„ä¼°æ£€ç´¢èƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ã€‚FRAMESæ•°æ®é›†åŒ…å«å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šè·³é—®é¢˜ï¼Œéœ€è¦æ•´åˆæ¥è‡ªå¤šä¸ªæ¥æºçš„ä¿¡æ¯ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡å½“å‰æœ€å…ˆè¿›çš„LLMsåœ¨æ²¡æœ‰æ£€ç´¢çš„æƒ…å†µä¸‹å‡†ç¡®ç‡ä»…ä¸º0.40ï¼Œä½†é€šè¿‡æˆ‘ä»¬æå‡ºçš„å¤šæ­¥éª¤æ£€ç´¢ç®¡é“ï¼Œå‡†ç¡®ç‡æé«˜è‡³0.66ï¼Œè¶…è¿‡50%çš„æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13591",
            "title": "Portrait Video Editing Empowered by Multimodal Generative Priors",
            "url": "https://huggingface.co/papers/2409.13591",
            "abstract": "We introduce PortraitGen, a powerful portrait video editing method that achieves consistent and expressive stylization with multimodal prompts. Traditional portrait video editing methods often struggle with 3D and temporal consistency, and typically lack in rendering quality and efficiency. To address these issues, we lift the portrait video frames to a unified dynamic 3D Gaussian field, which ensures structural and temporal coherence across frames. Furthermore, we design a novel Neural Gaussian Texture mechanism that not only enables sophisticated style editing but also achieves rendering speed over 100FPS. Our approach incorporates multimodal inputs through knowledge distilled from large-scale 2D generative models. Our system also incorporates expression similarity guidance and a face-aware portrait editing module, effectively mitigating degradation issues associated with iterative dataset updates. Extensive experiments demonstrate the temporal consistency, editing efficiency, and superior rendering quality of our method. The broad applicability of the proposed approach is demonstrated through various applications, including text-driven editing, image-driven editing, and relighting, highlighting its great potential to advance the field of video editing. Demo videos and released code are provided in our project page: https://ustc3dv.github.io/PortraitGen/",
            "score": 15,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 20",
                "zh": "9æœˆ20æ—¥"
            },
            "hash": "bdf416584245d302",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#3d",
                    "#dataset"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "PortraitGen: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚ÑƒÑ€",
                    "desc": "PortraitGen - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾ Ğ¿Ğ¾Ğ»Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ¸Ğ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°. PortraitGen Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… 2D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ†Ğ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Portrait Video Editing with PortraitGen",
                    "desc": "PortraitGen is a novel method for editing portrait videos that focuses on maintaining consistency and expressiveness through multimodal prompts. It overcomes challenges in traditional editing methods by utilizing a unified dynamic 3D Gaussian field, which ensures both structural and temporal coherence across video frames. The introduction of a Neural Gaussian Texture mechanism allows for advanced style editing while achieving high rendering speeds of over 100 frames per second. Extensive experiments validate its effectiveness in editing efficiency, rendering quality, and broad applicability in various editing tasks such as text-driven and image-driven editing."
                },
                "zh": {
                    "title": "PortraitGenï¼šé«˜æ•ˆä¸€è‡´çš„è‚–åƒè§†é¢‘ç¼–è¾‘æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºPortraitGençš„å¼ºå¤§è‚–åƒè§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œèƒ½å¤Ÿé€šè¿‡å¤šæ¨¡æ€æç¤ºå®ç°ä¸€è‡´ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„é£æ ¼åŒ–ã€‚ä¼ ç»Ÿçš„è‚–åƒè§†é¢‘ç¼–è¾‘æ–¹æ³•åœ¨ä¸‰ç»´å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œä¸”åœ¨æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡ä¸Šè¡¨ç°ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å°†è‚–åƒè§†é¢‘å¸§æå‡åˆ°ç»Ÿä¸€çš„åŠ¨æ€ä¸‰ç»´é«˜æ–¯åœºï¼Œä»è€Œç¡®ä¿å¸§ä¹‹é—´çš„ç»“æ„å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„ç¥ç»é«˜æ–¯çº¹ç†æœºåˆ¶ï¼Œä¸ä»…æ”¯æŒå¤æ‚çš„é£æ ¼ç¼–è¾‘ï¼Œè¿˜èƒ½å®ç°è¶…è¿‡100FPSçš„æ¸²æŸ“é€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13690",
            "title": "Colorful Diffuse Intrinsic Image Decomposition in the Wild",
            "url": "https://huggingface.co/papers/2409.13690",
            "abstract": "Intrinsic image decomposition aims to separate the surface reflectance and the effects from the illumination given a single photograph. Due to the complexity of the problem, most prior works assume a single-color illumination and a Lambertian world, which limits their use in illumination-aware image editing applications. In this work, we separate an input image into its diffuse albedo, colorful diffuse shading, and specular residual components. We arrive at our result by gradually removing first the single-color illumination and then the Lambertian-world assumptions. We show that by dividing the problem into easier sub-problems, in-the-wild colorful diffuse shading estimation can be achieved despite the limited ground-truth datasets. Our extended intrinsic model enables illumination-aware analysis of photographs and can be used for image editing applications such as specularity removal and per-pixel white balancing.",
            "score": 12,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 20",
                "zh": "9æœˆ20æ—¥"
            },
            "hash": "49e7d3ade160e4b6",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ”ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾Ñ‚Ğ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ: Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ½Ğ¾Ğµ Ğ°Ğ»ÑŒĞ±ĞµĞ´Ğ¾, Ñ†Ğ²ĞµÑ‚Ğ½Ğ¾Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ½Ğ¾Ğµ Ğ·Ğ°Ñ‚ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ·ĞµÑ€ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ»Ğ°Ğ¼Ğ±ĞµÑ€Ñ‚Ğ¾Ğ²ÑĞºĞ¸Ğ¼ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸ÑÑ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¸Ñ… Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Image Editing with Advanced Intrinsic Decomposition",
                    "desc": "This paper presents a method for intrinsic image decomposition, which separates an image into its surface reflectance and illumination effects. Unlike previous approaches that relied on single-color lighting and Lambertian surfaces, this method tackles the problem by breaking it down into simpler components: diffuse albedo, colorful diffuse shading, and specular residuals. By relaxing the assumptions of uniform illumination, the authors demonstrate that it is possible to estimate colorful shading in real-world images, even with limited training data. The proposed model enhances the ability to perform illumination-aware image editing tasks, such as removing specularity and adjusting colors on a per-pixel basis."
                },
                "zh": {
                    "title": "åˆ†ç¦»å…‰ç…§ä¸åå°„ï¼Œæå‡å›¾åƒç¼–è¾‘èƒ½åŠ›",
                    "desc": "å†…åœ¨å›¾åƒåˆ†è§£çš„ç›®æ ‡æ˜¯ä»å•å¼ ç…§ç‰‡ä¸­åˆ†ç¦»å‡ºè¡¨é¢åå°„ç‡å’Œå…‰ç…§æ•ˆæœã€‚ä»¥å¾€çš„ç ”ç©¶å¤§å¤šå‡è®¾å•ä¸€é¢œè‰²çš„å…‰ç…§å’Œæœ—ä¼¯ä¸–ç•Œï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å…‰ç…§æ„ŸçŸ¥å›¾åƒç¼–è¾‘ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è¾“å…¥å›¾åƒåˆ†è§£ä¸ºæ¼«åå°„åç…§ç‡ã€ä¸°å¯Œçš„æ¼«åå°„é˜´å½±å’Œé•œé¢æ®‹ä½™æˆåˆ†ã€‚é€šè¿‡é€æ­¥å»é™¤å•ä¸€é¢œè‰²å…‰ç…§å’Œæœ—ä¼¯å‡è®¾ï¼Œæˆ‘ä»¬çš„æ‰©å±•å†…åœ¨æ¨¡å‹å®ç°äº†å…‰ç…§æ„ŸçŸ¥çš„ç…§ç‰‡åˆ†æï¼Œå¹¶å¯ç”¨äºå›¾åƒç¼–è¾‘åº”ç”¨ï¼Œå¦‚å»é™¤é•œé¢åå°„å’Œé€åƒç´ ç™½å¹³è¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13648",
            "title": "V^3: Viewing Volumetric Videos on Mobiles via Streamable 2D Dynamic Gaussians",
            "url": "https://huggingface.co/papers/2409.13648",
            "abstract": "Experiencing high-fidelity volumetric video as seamlessly as 2D videos is a long-held dream. However, current dynamic 3DGS methods, despite their high rendering quality, face challenges in streaming on mobile devices due to computational and bandwidth constraints. In this paper, we introduce V3(Viewing Volumetric Videos), a novel approach that enables high-quality mobile rendering through the streaming of dynamic Gaussians. Our key innovation is to view dynamic 3DGS as 2D videos, facilitating the use of hardware video codecs. Additionally, we propose a two-stage training strategy to reduce storage requirements with rapid training speed. The first stage employs hash encoding and shallow MLP to learn motion, then reduces the number of Gaussians through pruning to meet the streaming requirements, while the second stage fine tunes other Gaussian attributes using residual entropy loss and temporal loss to improve temporal continuity. This strategy, which disentangles motion and appearance, maintains high rendering quality with compact storage requirements. Meanwhile, we designed a multi-platform player to decode and render 2D Gaussian videos. Extensive experiments demonstrate the effectiveness of V3, outperforming other methods by enabling high-quality rendering and streaming on common devices, which is unseen before. As the first to stream dynamic Gaussians on mobile devices, our companion player offers users an unprecedented volumetric video experience, including smooth scrolling and instant sharing. Our project page with source code is available at https://authoritywang.github.io/v3/.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 20",
                "zh": "9æœˆ20æ—¥"
            },
            "hash": "6754c4dc77bfb7a4",
            "data": {
                "categories": [
                    "#3d",
                    "#video",
                    "#inference",
                    "#training"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ Ğ¾Ğ±ÑŠĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ V3 (Viewing Volumetric Videos) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³Ñƒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ»Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ 3DGS ĞºĞ°Ğº 2D Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ´ĞµĞºĞ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ…ÑÑˆ-ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ½ĞµĞ³Ğ»ÑƒĞ±Ğ¾ĞºĞ°Ñ MLP Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¿Ñ€Ğ¾Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ². Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Stream High-Quality Volumetric Videos Seamlessly on Mobile!",
                    "desc": "This paper presents V3, a new method for streaming high-quality volumetric videos on mobile devices. It addresses the limitations of current dynamic 3D Gaussian streaming methods by treating them like 2D videos, allowing the use of efficient hardware video codecs. The authors introduce a two-stage training strategy that optimizes storage and improves rendering quality by separating motion from appearance. Extensive tests show that V3 significantly enhances the user experience by enabling smooth playback and quick sharing of volumetric content on common devices."
                },
                "zh": {
                    "title": "ç§»åŠ¨è®¾å¤‡ä¸Šçš„é«˜è´¨é‡ä½“ç§¯è§†é¢‘æµåª’ä½“ä½“éªŒ",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºV3çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°é«˜è´¨é‡çš„ç§»åŠ¨è®¾å¤‡ä½“ç§¯è§†é¢‘æ¸²æŸ“ã€‚æˆ‘ä»¬å°†åŠ¨æ€3Dé«˜æ–¯è§†ä¸º2Dè§†é¢‘ï¼Œä»è€Œåˆ©ç”¨ç¡¬ä»¶è§†é¢‘ç¼–è§£ç å™¨æ¥è§£å†³æµåª’ä½“ä¼ è¾“ä¸­çš„è®¡ç®—å’Œå¸¦å®½é™åˆ¶ã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬æœ‰æ•ˆå‡å°‘äº†å­˜å‚¨éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ¸²æŸ“è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒV3åœ¨å¸¸è§è®¾å¤‡ä¸Šå®ç°äº†é«˜è´¨é‡çš„æ¸²æŸ“å’Œæµåª’ä½“ä¼ è¾“ï¼Œæä¾›äº†å‰æ‰€æœªæœ‰çš„ä½“ç§¯è§†é¢‘ä½“éªŒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13449",
            "title": "Minstrel: Structural Prompt Generation with Multi-Agents Coordination for Non-AI Experts",
            "url": "https://huggingface.co/papers/2409.13449",
            "abstract": "LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to assist them in their work poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat scattered optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structural design, incurring high learning costs and it is not conducive to the iterative updating of prompts, especially for non-AI experts. Inspired by structured reusable programming languages, we propose LangGPT, a structural prompt design framework. Furthermore, we introduce Minstrel, a multi-generative agent system with reflection to automate the generation of structural prompts. Experiments and the case study illustrate that structural prompts generated by Minstrel or written manually significantly enhance the performance of LLMs. Furthermore, we analyze the ease of use of structural prompts through a user survey in our online community.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 20",
                "zh": "9æœˆ20æ—¥"
            },
            "hash": "0e0e0cdbcc3fa527",
            "data": {
                "categories": [
                    "#agents",
                    "#prompt_engineering",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ LangGPT - ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Minstrel - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Minstrel Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ ÑƒĞ´Ğ¾Ğ±ÑÑ‚Ğ²Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Non-Experts with Structured Prompt Design for LLMs",
                    "desc": "This paper addresses the challenge of creating effective prompts for large language models (LLMs), particularly for users without AI expertise. It critiques existing prompt engineering methods for their lack of structure and high learning costs, which hinder iterative improvements. The authors introduce LangGPT, a framework for structured prompt design, and Minstrel, a system that automates the generation of these structured prompts. Experimental results show that using structural prompts improves LLM performance, and user feedback indicates that these prompts are easier to use."
                },
                "zh": {
                    "title": "ç»“æ„åŒ–æç¤ºï¼ŒåŠ©åŠ›LLMsæ›´å¼ºå¤§",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºLangGPTçš„ç»“æ„åŒ–æç¤ºè®¾è®¡æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©éäººå·¥æ™ºèƒ½ä¸“å®¶æ›´å¥½åœ°ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†Minstrelï¼Œä¸€ä¸ªå¤šç”Ÿæˆä»£ç†ç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–æç¤ºï¼Œä»è€Œç®€åŒ–æç¤ºå·¥ç¨‹çš„è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡Minstrelç”Ÿæˆçš„ç»“æ„åŒ–æç¤ºæˆ–æ‰‹åŠ¨ç¼–å†™çš„æç¤ºï¼Œæ˜¾è‘—æé«˜äº†LLMsçš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ç”¨æˆ·è°ƒæŸ¥åˆ†æäº†ç»“æ„åŒ–æç¤ºçš„æ˜“ç”¨æ€§ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13689",
            "title": "Temporally Aligned Audio for Video with Autoregression",
            "url": "https://huggingface.co/papers/2409.13689",
            "abstract": "We introduce V-AURA, the first autoregressive model to achieve high temporal alignment and relevance in video-to-audio generation. V-AURA uses a high-framerate visual feature extractor and a cross-modal audio-visual feature fusion strategy to capture fine-grained visual motion events and ensure precise temporal alignment. Additionally, we propose VisualSound, a benchmark dataset with high audio-visual relevance. VisualSound is based on VGGSound, a video dataset consisting of in-the-wild samples extracted from YouTube. During the curation, we remove samples where auditory events are not aligned with the visual ones. V-AURA outperforms current state-of-the-art models in temporal alignment and semantic relevance while maintaining comparable audio quality. Code, samples, VisualSound and models are available at https://v-aura.notion.site",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 20",
                "zh": "9æœˆ20æ—¥"
            },
            "hash": "f810134380d44eae",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#benchmark",
                    "#audio",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ·Ğ²ÑƒĞº Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ V-AURA - Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VisualSound Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ. V-AURA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ·Ğ²ÑƒĞºĞ°."
                },
                "en": {
                    "title": "V-AURA: Bridging Video and Audio with Precision",
                    "desc": "V-AURA is a novel autoregressive model designed for generating audio from video with high accuracy in timing and relevance. It employs a high-framerate visual feature extractor and a unique cross-modal feature fusion technique to effectively capture detailed visual movements, ensuring that the generated audio aligns well with the visual content. The paper also introduces VisualSound, a new benchmark dataset that enhances audio-visual relevance by filtering out misaligned samples from the existing VGGSound dataset. V-AURA demonstrates superior performance compared to existing models in both temporal alignment and semantic relevance, while also maintaining high audio quality."
                },
                "zh": {
                    "title": "V-AURAï¼šè§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "V-AURAæ˜¯é¦–ä¸ªè‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨è§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆä¸­å®ç°é«˜æ—¶é—´å¯¹é½å’Œç›¸å…³æ€§ã€‚å®ƒä½¿ç”¨é«˜å¸§ç‡çš„è§†è§‰ç‰¹å¾æå–å™¨å’Œè·¨æ¨¡æ€éŸ³è§†é¢‘ç‰¹å¾èåˆç­–ç•¥ï¼Œæ•æ‰ç»†ç²’åº¦çš„è§†è§‰è¿åŠ¨äº‹ä»¶ï¼Œç¡®ä¿ç²¾ç¡®çš„æ—¶é—´å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†VisualSoundï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰é«˜éŸ³è§†é¢‘ç›¸å…³æ€§çš„åŸºå‡†æ•°æ®é›†ï¼ŒåŸºäºVGGSoundè§†é¢‘æ•°æ®é›†ï¼ŒåŒ…å«ä»YouTubeæå–çš„çœŸå®æ ·æœ¬ã€‚V-AURAåœ¨æ—¶é—´å¯¹é½å’Œè¯­ä¹‰ç›¸å…³æ€§æ–¹é¢è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸å½“çš„éŸ³é¢‘è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11276",
            "title": "Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments",
            "url": "https://huggingface.co/papers/2409.11276",
            "abstract": "Large Language Models (LLMs) have shown remarkable potential across various domains, including cybersecurity. Using commercial cloud-based LLMs may be undesirable due to privacy concerns, costs, and network connectivity constraints. In this paper, we present Hackphyr, a locally fine-tuned LLM to be used as a red-team agent within network security environments. Our fine-tuned 7 billion parameter model can run on a single GPU card and achieves performance comparable with much larger and more powerful commercial models such as GPT-4. Hackphyr clearly outperforms other models, including GPT-3.5-turbo, and baselines, such as Q-learning agents in complex, previously unseen scenarios. To achieve this performance, we generated a new task-specific cybersecurity dataset to enhance the base model's capabilities. Finally, we conducted a comprehensive analysis of the agents' behaviors that provides insights into the planning abilities and potential shortcomings of such agents, contributing to the broader understanding of LLM-based agents in cybersecurity contexts",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "eb8622fef1dd25fe",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#rl",
                    "#interpretability"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ€Ğ¾ÑĞ°ĞµÑ‚ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ°Ğ¼ Ğ² ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Hackphyr - Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° ĞºÑ€Ğ°ÑĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ² ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ GPU Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Hackphyr: A Local LLM for Enhanced Cybersecurity Defense",
                    "desc": "This paper introduces Hackphyr, a locally fine-tuned large language model (LLM) designed for use as a red-team agent in cybersecurity. Unlike commercial cloud-based models, Hackphyr addresses privacy and cost concerns while maintaining high performance. The model, with 7 billion parameters, runs efficiently on a single GPU and competes effectively with larger models like GPT-4. Additionally, the authors created a new cybersecurity dataset to improve the model's training, and they analyzed the agent's behavior to understand its planning capabilities and limitations in complex scenarios."
                },
                "zh": {
                    "title": "Hackphyrï¼šæœ¬åœ°å¾®è°ƒçš„ç½‘ç»œå®‰å…¨çº¢é˜Ÿä»£ç†",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºHackphyrçš„æœ¬åœ°å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ—¨åœ¨ç½‘ç»œå®‰å…¨ç¯å¢ƒä¸­ä½œä¸ºçº¢é˜Ÿä»£ç†ã€‚è¯¥æ¨¡å‹æ‹¥æœ‰70äº¿ä¸ªå‚æ•°ï¼Œå¯ä»¥åœ¨å•ä¸ªGPUä¸Šè¿è¡Œï¼Œå…¶æ€§èƒ½ä¸æ›´å¤§æ›´å¼ºçš„å•†ä¸šæ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰ç›¸å½“ã€‚Hackphyråœ¨å¤æ‚ä¸”æœªè§è¿‡çš„åœºæ™¯ä¸­æ˜æ˜¾ä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒåŒ…æ‹¬GPT-3.5-turboå’ŒQå­¦ä¹ ä»£ç†ã€‚ä¸ºäº†æå‡æ¨¡å‹èƒ½åŠ›ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡ç‰¹å®šçš„ç½‘ç»œå®‰å…¨æ•°æ®é›†ï¼Œå¹¶å¯¹ä»£ç†çš„è¡Œä¸ºè¿›è¡Œäº†å…¨é¢åˆ†æï¼Œä»¥æ·±å…¥ç†è§£LLMåœ¨ç½‘ç»œå®‰å…¨ä¸­çš„åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11393",
            "title": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents",
            "url": "https://huggingface.co/papers/2409.11393",
            "abstract": "The integration of tools in LLM-based agents overcame the difficulties of standalone LLMs and traditional agents' limited capabilities. However, the conjunction of these technologies and the proposed enhancements in several state-of-the-art works followed a non-unified software architecture resulting in a lack of modularity. Indeed, they focused mainly on functionalities and overlooked the definition of the component's boundaries within the agent. This caused terminological and architectural ambiguities between researchers which we addressed in this paper by proposing a unified framework that establishes a clear foundation for LLM-based agents' development from both functional and software architectural perspectives.   Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework), clearly distinguishes between the different components of an agent, setting LLMs, and tools apart from a newly introduced element: the core-agent, playing the role of the central coordinator of the agent which comprises five modules: planning, memory, profile, action, and security, the latter often neglected in previous works. Differences in the internal structure of core-agents led us to classify them into a taxonomy of passive and active types. Based on this, we proposed different multi-core agent architectures combining unique characteristics of various individual agents.   For evaluation purposes, we applied this framework to a selection of state-of-the-art agents, thereby demonstrating its alignment with their functionalities and clarifying the overlooked architectural aspects. Moreover, we thoroughly assessed four of our proposed architectures by integrating distinctive agents into hybrid active/passive core-agents' systems. This analysis provided clear insights into potential improvements and highlighted the challenges involved in the combination of specific agents.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "ff5765ca9944c56e",
            "data": {
                "categories": [
                    "#agents",
                    "#architecture"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ 'core-agent' ĞºĞ°Ğº Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ° Ñ Ğ¿ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼Ğ¸: Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ, Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ, Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² core-agent. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°, LLM-Agent-UMF, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‡ĞµÑ‚ĞºĞ¾ Ñ€Ğ°Ğ·Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½ĞµÑÑĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unifying LLM-Based Agents for Clarity and Modularity",
                    "desc": "This paper addresses the challenges faced by LLM-based agents due to the lack of a unified software architecture, which has led to confusion among researchers. It introduces the LLM-Agent-UMF, a framework that clearly defines the components of LLM-based agents, including a new core-agent that coordinates various modules such as planning and security. The authors classify core-agents into passive and active types, allowing for the development of multi-core agent architectures that leverage the strengths of different agents. By applying this framework to existing agents, the paper demonstrates its effectiveness in clarifying functionalities and architectural aspects, while also identifying areas for improvement."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æ¡†æ¶ï¼Œæå‡æ™ºèƒ½ä½“èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ¡†æ¶ï¼Œåä¸ºLLM-Agent-UMFï¼Œç”¨äºæ”¹è¿›åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“çš„å¼€å‘ã€‚è¯¥æ¡†æ¶æ¸…æ™°åœ°åŒºåˆ†äº†æ™ºèƒ½ä½“çš„ä¸åŒç»„ä»¶ï¼ŒåŒ…æ‹¬LLMã€å·¥å…·å’Œæ ¸å¿ƒæ™ºèƒ½ä½“ï¼Œåè€…ä½œä¸ºä¸­å¤®åè°ƒè€…ï¼ŒåŒ…å«è§„åˆ’ã€è®°å¿†ã€ä¸ªäººèµ„æ–™ã€è¡ŒåŠ¨å’Œå®‰å…¨äº”ä¸ªæ¨¡å—ã€‚æˆ‘ä»¬è¿˜æ ¹æ®æ ¸å¿ƒæ™ºèƒ½ä½“çš„å†…éƒ¨ç»“æ„å°†å…¶åˆ†ç±»ä¸ºè¢«åŠ¨å‹å’Œä¸»åŠ¨å‹ï¼Œå¹¶æå‡ºäº†ä¸åŒçš„å¤šæ ¸å¿ƒæ™ºèƒ½ä½“æ¶æ„ã€‚é€šè¿‡å¯¹ç°æœ‰æ™ºèƒ½ä½“çš„è¯„ä¼°ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶ä¸å…¶åŠŸèƒ½çš„ä¸€è‡´æ€§ï¼Œå¹¶æ˜ç¡®äº†è¢«å¿½è§†çš„æ¶æ„æ–¹é¢ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-09-20.html",
    "link_next": "2024-09-24.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "20.09",
        "en": "09/20",
        "zh": "9æœˆ20æ—¥"
    },
    "short_date_next": {
        "ru": "24.09",
        "en": "09/24",
        "zh": "9æœˆ24æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 3,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 2,
        "#video": 3,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#medicine": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#translation": 0,
        "#prompt_engineering": 0
    }
}