{
    "date": {
        "ru": "23 сентября",
        "en": "September 23",
        "zh": "9月23日"
    },
    "time_utc": "2024-09-23 09:00",
    "weekday": 0,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-23",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.13346",
            "title": "Imagine yourself: Tuning-Free Personalized Image Generation",
            "url": "https://huggingface.co/papers/2409.13346",
            "abstract": "Diffusion models have demonstrated remarkable efficacy across various image-to-image tasks. In this research, we introduce Imagine yourself, a state-of-the-art model designed for personalized image generation. Unlike conventional tuning-based personalization techniques, Imagine yourself operates as a tuning-free model, enabling all users to leverage a shared framework without individualized adjustments. Moreover, previous work met challenges balancing identity preservation, following complex prompts and preserving good visual quality, resulting in models having strong copy-paste effect of the reference images. Thus, they can hardly generate images following prompts that require significant changes to the reference image, \\eg, changing facial expression, head and body poses, and the diversity of the generated images is low. To address these limitations, our proposed method introduces 1) a new synthetic paired data generation mechanism to encourage image diversity, 2) a fully parallel attention architecture with three text encoders and a fully trainable vision encoder to improve the text faithfulness, and 3) a novel coarse-to-fine multi-stage finetuning methodology that gradually pushes the boundary of visual quality. Our study demonstrates that Imagine yourself surpasses the state-of-the-art personalization model, exhibiting superior capabilities in identity preservation, visual quality, and text alignment. This model establishes a robust foundation for various personalization applications. Human evaluation results validate the model's SOTA superiority across all aspects (identity preservation, text faithfulness, and visual appeal) compared to the previous personalization models.",
            "score": 67,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "cd0a322cf520de60",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Революция в персонализированной генерации изображений без тонкой настройки",
                    "desc": "В статье представлена модель Imagine yourself для персонализированной генерации изображений. В отличие от методов тонкой настройки, эта модель не требует индивидуальной подстройки для каждого пользователя. Авторы предлагают новый механизм генерации синтетических парных данных, полностью параллельную архитектуру внимания и многоэтапную методологию обучения. Результаты показывают превосходство Imagine yourself над существующими моделями в сохранении идентичности, визуальном качестве и соответствии текстовым запросам."
                },
                "en": {
                    "title": "Personalized Image Generation Without Individual Tuning",
                    "desc": "This paper presents 'Imagine yourself', a cutting-edge diffusion model for personalized image generation that does not require individual tuning. It addresses the limitations of previous models by introducing a synthetic paired data generation mechanism to enhance image diversity and a parallel attention architecture to improve text alignment. The model employs a novel coarse-to-fine multi-stage finetuning approach to enhance visual quality while maintaining identity preservation. Human evaluations confirm that 'Imagine yourself' outperforms existing personalization models in identity preservation, text faithfulness, and overall visual appeal."
                },
                "zh": {
                    "title": "个性化图像生成的新突破",
                    "desc": "扩散模型在图像生成任务中表现出色。本研究提出了一种名为\"Imagine yourself\"的先进模型，旨在实现个性化图像生成。与传统的调优个性化技术不同，该模型无需个性化调整，允许所有用户在共享框架下使用。此外，我们的方法通过新颖的合成配对数据生成机制、全并行注意力架构和逐步细化的多阶段调优方法，克服了以往模型在身份保留、文本一致性和视觉质量方面的挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13592",
            "title": "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models",
            "url": "https://huggingface.co/papers/2409.13592",
            "abstract": "Understanding satire and humor is a challenging task for even current Vision-Language models. In this paper, we propose the challenging tasks of Satirical Image Detection (detecting whether an image is satirical), Understanding (generating the reason behind the image being satirical), and Completion (given one half of the image, selecting the other half from 2 given options, such that the complete image is satirical) and release a high-quality dataset YesBut, consisting of 2547 images, 1084 satirical and 1463 non-satirical, containing different artistic styles, to evaluate those tasks. Each satirical image in the dataset depicts a normal scenario, along with a conflicting scenario which is funny or ironic. Despite the success of current Vision-Language Models on multimodal tasks such as Visual QA and Image Captioning, our benchmarking experiments show that such models perform poorly on the proposed tasks on the YesBut Dataset in Zero-Shot Settings w.r.t both automated as well as human evaluation. Additionally, we release a dataset of 119 real, satirical photographs for further research. The dataset and code are available at https://github.com/abhi1nandy2/yesbut_dataset.",
            "score": 48,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "63915fb63f61f8bf",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Компьютер учится понимать сатиру в изображениях",
                    "desc": "Статья представляет новые задачи в области компьютерного зрения и обработки естественного языка: обнаружение, понимание и дополнение сатирических изображений. Авторы создали датасет YesBut, содержащий 2547 изображений (1084 сатирических и 1463 несатирических) различных художественных стилей. Эксперименты показали, что современные мультимодальные модели плохо справляются с этими задачами в режиме zero-shot. Также был выпущен дополнительный набор из 119 реальных сатирических фотографий для дальнейших исследований."
                },
                "en": {
                    "title": "Decoding Satire: A New Challenge for Vision-Language Models",
                    "desc": "This paper addresses the difficulty of understanding satire and humor in images using Vision-Language models. It introduces three tasks: Satirical Image Detection, Understanding the satire, and Completion of satirical images. The authors present a new dataset called YesBut, which includes 2547 images to evaluate these tasks, highlighting the contrast between normal and satirical scenarios. Benchmarking results reveal that existing models struggle with these tasks, indicating a gap in their ability to comprehend humor and irony in visual content."
                },
                "zh": {
                    "title": "揭示讽刺的挑战与机遇",
                    "desc": "理解讽刺和幽默对当前的视觉-语言模型来说是一个具有挑战性的任务。本文提出了三个任务：讽刺图像检测、理解讽刺原因和图像补全，并发布了一个高质量的数据集YesBut，包含2547张图像。尽管现有的视觉-语言模型在多模态任务上表现良好，但在YesBut数据集的基准测试中，这些模型在零样本设置下的表现却很差。我们还发布了119张真实的讽刺照片数据集，以供进一步研究。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13598",
            "title": "Prithvi WxC: Foundation Model for Weather and Climate",
            "url": "https://huggingface.co/papers/2409.13598",
            "abstract": "Triggered by the realization that AI emulators can rival the performance of traditional numerical weather prediction models running on HPC systems, there is now an increasing number of large AI models that address use cases such as forecasting, downscaling, or nowcasting. While the parallel developments in the AI literature focus on foundation models -- models that can be effectively tuned to address multiple, different use cases -- the developments on the weather and climate side largely focus on single-use cases with particular emphasis on mid-range forecasting. We close this gap by introducing Prithvi WxC, a 2.3 billion parameter foundation model developed using 160 variables from the Modern-Era Retrospective Analysis for Research and Applications, Version 2 (MERRA-2). Prithvi WxC employs an encoder-decoder-based architecture, incorporating concepts from various recent transformer models to effectively capture both regional and global dependencies in the input data. The model has been designed to accommodate large token counts to model weather phenomena in different topologies at fine resolutions. Furthermore, it is trained with a mixed objective that combines the paradigms of masked reconstruction with forecasting. We test the model on a set of challenging downstream tasks namely: Autoregressive rollout forecasting, Downscaling, Gravity wave flux parameterization, and Extreme events estimation. The pretrained model with 2.3 billion parameters, along with the associated fine-tuning workflows, has been publicly released as an open-source contribution via Hugging Face.",
            "score": 37,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "a0a84f660d5ff945",
            "data": {
                "categories": [
                    "#architecture",
                    "#dataset",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🌦️",
                "ru": {
                    "title": "Универсальная ИИ-модель для прогнозирования погоды и климата",
                    "desc": "Статья представляет Prithvi WxC - фундаментальную модель с 2,3 миллиардами параметров для прогнозирования погоды и климата. Модель использует архитектуру энкодер-декодер и обучена на 160 переменных из реанализа MERRA-2. Prithvi WxC может решать различные задачи, включая прогнозирование, даунскейлинг и оценку экстремальных событий. Модель и связанные с ней рабочие процессы доступны в открытом доступе через Hugging Face."
                },
                "en": {
                    "title": "Revolutionizing Weather Forecasting with AI Foundation Models",
                    "desc": "This paper introduces Prithvi WxC, a large foundation model designed for weather and climate applications, featuring 2.3 billion parameters. It utilizes an encoder-decoder architecture inspired by transformer models to effectively capture both regional and global dependencies in weather data. The model is trained on a diverse dataset from MERRA-2 and is capable of handling large token counts for fine-resolution weather phenomena modeling. It has been tested on various challenging tasks, including forecasting and downscaling, and is available as an open-source resource for further research."
                },
                "zh": {
                    "title": "基础模型助力天气预测新纪元",
                    "desc": "本论文介绍了一种名为Prithvi WxC的基础模型，具有23亿个参数，旨在解决天气和气候预测问题。该模型使用160个变量，基于现代时代回顾分析（MERRA-2）数据，采用编码器-解码器架构，能够有效捕捉输入数据的区域和全球依赖关系。Prithvi WxC设计用于处理大规模的标记数量，以在不同地形上以高分辨率模拟天气现象。模型经过混合目标训练，结合了掩蔽重建和预测的范式，并在多个下游任务上进行了测试。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13216",
            "title": "MuCodec: Ultra Low-Bitrate Music Codec",
            "url": "https://huggingface.co/papers/2409.13216",
            "abstract": "Music codecs are a vital aspect of audio codec research, and ultra low-bitrate compression holds significant importance for music transmission and generation. Due to the complexity of music backgrounds and the richness of vocals, solely relying on modeling semantic or acoustic information cannot effectively reconstruct music with both vocals and backgrounds. To address this issue, we propose MuCodec, specifically targeting music compression and reconstruction tasks at ultra low bitrates. MuCodec employs MuEncoder to extract both acoustic and semantic features, discretizes them with RVQ, and obtains Mel-VAE features via flow-matching. The music is then reconstructed using a pre-trained MEL-VAE decoder and HiFi-GAN. MuCodec can reconstruct high-fidelity music at ultra low (0.35kbps) or high bitrates (1.35kbps), achieving the best results to date in both subjective and objective metrics. Code and Demo: https://xuyaoxun.github.io/MuCodec_demo/.",
            "score": 22,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "82a6af61f8a6c886",
            "data": {
                "categories": [
                    "#audio"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "Революция в сжатии музыки: высокое качество при сверхнизких битрейтах",
                    "desc": "MuCodec - это новый подход к сжатию музыки при сверхнизких битрейтах. Он использует MuEncoder для извлечения акустических и семантических признаков, которые затем дискретизируются с помощью RVQ и преобразуются в признаки Mel-VAE. Реконструкция музыки выполняется предобученным декодером MEL-VAE и HiFi-GAN. MuCodec достигает наилучших результатов при битрейтах 0.35-1.35 кбит/с по субъективным и объективным метрикам."
                },
                "en": {
                    "title": "MuCodec: High-Fidelity Music Compression at Ultra Low Bitrates",
                    "desc": "This paper introduces MuCodec, a novel approach for music compression and reconstruction at ultra low bitrates. It addresses the challenge of effectively reconstructing music that includes both vocals and complex backgrounds by utilizing a combination of acoustic and semantic feature extraction. MuCodec employs a MuEncoder to gather these features, which are then processed using Residual Vector Quantization (RVQ) and flow-matching to obtain Mel-VAE features. The final music reconstruction is achieved through a pre-trained MEL-VAE decoder and HiFi-GAN, demonstrating superior performance in both subjective and objective evaluations at bitrates as low as 0.35kbps."
                },
                "zh": {
                    "title": "MuCodec：超低比特率音乐重建的创新解决方案",
                    "desc": "音乐编解码器在音频编解码研究中非常重要，超低比特率压缩对音乐传输和生成具有重要意义。由于音乐背景的复杂性和人声的丰富性，仅依靠建模语义或声学信息无法有效重建同时包含人声和背景的音乐。为了解决这个问题，我们提出了MuCodec，专门针对超低比特率下的音乐压缩和重建任务。MuCodec通过MuEncoder提取声学和语义特征，使用RVQ进行离散化，并通过流匹配获得Mel-VAE特征，最终利用预训练的MEL-VAE解码器和HiFi-GAN重建高保真音乐。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.12941",
            "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2409.12941",
            "abstract": "Large Language Models (LLMs) have demonstrated significant performance improvements across various cognitive tasks. An emerging application is using LLMs to enhance retrieval-augmented generation (RAG) capabilities. These systems require LLMs to understand user queries, retrieve relevant information, and synthesize coherent and accurate responses. Given the increasing real-world deployment of such systems, comprehensive evaluation becomes crucial. To this end, we propose FRAMES (Factuality, Retrieval, And reasoning MEasurement Set), a high-quality evaluation dataset designed to test LLMs' ability to provide factual responses, assess retrieval capabilities, and evaluate the reasoning required to generate final answers. While previous work has provided datasets and benchmarks to evaluate these abilities in isolation, FRAMES offers a unified framework that provides a clearer picture of LLM performance in end-to-end RAG scenarios. Our dataset comprises challenging multi-hop questions that require the integration of information from multiple sources. We present baseline results demonstrating that even state-of-the-art LLMs struggle with this task, achieving 0.40 accuracy with no retrieval. The accuracy is significantly improved with our proposed multi-step retrieval pipeline, achieving an accuracy of 0.66 (>50% improvement). We hope our work will help bridge evaluation gaps and assist in developing more robust and capable RAG systems.",
            "score": 20,
            "issue_id": 1,
            "pub_date": "2024-09-19",
            "pub_date_card": {
                "ru": "19 сентября",
                "en": "September 19",
                "zh": "9月19日"
            },
            "hash": "dc5c06fd6d7625ca",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#rag",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "FRAMES: Комплексная оценка LLM в задачах RAG",
                    "desc": "Статья представляет FRAMES - набор данных для оценки языковых моделей (LLM) в задачах извлечения и генерации информации (RAG). FRAMES тестирует способность моделей давать фактические ответы, оценивает возможности поиска и рассуждения при генерации ответов. Набор данных состоит из сложных многоэтапных вопросов, требующих интеграции информации из нескольких источников. Результаты показывают, что даже современные LLM испытывают трудности с этой задачей, но предложенный авторами многоэтапный конвейер поиска значительно улучшает точность."
                },
                "en": {
                    "title": "Enhancing LLMs with FRAMES for Better RAG Performance",
                    "desc": "This paper discusses the use of Large Language Models (LLMs) to improve retrieval-augmented generation (RAG) systems, which combine information retrieval and text generation. The authors introduce FRAMES, a new evaluation dataset that measures LLMs' factual accuracy, retrieval effectiveness, and reasoning skills in generating responses. Unlike previous benchmarks that assessed these abilities separately, FRAMES provides a comprehensive framework for evaluating LLM performance in real-world scenarios. The results show that while current LLMs perform poorly without retrieval, their accuracy significantly improves when using a multi-step retrieval approach."
                },
                "zh": {
                    "title": "提升检索增强生成系统的评估能力",
                    "desc": "大型语言模型（LLMs）在各种认知任务中表现出显著的性能提升。本文提出了一种新的评估数据集FRAMES，旨在测试LLMs在检索增强生成（RAG）系统中的能力，包括提供事实性回答、评估检索能力和推理能力。FRAMES数据集包含具有挑战性的多跳问题，需要整合来自多个来源的信息。我们的实验结果表明，尽管当前最先进的LLMs在没有检索的情况下准确率仅为0.40，但通过我们提出的多步骤检索管道，准确率提高至0.66，超过50%的提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13591",
            "title": "Portrait Video Editing Empowered by Multimodal Generative Priors",
            "url": "https://huggingface.co/papers/2409.13591",
            "abstract": "We introduce PortraitGen, a powerful portrait video editing method that achieves consistent and expressive stylization with multimodal prompts. Traditional portrait video editing methods often struggle with 3D and temporal consistency, and typically lack in rendering quality and efficiency. To address these issues, we lift the portrait video frames to a unified dynamic 3D Gaussian field, which ensures structural and temporal coherence across frames. Furthermore, we design a novel Neural Gaussian Texture mechanism that not only enables sophisticated style editing but also achieves rendering speed over 100FPS. Our approach incorporates multimodal inputs through knowledge distilled from large-scale 2D generative models. Our system also incorporates expression similarity guidance and a face-aware portrait editing module, effectively mitigating degradation issues associated with iterative dataset updates. Extensive experiments demonstrate the temporal consistency, editing efficiency, and superior rendering quality of our method. The broad applicability of the proposed approach is demonstrated through various applications, including text-driven editing, image-driven editing, and relighting, highlighting its great potential to advance the field of video editing. Demo videos and released code are provided in our project page: https://ustc3dv.github.io/PortraitGen/",
            "score": 15,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "bdf416584245d302",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal",
                    "#3d",
                    "#dataset"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "PortraitGen: Революция в редактировании портретных видео с помощью 3D гауссовых полей и нейронных текстур",
                    "desc": "PortraitGen - это метод редактирования портретных видео, обеспечивающий согласованную и выразительную стилизацию с помощью мультимодальных подсказок. Он использует динамическое 3D гауссово поле для обеспечения структурной и временной согласованности кадров. Метод включает механизм нейронной гауссовой текстуры для сложного стилевого редактирования и быстрого рендеринга. PortraitGen интегрирует знания из крупномасштабных 2D генеративных моделей и включает модули для сохранения выражения лица и улучшения качества редактирования."
                },
                "en": {
                    "title": "Revolutionizing Portrait Video Editing with PortraitGen",
                    "desc": "PortraitGen is a novel method for editing portrait videos that focuses on maintaining consistency and expressiveness through multimodal prompts. It overcomes challenges in traditional editing methods by utilizing a unified dynamic 3D Gaussian field, which ensures both structural and temporal coherence across video frames. The introduction of a Neural Gaussian Texture mechanism allows for advanced style editing while achieving high rendering speeds of over 100 frames per second. Extensive experiments validate its effectiveness in editing efficiency, rendering quality, and broad applicability in various editing tasks such as text-driven and image-driven editing."
                },
                "zh": {
                    "title": "PortraitGen：高效一致的肖像视频编辑新方法",
                    "desc": "本文介绍了一种名为PortraitGen的强大肖像视频编辑方法，能够通过多模态提示实现一致且富有表现力的风格化。传统的肖像视频编辑方法在三维和时间一致性方面常常面临挑战，且在渲染质量和效率上表现不足。为了解决这些问题，我们将肖像视频帧提升到统一的动态三维高斯场，从而确保帧之间的结构和时间一致性。此外，我们设计了一种新颖的神经高斯纹理机制，不仅支持复杂的风格编辑，还能实现超过100FPS的渲染速度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13690",
            "title": "Colorful Diffuse Intrinsic Image Decomposition in the Wild",
            "url": "https://huggingface.co/papers/2409.13690",
            "abstract": "Intrinsic image decomposition aims to separate the surface reflectance and the effects from the illumination given a single photograph. Due to the complexity of the problem, most prior works assume a single-color illumination and a Lambertian world, which limits their use in illumination-aware image editing applications. In this work, we separate an input image into its diffuse albedo, colorful diffuse shading, and specular residual components. We arrive at our result by gradually removing first the single-color illumination and then the Lambertian-world assumptions. We show that by dividing the problem into easier sub-problems, in-the-wild colorful diffuse shading estimation can be achieved despite the limited ground-truth datasets. Our extended intrinsic model enables illumination-aware analysis of photographs and can be used for image editing applications such as specularity removal and per-pixel white balancing.",
            "score": 12,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "49e7d3ade160e4b6",
            "data": {
                "categories": [
                    "#cv",
                    "#dataset"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Декомпозиция изображений для анализа освещения и редактирования фото",
                    "desc": "Статья посвящена декомпозиции изображений на составляющие: отражательную способность поверхности и эффекты освещения. Авторы предлагают метод разделения входного изображения на диффузное альбедо, цветное диффузное затенение и зеркальный остаток. Подход постепенно устраняет ограничения предыдущих работ, связанные с однородным освещением и ламбертовским отражением. Результаты позволяют проводить анализ освещения на фотографиях и применять их для редактирования изображений."
                },
                "en": {
                    "title": "Revolutionizing Image Editing with Advanced Intrinsic Decomposition",
                    "desc": "This paper presents a method for intrinsic image decomposition, which separates an image into its surface reflectance and illumination effects. Unlike previous approaches that relied on single-color lighting and Lambertian surfaces, this method tackles the problem by breaking it down into simpler components: diffuse albedo, colorful diffuse shading, and specular residuals. By relaxing the assumptions of uniform illumination, the authors demonstrate that it is possible to estimate colorful shading in real-world images, even with limited training data. The proposed model enhances the ability to perform illumination-aware image editing tasks, such as removing specularity and adjusting colors on a per-pixel basis."
                },
                "zh": {
                    "title": "分离光照与反射，提升图像编辑能力",
                    "desc": "内在图像分解的目标是从单张照片中分离出表面反射率和光照效果。以往的研究大多假设单一颜色的光照和朗伯世界，这限制了其在光照感知图像编辑中的应用。我们的方法将输入图像分解为漫反射反照率、丰富的漫反射阴影和镜面残余成分。通过逐步去除单一颜色光照和朗伯假设，我们的扩展内在模型实现了光照感知的照片分析，并可用于图像编辑应用，如去除镜面反射和逐像素白平衡。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13648",
            "title": "V^3: Viewing Volumetric Videos on Mobiles via Streamable 2D Dynamic Gaussians",
            "url": "https://huggingface.co/papers/2409.13648",
            "abstract": "Experiencing high-fidelity volumetric video as seamlessly as 2D videos is a long-held dream. However, current dynamic 3DGS methods, despite their high rendering quality, face challenges in streaming on mobile devices due to computational and bandwidth constraints. In this paper, we introduce V3(Viewing Volumetric Videos), a novel approach that enables high-quality mobile rendering through the streaming of dynamic Gaussians. Our key innovation is to view dynamic 3DGS as 2D videos, facilitating the use of hardware video codecs. Additionally, we propose a two-stage training strategy to reduce storage requirements with rapid training speed. The first stage employs hash encoding and shallow MLP to learn motion, then reduces the number of Gaussians through pruning to meet the streaming requirements, while the second stage fine tunes other Gaussian attributes using residual entropy loss and temporal loss to improve temporal continuity. This strategy, which disentangles motion and appearance, maintains high rendering quality with compact storage requirements. Meanwhile, we designed a multi-platform player to decode and render 2D Gaussian videos. Extensive experiments demonstrate the effectiveness of V3, outperforming other methods by enabling high-quality rendering and streaming on common devices, which is unseen before. As the first to stream dynamic Gaussians on mobile devices, our companion player offers users an unprecedented volumetric video experience, including smooth scrolling and instant sharing. Our project page with source code is available at https://authoritywang.github.io/v3/.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "6754c4dc77bfb7a4",
            "data": {
                "categories": [
                    "#3d",
                    "#video",
                    "#inference",
                    "#training"
                ],
                "emoji": "📱",
                "ru": {
                    "title": "Стриминг объемного видео на мобильных устройствах",
                    "desc": "Статья представляет V3 (Viewing Volumetric Videos) - новый подход к стримингу динамических гауссовых сплатов на мобильных устройствах. Авторы предлагают рассматривать динамические 3DGS как 2D видео, что позволяет использовать аппаратные видеокодеки. Применяется двухэтапная стратегия обучения: сначала используется хэш-кодирование и неглубокая MLP для изучения движения, затем производится прореживание гауссианов. Второй этап настраивает другие атрибуты гауссианов с помощью остаточной энтропийной потери и временной потери для улучшения временной непрерывности."
                },
                "en": {
                    "title": "Stream High-Quality Volumetric Videos Seamlessly on Mobile!",
                    "desc": "This paper presents V3, a new method for streaming high-quality volumetric videos on mobile devices. It addresses the limitations of current dynamic 3D Gaussian streaming methods by treating them like 2D videos, allowing the use of efficient hardware video codecs. The authors introduce a two-stage training strategy that optimizes storage and improves rendering quality by separating motion from appearance. Extensive tests show that V3 significantly enhances the user experience by enabling smooth playback and quick sharing of volumetric content on common devices."
                },
                "zh": {
                    "title": "移动设备上的高质量体积视频流媒体体验",
                    "desc": "本论文介绍了一种名为V3的新方法，旨在实现高质量的移动设备体积视频渲染。我们将动态3D高斯视为2D视频，从而利用硬件视频编解码器来解决流媒体传输中的计算和带宽限制。通过两阶段的训练策略，我们有效减少了存储需求，同时保持了高渲染质量。实验结果表明，V3在常见设备上实现了高质量的渲染和流媒体传输，提供了前所未有的体积视频体验。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13449",
            "title": "Minstrel: Structural Prompt Generation with Multi-Agents Coordination for Non-AI Experts",
            "url": "https://huggingface.co/papers/2409.13449",
            "abstract": "LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to assist them in their work poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat scattered optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structural design, incurring high learning costs and it is not conducive to the iterative updating of prompts, especially for non-AI experts. Inspired by structured reusable programming languages, we propose LangGPT, a structural prompt design framework. Furthermore, we introduce Minstrel, a multi-generative agent system with reflection to automate the generation of structural prompts. Experiments and the case study illustrate that structural prompts generated by Minstrel or written manually significantly enhance the performance of LLMs. Furthermore, we analyze the ease of use of structural prompts through a user survey in our online community.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "0e0e0cdbcc3fa527",
            "data": {
                "categories": [
                    "#agents",
                    "#prompt_engineering",
                    "#multimodal"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Структурные промпты: новый подход к управлению языковыми моделями",
                    "desc": "Исследователи предлагают LangGPT - структурный фреймворк для разработки промптов, вдохновленный языками программирования. Они также представляют Minstrel - мульти-генеративную агентскую систему для автоматической генерации структурных промптов. Эксперименты показывают, что структурные промпты, созданные Minstrel или написанные вручную, значительно улучшают производительность языковых моделей. Опрос пользователей подтверждает удобство использования структурных промптов."
                },
                "en": {
                    "title": "Empowering Non-Experts with Structured Prompt Design for LLMs",
                    "desc": "This paper addresses the challenge of creating effective prompts for large language models (LLMs), particularly for users without AI expertise. It critiques existing prompt engineering methods for their lack of structure and high learning costs, which hinder iterative improvements. The authors introduce LangGPT, a framework for structured prompt design, and Minstrel, a system that automates the generation of these structured prompts. Experimental results show that using structural prompts improves LLM performance, and user feedback indicates that these prompts are easier to use."
                },
                "zh": {
                    "title": "结构化提示，助力LLMs更强大",
                    "desc": "本论文提出了一种名为LangGPT的结构化提示设计框架，旨在帮助非人工智能专家更好地使用大型语言模型（LLMs）。我们还介绍了Minstrel，一个多生成代理系统，能够自动生成结构化提示，从而简化提示工程的过程。实验结果表明，通过Minstrel生成的结构化提示或手动编写的提示，显著提高了LLMs的性能。最后，我们通过用户调查分析了结构化提示的易用性，显示出其在实际应用中的优势。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.13689",
            "title": "Temporally Aligned Audio for Video with Autoregression",
            "url": "https://huggingface.co/papers/2409.13689",
            "abstract": "We introduce V-AURA, the first autoregressive model to achieve high temporal alignment and relevance in video-to-audio generation. V-AURA uses a high-framerate visual feature extractor and a cross-modal audio-visual feature fusion strategy to capture fine-grained visual motion events and ensure precise temporal alignment. Additionally, we propose VisualSound, a benchmark dataset with high audio-visual relevance. VisualSound is based on VGGSound, a video dataset consisting of in-the-wild samples extracted from YouTube. During the curation, we remove samples where auditory events are not aligned with the visual ones. V-AURA outperforms current state-of-the-art models in temporal alignment and semantic relevance while maintaining comparable audio quality. Code, samples, VisualSound and models are available at https://v-aura.notion.site",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2024-09-20",
            "pub_date_card": {
                "ru": "20 сентября",
                "en": "September 20",
                "zh": "9月20日"
            },
            "hash": "f810134380d44eae",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#benchmark",
                    "#audio",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Синхронизированный звук из видео: новый уровень генерации аудио",
                    "desc": "В статье представлена модель V-AURA - первая авторегрессионная модель для генерации аудио по видео с высокой временной синхронизацией и релевантностью. Модель использует экстрактор визуальных признаков с высокой частотой кадров и стратегию кросс-модального слияния аудио-визуальных признаков. Авторы также предлагают новый датасет VisualSound с высокой аудио-визуальной релевантностью. V-AURA превосходит современные модели по временной синхронизации и семантической релевантности при сопоставимом качестве звука."
                },
                "en": {
                    "title": "V-AURA: Bridging Video and Audio with Precision",
                    "desc": "V-AURA is a novel autoregressive model designed for generating audio from video with high accuracy in timing and relevance. It employs a high-framerate visual feature extractor and a unique cross-modal feature fusion technique to effectively capture detailed visual movements, ensuring that the generated audio aligns well with the visual content. The paper also introduces VisualSound, a new benchmark dataset that enhances audio-visual relevance by filtering out misaligned samples from the existing VGGSound dataset. V-AURA demonstrates superior performance compared to existing models in both temporal alignment and semantic relevance, while also maintaining high audio quality."
                },
                "zh": {
                    "title": "V-AURA：视频到音频生成的新突破",
                    "desc": "V-AURA是首个自回归模型，能够在视频到音频生成中实现高时间对齐和相关性。它使用高帧率的视觉特征提取器和跨模态音视频特征融合策略，捕捉细粒度的视觉运动事件，确保精确的时间对齐。此外，我们提出了VisualSound，这是一个具有高音视频相关性的基准数据集，基于VGGSound视频数据集，包含从YouTube提取的真实样本。V-AURA在时间对齐和语义相关性方面超越了当前最先进的模型，同时保持了相当的音频质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11276",
            "title": "Hackphyr: A Local Fine-Tuned LLM Agent for Network Security Environments",
            "url": "https://huggingface.co/papers/2409.11276",
            "abstract": "Large Language Models (LLMs) have shown remarkable potential across various domains, including cybersecurity. Using commercial cloud-based LLMs may be undesirable due to privacy concerns, costs, and network connectivity constraints. In this paper, we present Hackphyr, a locally fine-tuned LLM to be used as a red-team agent within network security environments. Our fine-tuned 7 billion parameter model can run on a single GPU card and achieves performance comparable with much larger and more powerful commercial models such as GPT-4. Hackphyr clearly outperforms other models, including GPT-3.5-turbo, and baselines, such as Q-learning agents in complex, previously unseen scenarios. To achieve this performance, we generated a new task-specific cybersecurity dataset to enhance the base model's capabilities. Finally, we conducted a comprehensive analysis of the agents' behaviors that provides insights into the planning abilities and potential shortcomings of such agents, contributing to the broader understanding of LLM-based agents in cybersecurity contexts",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "eb8622fef1dd25fe",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#rl",
                    "#interpretability"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Локальная языковая модель бросает вызов гигантам в кибербезопасности",
                    "desc": "Исследователи представили Hackphyr - локально дообученную языковую модель для использования в качестве агента красной команды в сетевой безопасности. Модель с 7 миллиардами параметров работает на одной GPU и показывает результаты, сравнимые с более крупными коммерческими моделями. Для улучшения возможностей базовой модели был создан специализированный набор данных по кибербезопасности. Проведен комплексный анализ поведения агентов, что способствует лучшему пониманию применения языковых моделей в контексте кибербезопасности."
                },
                "en": {
                    "title": "Hackphyr: A Local LLM for Enhanced Cybersecurity Defense",
                    "desc": "This paper introduces Hackphyr, a locally fine-tuned large language model (LLM) designed for use as a red-team agent in cybersecurity. Unlike commercial cloud-based models, Hackphyr addresses privacy and cost concerns while maintaining high performance. The model, with 7 billion parameters, runs efficiently on a single GPU and competes effectively with larger models like GPT-4. Additionally, the authors created a new cybersecurity dataset to improve the model's training, and they analyzed the agent's behavior to understand its planning capabilities and limitations in complex scenarios."
                },
                "zh": {
                    "title": "Hackphyr：本地微调的网络安全红队代理",
                    "desc": "本论文介绍了一种名为Hackphyr的本地微调大型语言模型（LLM），旨在网络安全环境中作为红队代理。该模型拥有70亿个参数，可以在单个GPU上运行，其性能与更大更强的商业模型（如GPT-4）相当。Hackphyr在复杂且未见过的场景中明显优于其他模型，包括GPT-3.5-turbo和Q学习代理。为了提升模型能力，我们生成了一个新的任务特定的网络安全数据集，并对代理的行为进行了全面分析，以深入理解LLM在网络安全中的应用。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.11393",
            "title": "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents",
            "url": "https://huggingface.co/papers/2409.11393",
            "abstract": "The integration of tools in LLM-based agents overcame the difficulties of standalone LLMs and traditional agents' limited capabilities. However, the conjunction of these technologies and the proposed enhancements in several state-of-the-art works followed a non-unified software architecture resulting in a lack of modularity. Indeed, they focused mainly on functionalities and overlooked the definition of the component's boundaries within the agent. This caused terminological and architectural ambiguities between researchers which we addressed in this paper by proposing a unified framework that establishes a clear foundation for LLM-based agents' development from both functional and software architectural perspectives.   Our framework, LLM-Agent-UMF (LLM-based Agent Unified Modeling Framework), clearly distinguishes between the different components of an agent, setting LLMs, and tools apart from a newly introduced element: the core-agent, playing the role of the central coordinator of the agent which comprises five modules: planning, memory, profile, action, and security, the latter often neglected in previous works. Differences in the internal structure of core-agents led us to classify them into a taxonomy of passive and active types. Based on this, we proposed different multi-core agent architectures combining unique characteristics of various individual agents.   For evaluation purposes, we applied this framework to a selection of state-of-the-art agents, thereby demonstrating its alignment with their functionalities and clarifying the overlooked architectural aspects. Moreover, we thoroughly assessed four of our proposed architectures by integrating distinctive agents into hybrid active/passive core-agents' systems. This analysis provided clear insights into potential improvements and highlighted the challenges involved in the combination of specific agents.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2024-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "ff5765ca9944c56e",
            "data": {
                "categories": [
                    "#agents",
                    "#architecture"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Унифицированная архитектура для создания интеллектуальных агентов нового поколения",
                    "desc": "Статья представляет унифицированную архитектуру для агентов на основе больших языковых моделей (LLM). Авторы вводят понятие 'core-agent' как центрального координатора с пятью модулями: планирование, память, профиль, действие и безопасность. Предложена таксономия пассивных и активных типов core-agent. Разработанная структура, LLM-Agent-UMF, позволяет четко разграничить компоненты агента и устранить терминологические неясности."
                },
                "en": {
                    "title": "Unifying LLM-Based Agents for Clarity and Modularity",
                    "desc": "This paper addresses the challenges faced by LLM-based agents due to the lack of a unified software architecture, which has led to confusion among researchers. It introduces the LLM-Agent-UMF, a framework that clearly defines the components of LLM-based agents, including a new core-agent that coordinates various modules such as planning and security. The authors classify core-agents into passive and active types, allowing for the development of multi-core agent architectures that leverage the strengths of different agents. By applying this framework to existing agents, the paper demonstrates its effectiveness in clarifying functionalities and architectural aspects, while also identifying areas for improvement."
                },
                "zh": {
                    "title": "统一框架，提升智能体能力",
                    "desc": "本文提出了一种统一的框架，名为LLM-Agent-UMF，用于改进基于大型语言模型（LLM）的智能体的开发。该框架清晰地区分了智能体的不同组件，包括LLM、工具和核心智能体，后者作为中央协调者，包含规划、记忆、个人资料、行动和安全五个模块。我们还根据核心智能体的内部结构将其分类为被动型和主动型，并提出了不同的多核心智能体架构。通过对现有智能体的评估，验证了该框架与其功能的一致性，并明确了被忽视的架构方面。"
                }
            }
        }
    ],
    "link_prev": "2024-09-20.html",
    "link_next": "2024-09-24.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "20.09",
        "en": "09/20",
        "zh": "9月20日"
    },
    "short_date_next": {
        "ru": "24.09",
        "en": "09/24",
        "zh": "9月24日"
    },
    "categories": {
        "#dataset": 7,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 3,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 2,
        "#video": 3,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#medicine": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#translation": 0,
        "#prompt_engineering": 0
    }
}