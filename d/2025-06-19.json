{
    "date": {
        "ru": "19 Ğ¸ÑĞ½Ñ",
        "en": "June 19",
        "zh": "6æœˆ19æ—¥"
    },
    "time_utc": "2025-06-19 05:12",
    "weekday": 3,
    "issue_id": 4373,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.15675",
            "title": "Sekai: A Video Dataset towards World Exploration",
            "url": "https://huggingface.co/papers/2506.15675",
            "abstract": "Sekai, a worldwide video dataset with comprehensive annotations, is introduced to support world exploration applications, enhancing video generation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning ``dream'' in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications.",
            "score": 9,
            "issue_id": 4373,
            "pub_date": "2025-06-18",
            "pub_date_card": {
                "ru": "18 Ğ¸ÑĞ½Ñ",
                "en": "June 18",
                "zh": "6æœˆ18æ—¥"
            },
            "hash": "4f989f259f55f0ee",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#synthetic",
                    "#dataset",
                    "#games",
                    "#data",
                    "#video"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Sekai: Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¸Ñ€",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Sekai Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ†ĞµĞ»ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ°. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 5000 Ñ‡Ğ°ÑĞ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ° Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 100 ÑÑ‚Ñ€Ğ°Ğ½ Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ°, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ YUME Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Sekai: Unlocking the World Through Video Generation",
                    "desc": "This paper presents Sekai, a new video dataset designed to enhance video generation models for world exploration applications. Sekai includes over 5,000 hours of first-person view videos from diverse locations, addressing the limitations of existing datasets by providing rich annotations such as location, scene, and weather conditions. The authors also introduce a toolbox for efficient video collection and annotation, ensuring high-quality data for training. The dataset is validated through experiments and is used to train an interactive video exploration model called YUME, showcasing its potential impact on video generation and exploration technologies."
                },
                "zh": {
                    "title": "Sekaiï¼šå…¨çƒæ¢ç´¢çš„æ–°è§†é‡",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºSekaiçš„å…¨çƒè§†é¢‘æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¯æŒä¸–ç•Œæ¢ç´¢åº”ç”¨å¹¶å¢å¼ºè§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ª100å¤šä¸ªå›½å®¶å’Œåœ°åŒºçš„5000å¤šä¸ªå°æ—¶çš„ç¬¬ä¸€äººç§°è§†è§’è§†é¢‘ï¼Œæ¶µç›–750ä¸ªåŸå¸‚ï¼Œå…·æœ‰ä¸°å¯Œçš„æ³¨é‡Šä¿¡æ¯ã€‚Sekaiè§£å†³äº†ç°æœ‰è§†é¢‘ç”Ÿæˆæ•°æ®é›†åœ¨ä½ç½®ã€æ—¶é•¿ã€åœºæ™¯é™æ€æ€§å’Œæ¢ç´¢æ³¨é‡Šç­‰æ–¹é¢çš„å±€é™æ€§ã€‚é€šè¿‡å®éªŒéªŒè¯äº†æ•°æ®é›†çš„è´¨é‡ï¼Œå¹¶ä½¿ç”¨å…¶å­é›†è®­ç»ƒäº†ä¸€ä¸ªåä¸ºYUMEçš„äº’åŠ¨è§†é¢‘ä¸–ç•Œæ¢ç´¢æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.15681",
            "title": "GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2506.15681",
            "abstract": "GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models in real-world scenarios, particularly on resource-constrained devices, remains challenging due to their substantial computational demands. This has spurred interest in distilling knowledge from large VLMs into smaller, more efficient counterparts. A key challenge arises here from the diversity of VLM architectures, which are built on different LLMs and employ varying token types-differing in vocabulary size, token splits, and token index ordering. To address this challenge of limitation to a specific VLM type, we present Generation after Recalibration (GenRecal), a novel, general-purpose distillation framework for VLMs. GenRecal incorporates a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, enabling effective knowledge transfer across different types of VLMs. Through extensive experiments on multiple challenging benchmarks, we demonstrate that GenRecal significantly improves baseline performances, eventually outperforming large-scale open- and closed-source VLMs.",
            "score": 6,
            "issue_id": 4370,
            "pub_date": "2025-06-18",
            "pub_date_card": {
                "ru": "18 Ğ¸ÑĞ½Ñ",
                "en": "June 18",
                "zh": "6æœˆ18æ—¥"
            },
            "hash": "2d531d89420a04d8",
            "authors": [
                "Byung-Kwan Lee",
                "Ryo Hachiuma",
                "Yong Man Ro",
                "Yu-Chiang Frank Wang",
                "Yueh-Hua Wu"
            ],
            "affiliations": [
                "KAIST",
                "NVIDIA",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.15681.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#optimization",
                    "#inference",
                    "#training",
                    "#multimodal",
                    "#dataset",
                    "#architecture"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "GenRecal: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "GenRecal - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Recalibrator Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ VLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GenRecal Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ VLM. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Aligning Features for Efficient Vision-Language Models",
                    "desc": "GenRecal is a new framework designed to enhance the performance of vision-language models (VLMs) by aligning their feature representations across various architectures. It addresses the challenge of transferring knowledge from large, complex VLMs to smaller, more efficient models, which is crucial for deployment on devices with limited resources. The framework includes a Recalibrator that adapts features from different VLMs, allowing for effective knowledge transfer despite differences in architecture and tokenization. Experimental results show that GenRecal not only improves baseline performance but also surpasses both open-source and closed-source VLMs in various benchmarks."
                },
                "zh": {
                    "title": "GenRecalï¼šæå‡è§†è§‰-è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ¡†æ¶",
                    "desc": "GenRecalæ˜¯ä¸€ç§æ–°é¢–çš„çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯¹ä¸åŒæ¶æ„çš„ç‰¹å¾è¡¨ç¤ºè¿›è¡Œå¯¹é½ï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç”±äºä¸åŒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¶æ„çš„å¤šæ ·æ€§è€Œå¯¼è‡´çš„çŸ¥è¯†è½¬ç§»æŒ‘æˆ˜ã€‚GenRecalå¼•å…¥äº†ä¸€ä¸ªé‡æ ¡å‡†å™¨ï¼Œèƒ½å¤Ÿåœ¨å¼‚æ„VLMä¹‹é—´å¯¹ç‰¹å¾è¡¨ç¤ºè¿›è¡Œé€‚é…ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„çŸ¥è¯†ä¼ é€’ã€‚é€šè¿‡åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜GenRecalæ˜¾è‘—æé«˜äº†åŸºçº¿æ€§èƒ½ï¼Œæœ€ç»ˆè¶…è¶Šäº†å¤§å‹å¼€æºå’Œé—­æºçš„VLMã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.15677",
            "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence",
            "url": "https://huggingface.co/papers/2506.15677",
            "abstract": "Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.  \t\t\t\t\tAI-generated summary \t\t\t\t AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.",
            "score": 4,
            "issue_id": 4370,
            "pub_date": "2025-06-18",
            "pub_date_card": {
                "ru": "18 Ğ¸ÑĞ½Ñ",
                "en": "June 18",
                "zh": "6æœˆ18æ—¥"
            },
            "hash": "9dff3e9d64c54e88",
            "authors": [
                "Yining Hong",
                "Rui Sun",
                "Bingxuan Li",
                "Xingcheng Yao",
                "Maxine Wu",
                "Alexander Chien",
                "Da Yin",
                "Ying Nian Wu",
                "Zhecan James Wang",
                "Kai-Wei Chang"
            ],
            "affiliations": [
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.15677.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#benchmark",
                    "#open_source",
                    "#agents",
                    "#multimodal",
                    "#agi",
                    "#reasoning"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ² Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° - Embodied Web Agents, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ Ğ²ĞµĞ±-Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D-ÑÑ€ĞµĞ´Ñ‹ Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµĞ±-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Embodied Web Agents Benchmark Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Bridging Physical and Digital Intelligence with Embodied Web Agents",
                    "desc": "The paper introduces Embodied Web Agents, a new type of AI that combines physical interaction with web-based reasoning. This integration allows the agents to perform tasks that require both physical actions and access to online information, such as cooking or navigating. The authors create a benchmark environment that includes realistic 3D settings and web interfaces to evaluate these agents' abilities. Their findings show that current AI systems still lag behind human performance, highlighting both the challenges and potential for improvement in this area."
                },
                "zh": {
                    "title": "å…·èº«ç½‘ç»œä»£ç†ï¼šè¿æ¥ç‰©ç†ä¸æ•°å­—æ™ºèƒ½çš„æ¡¥æ¢",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„äººå·¥æ™ºèƒ½ä»£ç†æ¨¡å‹â€”â€”å…·èº«ç½‘ç»œä»£ç†ï¼ˆEmbodied Web Agentsï¼‰ï¼Œå®ƒå°†ç‰©ç†äº¤äº’ä¸ç½‘ç»œè§„æ¨¡æ¨ç†ç»“åˆåœ¨ä¸€èµ·ï¼Œä»¥è¯„ä¼°è·¨é¢†åŸŸæ™ºèƒ½ã€‚å½“å‰çš„AIä»£ç†é€šå¸¸åªèƒ½åœ¨æ•°å­—ä¿¡æ¯æ£€ç´¢æˆ–ç‰©ç†ä¸–ç•Œäº¤äº’ä¸­å‘æŒ¥ä½œç”¨ï¼Œç¼ºä¹ä¸¤è€…çš„æ•´åˆï¼Œé™åˆ¶äº†å®ƒä»¬è§£å†³å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼€å‘äº†å…·èº«ç½‘ç»œä»£ç†ä»»åŠ¡ç¯å¢ƒï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ä»¿çœŸå¹³å°ï¼Œèƒ½å¤Ÿå°†çœŸå®çš„3Dç¯å¢ƒä¸åŠŸèƒ½æ€§ç½‘ç»œæ¥å£ç´§å¯†ç»“åˆã€‚é€šè¿‡è¿™ä¸€å¹³å°ï¼Œæˆ‘ä»¬æ„å»ºå¹¶å‘å¸ƒäº†å…·èº«ç½‘ç»œä»£ç†åŸºå‡†ï¼Œæ¶µç›–äº†çƒ¹é¥ªã€å¯¼èˆªã€è´­ç‰©ç­‰å¤šç§ä»»åŠ¡ï¼Œè¦æ±‚åœ¨ç‰©ç†å’Œæ•°å­—é¢†åŸŸä¹‹é—´è¿›è¡Œåè°ƒæ¨ç†ï¼Œä»¥ç³»ç»Ÿè¯„ä¼°è·¨é¢†åŸŸæ™ºèƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.15569",
            "title": "SciVer: Evaluating Foundation Models for Multimodal Scientific Claim\n  Verification",
            "url": "https://huggingface.co/papers/2506.15569",
            "abstract": "A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SciVer, the first benchmark specifically designed to evaluate the ability of foundation models to verify claims within a multimodal scientific context. SciVer consists of 3,000 expert-annotated examples over 1,113 scientific papers, covering four subsets, each representing a common reasoning type in multimodal scientific claim verification. To enable fine-grained evaluation, each example includes expert-annotated supporting evidence. We assess the performance of 21 state-of-the-art multimodal foundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and Qwen2.5-VL. Our experiment reveals a substantial performance gap between these models and human experts on SciVer. Through an in-depth analysis of retrieval-augmented generation (RAG), and human-conducted error evaluations, we identify critical limitations in current open-source models, offering key insights to advance models' comprehension and reasoning in multimodal scientific literature tasks.",
            "score": 4,
            "issue_id": 4371,
            "pub_date": "2025-06-18",
            "pub_date_card": {
                "ru": "18 Ğ¸ÑĞ½Ñ",
                "en": "June 18",
                "zh": "6æœˆ18æ—¥"
            },
            "hash": "0e3c39a143af668b",
            "authors": [
                "Chengye Wang",
                "Yifei Shen",
                "Zexi Kuang",
                "Arman Cohan",
                "Yilun Zhao"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2506.15569.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#reasoning",
                    "#rag",
                    "#science",
                    "#benchmark",
                    "#open_source"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "SciVer: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "SciVer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 3000 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· 1113 Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ñ‚Ğ¸Ğ¿Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 21 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ o4-mini, Gemini-2.5-Flash Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸-Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating Multimodal Models in Scientific Claim Verification",
                    "desc": "The paper introduces SciVer, a benchmark designed to assess how well multimodal foundation models can verify claims in scientific contexts. It includes 3,000 expert-annotated examples from 1,113 scientific papers, focusing on four reasoning types relevant to claim verification. The study evaluates 21 advanced multimodal models, revealing significant performance gaps compared to human experts. Additionally, it highlights limitations in current models and provides insights for improving their understanding and reasoning capabilities in scientific literature."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹çš„ç§‘å­¦éªŒè¯èƒ½åŠ›",
                    "desc": "SciVeræ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨ç§‘å­¦èƒŒæ™¯ä¸‹éªŒè¯å£°æ˜çš„èƒ½åŠ›ã€‚å®ƒåŒ…å«3000ä¸ªä¸“å®¶æ³¨é‡Šçš„ç¤ºä¾‹ï¼Œæ¶µç›–1113ç¯‡ç§‘å­¦è®ºæ–‡ï¼Œåˆ†ä¸ºå››ä¸ªå­é›†ï¼Œä»£è¡¨å¤šæ¨¡æ€ç§‘å­¦å£°æ˜éªŒè¯ä¸­çš„å¸¸è§æ¨ç†ç±»å‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†21ä¸ªæœ€å…ˆè¿›çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è¡¨ç°ï¼Œå‘ç°è¿™äº›æ¨¡å‹ä¸äººç±»ä¸“å®¶ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚é€šè¿‡å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œäººç±»é”™è¯¯è¯„ä¼°çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬è¯†åˆ«äº†å½“å‰å¼€æºæ¨¡å‹çš„å…³é”®å±€é™æ€§ï¼Œä¸ºæé«˜æ¨¡å‹åœ¨å¤šæ¨¡æ€ç§‘å­¦æ–‡çŒ®ä»»åŠ¡ä¸­çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›æä¾›äº†é‡è¦è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06279",
            "title": "CoMemo: LVLMs Need Image Context with Image Memory",
            "url": "https://huggingface.co/papers/2506.06279",
            "abstract": "CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Vision-Language Models built upon Large Language Models have established aligning visual features with LLM representations as the dominant paradigm. However, inherited LLM architectural designs introduce suboptimal characteristics for multimodal processing. First, LVLMs exhibit a bimodal distribution in attention allocation, leading to the progressive neglect of middle visual content as context expands. Second, conventional positional encoding schemes fail to preserve vital 2D structural relationships when processing dynamic high-resolution images. To address these limitations, we propose CoMemo - a dual-path architecture that combines a Context image path with an image Memory path for visual processing, effectively alleviating visual information neglect. Additionally, we introduce RoPE-DHR, a novel positional encoding mechanism that employs thumbnail-based positional aggregation to maintain 2D spatial awareness while mitigating remote decay in extended sequences. Evaluations across seven benchmarks,including long-context comprehension, multi-image reasoning, and visual question answering, demonstrate CoMemo's superior performance compared to conventional LVLM architectures. Project page is available at https://lalbj.github.io/projects/CoMemo/.",
            "score": 2,
            "issue_id": 4370,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 Ğ¸ÑĞ½Ñ",
                "en": "June 6",
                "zh": "6æœˆ6æ—¥"
            },
            "hash": "e2ad205a039e250b",
            "authors": [
                "Shi Liu",
                "Weijie Su",
                "Xizhou Zhu",
                "Wenhai Wang",
                "Jifeng Dai"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.06279.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CoMemo: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "CoMemo - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ½ĞµĞ±Ñ€ĞµĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¿ÑƒÑ‚ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. CoMemo Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ RoPE-DHR, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ°Ñ‚ÑÑ€. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ LVLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Visual Awareness in Multimodal Processing with CoMemo",
                    "desc": "CoMemo is a machine learning model designed to improve how visual information is processed alongside language. It uses a dual-path architecture that separates context and memory paths to better handle visual data, reducing the neglect of important visual details. The model also introduces a new positional encoding method called RoPE-DHR, which helps maintain the spatial relationships in images, especially in long sequences. Evaluations show that CoMemo outperforms traditional large vision-language models in various tasks, including understanding long contexts and answering visual questions."
                },
                "zh": {
                    "title": "CoMemoï¼šæå‡å¤šæ¨¡æ€å¤„ç†çš„è§†è§‰æ„è¯†",
                    "desc": "CoMemoæ˜¯ä¸€ç§æ–°å‹çš„åŒè·¯å¾„æ¶æ„ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤„ç†ä¸­çš„è§†è§‰ä¿¡æ¯å¿½è§†å’Œç©ºé—´æ„è¯†é—®é¢˜ã€‚å®ƒç»“åˆäº†ä¸Šä¸‹æ–‡å›¾åƒè·¯å¾„å’Œå›¾åƒè®°å¿†è·¯å¾„ï¼Œæœ‰æ•ˆç¼“è§£äº†è§†è§‰ä¿¡æ¯çš„å¿½è§†ç°è±¡ã€‚æ­¤å¤–ï¼ŒCoMemoå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä½ç½®ç¼–ç æœºåˆ¶RoPE-DHRï¼Œé€šè¿‡ç¼©ç•¥å›¾ä½ç½®èšåˆæ¥ä¿æŒäºŒç»´ç©ºé—´æ„è¯†ï¼Œå‡å°‘è¿œç¨‹è¡°å‡ã€‚é€šè¿‡åœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„è¯„ä¼°ï¼ŒCoMemoçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹æ¶æ„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.14435",
            "title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal\n  Models",
            "url": "https://huggingface.co/papers/2506.14435",
            "abstract": "MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.  \t\t\t\t\tAI-generated summary \t\t\t\t Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.",
            "score": 1,
            "issue_id": 4371,
            "pub_date": "2025-06-17",
            "pub_date_card": {
                "ru": "17 Ğ¸ÑĞ½Ñ",
                "en": "June 17",
                "zh": "6æœˆ17æ—¥"
            },
            "hash": "2f5b5f7f3c20ae10",
            "authors": [
                "Hongyu Wang",
                "Jiayu Xu",
                "Ruiping Wang",
                "Yan Feng",
                "Yitao Zhai",
                "Peng Pei",
                "Xunliang Cai",
                "Xilin Chen"
            ],
            "affiliations": [
                "Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences",
                "Meituan University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.14435.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#inference",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MoTE: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Mixture-of-Experts Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MoTE - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture-of-Experts Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµÑ€Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². MoTE Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰ÑƒÑ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ MoE-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ MoE-LLaVA Ğ½Ğ° 4.3% Ğ¿Ğ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "MoTE: Efficient Ternary Experts for Scalable Edge Deployment",
                    "desc": "The paper introduces MoTE, a new method that enhances Mixture-of-Experts (MoE) models by using low-precision ternary experts, which consist of parameters limited to -1, 0, and 1. This approach allows for a significant reduction in memory usage while maintaining competitive performance compared to traditional full-precision experts. MoTE is designed to be scalable and efficient, making it suitable for deployment on edge devices where memory is limited. Experimental results show that MoTE not only matches the performance of existing models but also improves accuracy when combined with post-training quantization techniques."
                },
                "zh": {
                    "title": "MoTEï¼šå†…å­˜é«˜æ•ˆçš„æ··åˆä¸“å®¶æ¨¡å‹",
                    "desc": "MoTEæ˜¯ä¸€ç§å¯æ‰©å±•ä¸”å†…å­˜é«˜æ•ˆçš„æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹è¿›æ··åˆä¸“å®¶æ¨¡å‹ï¼Œä½¿ç”¨ä½ç²¾åº¦çš„ä¸‰å…ƒä¸“å®¶æ¥æå‡æ€§èƒ½å¹¶å‡å°‘å†…å­˜å ç”¨ï¼Œé€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚ä¸ä»¥å¾€ä¸»è¦ä½¿ç”¨å…¨ç²¾åº¦ä¸“å®¶çš„ç¨€ç–ä¸Šå‡æ–¹æ³•ä¸åŒï¼ŒMoTEé€šè¿‡è®­ç»ƒæ›´å¤šä½ç²¾åº¦ä¸“å®¶æ¥å®ç°æ›´å¥½çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„å‰é¦ˆç½‘ç»œä½œä¸ºå…±äº«ä¸“å®¶ï¼Œå¹¶è®­ç»ƒå‚æ•°ä¸º{-1, 0, 1}çš„ä¸‰å…ƒè·¯ç”±ä¸“å®¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoTEåœ¨æ¨¡å‹è§„æ¨¡ä¸Šå…·æœ‰è‰¯å¥½çš„æ‰©å±•è¶‹åŠ¿ï¼Œå¹¶åœ¨å†…å­˜å—é™çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-06-18.html",
    "link_next": "2025-06-20.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "18.06",
        "en": "06/18",
        "zh": "6æœˆ18æ—¥"
    },
    "short_date_next": {
        "ru": "20.06",
        "en": "06/20",
        "zh": "6æœˆ20æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 1,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 1,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 0
    }
}