{
    "date": {
        "ru": "19 июня",
        "en": "June 19",
        "zh": "6月19日"
    },
    "time_utc": "2025-06-19 02:43",
    "weekday": 3,
    "issue_id": 4370,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.15681",
            "title": "GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models",
            "url": "https://huggingface.co/papers/2506.15681",
            "abstract": "GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models in real-world scenarios, particularly on resource-constrained devices, remains challenging due to their substantial computational demands. This has spurred interest in distilling knowledge from large VLMs into smaller, more efficient counterparts. A key challenge arises here from the diversity of VLM architectures, which are built on different LLMs and employ varying token types-differing in vocabulary size, token splits, and token index ordering. To address this challenge of limitation to a specific VLM type, we present Generation after Recalibration (GenRecal), a novel, general-purpose distillation framework for VLMs. GenRecal incorporates a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, enabling effective knowledge transfer across different types of VLMs. Through extensive experiments on multiple challenging benchmarks, we demonstrate that GenRecal significantly improves baseline performances, eventually outperforming large-scale open- and closed-source VLMs.",
            "score": 4,
            "issue_id": 4370,
            "pub_date": "2025-06-18",
            "pub_date_card": {
                "ru": "18 июня",
                "en": "June 18",
                "zh": "6月18日"
            },
            "hash": "2d531d89420a04d8",
            "authors": [
                "Byung-Kwan Lee",
                "Ryo Hachiuma",
                "Yong Man Ro",
                "Yu-Chiang Frank Wang",
                "Yueh-Hua Wu"
            ],
            "affiliations": [
                "KAIST",
                "NVIDIA",
                "National Taiwan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.15681.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#optimization",
                    "#inference",
                    "#training",
                    "#multimodal",
                    "#dataset",
                    "#architecture"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "GenRecal: Универсальная дистилляция для эффективных визуально-языковых моделей",
                    "desc": "GenRecal - это новая система дистилляции для визуально-языковых моделей (VLM), которая улучшает передачу знаний между разнородными архитектурами. Она использует Recalibrator для выравнивания и адаптации представлений признаков между различными типами VLM. Эксперименты показывают, что GenRecal значительно улучшает базовые показатели и превосходит крупномасштабные открытые и закрытые VLM. Это позволяет создавать более эффективные и компактные модели для применения в реальных сценариях с ограниченными ресурсами."
                },
                "en": {
                    "title": "Aligning Features for Efficient Vision-Language Models",
                    "desc": "GenRecal is a new framework designed to enhance the performance of vision-language models (VLMs) by aligning their feature representations across various architectures. It addresses the challenge of transferring knowledge from large, complex VLMs to smaller, more efficient models, which is crucial for deployment on devices with limited resources. The framework includes a Recalibrator that adapts features from different VLMs, allowing for effective knowledge transfer despite differences in architecture and tokenization. Experimental results show that GenRecal not only improves baseline performance but also surpasses both open-source and closed-source VLMs in various benchmarks."
                },
                "zh": {
                    "title": "GenRecal：提升视觉-语言模型性能的新框架",
                    "desc": "GenRecal是一种新颖的知识蒸馏框架，旨在通过对不同架构的特征表示进行对齐，提升视觉-语言模型的性能。该框架解决了由于不同视觉-语言模型（VLM）架构的多样性而导致的知识转移挑战。GenRecal引入了一个重校准器，能够在异构VLM之间对特征表示进行适配，从而实现有效的知识传递。通过在多个具有挑战性的基准测试上进行广泛实验，我们证明GenRecal显著提高了基线性能，最终超越了大型开源和闭源的VLM。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.15677",
            "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence",
            "url": "https://huggingface.co/papers/2506.15677",
            "abstract": "Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.  \t\t\t\t\tAI-generated summary \t\t\t\t AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.",
            "score": 1,
            "issue_id": 4370,
            "pub_date": "2025-06-18",
            "pub_date_card": {
                "ru": "18 июня",
                "en": "June 18",
                "zh": "6月18日"
            },
            "hash": "9dff3e9d64c54e88",
            "authors": [
                "Yining Hong",
                "Rui Sun",
                "Bingxuan Li",
                "Xingcheng Yao",
                "Maxine Wu",
                "Alexander Chien",
                "Da Yin",
                "Ying Nian Wu",
                "Zhecan James Wang",
                "Kai-Wei Chang"
            ],
            "affiliations": [
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.15677.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#benchmark",
                    "#open_source",
                    "#agents",
                    "#multimodal",
                    "#agi",
                    "#reasoning"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Объединение физического и цифрового миров в ИИ-агентах нового поколения",
                    "desc": "Статья представляет новую парадигму искусственного интеллекта - Embodied Web Agents, которая объединяет физическое взаимодействие и веб-масштабные рассуждения. Авторы разработали симуляционную платформу, интегрирующую реалистичные 3D-среды с функциональными веб-интерфейсами. На основе этой платформы создан набор тестовых заданий Embodied Web Agents Benchmark для оценки кросс-доменного интеллекта ИИ-систем. Результаты экспериментов показывают значительный разрыв между возможностями современных ИИ-систем и человека в задачах, требующих комбинированного физического и цифрового интеллекта."
                },
                "en": {
                    "title": "Bridging Physical and Digital Intelligence with Embodied Web Agents",
                    "desc": "The paper introduces Embodied Web Agents, a new type of AI that combines physical interaction with web-based reasoning. This integration allows the agents to perform tasks that require both physical actions and access to online information, such as cooking or navigating. The authors create a benchmark environment that includes realistic 3D settings and web interfaces to evaluate these agents' abilities. Their findings show that current AI systems still lag behind human performance, highlighting both the challenges and potential for improvement in this area."
                },
                "zh": {
                    "title": "具身网络代理：连接物理与数字智能的桥梁",
                    "desc": "本文介绍了一种新的人工智能代理模型——具身网络代理（Embodied Web Agents），它将物理交互与网络规模推理结合在一起，以评估跨领域智能。当前的AI代理通常只能在数字信息检索或物理世界交互中发挥作用，缺乏两者的整合，限制了它们解决复杂任务的能力。我们开发了具身网络代理任务环境，这是一个统一的仿真平台，能够将真实的3D环境与功能性网络接口紧密结合。通过这一平台，我们构建并发布了具身网络代理基准，涵盖了烹饪、导航、购物等多种任务，要求在物理和数字领域之间进行协调推理，以系统评估跨领域智能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.06279",
            "title": "CoMemo: LVLMs Need Image Context with Image Memory",
            "url": "https://huggingface.co/papers/2506.06279",
            "abstract": "CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Vision-Language Models built upon Large Language Models have established aligning visual features with LLM representations as the dominant paradigm. However, inherited LLM architectural designs introduce suboptimal characteristics for multimodal processing. First, LVLMs exhibit a bimodal distribution in attention allocation, leading to the progressive neglect of middle visual content as context expands. Second, conventional positional encoding schemes fail to preserve vital 2D structural relationships when processing dynamic high-resolution images. To address these limitations, we propose CoMemo - a dual-path architecture that combines a Context image path with an image Memory path for visual processing, effectively alleviating visual information neglect. Additionally, we introduce RoPE-DHR, a novel positional encoding mechanism that employs thumbnail-based positional aggregation to maintain 2D spatial awareness while mitigating remote decay in extended sequences. Evaluations across seven benchmarks,including long-context comprehension, multi-image reasoning, and visual question answering, demonstrate CoMemo's superior performance compared to conventional LVLM architectures. Project page is available at https://lalbj.github.io/projects/CoMemo/.",
            "score": 1,
            "issue_id": 4370,
            "pub_date": "2025-06-06",
            "pub_date_card": {
                "ru": "6 июня",
                "en": "June 6",
                "zh": "6月6日"
            },
            "hash": "e2ad205a039e250b",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#multimodal",
                    "#reasoning",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "CoMemo: Улучшение визуального восприятия в мультимодальных моделях",
                    "desc": "CoMemo - это новая архитектура для мультимодальной обработки, решающая проблемы пренебрежения визуальной информацией и пространственной осведомленности. Она использует двухпутевую архитектуру, сочетающую контекстный путь изображения с путем памяти изображения. CoMemo также вводит новый механизм позиционного кодирования RoPE-DHR, основанный на агрегации миниатюр. Эта модель превосходит традиционные архитектуры LVLM в задачах понимания длинного контекста, рассуждения по нескольким изображениям и визуальных вопросов-ответов."
                },
                "en": {
                    "title": "Enhancing Visual Awareness in Multimodal Processing with CoMemo",
                    "desc": "CoMemo is a machine learning model designed to improve how visual information is processed alongside language. It uses a dual-path architecture that separates context and memory paths to better handle visual data, reducing the neglect of important visual details. The model also introduces a new positional encoding method called RoPE-DHR, which helps maintain the spatial relationships in images, especially in long sequences. Evaluations show that CoMemo outperforms traditional large vision-language models in various tasks, including understanding long contexts and answering visual questions."
                },
                "zh": {
                    "title": "CoMemo：提升多模态处理的视觉意识",
                    "desc": "CoMemo是一种新型的双路径架构，旨在解决多模态处理中的视觉信息忽视和空间意识问题。它结合了上下文图像路径和图像记忆路径，有效缓解了视觉信息的忽视现象。此外，CoMemo引入了一种新颖的位置编码机制RoPE-DHR，通过缩略图位置聚合来保持二维空间意识，减少远程衰减。通过在七个基准测试中的评估，CoMemo的表现优于传统的大型视觉语言模型架构。"
                }
            }
        }
    ],
    "link_prev": "2025-06-18.html",
    "link_next": "2025-06-20.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "18.06",
        "en": "06/18",
        "zh": "6月18日"
    },
    "short_date_next": {
        "ru": "20.06",
        "en": "06/20",
        "zh": "6月20日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 0,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 1,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    }
}