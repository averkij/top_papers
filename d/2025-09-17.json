{
    "date": {
        "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 17",
        "zh": "9æœˆ17æ—¥"
    },
    "time_utc": "2025-09-17 02:15",
    "weekday": 2,
    "issue_id": 5929,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.13232",
            "title": "Single-stream Policy Optimization",
            "url": "https://huggingface.co/papers/2509.13232",
            "abstract": "Single-stream Policy Optimization (SPO) improves policy-gradient training for Large Language Models by eliminating group-based issues and providing a stable, low-variance learning signal, leading to better performance and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t We revisit policy-gradient optimization for Large Language Models (LLMs) from a single-stream perspective. Prevailing group-based methods like GRPO reduce variance with on-the-fly baselines but suffer from critical flaws: frequent degenerate groups erase learning signals, and synchronization barriers hinder scalability. We introduce Single-stream Policy Optimization (SPO), which eliminates these issues by design. SPO replaces per-group baselines with a persistent, KL-adaptive value tracker and normalizes advantages globally across the batch, providing a stable, low-variance learning signal for every sample. Being group-free, SPO enables higher throughput and scales effectively in long-horizon or tool-integrated settings where generation times vary. Furthermore, the persistent value tracker naturally enables an adaptive curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO converges more smoothly and attains higher accuracy than GRPO, while eliminating computation wasted on degenerate groups. Ablation studies confirm that SPO's gains stem from its principled approach to baseline estimation and advantage normalization, offering a more robust and efficient path for LLM reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25, +4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain in pass@k across the evaluated k values. SPO's success challenges the prevailing trend of adding incidental complexity to RL algorithms, highlighting a path where fundamental principles, not architectural workarounds, drive the next wave of progress in LLM reasoning.",
            "score": 7,
            "issue_id": 5929,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 16",
                "zh": "9æœˆ16æ—¥"
            },
            "hash": "ecea2ef463a2c2ff",
            "authors": [
                "Zhongwen Xu",
                "Zihan Ding"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13232.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#rl",
                    "#reasoning"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "SPO: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Single-stream Policy Optimization (SPO). SPO ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GRPO, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ KL-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‚Ñ€ĞµĞºĞµÑ€ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼Ñƒ Ğ±Ğ°Ñ‚Ñ‡Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Qwen3-8B Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SPO Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ»Ğ°Ğ²Ğ½ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ GRPO."
                },
                "en": {
                    "title": "Streamlining Learning for Better Language Model Performance",
                    "desc": "Single-stream Policy Optimization (SPO) enhances the training of Large Language Models (LLMs) by addressing the limitations of group-based policy-gradient methods. It eliminates issues like degenerate groups that disrupt learning signals and synchronization barriers that limit scalability. By using a persistent, KL-adaptive value tracker and normalizing advantages globally, SPO provides a stable and low-variance learning signal for each sample. This approach not only improves performance and efficiency but also supports adaptive curriculum learning through prioritized sampling, leading to significant gains in accuracy on challenging benchmarks."
                },
                "zh": {
                    "title": "å•æµç­–ç•¥ä¼˜åŒ–ï¼šæå‡å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡",
                    "desc": "å•æµç­–ç•¥ä¼˜åŒ–ï¼ˆSPOï¼‰é€šè¿‡æ¶ˆé™¤åŸºäºç»„çš„æ–¹æ³•é—®é¢˜ï¼Œæ”¹è¿›äº†å¤§è¯­è¨€æ¨¡å‹çš„ç­–ç•¥æ¢¯åº¦è®­ç»ƒã€‚ä¼ ç»Ÿçš„åŸºäºç»„çš„æ–¹æ³•è™½ç„¶å¯ä»¥é™ä½æ–¹å·®ï¼Œä½†å­˜åœ¨å­¦ä¹ ä¿¡å·ä¸¢å¤±å’ŒåŒæ­¥éšœç¢ç­‰å…³é”®ç¼ºé™·ã€‚SPOé€šè¿‡å¼•å…¥æŒä¹…çš„KLè‡ªé€‚åº”ä»·å€¼è¿½è¸ªå™¨ï¼Œæä¾›äº†ç¨³å®šã€ä½æ–¹å·®çš„å­¦ä¹ ä¿¡å·ï¼Œä»è€Œæé«˜äº†æ€§èƒ½å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPOåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿æ—¶é—´è·¨åº¦æˆ–å·¥å…·é›†æˆç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13317",
            "title": "3D Aware Region Prompted Vision Language Model",
            "url": "https://huggingface.co/papers/2509.13317",
            "abstract": "A Spatial Region 3D (SR-3D) vision-language model unifies 2D and 3D representations by enriching 2D features with 3D positional embeddings, enabling flexible region prompting and accurate spatial reasoning across frames.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Spatial Region 3D (SR-3D) aware vision-language model that connects single-view 2D images and multi-view 3D data through a shared visual token space. SR-3D supports flexible region prompting, allowing users to annotate regions with bounding boxes, segmentation masks on any frame, or directly in 3D, without the need for exhaustive multi-frame labeling. We achieve this by enriching 2D visual features with 3D positional embeddings, which allows the 3D model to draw upon strong 2D priors for more accurate spatial reasoning across frames, even when objects of interest do not co-occur within the same view. Extensive experiments on both general 2D vision language and specialized 3D spatial benchmarks demonstrate that SR-3D achieves state-of-the-art performance, underscoring its effectiveness for unifying 2D and 3D representation space on scene understanding. Moreover, we observe applicability to in-the-wild videos without sensory 3D inputs or ground-truth 3D annotations, where SR-3D accurately infers spatial relationships and metric measurements.",
            "score": 1,
            "issue_id": 5929,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 16",
                "zh": "9æœˆ16æ—¥"
            },
            "hash": "b8172aa326bbec5e",
            "authors": [
                "An-Chieh Cheng",
                "Yang Fu",
                "Yukang Chen",
                "Zhijian Liu",
                "Xiaolong Li",
                "Subhashree Radhakrishnan",
                "Song Han",
                "Yao Lu",
                "Jan Kautz",
                "Pavlo Molchanov",
                "Hongxu Yin",
                "Xiaolong Wang",
                "Sifei Liu"
            ],
            "affiliations": [
                "MIT",
                "NVIDIA",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13317.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#games",
                    "#3d",
                    "#cv",
                    "#multimodal",
                    "#reasoning"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ 2D Ğ¸ 3D Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°",
                    "desc": "SR-3D - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ 2D Ğ¸ 3D Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ° Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ğ°ĞµÑ‚ 2D Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ 3D Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ Ğ·Ğ°Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ¼ĞºĞ°Ğ¼Ğ¸, ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°ÑĞºĞ°Ğ¼Ğ¸ Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ² 3D, Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ²ÑĞµÑ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². SR-3D Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Unifying 2D and 3D for Enhanced Scene Understanding",
                    "desc": "The Spatial Region 3D (SR-3D) model integrates 2D and 3D visual data by enhancing 2D features with 3D positional information. This allows for flexible region prompting, enabling users to annotate images and 3D spaces without extensive labeling across multiple frames. By leveraging 3D embeddings, the model improves spatial reasoning, even when objects are not visible together in the same view. Experiments show that SR-3D outperforms existing methods in both 2D and 3D tasks, demonstrating its capability to understand scenes effectively, even in videos lacking 3D data."
                },
                "zh": {
                    "title": "ç»Ÿä¸€2Dä¸3Dè¡¨ç¤ºçš„ç©ºé—´åŒºåŸŸ3Dæ¨¡å‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç©ºé—´åŒºåŸŸ3Dï¼ˆSR-3Dï¼‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå®ƒé€šè¿‡å°†2Dç‰¹å¾ä¸3Dä½ç½®åµŒå…¥ç›¸ç»“åˆï¼Œå®ç°äº†2Då’Œ3Dè¡¨ç¤ºçš„ç»Ÿä¸€ã€‚SR-3Dæ”¯æŒçµæ´»çš„åŒºåŸŸæç¤ºï¼Œç”¨æˆ·å¯ä»¥åœ¨ä»»æ„å¸§ä¸Šä½¿ç”¨è¾¹ç•Œæ¡†æˆ–åˆ†å‰²æ©ç è¿›è¡Œæ ‡æ³¨ï¼Œè€Œæ— éœ€è¿›è¡Œç¹ççš„å¤šå¸§æ ‡æ³¨ã€‚é€šè¿‡å¢å¼º2Dè§†è§‰ç‰¹å¾ï¼ŒSR-3Dèƒ½å¤Ÿåœ¨ä¸åŒå¸§ä¹‹é—´è¿›è¡Œæ›´å‡†ç¡®çš„ç©ºé—´æ¨ç†ï¼Œå³ä½¿æ„Ÿå…´è¶£çš„ç‰©ä½“ä¸åœ¨åŒä¸€è§†å›¾ä¸­å‡ºç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSR-3Dåœ¨2Dè§†è§‰è¯­è¨€å’Œ3Dç©ºé—´åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶åœ¨åœºæ™¯ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13312",
            "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for\n  Open-Ended Deep Research",
            "url": "https://huggingface.co/papers/2509.13312",
            "abstract": "WebWeaver, a dual-agent framework, addresses open-ended deep research challenges by integrating adaptive planning and focused synthesis to produce high-quality, reliable reports.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper tackles open-ended deep research (OEDR), a complex challenge where AI agents must synthesize vast web-scale information into insightful reports. Current approaches are plagued by dual-fold limitations: static research pipelines that decouple planning from evidence acquisition and one-shot generation paradigms that easily suffer from long-context failure issues like \"loss in the middle\" and hallucinations. To address these challenges, we introduce WebWeaver, a novel dual-agent framework that emulates the human research process. The planner operates in a dynamic cycle, iteratively interleaving evidence acquisition with outline optimization to produce a comprehensive, source-grounded outline linking to a memory bank of evidence. The writer then executes a hierarchical retrieval and writing process, composing the report section by section. By performing targeted retrieval of only the necessary evidence from the memory bank for each part, it effectively mitigates long-context issues. Our framework establishes a new state-of-the-art across major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and DeepResearchGym. These results validate our human-centric, iterative methodology, demonstrating that adaptive planning and focused synthesis are crucial for producing high-quality, reliable, and well-structured reports.",
            "score": 1,
            "issue_id": 5929,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 16",
                "zh": "9æœˆ16æ—¥"
            },
            "hash": "7c9823f6fb958040",
            "authors": [
                "Zijian Li",
                "Xin Guan",
                "Bo Zhang",
                "Shen Huang",
                "Houquan Zhou",
                "Shaopeng Lai",
                "Ming Yan",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Jun Zhang",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13312.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#hallucinations",
                    "#long_context",
                    "#multimodal",
                    "#agents"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ WebWeaver - Ğ´Ğ²ÑƒÑ…Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ (Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº) Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½ Ñ ÑÑÑ‹Ğ»ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° Ğ±Ğ°Ğ½Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ (Ğ¿Ğ¸ÑĞ°Ñ‚ĞµĞ»ÑŒ) Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¾Ñ‚Ñ‡ĞµÑ‚ Ğ¿Ğ¾ Ñ‡Ğ°ÑÑ‚ÑĞ¼. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. WebWeaver Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "WebWeaver: Revolutionizing AI Research with Adaptive Planning and Focused Synthesis",
                    "desc": "This paper presents WebWeaver, a dual-agent framework designed to tackle open-ended deep research (OEDR) challenges in AI. It combines adaptive planning with focused synthesis to create high-quality reports by mimicking the human research process. The framework features a planner that dynamically interleaves evidence gathering with outline optimization, ensuring a comprehensive and reliable report structure. By using targeted retrieval from a memory bank, WebWeaver effectively addresses common issues like long-context failures and hallucinations, setting a new standard in OEDR performance."
                },
                "zh": {
                    "title": "WebWeaverï¼šäººæœ¬ç ”ç©¶çš„æ–°æ–¹æ³•",
                    "desc": "WebWeaveræ˜¯ä¸€ä¸ªåŒä»£ç†æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¼€æ”¾å¼æ·±åº¦ç ”ç©¶çš„æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡åŠ¨æ€è§„åˆ’å’Œèšç„¦åˆæˆï¼Œç”Ÿæˆé«˜è´¨é‡ã€å¯é çš„ç ”ç©¶æŠ¥å‘Šã€‚è¯¥æ¡†æ¶æ¨¡æ‹Ÿäººç±»ç ”ç©¶è¿‡ç¨‹ï¼Œè§„åˆ’è€…åœ¨åŠ¨æ€å¾ªç¯ä¸­ä¼˜åŒ–å¤§çº²å¹¶è·å–è¯æ®ï¼Œä½œå®¶åˆ™é€æ­¥æ’°å†™æŠ¥å‘Šã€‚WebWeaveråœ¨å¤šä¸ªå¼€æ”¾å¼æ·±åº¦ç ”ç©¶åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒéªŒè¯äº†å…¶äººæœ¬ã€è¿­ä»£çš„æ–¹æ³•è®ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13311",
            "title": "Towards General Agentic Intelligence via Environment Scaling",
            "url": "https://huggingface.co/papers/2509.13311",
            "abstract": "A scalable framework and two-phase fine-tuning strategy enhance function-calling capabilities of agents in diverse environments, improving performance on agentic benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Advanced agentic intelligence is a prerequisite for deploying Large Language Models in practical, real-world applications. Diverse real-world APIs demand precise, robust function-calling intelligence, which needs agents to develop these capabilities through interaction in varied environments. The breadth of function-calling competence is closely tied to the diversity of environments in which agents are trained. In this work, we scale up environments as a step towards advancing general agentic intelligence. This gives rise to two central challenges: (i) how to scale environments in a principled manner, and (ii) how to effectively train agentic capabilities from experiences derived through interactions with these environments. To address these, we design a scalable framework that automatically constructs heterogeneous environments that are fully simulated, systematically broadening the space of function-calling scenarios. We further adapt a two-phase agent fine-tuning strategy: first endowing agents with fundamental agentic capabilities, then specializing them for domain-specific contexts. Extensive experiments on agentic benchmarks, tau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model, AgentScaler, significantly enhances the function-calling capability of models.",
            "score": 1,
            "issue_id": 5929,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 16",
                "zh": "9æœˆ16æ—¥"
            },
            "hash": "c97e7569e926b9a7",
            "authors": [
                "Runnan Fang",
                "Shihao Cai",
                "Baixuan Li",
                "Jialong Wu",
                "Guangyu Li",
                "Wenbiao Yin",
                "Xinyu Wang",
                "Xiaobin Wang",
                "Liangcai Su",
                "Zhen Zhang",
                "Shibin Wu",
                "Zhengwei Tao",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13311.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#agi",
                    "#training",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğº Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñƒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ ÑĞ¿ĞµĞºÑ‚Ñ€ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ°Ğ´ĞµĞ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñƒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Scaling Agentic Intelligence for Enhanced Function-Calling",
                    "desc": "This paper presents a scalable framework and a two-phase fine-tuning strategy to improve the function-calling abilities of agents in various environments. The authors emphasize that training agents in diverse settings is crucial for developing robust function-calling intelligence, which is essential for real-world applications of Large Language Models. They propose a method to systematically create varied simulated environments and a training approach that first builds basic agentic skills before refining them for specific tasks. Experimental results show that their model, AgentScaler, outperforms existing benchmarks in function-calling tasks, indicating significant advancements in agentic intelligence."
                },
                "zh": {
                    "title": "æå‡æ™ºèƒ½ä½“å‡½æ•°è°ƒç”¨èƒ½åŠ›çš„å¯æ‰©å±•æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å¯æ‰©å±•çš„æ¡†æ¶å’Œä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œä»¥å¢å¼ºæ™ºèƒ½ä½“åœ¨å¤šæ ·åŒ–ç¯å¢ƒä¸­çš„å‡½æ•°è°ƒç”¨èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ™ºèƒ½ä½“çš„å‡½æ•°è°ƒç”¨èƒ½åŠ›ä¸å…¶è®­ç»ƒç¯å¢ƒçš„å¤šæ ·æ€§å¯†åˆ‡ç›¸å…³ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè‡ªåŠ¨æ„å»ºå¼‚æ„ç¯å¢ƒçš„æ¡†æ¶ï¼Œç³»ç»Ÿæ€§åœ°æ‰©å±•äº†å‡½æ•°è°ƒç”¨åœºæ™¯çš„ç©ºé—´ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä¸¤é˜¶æ®µçš„å¾®è°ƒç­–ç•¥ï¼Œæˆ‘ä»¬é¦–å…ˆèµ‹äºˆæ™ºèƒ½ä½“åŸºæœ¬èƒ½åŠ›ï¼Œç„¶åé’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œä¸“ä¸šåŒ–è®­ç»ƒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13309",
            "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon\n  Agents",
            "url": "https://huggingface.co/papers/2509.13309",
            "abstract": "WebResearcher, a deep-research framework, enhances AI agents' knowledge synthesis by reformulating research as a Markov Decision Process and using a scalable data synthesis engine, achieving superior performance across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in deep-research systems have demonstrated the potential for AI agents to autonomously discover and synthesize knowledge from external sources. In this paper, we introduce WebResearcher, a novel framework for building such agents through two key components: (1) WebResearcher, an iterative deep-research paradigm that reformulates deep research as a Markov Decision Process, where agents periodically consolidate findings into evolving reports while maintaining focused workspaces, overcoming the context suffocation and noise contamination that plague existing mono-contextual approaches; and (2) WebFrontier, a scalable data synthesis engine that generates high-quality training data through tool-augmented complexity escalation, enabling systematic creation of research tasks that bridge the gap between passive knowledge recall and active knowledge construction. Notably, we find that the training data from our paradigm significantly enhances tool-use capabilities even for traditional mono-contextual methods. Furthermore, our paradigm naturally scales through parallel thinking, enabling concurrent multi-agent exploration for more comprehensive conclusions. Extensive experiments across 6 challenging benchmarks demonstrate that WebResearcher achieves state-of-the-art performance, even surpassing frontier proprietary systems.",
            "score": 1,
            "issue_id": 5929,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 16",
                "zh": "9æœˆ16æ—¥"
            },
            "hash": "f064e62f63131809",
            "authors": [
                "Zile Qiao",
                "Guoxin Chen",
                "Xuanzhong Chen",
                "Donglei Yu",
                "Wenbiao Yin",
                "Xinyu Wang",
                "Zhen Zhang",
                "Baixuan Li",
                "Huifeng Yin",
                "Kuan Li",
                "Rui Min",
                "Minpeng Liao",
                "Yong Jiang",
                "Pengjun Xie",
                "Fei Huang",
                "Jingren Zhou"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13309.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#science",
                    "#dataset",
                    "#training",
                    "#agents",
                    "#transfer_learning"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "WebResearcher: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ˜Ğ˜",
                    "desc": "WebResearcher - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. WebResearcher Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ WebFrontier Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ WebResearcher Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ Ñ€ÑĞ´Ñƒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing AI Knowledge Synthesis with WebResearcher",
                    "desc": "WebResearcher is a deep-research framework that improves how AI agents gather and synthesize knowledge by treating research as a Markov Decision Process. It features an iterative approach where agents create evolving reports while managing focused workspaces, which helps avoid distractions from irrelevant information. Additionally, the framework includes WebFrontier, a data synthesis engine that produces high-quality training data, enhancing the agents' ability to construct knowledge actively. Experiments show that WebResearcher outperforms existing systems across multiple benchmarks, demonstrating its effectiveness in autonomous knowledge discovery."
                },
                "zh": {
                    "title": "WebResearcherï¼šæå‡AIçŸ¥è¯†ç»¼åˆçš„æ–°æ¡†æ¶",
                    "desc": "WebResearcheræ˜¯ä¸€ä¸ªæ·±åº¦ç ”ç©¶æ¡†æ¶ï¼Œé€šè¿‡å°†ç ”ç©¶é‡æ–°è¡¨è¿°ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¢å¼ºäº†äººå·¥æ™ºèƒ½ä»£ç†çš„çŸ¥è¯†ç»¼åˆèƒ½åŠ›ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šä¸€ä¸ªæ˜¯è¿­ä»£æ·±åº¦ç ”ç©¶èŒƒå¼ï¼Œèƒ½å¤Ÿå®šæœŸæ•´åˆå‘ç°å¹¶ç”Ÿæˆä¸æ–­æ¼”å˜çš„æŠ¥å‘Šï¼Œå…‹æœäº†ç°æœ‰å•ä¸€ä¸Šä¸‹æ–‡æ–¹æ³•çš„å±€é™æ€§ï¼›å¦ä¸€ä¸ªæ˜¯å¯æ‰©å±•çš„æ•°æ®åˆæˆå¼•æ“ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œä¿ƒè¿›ä¸»åŠ¨çŸ¥è¯†æ„å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWebResearcheråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³è¶…è¶Šäº†è®¸å¤šå‰æ²¿çš„ä¸“æœ‰ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.12815",
            "title": "Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset\n  Generation",
            "url": "https://huggingface.co/papers/2509.12815",
            "abstract": "Hunyuan3D Studio automates 3D asset creation using AI, integrating neural modules to transform concept images or text into high-quality, game-ready 3D models with optimized geometry and PBR textures.  \t\t\t\t\tAI-generated summary \t\t\t\t The creation of high-quality 3D assets, a cornerstone of modern game development, has long been characterized by labor-intensive and specialized workflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered content creation platform designed to revolutionize the game production pipeline by automating and streamlining the generation of game-ready 3D assets. At its core, Hunyuan3D Studio integrates a suite of advanced neural modules (such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into a cohesive and user-friendly system. This unified framework allows for the rapid transformation of a single concept image or textual description into a fully-realized, production-quality 3D model complete with optimized geometry and high-fidelity PBR textures. We demonstrate that assets generated by Hunyuan3D Studio are not only visually compelling but also adhere to the stringent technical requirements of contemporary game engines, significantly reducing iteration time and lowering the barrier to entry for 3D content creation. By providing a seamless bridge from creative intent to technical asset, Hunyuan3D Studio represents a significant leap forward for AI-assisted workflows in game development and interactive media.",
            "score": 1,
            "issue_id": 5929,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 16",
                "zh": "9æœˆ16æ—¥"
            },
            "hash": "3561c2f1ae6a316c",
            "authors": [
                "Biwen Lei",
                "Yang Li",
                "Xinhai Liu",
                "Shuhui Yang",
                "Lixin Xu",
                "Jingwei Huang",
                "Ruining Tang",
                "Haohan Weng",
                "Jian Liu",
                "Jing Xu",
                "Zhen Zhou",
                "Yiling Zhu",
                "Jiankai Xing",
                "Jiachen Xu",
                "Changfeng Ma",
                "Xinhao Yan",
                "Yunhan Yang",
                "Chunshi Wang",
                "Duoteng Xu",
                "Xueqi Ma",
                "Yuguang Chen",
                "Jing Li",
                "Mingxin Yang",
                "Sheng Zhang",
                "Yifei Feng",
                "Xin Huang",
                "Di Luo",
                "Zebin He",
                "Puhua Jiang",
                "Changrong Hu",
                "Zihan Qin",
                "Shiwei Miao",
                "Haolin Liu",
                "Yunfei Zhao",
                "Zeqiang Lai",
                "Qingxiang Lin",
                "Zibo Zhao",
                "Kunhong Li",
                "Xianghui Yang",
                "Huiwen Shi",
                "Xin Yang",
                "Yuxuan Wang",
                "Zebin Yao",
                "Yihang Lian",
                "Sicong Liu",
                "Xintong Han",
                "Wangchen Qin",
                "Caisheng Ouyang",
                "Jianyin Liu",
                "Tianwen Yuan",
                "Shuai Jiang",
                "Hong Duan",
                "Yanqi Niu",
                "Wencong Lin",
                "Yifu Sun",
                "Shirui Huang",
                "Lin Niu",
                "Gu Gong",
                "Guojian Xiao",
                "Bojian Zheng",
                "Xiang Yuan",
                "Qi Chen",
                "Jie Xiao",
                "Dongyang Zheng",
                "Xiaofeng Yang",
                "Kai Liu",
                "Jianchen Zhu",
                "Lifu Wang",
                "Qinglin Lu",
                "Jie Liu",
                "Liang Dong",
                "Fan Jiang",
                "Ruibin Chen",
                "Lei Wang",
                "Chao Zhang",
                "Jiaxin Lin",
                "Hao Zhang",
                "Zheng Ye",
                "Peng He",
                "Runzhou Wu",
                "Yinhe Wu",
                "Jiayao Du",
                "Jupeng Chen",
                "Xinyue Mao",
                "Dongyuan Guo",
                "Yixuan Tang",
                "Yulin Tsai",
                "Yonghao Tan",
                "Jiaao Yu",
                "Junlin Yu",
                "Keren Zhang",
                "Yifan Li",
                "Peng Chen",
                "Tian Liu",
                "Di Wang",
                "Yuhong Liu",
                "Linus",
                "Jie Jiang",
                "Zhuo Chen",
                "Chunchao Guo"
            ],
            "affiliations": [
                "Tencent Hunyuan Hunyuan3D Studio"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.12815.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ˜Ğ˜ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ 3D-Ğ°ÑÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ³Ñ€",
                    "desc": "Hunyuan3D Studio - ÑÑ‚Ğ¾ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-Ğ°ÑÑĞµÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ…. Hunyuan3D Studio Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ PBR-Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑÑ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ³Ñ€. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "Revolutionizing 3D Asset Creation with AI",
                    "desc": "Hunyuan3D Studio is an innovative AI-powered platform that automates the creation of high-quality 3D assets for game development. It utilizes advanced neural modules to convert concept images or text into fully-realized 3D models, complete with optimized geometry and PBR textures. This system streamlines the traditionally labor-intensive process, allowing for faster production and easier access to 3D content creation. By meeting the technical standards of modern game engines, Hunyuan3D Studio enhances the efficiency of game production workflows."
                },
                "zh": {
                    "title": "Hunyuan3D Studioï¼šæ¸¸æˆèµ„äº§åˆ›ä½œçš„AIé©å‘½",
                    "desc": "Hunyuan3D Studio æ˜¯ä¸€ä¸ªåŸºäºäººå·¥æ™ºèƒ½çš„å†…å®¹åˆ›ä½œå¹³å°ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–å’Œç®€åŒ–æ¸¸æˆåˆ¶ä½œæµç¨‹ã€‚å®ƒé€šè¿‡é›†æˆå…ˆè¿›çš„ç¥ç»æ¨¡å—ï¼Œå°†æ¦‚å¿µå›¾åƒæˆ–æ–‡æœ¬å¿«é€Ÿè½¬åŒ–ä¸ºé«˜è´¨é‡çš„3Dæ¨¡å‹ï¼Œæ¨¡å‹å…·å¤‡ä¼˜åŒ–çš„å‡ ä½•å½¢çŠ¶å’Œé«˜ä¿çœŸPBRçº¹ç†ã€‚è¯¥å¹³å°ä¸ä»…æé«˜äº†3Dèµ„äº§çš„ç”Ÿæˆæ•ˆç‡ï¼Œè¿˜ç¡®ä¿ç”Ÿæˆçš„èµ„äº§ç¬¦åˆç°ä»£æ¸¸æˆå¼•æ“çš„æŠ€æœ¯è¦æ±‚ã€‚Hunyuan3D Studio ä»£è¡¨äº†æ¸¸æˆå¼€å‘å’Œäº’åŠ¨åª’ä½“é¢†åŸŸä¸­AIè¾…åŠ©å·¥ä½œæµç¨‹çš„é‡å¤§è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.11526",
            "title": "Multiple Instance Learning Framework with Masked Hard Instance Mining\n  for Gigapixel Histopathology Image Analysis",
            "url": "https://huggingface.co/papers/2509.11526",
            "abstract": "A novel MIL framework, MHIM-MIL, uses masked hard instance mining with a Siamese structure and momentum teacher to improve cancer diagnosis and subtyping accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has opened new avenues for Computational Pathology (CPath). As positive tissue comprises only a small fraction of gigapixel WSIs, existing Multiple Instance Learning (MIL) methods typically focus on identifying salient instances via attention mechanisms. However, this leads to a bias towards easy-to-classify instances while neglecting challenging ones. Recent studies have shown that hard examples are crucial for accurately modeling discriminative boundaries. Applying such an idea at the instance level, we elaborate a novel MIL framework with masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure with a consistency constraint to explore the hard instances. Using a class-aware instance probability, MHIM-MIL employs a momentum teacher to mask salient instances and implicitly mine hard instances for training the student model. To obtain diverse, non-redundant hard instances, we adopt large-scale random masking while utilizing a global recycle network to mitigate the risk of losing key features. Furthermore, the student updates the teacher using an exponential moving average, which identifies new hard instances for subsequent training iterations and stabilizes optimization. Experimental results on cancer diagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate that MHIM-MIL outperforms the latest methods in both performance and efficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL.",
            "score": 1,
            "issue_id": 5929,
            "pub_date": "2025-09-15",
            "pub_date_card": {
                "ru": "15 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 15",
                "zh": "9æœˆ15æ—¥"
            },
            "hash": "50c5e6e28356cfc4",
            "authors": [
                "Wenhao Tang",
                "Sheng Huang",
                "Heng Fang",
                "Fengtao Zhou",
                "Bo Liu",
                "Qingshan Liu"
            ],
            "affiliations": [
                "CS, Hefei University of Technology, Hefei, China",
                "CS, Hong Kong University of Science and Technology, Hong Kong, China",
                "CS, Nanjing University of Posts and Telecommunications, Nanjing, China",
                "School of Big Data & Software Engineering, Chongqing University, Chongqing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.11526.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#science",
                    "#optimization",
                    "#training",
                    "#healthcare"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ñ€Ğ°ĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ°Ğ¼Ğ¸ (MIL) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ñ‚Ğ¸Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ĞºĞ°. ĞœĞµÑ‚Ğ¾Ğ´ MHIM-MIL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ñ ÑĞ¸Ğ°Ğ¼ÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ñ Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ¾Ğ¼. ĞĞ½ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² MIL. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ MHIM-MIL Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unlocking Cancer Insights with Hard Instance Mining",
                    "desc": "The paper introduces a new Multiple Instance Learning (MIL) framework called MHIM-MIL, which focuses on improving cancer diagnosis and subtyping accuracy. It employs masked hard instance mining using a Siamese network structure and a momentum teacher to effectively identify and utilize challenging instances in gigapixel Whole Slide Images (WSIs). By masking easier instances, the framework encourages the model to learn from harder examples, which are essential for better discriminative boundary modeling. Experimental results show that MHIM-MIL significantly outperforms existing methods in various cancer-related tasks, demonstrating its effectiveness and efficiency in computational pathology."
                },
                "zh": {
                    "title": "æ©è”½å›°éš¾å®ä¾‹æŒ–æ˜ï¼Œæå‡ç™Œç—‡è¯Šæ–­å‡†ç¡®æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šå®ä¾‹å­¦ä¹ æ¡†æ¶MHIM-MILï¼Œæ—¨åœ¨æé«˜ç™Œç—‡è¯Šæ–­å’Œäºšå‹åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†æ©è”½å›°éš¾å®ä¾‹æŒ–æ˜æŠ€æœ¯ï¼Œç»“åˆäº†Siameseç»“æ„å’ŒåŠ¨é‡æ•™å¸ˆï¼Œä»¥æ›´å¥½åœ°è¯†åˆ«éš¾ä»¥åˆ†ç±»çš„å®ä¾‹ã€‚é€šè¿‡ä½¿ç”¨ç±»æ„ŸçŸ¥å®ä¾‹æ¦‚ç‡ï¼ŒMHIM-MILèƒ½å¤Ÿæ©è”½æ˜¾è‘—å®ä¾‹å¹¶éšå¼æŒ–æ˜å›°éš¾å®ä¾‹ï¼Œä»è€Œä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMHIM-MILåœ¨ç™Œç—‡è¯Šæ–­ã€äºšå‹åˆ†ç±»å’Œç”Ÿå­˜åˆ†æç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-16.html",
    "link_next": "2025-09-18.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "16.09",
        "en": "09/16",
        "zh": "9æœˆ16æ—¥"
    },
    "short_date_next": {
        "ru": "18.09",
        "en": "09/18",
        "zh": "9æœˆ18æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 5,
        "#agents": 3,
        "#cv": 1,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 1,
        "#training": 4,
        "#robotics": 0,
        "#agi": 1,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 0
    }
}