{
    "date": {
        "ru": "7 марта",
        "en": "March 7",
        "zh": "3月7日"
    },
    "time_utc": "2025-03-07 21:08",
    "weekday": 4,
    "issue_id": 2596,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.04625",
            "title": "START: Self-taught Reasoner with Tools",
            "url": "https://huggingface.co/papers/2503.04625",
            "abstract": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview.",
            "score": 48,
            "issue_id": 2579,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "8961f69e1eda24ad",
            "authors": [
                "Chengpeng Li",
                "Mingfeng Xue",
                "Zhenru Zhang",
                "Jiaxi Yang",
                "Beichen Zhang",
                "Xiang Wang",
                "Bowen Yu",
                "Binyuan Hui",
                "Junyang Lin",
                "Dayiheng Liu"
            ],
            "affiliations": [
                "Alibaba Group",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04625.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#rl",
                    "#training",
                    "#architecture",
                    "#hallucinations",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "START: Самообучающаяся модель рассуждений с инструментами",
                    "desc": "Статья представляет START - новую модель для рассуждений с длинной цепочкой мыслей, интегрирующую внешние инструменты. START использует фреймворк самообучения, включающий технику Hint-infer для стимулирования использования инструментов, и Hint Rejection Sampling Fine-Tuning для улучшения траекторий рассуждений. Модель значительно превосходит базовую QwQ-32B и достигает результатов на уровне современных моделей в сложных задачах рассуждений. START демонстрирует высокую точность на наборах данных по науке, математике и программированию уровня PhD и соревнований."
                },
                "en": {
                    "title": "Enhancing Reasoning with External Tools: Introducing START",
                    "desc": "This paper presents START, a new long Chain-of-thought reasoning model that integrates external tools to improve reasoning capabilities. Traditional large reasoning models often struggle with hallucinations and inefficiencies, but START addresses these issues by allowing the model to perform complex computations and self-debugging. The innovation lies in two techniques: Hint-infer, which uses designed hints to encourage tool usage, and Hint Rejection Sampling Fine-Tuning, which refines the model's reasoning paths. START demonstrates superior performance on various benchmarks, surpassing its predecessor and competing effectively with state-of-the-art models."
                },
                "zh": {
                    "title": "工具整合，推理更强！",
                    "desc": "本文介绍了一种新型的长链推理模型START（自我学习推理器与工具），它通过整合外部工具来增强推理能力。START利用代码执行进行复杂计算、自我检查、探索多种方法和自我调试，从而克服了大型推理模型（LRMs）在推理过程中常见的幻觉和低效问题。核心创新在于自我学习框架，包括提示推理（Hint-infer）和提示拒绝采样微调（Hint-RFT）两种技术，前者通过插入设计的提示来激发模型使用外部工具的能力。经过微调，START在多个科学问答和数学基准测试中表现优异，准确率显著高于基础模型QwQ-32B。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04130",
            "title": "Token-Efficient Long Video Understanding for Multimodal LLMs",
            "url": "https://huggingface.co/papers/2503.04130",
            "abstract": "Recent advances in video-based multimodal large language models (Video-LLMs) have significantly improved video understanding by processing videos as sequences of image frames. However, many existing methods treat frames independently in the vision backbone, lacking explicit temporal modeling, which limits their ability to capture dynamic patterns and efficiently handle long videos. To address these limitations, we introduce STORM (Spatiotemporal TOken Reduction for Multimodal LLMs), a novel architecture incorporating a dedicated temporal encoder between the image encoder and the LLM. Our temporal encoder leverages the Mamba State Space Model to integrate temporal information into image tokens, generating enriched representations that preserve inter-frame dynamics across the entire video sequence. This enriched encoding not only enhances video reasoning capabilities but also enables effective token reduction strategies, including test-time sampling and training-based temporal and spatial pooling, substantially reducing computational demands on the LLM without sacrificing key temporal information. By integrating these techniques, our approach simultaneously reduces training and inference latency while improving performance, enabling efficient and robust video understanding over extended temporal contexts. Extensive evaluations show that STORM achieves state-of-the-art results across various long video understanding benchmarks (more than 5\\% improvement on MLVU and LongVideoBench) while reducing the computation costs by up to 8times and the decoding latency by 2.4-2.9times for the fixed numbers of input frames. Project page is available at https://research.nvidia.com/labs/lpr/storm",
            "score": 34,
            "issue_id": 2580,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "f1d1afacd41dc0c7",
            "authors": [
                "Jindong Jiang",
                "Xiuyu Li",
                "Zhijian Liu",
                "Muyang Li",
                "Guo Chen",
                "Zhiqi Li",
                "De-An Huang",
                "Guilin Liu",
                "Zhiding Yu",
                "Kurt Keutzer",
                "Sungjin Ahn",
                "Jan Kautz",
                "Hongxu Yin",
                "Yao Lu",
                "Song Han",
                "Wonmin Byeon"
            ],
            "affiliations": [
                "KAIST",
                "MIT",
                "NVIDIA",
                "Nanjing University",
                "Rutgers University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04130.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#optimization",
                    "#long_context",
                    "#architecture",
                    "#video",
                    "#inference",
                    "#multimodal"
                ],
                "emoji": "🌪️",
                "ru": {
                    "title": "STORM: Эффективное понимание длинных видео с помощью темпорального кодирования",
                    "desc": "STORM - это новая архитектура для видео-LLM, которая использует специальный темпоральный энкодер на основе модели Mamba State Space между энкодером изображений и LLM. Эта архитектура позволяет интегрировать временную информацию в токены изображений, создавая обогащенные представления, сохраняющие динамику между кадрами во всей видеопоследовательности. STORM применяет стратегии сокращения токенов, включая выборку во время тестирования и пулинг во время обучения, что значительно снижает вычислительные затраты LLM. Результаты экспериментов показывают, что STORM достигает улучшения более чем на 5% на бенчмарках длинных видео, одновременно сокращая вычислительные затраты до 8 раз."
                },
                "en": {
                    "title": "STORM: Revolutionizing Video Understanding with Temporal Insights",
                    "desc": "This paper presents STORM, a new architecture designed to enhance video understanding in multimodal large language models (LLMs) by incorporating a temporal encoder. Unlike previous methods that treat video frames independently, STORM uses the Mamba State Space Model to capture the dynamics between frames, resulting in richer representations of video content. The architecture also implements token reduction strategies that significantly lower computational costs and improve processing speed without losing important temporal information. Evaluations demonstrate that STORM outperforms existing models on long video benchmarks while achieving substantial reductions in computation and latency."
                },
                "zh": {
                    "title": "高效视频理解的新突破：STORM模型",
                    "desc": "最近，基于视频的多模态大语言模型（Video-LLMs）在视频理解方面取得了显著进展，但许多现有方法在视觉骨干中独立处理帧，缺乏明确的时间建模，限制了捕捉动态模式的能力。为了解决这些问题，我们提出了STORM（时空令牌减少模型），它在图像编码器和大语言模型之间引入了专门的时间编码器。我们的时间编码器利用Mamba状态空间模型，将时间信息整合到图像令牌中，生成丰富的表示，保留整个视频序列中的帧间动态。通过这些技术的整合，我们的方法在提高性能的同时，显著减少了计算需求和推理延迟，实现了对长视频的高效理解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04724",
            "title": "LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM",
            "url": "https://huggingface.co/papers/2503.04724",
            "abstract": "Recent advancements in speech-to-speech dialogue systems leverage LLMs for multimodal interactions, yet they remain hindered by fine-tuning requirements, high computational overhead, and text-speech misalignment. Existing speech-enabled LLMs often degrade conversational quality by modifying the LLM, thereby compromising its linguistic capabilities. In contrast, we propose LLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS system that generates high-quality speech with low latency, while fully preserving the capabilities of the base LLM. Our approach achieves a significantly lower Word Error Rate compared to speech-enabled LLMs, while operating at comparable latency and UTMOS score. By decoupling speech synthesis from LLM processing via a multi-queue token streaming system, LLMVoX supports seamless, infinite-length dialogues. Its plug-and-play design also facilitates extension to various tasks with different backbones. Furthermore, LLMVoX generalizes to new languages with only dataset adaptation, attaining a low Character Error Rate on an Arabic speech task. Additionally, we have integrated LLMVoX with a Vision-Language Model to create an omni-model with speech, text, and vision capabilities, without requiring additional multimodal training. Our code base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .",
            "score": 32,
            "issue_id": 2589,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "3113e03ae6f4ed42",
            "authors": [
                "Sambal Shikhar",
                "Mohammed Irfan Kurpath",
                "Sahal Shaji Mullappilly",
                "Jean Lahoud",
                "Fahad Khan",
                "Rao Muhammad Anwer",
                "Salman Khan",
                "Hisham Cholakkal"
            ],
            "affiliations": [
                "Linköping University, Sweden",
                "Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), UAE"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04724.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#small_models",
                    "#low_resource",
                    "#audio",
                    "#open_source",
                    "#long_context"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "LLMVoX: Универсальный синтез речи для языковых моделей",
                    "desc": "LLMVoX - это легковесная система синтеза речи, которая работает совместно с языковыми моделями, не требуя их модификации. Она обеспечивает высококачественный синтез речи с низкой задержкой, сохраняя все возможности базовой языковой модели. LLMVoX достигает более низкого показателя ошибок распознавания слов по сравнению с речевыми языковыми моделями, при этом работая с сопоставимой задержкой. Система может быть легко адаптирована для различных задач и языков, а также интегрирована с мультимодальными моделями."
                },
                "en": {
                    "title": "Seamless Speech Synthesis with LLMVoX",
                    "desc": "The paper introduces LLMVoX, a novel speech-to-speech dialogue system that utilizes a lightweight, 30M-parameter architecture to enhance multimodal interactions without compromising the linguistic capabilities of large language models (LLMs). Unlike existing systems that require extensive fine-tuning and often degrade conversational quality, LLMVoX operates efficiently with low latency and a significantly reduced Word Error Rate. It employs a multi-queue token streaming mechanism to decouple speech synthesis from LLM processing, enabling seamless dialogues of infinite length. Additionally, LLMVoX can adapt to new languages with minimal dataset adjustments and integrates with Vision-Language Models to support speech, text, and vision functionalities without extra multimodal training."
                },
                "zh": {
                    "title": "LLMVoX：高效语音合成的新选择",
                    "desc": "本文提出了一种名为LLMVoX的轻量级语音合成系统，具有3000万参数，能够与任何大型语言模型（LLM）兼容。LLMVoX通过多队列令牌流系统，将语音合成与LLM处理解耦，从而实现低延迟和高质量的语音生成。与现有的语音增强LLM相比，LLMVoX在词错误率上显著降低，同时保持相似的延迟和用户满意度评分。该系统还支持无缝的无限长度对话，并能够通过数据集适应轻松扩展到新语言。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03803",
            "title": "EgoLife: Towards Egocentric Life Assistant",
            "url": "https://huggingface.co/papers/2503.03803",
            "abstract": "We introduce EgoLife, a project to develop an egocentric life assistant that accompanies and enhances personal efficiency through AI-powered wearable glasses. To lay the foundation for this assistant, we conducted a comprehensive data collection study where six participants lived together for one week, continuously recording their daily activities - including discussions, shopping, cooking, socializing, and entertainment - using AI glasses for multimodal egocentric video capture, along with synchronized third-person-view video references. This effort resulted in the EgoLife Dataset, a comprehensive 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset with intensive annotation. Leveraging this dataset, we introduce EgoLifeQA, a suite of long-context, life-oriented question-answering tasks designed to provide meaningful assistance in daily life by addressing practical questions such as recalling past relevant events, monitoring health habits, and offering personalized recommendations. To address the key technical challenges of (1) developing robust visual-audio models for egocentric data, (2) enabling identity recognition, and (3) facilitating long-context question answering over extensive temporal information, we introduce EgoButler, an integrated system comprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on egocentric datasets, achieving state-of-the-art performance on egocentric video understanding. EgoRAG is a retrieval-based component that supports answering ultra-long-context questions. Our experimental studies verify their working mechanisms and reveal critical factors and bottlenecks, guiding future improvements. By releasing our datasets, models, and benchmarks, we aim to stimulate further research in egocentric AI assistants.",
            "score": 21,
            "issue_id": 2581,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 марта",
                "en": "March 5",
                "zh": "3月5日"
            },
            "hash": "52396234365a3fb0",
            "authors": [
                "Jingkang Yang",
                "Shuai Liu",
                "Hongming Guo",
                "Yuhao Dong",
                "Xiamengwei Zhang",
                "Sicheng Zhang",
                "Pengyun Wang",
                "Zitang Zhou",
                "Binzhu Xie",
                "Ziyue Wang",
                "Bei Ouyang",
                "Zhengyu Lin",
                "Marco Cominelli",
                "Zhongang Cai",
                "Yuanhan Zhang",
                "Peiyuan Zhang",
                "Fangzhou Hong",
                "Joerg Widmer",
                "Francesco Gringoli",
                "Lei Yang",
                "Bo Li",
                "Ziwei Liu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2503.03803.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#long_context",
                    "#dataset",
                    "#open_source",
                    "#agents",
                    "#multimodal",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "👓",
                "ru": {
                    "title": "EgoLife: ИИ-ассистент для повседневной жизни на базе умных очков",
                    "desc": "Проект EgoLife представляет собой разработку эгоцентричного ассистента на основе очков дополненной реальности с искусственным интеллектом. Исследователи собрали обширный набор данных EgoLife Dataset, включающий 300 часов эгоцентричного видео повседневной жизни шести участников. На основе этих данных создан набор задач EgoLifeQA для тестирования вопросно-ответных систем в контексте повседневной жизни. Для решения технических задач разработана система EgoButler, включающая мультимодальную модель EgoGPT и компонент для ответов на вопросы с длинным контекстом EgoRAG."
                },
                "en": {
                    "title": "Empowering Daily Life with Egocentric AI Assistance",
                    "desc": "The paper presents EgoLife, an AI-powered life assistant designed to enhance personal efficiency through wearable glasses that capture egocentric video data. A comprehensive dataset, the EgoLife Dataset, was created by recording daily activities of participants over a week, resulting in 300 hours of multimodal data with detailed annotations. The study introduces EgoLifeQA, a set of question-answering tasks that utilize this dataset to assist users with practical life questions and personalized recommendations. To tackle challenges in visual-audio model development and long-context question answering, the authors propose EgoButler, which includes EgoGPT for video understanding and EgoRAG for retrieval-based answering."
                },
                "zh": {
                    "title": "智能生活助手，提升个人效率",
                    "desc": "我们介绍了EgoLife项目，旨在开发一个以自我为中心的生活助手，通过AI驱动的可穿戴眼镜提升个人效率。我们进行了全面的数据收集研究，六名参与者共同生活一周，使用AI眼镜记录日常活动，形成了EgoLife数据集，包含300小时的多视角、多模态日常生活数据。基于该数据集，我们推出了EgoLifeQA，一个针对生活的长文本问答任务，旨在提供实用的日常生活帮助。为了解决关键技术挑战，我们引入了EgoButler系统，包括EgoGPT和EgoRAG，前者在自我中心视频理解上表现出色，后者支持超长文本问题的回答。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.02972",
            "title": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation",
            "url": "https://huggingface.co/papers/2503.02972",
            "abstract": "Effective evaluation of the reasoning capabilities of large language models (LLMs) are susceptible to overestimation due to data exposure of evaluation benchmarks. We introduce a framework for producing linguistic reasoning problems that reduces the effect of memorisation in model performance estimates and apply this framework to develop LINGOLY-TOO, a challenging evaluation benchmark for linguistic reasoning. By developing orthographic templates, we dynamically obfuscate the writing systems of real languages to generate numerous question variations. These variations preserve the reasoning steps required for each solution while reducing the likelihood of specific problem instances appearing in model training data. Our experiments demonstrate that frontier models, including OpenAI o1-preview and DeepSeem R1, struggle with advanced reasoning. Our analysis also shows that LLMs exhibit noticeable variance in accuracy across permutations of the same problem, and on average perform better on questions appearing in their original orthography. Our findings highlight the opaque nature of response generation in LLMs and provide evidence that prior data exposure contributes to overestimating the reasoning capabilities of frontier models.",
            "score": 19,
            "issue_id": 2585,
            "pub_date": "2025-03-04",
            "pub_date_card": {
                "ru": "4 марта",
                "en": "March 4",
                "zh": "3月4日"
            },
            "hash": "0eb195e2704f3d5d",
            "authors": [
                "Jude Khouja",
                "Karolina Korgul",
                "Simi Hellsten",
                "Lingyi Yang",
                "Vlad Neacs",
                "Harry Mayne",
                "Ryan Kearns",
                "Andrew Bean",
                "Adam Mahdi"
            ],
            "affiliations": [
                "Asia-Pacific Linguistics Olympiad",
                "Hong Kong Linguistics Olympiad",
                "National University of Science and Technology POLITEHNICA Bucharest, Romania",
                "United Kingdom Linguistics Olympiad",
                "University of Glasgow, Glasgow, United Kingdom",
                "University of Oxford, Oxford, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.02972.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#hallucinations",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Новый метод оценки рассуждений LLM без влияния предварительных знаний",
                    "desc": "Статья представляет новый подход к оценке способностей больших языковых моделей (LLM) к рассуждению. Авторы разработали фреймворк LINGOLY-TOO, который генерирует лингвистические задачи с обфускацией систем письма реальных языков. Эксперименты показали, что современные модели, включая OpenAI и DeepSeem, испытывают трудности с продвинутыми рассуждениями. Исследование выявило, что предварительное знакомство с данными может приводить к переоценке возможностей LLM в области рассуждений."
                },
                "en": {
                    "title": "Unmasking Reasoning: Evaluating LLMs Beyond Memorization",
                    "desc": "This paper addresses the challenge of accurately evaluating the reasoning abilities of large language models (LLMs) by introducing a new framework that minimizes the impact of memorization on performance assessments. The authors present LINGOLY-TOO, a benchmark designed to test linguistic reasoning through dynamically generated questions that obfuscate the original writing systems of languages. By using orthographic templates, they create multiple variations of questions that maintain the necessary reasoning steps while reducing the chance of models having seen specific instances during training. The results indicate that leading models struggle with complex reasoning tasks and show significant performance differences based on the question format, revealing the influence of prior data exposure on their evaluation."
                },
                "zh": {
                    "title": "揭示大型语言模型推理能力的真实面貌",
                    "desc": "本文提出了一种评估大型语言模型（LLMs）推理能力的新框架，旨在减少由于数据暴露导致的评估过高的问题。我们开发了LINGOLY-TOO，这是一个具有挑战性的语言推理评估基准，通过使用正字法模板动态模糊真实语言的书写系统，生成多种问题变体。实验结果表明，前沿模型在高级推理任务中表现不佳，并且在相同问题的不同排列中，LLMs的准确性存在显著差异。我们的研究揭示了LLMs响应生成的复杂性，并提供了证据表明，先前的数据暴露会导致对前沿模型推理能力的高估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20258",
            "title": "LLM as a Broken Telephone: Iterative Generation Distorts Information",
            "url": "https://huggingface.co/papers/2502.20258",
            "abstract": "As large language models are increasingly responsible for online content, concerns arise about the impact of repeatedly processing their own outputs. Inspired by the \"broken telephone\" effect in chained human communication, this study investigates whether LLMs similarly distort information through iterative generation. Through translation-based experiments, we find that distortion accumulates over time, influenced by language choice and chain complexity. While degradation is inevitable, it can be mitigated through strategic prompting techniques. These findings contribute to discussions on the long-term effects of AI-mediated information propagation, raising important questions about the reliability of LLM-generated content in iterative workflows.",
            "score": 15,
            "issue_id": 2580,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 февраля",
                "en": "February 27",
                "zh": "2月27日"
            },
            "hash": "5b26055854ae3d90",
            "authors": [
                "Amr Mohamed",
                "Mingmeng Geng",
                "Michalis Vazirgiannis",
                "Guokan Shang"
            ],
            "affiliations": [
                "Ecole Polytechnique",
                "MBZUAI",
                "SISSA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20258.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#data",
                    "#long_context",
                    "#alignment",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "📞",
                "ru": {
                    "title": "Эффект 'испорченного телефона' в языковых моделях",
                    "desc": "Исследование посвящено изучению эффекта искажения информации при многократной обработке собственных выходных данных большими языковыми моделями (LLM). Авторы провели эксперименты на основе переводов, чтобы оценить накопление искажений в зависимости от выбора языка и сложности цепочки обработки. Результаты показывают, что деградация информации неизбежна, но может быть смягчена с помощью стратегических методов формулировки запросов. Исследование поднимает важные вопросы о надежности контента, генерируемого LLM в итеративных рабочих процессах."
                },
                "en": {
                    "title": "Mitigating Distortion in Iterative LLM Outputs",
                    "desc": "This paper explores how large language models (LLMs) can distort information when they repeatedly process their own outputs, similar to the 'broken telephone' effect seen in human communication. The researchers conducted translation-based experiments to observe how information degradation occurs over time, which is affected by the choice of language and the complexity of the generation chain. They found that while some level of distortion is unavoidable, it can be reduced by using specific prompting strategies. These insights highlight the potential risks of relying on LLMs for generating content in iterative processes, raising concerns about the accuracy of AI-generated information."
                },
                "zh": {
                    "title": "探讨大型语言模型的信息扭曲与可靠性",
                    "desc": "本研究探讨了大型语言模型（LLM）在反复处理自身输出时是否会扭曲信息，类似于人类沟通中的“破电话”效应。通过基于翻译的实验，我们发现信息扭曲会随着时间的推移而累积，受语言选择和链条复杂性的影响。尽管信息退化是不可避免的，但通过战略性提示技术可以减轻这种影响。研究结果为AI介导的信息传播的长期影响提供了重要见解，提出了关于LLM生成内容在迭代工作流程中可靠性的重要问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04598",
            "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization",
            "url": "https://huggingface.co/papers/2503.04598",
            "abstract": "Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the location of layer normalization. While Pre-Norm structures facilitate easier training due to their more prominent identity path, they often yield suboptimal performance compared to Post-Norm. In this paper, we propose HybridNorm, a straightforward yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. This design not only stabilizes training but also enhances performance, particularly in the context of LLMs. Comprehensive experiments in both dense and sparse architectures show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches, achieving state-of-the-art results across various benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. %Code will be made publicly available. Code is available at https://github.com/BryceZhuo/HybridNorm.",
            "score": 12,
            "issue_id": 2579,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "78b05ed4e27a874b",
            "authors": [
                "Zhijian Zhuo",
                "Yutao Zeng",
                "Ya Wang",
                "Sijun Zhang",
                "Jian Yang",
                "Xiaoqing Li",
                "Xun Zhou",
                "Jinwen Ma"
            ],
            "affiliations": [
                "Capital University of Economics and Business",
                "School of Mathematical Sciences, Peking University",
                "SeedFoundation-Model, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04598.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "HybridNorm: Гибридная нормализация для улучшения обучения и производительности трансформеров",
                    "desc": "Статья представляет новый метод нормализации для трансформеров под названием HybridNorm. Этот подход сочетает преимущества Pre-Norm и Post-Norm стратегий, применяя QKV нормализацию в механизме внимания и Post-Norm в FFN каждого блока трансформера. HybridNorm показывает лучшие результаты по сравнению с существующими методами как для плотных, так и для разреженных архитектур. Эксперименты демонстрируют, что HybridNorm обеспечивает более стабильное обучение и улучшенную производительность для глубоких моделей трансформеров, особенно в контексте больших языковых моделей."
                },
                "en": {
                    "title": "HybridNorm: The Best of Both Normalization Worlds for Transformers",
                    "desc": "This paper introduces HybridNorm, a new normalization strategy for training deep transformer networks, which combines the strengths of Pre-Norm and Post-Norm methods. Pre-Norm structures help with training stability but often lead to lower performance, while Post-Norm structures typically perform better but can complicate training. HybridNorm applies QKV normalization in the attention mechanism and Post-Norm in the feed-forward network, resulting in improved training stability and performance. Experiments demonstrate that HybridNorm outperforms both Pre-Norm and Post-Norm across various benchmarks, making it a promising approach for enhancing large language models."
                },
                "zh": {
                    "title": "HybridNorm：提升变换器模型训练的稳定性与性能",
                    "desc": "本文提出了一种新的混合归一化策略，称为HybridNorm，旨在解决深度变换器网络训练中的层归一化位置问题。HybridNorm结合了Pre-Norm和Post-Norm的优点，在注意力机制中使用QKV归一化，而在前馈网络中使用Post-Norm。实验结果表明，HybridNorm在稠密和稀疏架构中均优于传统的Pre-Norm和Post-Norm方法，提升了大语言模型的训练稳定性和性能。该研究为深度变换器模型的训练和性能改进提供了新的思路。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04644",
            "title": "IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval",
            "url": "https://huggingface.co/papers/2503.04644",
            "abstract": "We introduce IFIR, the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature. Each subset addresses one or more domain-specific retrieval tasks, replicating real-world scenarios where customized instructions are critical. IFIR enables a detailed analysis of instruction-following retrieval capabilities by incorporating instructions at different levels of complexity. We also propose a novel LLM-based evaluation method to provide a more precise and reliable assessment of model performance in following instructions. Through extensive experiments on 15 frontier retrieval models, including those based on LLMs, our results reveal that current models face significant challenges in effectively following complex, domain-specific instructions. We further provide in-depth analyses to highlight these limitations, offering valuable insights to guide future advancements in retriever development.",
            "score": 10,
            "issue_id": 2585,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "c4f19dc17abc0e8f",
            "authors": [
                "Tingyu Song",
                "Guo Gan",
                "Mingsheng Shang",
                "Yilun Zhao"
            ],
            "affiliations": [
                "Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences",
                "School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences",
                "Yale University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04644.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#healthcare",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "IFIR: Новый стандарт оценки интеллектуального поиска информации",
                    "desc": "IFIR - это новый комплексный бенчмарк для оценки информационного поиска с выполнением инструкций в экспертных областях. Он включает 2426 примеров высокого качества в 8 подмножествах из 4 специализированных доменов: финансы, право, здравоохранение и научная литература. IFIR позволяет проводить детальный анализ возможностей поиска с выполнением инструкций разной сложности. Эксперименты на 15 передовых моделях поиска показали значительные трудности в эффективном выполнении сложных доменно-специфичных инструкций."
                },
                "en": {
                    "title": "IFIR: Benchmarking Instruction-Following in Expert Domains",
                    "desc": "The paper presents IFIR, a new benchmark for assessing how well information retrieval systems can follow instructions in specialized fields like finance, law, healthcare, and science. It consists of 2,426 examples that simulate real-world retrieval tasks requiring tailored instructions. The benchmark allows for a nuanced evaluation of retrieval models, particularly focusing on their ability to handle varying complexities of instructions. The authors also introduce a new evaluation method using large language models (LLMs) to measure performance, revealing that existing models struggle with complex, domain-specific tasks and providing insights for future improvements."
                },
                "zh": {
                    "title": "IFIR：专家领域指令跟随检索的首个基准",
                    "desc": "我们介绍了IFIR，这是第一个全面的基准，用于评估专家领域中的指令跟随信息检索（IR）。IFIR包含2426个高质量示例，涵盖金融、法律、医疗和科学文献等四个专业领域的八个子集。每个子集针对一个或多个特定领域的检索任务，模拟了需要定制指令的真实场景。通过对15个前沿检索模型的广泛实验，我们的结果显示，当前模型在有效跟随复杂的领域特定指令方面面临重大挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04222",
            "title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion",
            "url": "https://huggingface.co/papers/2503.04222",
            "abstract": "We introduce FuseChat-3.0, a suite of large language models (LLMs) developed by integrating the strengths of heterogeneous source LLMs into more compact target LLMs. Our source models include the powerful Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct. For target models, we focus on three widely-used smaller variants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along with two ultra-compact options, Llama-3.2-3B-Instruct and Llama-3.2-1B-Instruct. To leverage the diverse capabilities of these source models, we develop a specialized data construction protocol tailored to various tasks and domains. The FuseChat-3.0 training pipeline consists of two key stages: (1) supervised fine-tuning (SFT) to align the target and source model distributions, and (2) Direct Preference Optimization (DPO) to apply preferences from multiple source LLMs to fine-tune the target model. The resulting FuseChat-3.0 models exhibit significant performance gains across tasks such as instruction following, general knowledge, mathematics, and coding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target model, our fusion approach achieves an average improvement of 6.8 points across 14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and 30.1 points on the instruction-following benchmarks AlpacaEval-2 and Arena-Hard, respectively. Our code, models, and datasets are available at https://github.com/SLIT-AI/FuseChat-3.0.",
            "score": 9,
            "issue_id": 2578,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "c7d793a0b91efd5d",
            "authors": [
                "Ziyi Yang",
                "Fanqi Wan",
                "Longguang Zhong",
                "Canbin Huang",
                "Guosheng Liang",
                "Xiaojun Quan"
            ],
            "affiliations": [
                "School of Computer Science and Engineering, Sun Yat-sen University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04222.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#small_models",
                    "#rlhf",
                    "#transfer_learning",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Слияние мощи больших языковых моделей в компактном формате",
                    "desc": "FuseChat-3.0 представляет собой набор больших языковых моделей (LLM), разработанных путем интеграции сильных сторон гетерогенных исходных LLM в более компактные целевые модели. Процесс обучения включает два ключевых этапа: контролируемая тонкая настройка и оптимизация прямых предпочтений. Результирующие модели FuseChat-3.0 демонстрируют значительный прирост производительности в различных задачах, включая следование инструкциям, общие знания, математику и программирование. Используя Llama-3.1-8B-Instruct в качестве целевой модели, подход авторов достигает среднего улучшения на 6,8 пунктов по 14 бенчмаркам."
                },
                "en": {
                    "title": "Fusing Strengths for Smarter, Smaller Models",
                    "desc": "FuseChat-3.0 is a collection of large language models (LLMs) that combines the strengths of various larger source models into smaller, more efficient target models. The training process involves supervised fine-tuning to align the target models with the source models, followed by Direct Preference Optimization to enhance performance based on preferences from multiple sources. This innovative approach leads to significant improvements in tasks like instruction following, general knowledge, mathematics, and coding. The results show an average performance boost of 6.8 points across 14 benchmarks, with particularly impressive gains in instruction-following tasks."
                },
                "zh": {
                    "title": "融合多源模型，提升语言理解能力",
                    "desc": "我们介绍了FuseChat-3.0，这是一个通过整合不同来源的大型语言模型（LLMs）优势而开发的更紧凑的目标LLM套件。源模型包括强大的Gemma-2-27B-it、Mistral-Large-Instruct-2407、Qwen-2.5-72B-Instruct和Llama-3.1-70B-Instruct。目标模型则集中在三种广泛使用的小型变体上，以及两个超紧凑选项。通过专门的数据构建协议和两阶段的训练流程，FuseChat-3.0在多个任务上表现出显著的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04094",
            "title": "PokéChamp: an Expert-level Minimax Language Agent",
            "url": "https://huggingface.co/papers/2503.04094",
            "abstract": "We introduce Pok\\'eChamp, a minimax agent powered by Large Language Models (LLMs) for Pok\\'emon battles. Built on a general framework for two-player competitive games, Pok\\'eChamp leverages the generalist capabilities of LLMs to enhance minimax tree search. Specifically, LLMs replace three key modules: (1) player action sampling, (2) opponent modeling, and (3) value function estimation, enabling the agent to effectively utilize gameplay history and human knowledge to reduce the search space and address partial observability. Notably, our framework requires no additional LLM training. We evaluate Pok\\'eChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves a win rate of 76% against the best existing LLM-based bot and 84% against the strongest rule-based bot, demonstrating its superior performance. Even with an open-source 8-billion-parameter Llama 3.1 model, Pok\\'eChamp consistently outperforms the previous best LLM-based bot, Pok\\'ellmon powered by GPT-4o, with a 64% win rate. Pok\\'eChamp attains a projected Elo of 1300-1500 on the Pok\\'emon Showdown online ladder, placing it among the top 30%-10% of human players. In addition, this work compiles the largest real-player Pok\\'emon battle dataset, featuring over 3 million games, including more than 500k high-Elo matches. Based on this dataset, we establish a series of battle benchmarks and puzzles to evaluate specific battling skills. We further provide key updates to the local game engine. We hope this work fosters further research that leverage Pok\\'emon battle as benchmark to integrate LLM technologies with game-theoretic algorithms addressing general multiagent problems. Videos, code, and dataset available at https://sites.google.com/view/pokechamp-llm.",
            "score": 8,
            "issue_id": 2580,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "bffe348d8443d4c2",
            "authors": [
                "Seth Karten",
                "Andy Luu Nguyen",
                "Chi Jin"
            ],
            "affiliations": [
                "Department of Computer Science, Princeton University",
                "Department of Electrical and Computer Engineering, Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04094.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#agents",
                    "#games"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "PokeChamp: Революция в ИИ для игровых стратегий с использованием языковых моделей",
                    "desc": "Статья представляет PokeChamp - агента на основе минимакса, использующего большие языковые модели (LLM) для боев в Pokémon. PokeChamp применяет LLM для выборки действий игрока, моделирования оппонента и оценки функции ценности, что позволяет эффективно использовать историю игры и знания человека. При использовании GPT-4o агент достигает высокого процента побед против существующих ботов и попадает в топ-30% - 10% игроков на онлайн-платформе Pokémon Showdown. Исследование также включает создание крупнейшего датасета боев Pokémon и серии тестов для оценки навыков ведения боя."
                },
                "en": {
                    "title": "Pok'eChamp: Elevating Pokémon Battles with LLMs",
                    "desc": "Pok'eChamp is a minimax agent designed for Pokémon battles that utilizes Large Language Models (LLMs) to improve its decision-making process. It replaces traditional components of the minimax algorithm with LLMs for player action sampling, opponent modeling, and value function estimation, allowing it to better understand gameplay history and human strategies. The agent demonstrates impressive performance, achieving a 76% win rate against top LLM-based bots and a 64% win rate with a smaller model, showcasing its effectiveness in competitive play. Additionally, Pok'eChamp compiles a large dataset of over 3 million Pokémon battles to create benchmarks for evaluating battling skills and encourages further research in integrating LLMs with game-theoretic approaches."
                },
                "zh": {
                    "title": "PokéChamp：宝可梦对战中的智能代理",
                    "desc": "本文介绍了PokéChamp，一个基于大型语言模型（LLMs）的极小极大算法代理，用于宝可梦对战。该代理利用LLMs的通用能力，增强了极小极大树搜索，替代了玩家动作采样、对手建模和价值函数估计等关键模块。PokéChamp在不需要额外训练的情况下，能够有效利用游戏历史和人类知识，减少搜索空间并解决部分可观测性问题。经过评估，PokéChamp在宝可梦对战中表现优异，赢得了76%的胜率，展示了其在多智能体问题中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04725",
            "title": "L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling",
            "url": "https://huggingface.co/papers/2503.04725",
            "abstract": "We rigorously establish a bipartite mutual information scaling law in natural language that governs long-range dependencies. This scaling law, which we show is distinct from and scales independently of the conventional two-point mutual information, is the key to understanding long-context language modeling. Using this scaling law, we formulate the Long-context Language Modeling (L^2M) condition, which relates a model's capacity for effective long context length modeling to the scaling of its latent state size for storing past information. Our results are validated through experiments on both transformers and state space models. This work establishes a theoretical foundation that guides the development of large language models toward longer context lengths.",
            "score": 7,
            "issue_id": 2582,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "51e8f1332666da31",
            "authors": [
                "Zhuo Chen",
                "Oriol Mayné i Comas",
                "Zhuotao Jin",
                "Di Luo",
                "Marin Soljačić"
            ],
            "affiliations": [
                "Harvard University",
                "Massachusetts Institute of Technology",
                "NSF AI Institute for Artificial Intelligence and Fundamental Interactions",
                "Polytechnic University of Catalonia",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04725.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#math",
                    "#long_context"
                ],
                "emoji": "📏",
                "ru": {
                    "title": "Масштабирование взаимной информации - ключ к моделированию длинного контекста",
                    "desc": "Статья представляет собой исследование в области обработки естественного языка, фокусирующееся на закономерностях взаимной информации в длинных текстах. Авторы формулируют условие L^2M для моделирования длинного контекста, связывающее способность модели обрабатывать длинные последовательности с масштабированием ее скрытого состояния. Результаты подтверждены экспериментами на трансформерах и моделях пространства состояний. Работа закладывает теоретическую основу для развития больших языковых моделей с увеличенной длиной контекста."
                },
                "en": {
                    "title": "Unlocking Long-Range Dependencies in Language Models",
                    "desc": "This paper introduces a new scaling law for bipartite mutual information that is crucial for understanding long-range dependencies in natural language processing. Unlike traditional two-point mutual information, this scaling law operates independently and is essential for effective long-context language modeling. The authors propose the Long-context Language Modeling (L^2M) condition, which connects a model's ability to handle long contexts with the size of its latent state for retaining past information. Experimental validation on transformers and state space models supports the theoretical framework, paving the way for advancements in large language models with extended context lengths."
                },
                "zh": {
                    "title": "长上下文建模的新法则",
                    "desc": "本文严谨地建立了自然语言中的双向互信息缩放法则，该法则控制着长距离依赖关系。我们展示了这一缩放法则与传统的两点互信息不同，并且独立缩放，是理解长上下文语言建模的关键。通过这一缩放法则，我们提出了长上下文语言建模（L^2M）条件，将模型有效建模长上下文长度的能力与其存储过去信息的潜在状态大小的缩放联系起来。我们的结果通过对变换器和状态空间模型的实验得到了验证，为大型语言模型向更长上下文长度的发展奠定了理论基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03983",
            "title": "Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities",
            "url": "https://huggingface.co/papers/2503.03983",
            "abstract": "Understanding and reasoning over non-speech sounds and music are crucial for both humans and AI agents to interact effectively with their environments. In this paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM) with advanced audio understanding and reasoning capabilities. AF2 leverages (i) a custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio reasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves state-of-the-art performance with only a 3B parameter small language model, surpassing large open-source and proprietary models across over 20 benchmarks. Next, for the first time, we extend audio understanding to long audio segments (30 secs to 5 mins) and propose LongAudio, a large and novel dataset for training ALMs on long audio captioning and question-answering tasks. Fine-tuning AF2 on LongAudio leads to exceptional performance on our proposed LongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio understanding capabilities. We conduct extensive ablation studies to confirm the efficacy of our approach. Project Website: https://research.nvidia.com/labs/adlr/AF2/.",
            "score": 7,
            "issue_id": 2581,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "952e4cb72ec34df8",
            "authors": [
                "Sreyan Ghosh",
                "Zhifeng Kong",
                "Sonal Kumar",
                "S Sakshi",
                "Jaehyeon Kim",
                "Wei Ping",
                "Rafael Valle",
                "Dinesh Manocha",
                "Bryan Catanzaro"
            ],
            "affiliations": [
                "NVIDIA, Santa Clara, CA, USA",
                "University of Maryland, College Park, MD, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03983.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#long_context",
                    "#dataset",
                    "#small_models",
                    "#reasoning",
                    "#synthetic",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "AF2: Революция в понимании аудио искусственным интеллектом",
                    "desc": "Audio Flamingo 2 (AF2) - это усовершенствованная аудио-языковая модель (ALM) с продвинутыми возможностями понимания и рассуждения об аудио. Модель использует специальную модель CLAP, синтетические данные Audio QA и многоступенчатую стратегию обучения. AF2 достигает наилучших результатов среди существующих моделей на более чем 20 тестах, используя всего 3 миллиарда параметров. Кроме того, исследователи расширили возможности модели для работы с длинными аудиосегментами и создали набор данных LongAudio для обучения ALM на задачах описания и ответов на вопросы по длинным аудио."
                },
                "en": {
                    "title": "Revolutionizing Audio Understanding with Audio Flamingo 2",
                    "desc": "This paper presents Audio Flamingo 2 (AF2), an advanced Audio-Language Model (ALM) designed to enhance audio understanding and reasoning. AF2 utilizes a custom CLAP model and synthetic Audio QA data to improve fine-grained audio reasoning, along with a multi-stage curriculum learning approach. The model achieves state-of-the-art results with a relatively small 3B parameter language model, outperforming larger models on over 20 benchmarks. Additionally, it introduces LongAudio, a new dataset for training on long audio segments, and demonstrates exceptional performance on the LongAudioBench for evaluating long audio understanding."
                },
                "zh": {
                    "title": "音频理解的新突破：Audio Flamingo 2",
                    "desc": "本文介绍了Audio Flamingo 2（AF2），这是一种具有先进音频理解和推理能力的音频语言模型（ALM）。AF2利用了定制的CLAP模型、合成的音频问答数据以及多阶段的课程学习策略。AF2在仅使用一个3B参数的小型语言模型的情况下，超越了20多个基准测试中的大型开源和专有模型，达到了最先进的性能。此外，AF2首次扩展了对长音频片段（30秒到5分钟）的理解，并提出了LongAudio数据集，用于训练ALM在长音频标注和问答任务上的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04378",
            "title": "Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks",
            "url": "https://huggingface.co/papers/2503.04378",
            "abstract": "Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect data for and train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3.",
            "score": 6,
            "issue_id": 2578,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "926e56aa51a0fefe",
            "authors": [
                "Zhilin Wang",
                "Jiaqi Zeng",
                "Olivier Delalleau",
                "Daniel Egert",
                "Ellie Evans",
                "Hoo-Chang Shin",
                "Felipe Soares",
                "Yi Dong",
                "Oleksii Kuchaiev"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04378.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#inference",
                    "#reasoning",
                    "#rlhf",
                    "#optimization"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Масштабирование при выводе для открытых задач: от черновика к улучшенному ответу",
                    "desc": "Статья описывает новый метод масштабирования во время вывода для открытых задач общего домена. Авторы предлагают систему из трех моделей: одна генерирует начальный ответ, вторая дает обратную связь, а третья редактирует ответ на основе этой обратной связи. Эксперименты показывают, что такой подход позволяет значительно улучшить производительность на бенчмарке Arena Hard. При оптимальном масштабировании система на основе моделей семейства Llama 3 размером 70B достигает наилучших результатов, превосходя OpenAI o1 и DeepSeek R1."
                },
                "en": {
                    "title": "Enhancing Open-Ended Task Performance through Feedback and Editing",
                    "desc": "This paper discusses a novel approach to improve inference-time scaling in machine learning models, particularly for open-ended tasks. It introduces a system where one model generates an initial response, a second model provides feedback, and a third model edits the response based on that feedback. This method allows for more flexible applications beyond traditional tasks that require verifiable answers, such as math or coding. The authors demonstrate that by optimizing the number of drafts, feedback, and edits, their model achieves state-of-the-art performance on the Arena Hard benchmark, outperforming previous models."
                },
                "zh": {
                    "title": "推理时间扩展：提升开放性任务的性能",
                    "desc": "本文探讨了推理时间扩展在机器学习模型中的重要性，尤其是OpenAI o1和DeepSeek R1等模型。许多现有技术要求任务的答案可验证，这限制了它们在开放性任务中的应用。我们借鉴人类如何进行初步尝试、请求反馈并根据反馈进行改进的过程，开发了专门的反馈和编辑模型。通过优化初始响应草稿、有效反馈和编辑响应的数量，我们的模型在Arena Hard基准测试中达到了92.7的最新性能，超越了其他模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01901",
            "title": "Identifying Sensitive Weights via Post-quantization Integral",
            "url": "https://huggingface.co/papers/2503.01901",
            "abstract": "Serving Large Language Models (LLMs) is costly. However, post-training weight quantization can address this problem by both compressing their sizes for limited memory and saving bandwidth for acceleration. As not all weight dimensions are equally important, those methods typically rely on a sensitivity metric, which indicates the element-wise influence of weights on loss function and is used to preprocess original weights for better quantization. In this work, we conduct an empirical study on the accuracy of the sensitivity metric, and find that existing gradient and Hessian based metrics are very inaccurate: they underestimate quantization's impact on the loss function by orders of magnitude, mainly due to the small convergence radius of local 2nd order approximation, \\ie, gradient and Hessian term in Taylor's formula. To tackle this problem, we propose Post-quantization Integral (PQI), an accurate metric to estimate posterior sensitivity in a fine-grained manner. To leverage this accurate metric, we further propose ReQuant, a simple yet powerful framework that mainly consists of two Dense-and-Sparse detach components: self-adaptive outlier selection and step-wise significant weights detach. Results show that ReQuant boosts state-of-the-art post-training quantization methods, with a pronounced improvement of 2.66 perplexity gain on Llama 3.2 1B with QTIP.",
            "score": 5,
            "issue_id": 2585,
            "pub_date": "2025-02-28",
            "pub_date_card": {
                "ru": "28 февраля",
                "en": "February 28",
                "zh": "2月28日"
            },
            "hash": "1045983ed982a00b",
            "authors": [
                "Yuezhou Hu",
                "Weiyu Huang",
                "Zichen Liang",
                "Chang Chen",
                "Jintao Zhang",
                "Jun Zhu",
                "Jianfei Chen"
            ],
            "affiliations": [
                "Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01901.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Точная квантизация для эффективных языковых моделей",
                    "desc": "Эта статья представляет новый метод квантизации весов для крупных языковых моделей (LLM), называемый ReQuant. Авторы обнаружили, что существующие метрики чувствительности весов неточны и предложили более точную метрику - Post-quantization Integral (PQI). ReQuant использует PQI вместе с адаптивным выбором выбросов и пошаговым отделением значимых весов. Результаты показывают, что ReQuant улучшает современные методы пост-тренировочной квантизации, значительно повышая производительность модели Llama 3.2 1B."
                },
                "en": {
                    "title": "Enhancing LLM Efficiency with Accurate Quantization Metrics",
                    "desc": "This paper addresses the high costs associated with serving large language models (LLMs) by introducing post-training weight quantization techniques. It highlights the inadequacy of existing sensitivity metrics, which fail to accurately predict the impact of quantization on model performance. The authors propose a new metric called Post-quantization Integral (PQI) that provides a more precise estimation of weight sensitivity. Additionally, they introduce ReQuant, a framework that enhances quantization methods by effectively selecting significant weights and improving overall model accuracy."
                },
                "zh": {
                    "title": "提升量化精度，降低模型成本",
                    "desc": "这篇论文讨论了大型语言模型（LLMs）在服务时的高成本问题。通过后训练权重量化，可以压缩模型大小，节省内存和带宽。研究发现，现有的基于梯度和海森矩阵的敏感度度量不够准确，低估了量化对损失函数的影响。为了解决这个问题，提出了后量化积分（PQI）作为一种更精确的敏感度度量，并进一步提出了ReQuant框架，以提高后训练量化方法的效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01917",
            "title": "How to Steer LLM Latents for Hallucination Detection?",
            "url": "https://huggingface.co/papers/2503.01917",
            "abstract": "Hallucinations in LLMs pose a significant concern to their safe deployment in real-world applications. Recent approaches have leveraged the latent space of LLMs for hallucination detection, but their embeddings, optimized for linguistic coherence rather than factual accuracy, often fail to clearly separate truthful and hallucinated content. To this end, we propose the Truthfulness Separator Vector (TSV), a lightweight and flexible steering vector that reshapes the LLM's representation space during inference to enhance the separation between truthful and hallucinated outputs, without altering model parameters. Our two-stage framework first trains TSV on a small set of labeled exemplars to form compact and well-separated clusters. It then augments the exemplar set with unlabeled LLM generations, employing an optimal transport-based algorithm for pseudo-labeling combined with a confidence-based filtering process. Extensive experiments demonstrate that TSV achieves state-of-the-art performance with minimal labeled data, exhibiting strong generalization across datasets and providing a practical solution for real-world LLM applications.",
            "score": 4,
            "issue_id": 2592,
            "pub_date": "2025-03-01",
            "pub_date_card": {
                "ru": "1 марта",
                "en": "March 1",
                "zh": "3月1日"
            },
            "hash": "cbd43445771d58c3",
            "authors": [
                "Seongheon Park",
                "Xuefeng Du",
                "Min-Hsuan Yeh",
                "Haobo Wang",
                "Yixuan Li"
            ],
            "affiliations": [
                "Department of Computer Sciences, University of Wisconsin-Madison, USA",
                "School of Software Technology, Zhejiang University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01917.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#hallucinations"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "TSV: Эффективное разделение правды и вымысла в LLM",
                    "desc": "Статья представляет новый метод обнаружения галлюцинаций в больших языковых моделях (LLM) - Truthfulness Separator Vector (TSV). TSV - это легковесный вектор, который изменяет пространство представлений LLM во время вывода, чтобы улучшить разделение между правдивым и галлюцинированным содержанием. Метод использует двухэтапный подход: сначала TSV обучается на небольшом наборе размеченных примеров, а затем применяется алгоритм псевдо-разметки на основе оптимального транспорта для расширения набора данных. Эксперименты показывают, что TSV достигает современного уровня производительности с минимальным количеством размеченных данных."
                },
                "en": {
                    "title": "Enhancing Truthfulness in LLMs with TSV",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs), which can lead to the generation of false information. The authors introduce the Truthfulness Separator Vector (TSV), a novel method that modifies the representation space of LLMs during inference to better distinguish between accurate and hallucinated content. TSV is trained using a small set of labeled examples and then enhanced with additional unlabeled data through a unique pseudo-labeling technique. The results show that TSV significantly improves the accuracy of LLM outputs while requiring minimal labeled data, making it a valuable tool for safe LLM deployment."
                },
                "zh": {
                    "title": "提升LLM真实与幻觉内容分离的真相分离向量",
                    "desc": "在大型语言模型（LLMs）中，幻觉现象对其在实际应用中的安全部署构成了重大挑战。虽然最近的方法利用了LLMs的潜在空间进行幻觉检测，但其嵌入通常更注重语言连贯性而非事实准确性，导致难以清晰区分真实和幻觉内容。为此，我们提出了真相分离向量（TSV），这是一种轻量且灵活的引导向量，可以在推理过程中重塑LLM的表示空间，从而增强真实输出与幻觉输出之间的分离。我们的两阶段框架首先在小规模标记样本上训练TSV，以形成紧凑且分离良好的聚类，然后通过基于最优传输的伪标记算法和基于置信度的过滤过程，利用未标记的LLM生成数据来扩展样本集。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.02495",
            "title": "Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer",
            "url": "https://huggingface.co/papers/2503.02495",
            "abstract": "Mixture-of-Experts (MoE) enhances model performance while maintaining computational efficiency, making it well-suited for large-scale applications. However, expert in exist MoE paradigm works as an individual, thereby lacking high-quality expert interactions. Moreover, they have not been effectively extended to attention block, which constrains further efficiency improvements. To tackle these issues, we propose Union-of-Experts (UoE), which decomposes transformer into an equitant group of experts, and then implement dynamic routing on input data and experts. Our approach advances MoE design with three key innovations: (1) We conducted equitant expert decomposition on both MLP blocks and attention blocks based on matrix partition in tensor parallelism. (2) We developed two routing paradigms: patch wise data selection and expert selection, to apply routing across different levels. (3) We design the architecture of UoE model, including Selective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We develop parallel implementation of UoE's routing and computation operation, and optimize efficiency based on the hardware processing analysis. The experiments demonstrate that the model employed with UoE surpass Full Attention, state-of-art MoEs and efficient transformers in several tasks across image and natural language domains. The source codes are available at https://github.com/YujiaoYang-work/UoE.",
            "score": 4,
            "issue_id": 2586,
            "pub_date": "2025-03-04",
            "pub_date_card": {
                "ru": "4 марта",
                "en": "March 4",
                "zh": "3月4日"
            },
            "hash": "b879dd88adfef125",
            "authors": [
                "Yujiao Yang",
                "Jing Lian",
                "Linhui Li"
            ],
            "affiliations": [
                "School of Mechanical Engineering, Dalian University of Technology, Dalian 116024, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.02495.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#optimization",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "UoE: Новый уровень эффективности трансформеров через объединение экспертов",
                    "desc": "Статья представляет новый подход к архитектуре трансформеров, называемый Union-of-Experts (UoE). UoE улучшает модель Mixture-of-Experts (MoE), разбивая трансформер на группы экспертов и применяя динамическую маршрутизацию как к входным данным, так и к экспертам. Ключевые инновации включают декомпозицию экспертов для блоков MLP и внимания, новые парадигмы маршрутизации и оптимизированную параллельную реализацию. Эксперименты показывают, что UoE превосходит существующие модели в задачах обработки изображений и естественного языка."
                },
                "en": {
                    "title": "Union-of-Experts: Enhancing Efficiency and Interaction in Transformers",
                    "desc": "The paper introduces Union-of-Experts (UoE), a novel approach that enhances the Mixture-of-Experts (MoE) framework by improving expert interactions and extending its application to attention blocks. UoE decomposes transformers into groups of experts and utilizes dynamic routing to optimize input data processing. Key innovations include equitant expert decomposition, two routing paradigms for data and expert selection, and a new architecture featuring Selective Multi-Head Attention and Union-of-MLP-Experts. Experimental results show that UoE outperforms existing models in various tasks, demonstrating its effectiveness in both image and natural language processing."
                },
                "zh": {
                    "title": "专家联合模型：提升效率与性能的创新之路",
                    "desc": "混合专家模型（MoE）通过提高模型性能并保持计算效率，适合大规模应用。然而，现有的MoE模型中的专家作为个体工作，缺乏高质量的专家交互。此外，MoE尚未有效扩展到注意力模块，这限制了进一步的效率提升。为了解决这些问题，我们提出了专家联合模型（UoE），通过将变换器分解为等效的专家组，并在输入数据和专家之间实现动态路由，从而改进了MoE的设计。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04606",
            "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
            "url": "https://huggingface.co/papers/2503.04606",
            "abstract": "Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a sim14,000times compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Keling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.",
            "score": 4,
            "issue_id": 2580,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "c635690f3edc1186",
            "authors": [
                "Aoxiong Yin",
                "Kai Shen",
                "Yichong Leng",
                "Xu Tan",
                "Xinyu Zhou",
                "Juncheng Li",
                "Siliang Tang"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Zhejiang University, Hangzhou, China",
                "Moonshot AI, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04606.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#open_source",
                    "#benchmark",
                    "#long_context",
                    "#architecture",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "LanDiff: гибридный подход к генерации видео из текста",
                    "desc": "Статья представляет LanDiff - гибридную архитектуру для генерации видео из текста, сочетающую преимущества авторегрессионных языковых моделей и диффузионных моделей. LanDiff включает семантический токенизатор, языковую модель для генерации семантических токенов и потоковую диффузионную модель для уточнения деталей. Модель LanDiff объемом 5 миллиардов параметров превзошла современные открытые и коммерческие модели в бенчмарке VBench T2V. Также LanDiff показала лучшие результаты в генерации длинных видео по сравнению с другими открытыми моделями."
                },
                "en": {
                    "title": "LanDiff: Bridging Language and Visuals for Superior Video Generation",
                    "desc": "This paper introduces LanDiff, a novel framework for text-to-video (T2V) generation that combines the strengths of autoregressive language models and diffusion models. It addresses the limitations of each approach by using a coarse-to-fine generation strategy, which enhances both visual quality and semantic understanding. Key innovations include a semantic tokenizer for efficient compression of visual features, a language model that generates meaningful semantic tokens, and a streaming diffusion model that refines these tokens into high-quality videos. The results demonstrate that LanDiff outperforms existing state-of-the-art models in T2V tasks, particularly in generating long videos."
                },
                "zh": {
                    "title": "LanDiff：文本到视频生成的新突破",
                    "desc": "本文提出了一种名为LanDiff的混合框架，旨在结合自回归语言模型和扩散模型的优点，以实现文本到视频的生成。该框架通过粗到细的生成过程，克服了各自的局限性，提升了视觉质量和语义理解。LanDiff引入了三项关键创新，包括高效的语义压缩技术、生成高层语义关系的语言模型，以及将粗略语义精炼为高保真视频的流式扩散模型。实验结果表明，LanDiff在VBench T2V基准测试中表现优异，超越了现有的开源和商业模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04369",
            "title": "Lost in Literalism: How Supervised Training Shapes Translationese in LLMs",
            "url": "https://huggingface.co/papers/2503.04369",
            "abstract": "Large language models (LLMs) have achieved remarkable success in machine translation, demonstrating impressive performance across diverse languages. However, translationese, characterized by overly literal and unnatural translations, remains a persistent challenge in LLM-based translation systems. Despite their pre-training on vast corpora of natural utterances, LLMs exhibit translationese errors and generate unexpected unnatural translations, stemming from biases introduced during supervised fine-tuning (SFT). In this work, we systematically evaluate the prevalence of translationese in LLM-generated translations and investigate its roots during supervised training. We introduce methods to mitigate these biases, including polishing golden references and filtering unnatural training instances. Empirical evaluations demonstrate that these approaches significantly reduce translationese while improving translation naturalness, validated by human evaluations and automatic metrics. Our findings highlight the need for training-aware adjustments to optimize LLM translation outputs, paving the way for more fluent and target-language-consistent translations. We release the data and code at https://github.com/yafuly/LLM_Translationese.",
            "score": 3,
            "issue_id": 2586,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "7e6f548766f04dbf",
            "authors": [
                "Yafu Li",
                "Ronghao Zhang",
                "Zhilin Wang",
                "Huajian Zhang",
                "Leyang Cui",
                "Yongjing Yin",
                "Tong Xiao",
                "Yue Zhang"
            ],
            "affiliations": [
                "Northeastern University",
                "Shanghai AI Laboratory",
                "Westlake University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04369.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multilingual",
                    "#machine_translation",
                    "#data"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "Борьба с переводным языком: путь к естественным переводам с помощью LLM",
                    "desc": "Эта статья посвящена проблеме переводного языка (translationese) в системах машинного перевода на основе больших языковых моделей (LLM). Авторы исследуют причины возникновения неестественных переводов и предлагают методы для снижения этого эффекта, включая улучшение эталонных переводов и фильтрацию неестественных примеров в обучающих данных. Эмпирические оценки показывают, что эти подходы значительно уменьшают проявления переводного языка и улучшают естественность переводов. Исследование подчеркивает необходимость корректировки процесса обучения для оптимизации качества переводов, выполняемых LLM."
                },
                "en": {
                    "title": "Reducing Translationese for Natural Language Translations",
                    "desc": "This paper addresses the issue of translationese in large language models (LLMs) used for machine translation, which leads to unnatural and overly literal translations. The authors evaluate how prevalent translationese is in LLM outputs and identify biases introduced during supervised fine-tuning (SFT) as a key factor. They propose methods to reduce these biases, such as refining reference translations and filtering out unnatural training examples. Their empirical results show that these strategies significantly enhance the naturalness of translations, suggesting that careful adjustments during training can improve LLM performance in translation tasks."
                },
                "zh": {
                    "title": "优化大型语言模型翻译，减少翻译腔",
                    "desc": "大型语言模型（LLMs）在机器翻译中取得了显著成功，但仍面临翻译腔的问题，即翻译过于字面和不自然。尽管在大量自然语料上进行预训练，LLMs在监督微调（SFT）过程中引入的偏差导致了翻译腔错误。本文系统评估了LLM生成翻译中的翻译腔现象，并探讨了其在监督训练中的根源。我们提出了减轻这些偏差的方法，并通过实证评估证明这些方法显著降低了翻译腔，提高了翻译的自然性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.02191",
            "title": "Understanding and Predicting Derailment in Toxic Conversations on GitHub",
            "url": "https://huggingface.co/papers/2503.02191",
            "abstract": "Software projects thrive on the involvement and contributions of individuals from different backgrounds. However, toxic language and negative interactions can hinder the participation and retention of contributors and alienate newcomers. Proactive moderation strategies aim to prevent toxicity from occurring by addressing conversations that have derailed from their intended purpose. This study aims to understand and predict conversational derailment leading to toxicity on GitHub.   To facilitate this research, we curate a novel dataset comprising 202 toxic conversations from GitHub with annotated derailment points, along with 696 non-toxic conversations as a baseline. Based on this dataset, we identify unique characteristics of toxic conversations and derailment points, including linguistic markers such as second-person pronouns, negation terms, and tones of Bitter Frustration and Impatience, as well as patterns in conversational dynamics between project contributors and external participants.   Leveraging these empirical observations, we propose a proactive moderation approach to automatically detect and address potentially harmful conversations before escalation. By utilizing modern LLMs, we develop a conversation trajectory summary technique that captures the evolution of discussions and identifies early signs of derailment. Our experiments demonstrate that LLM prompts tailored to provide summaries of GitHub conversations achieve 69% F1-Score in predicting conversational derailment, strongly improving over a set of baseline approaches.",
            "score": 3,
            "issue_id": 2581,
            "pub_date": "2025-03-04",
            "pub_date_card": {
                "ru": "4 марта",
                "en": "March 4",
                "zh": "3月4日"
            },
            "hash": "1d0bc1069249c9e1",
            "authors": [
                "Mia Mohammad Imran",
                "Robert Zita",
                "Rebekah Copeland",
                "Preetha Chatterjee",
                "Rahat Rizvi Rahman",
                "Kostadin Damevski"
            ],
            "affiliations": [
                "Drexel University, Philadelphia, PA, USA",
                "Eastern Mennonite University, Harrisonburg, VA, USA",
                "Elmhurst University, Elmhurst, IL, USA",
                "Missouri University of Science and Technology, Rolla, MO, USA",
                "Virginia Commonwealth University, Richmond, VA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.02191.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#dataset",
                    "#multimodal",
                    "#data"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "ИИ на страже здоровой атмосферы в open-source сообществах",
                    "desc": "Исследование посвящено анализу и предотвращению токсичных взаимодействий в проектах на GitHub. Авторы создали датасет из 202 токсичных и 696 нетоксичных разговоров, выявив лингвистические маркеры и паттерны, характерные для деструктивных обсуждений. На основе этих данных была разработана система проактивной модерации с использованием больших языковых моделей (LLM). Эксперименты показали, что предложенный подход достигает 69% F1-меры в предсказании потенциально опасных разговоров на ранних стадиях."
                },
                "en": {
                    "title": "Proactive Moderation: Detecting Toxicity Before It Escalates",
                    "desc": "This paper investigates how toxic language in GitHub conversations can deter contributors and newcomers. It introduces a dataset of 202 toxic conversations with marked derailment points, alongside 696 non-toxic examples for comparison. The study identifies specific linguistic features and conversational dynamics that signal potential toxicity. Using large language models (LLMs), the authors propose a proactive moderation strategy that summarizes conversation trajectories to detect early signs of derailment, achieving a 69% F1-Score in predictions."
                },
                "zh": {
                    "title": "主动管理，防止对话偏离与有毒语言",
                    "desc": "本研究探讨了如何在GitHub上预测和理解对话偏离导致的有毒语言。我们创建了一个新数据集，包含202个有毒对话和696个非有毒对话，并标注了偏离点。通过分析这些对话的语言特征和动态模式，我们识别出有毒对话的独特特征。最后，我们提出了一种主动的管理策略，利用现代大语言模型自动检测和处理潜在的有害对话。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.01375",
            "title": "Combining Flow Matching and Transformers for Efficient Solution of Bayesian Inverse Problems",
            "url": "https://huggingface.co/papers/2503.01375",
            "abstract": "Solving Bayesian inverse problems efficiently remains a significant challenge due to the complexity of posterior distributions and the computational cost of traditional sampling methods. Given a series of observations and the forward model, we want to recover the distribution of the parameters, conditioned on observed experimental data. We show, that combining Conditional Flow Mathching (CFM) with transformer-based architecture, we can efficiently sample from such kind of distribution, conditioned on variable number of observations.",
            "score": 2,
            "issue_id": 2583,
            "pub_date": "2025-03-03",
            "pub_date_card": {
                "ru": "3 марта",
                "en": "March 3",
                "zh": "3月3日"
            },
            "hash": "9dd35b1faaa32b61",
            "authors": [
                "Daniil Sherki",
                "Ivan Oseledets",
                "Ekaterina Muravleva"
            ],
            "affiliations": [
                "Artificial Intelligence Research Institute",
                "Sberbank, AI4S Center",
                "Skolkovo Institute of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.01375.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#math"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Эффективная выборка из сложных апостериорных распределений с помощью CFM и трансформеров",
                    "desc": "Статья представляет новый подход к решению байесовских обратных задач. Авторы предлагают комбинацию метода Conditional Flow Matching (CFM) с архитектурой на основе трансформеров. Этот подход позволяет эффективно осуществлять выборку из сложных апостериорных распределений, обусловленных переменным числом наблюдений. Метод преодолевает ограничения традиционных методов выборки, снижая вычислительные затраты."
                },
                "en": {
                    "title": "Efficient Bayesian Inference with CFM and Transformers",
                    "desc": "This paper addresses the challenge of efficiently solving Bayesian inverse problems, which involve estimating parameter distributions based on observed data. Traditional sampling methods can be computationally expensive, especially when dealing with complex posterior distributions. The authors propose a novel approach that combines Conditional Flow Matching (CFM) with transformer-based architectures to improve sampling efficiency. This method allows for effective sampling from parameter distributions conditioned on a variable number of observations, enhancing the ability to recover accurate parameter estimates."
                },
                "zh": {
                    "title": "高效贝叶斯逆问题求解的新方法",
                    "desc": "解决贝叶斯逆问题的效率仍然是一个重大挑战，因为后验分布的复杂性和传统采样方法的计算成本较高。我们希望在给定一系列观测和前向模型的情况下，恢复参数的分布，这些分布是基于观察到的实验数据。我们展示了将条件流匹配（CFM）与基于变换器的架构相结合，可以有效地从这种分布中进行采样，且该分布可以根据观测数量的变化而变化。此方法为贝叶斯推断提供了一种新的高效解决方案。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03962",
            "title": "On the Acquisition of Shared Grammatical Representations in Bilingual Language Models",
            "url": "https://huggingface.co/papers/2503.03962",
            "abstract": "While crosslingual transfer is crucial to contemporary language models' multilingual capabilities, how it occurs is not well understood. In this paper, we ask what happens to a monolingual language model when it begins to be trained on a second language. Specifically, we train small bilingual models for which we control the amount of data for each language and the order of language exposure. To find evidence of shared multilingual representations, we turn to structural priming, a method used to study grammatical representations in humans. We first replicate previous crosslingual structural priming results and find that after controlling for training data quantity and language exposure, there are asymmetrical effects across language pairs and directions. We argue that this asymmetry may shape hypotheses about human structural priming effects. We also find that structural priming effects are less robust for less similar language pairs, highlighting potential limitations of crosslingual transfer learning and shared representations for typologically diverse languages.",
            "score": 0,
            "issue_id": 2592,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 марта",
                "en": "March 5",
                "zh": "3月5日"
            },
            "hash": "5d7eee50132fd29c",
            "authors": [
                "Catherine Arnett",
                "Tyler A. Chang",
                "James A. Michaelov",
                "Benjamin K. Bergen"
            ],
            "affiliations": [
                "Department of Brain and Cognitive Science, Massachusetts Institute of Technology",
                "Department of Cognitive Science, University of California San Diego",
                "Department of Linguistics, University of California San Diego",
                "Halıcıoglu Data Science Institute, University of California San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03962.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#training",
                    "#transfer_learning",
                    "#data",
                    "#multilingual"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрывая тайны кросс-лингвистического переноса в нейронных сетях",
                    "desc": "Статья исследует процесс кросс-лингвистического переноса в двуязычных языковых моделях. Авторы изучают, как модель, обученная на одном языке, начинает воспринимать второй язык. Для анализа общих многоязычных представлений используется метод структурного прайминга, заимствованный из психолингвистики. Результаты показывают асимметричные эффекты между парами языков и направлениями переноса, а также меньшую устойчивость структурного прайминга для типологически различных языков."
                },
                "en": {
                    "title": "Understanding Crosslingual Transfer in Language Models",
                    "desc": "This paper investigates how training a monolingual language model on a second language affects its performance and understanding. The authors create small bilingual models and manipulate the amount of training data and the sequence of language exposure. They utilize structural priming to analyze the grammatical representations that emerge from this bilingual training. The findings reveal asymmetrical effects in language pairs and suggest that less similar languages may limit the effectiveness of crosslingual transfer learning."
                },
                "zh": {
                    "title": "探索跨语言迁移的非对称性",
                    "desc": "本文探讨了跨语言迁移在现代语言模型中的重要性，尤其是当单语模型开始接受第二语言训练时会发生什么。我们训练了小型双语模型，控制每种语言的数据量和语言暴露的顺序。通过结构性启动的方法，我们发现不同语言对之间的影响不对称，这可能影响人类的结构性启动假设。研究还表明，对于语言类型差异较大的语言对，结构性启动效果较弱，突显了跨语言迁移学习的潜在局限性。"
                }
            }
        }
    ],
    "link_prev": "2025-03-06.html",
    "link_next": "2025-03-10.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "06.03",
        "en": "03/06",
        "zh": "3月6日"
    },
    "short_date_next": {
        "ru": "10.03",
        "en": "03/10",
        "zh": "3月10日"
    },
    "categories": {
        "#dataset": 7,
        "#data": 5,
        "#benchmark": 10,
        "#agents": 2,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 4,
        "#3d": 0,
        "#audio": 2,
        "#video": 3,
        "#multimodal": 5,
        "#math": 2,
        "#multilingual": 2,
        "#architecture": 7,
        "#healthcare": 1,
        "#training": 11,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 5,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 4,
        "#long_context": 8,
        "#synthetic": 1,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 8,
        "#small_models": 3,
        "#science": 1,
        "#low_resource": 2
    },
    "zh": {
        "text": "这篇文章介绍了一种新的大型语言模型，叫做 START。它结合了外部工具来提升复杂推理任务的能力。传统模型如 OpenAI-o1 和 DeepSeek-R1 常常因为依赖内部推理而产生幻觉和低效。START 通过执行代码，进行复杂计算、自我检查、探索多种方法和自我调试，解决了这些问题。其核心创新在于自学习框架，包括两种技术：Hint-infer 和 Hint-RFT。START 在高难度科学问答和数学竞赛中表现优异，显著超越了基础模型 QwQ-32B。",
        "title": "START: Self-taught Reasoner with Tools",
        "pinyin": "这篇文章介绍了一种新的大型语言模型，叫做 START。它结合了外部工具来提升复杂推理任务的能力。传统模型如 OpenAI-o1 和 DeepSeek-R1 常常因为依赖内部推理而产生幻觉和低效。START 通过执行代码，进行复杂计算、自我检查、探索多种方法和自我调试，解决了这些问题。其核心创新在于自学习框架，包括两种技术：Hint-infer 和 Hint-RFT。START 在高难度科学问答和数学竞赛中表现优异，显著超越了基础模型 QwQ-32B。\n\nZhè piān wénzhāng jièshào le yī zhǒng xīn de dàxíng yǔyán móxíng, jiào zuò START. Tā jiéhé le wàibù gōngjù lái tíshēng fùzá xīnglǐ rènwù de nénglì. Chuántǒng móxíng rú OpenAI-o1 hé DeepSeek-R1 chángcháng yīnwèi yīlài nèibù tuīlǐ ér chǎnshēng huànjué hé dīxìo. START tōngguò zhíxíng dàimǎ, jìnxíng fùzá jìsuàn, zìwǒ jiǎnchá, tànsuǒ duōzhǒng fāngfǎ hé zìwǒ tiáoshì, jiějué le zhèxiē wèntí. Qí héxīn chuàngxīn zài yú zìxuéxí kuàngjià, bāokuò liǎng zhǒng jìshù: Hint-infer hé Hint-RFT. START zài gāo nándù kēxué wèndá hé shùxué jìngsài zhōng biǎoxiàn yōuyù, xiǎnzhù chāoyuè le jīchǔ móxíng QwQ-32B.",
        "vocab": "[{'word': '大型', 'pinyin': 'dàxíng', 'trans': 'large-scale'},\n{'word': '语言模型', 'pinyin': 'yǔyán móxíng', 'trans': 'language model'},\n{'word': '结合', 'pinyin': 'jiéhé', 'trans': 'combine'},\n{'word': '外部', 'pinyin': 'wàibù', 'trans': 'external'},\n{'word': '工具', 'pinyin': 'gōngjù', 'trans': 'tool'},\n{'word': '提升', 'pinyin': 'tíshēng', 'trans': 'enhance'},\n{'word': '复杂', 'pinyin': 'fùzá', 'trans': 'complex'},\n{'word': '推理', 'pinyin': 'tuīlǐ', 'trans': 'reasoning'},\n{'word': '任务', 'pinyin': 'rènwù', 'trans': 'task'},\n{'word': '能力', 'pinyin': 'nénglì', 'trans': 'ability'},\n{'word': '传统', 'pinyin': 'chuántǒng', 'trans': 'traditional'},\n{'word': '依赖', 'pinyin': 'yīlài', 'trans': 'rely'},\n{'word': '内部', 'pinyin': 'nèibù', 'trans': 'internal'},\n{'word': '产生', 'pinyin': 'chǎnshēng', 'trans': 'generate'},\n{'word': '幻觉', 'pinyin': 'huànjué', 'trans': 'hallucination'},\n{'word': '低效', 'pinyin': 'dīxiào', 'trans': 'inefficient'},\n{'word': '执行', 'pinyin': 'zhíxíng', 'trans': 'execute'},\n{'word': '计算', 'pinyin': 'jìsuàn', 'trans': 'calculation'},\n{'word': '自我检查', 'pinyin': 'zìwǒ jiǎnchá', 'trans': 'self-check'},\n{'word': '探索', 'pinyin': 'tànsuǒ', 'trans': 'explore'},\n{'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'},\n{'word': '自我调试', 'pinyin': 'zìwǒ tiáoshì', 'trans': 'self-debug'},\n{'word': '解决', 'pinyin': 'jiějué', 'trans': 'solve'},\n{'word': '核心', 'pinyin': 'héxīn', 'trans': 'core'},\n{'word': '创新', 'pinyin': 'chuàngxīn', 'trans': 'innovation'},\n{'word': '框架', 'pinyin': 'kuàngjià', 'trans': 'framework'},\n{'word': '技术', 'pinyin': 'jìshù', 'trans': 'technology'},\n{'word': '高难度', 'pinyin': 'gāo nándù', 'trans': 'high difficulty'},\n{'word': '科学问答', 'pinyin': 'kēxué wèndá', 'trans': 'scientific Q&A'},\n{'word': '数学竞赛', 'pinyin': 'shùxué jìngsài', 'trans': 'math competition'},\n{'word': '表现', 'pinyin': 'biǎoxiàn', 'trans': 'performance'},\n{'word': '优异', 'pinyin': 'yōuyì', 'trans': 'excellent'},\n{'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'},\n{'word': '超越', 'pinyin': 'chāoyuè', 'trans': 'surpass'},\n{'word': '基础模型', 'pinyin': 'jīchǔ móxíng', 'trans': 'base model'}]",
        "trans": "This article introduces a new large language model called START, which combines external tools to enhance its capabilities in complex reasoning tasks. Traditional models such as OpenAI-o1 and DeepSeek-R1 often suffer from hallucinations and inefficiencies due to their reliance on internal reasoning. START addresses these issues by executing code, performing complex calculations, self-checking, exploring multiple methods, and self-debugging. Its core innovation lies in a self-learning framework that includes two technologies: Hint-infer and Hint-RFT. START demonstrates excellent performance in high-difficulty scientific question-answering and mathematical competitions, significantly outperforming the baseline model QwQ-32B.",
        "update_ts": "2025-03-07 09:11"
    }
}