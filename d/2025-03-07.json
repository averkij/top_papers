{
    "date": {
        "ru": "7 марта",
        "en": "March 7",
        "zh": "3月7日"
    },
    "time_utc": "2025-03-07 07:10",
    "weekday": 4,
    "issue_id": 2582,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.04625",
            "title": "START: Self-taught Reasoner with Tools",
            "url": "https://huggingface.co/papers/2503.04625",
            "abstract": "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview.",
            "score": 28,
            "issue_id": 2579,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "8961f69e1eda24ad",
            "authors": [
                "Chengpeng Li",
                "Mingfeng Xue",
                "Zhenru Zhang",
                "Jiaxi Yang",
                "Beichen Zhang",
                "Xiang Wang",
                "Bowen Yu",
                "Binyuan Hui",
                "Junyang Lin",
                "Dayiheng Liu"
            ],
            "affiliations": [
                "Alibaba Group",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04625.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#rl",
                    "#training",
                    "#architecture",
                    "#hallucinations",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "START: Самообучающаяся модель рассуждений с инструментами",
                    "desc": "Статья представляет START - новую модель для рассуждений с длинной цепочкой мыслей, интегрирующую внешние инструменты. START использует фреймворк самообучения, включающий технику Hint-infer для стимулирования использования инструментов, и Hint Rejection Sampling Fine-Tuning для улучшения траекторий рассуждений. Модель значительно превосходит базовую QwQ-32B и достигает результатов на уровне современных моделей в сложных задачах рассуждений. START демонстрирует высокую точность на наборах данных по науке, математике и программированию уровня PhD и соревнований."
                },
                "en": {
                    "title": "Enhancing Reasoning with External Tools: Introducing START",
                    "desc": "This paper presents START, a new long Chain-of-thought reasoning model that integrates external tools to improve reasoning capabilities. Traditional large reasoning models often struggle with hallucinations and inefficiencies, but START addresses these issues by allowing the model to perform complex computations and self-debugging. The innovation lies in two techniques: Hint-infer, which uses designed hints to encourage tool usage, and Hint Rejection Sampling Fine-Tuning, which refines the model's reasoning paths. START demonstrates superior performance on various benchmarks, surpassing its predecessor and competing effectively with state-of-the-art models."
                },
                "zh": {
                    "title": "工具整合，推理更强！",
                    "desc": "本文介绍了一种新型的长链推理模型START（自我学习推理器与工具），它通过整合外部工具来增强推理能力。START利用代码执行进行复杂计算、自我检查、探索多种方法和自我调试，从而克服了大型推理模型（LRMs）在推理过程中常见的幻觉和低效问题。核心创新在于自我学习框架，包括提示推理（Hint-infer）和提示拒绝采样微调（Hint-RFT）两种技术，前者通过插入设计的提示来激发模型使用外部工具的能力。经过微调，START在多个科学问答和数学基准测试中表现优异，准确率显著高于基础模型QwQ-32B。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03803",
            "title": "EgoLife: Towards Egocentric Life Assistant",
            "url": "https://huggingface.co/papers/2503.03803",
            "abstract": "We introduce EgoLife, a project to develop an egocentric life assistant that accompanies and enhances personal efficiency through AI-powered wearable glasses. To lay the foundation for this assistant, we conducted a comprehensive data collection study where six participants lived together for one week, continuously recording their daily activities - including discussions, shopping, cooking, socializing, and entertainment - using AI glasses for multimodal egocentric video capture, along with synchronized third-person-view video references. This effort resulted in the EgoLife Dataset, a comprehensive 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset with intensive annotation. Leveraging this dataset, we introduce EgoLifeQA, a suite of long-context, life-oriented question-answering tasks designed to provide meaningful assistance in daily life by addressing practical questions such as recalling past relevant events, monitoring health habits, and offering personalized recommendations. To address the key technical challenges of (1) developing robust visual-audio models for egocentric data, (2) enabling identity recognition, and (3) facilitating long-context question answering over extensive temporal information, we introduce EgoButler, an integrated system comprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on egocentric datasets, achieving state-of-the-art performance on egocentric video understanding. EgoRAG is a retrieval-based component that supports answering ultra-long-context questions. Our experimental studies verify their working mechanisms and reveal critical factors and bottlenecks, guiding future improvements. By releasing our datasets, models, and benchmarks, we aim to stimulate further research in egocentric AI assistants.",
            "score": 8,
            "issue_id": 2581,
            "pub_date": "2025-03-05",
            "pub_date_card": {
                "ru": "5 марта",
                "en": "March 5",
                "zh": "3月5日"
            },
            "hash": "52396234365a3fb0",
            "authors": [
                "Jingkang Yang",
                "Shuai Liu",
                "Hongming Guo",
                "Yuhao Dong",
                "Xiamengwei Zhang",
                "Sicheng Zhang",
                "Pengyun Wang",
                "Zitang Zhou",
                "Binzhu Xie",
                "Ziyue Wang",
                "Bei Ouyang",
                "Zhengyu Lin",
                "Marco Cominelli",
                "Zhongang Cai",
                "Yuanhan Zhang",
                "Peiyuan Zhang",
                "Fangzhou Hong",
                "Joerg Widmer",
                "Francesco Gringoli",
                "Lei Yang",
                "Bo Li",
                "Ziwei Liu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2503.03803.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#long_context",
                    "#dataset",
                    "#open_source",
                    "#agents",
                    "#multimodal",
                    "#data",
                    "#benchmark"
                ],
                "emoji": "👓",
                "ru": {
                    "title": "EgoLife: ИИ-ассистент для повседневной жизни на базе умных очков",
                    "desc": "Проект EgoLife представляет собой разработку эгоцентричного ассистента на основе очков дополненной реальности с искусственным интеллектом. Исследователи собрали обширный набор данных EgoLife Dataset, включающий 300 часов эгоцентричного видео повседневной жизни шести участников. На основе этих данных создан набор задач EgoLifeQA для тестирования вопросно-ответных систем в контексте повседневной жизни. Для решения технических задач разработана система EgoButler, включающая мультимодальную модель EgoGPT и компонент для ответов на вопросы с длинным контекстом EgoRAG."
                },
                "en": {
                    "title": "Empowering Daily Life with Egocentric AI Assistance",
                    "desc": "The paper presents EgoLife, an AI-powered life assistant designed to enhance personal efficiency through wearable glasses that capture egocentric video data. A comprehensive dataset, the EgoLife Dataset, was created by recording daily activities of participants over a week, resulting in 300 hours of multimodal data with detailed annotations. The study introduces EgoLifeQA, a set of question-answering tasks that utilize this dataset to assist users with practical life questions and personalized recommendations. To tackle challenges in visual-audio model development and long-context question answering, the authors propose EgoButler, which includes EgoGPT for video understanding and EgoRAG for retrieval-based answering."
                },
                "zh": {
                    "title": "智能生活助手，提升个人效率",
                    "desc": "我们介绍了EgoLife项目，旨在开发一个以自我为中心的生活助手，通过AI驱动的可穿戴眼镜提升个人效率。我们进行了全面的数据收集研究，六名参与者共同生活一周，使用AI眼镜记录日常活动，形成了EgoLife数据集，包含300小时的多视角、多模态日常生活数据。基于该数据集，我们推出了EgoLifeQA，一个针对生活的长文本问答任务，旨在提供实用的日常生活帮助。为了解决关键技术挑战，我们引入了EgoButler系统，包括EgoGPT和EgoRAG，前者在自我中心视频理解上表现出色，后者支持超长文本问题的回答。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04598",
            "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization",
            "url": "https://huggingface.co/papers/2503.04598",
            "abstract": "Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the location of layer normalization. While Pre-Norm structures facilitate easier training due to their more prominent identity path, they often yield suboptimal performance compared to Post-Norm. In this paper, we propose HybridNorm, a straightforward yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. This design not only stabilizes training but also enhances performance, particularly in the context of LLMs. Comprehensive experiments in both dense and sparse architectures show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches, achieving state-of-the-art results across various benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. %Code will be made publicly available. Code is available at https://github.com/BryceZhuo/HybridNorm.",
            "score": 5,
            "issue_id": 2579,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "78b05ed4e27a874b",
            "authors": [
                "Zhijian Zhuo",
                "Yutao Zeng",
                "Ya Wang",
                "Sijun Zhang",
                "Jian Yang",
                "Xiaoqing Li",
                "Xun Zhou",
                "Jinwen Ma"
            ],
            "affiliations": [
                "Capital University of Economics and Business",
                "School of Mathematical Sciences, Peking University",
                "SeedFoundation-Model, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04598.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "HybridNorm: Гибридная нормализация для улучшения обучения и производительности трансформеров",
                    "desc": "Статья представляет новый метод нормализации для трансформеров под названием HybridNorm. Этот подход сочетает преимущества Pre-Norm и Post-Norm стратегий, применяя QKV нормализацию в механизме внимания и Post-Norm в FFN каждого блока трансформера. HybridNorm показывает лучшие результаты по сравнению с существующими методами как для плотных, так и для разреженных архитектур. Эксперименты демонстрируют, что HybridNorm обеспечивает более стабильное обучение и улучшенную производительность для глубоких моделей трансформеров, особенно в контексте больших языковых моделей."
                },
                "en": {
                    "title": "HybridNorm: The Best of Both Normalization Worlds for Transformers",
                    "desc": "This paper introduces HybridNorm, a new normalization strategy for training deep transformer networks, which combines the strengths of Pre-Norm and Post-Norm methods. Pre-Norm structures help with training stability but often lead to lower performance, while Post-Norm structures typically perform better but can complicate training. HybridNorm applies QKV normalization in the attention mechanism and Post-Norm in the feed-forward network, resulting in improved training stability and performance. Experiments demonstrate that HybridNorm outperforms both Pre-Norm and Post-Norm across various benchmarks, making it a promising approach for enhancing large language models."
                },
                "zh": {
                    "title": "HybridNorm：提升变换器模型训练的稳定性与性能",
                    "desc": "本文提出了一种新的混合归一化策略，称为HybridNorm，旨在解决深度变换器网络训练中的层归一化位置问题。HybridNorm结合了Pre-Norm和Post-Norm的优点，在注意力机制中使用QKV归一化，而在前馈网络中使用Post-Norm。实验结果表明，HybridNorm在稠密和稀疏架构中均优于传统的Pre-Norm和Post-Norm方法，提升了大语言模型的训练稳定性和性能。该研究为深度变换器模型的训练和性能改进提供了新的思路。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04222",
            "title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion",
            "url": "https://huggingface.co/papers/2503.04222",
            "abstract": "We introduce FuseChat-3.0, a suite of large language models (LLMs) developed by integrating the strengths of heterogeneous source LLMs into more compact target LLMs. Our source models include the powerful Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct. For target models, we focus on three widely-used smaller variants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along with two ultra-compact options, Llama-3.2-3B-Instruct and Llama-3.2-1B-Instruct. To leverage the diverse capabilities of these source models, we develop a specialized data construction protocol tailored to various tasks and domains. The FuseChat-3.0 training pipeline consists of two key stages: (1) supervised fine-tuning (SFT) to align the target and source model distributions, and (2) Direct Preference Optimization (DPO) to apply preferences from multiple source LLMs to fine-tune the target model. The resulting FuseChat-3.0 models exhibit significant performance gains across tasks such as instruction following, general knowledge, mathematics, and coding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target model, our fusion approach achieves an average improvement of 6.8 points across 14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and 30.1 points on the instruction-following benchmarks AlpacaEval-2 and Arena-Hard, respectively. Our code, models, and datasets are available at https://github.com/SLIT-AI/FuseChat-3.0.",
            "score": 5,
            "issue_id": 2578,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "c7d793a0b91efd5d",
            "authors": [
                "Ziyi Yang",
                "Fanqi Wan",
                "Longguang Zhong",
                "Canbin Huang",
                "Guosheng Liang",
                "Xiaojun Quan"
            ],
            "affiliations": [
                "School of Computer Science and Engineering, Sun Yat-sen University, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04222.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#small_models",
                    "#rlhf",
                    "#transfer_learning",
                    "#open_source",
                    "#optimization"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Слияние мощи больших языковых моделей в компактном формате",
                    "desc": "FuseChat-3.0 представляет собой набор больших языковых моделей (LLM), разработанных путем интеграции сильных сторон гетерогенных исходных LLM в более компактные целевые модели. Процесс обучения включает два ключевых этапа: контролируемая тонкая настройка и оптимизация прямых предпочтений. Результирующие модели FuseChat-3.0 демонстрируют значительный прирост производительности в различных задачах, включая следование инструкциям, общие знания, математику и программирование. Используя Llama-3.1-8B-Instruct в качестве целевой модели, подход авторов достигает среднего улучшения на 6,8 пунктов по 14 бенчмаркам."
                },
                "en": {
                    "title": "Fusing Strengths for Smarter, Smaller Models",
                    "desc": "FuseChat-3.0 is a collection of large language models (LLMs) that combines the strengths of various larger source models into smaller, more efficient target models. The training process involves supervised fine-tuning to align the target models with the source models, followed by Direct Preference Optimization to enhance performance based on preferences from multiple sources. This innovative approach leads to significant improvements in tasks like instruction following, general knowledge, mathematics, and coding. The results show an average performance boost of 6.8 points across 14 benchmarks, with particularly impressive gains in instruction-following tasks."
                },
                "zh": {
                    "title": "融合多源模型，提升语言理解能力",
                    "desc": "我们介绍了FuseChat-3.0，这是一个通过整合不同来源的大型语言模型（LLMs）优势而开发的更紧凑的目标LLM套件。源模型包括强大的Gemma-2-27B-it、Mistral-Large-Instruct-2407、Qwen-2.5-72B-Instruct和Llama-3.1-70B-Instruct。目标模型则集中在三种广泛使用的小型变体上，以及两个超紧凑选项。通过专门的数据构建协议和两阶段的训练流程，FuseChat-3.0在多个任务上表现出显著的性能提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04094",
            "title": "PokéChamp: an Expert-level Minimax Language Agent",
            "url": "https://huggingface.co/papers/2503.04094",
            "abstract": "We introduce Pok\\'eChamp, a minimax agent powered by Large Language Models (LLMs) for Pok\\'emon battles. Built on a general framework for two-player competitive games, Pok\\'eChamp leverages the generalist capabilities of LLMs to enhance minimax tree search. Specifically, LLMs replace three key modules: (1) player action sampling, (2) opponent modeling, and (3) value function estimation, enabling the agent to effectively utilize gameplay history and human knowledge to reduce the search space and address partial observability. Notably, our framework requires no additional LLM training. We evaluate Pok\\'eChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves a win rate of 76% against the best existing LLM-based bot and 84% against the strongest rule-based bot, demonstrating its superior performance. Even with an open-source 8-billion-parameter Llama 3.1 model, Pok\\'eChamp consistently outperforms the previous best LLM-based bot, Pok\\'ellmon powered by GPT-4o, with a 64% win rate. Pok\\'eChamp attains a projected Elo of 1300-1500 on the Pok\\'emon Showdown online ladder, placing it among the top 30%-10% of human players. In addition, this work compiles the largest real-player Pok\\'emon battle dataset, featuring over 3 million games, including more than 500k high-Elo matches. Based on this dataset, we establish a series of battle benchmarks and puzzles to evaluate specific battling skills. We further provide key updates to the local game engine. We hope this work fosters further research that leverage Pok\\'emon battle as benchmark to integrate LLM technologies with game-theoretic algorithms addressing general multiagent problems. Videos, code, and dataset available at https://sites.google.com/view/pokechamp-llm.",
            "score": 4,
            "issue_id": 2580,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "bffe348d8443d4c2",
            "authors": [
                "Seth Karten",
                "Andy Luu Nguyen",
                "Chi Jin"
            ],
            "affiliations": [
                "Department of Computer Science, Princeton University",
                "Department of Electrical and Computer Engineering, Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04094.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#agents",
                    "#games"
                ],
                "emoji": "🎮",
                "ru": {
                    "title": "PokeChamp: Революция в ИИ для игровых стратегий с использованием языковых моделей",
                    "desc": "Статья представляет PokeChamp - агента на основе минимакса, использующего большие языковые модели (LLM) для боев в Pokémon. PokeChamp применяет LLM для выборки действий игрока, моделирования оппонента и оценки функции ценности, что позволяет эффективно использовать историю игры и знания человека. При использовании GPT-4o агент достигает высокого процента побед против существующих ботов и попадает в топ-30% - 10% игроков на онлайн-платформе Pokémon Showdown. Исследование также включает создание крупнейшего датасета боев Pokémon и серии тестов для оценки навыков ведения боя."
                },
                "en": {
                    "title": "Pok'eChamp: Elevating Pokémon Battles with LLMs",
                    "desc": "Pok'eChamp is a minimax agent designed for Pokémon battles that utilizes Large Language Models (LLMs) to improve its decision-making process. It replaces traditional components of the minimax algorithm with LLMs for player action sampling, opponent modeling, and value function estimation, allowing it to better understand gameplay history and human strategies. The agent demonstrates impressive performance, achieving a 76% win rate against top LLM-based bots and a 64% win rate with a smaller model, showcasing its effectiveness in competitive play. Additionally, Pok'eChamp compiles a large dataset of over 3 million Pokémon battles to create benchmarks for evaluating battling skills and encourages further research in integrating LLMs with game-theoretic approaches."
                },
                "zh": {
                    "title": "PokéChamp：宝可梦对战中的智能代理",
                    "desc": "本文介绍了PokéChamp，一个基于大型语言模型（LLMs）的极小极大算法代理，用于宝可梦对战。该代理利用LLMs的通用能力，增强了极小极大树搜索，替代了玩家动作采样、对手建模和价值函数估计等关键模块。PokéChamp在不需要额外训练的情况下，能够有效利用游戏历史和人类知识，减少搜索空间并解决部分可观测性问题。经过评估，PokéChamp在宝可梦对战中表现优异，赢得了76%的胜率，展示了其在多智能体问题中的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04130",
            "title": "Token-Efficient Long Video Understanding for Multimodal LLMs",
            "url": "https://huggingface.co/papers/2503.04130",
            "abstract": "Recent advances in video-based multimodal large language models (Video-LLMs) have significantly improved video understanding by processing videos as sequences of image frames. However, many existing methods treat frames independently in the vision backbone, lacking explicit temporal modeling, which limits their ability to capture dynamic patterns and efficiently handle long videos. To address these limitations, we introduce STORM (Spatiotemporal TOken Reduction for Multimodal LLMs), a novel architecture incorporating a dedicated temporal encoder between the image encoder and the LLM. Our temporal encoder leverages the Mamba State Space Model to integrate temporal information into image tokens, generating enriched representations that preserve inter-frame dynamics across the entire video sequence. This enriched encoding not only enhances video reasoning capabilities but also enables effective token reduction strategies, including test-time sampling and training-based temporal and spatial pooling, substantially reducing computational demands on the LLM without sacrificing key temporal information. By integrating these techniques, our approach simultaneously reduces training and inference latency while improving performance, enabling efficient and robust video understanding over extended temporal contexts. Extensive evaluations show that STORM achieves state-of-the-art results across various long video understanding benchmarks (more than 5\\% improvement on MLVU and LongVideoBench) while reducing the computation costs by up to 8times and the decoding latency by 2.4-2.9times for the fixed numbers of input frames. Project page is available at https://research.nvidia.com/labs/lpr/storm",
            "score": 3,
            "issue_id": 2580,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "f1d1afacd41dc0c7",
            "authors": [
                "Jindong Jiang",
                "Xiuyu Li",
                "Zhijian Liu",
                "Muyang Li",
                "Guo Chen",
                "Zhiqi Li",
                "De-An Huang",
                "Guilin Liu",
                "Zhiding Yu",
                "Kurt Keutzer",
                "Sungjin Ahn",
                "Jan Kautz",
                "Hongxu Yin",
                "Yao Lu",
                "Song Han",
                "Wonmin Byeon"
            ],
            "affiliations": [
                "KAIST",
                "MIT",
                "NVIDIA",
                "Nanjing University",
                "Rutgers University",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04130.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#optimization",
                    "#long_context",
                    "#architecture",
                    "#video",
                    "#inference",
                    "#multimodal"
                ],
                "emoji": "🌪️",
                "ru": {
                    "title": "STORM: Эффективное понимание длинных видео с помощью темпорального кодирования",
                    "desc": "STORM - это новая архитектура для видео-LLM, которая использует специальный темпоральный энкодер на основе модели Mamba State Space между энкодером изображений и LLM. Эта архитектура позволяет интегрировать временную информацию в токены изображений, создавая обогащенные представления, сохраняющие динамику между кадрами во всей видеопоследовательности. STORM применяет стратегии сокращения токенов, включая выборку во время тестирования и пулинг во время обучения, что значительно снижает вычислительные затраты LLM. Результаты экспериментов показывают, что STORM достигает улучшения более чем на 5% на бенчмарках длинных видео, одновременно сокращая вычислительные затраты до 8 раз."
                },
                "en": {
                    "title": "STORM: Revolutionizing Video Understanding with Temporal Insights",
                    "desc": "This paper presents STORM, a new architecture designed to enhance video understanding in multimodal large language models (LLMs) by incorporating a temporal encoder. Unlike previous methods that treat video frames independently, STORM uses the Mamba State Space Model to capture the dynamics between frames, resulting in richer representations of video content. The architecture also implements token reduction strategies that significantly lower computational costs and improve processing speed without losing important temporal information. Evaluations demonstrate that STORM outperforms existing models on long video benchmarks while achieving substantial reductions in computation and latency."
                },
                "zh": {
                    "title": "高效视频理解的新突破：STORM模型",
                    "desc": "最近，基于视频的多模态大语言模型（Video-LLMs）在视频理解方面取得了显著进展，但许多现有方法在视觉骨干中独立处理帧，缺乏明确的时间建模，限制了捕捉动态模式的能力。为了解决这些问题，我们提出了STORM（时空令牌减少模型），它在图像编码器和大语言模型之间引入了专门的时间编码器。我们的时间编码器利用Mamba状态空间模型，将时间信息整合到图像令牌中，生成丰富的表示，保留整个视频序列中的帧间动态。通过这些技术的整合，我们的方法在提高性能的同时，显著减少了计算需求和推理延迟，实现了对长视频的高效理解。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04725",
            "title": "L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling",
            "url": "https://huggingface.co/papers/2503.04725",
            "abstract": "We rigorously establish a bipartite mutual information scaling law in natural language that governs long-range dependencies. This scaling law, which we show is distinct from and scales independently of the conventional two-point mutual information, is the key to understanding long-context language modeling. Using this scaling law, we formulate the Long-context Language Modeling (L^2M) condition, which relates a model's capacity for effective long context length modeling to the scaling of its latent state size for storing past information. Our results are validated through experiments on both transformers and state space models. This work establishes a theoretical foundation that guides the development of large language models toward longer context lengths.",
            "score": 2,
            "issue_id": 2582,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "51e8f1332666da31",
            "authors": [
                "Zhuo Chen",
                "Oriol Mayné i Comas",
                "Zhuotao Jin",
                "Di Luo",
                "Marin Soljačić"
            ],
            "affiliations": [
                "Harvard University",
                "Massachusetts Institute of Technology",
                "NSF AI Institute for Artificial Intelligence and Fundamental Interactions",
                "Polytechnic University of Catalonia",
                "University of California, Los Angeles"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04725.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#math",
                    "#long_context"
                ],
                "emoji": "📏",
                "ru": {
                    "title": "Масштабирование взаимной информации - ключ к моделированию длинного контекста",
                    "desc": "Статья представляет собой исследование в области обработки естественного языка, фокусирующееся на закономерностях взаимной информации в длинных текстах. Авторы формулируют условие L^2M для моделирования длинного контекста, связывающее способность модели обрабатывать длинные последовательности с масштабированием ее скрытого состояния. Результаты подтверждены экспериментами на трансформерах и моделях пространства состояний. Работа закладывает теоретическую основу для развития больших языковых моделей с увеличенной длиной контекста."
                },
                "en": {
                    "title": "Unlocking Long-Range Dependencies in Language Models",
                    "desc": "This paper introduces a new scaling law for bipartite mutual information that is crucial for understanding long-range dependencies in natural language processing. Unlike traditional two-point mutual information, this scaling law operates independently and is essential for effective long-context language modeling. The authors propose the Long-context Language Modeling (L^2M) condition, which connects a model's ability to handle long contexts with the size of its latent state for retaining past information. Experimental validation on transformers and state space models supports the theoretical framework, paving the way for advancements in large language models with extended context lengths."
                },
                "zh": {
                    "title": "长上下文建模的新法则",
                    "desc": "本文严谨地建立了自然语言中的双向互信息缩放法则，该法则控制着长距离依赖关系。我们展示了这一缩放法则与传统的两点互信息不同，并且独立缩放，是理解长上下文语言建模的关键。通过这一缩放法则，我们提出了长上下文语言建模（L^2M）条件，将模型有效建模长上下文长度的能力与其存储过去信息的潜在状态大小的缩放联系起来。我们的结果通过对变换器和状态空间模型的实验得到了验证，为大型语言模型向更长上下文长度的发展奠定了理论基础。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.03983",
            "title": "Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities",
            "url": "https://huggingface.co/papers/2503.03983",
            "abstract": "Understanding and reasoning over non-speech sounds and music are crucial for both humans and AI agents to interact effectively with their environments. In this paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM) with advanced audio understanding and reasoning capabilities. AF2 leverages (i) a custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio reasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves state-of-the-art performance with only a 3B parameter small language model, surpassing large open-source and proprietary models across over 20 benchmarks. Next, for the first time, we extend audio understanding to long audio segments (30 secs to 5 mins) and propose LongAudio, a large and novel dataset for training ALMs on long audio captioning and question-answering tasks. Fine-tuning AF2 on LongAudio leads to exceptional performance on our proposed LongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio understanding capabilities. We conduct extensive ablation studies to confirm the efficacy of our approach. Project Website: https://research.nvidia.com/labs/adlr/AF2/.",
            "score": 2,
            "issue_id": 2581,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "952e4cb72ec34df8",
            "authors": [
                "Sreyan Ghosh",
                "Zhifeng Kong",
                "Sonal Kumar",
                "S Sakshi",
                "Jaehyeon Kim",
                "Wei Ping",
                "Rafael Valle",
                "Dinesh Manocha",
                "Bryan Catanzaro"
            ],
            "affiliations": [
                "NVIDIA, Santa Clara, CA, USA",
                "University of Maryland, College Park, MD, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.03983.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#long_context",
                    "#dataset",
                    "#small_models",
                    "#reasoning",
                    "#synthetic",
                    "#open_source",
                    "#benchmark"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "AF2: Революция в понимании аудио искусственным интеллектом",
                    "desc": "Audio Flamingo 2 (AF2) - это усовершенствованная аудио-языковая модель (ALM) с продвинутыми возможностями понимания и рассуждения об аудио. Модель использует специальную модель CLAP, синтетические данные Audio QA и многоступенчатую стратегию обучения. AF2 достигает наилучших результатов среди существующих моделей на более чем 20 тестах, используя всего 3 миллиарда параметров. Кроме того, исследователи расширили возможности модели для работы с длинными аудиосегментами и создали набор данных LongAudio для обучения ALM на задачах описания и ответов на вопросы по длинным аудио."
                },
                "en": {
                    "title": "Revolutionizing Audio Understanding with Audio Flamingo 2",
                    "desc": "This paper presents Audio Flamingo 2 (AF2), an advanced Audio-Language Model (ALM) designed to enhance audio understanding and reasoning. AF2 utilizes a custom CLAP model and synthetic Audio QA data to improve fine-grained audio reasoning, along with a multi-stage curriculum learning approach. The model achieves state-of-the-art results with a relatively small 3B parameter language model, outperforming larger models on over 20 benchmarks. Additionally, it introduces LongAudio, a new dataset for training on long audio segments, and demonstrates exceptional performance on the LongAudioBench for evaluating long audio understanding."
                },
                "zh": {
                    "title": "音频理解的新突破：Audio Flamingo 2",
                    "desc": "本文介绍了Audio Flamingo 2（AF2），这是一种具有先进音频理解和推理能力的音频语言模型（ALM）。AF2利用了定制的CLAP模型、合成的音频问答数据以及多阶段的课程学习策略。AF2在仅使用一个3B参数的小型语言模型的情况下，超越了20多个基准测试中的大型开源和专有模型，达到了最先进的性能。此外，AF2首次扩展了对长音频片段（30秒到5分钟）的理解，并提出了LongAudio数据集，用于训练ALM在长音频标注和问答任务上的能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.02191",
            "title": "Understanding and Predicting Derailment in Toxic Conversations on GitHub",
            "url": "https://huggingface.co/papers/2503.02191",
            "abstract": "Software projects thrive on the involvement and contributions of individuals from different backgrounds. However, toxic language and negative interactions can hinder the participation and retention of contributors and alienate newcomers. Proactive moderation strategies aim to prevent toxicity from occurring by addressing conversations that have derailed from their intended purpose. This study aims to understand and predict conversational derailment leading to toxicity on GitHub.   To facilitate this research, we curate a novel dataset comprising 202 toxic conversations from GitHub with annotated derailment points, along with 696 non-toxic conversations as a baseline. Based on this dataset, we identify unique characteristics of toxic conversations and derailment points, including linguistic markers such as second-person pronouns, negation terms, and tones of Bitter Frustration and Impatience, as well as patterns in conversational dynamics between project contributors and external participants.   Leveraging these empirical observations, we propose a proactive moderation approach to automatically detect and address potentially harmful conversations before escalation. By utilizing modern LLMs, we develop a conversation trajectory summary technique that captures the evolution of discussions and identifies early signs of derailment. Our experiments demonstrate that LLM prompts tailored to provide summaries of GitHub conversations achieve 69% F1-Score in predicting conversational derailment, strongly improving over a set of baseline approaches.",
            "score": 1,
            "issue_id": 2581,
            "pub_date": "2025-03-04",
            "pub_date_card": {
                "ru": "4 марта",
                "en": "March 4",
                "zh": "3月4日"
            },
            "hash": "1d0bc1069249c9e1",
            "authors": [
                "Mia Mohammad Imran",
                "Robert Zita",
                "Rebekah Copeland",
                "Preetha Chatterjee",
                "Rahat Rizvi Rahman",
                "Kostadin Damevski"
            ],
            "affiliations": [
                "Drexel University, Philadelphia, PA, USA",
                "Eastern Mennonite University, Harrisonburg, VA, USA",
                "Elmhurst University, Elmhurst, IL, USA",
                "Missouri University of Science and Technology, Rolla, MO, USA",
                "Virginia Commonwealth University, Richmond, VA, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.02191.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#dataset",
                    "#multimodal",
                    "#data"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "ИИ на страже здоровой атмосферы в open-source сообществах",
                    "desc": "Исследование посвящено анализу и предотвращению токсичных взаимодействий в проектах на GitHub. Авторы создали датасет из 202 токсичных и 696 нетоксичных разговоров, выявив лингвистические маркеры и паттерны, характерные для деструктивных обсуждений. На основе этих данных была разработана система проактивной модерации с использованием больших языковых моделей (LLM). Эксперименты показали, что предложенный подход достигает 69% F1-меры в предсказании потенциально опасных разговоров на ранних стадиях."
                },
                "en": {
                    "title": "Proactive Moderation: Detecting Toxicity Before It Escalates",
                    "desc": "This paper investigates how toxic language in GitHub conversations can deter contributors and newcomers. It introduces a dataset of 202 toxic conversations with marked derailment points, alongside 696 non-toxic examples for comparison. The study identifies specific linguistic features and conversational dynamics that signal potential toxicity. Using large language models (LLMs), the authors propose a proactive moderation strategy that summarizes conversation trajectories to detect early signs of derailment, achieving a 69% F1-Score in predictions."
                },
                "zh": {
                    "title": "主动管理，防止对话偏离与有毒语言",
                    "desc": "本研究探讨了如何在GitHub上预测和理解对话偏离导致的有毒语言。我们创建了一个新数据集，包含202个有毒对话和696个非有毒对话，并标注了偏离点。通过分析这些对话的语言特征和动态模式，我们识别出有毒对话的独特特征。最后，我们提出了一种主动的管理策略，利用现代大语言模型自动检测和处理潜在的有害对话。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.20258",
            "title": "LLM as a Broken Telephone: Iterative Generation Distorts Information",
            "url": "https://huggingface.co/papers/2502.20258",
            "abstract": "As large language models are increasingly responsible for online content, concerns arise about the impact of repeatedly processing their own outputs. Inspired by the \"broken telephone\" effect in chained human communication, this study investigates whether LLMs similarly distort information through iterative generation. Through translation-based experiments, we find that distortion accumulates over time, influenced by language choice and chain complexity. While degradation is inevitable, it can be mitigated through strategic prompting techniques. These findings contribute to discussions on the long-term effects of AI-mediated information propagation, raising important questions about the reliability of LLM-generated content in iterative workflows.",
            "score": 1,
            "issue_id": 2580,
            "pub_date": "2025-02-27",
            "pub_date_card": {
                "ru": "27 февраля",
                "en": "February 27",
                "zh": "2月27日"
            },
            "hash": "5b26055854ae3d90",
            "authors": [
                "Amr Mohamed",
                "Mingmeng Geng",
                "Michalis Vazirgiannis",
                "Guokan Shang"
            ],
            "affiliations": [
                "Ecole Polytechnique",
                "MBZUAI",
                "SISSA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.20258.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#data",
                    "#long_context",
                    "#alignment",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "📞",
                "ru": {
                    "title": "Эффект 'испорченного телефона' в языковых моделях",
                    "desc": "Исследование посвящено изучению эффекта искажения информации при многократной обработке собственных выходных данных большими языковыми моделями (LLM). Авторы провели эксперименты на основе переводов, чтобы оценить накопление искажений в зависимости от выбора языка и сложности цепочки обработки. Результаты показывают, что деградация информации неизбежна, но может быть смягчена с помощью стратегических методов формулировки запросов. Исследование поднимает важные вопросы о надежности контента, генерируемого LLM в итеративных рабочих процессах."
                },
                "en": {
                    "title": "Mitigating Distortion in Iterative LLM Outputs",
                    "desc": "This paper explores how large language models (LLMs) can distort information when they repeatedly process their own outputs, similar to the 'broken telephone' effect seen in human communication. The researchers conducted translation-based experiments to observe how information degradation occurs over time, which is affected by the choice of language and the complexity of the generation chain. They found that while some level of distortion is unavoidable, it can be reduced by using specific prompting strategies. These insights highlight the potential risks of relying on LLMs for generating content in iterative processes, raising concerns about the accuracy of AI-generated information."
                },
                "zh": {
                    "title": "探讨大型语言模型的信息扭曲与可靠性",
                    "desc": "本研究探讨了大型语言模型（LLM）在反复处理自身输出时是否会扭曲信息，类似于人类沟通中的“破电话”效应。通过基于翻译的实验，我们发现信息扭曲会随着时间的推移而累积，受语言选择和链条复杂性的影响。尽管信息退化是不可避免的，但通过战略性提示技术可以减轻这种影响。研究结果为AI介导的信息传播的长期影响提供了重要见解，提出了关于LLM生成内容在迭代工作流程中可靠性的重要问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04606",
            "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
            "url": "https://huggingface.co/papers/2503.04606",
            "abstract": "Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a sim14,000times compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Keling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.",
            "score": 1,
            "issue_id": 2580,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "c635690f3edc1186",
            "authors": [
                "Aoxiong Yin",
                "Kai Shen",
                "Yichong Leng",
                "Xu Tan",
                "Xinyu Zhou",
                "Juncheng Li",
                "Siliang Tang"
            ],
            "affiliations": [
                "College of Computer Science and Technology, Zhejiang University, Hangzhou, China",
                "Moonshot AI, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04606.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#open_source",
                    "#benchmark",
                    "#long_context",
                    "#architecture",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "LanDiff: гибридный подход к генерации видео из текста",
                    "desc": "Статья представляет LanDiff - гибридную архитектуру для генерации видео из текста, сочетающую преимущества авторегрессионных языковых моделей и диффузионных моделей. LanDiff включает семантический токенизатор, языковую модель для генерации семантических токенов и потоковую диффузионную модель для уточнения деталей. Модель LanDiff объемом 5 миллиардов параметров превзошла современные открытые и коммерческие модели в бенчмарке VBench T2V. Также LanDiff показала лучшие результаты в генерации длинных видео по сравнению с другими открытыми моделями."
                },
                "en": {
                    "title": "LanDiff: Bridging Language and Visuals for Superior Video Generation",
                    "desc": "This paper introduces LanDiff, a novel framework for text-to-video (T2V) generation that combines the strengths of autoregressive language models and diffusion models. It addresses the limitations of each approach by using a coarse-to-fine generation strategy, which enhances both visual quality and semantic understanding. Key innovations include a semantic tokenizer for efficient compression of visual features, a language model that generates meaningful semantic tokens, and a streaming diffusion model that refines these tokens into high-quality videos. The results demonstrate that LanDiff outperforms existing state-of-the-art models in T2V tasks, particularly in generating long videos."
                },
                "zh": {
                    "title": "LanDiff：文本到视频生成的新突破",
                    "desc": "本文提出了一种名为LanDiff的混合框架，旨在结合自回归语言模型和扩散模型的优点，以实现文本到视频的生成。该框架通过粗到细的生成过程，克服了各自的局限性，提升了视觉质量和语义理解。LanDiff引入了三项关键创新，包括高效的语义压缩技术、生成高层语义关系的语言模型，以及将粗略语义精炼为高保真视频的流式扩散模型。实验结果表明，LanDiff在VBench T2V基准测试中表现优异，超越了现有的开源和商业模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.04378",
            "title": "Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks",
            "url": "https://huggingface.co/papers/2503.04378",
            "abstract": "Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect data for and train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3.",
            "score": 1,
            "issue_id": 2578,
            "pub_date": "2025-03-06",
            "pub_date_card": {
                "ru": "6 марта",
                "en": "March 6",
                "zh": "3月6日"
            },
            "hash": "926e56aa51a0fefe",
            "authors": [
                "Zhilin Wang",
                "Jiaqi Zeng",
                "Olivier Delalleau",
                "Daniel Egert",
                "Ellie Evans",
                "Hoo-Chang Shin",
                "Felipe Soares",
                "Yi Dong",
                "Oleksii Kuchaiev"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.04378.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#benchmark",
                    "#inference",
                    "#reasoning",
                    "#rlhf",
                    "#optimization"
                ],
                "emoji": "🔄",
                "ru": {
                    "title": "Масштабирование при выводе для открытых задач: от черновика к улучшенному ответу",
                    "desc": "Статья описывает новый метод масштабирования во время вывода для открытых задач общего домена. Авторы предлагают систему из трех моделей: одна генерирует начальный ответ, вторая дает обратную связь, а третья редактирует ответ на основе этой обратной связи. Эксперименты показывают, что такой подход позволяет значительно улучшить производительность на бенчмарке Arena Hard. При оптимальном масштабировании система на основе моделей семейства Llama 3 размером 70B достигает наилучших результатов, превосходя OpenAI o1 и DeepSeek R1."
                },
                "en": {
                    "title": "Enhancing Open-Ended Task Performance through Feedback and Editing",
                    "desc": "This paper discusses a novel approach to improve inference-time scaling in machine learning models, particularly for open-ended tasks. It introduces a system where one model generates an initial response, a second model provides feedback, and a third model edits the response based on that feedback. This method allows for more flexible applications beyond traditional tasks that require verifiable answers, such as math or coding. The authors demonstrate that by optimizing the number of drafts, feedback, and edits, their model achieves state-of-the-art performance on the Arena Hard benchmark, outperforming previous models."
                },
                "zh": {
                    "title": "推理时间扩展：提升开放性任务的性能",
                    "desc": "本文探讨了推理时间扩展在机器学习模型中的重要性，尤其是OpenAI o1和DeepSeek R1等模型。许多现有技术要求任务的答案可验证，这限制了它们在开放性任务中的应用。我们借鉴人类如何进行初步尝试、请求反馈并根据反馈进行改进的过程，开发了专门的反馈和编辑模型。通过优化初始响应草稿、有效反馈和编辑响应的数量，我们的模型在Arena Hard基准测试中达到了92.7的最新性能，超越了其他模型。"
                }
            }
        }
    ],
    "link_prev": "2025-03-06.html",
    "link_next": "2025-03-10.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "06.03",
        "en": "03/06",
        "zh": "3月6日"
    },
    "short_date_next": {
        "ru": "10.03",
        "en": "03/10",
        "zh": "3月10日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 3,
        "#benchmark": 8,
        "#agents": 2,
        "#cv": 0,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 5,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 4,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 7,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 2,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "大型语言模型（LLMs）彻底改变了自然语言处理（NLP），但开源的多语言LLMs仍然稀缺，现有模型通常语言覆盖有限。这些模型通常优先考虑资源丰富的语言，而忽略了广泛使用但资源匮乏的语言。为了解决这一差距，我们介绍了Babel，一个开放的多语言LLM，涵盖了按使用人数排名前25的语言，支持超过90%的全球人口，并包括许多其他开放多语言LLMs忽略的语言。与传统的继续预训练方法不同，Babel通过一种层扩展技术增加其参数数量，从而提高了Babel的性能上限。我们引入了两个变体：Babel-9B，用于高效推理和微调，以及Babel-83B，为开放多语言LLMs设定了新标准。广泛的多语言任务评估证明了其优越的性能。此外，使用开源监督微调数据集，Babel取得了显著的性能，Babel-9B-Chat在10B大小的LLMs中领先，Babel-83B-Chat在多语言任务中设定了新标准，达到了商业模型的水平。",
        "title": "Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers",
        "pinyin": "Dàxíng yǔyán móxíng (LLMs) chèdǐ gǎibiànle zìrán yǔyán chǔlǐ (NLP), dàn kāiyuán de duōyǔyán LLMs réngrán xīquē, xiànyǒu móxíng tōngcháng yǔyán fúgài yǒuxiàn. Zhèxiē móxíng tōngcháng yōuxiān kǎolǜ zīyuán fēngfù de yǔyán, ér hūlüe le guǎngfàn shǐyòng dàn zīyuán kuìfá de yǔyán. Wèile jiějué zhè yī chājù, wǒmen jièshào le Babel, yīgè kāifàng de duōyǔyán LLM, hànhuòle àn shǐyòng rénshù páimíng qián 25 de yǔyán, zhīchí chāoguò 90% de quánqiú rénkǒu, bìng bāokuò xǔduō qítā kāifàng duōyǔyán LLMs hūlüe de yǔyán. Yǔ chuántǒng de jìxù yùxùn fāngfǎ bùtóng, Babel tōngguò yīzhǒng céng kuòzhǎn jìshù zēngjiā qí cānshù shùliàng, dàngrán tígāole Babel de xìngnéng shàngxiàn. Wǒmen yǐnrùle liǎnggè biàntǐ: Babel-9B, yòngyú gāoxiào tuīlǐ hé wēitiáo, yǐjiǎ Babel-83B, wèi kāifàng duōyǔyán LLMs shèdìngle xīn biāozhǔn. Guǎngfàn de duōyǔyán rènwù pínggū zhèngmíngle qí yōubiè de xìngnéng. Cǐwài, shǐyòng kāiyuán jiànshǐ wēitiáo shùjújí, Babel quèdéle xiǎnzhù de xìngnéng, Babel-9B-Chat zài 10B dàxìng de LLMs zhōng lǐngxiān, Babel-83B-Chat zài duōyǔyán rènwù zhōng shèdìngle xīn biāozhǔn, dále shāngyè móxíng de shuǐpíng.",
        "vocab": "[\n    {\"word\": \"大型\", \"pinyin\": \"dàxíng\", \"trans\": \"large-scale\"},\n    {\"word\": \"彻底\", \"pinyin\": \"chèdǐ\", \"trans\": \"thoroughly\"},\n    {\"word\": \"自然语言处理\", \"pinyin\": \"zìrán yǔyán chǔlǐ\", \"trans\": \"Natural Language Processing\"},\n    {\"word\": \"稀缺\", \"pinyin\": \"xīquē\", \"trans\": \"scarce\"},\n    {\"word\": \"覆盖\", \"pinyin\": \"fùgài\", \"trans\": \"cover\"},\n    {\"word\": \"有限\", \"pinyin\": \"yǒuxiàn\", \"trans\": \"limited\"},\n    {\"word\": \"优先\", \"pinyin\": \"yōuxiān\", \"trans\": \"prioritize\"},\n    {\"word\": \"资源\", \"pinyin\": \"zīyuán\", \"trans\": \"resources\"},\n    {\"word\": \"丰富\", \"pinyin\": \"fēngfù\", \"trans\": \"abundant\"},\n    {\"word\": \"匮乏\", \"pinyin\": \"kuìfá\", \"trans\": \"scarce\"},\n    {\"word\": \"差距\", \"pinyin\": \"chājù\", \"trans\": \"gap\"},\n    {\"word\": \"介绍\", \"pinyin\": \"jièshào\", \"trans\": \"introduce\"},\n    {\"word\": \"涵盖\", \"pinyin\": \"hángài\", \"trans\": \"cover\"},\n    {\"word\": \"按\", \"pinyin\": \"àn\", \"trans\": \"according to\"},\n    {\"word\": \"排名\", \"pinyin\": \"páimíng\", \"trans\": \"ranking\"},\n    {\"word\": \"支持\", \"pinyin\": \"zhīchí\", \"trans\": \"support\"},\n    {\"word\": \"全球\", \"pinyin\": \"quánqiú\", \"trans\": \"global\"},\n    {\"word\": \"人口\", \"pinyin\": \"rénkǒu\", \"trans\": \"population\"},\n    {\"word\": \"继续\", \"pinyin\": \"jìxù\", \"trans\": \"continue\"},\n    {\"word\": \"预训练\", \"pinyin\": \"yù xùnliàn\", \"trans\": \"pre-training\"},\n    {\"word\": \"方法\", \"pinyin\": \"fāngfǎ\", \"trans\": \"method\"},\n    {\"word\": \"层\", \"pinyin\": \"céng\", \"trans\": \"layer\"},\n    {\"word\": \"扩展\", \"pinyin\": \"kuòzhǎn\", \"trans\": \"expand\"},\n    {\"word\": \"技术\", \"pinyin\": \"jìshù\", \"trans\": \"technology\"},\n    {\"word\": \"参数\", \"pinyin\": \"cānshǔ\", \"trans\": \"parameters\"},\n    {\"word\": \"数量\", \"pinyin\": \"shùliàng\", \"trans\": \"quantity\"},\n    {\"word\": \"提高\", \"pinyin\": \"tígāo\", \"trans\": \"improve\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìngnéng\", \"trans\": \"performance\"},\n    {\"word\": \"上限\", \"pinyin\": \"shàngxiàn\", \"trans\": \"upper limit\"},\n    {\"word\": \"引入\", \"pinyin\": \"yǐnrù\", \"trans\": \"introduce\"},\n    {\"word\": \"变体\", \"pinyin\": \"biàntǐ\", \"trans\": \"variants\"},\n    {\"word\": \"高效\", \"pinyin\": \"gāoxiào\", \"trans\": \"efficient\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuīlǐ\", \"trans\": \"inference\"},\n    {\"word\": \"微调\", \"pinyin\": \"wēitiáo\", \"trans\": \"fine-tuning\"},\n    {\"word\": \"设定\", \"pinyin\": \"shèdìng\", \"trans\": \"set\"},\n    {\"word\": \"标准\", \"pinyin\": \"biāozhǔn\", \"trans\": \"standard\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"evaluation\"},\n    {\"word\": \"证明\", \"pinyin\": \"zhèngmíng\", \"trans\": \"prove\"},\n    {\"word\": \"优越\", \"pinyin\": \"yōuyuè\", \"trans\": \"superior\"},\n    {\"word\": \"监督\", \"pinyin\": \"jiàndū\", \"trans\": \"supervised\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shùjù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎnzhù\", \"trans\": \"significant\"},\n    {\"word\": \"领先\", \"pinyin\": \"lǐngxiān\", \"trans\": \"lead\"},\n    {\"word\": \"商业\", \"pinyin\": \"shāngyè\", \"trans\": \"commercial\"},\n    {\"word\": \"水平\", \"pinyin\": \"shuǐpíng\", \"trans\": \"level\"}\n]",
        "trans": "Large language models (LLMs) have revolutionized natural language processing (NLP), but open-source multilingual LLMs remain scarce, with existing models often having limited language coverage. These models typically prioritize resource-rich languages while neglecting widely used but resource-scarce languages. To address this gap, we introduce Babel, an open multilingual LLM that covers the top 25 languages by number of speakers, supporting over 90% of the global population and including many languages overlooked by other open multilingual LLMs. Unlike traditional continued pre-training methods, Babel enhances its parameter count through a layer expansion technique, raising Babel's performance ceiling. We introduce two variants: Babel-9B for efficient inference and fine-tuning, and Babel-83B, setting a new standard for open multilingual LLMs. Extensive multilingual task evaluations demonstrate its superior performance. Additionally, using open-source supervised fine-tuning datasets, Babel achieves significant performance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat setting new standards in multilingual tasks, reaching the level of commercial models.",
        "update_ts": "2025-03-06 09:11"
    }
}