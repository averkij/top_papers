
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 11 papers. April 3.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">3 апреля</span> | <span id="title-articles-count">11 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-04-02.html">⬅️ <span id="prev-date">02.04</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-04-04.html">➡️ <span id="next-date">04.04</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-04.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'};
        let feedDateNext = {'ru': '04.04', 'en': '04/04', 'zh': '4月4日'};
        let feedDatePrev = {'ru': '02.04', 'en': '04/02', 'zh': '4月2日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.00883', 'title': 'Improved Visual-Spatial Reasoning via R1-Zero-Like Training', 'url': 'https://huggingface.co/papers/2504.00883', 'abstract': 'Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.', 'score': 29, 'issue_id': 3041, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '43fba84cc49adfad', 'authors': ['Zhenyi Liao', 'Qingsong Xie', 'Yanhao Zhang', 'Zijian Kong', 'Haonan Lu', 'Zhenyu Yang', 'Zhijie Deng'], 'affiliations': ['OPPO AI Center', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00883.jpg', 'data': {'categories': ['#training', '#dataset', '#optimization', '#video', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в визуально-пространственном мышлении мультимодальных ИИ', 'desc': 'Это исследование посвящено улучшению визуально-пространственного мышления мультимодальных больших языковых моделей (MLLM) с помощью обучения, подобного R1-Zero. Авторы обнаружили, что способности к визуально-пространственному мышлению у небольших и средних моделей Qwen2-VL не могут быть активированы с помощью подсказок цепочки рассуждений (CoT). Они применили обучение GRPO с использованием тщательно подобранного набора данных VSI-100k для улучшения визуально-пространственного мышления. В результате их модель vsGRPO-7B, дообученная на основе Qwen2-VL-7B, достигла производительности, сравнимой с лучшей моделью с открытым исходным кодом LLaVA-NeXT-Video-72B.'}, 'en': {'title': 'Enhancing Visual-Spatial Reasoning in MLLMs with GRPO Training', 'desc': 'This paper focuses on enhancing the visual-spatial reasoning abilities of multi-modal large language models (MLLMs), which are crucial for AI agents interacting with the physical world. The authors introduce a novel training method called GRPO, applied to the Qwen2-VL models, to improve their reasoning capabilities using a specially curated dataset named VSI-100k. They discover that traditional Chain of Thought prompts are ineffective for activating these reasoning skills in smaller models. The results show that their fine-tuned models significantly outperform baseline models, demonstrating the effectiveness of their approach in advancing visual-spatial intelligence in MLLMs.'}, 'zh': {'title': '提升多模态模型的视觉空间推理能力', 'desc': '本研究聚焦于提升多模态大型语言模型（MLLMs）的视觉空间推理能力，特别是在视频基础的视觉空间智能（VSI）方面。我们发现小到中型的Qwen2-VL模型无法通过思维链（CoT）提示激活其视觉空间推理能力，因此引入了GRPO训练方法，并使用精心策划的VSI-100k数据集进行改进。经过120个GPU小时的训练，我们的vsGRPO-2B模型在性能上超越了基础模型12.1%，并且vsGRPO-7B模型的表现与最佳开源模型LLaVA-NeXT-Video-72B相当。我们的研究表明，保持KL惩罚在GRPO中是必要的，并且vsGRPO在与监督微调和直接偏好优化基线的比较中表现出明显的优势。'}}}, {'id': 'https://huggingface.co/papers/2504.00999', 'title': 'MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization', 'url': 'https://huggingface.co/papers/2504.00999', 'abstract': 'Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at https://apexgen-x.github.io/MergeVQ.', 'score': 28, 'issue_id': 3040, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'bb6506ffd72aed19', 'authors': ['Siyuan Li', 'Luyuan Zhang', 'Zedong Wang', 'Juanxi Tian', 'Cheng Tan', 'Zicheng Liu', 'Chang Yu', 'Qingsong Xie', 'Haonan Lu', 'Haoqian Wang', 'Zhen Lei'], 'affiliations': ['CAIR, HKISI-CAS', 'MAIS CASIA', 'OPPO AI Center', 'The Hong Kong University of Science and Technology', 'Tsinghua University', 'University of Chinese Academy of Sciences', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00999.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'MergeVQ: Эффективное объединение генерации изображений и обучения представлений', 'desc': 'Статья представляет новый метод MergeVQ, объединяющий маскированное моделирование изображений (MIM) с векторным квантованием (VQ) для улучшения генерации изображений и обучения представлений. MergeVQ использует технику слияния токенов в кодировщике для эффективного квантования и глобального выравнивания, а также кросс-внимание в декодере для восстановления деталей. Для генерации второго этапа предлагается MergeAR, выполняющий сжатие KV-кэша для эффективного предсказания в растровом порядке. Эксперименты на ImageNet показывают, что MergeVQ достигает конкурентоспособных результатов в задачах обучения визуальных представлений и генерации изображений, сохраняя высокую эффективность токенов и скорость вывода.'}, 'en': {'title': 'MergeVQ: Bridging Image Generation and Representation Learning Efficiently', 'desc': 'This paper introduces MergeVQ, a novel approach that enhances Masked Image Modeling (MIM) using Vector Quantization (VQ) techniques. It addresses the challenge of balancing image generation quality with efficient representation learning by integrating token merging into the VQ framework. During the pre-training phase, MergeVQ utilizes a token merge module to separate high-level semantics from the latent space, allowing for improved quantization and detail recovery. The second stage, MergeAR, optimizes the generation process through KV Cache compression, resulting in a model that excels in both visual representation and image generation while ensuring efficiency in token usage and inference speed.'}, 'zh': {'title': 'MergeVQ：提升图像生成与表示学习的统一模型', 'desc': '本文提出了一种新的模型MergeVQ，旨在改善基于向量量化的图像生成和视觉表示学习之间的平衡。通过在编码器中引入令牌合并技术，MergeVQ能够在自注意力块后解耦潜在空间中的语义，从而提高生成质量和表示学习的效率。该模型在预训练阶段通过交叉注意力恢复细节，并在生成阶段使用KV缓存压缩来提高预测效率。实验结果表明，MergeVQ在视觉表示学习和图像生成任务中表现出色，同时保持了良好的令牌效率和推理速度。'}}}, {'id': 'https://huggingface.co/papers/2504.01014', 'title': 'AnimeGamer: Infinite Anime Life Simulation with Next Game State\n  Prediction', 'url': 'https://huggingface.co/papers/2504.01014', 'abstract': 'Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at https://github.com/TencentARC/AnimeGamer.', 'score': 13, 'issue_id': 3041, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '98efa783105c3173', 'authors': ['Junhao Cheng', 'Yuying Ge', 'Yixiao Ge', 'Jing Liao', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'City University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.01014.jpg', 'data': {'categories': ['#diffusion', '#games', '#multimodal', '#video'], 'emoji': '🎮', 'ru': {'title': 'AnimeGamer: погружение в интерактивный мир аниме через языковые инструкции', 'desc': 'AnimeGamer - это новый подход к созданию интерактивных игр с персонажами аниме, использующий мультимодальные языковые модели (MLLM). Система генерирует динамические анимационные кадры, отображающие движения персонажей и изменения их состояний, на основе текстовых диалогов. AnimeGamer вводит новые мультимодальные представления с учетом действий, которые декодируются в видеоклипы высокого качества с помощью диффузионной модели. Метод превосходит существующие подходы по различным аспектам игрового опыта, что подтверждается автоматическими метриками и оценками людей.'}, 'en': {'title': 'Transforming Anime into Interactive Gaming with Dynamic AI', 'desc': 'This paper presents AnimeGamer, a novel approach to infinite anime life simulation games that utilizes Multimodal Large Language Models (MLLMs). Unlike previous methods, AnimeGamer incorporates historical visual context to ensure consistent gameplay and generates dynamic animations rather than just static images. By employing action-aware multimodal representations, it can create high-quality video clips that reflect character movements and state changes. The results show that AnimeGamer significantly enhances the gaming experience compared to existing techniques, as validated by both automated and human evaluations.'}, 'zh': {'title': 'AnimeGamer：动态互动的无限动漫游戏体验', 'desc': '最近在图像和视频合成方面的进展为生成游戏带来了新的希望。本文提出了一种名为AnimeGamer的方法，利用多模态大语言模型生成动态动画镜头，以增强游戏的互动性和沉浸感。通过引入动作感知的多模态表示，AnimeGamer能够生成具有上下文一致性和动态变化的游戏状态。实验结果表明，AnimeGamer在游戏体验的各个方面优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2504.00824', 'title': 'ScholarCopilot: Training Large Language Models for Academic Writing with\n  Accurate Citations', 'url': 'https://huggingface.co/papers/2504.00824', 'abstract': "Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their capacity to adequately support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], and then utilizes its representation to look up relevant citations from a database. The retrieved references are fed into the model to augment the generation process. We jointly optimize both the generation and citation tasks within a single framework to increase efficiency. Trained on 500K papers from arXiv, our model achieves a top-1 retrieval accuracy of 40.1% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality (measured across relevance, coherence, academic rigor, completeness, and innovation), surpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct (15.8/25). Human studies also confirm ScholarCopilot's superior performance in citation recall, writing efficiency, and overall user experience, confirming the effectiveness of our approach.", 'score': 9, 'issue_id': 3040, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'b135f3f003dcaaff', 'authors': ['Yubo Wang', 'Xueguang Ma', 'Ping Nie', 'Huaye Zeng', 'Zhiheng Lyu', 'Yuxuan Zhang', 'Benjamin Schneider', 'Yi Lu', 'Xiang Yue', 'Wenhu Chen'], 'affiliations': ['Carnegie Mellon University', 'Independent Researcher', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2504.00824.jpg', 'data': {'categories': ['#science', '#dataset', '#rag', '#multimodal', '#alignment'], 'emoji': '🎓', 'ru': {'title': 'ScholarCopilot: ИИ-помощник для профессионального академического письма', 'desc': 'ScholarCopilot - это унифицированная система, улучшающая существующие большие языковые модели для генерации профессиональных академических статей с точными и контекстуально релевантными цитатами. Она динамически определяет, когда извлекать научные ссылки, генерируя токен [RET], и использует его представление для поиска релевантных цитат в базе данных. ScholarCopilot совместно оптимизирует задачи генерации и цитирования в рамках единой системы для повышения эффективности. Обученная на 500 тысячах статей из arXiv, модель достигает точности извлечения top-1 в 40.1% на оценочном наборе данных, превосходя базовые модели.'}, 'en': {'title': 'Enhancing Academic Writing with ScholarCopilot', 'desc': 'This paper presents ScholarCopilot, a new framework that improves large language models for generating academic articles with accurate citations. It uses a retrieval token to decide when to fetch scholarly references, enhancing the text generation process with relevant citations. The model is trained on a large dataset of academic papers and shows significant improvements in both citation accuracy and writing quality compared to existing models. Human evaluations further validate its effectiveness in citation recall and overall user experience.'}, 'zh': {'title': 'ScholarCopilot：提升学术写作的智能助手', 'desc': '本研究提出了ScholarCopilot，一个统一框架，旨在提升大型语言模型在生成专业学术文章时的准确性和相关性。该系统通过生成检索标记[RET]，动态决定何时检索学术参考文献，并利用其表示从数据库中查找相关引用。ScholarCopilot在生成和引用任务上进行联合优化，以提高效率。经过在500K篇arXiv论文上的训练，该模型在评估数据集上实现了40.1%的顶级检索准确率，且在学术写作样本的生成质量上超越了参数更多的模型。'}}}, {'id': 'https://huggingface.co/papers/2504.01956', 'title': 'VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in\n  One Step', 'url': 'https://huggingface.co/papers/2504.01956', 'abstract': 'Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts that do not align with real-world geometry structure. In this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications. Project Page: https://hanyang-21.github.io/VideoScene', 'score': 6, 'issue_id': 3042, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '44f1db8ef8cc244a', 'authors': ['Hanyang Wang', 'Fangfu Liu', 'Jiawei Chi', 'Yueqi Duan'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01956.jpg', 'data': {'categories': ['#video', '#diffusion', '#3d'], 'emoji': '🎬', 'ru': {'title': 'VideoScene: Эффективная генерация 3D сцен из видео с помощью дистилляции диффузионных моделей', 'desc': 'Статья представляет VideoScene - метод для эффективной генерации трехмерных сцен из разреженных видов. Авторы предлагают стратегию дистилляции 3D-aware leap flow для преодоления избыточной информации и обучения динамической политики шумоподавления. VideoScene достигает более быстрых и качественных результатов генерации 3D сцен по сравнению с предыдущими моделями диффузии видео. Этот подход открывает новые возможности для приложений преобразования видео в 3D.'}, 'en': {'title': 'Efficient 3D Scene Generation from Sparse Views Using Video Diffusion Models', 'desc': 'This paper addresses the challenge of reconstructing 3D scenes from sparse views, which is often complicated by the lack of visual information. Traditional methods have limitations, particularly when input views have minimal overlap, leading to degraded performance. The authors introduce VideoScene, a novel approach that utilizes video diffusion models to efficiently generate 3D scenes in a single step. By implementing a 3D-aware leap flow distillation strategy and a dynamic denoising policy network, VideoScene significantly improves the speed and quality of 3D scene generation compared to existing methods.'}, 'zh': {'title': '高效生成3D场景的VideoScene', 'desc': '从稀疏视图恢复3D场景是一项具有挑战性的任务，因为它本质上是一个不适定的问题。传统方法通过几何正则化或前馈确定性模型等专门解决方案来缓解这一问题，但在输入视图重叠较少且视觉信息不足时，性能仍然下降。最近的视频生成模型显示出解决这一挑战的潜力，能够生成具有合理3D结构的视频片段。本文提出了VideoScene，通过视频扩散模型提炼生成3D场景，设计了3D感知的跃迁流蒸馏策略，以提高生成效率和效果。'}}}, {'id': 'https://huggingface.co/papers/2504.01934', 'title': 'ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and\n  Diffusion Refinement', 'url': 'https://huggingface.co/papers/2504.01934', 'abstract': 'We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: https://illume-unified-mllm.github.io/.', 'score': 6, 'issue_id': 3042, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': 'a50e19f04d94405f', 'authors': ['Runhui Huang', 'Chunwei Wang', 'Junwei Yang', 'Guansong Lu', 'Yunlong Yuan', 'Jianhua Han', 'Lu Hou', 'Wei Zhang', 'Lanqing Hong', 'Hengshuang Zhao', 'Hang Xu'], 'affiliations': ['Huawei Noahs Ark Lab', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.01934.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#cv', '#training', '#games', '#multimodal', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'ILLUME+: Унифицированная мультимодальная модель нового поколения', 'desc': 'ILLUME+ - это новая мультимодальная языковая модель, использующая двойную визуальную токенизацию и диффузионный декодер. Она улучшает глубокое семантическое понимание и высококачественную генерацию изображений. ILLUME+ вводит унифицированный двойной визуальный токенизатор DualViTok, сохраняющий как мелкие текстуры, так и семантику, согласованную с текстом. Модель демонстрирует конкурентоспособную производительность в задачах мультимодального понимания, генерации и редактирования по сравнению с существующими унифицированными MLLM и специализированными моделями.'}, 'en': {'title': 'ILLUME+: Unifying Understanding, Generation, and Editing in One Model', 'desc': 'ILLUME+ is a novel model that combines dual visual tokenization and a diffusion decoder to enhance deep semantic understanding and high-quality image generation. Unlike previous models that struggled with understanding, generation, and editing simultaneously, ILLUME+ effectively integrates these capabilities through its innovative DualViTok tokenizer. This approach preserves fine textures and aligns semantics, allowing for better image editing and generation. With a continuous-input, discrete-output framework and progressive training, ILLUME+ achieves competitive performance in multimodal tasks, paving the way for future applications.'}, 'zh': {'title': 'ILLUME+: 多模态理解与生成的新突破', 'desc': 'ILLUME+ 是一种新型的多模态学习模型，结合了双重视觉标记和扩散解码器，旨在提升深层语义理解和高保真图像生成能力。与现有的统一模型相比，ILLUME+ 能够更好地处理理解、生成和编辑这三种基本能力。通过引入 DualViTok，ILLUME+ 保留了细致的纹理和文本对齐的语义，同时采用粗到细的图像表示策略，增强了多模态理解和生成的能力。此外，ILLUME+ 在图像生成质量和超分辨率方面表现出色，展现了与现有模型的竞争力，为未来的多模态应用奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2504.01308', 'title': 'Safeguarding Vision-Language Models: Mitigating Vulnerabilities to\n  Gaussian Noise in Perturbation-based Attacks', 'url': 'https://huggingface.co/papers/2504.01308', 'abstract': 'Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.', 'score': 6, 'issue_id': 3040, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '315938d70f25095e', 'authors': ['Jiawei Wang', 'Yushen Zuo', 'Yuanjun Chai', 'Zhendong Liu', 'Yichen Fu', 'Yichun Feng', 'Kin-man Lam'], 'affiliations': ['Nanjing University', 'Stanford University', 'The Hong Kong Polytechnic University', 'University of Science and Technology of China', 'University of Washington', 'University of the Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2504.01308.jpg', 'data': {'categories': ['#security', '#cv', '#diffusion', '#training', '#dataset', '#optimization', '#multimodal'], 'emoji': '🛡️', 'ru': {'title': 'Защита визуально-языковых моделей от атак с помощью шумовой аугментации', 'desc': 'Исследователи обнаружили уязвимость визуально-языковых моделей (VLM) к атакам с использованием зашумленных или искаженных изображений. Для решения этой проблемы они предложили метод Robust-VLGuard, включающий набор данных для обучения и дообучение с аугментацией шумом. Также был разработан метод DiffPure-VLM, использующий диффузионные модели для преобразования состязательных возмущений в гауссовский шум. Эксперименты показали, что предложенные методы значительно повышают устойчивость VLM к различным типам атак.'}, 'en': {'title': 'Strengthening Vision-Language Models Against Noise Attacks', 'desc': 'This paper addresses the vulnerabilities of Vision-Language Models (VLMs) to jailbreak attacks, particularly when they encounter noisy or corrupted images. The authors highlight that existing security measures during training do not adequately account for noise-augmented visual inputs, leading to significant security gaps. To combat this issue, they introduce Robust-VLGuard, a dataset designed for multimodal safety that includes both aligned and misaligned image-text pairs, along with a noise-augmented fine-tuning process. Additionally, they propose DiffPure-VLM, which uses diffusion models to transform adversarial perturbations into Gaussian-like noise, enhancing the robustness of VLMs against such attacks while maintaining their functionality.'}, 'zh': {'title': '增强视觉语言模型的安全性', 'desc': '视觉语言模型（VLMs）通过结合视觉信息扩展了大型语言模型（LLMs）的能力，但在处理噪声或损坏的图像时仍然容易受到攻击。现有的VLMs在训练过程中采取了安全措施以减轻这些攻击，但对噪声增强视觉输入的脆弱性却未给予足够重视。我们提出了Robust-VLGuard，这是一个多模态安全数据集，结合了对齐和不对齐的图像-文本对，并通过噪声增强的微调来降低攻击成功率，同时保持VLM的功能。实验结果表明，扩散模型的分布转移特性与我们微调的VLMs很好地对齐，显著减轻了不同强度的对抗扰动。'}}}, {'id': 'https://huggingface.co/papers/2504.01848', 'title': "PaperBench: Evaluating AI's Ability to Replicate AI Research", 'url': 'https://huggingface.co/papers/2504.01848', 'abstract': "We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We https://github.com/openai/preparedness{open-source our code} to facilitate future research in understanding the AI engineering capabilities of AI agents.", 'score': 5, 'issue_id': 3041, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '60923777325e85cc', 'authors': ['Giulio Starace', 'Oliver Jaffe', 'Dane Sherburn', 'James Aung', 'Jun Shern Chan', 'Leon Maksin', 'Rachel Dias', 'Evan Mays', 'Benjamin Kinsella', 'Wyatt Thompson', 'Johannes Heidecke', 'Amelia Glaese', 'Tejal Patwardhan'], 'affiliations': ['OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2504.01848.jpg', 'data': {'categories': ['#open_source', '#agents', '#benchmark', '#survey'], 'emoji': '🧠', 'ru': {'title': 'PaperBench: измеряем способность ИИ воспроизводить передовые исследования', 'desc': 'PaperBench - это новый бенчмарк для оценки способности ИИ-агентов воспроизводить современные исследования в области искусственного интеллекта. Он включает в себя 20 статей из ICML 2024, которые агенты должны воспроизвести с нуля, включая понимание вклада статьи, разработку кодовой базы и успешное выполнение экспериментов. Для объективной оценки разработаны рубрики, которые иерархически разбивают каждую задачу на подзадачи с четкими критериями оценки. Лучший протестированный агент, Claude 3.5 Sonnet (New) с открытым исходным кодом, достиг среднего балла воспроизведения 21.0%.'}, 'en': {'title': "Evaluating AI's Research Replication Skills with PaperBench", 'desc': "This paper presents PaperBench, a benchmark designed to assess AI agents' ability to replicate advanced AI research. The benchmark includes 20 selected papers from ICML 2024, requiring agents to comprehend contributions, create a codebase, and conduct experiments. To ensure objective evaluation, the authors developed detailed rubrics that break down replication tasks into smaller, graded components, totaling 8,316 tasks. The study also introduces an LLM-based judge for automated grading and compares the performance of AI agents against human experts, revealing that current models still lag behind human capabilities."}, 'zh': {'title': 'PaperBench：评估AI复制研究能力的基准', 'desc': '我们介绍了PaperBench，这是一个评估人工智能代理复制最先进AI研究能力的基准。代理需要从头开始复制20篇ICML 2024的亮点和口头论文，包括理解论文贡献、开发代码库和成功执行实验。为了进行客观评估，我们开发了分层的评分标准，将每个复制任务分解为更小的子任务，并设定明确的评分标准。我们的评估还包括使用基于大型语言模型的评审者自动评分，并与顶尖的机器学习博士进行比较，发现目前的模型尚未超越人类基线。'}}}, {'id': 'https://huggingface.co/papers/2504.01724', 'title': 'DreamActor-M1: Holistic, Expressive and Robust Human Image Animation\n  with Hybrid Guidance', 'url': 'https://huggingface.co/papers/2504.01724', 'abstract': 'While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/.', 'score': 3, 'issue_id': 3041, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': 'd59102a274145730', 'authors': ['Yuxuan Luo', 'Zhengkun Rong', 'Lizhen Wang', 'Longhao Zhang', 'Tianshu Hu', 'Yongming Zhu'], 'affiliations': ['Bytedance Intelligent Creation'], 'pdf_title_img': 'assets/pdf/title_img/2504.01724.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#3d', '#diffusion'], 'emoji': '🎭', 'ru': {'title': 'Реалистичная анимация человека с точным контролем мимики и движений', 'desc': 'DreamActor-M1 - это новая модель на основе диффузионного трансформера для анимации человека по изображениям. Она использует гибридные сигналы управления, включающие неявные лицевые представления, 3D-сферы головы и 3D-скелеты тела для точного контроля мимики и движений. Модель применяет прогрессивное обучение на данных разного масштаба для адаптации к различным позам и ракурсам. DreamActor-M1 также интегрирует паттерны движения из последовательных кадров с визуальными ориентирами для обеспечения долгосрочной согласованности анимации.'}, 'en': {'title': 'DreamActor-M1: Revolutionizing Human Animation with Robust Control and Consistency', 'desc': 'The paper presents DreamActor-M1, a novel framework that utilizes a diffusion transformer (DiT) to enhance human animation by addressing key limitations in existing methods. It introduces hybrid control signals that combine facial representations, 3D head spheres, and body skeletons to improve the expressiveness and control of animations. The framework also employs a progressive training strategy to adapt to various body poses and image scales, ensuring versatility in generating animations from portraits to full-body views. Additionally, it integrates motion patterns from sequential frames to maintain long-term temporal coherence, resulting in more robust and visually appealing animations.'}, 'zh': {'title': '突破动画生成的局限性，DreamActor-M1引领新潮流！', 'desc': '本文提出了一种基于扩散变换器的框架DreamActor-M1，旨在解决现有图像基础的人体动画方法在细粒度整体可控性、多尺度适应性和长期时间一致性方面的不足。通过混合控制信号，结合隐式面部表示、3D头部球体和3D身体骨架，实现了对面部表情和身体动作的强大控制，同时保持动画的表现力和身份一致性。为了适应不同的身体姿势和图像尺度，采用了渐进训练策略，使用不同分辨率和尺度的数据进行训练。实验结果表明，该方法在肖像、上半身和全身生成方面优于现有的最先进技术，具有强大的长期一致性。'}}}, {'id': 'https://huggingface.co/papers/2504.01204', 'title': 'Articulated Kinematics Distillation from Video Diffusion Models', 'url': 'https://huggingface.co/papers/2504.01204', 'abstract': 'We present Articulated Kinematics Distillation (AKD), a framework for generating high-fidelity character animations by merging the strengths of skeleton-based animation and modern generative models. AKD uses a skeleton-based representation for rigged 3D assets, drastically reducing the Degrees of Freedom (DoFs) by focusing on joint-level control, which allows for efficient, consistent motion synthesis. Through Score Distillation Sampling (SDS) with pre-trained video diffusion models, AKD distills complex, articulated motions while maintaining structural integrity, overcoming challenges faced by 4D neural deformation fields in preserving shape consistency. This approach is naturally compatible with physics-based simulation, ensuring physically plausible interactions. Experiments show that AKD achieves superior 3D consistency and motion quality compared with existing works on text-to-4D generation. Project page: https://research.nvidia.com/labs/dir/akd/', 'score': 3, 'issue_id': 3041, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'c8630e7ca691cef3', 'authors': ['Xuan Li', 'Qianli Ma', 'Tsung-Yi Lin', 'Yongxin Chen', 'Chenfanfu Jiang', 'Ming-Yu Liu', 'Donglai Xiang'], 'affiliations': ['NVIDIA', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2504.01204.jpg', 'data': {'categories': ['#diffusion', '#games', '#3d'], 'emoji': '🦾', 'ru': {'title': 'Революция в анимации персонажей: от скелета к реалистичному движению', 'desc': 'Articulated Kinematics Distillation (AKD) - это новый подход к генерации высококачественной анимации персонажей, объединяющий скелетную анимацию и современные генеративные модели. AKD использует скелетное представление для 3D-моделей, значительно уменьшая количество степеней свободы за счет фокуса на управлении суставами. Метод применяет Score Distillation Sampling с предобученными видео-диффузионными моделями для синтеза сложных движений, сохраняя структурную целостность. AKD показывает превосходные результаты по сравнению с существующими методами генерации текста в 4D анимацию.'}, 'en': {'title': 'Revolutionizing Character Animation with AKD', 'desc': 'Articulated Kinematics Distillation (AKD) is a new framework designed to create realistic character animations by combining skeleton-based animation techniques with advanced generative models. It simplifies the animation process by using a skeleton representation, which reduces the Degrees of Freedom (DoFs) and allows for better control over joint movements. AKD employs Score Distillation Sampling (SDS) with pre-trained video diffusion models to generate complex motions while ensuring that the shapes of the characters remain consistent. This method also integrates well with physics-based simulations, resulting in animations that are not only visually appealing but also physically plausible.'}, 'zh': {'title': '关节运动蒸馏：高保真动画的新方法', 'desc': '本文提出了一种名为关节运动蒸馏（AKD）的框架，用于生成高保真角色动画。AKD通过使用基于骨骼的表示，显著减少了自由度，专注于关节级控制，从而实现高效且一致的运动合成。通过与预训练的视频扩散模型结合的得分蒸馏采样（SDS），AKD能够蒸馏复杂的关节运动，同时保持结构完整性，克服了4D神经变形场在保持形状一致性方面的挑战。实验表明，AKD在3D一致性和运动质量上优于现有的文本到4D生成方法。'}}}, {'id': 'https://huggingface.co/papers/2503.23135', 'title': 'LSNet: See Large, Focus Small', 'url': 'https://huggingface.co/papers/2503.23135', 'abstract': "Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose a ``See Large, Focus Small'' strategy for lightweight vision network design. We introduce LS (Large-Small) convolution, which combines large-kernel perception and small-kernel aggregation. It can efficiently capture a wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, a new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks. Codes and models are available at https://github.com/jameslahm/lsnet.", 'score': 3, 'issue_id': 3040, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': 'd2ac65a2356c89c3', 'authors': ['Ao Wang', 'Hui Chen', 'Zijia Lin', 'Jungong Han', 'Guiguang Ding'], 'affiliations': ['BNRist, Tsinghua University', 'Department of Automation, Tsinghua University', 'School of Software, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.23135.jpg', 'data': {'categories': ['#architecture', '#training', '#cv'], 'emoji': '👁️', 'ru': {'title': "LSNet: Эффективное компьютерное зрение по принципу 'Смотри широко, фокусируйся узко'", 'desc': "Статья представляет новый подход к проектированию легковесных нейронных сетей для компьютерного зрения, вдохновленный человеческой системой зрения. Авторы предлагают стратегию 'Смотри широко, фокусируйся узко' и вводят LS-свертку, сочетающую восприятие с большим ядром и агрегацию с малым ядром. На основе LS-свертки разработано семейство моделей LSNet, которое эффективно обрабатывает визуальную информацию. Эксперименты показывают, что LSNet превосходит существующие легковесные сети по производительности и эффективности в различных задачах компьютерного зрения."}, 'en': {'title': 'See Large, Focus Small: Efficient Vision Networks', 'desc': "This paper addresses the challenges of deploying complex vision networks like Convolutional Neural Networks and Vision Transformers in real-time applications due to their heavy computations. The authors propose a new lightweight network design strategy called 'See Large, Focus Small', inspired by the human vision system's ability to dynamically adjust focus. They introduce LS convolution, which effectively combines large-kernel perception for broad information capture and small-kernel aggregation for precise feature refinement. The resulting LSNet model demonstrates improved performance and efficiency in various vision tasks compared to existing lightweight networks."}, 'zh': {'title': '轻量级视觉网络的新策略：大视野，小聚焦', 'desc': '本文提出了一种新的轻量级视觉网络设计策略，称为“See Large, Focus Small”。该策略结合了大核感知和小核聚合的LS卷积，能够高效捕捉广泛的感知信息并实现精确的特征聚合。通过这种方法，LSNet在多种视觉任务中展现出优越的性能和效率，克服了现有轻量级模型在计算预算有限情况下的局限性。实验结果表明，LSNet在实时应用中表现出色，适合实际部署。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (1)', '#agi', '#alignment (1)', '#architecture (3)', '#audio', '#benchmark (2)', '#cv (5)', '#data', '#dataset (3)', '#diffusion (6)', '#ethics', '#games (3)', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (5)', '#open_source (1)', '#optimization (4)', '#plp', '#rag (1)', '#reasoning (1)', '#rl', '#rlhf', '#robotics', '#science (1)', '#security (1)', '#small_models', '#story_generation', '#survey (1)', '#synthetic', '#training (6)', '#transfer_learning', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-04-03 04:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-03 04:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-03 04:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    