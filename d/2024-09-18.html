
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 16 papers. September 18.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }        
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">18 сентября</span> | <span id="title-articles-count">16 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-09-17.html">⬅️ <span id="prev-date">17.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-09-19.html">➡️ <span id="next-date">19.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-09.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'};
        let feedDateNext = {'ru': '19.09', 'en': '09/19', 'zh': '9月19日'};
        let feedDatePrev = {'ru': '17.09', 'en': '09/17', 'zh': '9月17日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2409.11340', 'title': 'OmniGen: Unified Image Generation', 'url': 'https://huggingface.co/papers/2409.11340', 'abstract': "In this work, we introduce OmniGen, a new diffusion model for unified image generation. Unlike popular diffusion models (e.g., Stable Diffusion), OmniGen no longer requires additional modules such as ControlNet or IP-Adapter to process diverse control conditions. OmniGenis characterized by the following features: 1) Unification: OmniGen not only demonstrates text-to-image generation capabilities but also inherently supports other downstream tasks, such as image editing, subject-driven generation, and visual-conditional generation. Additionally, OmniGen can handle classical computer vision tasks by transforming them into image generation tasks, such as edge detection and human pose recognition. 2) Simplicity: The architecture of OmniGen is highly simplified, eliminating the need for additional text encoders. Moreover, it is more user-friendly compared to existing diffusion models, enabling complex tasks to be accomplished through instructions without the need for extra preprocessing steps (e.g., human pose estimation), thereby significantly simplifying the workflow of image generation. 3) Knowledge Transfer: Through learning in a unified format, OmniGen effectively transfers knowledge across different tasks, manages unseen tasks and domains, and exhibits novel capabilities. We also explore the model's reasoning capabilities and potential applications of chain-of-thought mechanism. This work represents the first attempt at a general-purpose image generation model, and there remain several unresolved issues. We will open-source the related resources at https://github.com/VectorSpaceLab/OmniGen to foster advancements in this field.", 'score': 106, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': 'f00584cb183c5a7a', 'data': {'categories': ['#reasoning', '#cv', '#transfer_learning', '#open_source', '#diffusion', '#architecture', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'OmniGen: универсальная модель для всех задач генерации изображений', 'desc': 'OmniGen - это новая диффузионная модель для универсальной генерации изображений. Она объединяет возможности текст-в-изображение, редактирования изображений и другие задачи без необходимости дополнительных модулей. Архитектура OmniGen упрощена и не требует отдельных текстовых энкодеров. Модель эффективно переносит знания между задачами и демонстрирует способности к рассуждению.'}, 'en': {'title': 'OmniGen: Simplifying Unified Image Generation for All Tasks', 'desc': "OmniGen is a novel diffusion model designed for unified image generation, capable of handling various tasks without needing extra modules. It simplifies the process by integrating text-to-image generation with other functionalities like image editing and classical computer vision tasks. The model's architecture is streamlined, allowing users to perform complex tasks through simple instructions, thus enhancing usability. Additionally, OmniGen facilitates knowledge transfer across different tasks, showcasing its potential for reasoning and adaptability in unseen domains."}, 'zh': {'title': 'OmniGen：统一图像生成的新纪元', 'desc': '本文介绍了一种新的扩散模型OmniGen，用于统一的图像生成。与流行的扩散模型不同，OmniGen不再需要额外的模块来处理多样的控制条件。OmniGen具有统一性、简化性和知识转移等特点，能够支持文本到图像生成、图像编辑等多种下游任务，并且简化了用户操作流程。该模型的首次尝试为通用图像生成模型，未来将开源相关资源以促进该领域的发展。'}}}, {'id': 'https://huggingface.co/papers/2409.11402', 'title': 'NVLM: Open Frontier-Class Multimodal LLMs', 'url': 'https://huggingface.co/papers/2409.11402', 'abstract': 'We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. In terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets. Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities. To advance research in the field, we are releasing the model weights and will open-source the code for the community: https://nvlm-project.github.io/.', 'score': 71, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': '48599eb4efe8218f', 'data': {'categories': ['#reasoning', '#dataset', '#cv', '#training', '#math', '#plp', '#benchmark', '#games', '#open_source', '#architecture', '#synthetic', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'NVLM 1.0: Новый рубеж в мультимодальном ИИ', 'desc': 'NVLM 1.0 - это новое семейство мультимодальных больших языковых моделей (LLM), достигающих передовых результатов в задачах связи зрения и языка. Модель использует новую архитектуру, сочетающую преимущества декодер-только и кросс-внимание подходов, а также вводит 1D тайл-тегирование для обработки изображений высокого разрешения. Исследователи обнаружили, что качество и разнообразие данных важнее их объема даже на этапе предобучения. NVLM 1.0 демонстрирует улучшенную производительность в текстовых задачах по сравнению с базовой LLM после мультимодального обучения.'}, 'en': {'title': 'NVLM 1.0: Redefining Multimodal Language Models for Superior Performance', 'desc': 'The NVLM 1.0 is a new family of multimodal large language models that excel in tasks involving both vision and language, achieving top results compared to existing models. It demonstrates enhanced performance in text-only tasks after being trained with multimodal data. The paper compares different model architectures and proposes a new design that improves training efficiency and reasoning abilities. Additionally, it emphasizes the importance of high-quality datasets over sheer scale, and the authors plan to share their model and code with the research community.'}, 'zh': {'title': 'NVLM 1.0：多模态语言模型的新突破', 'desc': '我们介绍了NVLM 1.0，这是一种前沿的多模态大型语言模型，能够在视觉-语言任务上取得最先进的成果。NVLM 1.0在多模态训练后，文本性能显著提升，超越了其基础的语言模型。我们比较了仅解码器的多模态模型和基于交叉注意力的模型，提出了一种新架构，提升了训练效率和多模态推理能力。此外，我们精心策划了多模态预训练和监督微调数据集，发现数据集质量和任务多样性比规模更为重要。'}}}, {'id': 'https://huggingface.co/papers/2409.11355', 'title': 'Fine-Tuning Image-Conditional Diffusion Models is Easier than You Think', 'url': 'https://huggingface.co/papers/2409.11355', 'abstract': 'Recent work showed that large diffusion models can be reused as highly precise monocular depth estimators by casting depth estimation as an image-conditional image generation task. While the proposed model achieved state-of-the-art results, high computational demands due to multi-step inference limited its use in many scenarios. In this paper, we show that the perceived inefficiency was caused by a flaw in the inference pipeline that has so far gone unnoticed. The fixed model performs comparably to the best previously reported configuration while being more than 200times faster. To optimize for downstream task performance, we perform end-to-end fine-tuning on top of the single-step model with task-specific losses and get a deterministic model that outperforms all other diffusion-based depth and normal estimation models on common zero-shot benchmarks. We surprisingly find that this fine-tuning protocol also works directly on Stable Diffusion and achieves comparable performance to current state-of-the-art diffusion-based depth and normal estimation models, calling into question some of the conclusions drawn from prior works.', 'score': 28, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': '4d17730da7f1e732', 'data': {'categories': ['#cv', '#training', '#inference', '#optimization', '#benchmark', '#diffusion'], 'emoji': '🔍', 'ru': {'title': 'Революционное ускорение оценки глубины изображений с помощью диффузионных моделей', 'desc': 'Исследователи обнаружили, что большие диффузионные модели можно использовать для точной оценки глубины изображений. Они выявили и исправили недостаток в процессе вывода, что позволило ускорить работу модели более чем в 200 раз без потери качества. Применив дообучение с учетом специфики задачи, они получили детерминированную модель, превосходящую аналоги в оценке глубины и нормалей. Неожиданно, такой подход оказался эффективным и для Stable Diffusion, что ставит под сомнение выводы предыдущих работ.'}, 'en': {'title': 'Revolutionizing Depth Estimation: Fast and Efficient Diffusion Models', 'desc': "This paper addresses the inefficiencies in using large diffusion models for monocular depth estimation, which were previously thought to require multi-step inference. The authors identify a flaw in the inference pipeline that, when corrected, allows for a single-step model that is over 200 times faster while maintaining competitive performance. They also introduce an end-to-end fine-tuning approach that enhances the model's performance on specific tasks, surpassing existing diffusion-based models in zero-shot benchmarks. Additionally, the findings challenge previous assumptions about the capabilities of diffusion models in depth and normal estimation tasks."}, 'zh': {'title': '提升深度估计效率的突破性进展', 'desc': '最近的研究表明，大型扩散模型可以作为高精度的单目深度估计器，通过将深度估计视为图像条件的图像生成任务来实现。尽管所提出的模型达到了最先进的结果，但由于多步推理的高计算需求，限制了其在许多场景中的使用。本文揭示了推理流程中的一个未被注意的缺陷，导致了感知上的低效率。经过修正的模型在性能上与之前报告的最佳配置相当，但速度提高了200倍以上，并且通过端到端的微调，进一步优化了下游任务的表现。'}}}, {'id': 'https://huggingface.co/papers/2409.11406', 'title': 'Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion', 'url': 'https://huggingface.co/papers/2409.11406', 'abstract': 'In 3D modeling, designers often use an existing 3D model as a reference to create new ones. This practice has inspired the development of Phidias, a novel generative model that uses diffusion for reference-augmented 3D generation. Given an image, our method leverages a retrieved or user-provided 3D reference model to guide the generation process, thereby enhancing the generation quality, generalization ability, and controllability. Our model integrates three key components: 1) meta-ControlNet that dynamically modulates the conditioning strength, 2) dynamic reference routing that mitigates misalignment between the input image and 3D reference, and 3) self-reference augmentations that enable self-supervised training with a progressive curriculum. Collectively, these designs result in a clear improvement over existing methods. Phidias establishes a unified framework for 3D generation using text, image, and 3D conditions with versatile applications.', 'score': 25, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': '67298e05fdb45375', 'data': {'categories': ['#cv', '#training', '#games', '#diffusion', '#architecture', '#3d'], 'emoji': '🗿', 'ru': {'title': 'Phidias: Новый уровень 3D-генерации с использованием референсов', 'desc': 'Phidias - это новая генеративная модель для создания 3D-объектов с использованием диффузии и опорных 3D-моделей. Модель использует изображение и 3D-референс для улучшения качества генерации, обобщающей способности и управляемости. Ключевые компоненты включают мета-ControlNet, динамическую маршрутизацию референсов и самореференсные аугментации. Phidias предоставляет унифицированный фреймворк для 3D-генерации с использованием текста, изображений и 3D-условий.'}, 'en': {'title': 'Phidias: Enhancing 3D Generation with Smart References', 'desc': 'Phidias is a new generative model designed for creating 3D models by using existing 3D references. It employs a diffusion process to enhance the quality and control of the generated models based on input images. The model features three main components: a meta-ControlNet for adjusting conditioning strength, dynamic reference routing to align images with 3D references, and self-reference augmentations for self-supervised training. Overall, Phidias improves upon previous methods and offers a flexible framework for 3D generation using various input types.'}, 'zh': {'title': 'Phidias：增强三维生成的新方法', 'desc': '在三维建模中，设计师常常使用现有的三维模型作为参考来创建新的模型。Phidias是一种新颖的生成模型，它利用扩散技术进行参考增强的三维生成。该方法通过结合检索到的或用户提供的三维参考模型，来指导生成过程，从而提高生成质量、泛化能力和可控性。Phidias建立了一个统一的框架，可以使用文本、图像和三维条件进行三维生成，具有多种应用。'}}}, {'id': 'https://huggingface.co/papers/2409.11136', 'title': 'Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models', 'url': 'https://huggingface.co/papers/2409.11136', 'abstract': 'Instruction-tuned language models (LM) are able to respond to imperative commands, providing a more natural user interface compared to their base counterparts. In this work, we present Promptriever, the first retrieval model able to be prompted like an LM. To train Promptriever, we curate and release a new instance-level instruction training set from MS MARCO, spanning nearly 500k instances. Promptriever not only achieves strong performance on standard retrieval tasks, but also follows instructions. We observe: (1) large gains (reaching SoTA) on following detailed relevance instructions (+14.3 p-MRR / +3.1 nDCG on FollowIR), (2) significantly increased robustness to lexical choices/phrasing in the query+instruction (+12.9 Robustness@10 on InstructIR), and (3) the ability to perform hyperparameter search via prompting to reliably improve retrieval performance (+1.4 average increase on BEIR). Promptriever demonstrates that retrieval models can be controlled with prompts on a per-query basis, setting the stage for future work aligning LM prompting techniques with information retrieval.', 'score': 21, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': 'e070d3d767ca4cff', 'data': {'categories': ['#dataset', '#training', '#rag', '#instruction_tuning', '#retrieval', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'Promptriever: Революция в управлении поисковыми моделями', 'desc': 'Исследователи представили Promptriever - первую модель поиска, способную работать с командами, как языковая модель. Для обучения Promptriever был создан новый набор данных из MS MARCO, содержащий около 500 тысяч примеров с инструкциями. Модель демонстрирует высокую производительность в стандартных задачах поиска и способность следовать инструкциям, значительно повышая устойчивость к лексическому выбору в запросах. Promptriever открывает новые возможности для управления моделями поиска с помощью промптов для каждого запроса.'}, 'en': {'title': 'Prompting the Future of Information Retrieval with Promptriever', 'desc': 'This paper introduces Promptriever, a novel retrieval model that can be prompted similarly to language models (LMs). It is trained on a new dataset derived from MS MARCO, which includes nearly 500,000 instances of instruction-based queries. Promptriever shows significant improvements in retrieval tasks, achieving state-of-the-art results when following detailed relevance instructions and demonstrating enhanced robustness to variations in query phrasing. Additionally, it can optimize its performance through hyperparameter tuning via prompts, paving the way for integrating LM prompting methods into information retrieval systems.'}, 'zh': {'title': 'Promptriever：指令驱动的检索模型', 'desc': '本论文介绍了一种名为Promptriever的检索模型，它能够像语言模型一样响应指令。我们从MS MARCO中整理并发布了一个新的实例级指令训练集，包含近50万个实例。Promptriever在标准检索任务上表现出色，并且能够有效地遵循详细的相关性指令。研究表明，Promptriever在检索性能上有显著提升，并且能够通过提示进行超参数搜索，从而进一步提高检索效果。'}}}, {'id': 'https://huggingface.co/papers/2409.10819', 'title': 'EzAudio: Enhancing Text-to-Audio Generation with Efficient Diffusion Transformer', 'url': 'https://huggingface.co/papers/2409.10819', 'abstract': 'Latent diffusion models have shown promising results in text-to-audio (T2A) generation tasks, yet previous models have encountered difficulties in generation quality, computational cost, diffusion sampling, and data preparation. In this paper, we introduce EzAudio, a transformer-based T2A diffusion model, to handle these challenges. Our approach includes several key innovations: (1) We build the T2A model on the latent space of a 1D waveform Variational Autoencoder (VAE), avoiding the complexities of handling 2D spectrogram representations and using an additional neural vocoder. (2) We design an optimized diffusion transformer architecture specifically tailored for audio latent representations and diffusion modeling, which enhances convergence speed, training stability, and memory usage, making the training process easier and more efficient. (3) To tackle data scarcity, we adopt a data-efficient training strategy that leverages unlabeled data for learning acoustic dependencies, audio caption data annotated by audio-language models for text-to-audio alignment learning, and human-labeled data for fine-tuning. (4) We introduce a classifier-free guidance (CFG) rescaling method that simplifies EzAudio by achieving strong prompt alignment while preserving great audio quality when using larger CFG scores, eliminating the need to struggle with finding the optimal CFG score to balance this trade-off. EzAudio surpasses existing open-source models in both objective metrics and subjective evaluations, delivering realistic listening experiences while maintaining a streamlined model structure, low training costs, and an easy-to-follow training pipeline. Code, data, and pre-trained models are released at: https://haidog-yaqub.github.io/EzAudio-Page/.', 'score': 17, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': 'dc5844f3b0b10c50', 'data': {'categories': ['#audio', '#training', '#data', '#open_source', '#diffusion', '#architecture', '#synthetic'], 'emoji': '🎵', 'ru': {'title': 'EzAudio: простая и эффективная генерация аудио по тексту', 'desc': 'EzAudio - это новая модель генерации аудио по тексту на основе латентной диффузии. Она использует трансформер для работы с латентным пространством 1D VAE, что упрощает обработку аудио. Модель применяет оптимизированную архитектуру и стратегию обучения с использованием размеченных и неразмеченных данных. EzAudio превосходит существующие открытые модели по объективным и субъективным оценкам.'}, 'en': {'title': 'EzAudio: Simplifying Text-to-Audio Generation with Efficiency and Quality', 'desc': 'This paper presents EzAudio, a transformer-based model designed for text-to-audio (T2A) generation, addressing issues like generation quality and computational efficiency. By utilizing a latent space from a 1D waveform Variational Autoencoder (VAE), EzAudio simplifies the process of audio generation without the need for complex spectrograms. The model features an optimized diffusion transformer architecture that improves training stability and reduces memory usage, making it more efficient. Additionally, it employs a data-efficient training strategy and a classifier-free guidance method to enhance audio quality and prompt alignment, outperforming existing models in both objective and subjective evaluations.'}, 'zh': {'title': 'EzAudio：高效的文本到音频生成模型', 'desc': '本论文介绍了一种新的文本到音频生成模型EzAudio，旨在解决现有模型在生成质量、计算成本和数据准备方面的挑战。我们采用了一维波形变分自编码器（VAE）的潜在空间来构建模型，避免了处理二维声谱图的复杂性。通过优化的扩散变换器架构，我们提高了收敛速度和训练稳定性，同时降低了内存使用。我们还采用了一种数据高效的训练策略，利用无标签数据和人类标注数据来增强模型的学习能力。'}}}, {'id': 'https://huggingface.co/papers/2409.11055', 'title': 'A Comprehensive Evaluation of Quantized Instruction-Tuned Large Language Models: An Experimental Analysis up to 405B', 'url': 'https://huggingface.co/papers/2409.11055', 'abstract': 'Prior research works have evaluated quantized LLMs using limited metrics such as perplexity or a few basic knowledge tasks and old datasets. Additionally, recent large-scale models such as Llama 3.1 with up to 405B have not been thoroughly examined. This paper evaluates the performance of instruction-tuned LLMs across various quantization methods (GPTQ, AWQ, SmoothQuant, and FP8) on models ranging from 7B to 405B. Using 13 benchmarks, we assess performance across six task types: commonsense Q\\&A, knowledge and language understanding, instruction following, hallucination detection, mathematics, and dialogue. Our key findings reveal that (1) quantizing a larger LLM to a similar size as a smaller FP16 LLM generally performs better across most benchmarks, except for hallucination detection and instruction following; (2) performance varies significantly with different quantization methods, model size, and bit-width, with weight-only methods often yielding better results in larger models; (3) task difficulty does not significantly impact accuracy degradation due to quantization; and (4) the MT-Bench evaluation method has limited discriminatory power among recent high-performing LLMs.', 'score': 16, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': 'bb004f38b9982a21', 'data': {'categories': ['#hallucinations', '#training', '#inference', '#optimization', '#benchmark', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Квантование больших языковых моделей: больше - не всегда лучше', 'desc': 'Статья посвящена оценке производительности квантованных языковых моделей (LLM) с использованием различных методов квантования. Авторы провели обширное исследование на моделях размером от 7B до 405B параметров, используя 13 бенчмарков для оценки шести типов задач. Ключевые выводы показывают, что квантование большей модели до размера меньшей обычно дает лучшие результаты, но производительность сильно зависит от метода квантования, размера модели и битовой ширины. Исследование также выявило ограниченную дискриминационную способность метода оценки MT-Bench для современных высокопроизводительных LLM.'}, 'en': {'title': 'Unlocking the Power of Quantized Large Language Models', 'desc': 'This paper investigates the performance of instruction-tuned large language models (LLMs) when subjected to various quantization techniques, including GPTQ, AWQ, SmoothQuant, and FP8. It evaluates models ranging from 7 billion to 405 billion parameters across 13 diverse benchmarks, covering tasks like commonsense Q&A and dialogue. The findings indicate that larger quantized models often outperform smaller FP16 models, although performance varies with quantization methods and model sizes. Additionally, the study highlights that task difficulty does not significantly affect accuracy loss due to quantization, and the MT-Bench evaluation method may not effectively differentiate between high-performing LLMs.'}, 'zh': {'title': '量化方法对大型语言模型性能的全面评估', 'desc': '本论文评估了不同量化方法对指令调优的大型语言模型（LLM）的性能，包括GPTQ、AWQ、SmoothQuant和FP8。我们使用13个基准测试，涵盖了常识问答、知识和语言理解、指令跟随、幻觉检测、数学和对话等六种任务类型。研究发现，量化较大的LLM通常在大多数基准测试中表现优于相同大小的较小FP16 LLM，尽管在幻觉检测和指令跟随任务中表现有所不同。此外，不同的量化方法、模型大小和位宽对性能的影响显著，而任务难度对量化引起的准确性下降影响不大。'}}}, {'id': 'https://huggingface.co/papers/2409.11367', 'title': 'OSV: One Step is Enough for High-Quality Image to Video Generation', 'url': 'https://huggingface.co/papers/2409.11367', 'abstract': 'Video diffusion models have shown great potential in generating high-quality videos, making them an increasingly popular focus. However, their inherent iterative nature leads to substantial computational and time costs. While efforts have been made to accelerate video diffusion by reducing inference steps (through techniques like consistency distillation) and GAN training (these approaches often fall short in either performance or training stability). In this work, we introduce a two-stage training framework that effectively combines consistency distillation with GAN training to address these challenges. Additionally, we propose a novel video discriminator design, which eliminates the need for decoding the video latents and improves the final performance. Our model is capable of producing high-quality videos in merely one-step, with the flexibility to perform multi-step refinement for further performance enhancement. Our quantitative evaluation on the OpenWebVid-1M benchmark shows that our model significantly outperforms existing methods. Notably, our 1-step performance(FVD 171.15) exceeds the 8-step performance of the consistency distillation based method, AnimateLCM (FVD 184.79), and approaches the 25-step performance of advanced Stable Video Diffusion (FVD 156.94).', 'score': 13, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': 'b0d563ca10cd945c', 'data': {'categories': ['#video', '#training', '#optimization', '#benchmark', '#diffusion', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Революция в генерации видео: качество за один шаг', 'desc': 'В этой статье представлена новая двухэтапная система обучения для ускорения видео-диффузионных моделей. Авторы объединяют дистилляцию консистентности с GAN-обучением и предлагают новый дизайн видео-дискриминатора. Модель способна генерировать качественные видео за один шаг, с возможностью многошагового уточнения. Количественная оценка на бенчмарке OpenWebVid-1M показывает значительное превосходство над существующими методами.'}, 'en': {'title': 'Revolutionizing Video Generation: One-Step High-Quality Output!', 'desc': 'This paper presents a new approach to video generation using diffusion models, which are known for their ability to create high-quality videos. The authors introduce a two-stage training framework that merges consistency distillation with Generative Adversarial Network (GAN) training, aiming to reduce the computational costs associated with video generation. A key innovation is the design of a video discriminator that avoids the need to decode video latents, enhancing performance. The proposed model achieves impressive results, generating high-quality videos in just one step while also allowing for optional multi-step refinement, outperforming existing methods on benchmark evaluations.'}, 'zh': {'title': '高效视频生成的新突破', 'desc': '视频扩散模型在生成高质量视频方面展现了巨大的潜力，但其迭代特性导致了较高的计算和时间成本。本文提出了一种两阶段训练框架，有效结合了一致性蒸馏和GAN训练，以解决这些挑战。我们还设计了一种新的视频鉴别器，省去了对视频潜变量的解码，提升了最终性能。我们的模型能够在仅一步中生成高质量视频，并具备多步精细化的灵活性，显著超越了现有方法。'}}}, {'id': 'https://huggingface.co/papers/2409.10568', 'title': 'On the limits of agency in agent-based models', 'url': 'https://huggingface.co/papers/2409.10568', 'abstract': "Agent-based modeling (ABM) seeks to understand the behavior of complex systems by simulating a collection of agents that act and interact within an environment. Their practical utility requires capturing realistic environment dynamics and adaptive agent behavior while efficiently simulating million-size populations. Recent advancements in large language models (LLMs) present an opportunity to enhance ABMs by using LLMs as agents with further potential to capture adaptive behavior. However, the computational infeasibility of using LLMs for large populations has hindered their widespread adoption. In this paper, we introduce AgentTorch -- a framework that scales ABMs to millions of agents while capturing high-resolution agent behavior using LLMs. We benchmark the utility of LLMs as ABM agents, exploring the trade-off between simulation scale and individual agency. Using the COVID-19 pandemic as a case study, we demonstrate how AgentTorch can simulate 8.4 million agents representing New York City, capturing the impact of isolation and employment behavior on health and economic outcomes. We compare the performance of different agent architectures based on heuristic and LLM agents in predicting disease waves and unemployment rates. Furthermore, we showcase AgentTorch's capabilities for retrospective, counterfactual, and prospective analyses, highlighting how adaptive agent behavior can help overcome the limitations of historical data in policy design. AgentTorch is an open-source project actively being used for policy-making and scientific discovery around the world. The framework is available here: github.com/AgentTorch/AgentTorch.", 'score': 12, 'issue_id': 1, 'pub_date': '2024-09-14', 'pub_date_card': {'ru': '14 сентября', 'en': 'September 14', 'zh': '9月14日'}, 'hash': '14749474dc3c2b04', 'data': {'categories': ['#science', '#training', '#agi', '#healthcare', '#agents', '#benchmark', '#open_source', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'AgentTorch: Миллионы умных агентов для моделирования сложных систем', 'desc': 'AgentTorch - это фреймворк для масштабирования агентного моделирования (АМ) до миллионов агентов с использованием больших языковых моделей (LLM). Он позволяет симулировать сложные системы, сочетая реалистичную динамику среды и адаптивное поведение агентов. На примере пандемии COVID-19 в Нью-Йорке авторы демонстрируют возможности AgentTorch для ретроспективного, контрфактического и перспективного анализа. Фреймворк исследует компромисс между масштабом симуляции и индивидуальным поведением агентов, используя LLM.'}, 'en': {'title': 'Scaling Agent-Based Models with Language Intelligence', 'desc': 'This paper presents AgentTorch, a framework designed to enhance agent-based modeling (ABM) by integrating large language models (LLMs) as agents. It addresses the challenge of simulating millions of agents while maintaining realistic behaviors and interactions in complex environments. The framework is demonstrated through a case study on the COVID-19 pandemic, simulating 8.4 million agents to analyze health and economic outcomes. AgentTorch not only benchmarks the performance of LLMs against traditional agent architectures but also supports various analytical approaches for effective policy-making.'}, 'zh': {'title': 'AgentTorch：大规模代理建模的新突破', 'desc': '代理基础建模（ABM）通过模拟一组在环境中行动和互动的代理，来理解复杂系统的行为。本文提出了AgentTorch框架，能够将ABM扩展到数百万个代理，同时利用大型语言模型（LLM）捕捉高分辨率的代理行为。我们以COVID-19疫情为案例，展示了AgentTorch如何模拟840万名代表纽约市的代理，分析隔离和就业行为对健康和经济结果的影响。AgentTorch是一个开源项目，正在全球范围内用于政策制定和科学发现。'}}}, {'id': 'https://huggingface.co/papers/2409.10923', 'title': 'Agile Continuous Jumping in Discontinuous Terrains', 'url': 'https://huggingface.co/papers/2409.10923', 'abstract': 'We focus on agile, continuous, and terrain-adaptive jumping of quadrupedal robots in discontinuous terrains such as stairs and stepping stones. Unlike single-step jumping, continuous jumping requires accurately executing highly dynamic motions over long horizons, which is challenging for existing approaches. To accomplish this task, we design a hierarchical learning and control framework, which consists of a learned heightmap predictor for robust terrain perception, a reinforcement-learning-based centroidal-level motion policy for versatile and terrain-adaptive planning, and a low-level model-based leg controller for accurate motion tracking. In addition, we minimize the sim-to-real gap by accurately modeling the hardware characteristics. Our framework enables a Unitree Go1 robot to perform agile and continuous jumps on human-sized stairs and sparse stepping stones, for the first time to the best of our knowledge. In particular, the robot can cross two stair steps in each jump and completes a 3.5m long, 2.8m high, 14-step staircase in 4.5 seconds. Moreover, the same policy outperforms baselines in various other parkour tasks, such as jumping over single horizontal or vertical discontinuities. Experiment videos can be found at https://yxyang.github.io/jumping\\_cod/.', 'score': 11, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': 'ae280e28c063dda1', 'data': {'categories': ['#rl', '#optimization', '#games', '#robotics', '#3d'], 'emoji': '🦿', 'ru': {'title': 'Прыжки роботов: от симуляции к реальности', 'desc': 'Статья представляет иерархическую систему обучения и управления для прыжков четвероногих роботов на сложных поверхностях. Система включает нейросеть для предсказания рельефа местности, политику движения на основе обучения с подкреплением и низкоуровневый контроллер для точного отслеживания движений. Авторы минимизировали разрыв между симуляцией и реальностью, точно моделируя характеристики оборудования. Разработанная система позволила роботу Unitree Go1 выполнять непрерывные прыжки по лестнице и редким опорным точкам, превзойдя существующие методы в различных паркур-задачах.'}, 'en': {'title': 'Agile Jumping: Quadrupedal Robots Conquering Complex Terrains!', 'desc': 'This paper presents a novel approach for enabling quadrupedal robots to perform agile and continuous jumps over complex terrains like stairs and stepping stones. The authors introduce a hierarchical learning and control framework that includes a heightmap predictor for terrain perception, a reinforcement learning policy for adaptive motion planning, and a model-based leg controller for precise movement execution. By effectively bridging the simulation-to-reality gap, the framework allows the Unitree Go1 robot to achieve impressive jumping capabilities, such as crossing multiple stair steps in a single jump. The results demonstrate significant improvements over existing methods in various parkour tasks, showcasing the potential for advanced robotic agility in challenging environments.'}, 'zh': {'title': '四足机器人：敏捷跳跃的新突破', 'desc': '本文研究了四足机器人在不连续地形（如楼梯和跳石）上的敏捷、连续和适应性跳跃。我们提出了一个分层学习和控制框架，包括一个用于稳健地形感知的高度图预测器、基于强化学习的质心级运动策略以及一个低级模型驱动的腿部控制器。通过准确建模硬件特性，我们减少了模拟与现实之间的差距。我们的框架使Unitree Go1机器人首次能够在楼梯和跳石上进行敏捷的连续跳跃，展示了其在多种障碍物跳跃任务中的优越性能。'}}}, {'id': 'https://huggingface.co/papers/2409.11211', 'title': 'SplatFields: Neural Gaussian Splats for Sparse 3D and 4D Reconstruction', 'url': 'https://huggingface.co/papers/2409.11211', 'abstract': 'Digitizing 3D static scenes and 4D dynamic events from multi-view images has long been a challenge in computer vision and graphics. Recently, 3D Gaussian Splatting (3DGS) has emerged as a practical and scalable reconstruction method, gaining popularity due to its impressive reconstruction quality, real-time rendering capabilities, and compatibility with widely used visualization tools. However, the method requires a substantial number of input views to achieve high-quality scene reconstruction, introducing a significant practical bottleneck. This challenge is especially severe in capturing dynamic scenes, where deploying an extensive camera array can be prohibitively costly. In this work, we identify the lack of spatial autocorrelation of splat features as one of the factors contributing to the suboptimal performance of the 3DGS technique in sparse reconstruction settings. To address the issue, we propose an optimization strategy that effectively regularizes splat features by modeling them as the outputs of a corresponding implicit neural field. This results in a consistent enhancement of reconstruction quality across various scenarios. Our approach effectively handles static and dynamic cases, as demonstrated by extensive testing across different setups and scene complexities.', 'score': 8, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': 'cacfe3e60ddde42f', 'data': {'categories': ['#cv', '#graphs', '#optimization', '#architecture', '#3d'], 'emoji': '🌟', 'ru': {'title': 'Улучшение 3D реконструкции с помощью неявных нейронных полей', 'desc': 'Эта статья представляет новый подход к улучшению метода 3D Gaussian Splatting (3DGS) для реконструкции трехмерных сцен. Авторы предлагают стратегию оптимизации, которая регуляризирует признаки сплатов, моделируя их как выходы соответствующего неявного нейронного поля. Данный метод позволяет улучшить качество реконструкции при использовании меньшего количества входных изображений. Подход эффективно работает как для статических, так и для динамических сцен, что подтверждается обширными тестами.'}, 'en': {'title': 'Enhancing 3D Reconstruction with Implicit Neural Fields', 'desc': 'This paper addresses the challenges of reconstructing 3D static scenes and 4D dynamic events from multiple images. It highlights the limitations of the 3D Gaussian Splatting (3DGS) method, particularly its need for many input views to produce high-quality results. The authors propose a new optimization strategy that improves the performance of 3DGS by regularizing splat features through an implicit neural field model. Their approach shows significant improvements in reconstruction quality for both static and dynamic scenes, making it more effective in various scenarios.'}, 'zh': {'title': '提升3D重建质量的新策略', 'desc': '本文探讨了从多视角图像中数字化三维静态场景和四维动态事件的挑战。我们提出了一种优化策略，通过将3D高斯点特征建模为隐式神经场的输出，来改善稀疏重建中的性能。该方法有效地正则化了特征，提升了重建质量，适用于静态和动态场景。实验结果表明，我们的方法在不同设置和场景复杂性下均表现出一致的质量提升。'}}}, {'id': 'https://huggingface.co/papers/2409.11733', 'title': 'Human-like Affective Cognition in Foundation Models', 'url': 'https://huggingface.co/papers/2409.11733', 'abstract': "Understanding emotions is fundamental to human interaction and experience. Humans easily infer emotions from situations or facial expressions, situations from emotions, and do a variety of other affective cognition. How adept is modern AI at these inferences? We introduce an evaluation framework for testing affective cognition in foundation models. Starting from psychological theory, we generate 1,280 diverse scenarios exploring relationships between appraisals, emotions, expressions, and outcomes. We evaluate the abilities of foundation models (GPT-4, Claude-3, Gemini-1.5-Pro) and humans (N = 567) across carefully selected conditions. Our results show foundation models tend to agree with human intuitions, matching or exceeding interparticipant agreement. In some conditions, models are ``superhuman'' -- they better predict modal human judgements than the average human. All models benefit from chain-of-thought reasoning. This suggests foundation models have acquired a human-like understanding of emotions and their influence on beliefs and behavior.", 'score': 5, 'issue_id': 1, 'pub_date': '2024-09-18', 'pub_date_card': {'ru': '18 сентября', 'en': 'September 18', 'zh': '9月18日'}, 'hash': 'c0d82f8c3405bbb7', 'data': {'categories': ['#reasoning', '#cv', '#agi', '#benchmark', '#alignment', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'ИИ превосходит человека в понимании эмоций', 'desc': 'Статья представляет новую систему оценки аффективного познания в фундаментальных моделях искусственного интеллекта. Исследователи создали 1280 разнообразных сценариев, основанных на психологических теориях, для тестирования способностей ИИ в понимании эмоций. Результаты показывают, что современные языковые модели, такие как GPT-4, Claude-3 и Gemini-1.5-Pro, демонстрируют уровень понимания эмоций, сравнимый с человеческим, а в некоторых случаях даже превосходящий его. Исследование подчеркивает, что фундаментальные модели ИИ приобрели человекоподобное понимание эмоций и их влияния на убеждения и поведение.'}, 'en': {'title': "AI's Emotional Intelligence: Surpassing Human Intuition", 'desc': 'This paper explores how well modern AI can understand and infer emotions, which is crucial for human interactions. The authors create a framework to evaluate the affective cognition of foundation models like GPT-4 and Claude-3 by generating 1,280 scenarios based on psychological theories. The study finds that these models often align with human emotional intuitions and, in some cases, outperform average human judgments. The results indicate that foundation models utilize chain-of-thought reasoning, suggesting they have developed a sophisticated understanding of emotions and their impact on human beliefs and behaviors.'}, 'zh': {'title': '基础模型的情感理解能力', 'desc': '理解情感对人类互动和体验至关重要。本文提出了一种评估框架，用于测试基础模型在情感认知方面的能力。我们生成了1280个多样化的场景，探讨评估、情感、表情和结果之间的关系。研究结果表明，基础模型在某些条件下的表现超越了普通人类，显示出它们在情感理解方面已接近人类水平。'}}}, {'id': 'https://huggingface.co/papers/2409.11242', 'title': 'Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse', 'url': 'https://huggingface.co/papers/2409.11242', 'abstract': 'LLMs are an integral part of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the quality of end-to-end RAG systems, there is a lack of research on understanding the appropriateness of an LLM for the RAG task. Thus, we introduce a new metric, Trust-Score, that provides a holistic evaluation of the trustworthiness of LLMs in an RAG framework. We show that various prompting methods, such as in-context learning, fail to adapt LLMs effectively to the RAG task. Thus, we propose Trust-Align, a framework to align LLMs for higher Trust-Score. LLaMA-3-8b, aligned with our method, significantly outperforms open-source LLMs of comparable sizes on ASQA (up 10.7), QAMPARI (up 29.2) and ELI5 (up 14.9). We release our code at: https://github.com/declare-lab/trust-align.', 'score': 5, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': 'b54be5fdb3e2ac7d', 'data': {'categories': ['#training', '#rag', '#alignment', '#benchmark', '#open_source', '#small_models'], 'emoji': '🔍', 'ru': {'title': 'Повышение надежности языковых моделей в системах RAG', 'desc': 'В этой статье представлен новый метрик под названием Trust-Score для оценки надежности языковых моделей (ЯМ) в системах генерации с извлечением информации (RAG). Исследователи обнаружили, что существующие методы настройки ЯМ неэффективны для задач RAG. Они предложили новый фреймворк Trust-Align для адаптации ЯМ к таким задачам. Эксперименты показали значительное улучшение производительности модели LLaMA-3-8b на нескольких наборах данных после применения Trust-Align.'}, 'en': {'title': 'Enhancing LLM Trustworthiness in RAG Systems', 'desc': 'This paper addresses the role of large language models (LLMs) in retrieval-augmented generation (RAG) systems, highlighting a gap in understanding how well these models perform in this context. The authors introduce a new evaluation metric called Trust-Score, which assesses the reliability of LLMs when integrated into RAG frameworks. They find that common prompting techniques, like in-context learning, do not effectively prepare LLMs for RAG tasks. To improve performance, they propose Trust-Align, a method that aligns LLMs to achieve higher Trust-Scores, demonstrating significant improvements in benchmark tasks with their aligned model, LLaMA-3-8b.'}, 'zh': {'title': '提升LLMs在RAG系统中的信任度', 'desc': '本论文探讨了大型语言模型（LLMs）在检索增强生成（RAG）系统中的适用性。我们提出了一种新的评估指标，称为信任分数（Trust-Score），用于全面评估LLMs在RAG框架中的可信度。研究表明，现有的提示方法（如上下文学习）未能有效调整LLMs以适应RAG任务。我们提出了Trust-Align框架，以提高LLMs的信任分数，并展示了与我们方法对齐的LLaMA-3-8b在多个基准测试中显著优于同类开源LLMs。'}}}, {'id': 'https://huggingface.co/papers/2409.09323', 'title': 'Implicit Neural Representations with Fourier Kolmogorov-Arnold Networks', 'url': 'https://huggingface.co/papers/2409.09323', 'abstract': 'Implicit neural representations (INRs) use neural networks to provide continuous and resolution-independent representations of complex signals with a small number of parameters. However, existing INR models often fail to capture important frequency components specific to each task. To address this issue, in this paper, we propose a Fourier Kolmogorov Arnold network (FKAN) for INRs. The proposed FKAN utilizes learnable activation functions modeled as Fourier series in the first layer to effectively control and learn the task-specific frequency components. In addition, the activation functions with learnable Fourier coefficients improve the ability of the network to capture complex patterns and details, which is beneficial for high-resolution and high-dimensional data. Experimental results show that our proposed FKAN model outperforms three state-of-the-art baseline schemes, and improves the peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) for the image representation task and intersection over union (IoU) for the 3D occupancy volume representation task, respectively.', 'score': 5, 'issue_id': 1, 'pub_date': '2024-09-14', 'pub_date_card': {'ru': '14 сентября', 'en': 'September 14', 'zh': '9月14日'}, 'hash': '44a56990a6b294f3', 'data': {'categories': ['#optimization', '#architecture', '#cv', '#3d'], 'emoji': '🔬', 'ru': {'title': 'FKAN: Улучшение имплицитных нейронных представлений с помощью обучаемых рядов Фурье', 'desc': 'Статья представляет новый подход к имплицитным нейронным представлениям (INR) - сеть Фурье-Колмогорова-Арнольда (FKAN). FKAN использует обучаемые активационные функции, моделируемые как ряды Фурье в первом слое, для эффективного контроля и обучения специфичным для задачи частотным компонентам. Этот метод улучшает способность сети захватывать сложные паттерны и детали, что особенно полезно для данных высокого разрешения и высокой размерности. Экспериментальные результаты показывают превосходство FKAN над современными базовыми схемами в задачах представления изображений и 3D-объемов.'}, 'en': {'title': 'Enhancing Implicit Neural Representations with Fourier Kolmogorov Arnold Networks', 'desc': 'This paper introduces the Fourier Kolmogorov Arnold network (FKAN), a novel approach to implicit neural representations (INRs) that enhances the ability to capture task-specific frequency components. By employing learnable activation functions modeled as Fourier series, FKAN effectively controls the frequency characteristics needed for different tasks. This method allows the network to better learn complex patterns and details, making it particularly useful for high-resolution and high-dimensional data. Experimental results demonstrate that FKAN significantly outperforms existing models in terms of image representation and 3D occupancy volume tasks, as indicated by improved PSNR, SSIM, and IoU metrics.'}, 'zh': {'title': '傅里叶网络：提升隐式神经表示的频率捕捉能力', 'desc': '隐式神经表示（INRs）使用神经网络以少量参数提供连续且与分辨率无关的复杂信号表示。然而，现有的INR模型往往无法捕捉到特定任务的重要频率成分。为了解决这个问题，本文提出了一种傅里叶科尔莫戈罗夫阿诺德网络（FKAN）。该网络通过在第一层使用可学习的傅里叶级数激活函数，有效地控制和学习任务特定的频率成分，从而提高了网络捕捉复杂模式和细节的能力。'}}}, {'id': 'https://huggingface.co/papers/2409.10836', 'title': 'Single-Layer Learnable Activation for Implicit Neural Representation (SL$^{2}$A-INR)', 'url': 'https://huggingface.co/papers/2409.10836', 'abstract': 'Implicit Neural Representation (INR), leveraging a neural network to transform coordinate input into corresponding attributes, has recently driven significant advances in several vision-related domains. However, the performance of INR is heavily influenced by the choice of the nonlinear activation function used in its multilayer perceptron (MLP) architecture. Multiple nonlinearities have been investigated; yet, current INRs face limitations in capturing high-frequency components, diverse signal types, and handling inverse problems. We have identified that these problems can be greatly alleviated by introducing a paradigm shift in INRs. We find that an architecture with learnable activations in initial layers can represent fine details in the underlying signals. Specifically, we propose SL^{2}A-INR, a hybrid network for INR with a single-layer learnable activation function, prompting the effectiveness of traditional ReLU-based MLPs. Our method performs superior across diverse tasks, including image representation, 3D shape reconstructions, inpainting, single image super-resolution, CT reconstruction, and novel view synthesis. Through comprehensive experiments, SL^{2}A-INR sets new benchmarks in accuracy, quality, and convergence rates for INR.', 'score': 4, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': '84d7067757cce461', 'data': {'categories': ['#cv', '#optimization', '#benchmark', '#diffusion', '#architecture', '#3d'], 'emoji': '🧠', 'ru': {'title': 'Обучаемые активации повышают эффективность неявного нейронного представления', 'desc': 'Статья представляет новый подход к неявному нейронному представлению (INR), который использует обучаемые функции активации в начальных слоях нейронной сети. Авторы предлагают архитектуру SL^{2}A-INR, которая сочетает однослойную обучаемую функцию активации с традиционным многослойным перцептроном на основе ReLU. Этот метод показывает превосходные результаты в различных задачах компьютерного зрения, включая представление изображений, 3D-реконструкцию, инпейнтинг и суперразрешение. Эксперименты демонстрируют, что SL^{2}A-INR устанавливает новые стандарты точности, качества и скорости сходимости для INR.'}, 'en': {'title': 'Revolutionizing Implicit Neural Representation with Learnable Activations', 'desc': "This paper introduces a new approach to Implicit Neural Representation (INR) by proposing a hybrid network called SL^{2}A-INR. The key innovation is the use of learnable activation functions in the initial layers of the multilayer perceptron (MLP), which enhances the network's ability to capture fine details in various signals. The authors demonstrate that this architecture significantly improves performance in tasks such as image representation, 3D shape reconstruction, and super-resolution. Overall, SL^{2}A-INR achieves state-of-the-art results in accuracy and convergence for INR applications."}, 'zh': {'title': '引入可学习激活函数，提升隐式神经表示的性能', 'desc': '隐式神经表示（INR）利用神经网络将坐标输入转换为相应的属性，最近在多个视觉相关领域取得了显著进展。然而，INR的性能受到多层感知器（MLP）架构中非线性激活函数选择的影响。我们发现，通过在INR中引入可学习的激活函数，可以有效捕捉信号中的细节，特别是我们提出的SL^{2}A-INR网络在多个任务中表现优越，包括图像表示、3D形状重建和单图像超分辨率等。通过全面的实验，SL^{2}A-INR在准确性、质量和收敛速度上设定了新的基准。'}}}, {'id': 'https://huggingface.co/papers/2409.10831', 'title': 'PDMX: A Large-Scale Public Domain MusicXML Dataset for Symbolic Music Processing', 'url': 'https://huggingface.co/papers/2409.10831', 'abstract': 'The recent explosion of generative AI-Music systems has raised numerous concerns over data copyright, licensing music from musicians, and the conflict between open-source AI and large prestige companies. Such issues highlight the need for publicly available, copyright-free musical data, in which there is a large shortage, particularly for symbolic music data. To alleviate this issue, we present PDMX: a large-scale open-source dataset of over 250K public domain MusicXML scores collected from the score-sharing forum MuseScore, making it the largest available copyright-free symbolic music dataset to our knowledge. PDMX additionally includes a wealth of both tag and user interaction metadata, allowing us to efficiently analyze the dataset and filter for high quality user-generated scores. Given the additional metadata afforded by our data collection process, we conduct multitrack music generation experiments evaluating how different representative subsets of PDMX lead to different behaviors in downstream models, and how user-rating statistics can be used as an effective measure of data quality. Examples can be found at https://pnlong.github.io/PDMX.demo/.', 'score': 4, 'issue_id': 1, 'pub_date': '2024-09-17', 'pub_date_card': {'ru': '17 сентября', 'en': 'September 17', 'zh': '9月17日'}, 'hash': 'd51dd5272b8dfc7e', 'data': {'categories': ['#audio', '#dataset', '#ethics', '#data', '#open_source', '#synthetic', '#multimodal'], 'emoji': '🎼', 'ru': {'title': 'PDMX: Большой открытый датасет для генеративного ИИ в музыке', 'desc': 'Статья представляет PDMX - крупнейший открытый набор данных из более чем 250 тысяч нотных партитур, находящихся в общественном достоянии. Этот датасет призван решить проблему нехватки свободных от авторских прав музыкальных данных для обучения генеративных моделей искусственного интеллекта. PDMX включает метаданные о тегах и взаимодействии пользователей, что позволяет анализировать качество партитур. Авторы также провели эксперименты по генерации многодорожечной музыки, оценивая влияние различных подмножеств данных на поведение моделей.'}, 'en': {'title': 'Unlocking Music Creativity with PDMX: A Treasure Trove of Copyright-Free Scores', 'desc': 'This paper introduces PDMX, a large-scale open-source dataset containing over 250,000 public domain MusicXML scores, addressing the shortage of copyright-free symbolic music data. The dataset is collected from the MuseScore platform and includes valuable metadata such as tags and user interactions, which enhances the analysis and quality filtering of the scores. The authors conduct experiments on multitrack music generation to explore how different subsets of PDMX affect the performance of machine learning models. Additionally, they demonstrate that user-rating statistics can serve as a reliable indicator of data quality in music generation tasks.'}, 'zh': {'title': 'PDMX：开源音乐数据集，助力版权问题解决', 'desc': '本文介绍了PDMX，这是一个大型开源数据集，包含超过25万份公共领域的MusicXML乐谱，旨在解决符号音乐数据的版权问题。该数据集来源于乐谱分享论坛MuseScore，是目前已知的最大版权免费符号音乐数据集。PDMX还包含丰富的标签和用户交互元数据，便于高效分析和筛选高质量的用户生成乐谱。通过对不同子集的多轨音乐生成实验，本文探讨了用户评分统计如何作为数据质量的有效衡量标准。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (5)', '#agents (1)', '#agi (2)', '#alignment (3)', '#architecture (10)', '#audio (2)', '#benchmark (8)', '#cv (8)', '#data (2)', '#dataset (3)', '#diffusion (6)', '#ethics (1)', '#games (3)', '#graphs (1)', '#hallucinations (1)', '#healthcare (1)', '#inference (2)', '#instruction_tuning', '#interpretability', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (3)', '#open_source (7)', '#optimization (7)', '#plp (1)', '#rag (2)', '#reasoning (3)', '#retrieval', '#rl (1)', '#rlhf', '#robotics (1)', '#science (1)', '#security', '#small_models (1)', '#story_generation', '#survey', '#synthetic (3)', '#training (9)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-09-18 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-09-18 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-09-18 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    