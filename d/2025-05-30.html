
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 14 papers. May 30.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">30 мая</span> | <span id="title-articles-count">14 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-29.html">⬅️ <span id="prev-date">29.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-02.html">➡️ <span id="next-date">02.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'};
        let feedDateNext = {'ru': '02.06', 'en': '06/02', 'zh': '6月2日'};
        let feedDatePrev = {'ru': '29.05', 'en': '05/29', 'zh': '5月29日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.23693', 'title': 'VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC\n  Videos', 'url': 'https://huggingface.co/papers/2505.23693', 'abstract': 'A new benchmark, VF-Eval, evaluates the capabilities of MLLMs in interpreting AI-generated content videos across four tasks, highlighting challenges and demonstrating benefits in video generation through human feedback alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t MLLMs have been widely studied for video question answering recently. However, most existing assessments focus on natural videos, overlooking synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in video generation rely on MLLMs to evaluate the quality of generated videos, but the capabilities of MLLMs on interpreting AIGC videos remain largely underexplored. To address this, we propose a new benchmark, VF-Eval, which introduces four tasks-coherence validation, error awareness, error type detection, and reasoning evaluation-to comprehensively evaluate the abilities of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks. This highlights the challenging nature of our benchmark. Additionally, to investigate the practical applications of VF-Eval in improving video generation, we conduct an experiment, RePrompt, demonstrating that aligning MLLMs more closely with human feedback can benefit video generation.', 'score': 9, 'issue_id': 4035, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '1a301b9c565967e9', 'authors': ['Tingyu Song', 'Tongyan Hu', 'Guo Gan', 'Yilun Zhao'], 'affiliations': ['National University of Singapore', 'School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences', 'Yale University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23693.jpg', 'data': {'categories': ['#video', '#games', '#interpretability', '#benchmark', '#reasoning', '#alignment', '#rlhf'], 'emoji': '🎥', 'ru': {'title': 'VF-Eval: новый рубеж в оценке ИИ-видео мультимодальными моделями', 'desc': 'Новый бенчмарк VF-Eval оценивает способности мультимодальных языковых моделей (MLLM) в интерпретации видео, созданных искусственным интеллектом. Бенчмарк включает четыре задачи: проверку согласованности, обнаружение ошибок, определение типов ошибок и оценку рассуждений. Эксперименты показали, что даже лучшие модели, такие как GPT-4.1, сталкиваются с трудностями при выполнении всех задач. Исследование также демонстрирует, что согласование MLLM с обратной связью от людей может улучшить генерацию видео.'}, 'en': {'title': 'Evaluating MLLMs: Bridging AI-Generated Videos and Human Feedback', 'desc': 'The paper introduces VF-Eval, a new benchmark designed to assess the performance of Multimodal Language Models (MLLMs) in interpreting AI-generated content videos. It focuses on four specific tasks: coherence validation, error awareness, error type detection, and reasoning evaluation, which are crucial for understanding synthetic videos. The study evaluates 13 advanced MLLMs, revealing that even the top model, GPT-4.1, faces challenges in consistently performing well across these tasks. Furthermore, the paper explores how aligning MLLMs with human feedback can enhance the quality of video generation, showcasing the practical implications of the benchmark.'}, 'zh': {'title': '评估AI生成视频的能力新基准', 'desc': '本文提出了一个新的基准测试VF-Eval，用于评估多模态大语言模型（MLLMs）在解读AI生成内容视频方面的能力。该基准包含四个任务：连贯性验证、错误意识、错误类型检测和推理评估，旨在全面评估MLLMs在处理合成视频时的表现。研究发现，即使是表现最好的模型GPT-4.1，在所有任务中也难以保持一致的良好表现，显示出基准测试的挑战性。此外，通过实验RePrompt，研究表明将MLLMs与人类反馈更紧密对齐可以改善视频生成的质量。'}}}, {'id': 'https://huggingface.co/papers/2505.22653', 'title': 'The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in\n  Learning to Reason', 'url': 'https://huggingface.co/papers/2505.22653', 'abstract': "LLMs exhibit robustness to reward noise during post-training and achieve high performance using reasoning pattern rewards (RPR) in conjunction with noisy reward models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies on post-training large language models (LLMs) for reasoning through reinforcement learning (RL) typically focus on tasks that can be accurately verified and rewarded, such as solving math problems. In contrast, our research investigates the impact of reward noise, a more practical consideration for real-world scenarios involving the post-training of LLMs using reward models. We found that LLMs demonstrate strong robustness to substantial reward noise. For example, manually flipping 40% of the reward function's outputs in math tasks still allows a Qwen-2.5-7B model to achieve rapid convergence, improving its performance on math tasks from 5% to 72%, compared to the 75% accuracy achieved by a model trained with noiseless rewards. Surprisingly, by only rewarding the appearance of key reasoning phrases (namely reasoning pattern reward, RPR), such as ``first, I need to''-without verifying the correctness of answers, the model achieved peak downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models trained with strict correctness verification and accurate rewards. Recognizing the importance of the reasoning process over the final results, we combined RPR with noisy reward models. RPR helped calibrate the noisy reward models, mitigating potential false negatives and enhancing the LLM's performance on open-ended tasks. These findings suggest the importance of improving models' foundational abilities during the pre-training phase while providing insights for advancing post-training techniques. Our code and scripts are available at https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.", 'score': 9, 'issue_id': 4035, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '7d2f43b6997c2647', 'authors': ['Ang Lv', 'Ruobing Xie', 'Xingwu Sun', 'Zhanhui Kang', 'Rui Yan'], 'affiliations': ['GSAI, Renmin University of China', 'Large Language Model Department, Tencent', 'School of Computer Science, Wuhan University', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2505.22653.jpg', 'data': {'categories': ['#rl', '#optimization', '#reasoning', '#training', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'LLM устойчивы к шуму: вознаграждение за процесс важнее результата', 'desc': 'Исследование показывает, что большие языковые модели (LLM) демонстрируют устойчивость к шуму в функции вознаграждения при пост-обучении. Модели достигают высокой производительности, используя вознаграждения за паттерны рассуждений (RPR) в сочетании с зашумленными моделями вознаграждения. Например, модель Qwen-2.5-7B смогла улучшить свою точность в математических задачах с 5% до 72%, даже когда 40% выходов функции вознаграждения были намеренно искажены. Результаты подчеркивают важность улучшения базовых способностей моделей на этапе предварительного обучения.'}, 'en': {'title': 'Robust LLMs: Thriving Amid Reward Noise with Reasoning Patterns', 'desc': 'This paper explores how large language models (LLMs) can effectively handle noise in reward signals during post-training, particularly in reinforcement learning scenarios. The authors demonstrate that even with significant reward noise, such as flipping 40% of reward outputs, LLMs like Qwen-2.5-7B can still achieve impressive performance improvements on math tasks. They introduce a novel approach called reasoning pattern rewards (RPR), which focuses on rewarding the presence of key reasoning phrases rather than the correctness of answers, leading to high accuracy. The study emphasizes the importance of enhancing foundational skills during pre-training and offers insights for refining post-training methods in real-world applications.'}, 'zh': {'title': '提升模型鲁棒性，克服奖励噪声', 'desc': '本研究探讨了在后训练阶段，大型语言模型（LLMs）对奖励噪声的鲁棒性。我们发现，即使在奖励函数输出中手动翻转40%的结果，模型仍能快速收敛，数学任务的准确率从5%提升至72%。通过仅奖励关键推理短语的出现（即推理模式奖励RPR），模型在下游任务中达到了超过70%的准确率，表现与严格验证的模型相当。我们的研究强调了在预训练阶段提升模型基础能力的重要性，并为后训练技术的进步提供了新思路。'}}}, {'id': 'https://huggingface.co/papers/2505.23604', 'title': 'Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software\n  Engineering', 'url': 'https://huggingface.co/papers/2505.23604', 'abstract': "EvoScale, an evolutionary and reinforcement learning-based method, enhances small language models' performance on real-world software engineering tasks by iteratively improving and refining outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models (LMs) perform well on standardized coding benchmarks but struggle with real-world software engineering tasks such as resolving GitHub issues in SWE-Bench, especially when model parameters are less than 100B. While smaller models are preferable in practice due to their lower computational cost, improving their performance remains challenging. Existing approaches primarily rely on supervised fine-tuning (SFT) with high-quality data, which is expensive to curate at scale. An alternative is test-time scaling: generating multiple outputs, scoring them using a verifier, and selecting the best one. Although effective, this strategy often requires excessive sampling and costly scoring, limiting its practical application. We propose Evolutionary Test-Time Scaling (EvoScale), a sample-efficient method that treats generation as an evolutionary process. By iteratively refining outputs via selection and mutation, EvoScale shifts the output distribution toward higher-scoring regions, reducing the number of samples needed to find correct solutions. To reduce the overhead from repeatedly sampling and selection, we train the model to self-evolve using reinforcement learning (RL). Rather than relying on external verifiers at inference time, the model learns to self-improve the scores of its own generations across iterations. Evaluated on SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or exceed the performance of models with over 100B parameters while using a few samples. Code, data, and models will be fully open-sourced.", 'score': 7, 'issue_id': 4035, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '3c092222b4cf7a3d', 'authors': ['Guangtao Zeng', 'Maohao Shen', 'Delin Chen', 'Zhenting Qi', 'Subhro Das', 'Dan Gutfreund', 'David Cox', 'Gregory Wornell', 'Wei Lu', 'Zhang-Wei Hong', 'Chuang Gan'], 'affiliations': ['Department of EECS, MIT', 'Harvard', 'MIT-IBM Watson AI Lab, IBM Research', 'Singapore University of Technology and Design', 'UMass Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2505.23604.jpg', 'data': {'categories': ['#small_models', '#rl', '#optimization', '#open_source', '#training'], 'emoji': '🧬', 'ru': {'title': 'Эволюционное масштабирование для повышения эффективности малых языковых моделей', 'desc': 'EvoScale - это метод, сочетающий эволюционное обучение и обучение с подкреплением для улучшения работы малых языковых моделей на реальных задачах разработки программного обеспечения. Он итеративно улучшает и уточняет выходные данные модели, смещая распределение в сторону более высоких оценок. EvoScale позволяет 32B модели достичь производительности моделей с более чем 100B параметров, используя меньше образцов. Метод обучает модель самостоятельно улучшать оценки своих собственных генераций без использования внешних верификаторов на этапе вывода.'}, 'en': {'title': 'EvoScale: Evolving Small Models for Big Performance', 'desc': 'EvoScale is a novel method that combines evolutionary strategies and reinforcement learning to enhance the performance of small language models on real-world software engineering tasks. It addresses the limitations of existing approaches that rely heavily on supervised fine-tuning and expensive data curation. By treating the output generation as an evolutionary process, EvoScale iteratively refines model outputs through selection and mutation, significantly reducing the number of samples needed for effective solutions. This approach allows smaller models to achieve performance levels comparable to much larger models, making them more practical for real-world applications.'}, 'zh': {'title': '进化测试时间缩放：小模型的强大提升', 'desc': 'EvoScale是一种基于进化和强化学习的方法，旨在提高小型语言模型在实际软件工程任务中的表现。该方法通过迭代改进和优化输出，减少了寻找正确解决方案所需的样本数量。与传统的监督微调方法相比，EvoScale采用自我进化的方式，使模型能够在推理时自我提升生成结果的评分。经过评估，EvoScale使得32B参数的模型Satori-SWE-32B在性能上能够匹敌或超过100B参数的模型，同时使用的样本数量更少。'}}}, {'id': 'https://huggingface.co/papers/2505.23762', 'title': 'ZeroGUI: Automating Online GUI Learning at Zero Human Cost', 'url': 'https://huggingface.co/papers/2505.23762', 'abstract': "ZeroGUI is an online learning framework that uses Vision-Language Models for task generation and reward estimation, enhancing GUI Agents' performance with minimal human intervention.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of large Vision-Language Models (VLMs) has propelled the development of pure-vision-based GUI Agents, capable of perceiving and operating Graphical User Interfaces (GUI) to autonomously fulfill user instructions. However, existing approaches usually adopt an offline learning framework, which faces two core limitations: (1) heavy reliance on high-quality manual annotations for element grounding and action supervision, and (2) limited adaptability to dynamic and interactive environments. To address these limitations, we propose ZeroGUI, a scalable, online learning framework for automating GUI Agent training at Zero human cost. Specifically, ZeroGUI integrates (i) VLM-based automatic task generation to produce diverse training goals from the current environment state, (ii) VLM-based automatic reward estimation to assess task success without hand-crafted evaluation functions, and (iii) two-stage online reinforcement learning to continuously interact with and learn from GUI environments. Experiments on two advanced GUI Agents (UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance across OSWorld and AndroidLab environments. The code is available at https://github.com/OpenGVLab/ZeroGUI.", 'score': 6, 'issue_id': 4035, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'd159a7e52c608567', 'authors': ['Chenyu Yang', 'Shiqian Su', 'Shi Liu', 'Xuan Dong', 'Yue Yu', 'Weijie Su', 'Xuehui Wang', 'Zhaoyang Liu', 'Jinguo Zhu', 'Hao Li', 'Wenhai Wang', 'Yu Qiao', 'Xizhou Zhu', 'Jifeng Dai'], 'affiliations': ['Hong Kong University of Science and Technology', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23762.jpg', 'data': {'categories': ['#agents', '#rl', '#optimization', '#games', '#rlhf'], 'emoji': '🤖', 'ru': {'title': 'Автоматизация обучения ГПИ-агентов без участия человека', 'desc': 'ZeroGUI - это фреймворк онлайн-обучения, использующий визуально-языковые модели для генерации задач и оценки вознаграждений, что улучшает производительность ГПИ-агентов с минимальным вмешательством человека. Он решает проблемы зависимости от ручной разметки и ограниченной адаптивности в динамических средах. ZeroGUI включает автоматическую генерацию задач, оценку вознаграждений и двухэтапное обучение с подкреплением для взаимодействия с ГПИ-средами. Эксперименты показывают значительное повышение производительности агентов в различных средах.'}, 'en': {'title': 'Empowering GUI Agents with Zero Human Cost', 'desc': 'ZeroGUI is an innovative online learning framework that leverages Vision-Language Models (VLMs) to enhance the training of GUI Agents with minimal human input. It addresses the challenges of traditional offline learning methods, which require extensive manual annotations and struggle to adapt to changing environments. By utilizing VLMs for automatic task generation and reward estimation, ZeroGUI enables continuous learning and interaction with GUI systems. Experiments show that this approach significantly improves the performance of advanced GUI Agents in various environments.'}, 'zh': {'title': 'ZeroGUI：无人工成本的GUI代理训练框架', 'desc': 'ZeroGUI是一个在线学习框架，利用视觉-语言模型（VLM）进行任务生成和奖励评估，从而在最小人力干预下提升图形用户界面（GUI）代理的性能。该框架解决了传统离线学习方法的两个主要限制：对高质量手动标注的依赖和对动态交互环境的适应性不足。ZeroGUI通过自动生成多样化的训练目标和自动评估任务成功率，来实现无人工成本的GUI代理训练。实验结果表明，ZeroGUI在OSWorld和AndroidLab环境中显著提升了两种先进GUI代理的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.23559', 'title': 'SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents', 'url': 'https://huggingface.co/papers/2505.23559', 'abstract': 'SafeScientist is an AI framework that enhances safety in AI-driven scientific research through multiple defensive mechanisms and is validated using the SciSafetyBench benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns. To systematically address these challenges, we introduce SafeScientist, an innovative AI scientist framework explicitly designed to enhance safety and ethical responsibility in AI-driven scientific exploration. SafeScientist proactively refuses ethically inappropriate or high-risk tasks and rigorously emphasizes safety throughout the research process. To achieve comprehensive safety oversight, we integrate multiple defensive mechanisms, including prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Complementing SafeScientist, we propose SciSafetyBench, a novel benchmark specifically designed to evaluate AI safety in scientific contexts, comprising 240 high-risk scientific tasks across 6 domains, alongside 30 specially designed scientific tools and 120 tool-related risk tasks. Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35\\% compared to traditional AI scientist frameworks, without compromising scientific output quality. Additionally, we rigorously validate the robustness of our safety pipeline against diverse adversarial attack methods, further confirming the effectiveness of our integrated approach. The code and data will be available at https://github.com/ulab-uiuc/SafeScientist. red{Warning: this paper contains example data that may be offensive or harmful.}', 'score': 6, 'issue_id': 4035, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '47b1655c578567ee', 'authors': ['Kunlun Zhu', 'Jiaxun Zhang', 'Ziheng Qi', 'Nuoxing Shang', 'Zijia Liu', 'Peixuan Han', 'Yue Su', 'Haofei Yu', 'Jiaxuan You'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.23559.jpg', 'data': {'categories': ['#agents', '#healthcare', '#ethics', '#science', '#benchmark', '#open_source', '#security'], 'emoji': '🔬', 'ru': {'title': 'Безопасный ИИ-ученый: этичные исследования без компромиссов', 'desc': 'SafeScientist - это фреймворк искусственного интеллекта, который повышает безопасность научных исследований, проводимых с помощью ИИ, используя несколько защитных механизмов. Он отказывается от этически неприемлемых или высокорисковых задач и делает акцент на безопасности на протяжении всего исследовательского процесса. SafeScientist включает в себя мониторинг промптов, взаимодействия агентов, использования инструментов, а также компонент этической проверки. Эффективность фреймворка подтверждена с помощью специально разработанного бенчмарка SciSafetyBench.'}, 'en': {'title': 'Enhancing Safety in AI-Driven Science with SafeScientist', 'desc': 'SafeScientist is an AI framework designed to improve safety in AI-driven scientific research by implementing various defensive mechanisms. It proactively avoids engaging in ethically questionable or high-risk tasks, ensuring a focus on safety throughout the research process. The framework includes features like prompt monitoring and agent collaboration oversight, which help maintain ethical standards. Additionally, SafeScientist is validated using the SciSafetyBench benchmark, demonstrating a 35% improvement in safety performance compared to traditional AI frameworks while maintaining high-quality scientific output.'}, 'zh': {'title': '安全科学家：提升AI科学研究的安全性', 'desc': 'SafeScientist是一个人工智能框架，旨在通过多种防御机制提高AI驱动科学研究的安全性。该框架能够主动拒绝不道德或高风险的任务，并在整个研究过程中强调安全性。我们还提出了SciSafetyBench，这是一个专门设计的基准，用于评估科学领域中AI的安全性，涵盖240个高风险科学任务。实验结果表明，SafeScientist在安全性能上比传统AI科学家框架提高了35%，同时不影响科学输出的质量。'}}}, {'id': 'https://huggingface.co/papers/2505.22961', 'title': 'ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind', 'url': 'https://huggingface.co/papers/2505.22961', 'abstract': "ToMAP enhances LLM persuaders with Theory of Mind modules, improving opponent awareness and argument quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have shown promising potential in persuasion, but existing works on training LLM persuaders are still preliminary. Notably, while humans are skilled in modeling their opponent's thoughts and opinions proactively and dynamically, current LLMs struggle with such Theory of Mind (ToM) reasoning, resulting in limited diversity and opponent awareness. To address this limitation, we introduce Theory of Mind Augmented Persuader (ToMAP), a novel approach for building more flexible persuader agents by incorporating two theory of mind modules that enhance the persuader's awareness and analysis of the opponent's mental state. Specifically, we begin by prompting the persuader to consider possible objections to the target central claim, and then use a text encoder paired with a trained MLP classifier to predict the opponent's current stance on these counterclaims. Our carefully designed reinforcement learning schema enables the persuader learns how to analyze opponent-related information and utilize it to generate more effective arguments. Experiments show that the ToMAP persuader, while containing only 3B parameters, outperforms much larger baselines, like GPT-4o, with a relative gain of 39.4% across multiple persuadee models and diverse corpora. Notably, ToMAP exhibits complex reasoning chains and reduced repetition during training, which leads to more diverse and effective arguments. The opponent-aware feature of ToMAP also makes it suitable for long conversations and enables it to employ more logical and opponent-aware strategies. These results underscore our method's effectiveness and highlight its potential for developing more persuasive language agents. Code is available at: https://github.com/ulab-uiuc/ToMAP.", 'score': 6, 'issue_id': 4035, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '13c778698d292f73', 'authors': ['Peixuan Han', 'Zijia Liu', 'Jiaxuan You'], 'affiliations': ['Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.22961.jpg', 'data': {'categories': ['#agents', '#rl', '#training', '#reasoning', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'ToMAP: ИИ-убеждающий с пониманием оппонента', 'desc': 'Статья представляет ToMAP - новый подход к созданию более гибких агентов-убеждающих с использованием модулей теории разума. ToMAP улучшает осведомленность ИИ-убеждающего о мыслях и мнениях оппонента, что позволяет генерировать более эффективные аргументы. Эксперименты показывают, что ToMAP превосходит более крупные базовые модели, демонстрируя сложные цепочки рассуждений и меньшую повторяемость. Этот метод особенно эффективен для длительных бесед и использования более логичных и ориентированных на оппонента стратегий убеждения.'}, 'en': {'title': 'Empowering Persuasion with Theory of Mind', 'desc': 'This paper presents Theory of Mind Augmented Persuader (ToMAP), a new method that enhances large language models (LLMs) for persuasive tasks by integrating Theory of Mind (ToM) modules. These modules allow the persuader to better understand and anticipate the thoughts and objections of opponents, leading to improved argument quality and diversity. By employing a reinforcement learning approach, ToMAP trains the persuader to analyze opponent-related information effectively, resulting in more logical and nuanced arguments. Experimental results demonstrate that ToMAP significantly outperforms larger models, showcasing its potential for creating advanced persuasive agents.'}, 'zh': {'title': '提升说服力的理论心智增强模型', 'desc': '本论文提出了一种名为ToMAP的理论心智增强说服者模型，旨在提高大型语言模型（LLM）在说服过程中的对手意识和论证质量。通过引入两个理论心智模块，ToMAP能够更好地分析对手的心理状态，从而生成更有效的论点。研究表明，尽管ToMAP的参数量仅为30亿，但在多个说服模型和不同语料库上，其表现超过了更大规模的基线模型，如GPT-4o，提升幅度达到39.4%。此外，ToMAP在训练过程中展现出复杂的推理链和减少重复的能力，使其在长时间对话中更具逻辑性和灵活性。'}}}, {'id': 'https://huggingface.co/papers/2505.23735', 'title': 'ATLAS: Learning to Optimally Memorize the Context at Test Time', 'url': 'https://huggingface.co/papers/2505.23735', 'abstract': 'Transformers have been established as the most popular backbones in sequence modeling, mainly due to their effectiveness in in-context retrieval tasks and the ability to learn at scale. Their quadratic memory and time complexity, however, bound their applicability in longer sequences and so has motivated researchers to explore effective alternative architectures such as modern recurrent neural networks (a.k.a long-term recurrent memory module). Despite their recent success in diverse downstream tasks, they struggle in tasks that requires long context understanding and extrapolation to longer sequences. We observe that these shortcomings come from three disjoint aspects in their design: (1) limited memory capacity that is bounded by the architecture of memory and feature mapping of the input; (2) online nature of update, i.e., optimizing the memory only with respect to the last input; and (3) less expressive management of their fixed-size memory. To enhance all these three aspects, we present ATLAS, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens, overcoming the online nature of long-term memory models. Building on this insight, we present a new family of Transformer-like architectures, called DeepTransformers, that are strict generalizations of the original Transformer architecture. Our experimental results on language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks show that ATLAS surpasses the performance of Transformers and recent linear recurrent models. ATLAS further improves the long context performance of Titans, achieving +80\\% accuracy in 10M context length of BABILong benchmark.', 'score': 5, 'issue_id': 4035, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'e9763dde29798ac1', 'authors': ['Ali Behrouz', 'Zeman Li', 'Praneeth Kacham', 'Majid Daliri', 'Yuan Deng', 'Peilin Zhong', 'Meisam Razaviyayn', 'Vahab Mirrokni'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2505.23735.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#long_context', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'ATLAS: Революция в долговременной памяти нейросетей', 'desc': 'Статья представляет ATLAS - новый модуль долговременной памяти для нейронных сетей. ATLAS преодолевает ограничения современных рекуррентных нейронных сетей, оптимизируя память на основе текущих и прошлых токенов. На базе ATLAS авторы создали семейство архитектур DeepTransformers, обобщающих оригинальную архитектуру Transformer. Эксперименты показывают превосходство ATLAS над трансформерами и линейными рекуррентными моделями в задачах языкового моделирования и понимания длинного контекста.'}, 'en': {'title': 'ATLAS: Revolutionizing Long-Term Memory in Transformers', 'desc': 'This paper introduces ATLAS, a long-term memory module designed to enhance the performance of sequence modeling tasks. It addresses the limitations of traditional architectures, particularly in handling long contexts, by optimizing memory based on both current and past inputs. The authors propose DeepTransformers, a new family of architectures that generalize the original Transformer model while improving memory capacity and management. Experimental results demonstrate that ATLAS outperforms existing models, achieving significant accuracy improvements in tasks requiring long-context understanding.'}, 'zh': {'title': 'ATLAS：超越传统的长时记忆模块', 'desc': '本文介绍了一种新的长时记忆模块ATLAS，旨在解决现有模型在处理长序列时的局限性。ATLAS通过优化记忆结构，能够更好地存储和利用上下文信息，从而克服了传统长时记忆模型的在线更新问题。研究表明，ATLAS在语言建模、常识推理和长上下文理解等任务中表现优于现有的Transformer和线性递归模型。最终，ATLAS在BABILong基准测试中实现了10M上下文长度的+80\\%准确率，展示了其强大的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.23585', 'title': 'On-Policy RL with Optimal Reward Baseline', 'url': 'https://huggingface.co/papers/2505.23585', 'abstract': 'Reinforcement learning algorithms are fundamental to align large language models with human preferences and to enhance their reasoning capabilities. However, current reinforcement learning algorithms often suffer from training instability due to loose on-policy constraints and computational inefficiency due to auxiliary models. In this work, we propose On-Policy RL with Optimal reward baseline (OPO), a novel and simplified reinforcement learning algorithm designed to address these challenges. OPO emphasizes the importance of exact on-policy training, which empirically stabilizes the training process and enhances exploration. Moreover, OPO introduces the optimal reward baseline that theoretically minimizes gradient variance. We evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its superior performance and training stability without additional models or regularization terms. Furthermore, OPO achieves lower policy shifts and higher output entropy, encouraging more diverse and less repetitive responses. These results highlight OPO as a promising direction for stable and effective reinforcement learning in large language model alignment and reasoning tasks. The implementation is provided at https://github.com/microsoft/LMOps/tree/main/opo.', 'score': 4, 'issue_id': 4035, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '8a27563b11999352', 'authors': ['Yaru Hao', 'Li Dong', 'Xun Wu', 'Shaohan Huang', 'Zewen Chi', 'Furu Wei'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.23585.jpg', 'data': {'categories': ['#rl', '#optimization', '#training', '#math', '#reasoning', '#alignment', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'OPO: Стабильное обучение с подкреплением для улучшения языковых моделей', 'desc': 'Статья представляет новый алгоритм обучения с подкреплением под названием OPO (On-Policy RL with Optimal reward baseline). OPO решает проблемы нестабильности и неэффективности существующих алгоритмов при обучении больших языковых моделей. Основные особенности OPO - точное обучение on-policy и оптимальная базовая линия вознаграждения, что улучшает стабильность и исследование. Эксперименты показывают превосходство OPO в задачах математических рассуждений, а также более разнообразные и менее повторяющиеся ответы модели.'}, 'en': {'title': 'Stabilizing Reinforcement Learning for Language Models with OPO', 'desc': 'This paper introduces a new reinforcement learning algorithm called On-Policy RL with Optimal reward baseline (OPO) to improve the training of large language models. OPO focuses on exact on-policy training, which helps stabilize the learning process and encourages better exploration of solutions. The algorithm also incorporates an optimal reward baseline that reduces gradient variance, leading to more reliable updates during training. Evaluations show that OPO outperforms existing methods in terms of stability and diversity of responses, making it a strong candidate for aligning language models with human preferences.'}, 'zh': {'title': 'OPO：稳定高效的强化学习新方向', 'desc': '本研究提出了一种新的强化学习算法，称为最优奖励基线的在线强化学习（OPO），旨在解决当前算法在训练不稳定性和计算效率方面的问题。OPO强调精确的在线训练，这在实践中稳定了训练过程并增强了探索能力。此外，OPO引入了最优奖励基线，理论上可以最小化梯度方差。通过在数学推理基准上的评估，OPO展示了其优越的性能和训练稳定性，且无需额外的模型或正则化项。'}}}, {'id': 'https://huggingface.co/papers/2505.23747', 'title': 'Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial\n  Intelligence', 'url': 'https://huggingface.co/papers/2505.23747', 'abstract': 'Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a spatial encoder-initialized from the backbone of the visual geometry model-to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k dataset and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that our spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks. Project page: https://diankun-wu.github.io/Spatial-MLLM/.', 'score': 3, 'issue_id': 4035, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '5ee23f045f054465', 'authors': ['Diankun Wu', 'Fangfu Liu', 'Yi-Hsin Hung', 'Yueqi Duan'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23747.jpg', 'data': {'categories': ['#3d', '#dataset', '#multimodal', '#architecture', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Пространственный интеллект из 2D наблюдений', 'desc': 'Статья представляет Spatial-MLLM - новую модель для пространственного анализа на основе только 2D изображений и видео. В отличие от существующих 3D мультимодальных языковых моделей, Spatial-MLLM использует двойной энкодер: семантический и пространственный, объединяя их результаты для улучшенного понимания 3D структуры. Модель также применяет стратегию выборки кадров с учетом пространственной информации при выводе. Тестирование на различных наборах данных показало превосходство Spatial-MLLM в задачах пространственного анализа и рассуждений.'}, 'en': {'title': 'Unlocking 3D Spatial Reasoning from 2D Inputs', 'desc': 'This paper introduces Spatial-MLLM, a new framework designed to enhance spatial reasoning using only 2D inputs like images and videos. Unlike traditional 3D models that require additional 3D data, Spatial-MLLM utilizes a dual-encoder architecture that combines a pretrained 2D visual encoder for semantic features with a spatial encoder for 3D structure features. The model integrates these features into unified visual tokens, improving its ability to understand spatial relationships. Additionally, a novel space-aware frame sampling strategy is employed to prioritize important frames during inference, leading to superior performance in various spatial reasoning tasks.'}, 'zh': {'title': '从2D到3D的空间推理新突破', 'desc': '本论文提出了一种新的框架，称为Spatial-MLLM，旨在从纯2D观察中进行视觉基础的空间推理。与传统的依赖于CLIP视觉编码器的视频多模态大语言模型不同，我们的模型利用了视觉几何基础模型的强结构先验。我们设计了一个双编码器架构，结合了预训练的2D视觉编码器和空间编码器，以提取语义特征和3D结构特征。通过在推理时采用空间感知的帧采样策略，我们的模型在多种视觉空间理解和推理任务中实现了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.23745', 'title': "To Trust Or Not To Trust Your Vision-Language Model's Prediction", 'url': 'https://huggingface.co/papers/2505.23745', 'abstract': "TrustVLM enhances the reliability of Vision-Language Models by estimating prediction trustworthiness without retraining, improving misclassification detection in multimodal tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM.", 'score': 1, 'issue_id': 4035, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '55ac97c649fb77b4', 'authors': ['Hao Dong', 'Moru Liu', 'Jian Liang', 'Eleni Chatzi', 'Olga Fink'], 'affiliations': ['EPFL', 'ETH Zürich', 'NLPR & MAIS, Institute of Automation, Chinese Academy of Sciences', 'Technical University of Munich', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.23745.jpg', 'data': {'categories': ['#transfer_learning', '#multimodal', '#interpretability', '#benchmark', '#architecture', '#security'], 'emoji': '🔍', 'ru': {'title': 'Повышение надежности мультимодальных моделей без переобучения', 'desc': 'TrustVLM - это новый подход к повышению надежности мультимодальных моделей машинного обучения, работающих с изображениями и текстом. Он позволяет оценивать достоверность предсказаний модели без необходимости ее переобучения. TrustVLM использует особенности пространства эмбеддингов изображений для улучшения обнаружения ошибочных классификаций. Метод показал значительное улучшение по сравнению с существующими базовыми подходами на 17 различных наборах данных.'}, 'en': {'title': 'Trust Your Vision-Language Model with TrustVLM!', 'desc': 'TrustVLM is a framework that enhances the reliability of Vision-Language Models (VLMs) by estimating the trustworthiness of their predictions without the need for retraining. It addresses the issue of misclassification in VLMs, which can produce confident but incorrect outputs, especially in critical applications. By introducing a novel confidence-scoring function that utilizes the image embedding space, TrustVLM improves the detection of misclassifications. The framework has been rigorously tested across multiple datasets and architectures, showing significant performance improvements in reliability metrics.'}, 'zh': {'title': '提升视觉-语言模型的可信度', 'desc': 'TrustVLM 是一种增强视觉-语言模型（VLM）可靠性的框架，它可以在不重新训练的情况下评估预测的可信度。该方法通过引入一种新的置信评分函数，利用图像嵌入空间来改善错误分类的检测。研究表明，TrustVLM 在 17 个不同的数据集上进行了严格评估，显示出在多种指标上显著优于现有基线。通过提高模型的可靠性，TrustVLM 为 VLM 在实际应用中的安全部署铺平了道路。'}}}, {'id': 'https://huggingface.co/papers/2505.23742', 'title': 'MAGREF: Masked Guidance for Any-Reference Video Generation', 'url': 'https://huggingface.co/papers/2505.23742', 'abstract': 'Video generation has made substantial strides with the emergence of deep generative models, especially diffusion-based approaches. However, video generation based on multiple reference subjects still faces significant challenges in maintaining multi-subject consistency and ensuring high generation quality. In this paper, we propose MAGREF, a unified framework for any-reference video generation that introduces masked guidance to enable coherent multi-subject video synthesis conditioned on diverse reference images and a textual prompt. Specifically, we propose (1) a region-aware dynamic masking mechanism that enables a single model to flexibly handle various subject inference, including humans, objects, and backgrounds, without architectural changes, and (2) a pixel-wise channel concatenation mechanism that operates on the channel dimension to better preserve appearance features. Our model delivers state-of-the-art video generation quality, generalizing from single-subject training to complex multi-subject scenarios with coherent synthesis and precise control over individual subjects, outperforming existing open-source and commercial baselines. To facilitate evaluation, we also introduce a comprehensive multi-subject video benchmark. Extensive experiments demonstrate the effectiveness of our approach, paving the way for scalable, controllable, and high-fidelity multi-subject video synthesis. Code and model can be found at: https://github.com/MAGREF-Video/MAGREF', 'score': 1, 'issue_id': 4035, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '7234edb0d22029a4', 'authors': ['Yufan Deng', 'Xun Guo', 'Yuanyang Yin', 'Jacob Zhiyuan Fang', 'Yiding Yang', 'Yizhi Wang', 'Shenghai Yuan', 'Angtian Wang', 'Bo Liu', 'Haibin Huang', 'Chongyang Ma'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.23742.jpg', 'data': {'categories': ['#video', '#diffusion', '#benchmark', '#open_source'], 'emoji': '🎬', 'ru': {'title': 'MAGREF: Революция в генерации видео с несколькими объектами', 'desc': 'Статья представляет MAGREF - новую систему для генерации видео на основе нескольких референсных изображений и текстового описания. Авторы предлагают механизм динамического маскирования для гибкой обработки различных объектов и фона, а также механизм конкатенации каналов для лучшего сохранения признаков внешнего вида. MAGREF превосходит существующие методы в качестве генерации видео с несколькими объектами, обеспечивая согласованный синтез и точный контроль над отдельными субъектами. Авторы также представляют новый набор данных для оценки качества генерации видео с несколькими объектами.'}, 'en': {'title': 'MAGREF: Mastering Multi-Subject Video Generation with Flexibility and Precision', 'desc': 'This paper presents MAGREF, a new framework for generating videos that can include multiple subjects from various reference images and text prompts. It introduces a masked guidance technique that allows the model to adaptively focus on different subjects like people and objects without needing to change its structure. Additionally, it employs a pixel-wise channel concatenation method to maintain the visual features of the subjects during generation. The results show that MAGREF achieves superior video quality and consistency compared to existing methods, making it a significant advancement in multi-subject video synthesis.'}, 'zh': {'title': 'MAGREF：高质量多主体视频生成的新框架', 'desc': '本论文提出了一种名为MAGREF的统一框架，用于基于任意参考图像的视频生成。该框架引入了掩蔽引导机制，以实现多主体视频合成，确保生成的一致性和高质量。我们提出的动态掩蔽机制能够灵活处理不同的主体推断，而像素级通道连接机制则更好地保留外观特征。实验结果表明，MAGREF在多主体视频合成中表现出色，超越了现有的开源和商业基线。'}}}, {'id': 'https://huggingface.co/papers/2505.23419', 'title': 'SWE-bench Goes Live!', 'url': 'https://huggingface.co/papers/2505.23419', 'abstract': 'The issue-resolving task, where a model generates patches to fix real-world bugs, has emerged as a critical benchmark for evaluating the capabilities of large language models (LLMs). While SWE-bench and its variants have become standard in this domain, they suffer from key limitations: they have not been updated since their initial releases, cover a narrow set of repositories, and depend heavily on manual effort for instance construction and environment setup. These factors hinder scalability and introduce risks of overfitting and data contamination. In this work, we present SWE-bench-Live, a live-updatable benchmark designed to overcome these challenges. Our initial release consists of 1,319 tasks derived from real GitHub issues created since 2024, spanning 93 repositories. Each task is accompanied by a dedicated Docker image to ensure reproducible execution. Central to our benchmark is \\method, an automated curation pipeline that streamlines the entire process from instance creation to environment setup, removing manual bottlenecks and enabling scalability and continuous updates. We evaluate a range of state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a substantial performance gap compared to static benchmarks like SWE-bench, even under controlled evaluation conditions. To better understand this discrepancy, we perform detailed analyses across repository origin, issue recency, and task difficulty. By providing a fresh, diverse, and executable benchmark grounded in live repository activity, SWE-bench-Live facilitates rigorous, contamination-resistant evaluation of LLMs and agents in dynamic, real-world software development settings.', 'score': 1, 'issue_id': 4035, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '56fc88c3eb857590', 'authors': ['Linghao Zhang', 'Shilin He', 'Chaoyun Zhang', 'Yu Kang', 'Bowen Li', 'Chengxing Xie', 'Junhao Wang', 'Maoquan Wang', 'Yufan Huang', 'Shengyu Fu', 'Elsie Nallipogu', 'Qingwei Lin', 'Yingnong Dang', 'Saravan Rajmohan', 'Dongmei Zhang'], 'affiliations': ['Microsoft', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2505.23419.jpg', 'data': {'categories': ['#optimization', '#dataset', '#benchmark', '#survey'], 'emoji': '🔄', 'ru': {'title': 'SWE-bench-Live: Динамичный эталон для оценки ИИ в реальной разработке ПО', 'desc': 'Статья представляет SWE-bench-Live - новый эталонный тест для оценки возможностей больших языковых моделей в решении реальных программных ошибок. В отличие от существующих статических тестов, SWE-bench-Live обновляется в реальном времени, охватывает широкий спектр репозиториев и использует автоматизированный процесс создания задач. Тест включает 1319 задач из 93 репозиториев, каждая с собственным Docker-образом для воспроизводимости. Оценка современных агентов и языковых моделей на SWE-bench-Live показала значительный разрыв в производительности по сравнению со статическими тестами.'}, 'en': {'title': 'SWE-bench-Live: A Dynamic Benchmark for Evaluating LLMs in Bug Fixing', 'desc': 'This paper introduces SWE-bench-Live, a new benchmark for evaluating large language models (LLMs) in the task of generating patches for real-world software bugs. Unlike previous benchmarks like SWE-bench, SWE-bench-Live is continuously updated and includes a broader range of tasks derived from recent GitHub issues, ensuring relevance and diversity. The benchmark features an automated curation pipeline that simplifies the process of creating tasks and setting up environments, reducing manual effort and enhancing scalability. The authors demonstrate that LLMs perform significantly better on SWE-bench-Live compared to static benchmarks, highlighting the importance of using dynamic and up-to-date datasets for evaluation.'}, 'zh': {'title': '实时更新的基准测试，提升模型评估能力', 'desc': '本论文提出了SWE-bench-Live，这是一个可实时更新的基准测试，旨在解决现有基准测试的局限性。现有的SWE-bench及其变体未能及时更新，且依赖于手动构建实例和环境设置，限制了其可扩展性。SWE-bench-Live包含来自2024年后真实GitHub问题的1,319个任务，并提供专用的Docker镜像以确保可重复执行。通过对多种先进的代理框架和大型语言模型进行评估，发现SWE-bench-Live在动态软件开发环境中提供了更为严谨和抗污染的评估。'}}}, {'id': 'https://huggingface.co/papers/2505.22943', 'title': 'Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of\n  Pre-trained Multimodal Representation via Text Updates', 'url': 'https://huggingface.co/papers/2505.22943', 'abstract': 'A benchmark using deceptive text samples to evaluate compositional vulnerabilities in multimodal representations is introduced, and a self-training approach improves zero-shot methods by enhancing attack success and sample diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t While pre-trained multimodal representations (e.g., CLIP) have shown impressive capabilities, they exhibit significant compositional vulnerabilities leading to counterintuitive judgments. We introduce Multimodal Adversarial Compositionality (MAC), a benchmark that leverages large language models (LLMs) to generate deceptive text samples to exploit these vulnerabilities across different modalities and evaluates them through both sample-wise attack success rate and group-wise entropy-based diversity. To improve zero-shot methods, we propose a self-training approach that leverages rejection-sampling fine-tuning with diversity-promoting filtering, which enhances both attack success rate and sample diversity. Using smaller language models like Llama-3.1-8B, our approach demonstrates superior performance in revealing compositional vulnerabilities across various multimodal representations, including images, videos, and audios.', 'score': 1, 'issue_id': 4035, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'}, 'hash': '0e150bd219867ad7', 'authors': ['Jaewoo Ahn', 'Heeseung Yun', 'Dayoon Ko', 'Gunhee Kim'], 'affiliations': ['Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2505.22943.jpg', 'data': {'categories': ['#hallucinations', '#multimodal', '#benchmark', '#security', '#training'], 'emoji': '🎭', 'ru': {'title': 'Раскрытие слабых мест мультимодальных моделей с помощью обманчивых текстов', 'desc': 'Статья представляет новый бенчмарк Multimodal Adversarial Compositionality (MAC) для оценки уязвимостей в мультимодальных представлениях. MAC использует большие языковые модели для генерации обманчивых текстовых образцов, эксплуатирующих эти уязвимости. Авторы предлагают метод самообучения на основе отбора образцов и фильтрации для улучшения атак с нулевым обучением. Эксперименты показывают, что этот подход эффективно выявляет композиционные уязвимости в различных мультимодальных представлениях.'}, 'en': {'title': 'Enhancing AI Resilience Against Deceptive Text in Multimodal Models', 'desc': 'This paper introduces a benchmark called Multimodal Adversarial Compositionality (MAC) to assess the weaknesses in multimodal representations, such as those used in AI models like CLIP. It highlights how these models can make incorrect judgments when faced with deceptive text samples generated by large language models. The authors propose a self-training method that improves zero-shot learning by enhancing the success of attacks and increasing the diversity of the samples used. Their approach, which utilizes smaller language models, shows better performance in identifying these vulnerabilities across different types of media, including images, videos, and audio.'}, 'zh': {'title': '揭示多模态表示的组合脆弱性', 'desc': '本文介绍了一种基准测试，利用欺骗性文本样本来评估多模态表示中的组合脆弱性。我们提出了多模态对抗组合性（MAC），通过大型语言模型生成欺骗性文本样本，利用这些脆弱性进行评估。为了改善零样本方法，我们提出了一种自我训练的方法，结合拒绝采样微调和多样性促进过滤，从而提高攻击成功率和样本多样性。使用较小的语言模型，如Llama-3.1-8B，我们的方法在揭示各种多模态表示（包括图像、视频和音频）的组合脆弱性方面表现优越。'}}}, {'id': 'https://huggingface.co/papers/2505.18087', 'title': 'CXReasonBench: A Benchmark for Evaluating Structured Diagnostic\n  Reasoning in Chest X-rays', 'url': 'https://huggingface.co/papers/2505.18087', 'abstract': 'CheXStruct and CXReasonBench evaluate Large Vision-Language Models in clinical diagnosis by assessing structured reasoning, visual grounding, and generalization using the MIMIC-CXR-JPG dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning. The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench', 'score': 0, 'issue_id': 4035, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': 'af9bc7f06b3e9889', 'authors': ['Hyungyung Lee', 'Geon Choi', 'Jung-Oh Lee', 'Hangyul Yoon', 'Hyuk Gi Hong', 'Edward Choi'], 'affiliations': ['KAIST', 'Seoul Medical Center', 'Seoul National University Hospital'], 'pdf_title_img': 'assets/pdf/title_img/2505.18087.jpg', 'data': {'categories': ['#healthcare', '#dataset', '#science', '#multimodal', '#cv', '#benchmark', '#interpretability', '#reasoning'], 'emoji': '\U0001fa7b', 'ru': {'title': 'Структурированное рассуждение в медицинском ИИ: новый подход к оценке', 'desc': 'CheXStruct и CXReasonBench - это новые инструменты для оценки крупных визуально-языковых моделей в клинической диагностике. Они используют набор данных MIMIC-CXR-JPG для оценки структурированного рассуждения, визуальной привязки и обобщения. CheXStruct автоматически выводит последовательность промежуточных шагов рассуждения непосредственно из рентгеновских снимков грудной клетки. CXReasonBench использует этот конвейер для оценки способности моделей выполнять клинически обоснованные шаги рассуждения и учиться на структурированных рекомендациях.'}, 'en': {'title': 'Enhancing Clinical Diagnosis with Structured Reasoning in AI', 'desc': "The paper introduces CheXStruct and CXReasonBench, benchmarks designed to evaluate Large Vision-Language Models (LVLMs) in clinical diagnosis using the MIMIC-CXR-JPG dataset. These tools focus on assessing structured reasoning, visual grounding, and the models' ability to generalize across various diagnostic tasks. CheXStruct automates the extraction of intermediate reasoning steps from chest X-rays, while CXReasonBench tests the models' capacity to perform clinically valid reasoning and learn from structured guidance. The findings reveal that even the best LVLMs struggle with structured reasoning and connecting abstract knowledge to visual data, highlighting the need for improved model capabilities in medical contexts."}, 'zh': {'title': '评估临床诊断中的结构化推理能力', 'desc': 'CheXStruct和CXReasonBench是用于评估大型视觉语言模型在临床诊断中的工具。它们通过分析胸部X光图像，自动生成中间推理步骤，帮助模型进行结构化推理。该基准测试包含18,988个问答对，涵盖12个诊断任务，支持多路径和多阶段评估。尽管有10个模型参与评估，但它们在结构化推理和泛化能力上仍然面临挑战。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (3)', '#agi', '#alignment (3)', '#architecture (3)', '#audio', '#benchmark (8)', '#cv (1)', '#data', '#dataset (3)', '#diffusion (1)', '#ethics (1)', '#games (2)', '#graphs', '#hallucinations (1)', '#healthcare (2)', '#inference', '#interpretability (3)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (4)', '#open_source (3)', '#optimization (5)', '#plp', '#rag', '#reasoning (7)', '#rl (5)', '#rlhf (4)', '#robotics', '#science (2)', '#security (3)', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic', '#training (6)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-05-30 02:29',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-30 02:29')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-30 02:29')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    