{
    "date": {
        "ru": "18 Ğ¸ÑĞ»Ñ",
        "en": "July 18",
        "zh": "7æœˆ18æ—¥"
    },
    "time_utc": "2025-07-18 00:58",
    "weekday": 4,
    "issue_id": 4882,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2507.09477",
            "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs",
            "url": "https://huggingface.co/papers/2507.09477",
            "abstract": "This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language Models (LLMs) by injecting external knowledge, yet it falls short on problems that demand multi-step inference; conversely, purely reasoning-oriented approaches often hallucinate or mis-ground facts. This survey synthesizes both strands under a unified reasoning-retrieval perspective. We first map how advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then, we show how retrieved knowledge of different type supply missing premises and expand context for complex inference (RAG-Enhanced Reasoning). Finally, we spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs iteratively interleave search and reasoning to achieve state-of-the-art performance across knowledge-intensive benchmarks. We categorize methods, datasets, and open challenges, and outline research avenues toward deeper RAG-Reasoning systems that are more effective, multimodally-adaptive, trustworthy, and human-centric. The collection is available at https://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
            "score": 48,
            "issue_id": 4862,
            "pub_date": "2025-07-13",
            "pub_date_card": {
                "ru": "13 Ğ¸ÑĞ»Ñ",
                "en": "July 13",
                "zh": "7æœˆ13æ—¥"
            },
            "hash": "da4aa711048f0a7f",
            "authors": [
                "Yangning Li",
                "Weizhi Zhang",
                "Yuyao Yang",
                "Wei-Chieh Huang",
                "Yaozu Wu",
                "Junyu Luo",
                "Yuanchen Bei",
                "Henry Peng Zou",
                "Xiao Luo",
                "Yusheng Zhao",
                "Chunkit Chan",
                "Yankai Chen",
                "Zhongfen Deng",
                "Yinghui Li",
                "Hai-Tao Zheng",
                "Dongyuan Li",
                "Renhe Jiang",
                "Ming Zhang",
                "Yangqiu Song",
                "Philip S. Yu"
            ],
            "affiliations": [
                "HKUST",
                "Peking University",
                "The University of Tokyo",
                "Tsinghua University",
                "University of California, Los Angeles",
                "University of Illinois Chicago",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.09477.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multimodal",
                    "#benchmark",
                    "#survey",
                    "#rag"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğµ Retrieval-Augmented Generation (RAG) Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¸Ğ· Ğ½Ğ¸Ñ… Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ ÑĞ¸Ğ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ RAG-Reasoning, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ±Ğ·Ğ¾Ñ€ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‡ĞµÑ€Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing LLMs with Synergized Retrieval and Reasoning",
                    "desc": "This paper discusses how to improve the accuracy and reasoning abilities of Large Language Models (LLMs) by combining retrieval and reasoning techniques. It introduces the concept of Retrieval-Augmented Generation (RAG), which enhances LLMs by providing them with external knowledge, but notes that RAG struggles with complex, multi-step reasoning tasks. The authors propose a unified framework that integrates advanced reasoning into RAG, allowing LLMs to better utilize retrieved information for deeper inference. They also highlight future research directions to create more effective and trustworthy systems that can adapt to various types of knowledge and user needs."
                },
                "zh": {
                    "title": "æ¨ç†ä¸æ£€ç´¢çš„ååŒæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡è°ƒæŸ¥äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†ä¸æ£€ç´¢çš„ç»“åˆï¼Œä»¥æé«˜äº‹å®å‡†ç¡®æ€§å’Œå¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡å¼•å…¥å¤–éƒ¨çŸ¥è¯†æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„äº‹å®æ€§ï¼Œä½†åœ¨éœ€è¦å¤šæ­¥æ¨ç†çš„é—®é¢˜ä¸Šè¡¨ç°ä¸è¶³ã€‚è®ºæ–‡æå‡ºäº†ç»Ÿä¸€çš„æ¨ç†-æ£€ç´¢è§†è§’ï¼Œå±•ç¤ºäº†å¦‚ä½•é€šè¿‡å…ˆè¿›çš„æ¨ç†ä¼˜åŒ–RAGçš„æ¯ä¸ªé˜¶æ®µï¼Œå¹¶å¼ºè°ƒäº†æ–°å…´çš„ååŒRAG-æ¨ç†æ¡†æ¶ã€‚æœ€åï¼Œè®ºæ–‡åˆ†ç±»äº†æ–¹æ³•ã€æ•°æ®é›†å’Œå¼€æ”¾æŒ‘æˆ˜ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä»¥æ„å»ºæ›´æœ‰æ•ˆã€é€‚åº”å¤šæ¨¡æ€ã€å¯ä¿¡èµ–å’Œä»¥äººä¸ºæœ¬çš„RAG-æ¨ç†ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12465",
            "title": "PhysX: Physical-Grounded 3D Asset Generation",
            "url": "https://huggingface.co/papers/2507.12465",
            "abstract": "PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D modeling is moving from virtual to physical. Existing 3D generation primarily emphasizes geometries and textures while neglecting physical-grounded modeling. Consequently, despite the rapid development of 3D generative models, the synthesized 3D assets often overlook rich and important physical properties, hampering their real-world application in physical domains like simulation and embodied AI. As an initial attempt to address this challenge, we propose PhysX, an end-to-end paradigm for physical-grounded 3D asset generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we present PhysXNet - the first physics-grounded 3D dataset systematically annotated across five foundational dimensions: absolute scale, material, affordance, kinematics, and function description. In particular, we devise a scalable human-in-the-loop annotation pipeline based on vision-language models, which enables efficient creation of physics-first assets from raw 3D assets.2) Furthermore, we propose PhysXGen, a feed-forward framework for physics-grounded image-to-3D asset generation, injecting physical knowledge into the pre-trained 3D structural space. Specifically, PhysXGen employs a dual-branch architecture to explicitly model the latent correlations between 3D structures and physical properties, thereby producing 3D assets with plausible physical predictions while preserving the native geometry quality. Extensive experiments validate the superior performance and promising generalization capability of our framework. All the code, data, and models will be released to facilitate future research in generative physical AI.",
            "score": 20,
            "issue_id": 4861,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ»Ñ",
                "en": "July 16",
                "zh": "7æœˆ16æ—¥"
            },
            "hash": "ece62f7e4ecd0487",
            "authors": [
                "Ziang Cao",
                "Zhaoxi Chen",
                "Linag Pan",
                "Ziwei Liu"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12465.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#synthetic",
                    "#games",
                    "#3d",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ§±",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "PhysX Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ PhysXNet Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº PhysXGen Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. PhysXGen Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ 3D-ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Bridging 3D Generation with Real-World Physics",
                    "desc": "PhysX introduces a new approach to 3D asset generation by incorporating physical properties into the modeling process. It presents PhysXNet, a unique dataset that annotates 3D models with essential physical attributes like scale, material, and function. Additionally, PhysXGen is a framework that uses this dataset to generate 3D assets that not only look good but also behave realistically in physical simulations. This work aims to enhance the applicability of AI-generated 3D models in real-world scenarios, such as robotics and virtual simulations."
                },
                "zh": {
                    "title": "ç‰©ç†é©±åŠ¨çš„3Dèµ„äº§ç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "PhysXæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è§£å†³3Dç”Ÿæˆæ¨¡å‹ä¸­ç¼ºä¹ç‰©ç†å±æ€§çš„é—®é¢˜ã€‚å®ƒå¼•å…¥äº†PhysXNetï¼Œè¿™æ˜¯ä¸€ä¸ªç‰©ç†æ³¨é‡Šçš„æ•°æ®é›†ï¼Œç³»ç»Ÿåœ°æ ‡æ³¨äº†äº”ä¸ªåŸºç¡€ç»´åº¦ï¼ŒåŒ…æ‹¬ç»å¯¹å°ºåº¦ã€ææ–™ã€å¯ç”¨æ€§ã€è¿åŠ¨å­¦å’ŒåŠŸèƒ½æè¿°ã€‚é€šè¿‡PhysXGenæ¡†æ¶ï¼Œç‰©ç†çŸ¥è¯†è¢«æ•´åˆåˆ°3Dèµ„äº§ç”Ÿæˆä¸­ï¼Œåˆ©ç”¨åŒåˆ†æ”¯æ¶æ„å»ºæ¨¡3Dç»“æ„ä¸ç‰©ç†å±æ€§ä¹‹é—´çš„æ½œåœ¨å…³è”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ç”Ÿæˆå…·æœ‰å¯ä¿¡ç‰©ç†é¢„æµ‹çš„3Dèµ„äº§æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12415",
            "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World\n  Repositories?",
            "url": "https://huggingface.co/papers/2507.12415",
            "abstract": "SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.  \t\t\t\t\tAI-generated summary \t\t\t\t Code performance optimization is paramount in real-world software engineering and critical for production-level systems. While Large Language Models (LLMs) have demonstrated impressive capabilities in code generation and bug fixing, their proficiency in enhancing code performance at the repository level remains largely unexplored. To address this gap, we introduce SWE-Perf, the first benchmark specifically designed to systematically evaluate LLMs on code performance optimization tasks within authentic repository contexts. SWE-Perf comprises 140 carefully curated instances, each derived from performance-improving pull requests from popular GitHub repositories. Each benchmark instance includes the relevant codebase, target functions, performance-related tests, expert-authored patches, and executable environments. Through a comprehensive evaluation of representative methods that span file-level and repo-level approaches (e.g., Agentless and OpenHands), we reveal a substantial capability gap between existing LLMs and expert-level optimization performance, highlighting critical research opportunities in this emerging field.",
            "score": 18,
            "issue_id": 4864,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ»Ñ",
                "en": "July 16",
                "zh": "7æœˆ16æ—¥"
            },
            "hash": "6be5b41deae78198",
            "authors": [
                "Xinyi He",
                "Qian Liu",
                "Mingzhe Du",
                "Lin Yan",
                "Zhijie Fan",
                "Yiming Huang",
                "Zejian Yuan",
                "Zejun Ma"
            ],
            "affiliations": [
                "National University of Singapore",
                "TikTok",
                "University of California San Diego",
                "Xian Jiaotong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12415.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "SWE-Perf: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM",
                    "desc": "SWE-Perf - ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ². ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 140 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… GitHub-Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ², Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ´Ğ¾Ğ²ÑƒÑ Ğ±Ğ°Ğ·Ñƒ, Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸, Ñ‚ĞµÑÑ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ LLM Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. SWE-Perf Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ LLM Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Unlocking Code Efficiency: Evaluating LLMs with SWE-Perf",
                    "desc": "SWE-Perf is a new benchmark designed to evaluate how well Large Language Models (LLMs) can optimize code performance using real-world data from software repositories. It focuses on the important task of improving code efficiency, which is essential for high-quality software systems. The benchmark includes 140 instances based on actual performance-enhancing pull requests from GitHub, providing a realistic testing environment. The study reveals that current LLMs significantly lag behind expert-level optimization, indicating a need for further research in this area."
                },
                "zh": {
                    "title": "SWE-Perfï¼šè¯„ä¼°ä»£ç æ€§èƒ½ä¼˜åŒ–çš„æ–°åŸºå‡†",
                    "desc": "SWE-Perfæ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç æ€§èƒ½ä¼˜åŒ–æ–¹é¢çš„è¡¨ç°ã€‚è¯¥åŸºå‡†æµ‹è¯•ä½¿ç”¨çœŸå®çš„ä»£ç åº“æ•°æ®ï¼Œä¸“æ³¨äºè½¯ä»¶å·¥ç¨‹ä¸­çš„ä»£ç æ€§èƒ½ä¼˜åŒ–ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆå’Œé”™è¯¯ä¿®å¤æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨æå‡ä»£ç æ€§èƒ½æ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚é€šè¿‡å¯¹140ä¸ªç²¾å¿ƒæŒ‘é€‰çš„å®ä¾‹è¿›è¡Œè¯„ä¼°ï¼ŒSWE-Perfæ­ç¤ºäº†ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸ä¸“å®¶çº§ä¼˜åŒ–æ€§èƒ½ä¹‹é—´çš„æ˜¾è‘—å·®è·ï¼ŒæŒ‡å‡ºäº†è¿™ä¸€æ–°å…´é¢†åŸŸä¸­çš„é‡è¦ç ”ç©¶æœºä¼šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12463",
            "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior\n  Understanding",
            "url": "https://huggingface.co/papers/2507.12463",
            "abstract": "A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans are integral components of the transportation ecosystem, and understanding their behaviors is crucial to facilitating the development of safe driving systems. Although recent progress has explored various aspects of human behaviorx2014such as motion, trajectories, and intentionx2014a comprehensive benchmark for evaluating human behavior understanding in autonomous driving remains unavailable. In this work, we propose MMHU, a large-scale benchmark for human behavior analysis featuring rich annotations, such as human motion and trajectories, text description for human motions, human intention, and critical behavior labels relevant to driving safety. Our dataset encompasses 57k human motion clips and 1.73M frames gathered from diverse sources, including established driving datasets such as Waymo, in-the-wild videos from YouTube, and self-collected data. A human-in-the-loop annotation pipeline is developed to generate rich behavior captions. We provide a thorough dataset analysis and benchmark multiple tasksx2014ranging from motion prediction to motion generation and human behavior question answeringx2014thereby offering a broad evaluation suite. Project page : https://MMHU-Benchmark.github.io.",
            "score": 16,
            "issue_id": 4864,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ»Ñ",
                "en": "July 16",
                "zh": "7æœˆ16æ—¥"
            },
            "hash": "c5061caaead150e7",
            "authors": [
                "Renjie Li",
                "Ruijie Ye",
                "Mingyang Wu",
                "Hao Frank Yang",
                "Zhiwen Fan",
                "Hezhen Hu",
                "Zhengzhong Tu"
            ],
            "affiliations": [
                "Brown University",
                "Johns Hopkins University",
                "Texas A&M University",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12463.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "MMHU: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MMHU Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ 57 Ñ‚Ñ‹ÑÑÑ‡ ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ¸ 1,73 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° ĞºĞ°Ğ´Ñ€Ğ¾Ğ². MMHU Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "MMHU: Advancing Human Behavior Analysis for Safer Autonomous Driving",
                    "desc": "The paper introduces MMHU, a comprehensive benchmark designed for analyzing human behavior in the context of autonomous driving. It includes extensive annotations on human motion, trajectories, intentions, and safety-related behaviors, making it a valuable resource for researchers. The dataset consists of 57,000 motion clips and 1.73 million frames sourced from various platforms, including established driving datasets and real-world videos. By benchmarking multiple tasks such as motion prediction and behavior question answering, MMHU aims to enhance the understanding of human behavior in driving scenarios."
                },
                "zh": {
                    "title": "MMHUï¼šè‡ªåŠ¨é©¾é©¶äººç±»è¡Œä¸ºåˆ†æçš„æ–°åŸºå‡†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºMMHUçš„å¤§è§„æ¨¡åŸºå‡†ï¼Œç”¨äºåˆ†æè‡ªåŠ¨é©¾é©¶ä¸­çš„äººç±»è¡Œä¸ºã€‚è¯¥åŸºå‡†åŒ…å«ä¸°å¯Œçš„æ³¨é‡Šå’Œå¤šæ ·çš„æ•°æ®æ¥æºï¼Œæ¶µç›–äº†äººç±»è¿åŠ¨ã€è½¨è¿¹ã€æ„å›¾ç­‰å¤šä¸ªæ–¹é¢ã€‚æ•°æ®é›†åŒ…æ‹¬57,000ä¸ªè¿åŠ¨ç‰‡æ®µå’Œ173ä¸‡å¸§ï¼Œæ¥æºäºçŸ¥åçš„é©¾é©¶æ•°æ®é›†å’ŒYouTubeç­‰å¹³å°ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªäººæœºåä½œçš„æ³¨é‡Šæµç¨‹ï¼Œä»¥ç”Ÿæˆè¯¦ç»†çš„è¡Œä¸ºæè¿°ï¼Œå¹¶å¯¹å¤šä¸ªä»»åŠ¡è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬è¿åŠ¨é¢„æµ‹å’Œè¡Œä¸ºé—®ç­”ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11949",
            "title": "MOSPA: Human Motion Generation Driven by Spatial Audio",
            "url": "https://huggingface.co/papers/2507.11949",
            "abstract": "A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human Motion (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human MOtion generation driven by SPatial Audio, termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA could generate diverse realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our model and dataset will be open-sourced upon acceptance. Please refer to our supplementary video for more details.",
            "score": 13,
            "issue_id": 4862,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ»Ñ",
                "en": "July 16",
                "zh": "7æœˆ16æ—¥"
            },
            "hash": "60ad5621a3842ae5",
            "authors": [
                "Shuyang Xu",
                "Zhiyang Dou",
                "Mingyi Shi",
                "Liang Pan",
                "Leo Ho",
                "Jingbo Wang",
                "Yuan Liu",
                "Cheng Lin",
                "Yuexin Ma",
                "Wenping Wang",
                "Taku Komura"
            ],
            "affiliations": [
                "Macau University of Science and Technology",
                "Shanghai AI Lab",
                "ShanghaiTech University",
                "Texas A&M University",
                "The Hong Kong University of Science and Technology",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11949.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#benchmark",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MOSPA - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SAM, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. MOSPA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‚ĞµĞ»Ğ° Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ·Ğ²ÑƒĞºĞ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Bridging Sound and Motion: MOSPA for Realistic Human Animation",
                    "desc": "The paper presents MOSPA, a diffusion-based generative framework designed to model human motion in response to spatial audio. It introduces the SAM dataset, which is the first of its kind, containing high-quality spatial audio and corresponding human motion data. The framework effectively captures the relationship between body movements and spatial audio through a novel fusion mechanism. By training on this dataset, MOSPA can generate diverse and realistic human motions that respond dynamically to different auditory stimuli, achieving state-of-the-art results in this area."
                },
                "zh": {
                    "title": "ç©ºé—´éŸ³é¢‘é©±åŠ¨çš„äººç±»è¿åŠ¨ç”Ÿæˆæ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¡†æ¶MOSPAï¼Œç”¨äºå»ºæ¨¡äººç±»åœ¨ç©ºé—´éŸ³é¢‘åˆºæ¿€ä¸‹çš„è¿åŠ¨ã€‚æˆ‘ä»¬åˆ›å»ºäº†é¦–ä¸ªç»¼åˆæ€§çš„ç©ºé—´éŸ³é¢‘é©±åŠ¨äººç±»è¿åŠ¨ï¼ˆSAMï¼‰æ•°æ®é›†ï¼ŒåŒ…å«å¤šæ ·åŒ–å’Œé«˜è´¨é‡çš„ç©ºé—´éŸ³é¢‘ä¸è¿åŠ¨æ•°æ®ã€‚MOSPAé€šè¿‡æœ‰æ•ˆçš„èåˆæœºåˆ¶ï¼Œå‡†ç¡®æ•æ‰èº«ä½“è¿åŠ¨ä¸ç©ºé—´éŸ³é¢‘ä¹‹é—´çš„å…³ç³»ï¼Œèƒ½å¤Ÿç”Ÿæˆå¤šæ ·ä¸”çœŸå®çš„äººç±»è¿åŠ¨ã€‚ç»è¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿™ä¸€ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11412",
            "title": "Seq vs Seq: An Open Suite of Paired Encoders and Decoders",
            "url": "https://huggingface.co/papers/2507.11412",
            "abstract": "The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. We introduce the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, we show that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training.",
            "score": 11,
            "issue_id": 4871,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 Ğ¸ÑĞ»Ñ",
                "en": "July 15",
                "zh": "7æœˆ15æ—¥"
            },
            "hash": "569bfad741b150bd",
            "authors": [
                "Orion Weller",
                "Kathryn Ricci",
                "Marc Marone",
                "Antoine Chaffin",
                "Dawn Lawrie",
                "Benjamin Van Durme"
            ],
            "affiliations": [
                "Johns Hopkins University",
                "LightOn"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11412.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#open_source",
                    "#training",
                    "#dataset"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ettin: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ettin, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€-Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¾Ñ‚ 17 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ¾ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ½Ğ° 2 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² ÑĞ²Ğ¾Ğ¸Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ…. ĞŸĞ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ° Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ñ‹ - Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ° Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° (Ğ¸ Ğ½Ğ°Ğ¾Ğ±Ğ¾Ñ€Ğ¾Ñ‚) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹."
                },
                "en": {
                    "title": "Unlocking the Power of Encoder and Decoder Models in NLP",
                    "desc": "This paper discusses the comparison between encoder-only and decoder-only language models in the context of machine learning. It introduces the Ettin suite, which consists of paired models of both types, ensuring they are trained under the same conditions for a fair comparison. The findings reveal that encoder-only models are better suited for classification and retrieval tasks, while decoder-only models excel in text generation. Additionally, the study shows that adapting models for different tasks through continued training is less effective than using models specifically designed for those tasks."
                },
                "zh": {
                    "title": "Ettinæ¨¡å‹ï¼šç¼–ç å™¨ä¸è§£ç å™¨çš„å®Œç¾ç»“åˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¨¡å‹å¥—ä»¶Ettinï¼ŒåŒ…å«é…å¯¹çš„ç¼–ç å™¨å’Œè§£ç å™¨æ¨¡å‹ï¼Œå‚æ•°èŒƒå›´ä»1700ä¸‡åˆ°10äº¿ï¼Œè®­ç»ƒæ•°æ®è¾¾åˆ°2ä¸‡äº¿ä¸ªæ ‡è®°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç¼–ç å™¨æ¨¡å‹åœ¨åˆ†ç±»å’Œæ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè€Œè§£ç å™¨æ¨¡å‹åœ¨ç”Ÿæˆä»»åŠ¡ä¸­æ›´å…·ä¼˜åŠ¿ã€‚é€šè¿‡ç›¸åŒçš„è®­ç»ƒæ–¹æ³•ï¼ŒEttinæ¨¡å‹åœ¨å„è‡ªçš„ç±»åˆ«ä¸­è¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°ä»£çš„BERTå’ŒLlama 3.2ç­‰æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œç»§ç»­è®­ç»ƒè§£ç å™¨æ¨¡å‹ä»¥é€‚åº”ç¼–ç å™¨ä»»åŠ¡çš„æ•ˆæœä¸å¦‚ç›´æ¥ä½¿ç”¨ç¼–ç å™¨æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11527",
            "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in\n  Civil Engineering",
            "url": "https://huggingface.co/papers/2507.11527",
            "abstract": "DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench.",
            "score": 10,
            "issue_id": 4861,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 Ğ¸ÑĞ»Ñ",
                "en": "July 15",
                "zh": "7æœˆ15æ—¥"
            },
            "hash": "e6f20729b2c748f9",
            "authors": [
                "Yinsheng Li",
                "Zhen Dong",
                "Yi Shao"
            ],
            "affiliations": [
                "Department of Civil Engineering McGill University",
                "NVIDIA",
                "UC Santa Barbara"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11527.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#open_source",
                    "#long_context",
                    "#agents"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "DrafterBench: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "DrafterBench - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‡ĞµÑ€Ñ‚ĞµĞ¶ĞµĞ¹. ĞĞ½ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹, ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 12 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‡ĞµÑ€Ñ‚ĞµĞ¶Ğ°Ñ…, Ñ 46 ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ²ÑĞµĞ³Ğ¾ 1920 Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹. DrafterBench Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¶Ğµ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ñ†ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ LLM Ğ² Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "DrafterBench: Evaluating LLMs for Technical Drawing Mastery",
                    "desc": "DrafterBench is an open-source benchmark designed to evaluate Large Language Model (LLM) agents specifically in the area of technical drawing revision. It includes twelve task types derived from real-world drawing files, featuring 46 customized functions and a total of 1920 tasks. The benchmark assesses LLM agents on their abilities in structured data comprehension, function execution, instruction following, and critical reasoning. By providing detailed analysis of task accuracy and error statistics, DrafterBench aims to enhance the understanding of LLM capabilities and identify areas for improvement in engineering applications."
                },
                "zh": {
                    "title": "DrafterBenchï¼šè¯„ä¼°LLMä»£ç†çš„æŠ€æœ¯å›¾çº¸ä¿®è®¢èƒ½åŠ›",
                    "desc": "DrafterBenchæ˜¯ä¸€ä¸ªå¼€æºåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨æŠ€æœ¯å›¾çº¸ä¿®è®¢ä¸­çš„èƒ½åŠ›ã€‚å®ƒæ¶µç›–äº†ç»“æ„åŒ–æ•°æ®ç†è§£ã€åŠŸèƒ½æ‰§è¡Œã€æŒ‡ä»¤éµå¾ªå’Œæ‰¹åˆ¤æ€§æ¨ç†ç­‰å¤šä¸ªæ–¹é¢ã€‚è¯¥åŸºå‡†åŒ…å«æ¥è‡ªçœŸå®ç»˜å›¾æ–‡ä»¶çš„åäºŒç§ä»»åŠ¡ï¼Œæä¾›äº†46ç§å®šåˆ¶åŠŸèƒ½å’Œ1920ä¸ªä»»åŠ¡ã€‚DrafterBenchæ—¨åœ¨æ·±å…¥åˆ†æä»£ç†çš„èƒ½åŠ›ï¼Œå¸®åŠ©è¯†åˆ«åœ¨å·¥ç¨‹åº”ç”¨ä¸­æ•´åˆLLMçš„æ”¹è¿›ç›®æ ‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.09025",
            "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
            "url": "https://huggingface.co/papers/2507.09025",
            "abstract": "Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into flexible, subquadratic architectures for infinite-context generation. Transformer-based LLMs face significant memory and computational bottlenecks as context lengths increase, due to the quadratic complexity of softmax attention and the growing key-value (KV) cache. Lizard addresses these limitations by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving the output quality. Unlike previous linearization methods, which are often limited by fixed model structures and therefore exclude gating mechanisms, Lizard incorporates a gating module inspired by recent state-of-the-art linear models. This enables adaptive memory control, supports constant-memory inference, offers strong length generalization, and allows more flexible model design. Lizard combines gated linear attention for global context compression with sliding window attention enhanced by meta memory, forming a hybrid mechanism that captures both long-range dependencies and fine-grained local interactions. Moreover, we introduce a hardware-aware algorithm that accelerates the training speed of our models. Extensive experiments show that Lizard achieves near-lossless recovery of the teacher model's performance across standard language modeling tasks, while significantly outperforming previous linearization methods. On the 5-shot MMLU benchmark, Lizard improves over prior models by 18 points and shows significant improvements on associative recall tasks.",
            "score": 6,
            "issue_id": 4861,
            "pub_date": "2025-07-11",
            "pub_date_card": {
                "ru": "11 Ğ¸ÑĞ»Ñ",
                "en": "July 11",
                "zh": "7æœˆ11æ—¥"
            },
            "hash": "3490901c2a32da3d",
            "authors": [
                "Chien Van Nguyen",
                "Ruiyi Zhang",
                "Hanieh Deilamsalehy",
                "Puneet Mathur",
                "Viet Dac Lai",
                "Haoliang Wang",
                "Jayakumar Subramanian",
                "Ryan A. Rossi",
                "Trung Bui",
                "Nikos Vlassis",
                "Franck Dernoncourt",
                "Thien Huu Nguyen"
            ],
            "affiliations": [
                "Adobe Research",
                "University of Oregon"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.09025.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#architecture",
                    "#optimization",
                    "#training",
                    "#long_context"
                ],
                "emoji": "ğŸ¦",
                "ru": {
                    "title": "Lizard: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "Lizard - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ»Ğ¸Ğ½ĞµĞ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑÑƒĞ±ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ¸ Ğ¾ĞºĞ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¼ĞµÑ‚Ğ°-Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ. Lizard Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Lizard Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Lizard: Efficient Infinite-Context Generation for Transformers",
                    "desc": "Lizard is a framework designed to improve the efficiency of Transformer-based Large Language Models (LLMs) by transforming them into subquadratic architectures. It addresses the challenges of memory and computation that arise with longer context lengths by implementing a hybrid attention mechanism that approximates softmax attention while maintaining output quality. The framework incorporates a gating module for adaptive memory control, allowing for constant-memory inference and enhanced model flexibility. Experimental results demonstrate that Lizard not only preserves the performance of traditional models but also significantly enhances their capabilities on various language tasks."
                },
                "zh": {
                    "title": "Lizardï¼šé«˜æ•ˆæ— é™ä¸Šä¸‹æ–‡ç”Ÿæˆçš„æ–°æ¡†æ¶",
                    "desc": "Lizardæ˜¯ä¸€ä¸ªçº¿æ€§åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨å°†åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è½¬å˜ä¸ºçµæ´»çš„äºšäºŒæ¬¡æ¶æ„ï¼Œä»¥å®ç°é«˜æ•ˆçš„æ— é™ä¸Šä¸‹æ–‡ç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥ä¸€ç§äºšäºŒæ¬¡æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…‹æœäº†ä¼ ç»Ÿsoftmaxæ³¨æ„åŠ›åœ¨ä¸Šä¸‹æ–‡é•¿åº¦å¢åŠ æ—¶çš„å†…å­˜å’Œè®¡ç®—ç“¶é¢ˆï¼ŒåŒæ—¶ä¿æŒè¾“å‡ºè´¨é‡ã€‚Lizardè¿˜ç»“åˆäº†é—¨æ§æ¨¡å—ï¼Œæ”¯æŒè‡ªé€‚åº”å†…å­˜æ§åˆ¶å’Œå¸¸é‡å†…å­˜æ¨ç†ï¼Œå¢å¼ºäº†æ¨¡å‹è®¾è®¡çš„çµæ´»æ€§ã€‚é€šè¿‡æ··åˆé—¨æ§çº¿æ€§æ³¨æ„åŠ›å’Œæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼ŒLizardèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰é•¿è·ç¦»ä¾èµ–å’Œç»†ç²’åº¦çš„å±€éƒ¨äº¤äº’ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.02857",
            "title": "AnyI2V: Animating Any Conditional Image with Motion Control",
            "url": "https://huggingface.co/papers/2507.02857",
            "abstract": "AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in video generation, particularly in diffusion models, have driven notable progress in text-to-video (T2V) and image-to-video (I2V) synthesis. However, challenges remain in effectively integrating dynamic motion signals and flexible spatial constraints. Existing T2V methods typically rely on text prompts, which inherently lack precise control over the spatial layout of generated content. In contrast, I2V methods are limited by their dependence on real images, which restricts the editability of the synthesized content. Although some methods incorporate ControlNet to introduce image-based conditioning, they often lack explicit motion control and require computationally expensive training. To address these limitations, we propose AnyI2V, a training-free framework that animates any conditional images with user-defined motion trajectories. AnyI2V supports a broader range of modalities as the conditional image, including data types such as meshes and point clouds that are not supported by ControlNet, enabling more flexible and versatile video generation. Additionally, it supports mixed conditional inputs and enables style transfer and editing via LoRA and text prompts. Extensive experiments demonstrate that the proposed AnyI2V achieves superior performance and provides a new perspective in spatial- and motion-controlled video generation. Code is available at https://henghuiding.com/AnyI2V/.",
            "score": 5,
            "issue_id": 4864,
            "pub_date": "2025-07-03",
            "pub_date_card": {
                "ru": "3 Ğ¸ÑĞ»Ñ",
                "en": "July 3",
                "zh": "7æœˆ3æ—¥"
            },
            "hash": "5a06d615e806a525",
            "authors": [
                "Ziye Li",
                "Hao Luo",
                "Xincheng Shuai",
                "Henghui Ding"
            ],
            "affiliations": [
                "DAMO Academy, Alibaba group",
                "Fudan University",
                "Hupan Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.02857.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "AnyI2V - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞµÑ‚ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¸Ğµ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. AnyI2V Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ²-Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LoRA Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº."
                },
                "en": {
                    "title": "AnyI2V: Freedom in Motion-Controlled Video Generation",
                    "desc": "AnyI2V is a novel framework designed for animating images based on user-defined motion paths without the need for extensive training. It addresses the limitations of existing text-to-video and image-to-video methods by allowing for greater control over spatial layouts and dynamic motion signals. This framework supports various data types, including meshes and point clouds, which enhances its versatility in video generation. Through extensive testing, AnyI2V has shown to outperform previous methods, offering a fresh approach to generating videos with precise motion and spatial control."
                },
                "zh": {
                    "title": "AnyI2Vï¼šæ— è®­ç»ƒçš„çµæ´»è§†é¢‘ç”Ÿæˆæ¡†æ¶",
                    "desc": "AnyI2Væ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·å®šä¹‰çš„è¿åŠ¨è½¨è¿¹ä¸ºæ¡ä»¶å›¾åƒæ·»åŠ åŠ¨ç”»ï¼Œæ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼Œä»è€Œå®ç°çµæ´»çš„è§†é¢‘ç”Ÿæˆã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰å’Œå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰åˆæˆä¸­çš„åŠ¨æ€è¿åŠ¨ä¿¡å·å’Œç©ºé—´çº¦æŸæ•´åˆé—®é¢˜ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒAnyI2Vä¸ä¾èµ–äºçœŸå®å›¾åƒï¼Œå…è®¸æ›´é«˜çš„å¯ç¼–è¾‘æ€§ï¼Œå¹¶æ”¯æŒæ··åˆæ¡ä»¶è¾“å…¥å’Œé£æ ¼è½¬ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAnyI2Våœ¨ç©ºé—´å’Œè¿åŠ¨æ§åˆ¶çš„è§†é¢‘ç”Ÿæˆæ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œæä¾›äº†æ–°çš„è§†è§’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12462",
            "title": "SpatialTrackerV2: 3D Point Tracking Made Easy",
            "url": "https://huggingface.co/papers/2507.12462",
            "abstract": "SpatialTrackerV2 is a feed-forward 3D point tracking method for monocular videos that integrates point tracking, monocular depth, and camera pose estimation into a unified, end-to-end architecture, achieving high performance and speed.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SpatialTrackerV2, a feed-forward 3D point tracking method for monocular videos. Going beyond modular pipelines built on off-the-shelf components for 3D tracking, our approach unifies the intrinsic connections between point tracking, monocular depth, and camera pose estimation into a high-performing and feedforward 3D point tracker. It decomposes world-space 3D motion into scene geometry, camera ego-motion, and pixel-wise object motion, with a fully differentiable and end-to-end architecture, allowing scalable training across a wide range of datasets, including synthetic sequences, posed RGB-D videos, and unlabeled in-the-wild footage. By learning geometry and motion jointly from such heterogeneous data, SpatialTrackerV2 outperforms existing 3D tracking methods by 30%, and matches the accuracy of leading dynamic 3D reconstruction approaches while running 50times faster.",
            "score": 4,
            "issue_id": 4867,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ»Ñ",
                "en": "July 16",
                "zh": "7æœˆ16æ—¥"
            },
            "hash": "a30fc76bcf7ffd92",
            "authors": [
                "Yuxi Xiao",
                "Jianyuan Wang",
                "Nan Xue",
                "Nikita Karaev",
                "Yuri Makarov",
                "Bingyi Kang",
                "Xing Zhu",
                "Hujun Bao",
                "Yujun Shen",
                "Xiaowei Zhou"
            ],
            "affiliations": [
                "Ant Group",
                "Bytedance Seed",
                "Oxford",
                "Pixelwise AI",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12462.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#3d"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ 3D-Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ° Ğ² Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "SpatialTrackerV2 - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ 3D Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑĞºĞ²Ğ¾Ğ·Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ 3D Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ½Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹, ÑĞ³Ğ¾-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². SpatialTrackerV2 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ 3D-Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ° Ğ½Ğ° 30% Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ² 50 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ."
                },
                "en": {
                    "title": "Unified 3D Point Tracking at Lightning Speed",
                    "desc": "SpatialTrackerV2 is a novel method for tracking 3D points in monocular videos using a feed-forward architecture. It combines point tracking, monocular depth estimation, and camera pose estimation into a single, efficient model. This approach breaks down 3D motion into components like scene geometry and camera movement, enabling it to learn from diverse datasets effectively. As a result, SpatialTrackerV2 achieves a 30% improvement over existing methods and operates 50 times faster than traditional dynamic 3D reconstruction techniques."
                },
                "zh": {
                    "title": "é«˜æ•ˆå¿«é€Ÿçš„3Dç‚¹è·Ÿè¸ªæ–°æ–¹æ³•",
                    "desc": "SpatialTrackerV2æ˜¯ä¸€ç§ç”¨äºå•ç›®è§†é¢‘çš„å‰é¦ˆ3Dç‚¹è·Ÿè¸ªæ–¹æ³•ã€‚å®ƒå°†ç‚¹è·Ÿè¸ªã€å•ç›®æ·±åº¦å’Œç›¸æœºå§¿æ€ä¼°è®¡æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„ç«¯åˆ°ç«¯æ¶æ„ä¸­ï¼Œä»è€Œå®ç°é«˜æ€§èƒ½å’Œå¿«é€Ÿå¤„ç†ã€‚è¯¥æ–¹æ³•å°†ä¸–ç•Œç©ºé—´ä¸­çš„3Dè¿åŠ¨åˆ†è§£ä¸ºåœºæ™¯å‡ ä½•ã€ç›¸æœºè‡ªè¿åŠ¨å’Œé€åƒç´ çš„ç‰©ä½“è¿åŠ¨ï¼Œæ”¯æŒåœ¨å¤šç§æ•°æ®é›†ä¸Šè¿›è¡Œå¯æ‰©å±•è®­ç»ƒã€‚é€šè¿‡ä»å¼‚æ„æ•°æ®ä¸­è”åˆå­¦ä¹ å‡ ä½•å’Œè¿åŠ¨ï¼ŒSpatialTrackerV2çš„æ€§èƒ½æ¯”ç°æœ‰çš„3Dè·Ÿè¸ªæ–¹æ³•æé«˜äº†30%ï¼Œå¹¶ä¸”è¿è¡Œé€Ÿåº¦æ¯”é¢†å…ˆçš„åŠ¨æ€3Dé‡å»ºæ–¹æ³•å¿«50å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.05065",
            "title": "Replacing thinking with tool usage enables reasoning in small language\n  models",
            "url": "https://huggingface.co/papers/2507.05065",
            "abstract": "A new approach formats tokens as a multi-turn interaction trace with a stateful tool for training Large Language Models, enabling faster sampling and denser reward signals for tasks like repairing Python code.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances have established a new machine learning paradigm based on scaling up compute at inference time as well as at training time. In that line of work, a combination of Supervised Fine-Tuning (SFT) on synthetic demonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is used for training Large Language Models to expend extra compute during inference in the form of \"thoughts\" expressed in natural language. In this paper, we propose to instead format these tokens as a multi-turn interaction trace with a stateful tool. At each turn, the new state of the tool is appended to the context of the model, whose job is to generate the tokens necessary to control the tool via a custom DSL. We benchmark this approach on the problem of repairing malfunctioning Python code, and show that this constrained setup allows for faster sampling of experience and a denser reward signal, allowing even models of size up to 3B parameters to learn how to proficiently expend additional compute on the task.",
            "score": 3,
            "issue_id": 4866,
            "pub_date": "2025-07-07",
            "pub_date_card": {
                "ru": "7 Ğ¸ÑĞ»Ñ",
                "en": "July 7",
                "zh": "7æœˆ7æ—¥"
            },
            "hash": "31f9493dba1b5054",
            "authors": [
                "Corrado Rainone",
                "Tim Bakker",
                "Roland Memisevic"
            ],
            "affiliations": [
                "Qualcomm AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.05065.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#plp",
                    "#training",
                    "#transfer_learning",
                    "#benchmark"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ĞºĞ°Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ»ĞµĞ´ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ°ĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ĞºĞ°Ğº Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Python. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ DSL Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ° Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ´Ğ¾ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸."
                },
                "en": {
                    "title": "Empowering Language Models with Stateful Interaction for Enhanced Learning",
                    "desc": "This paper introduces a novel method for training Large Language Models (LLMs) by using a multi-turn interaction trace with a stateful tool. Instead of relying solely on traditional training methods, the approach allows the model to generate tokens that control the tool through a custom domain-specific language (DSL). This setup enhances the model's ability to learn from its interactions, particularly in tasks like repairing Python code, by providing faster sampling and richer reward signals. The results demonstrate that even smaller models, with up to 3 billion parameters, can effectively utilize additional computational resources to improve their performance."
                },
                "zh": {
                    "title": "å¤šè½®äº¤äº’ï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å­¦ä¹ æ•ˆç‡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå°†ä»¤ç‰Œæ ¼å¼åŒ–ä¸ºå¤šè½®äº¤äº’è½¨è¿¹ï¼Œå¹¶ä½¿ç”¨æœ‰çŠ¶æ€çš„å·¥å…·æ¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•å¯ä»¥åŠ å¿«é‡‡æ ·é€Ÿåº¦ï¼Œå¹¶ä¸ºä¿®å¤Pythonä»£ç ç­‰ä»»åŠ¡æä¾›æ›´å¯†é›†çš„å¥–åŠ±ä¿¡å·ã€‚é€šè¿‡åœ¨æ¯ä¸€è½®ä¸­å°†å·¥å…·çš„æ–°çŠ¶æ€é™„åŠ åˆ°æ¨¡å‹çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ§åˆ¶å·¥å…·æ‰€éœ€çš„ä»¤ç‰Œã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯å‚æ•°é‡è¾¾åˆ°30äº¿çš„æ¨¡å‹ï¼Œä¹Ÿèƒ½æœ‰æ•ˆå­¦ä¹ å¦‚ä½•åœ¨ä»»åŠ¡ä¸­åˆç†ä½¿ç”¨é¢å¤–çš„è®¡ç®—èµ„æºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.11764",
            "title": "AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings\n  with Sentiment for Subjectivity Detection in News Articles",
            "url": "https://huggingface.co/papers/2507.11764",
            "abstract": "Sentiment-augmented transformer-based classifiers improve subjectivity detection in multilingual and zero-shot settings, achieving high performance and ranking first for Greek.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab Task 1: Subjectivity Detection in News Articles, classifying sentences as subjective/objective in monolingual, multilingual, and zero-shot settings. Training/development datasets were provided for Arabic, German, English, Italian, and Bulgarian; final evaluation included additional unseen languages (e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our primary strategy enhanced transformer-based classifiers by integrating sentiment scores, derived from an auxiliary model, with sentence representations, aiming to improve upon standard fine-tuning. We explored this sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base (English), and Llama3.2-1B. To address class imbalance, prevalent across languages, we employed decision threshold calibration optimized on the development set. Our experiments show sentiment feature integration significantly boosts performance, especially subjective F1 score. This framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).",
            "score": 2,
            "issue_id": 4871,
            "pub_date": "2025-07-15",
            "pub_date_card": {
                "ru": "15 Ğ¸ÑĞ»Ñ",
                "en": "July 15",
                "zh": "7æœˆ15æ—¥"
            },
            "hash": "73387831285ecb12",
            "authors": [
                "Matteo Fasulo",
                "Luca Babboni",
                "Luca Tedeschini"
            ],
            "affiliations": [
                "Department of Computer Science and Engineering (DISI) - University of Bologna"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.11764.jpg",
            "data": {
                "categories": [
                    "#multilingual",
                    "#machine_translation",
                    "#architecture",
                    "#low_resource",
                    "#training"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ¡ĞµĞ½Ñ‚Ğ¸Ğ¼ĞµĞ½Ñ‚-ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¿Ğ¾ĞºĞ¾Ñ€ÑÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¸ zero-shot ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞµĞ½Ñ‚Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ F1-Ğ¼ĞµÑ€Ñƒ Ğ´Ğ»Ñ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑĞ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ CLEF 2025 CheckThat!, Ğ·Ğ°Ğ½ÑĞ² Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ»Ñ Ğ³Ñ€ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°."
                },
                "en": {
                    "title": "Boosting Subjectivity Detection with Sentiment-Enhanced Transformers",
                    "desc": "This paper discusses a method to improve the detection of subjective and objective sentences in news articles using advanced transformer-based classifiers. The authors enhanced these classifiers by incorporating sentiment scores from an auxiliary model, which helped in better understanding the context of sentences. They tested their approach on multiple languages, including unseen ones, to ensure the model's ability to generalize. The results showed that this sentiment-augmented method significantly improved performance, particularly in identifying subjective content, achieving top rankings for Greek."
                },
                "zh": {
                    "title": "æƒ…æ„Ÿå¢å¼ºå˜æ¢å™¨æå‡å¤šè¯­è¨€ä¸»è§‚æ€§æ£€æµ‹",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†AI Wizardsåœ¨CLEF 2025 CheckThat! Lab Task 1ä¸­çš„å‚ä¸ï¼Œæ—¨åœ¨å¯¹æ–°é—»æ–‡ç« ä¸­çš„å¥å­è¿›è¡Œä¸»è§‚æ€§æ£€æµ‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºå‹çš„å˜æ¢å™¨åˆ†ç±»å™¨ï¼Œé€šè¿‡å°†æƒ…æ„Ÿåˆ†æ•°ä¸å¥å­è¡¨ç¤ºç»“åˆï¼Œæ¥æé«˜æ¨¡å‹åœ¨å•è¯­ã€å¤šè¯­å’Œé›¶æ ·æœ¬è®¾ç½®ä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæƒ…æ„Ÿç‰¹å¾çš„æ•´åˆæ˜¾è‘—æå‡äº†ä¸»è§‚æ€§F1åˆ†æ•°ï¼Œå°¤å…¶æ˜¯åœ¨å¸Œè…Šè¯­ä¸­å–å¾—äº†ç¬¬ä¸€åçš„ä¼˜å¼‚æˆç»©ã€‚æˆ‘ä»¬è¿˜é€šè¿‡ä¼˜åŒ–å†³ç­–é˜ˆå€¼æ¥è§£å†³å„è¯­è¨€é—´çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07451",
            "title": "RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning",
            "url": "https://huggingface.co/papers/2507.07451",
            "abstract": "RLEP, a reinforcement learning framework with experience replay, enhances large language model training by focusing on high-quality examples, leading to faster convergence and improved performance on math-related benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) for large language models is an energy-intensive endeavor: training can be unstable, and the policy may gradually drift away from its pretrained weights. We present RLEP\\, -- \\,Reinforcement Learning with Experience rePlay\\, -- \\,a two-phase framework that first collects verified trajectories and then replays them during subsequent training. At every update step, the policy is optimized on mini-batches that blend newly generated rollouts with these replayed successes. By replaying high-quality examples, RLEP steers the model away from fruitless exploration, focuses learning on promising reasoning paths, and delivers both faster convergence and stronger final performance. On the Qwen2.5-Math-7B base model, RLEP reaches baseline peak accuracy with substantially fewer updates and ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%, on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our code, datasets, and checkpoints are publicly available at https://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further research.",
            "score": 2,
            "issue_id": 4867,
            "pub_date": "2025-07-10",
            "pub_date_card": {
                "ru": "10 Ğ¸ÑĞ»Ñ",
                "en": "July 10",
                "zh": "7æœˆ10æ—¥"
            },
            "hash": "18c4eecefe9abb01",
            "authors": [
                "Hongzhi Zhang",
                "Jia Fu",
                "Jingyuan Zhang",
                "Kai Fu",
                "Qi Wang",
                "Fuzheng Zhang",
                "Guorui Zhou"
            ],
            "affiliations": [
                "Klear Team, Kuaishou Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07451.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#optimization",
                    "#dataset",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "RLEP: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°",
                    "desc": "RLEP - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. RLEP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸Ñ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ±ĞµÑĞ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "RLEP: Accelerating Learning with Experience Replay",
                    "desc": "RLEP is a reinforcement learning framework designed to improve the training of large language models by utilizing experience replay. It operates in two phases: first, it collects high-quality training examples, and then it replays these examples during the training process. This method helps the model focus on successful strategies and reduces the time spent on ineffective exploration. As a result, RLEP achieves faster convergence and better performance on math-related tasks, significantly enhancing accuracy on various benchmarks."
                },
                "zh": {
                    "title": "RLEPï¼šé«˜æ•ˆå¼ºåŒ–å­¦ä¹ ä¸ç»éªŒé‡æ”¾çš„ç»“åˆ",
                    "desc": "RLEPæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†ç»éªŒé‡æ”¾ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡æ”¶é›†ç»è¿‡éªŒè¯çš„è½¨è¿¹ï¼Œå¹¶åœ¨åç»­è®­ç»ƒä¸­é‡æ”¾è¿™äº›è½¨è¿¹ï¼Œæ¥ä¼˜åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚é€šè¿‡é‡æ”¾é«˜è´¨é‡çš„ç¤ºä¾‹ï¼ŒRLEPèƒ½å¤Ÿå¼•å¯¼æ¨¡å‹é¿å…æ— æ•ˆçš„æ¢ç´¢ï¼Œä¸“æ³¨äºæœ‰å‰æ™¯çš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRLEPåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œä¸”æ‰€éœ€çš„æ›´æ–°æ¬¡æ•°å¤§å¹…å‡å°‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.12367",
            "title": "GitChameleon: Evaluating AI Code Generation Against Python Library\n  Version Incompatibilities",
            "url": "https://huggingface.co/papers/2507.12367",
            "abstract": "GitChameleon is a dataset for evaluating version-conditioned code generation by large language models, LLM-powered agents, code assistants, and RAG systems using execution-based tests.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid evolution of software libraries poses a considerable hurdle for code generation, necessitating continuous adaptation to frequent version updates while preserving backward compatibility. While existing code evolution benchmarks provide valuable insights, they typically lack execution-based evaluation for generating code compliant with specific library versions. To address this, we introduce GitChameleon, a novel, meticulously curated dataset comprising 328 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. GitChameleon rigorously evaluates the capacity of contemporary large language models (LLMs), LLM-powered agents, code assistants, and RAG systems to perform version-conditioned code generation that demonstrates functional accuracy through execution. Our extensive evaluations indicate that state-of-the-art systems encounter significant challenges with this task; enterprise models achieving baseline success rates in the 48-51\\% range, underscoring the intricacy of the problem. By offering an execution-based benchmark emphasizing the dynamic nature of code libraries, GitChameleon enables a clearer understanding of this challenge and helps guide the development of more adaptable and dependable AI code generation methods. We make the dataset and evaluation code publicly available at https://github.com/mrcabbage972/GitChameleonBenchmark.",
            "score": 0,
            "issue_id": 4882,
            "pub_date": "2025-07-16",
            "pub_date_card": {
                "ru": "16 Ğ¸ÑĞ»Ñ",
                "en": "July 16",
                "zh": "7æœˆ16æ—¥"
            },
            "hash": "10957c95ade59404",
            "authors": [
                "Diganta Misra",
                "Nizar Islah",
                "Victor May",
                "Brice Rauby",
                "Zihan Wang",
                "Justine Gehring",
                "Antonio Orvieto",
                "Muawiz Chaudhary",
                "Eilif B. Muller",
                "Irina Rish",
                "Samira Ebrahimi Kahou",
                "Massimo Caccia"
            ],
            "affiliations": [
                "ELLIS Institute TÃ¼bingen",
                "Gologic",
                "Google",
                "MPI-IS TÃ¼bingen",
                "McGill University, MontrÃ©al",
                "Mila Quebec AI Institute",
                "Moderne",
                "Polytechnique MontrÃ©al",
                "ServiceNow Research",
                "TÃ¼bingen AI Center",
                "UniversitÃ© de MontrÃ©al"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.12367.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#rag",
                    "#dataset",
                    "#benchmark",
                    "#science"
                ],
                "emoji": "ğŸ¦",
                "ru": {
                    "title": "GitChameleon: Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğº Ğ²ĞµÑ€ÑĞ¸ÑĞ¼ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº",
                    "desc": "GitChameleon - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ĞºĞ¾Ğ´Ñƒ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ RAG. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 328 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Python, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ²ÑĞ·Ğ°Ğ½Ğ° Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ Ğ¸ ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹, Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğµ 48-51%. GitChameleon Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñƒ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "GitChameleon: Adapting Code Generation to Evolving Libraries",
                    "desc": "GitChameleon is a new dataset designed to test how well large language models (LLMs) can generate code that works with specific versions of software libraries. It includes 328 Python coding problems that are linked to particular library versions and come with executable tests to check if the generated code is correct. The dataset highlights the difficulties faced by current AI systems in adapting to frequent library updates while maintaining backward compatibility. By providing a benchmark that focuses on execution-based evaluation, GitChameleon aims to improve the development of more reliable AI code generation tools."
                },
                "zh": {
                    "title": "GitChameleonï¼šè¯„ä¼°ç‰ˆæœ¬æ¡ä»¶ä¸‹ä»£ç ç”Ÿæˆçš„åŸºå‡†æ•°æ®é›†",
                    "desc": "GitChameleonæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰¹å®šç‰ˆæœ¬æ¡ä»¶ä¸‹ç”Ÿæˆä»£ç èƒ½åŠ›çš„æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«328ä¸ªPythonä»£ç è¡¥å…¨é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½ä¸ç‰¹å®šçš„åº“ç‰ˆæœ¬ç›¸å…³ï¼Œå¹¶é™„æœ‰å¯æ‰§è¡Œçš„å•å…ƒæµ‹è¯•ã€‚é€šè¿‡æ‰§è¡ŒåŸºå‡†æµ‹è¯•ï¼ŒGitChameleonèƒ½å¤Ÿä¸¥æ ¼è¯„ä¼°å½“å‰LLMã€ä»£ç åŠ©æ‰‹å’ŒRAGç³»ç»Ÿåœ¨ç‰ˆæœ¬æ¡ä»¶ä¸‹ç”Ÿæˆä»£ç çš„åŠŸèƒ½å‡†ç¡®æ€§ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„å…ˆè¿›ç³»ç»Ÿåœ¨è¿™ä¸€ä»»åŠ¡ä¸Šé¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼ŒæˆåŠŸç‡ä»…åœ¨48-51%ä¹‹é—´ï¼Œçªæ˜¾äº†è¿™ä¸€é—®é¢˜çš„å¤æ‚æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.10015",
            "title": "(Almost) Free Modality Stitching of Foundation Models",
            "url": "https://huggingface.co/papers/2507.10015",
            "abstract": "Hypernetwork Model Alignment (Hyma) optimizes uni-modal model selection and connector training for multi-modal models, reducing search costs while maintaining performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation multi-modal models are often designed by stitching of multiple existing pretrained uni-modal models: for example, an image classifier with an text model. This stitching process is performed by training a connector module that aims to align the representation spaces of these uni-modal models towards a multi-modal objective. However, given the complexity of training such connectors on large scale web-based datasets coupled with the ever-increasing number of available pretrained uni-modal models, the task of uni-modal models selection and subsequent connector module training becomes computationally demanding. To address this under-studied critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel all-in-one solution for optimal uni-modal model selection and connector training by leveraging hypernetworks. Specifically, our framework utilizes the parameter prediction capability of a hypernetwork to obtain jointly trained connector modules for N times M combinations of uni-modal models. In our experiments, Hyma reduces the cost of searching for the best performing uni-modal model pair by 10times, while matching the ranking and trained connector performance obtained via grid search across a suite of diverse multi-modal benchmarks.",
            "score": 0,
            "issue_id": 4882,
            "pub_date": "2025-07-14",
            "pub_date_card": {
                "ru": "14 Ğ¸ÑĞ»Ñ",
                "en": "July 14",
                "zh": "7æœˆ14æ—¥"
            },
            "hash": "c32f5ce03817b95b",
            "authors": [
                "Jaisidh Singh",
                "Diganta Misra",
                "Boris Knyazev",
                "Antonio Orvieto"
            ],
            "affiliations": [
                "ELLIS Institute TÃ¼bingen",
                "MPI-IS TÃ¼bingen",
                "SAIT AI Lab MontrÃ©al",
                "TÃ¼bingen AI Center",
                "University of TÃ¼bingen",
                "Zuse School ELIZA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.10015.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#benchmark",
                    "#multimodal",
                    "#alignment"
                ],
                "emoji": "ğŸ”—",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Hypernetwork Model Alignment (Hyma) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑƒĞ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ğ½ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Hyma Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ğ½ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ ÑƒĞ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¿Ğ¾Ğ¸ÑĞº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ñ‹ ÑƒĞ½Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² 10 Ñ€Ğ°Ğ·. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Hyma ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ğ½ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Streamlining Multi-Modal Model Selection with Hypernetworks",
                    "desc": "The Hypernetwork Model Alignment (Hyma) introduces a new approach to optimize the selection of uni-modal models and the training of connector modules for multi-modal applications. By using hypernetworks, Hyma efficiently predicts parameters for connector modules, allowing for simultaneous training across multiple combinations of uni-modal models. This method significantly reduces the computational cost of finding the best model pair, achieving a tenfold decrease in search time. Despite the reduction in search costs, Hyma maintains performance levels comparable to traditional grid search methods across various multi-modal benchmarks."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹çš„è¿æ¥å™¨è®­ç»ƒ",
                    "desc": "Hypernetworkæ¨¡å‹å¯¹é½ï¼ˆHymaï¼‰é€šè¿‡ä¼˜åŒ–å•æ¨¡æ€æ¨¡å‹é€‰æ‹©å’Œè¿æ¥å™¨è®­ç»ƒï¼Œé™ä½äº†å¤šæ¨¡æ€æ¨¡å‹çš„æœç´¢æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½ã€‚å¤šæ¨¡æ€æ¨¡å‹é€šå¸¸æ˜¯é€šè¿‡å°†å¤šä¸ªé¢„è®­ç»ƒçš„å•æ¨¡æ€æ¨¡å‹æ‹¼æ¥è€Œæˆçš„ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»å™¨ä¸æ–‡æœ¬æ¨¡å‹ã€‚Hymaåˆ©ç”¨è¶…ç½‘ç»œçš„å‚æ•°é¢„æµ‹èƒ½åŠ›ï¼Œä¸ºNä¸ªå•æ¨¡æ€æ¨¡å‹çš„Mç§ç»„åˆè·å¾—è”åˆè®­ç»ƒçš„è¿æ¥å™¨æ¨¡å—ï¼Œä»è€Œç®€åŒ–äº†è¿æ¥å™¨çš„è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒHymaåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å°†æœ€ä½³å•æ¨¡æ€æ¨¡å‹å¯¹çš„æœç´¢æˆæœ¬é™ä½äº†10å€ï¼ŒåŒæ—¶ä¿æŒäº†ä¸ç½‘æ ¼æœç´¢ç›¸åŒ¹é…çš„æ’åå’Œè®­ç»ƒè¿æ¥å™¨æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2507.07015",
            "title": "MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge\n  Distillation",
            "url": "https://huggingface.co/papers/2507.07015",
            "abstract": "MST-Distill, a novel cross-modal knowledge distillation framework, uses a mixture of specialized teachers and an instance-level routing network to address distillation path selection and knowledge drift, outperforming existing methods across multimodal datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge distillation as an efficient knowledge transfer technique, has achieved remarkable success in unimodal scenarios. However, in cross-modal settings, conventional distillation methods encounter significant challenges due to data and statistical heterogeneities, failing to leverage the complementary prior knowledge embedded in cross-modal teacher models. This paper empirically reveals two critical issues in existing approaches: distillation path selection and knowledge drift. To address these limitations, we propose MST-Distill, a novel cross-modal knowledge distillation framework featuring a mixture of specialized teachers. Our approach employs a diverse ensemble of teacher models across both cross-modal and multimodal configurations, integrated with an instance-level routing network that facilitates adaptive and dynamic distillation. This architecture effectively transcends the constraints of traditional methods that rely on monotonous and static teacher models. Additionally, we introduce a plug-in masking module, independently trained to suppress modality-specific discrepancies and reconstruct teacher representations, thereby mitigating knowledge drift and enhancing transfer effectiveness. Extensive experiments across five diverse multimodal datasets, spanning visual, audio, and text, demonstrate that our method significantly outperforms existing state-of-the-art knowledge distillation methods in cross-modal distillation tasks. The source code is available at https://github.com/Gray-OREO/MST-Distill.",
            "score": 0,
            "issue_id": 4882,
            "pub_date": "2025-07-09",
            "pub_date_card": {
                "ru": "9 Ğ¸ÑĞ»Ñ",
                "en": "July 9",
                "zh": "7æœˆ9æ—¥"
            },
            "hash": "acd4e69e1b8c0567",
            "authors": [
                "Hui Li",
                "Pengfei Yang",
                "Juanyang Chen",
                "Le Dong",
                "Yanxin Chen",
                "Quan Wang"
            ],
            "affiliations": [
                "Xidian University Xian, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2507.07015.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#optimization",
                    "#dataset",
                    "#transfer_learning",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "MST-Distill - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¼ĞµÑÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞµÑ‚ÑŒ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ². ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿ÑƒÑ‚Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ´Ñ€ĞµĞ¹Ñ„Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ² Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. MST-Distill Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ."
                },
                "en": {
                    "title": "MST-Distill: Advancing Cross-Modal Knowledge Transfer",
                    "desc": "MST-Distill is a new framework designed for cross-modal knowledge distillation, which is the process of transferring knowledge from one model to another across different types of data, like images and text. It addresses two main challenges: choosing the right path for distillation and managing knowledge drift, which is when the information from the teacher model becomes less relevant. The framework uses a mix of specialized teacher models and an instance-level routing network to adaptively select the best distillation paths. Experiments show that MST-Distill outperforms existing methods on various multimodal datasets, making it a significant advancement in the field."
                },
                "zh": {
                    "title": "è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦çš„æ–°çªç ´",
                    "desc": "MST-Distillæ˜¯ä¸€ç§æ–°é¢–çš„è·¨æ¨¡æ€çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è’¸é¦è·¯å¾„é€‰æ‹©å’ŒçŸ¥è¯†æ¼‚ç§»çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¤šç§ä¸“ä¸šæ•™å¸ˆæ¨¡å‹å’Œå®ä¾‹çº§è·¯ç”±ç½‘ç»œï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡ŒåŠ¨æ€è’¸é¦ã€‚é€šè¿‡å¼•å…¥æ’ä»¶æ©è”½æ¨¡å—ï¼ŒMST-Distillèƒ½å¤ŸæŠ‘åˆ¶æ¨¡æ€ç‰¹å®šçš„å·®å¼‚ï¼Œé‡å»ºæ•™å¸ˆè¡¨ç¤ºï¼Œä»è€Œæé«˜çŸ¥è¯†è½¬ç§»çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMST-Distillåœ¨å¤šä¸ªè·¨æ¨¡æ€æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-07-17.html",
    "link_next": "2025-07-21.html",
    "link_month": "2025-07.html",
    "short_date_prev": {
        "ru": "17.07",
        "en": "07/17",
        "zh": "7æœˆ17æ—¥"
    },
    "short_date_next": {
        "ru": "21.07",
        "en": "07/21",
        "zh": "7æœˆ21æ—¥"
    },
    "categories": {
        "#dataset": 8,
        "#data": 0,
        "#benchmark": 9,
        "#agents": 2,
        "#cv": 0,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 2,
        "#plp": 1,
        "#inference": 0,
        "#3d": 2,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 5,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 3,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 8,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 1,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 0,
        "#science": 1,
        "#low_resource": 1
    }
}