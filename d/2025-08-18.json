{
    "date": {
        "ru": "18 августа",
        "en": "August 18",
        "zh": "8月18日"
    },
    "time_utc": "2025-08-18 12:23",
    "weekday": 0,
    "issue_id": 5403,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.10874",
            "title": "SSRL: Self-Search Reinforcement Learning",
            "url": "https://huggingface.co/papers/2508.10874",
            "abstract": "LLMs can serve as efficient simulators for RL tasks, reducing reliance on external search engines through a method called Self-Search RL, which enhances internal knowledge utilization.  \t\t\t\t\tAI-generated summary \t\t\t\t We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interactions with external search engines. To this end, we first quantify the intrinsic search capability of LLMs via structured prompting and repeated sampling, which we term Self-Search. Our results reveal that LLMs exhibit strong scaling behavior with respect to the inference budget, achieving high pass@k on question-answering benchmarks, including the challenging BrowseComp task. Building on these observations, we introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability through format-based and rule-based rewards. SSRL enables models to iteratively refine their knowledge utilization internally, without requiring access to external tools. Empirical evaluations demonstrate that SSRL-trained policy models provide a cost-effective and stable environment for search-driven RL training, reducing reliance on external search engines and facilitating robust sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world knowledge that can be effectively elicited to achieve high performance; 2) SSRL demonstrates the potential of leveraging internal knowledge to reduce hallucination; 3) SSRL-trained models integrate seamlessly with external search engines without additional effort. Our findings highlight the potential of LLMs to support more scalable RL agent training.",
            "score": 36,
            "issue_id": 5400,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "dbeffb2b7b345a96",
            "authors": [
                "Yuchen Fan",
                "Kaiyan Zhang",
                "Heng Zhou",
                "Yuxin Zuo",
                "Yanxu Chen",
                "Yu Fu",
                "Xinwei Long",
                "Xuekai Zhu",
                "Che Jiang",
                "Yuchen Zhang",
                "Li Kang",
                "Gang Chen",
                "Cheng Huang",
                "Zhizhou He",
                "Bingning Wang",
                "Lei Bai",
                "Ning Ding",
                "Bowen Zhou"
            ],
            "affiliations": [
                "CSCEC Third Bureau",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University College London",
                "WeChat AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10874.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#transfer_learning",
                    "#rl",
                    "#agents",
                    "#hallucinations",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LLM как эффективные симуляторы для обучения с подкреплением",
                    "desc": "Исследование показывает, что большие языковые модели (LLM) могут эффективно симулировать задачи поиска в обучении с подкреплением (RL). Предложенный метод Self-Search RL (SSRL) улучшает использование внутренних знаний модели через структурированные подсказки и повторную выборку. SSRL позволяет моделям итеративно улучшать использование знаний без доступа к внешним инструментам. Эмпирические оценки демонстрируют, что модели, обученные с помощью SSRL, обеспечивают экономически эффективную и стабильную среду для обучения RL, уменьшая зависимость от внешних поисковых систем."
                },
                "en": {
                    "title": "Harnessing LLMs for Efficient Reinforcement Learning Simulations",
                    "desc": "This paper explores how large language models (LLMs) can act as effective simulators for reinforcement learning (RL) tasks, minimizing the need for external search engines. The authors introduce a method called Self-Search RL (SSRL), which enhances the LLMs' ability to utilize their internal knowledge through structured prompting and rewards. The study shows that LLMs can achieve high performance on question-answering tasks, indicating their strong search capabilities. Ultimately, SSRL allows for more efficient and stable RL training by leveraging the LLMs' internal knowledge, leading to better sim-to-real transfer and reduced reliance on external tools."
                },
                "zh": {
                    "title": "利用LLMs提升强化学习的效率",
                    "desc": "本文探讨了大型语言模型（LLMs）在强化学习（RL）任务中作为高效模拟器的潜力，减少对外部搜索引擎的依赖。我们提出了一种称为自搜索强化学习（Self-Search RL, SSRL）的方法，通过格式化和规则奖励来增强LLMs的自搜索能力。研究表明，LLMs在推理预算方面表现出强大的扩展性，能够在问答基准测试中取得高分。我们的实证评估显示，SSRL训练的策略模型为搜索驱动的RL训练提供了成本效益高且稳定的环境。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.11630",
            "title": "Thyme: Think Beyond Images",
            "url": "https://huggingface.co/papers/2508.11630",
            "abstract": "Thyme, a novel paradigm, enables MLLMs to autonomously perform image manipulations and computations, enhancing performance in perception and reasoning tasks through a two-stage training strategy and GRPO-ATS algorithm.  \t\t\t\t\tAI-generated summary \t\t\t\t Following OpenAI's introduction of the ``thinking with images'' concept, recent efforts have explored stimulating the use of visual information in the reasoning process to enhance model performance in perception and reasoning tasks. However, to the best of our knowledge, no open-source work currently offers a feature set as rich as proprietary models (O3), which can perform diverse image manipulations and simultaneously enhance logical reasoning capabilities through code. In this paper, we make a preliminary attempt in this direction by introducing Thyme (Think Beyond Images), a novel paradigm for enabling MLLMs to transcend existing ``think with images'' approaches by autonomously generating and executing diverse image processing and computational operations via executable code. This approach not only facilitates a rich, on-the-fly set of image manipulations (e.g., cropping, rotation, contrast enhancement) but also allows for mathematical computations, all while maintaining high autonomy in deciding when and how to apply these operations. We activate this capability through a two-stage training strategy: an initial SFT on a curated dataset of 500K samples to teach code generation, followed by a RL phase to refine decision-making. For the RL stage, we manually collect and design high-resolution question-answer pairs to increase the learning difficulty, and we propose GRPO-ATS (Group Relative Policy Optimization with Adaptive Temperature Sampling), an algorithm that applies distinct temperatures to text and code generation to balance reasoning exploration with code execution precision. We conduct extensive experimental analysis and ablation studies. Comprehensive evaluations on nearly 20 benchmarks show that Thyme yields significant and consistent performance gains, particularly in challenging high-resolution perception and complex reasoning tasks.",
            "score": 29,
            "issue_id": 5394,
            "pub_date": "2025-08-15",
            "pub_date_card": {
                "ru": "15 августа",
                "en": "August 15",
                "zh": "8月15日"
            },
            "hash": "9df408fb7ce360bf",
            "authors": [
                "Yi-Fan Zhang",
                "Xingyu Lu",
                "Shukang Yin",
                "Chaoyou Fu",
                "Wei Chen",
                "Xiao Hu",
                "Bin Wen",
                "Kaiyu Jiang",
                "Changyi Liu",
                "Tianke Zhang",
                "Haonan Fan",
                "Kaibing Chen",
                "Jiankang Chen",
                "Haojie Ding",
                "Kaiyu Tang",
                "Zhang Zhang",
                "Liang Wang",
                "Fan Yang",
                "Tingting Gao",
                "Guorui Zhou"
            ],
            "affiliations": [
                "CASIA",
                "Kwai Keye",
                "NJU",
                "THU",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11630.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#agents",
                    "#cv",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Тайм: новый уровень мышления ИИ с изображениями",
                    "desc": "Тайм (Thyme) - это новая парадигма, позволяющая мультимодальным языковым моделям (MLLM) автономно выполнять манипуляции с изображениями и вычисления. Она использует двухэтапную стратегию обучения и алгоритм GRPO-ATS для улучшения восприятия и рассуждений. Тайм позволяет моделям самостоятельно генерировать и выполнять разнообразные операции обработки изображений и вычисления через исполняемый код. Эксперименты на почти 20 бенчмарках показали значительное и последовательное улучшение производительности, особенно в сложных задачах восприятия изображений высокого разрешения и комплексных рассуждений."
                },
                "en": {
                    "title": "Think Beyond Images with Thyme!",
                    "desc": "Thyme is a new approach that allows Multi-Modal Language Models (MLLMs) to independently perform various image manipulations and computations, improving their ability to understand and reason about images. It uses a two-stage training method, starting with supervised fine-tuning on a large dataset to teach the model how to generate code for image processing. The second stage involves reinforcement learning to enhance the model's decision-making skills when applying these operations. The GRPO-ATS algorithm is introduced to optimize the balance between exploring reasoning tasks and executing code accurately, leading to better performance on numerous benchmarks."
                },
                "zh": {
                    "title": "超越图像思维的自主处理能力",
                    "desc": "Thyme是一种新颖的范式，旨在使多模态大语言模型（MLLMs）能够自主执行图像处理和计算操作，从而提升感知和推理任务的表现。该方法采用两阶段训练策略，首先在一个包含50万样本的精心策划数据集上进行监督微调（SFT），然后通过强化学习（RL）阶段来优化决策过程。我们提出的GRPO-ATS算法通过对文本和代码生成应用不同的温度，平衡推理探索与代码执行的精确性。实验结果表明，Thyme在近20个基准测试中表现出显著且一致的性能提升，尤其是在高分辨率感知和复杂推理任务中。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10104",
            "title": "DINOv3",
            "url": "https://huggingface.co/papers/2508.10104",
            "abstract": "DINOv3, a self-supervised learning model, achieves superior performance across various vision tasks by scaling datasets and models, addressing dense feature degradation, and enhancing flexibility with post-hoc strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images -- using a single algorithm. This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies. First, we leverage the benefit of scaling both dataset and model size by careful data preparation, design, and optimization. Second, we introduce a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Finally, we apply post-hoc strategies that further enhance our models' flexibility with respect to resolution, model size, and alignment with text. As a result, we present a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. We also share the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios.",
            "score": 21,
            "issue_id": 5400,
            "pub_date": "2025-08-13",
            "pub_date_card": {
                "ru": "13 августа",
                "en": "August 13",
                "zh": "8月13日"
            },
            "hash": "98d721aa762199e4",
            "authors": [
                "Oriane Siméoni",
                "Huy V. Vo",
                "Maximilian Seitzer",
                "Federico Baldassarre",
                "Maxime Oquab",
                "Cijo Jose",
                "Vasil Khalidov",
                "Marc Szafraniec",
                "Seungeun Yi",
                "Michaël Ramamonjisoa",
                "Francisco Massa",
                "Daniel Haziza",
                "Luca Wehrstedt",
                "Jianyuan Wang",
                "Timothée Darcet",
                "Théo Moutakanni",
                "Leonel Sentana",
                "Claire Roberts",
                "Andrea Vedaldi",
                "Jamie Tolan",
                "John Brandt",
                "Camille Couprie",
                "Julien Mairal",
                "Hervé Jégou",
                "Patrick Labatut",
                "Piotr Bojanowski"
            ],
            "affiliations": [
                "Inria",
                "Meta AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10104.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#optimization",
                    "#open_source",
                    "#survey",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "DINOv3: Универсальная самообучающаяся модель для задач компьютерного зрения",
                    "desc": "DINOv3 - это модель самоконтролируемого обучения, которая достигает превосходных результатов в различных задачах компьютерного зрения. Модель использует масштабирование наборов данных и архитектуры, решает проблему деградации плотных признаков с помощью метода Gram anchoring. DINOv3 применяет постобработку для повышения гибкости в отношении разрешения, размера модели и согласования с текстом. В результате получена универсальная модель компьютерного зрения, превосходящая специализированные решения в широком спектре задач без дополнительного обучения."
                },
                "en": {
                    "title": "DINOv3: Scaling Self-Supervised Learning for Superior Vision Performance",
                    "desc": "DINOv3 is a self-supervised learning model that excels in various vision tasks by effectively scaling datasets and model sizes. It addresses the challenge of dense feature degradation during long training periods through a novel technique called Gram anchoring. Additionally, DINOv3 enhances flexibility with post-hoc strategies that allow for adjustments in resolution and model size without the need for fine-tuning. This model demonstrates superior performance compared to existing self- and weakly-supervised models, making it a significant advancement in the field of vision foundation models."
                },
                "zh": {
                    "title": "DINOv3：自监督学习的新里程碑",
                    "desc": "DINOv3是一种自监督学习模型，通过扩展数据集和模型规模，解决了密集特征退化的问题，并通过后处理策略增强了灵活性，从而在各种视觉任务中取得了优异的表现。该模型不需要手动数据标注，能够轻松适应大规模数据集和更大架构，学习来自不同来源的视觉表示。DINOv3采用了简单而有效的策略，包括数据准备、设计和优化，以充分利用数据集和模型规模的优势。最终，DINOv3展示了其在多种设置下的卓越性能，显著超越了以往的自监督和弱监督基础模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10395",
            "title": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization",
            "url": "https://huggingface.co/papers/2508.10395",
            "abstract": "XQuant and XQuant-CL reduce memory consumption in LLM inference through low-bit quantization and cross-layer similarity exploitation, achieving significant memory savings with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Although LLM inference has emerged as a critical workload for many downstream applications, efficiently inferring LLMs is challenging due to the substantial memory footprint and bandwidth requirements. In parallel, compute capabilities have steadily outpaced both memory capacity and bandwidth over the last few decades, a trend that remains evident in modern GPU hardware and exacerbates the challenge of LLM inference. As such, new algorithms are emerging that trade increased computation for reduced memory operations. To that end, we present XQuant, which takes advantage of this trend, enabling an order-of-magnitude reduction in memory consumption through low-bit quantization with substantial accuracy benefits relative to state-of-the-art KV cache quantization methods. We accomplish this by quantizing and caching the layer input activations X, instead of using standard KV caching, and then rematerializing the Keys and Values on-the-fly during inference. This results in an immediate 2times memory savings compared to KV caching. By applying XQuant, we achieve up to sim 7.7times memory savings with <0.1 perplexity degradation compared to the FP16 baseline. Furthermore, our approach leverages the fact that X values are similar across layers. Building on this observation, we introduce XQuant-CL, which exploits the cross-layer similarity in the X embeddings for extreme compression. Across different models, XQuant-CL attains up to 10times memory savings relative to the FP16 baseline with only 0.01 perplexity degradation, and 12.5times memory savings with only 0.1 perplexity degradation. XQuant exploits the rapidly increasing compute capabilities of hardware platforms to eliminate the memory bottleneck, while surpassing state-of-the-art KV cache quantization methods and achieving near-FP16 accuracy across a wide range of models.",
            "score": 9,
            "issue_id": 5400,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "85df7c343192e504",
            "authors": [
                "Aditya Tomar",
                "Coleman Hooper",
                "Minjae Lee",
                "Haocheng Xi",
                "Rishabh Tiwari",
                "Wonjun Kang",
                "Luca Manolache",
                "Michael W. Mahoney",
                "Kurt Keutzer",
                "Amir Gholami"
            ],
            "affiliations": [
                "KAIST",
                "Lawrence Berkeley National Laboratory",
                "UC Berkeley",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10395.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Революция в эффективности LLM: меньше памяти, та же точность",
                    "desc": "XQuant и XQuant-CL - это новые методы для уменьшения потребления памяти при инференсе больших языковых моделей (LLM). Они используют низкобитную квантизацию и эксплуатацию схожести между слоями модели. Эти подходы позволяют достичь значительной экономии памяти при минимальной потере точности по сравнению с базовыми моделями в формате FP16. XQuant-CL демонстрирует особенно впечатляющие результаты, достигая 10-кратной экономии памяти с деградацией перплексии всего на 0.01."
                },
                "en": {
                    "title": "Revolutionizing Memory Efficiency in LLM Inference",
                    "desc": "The paper introduces XQuant and XQuant-CL, two innovative methods designed to reduce memory usage during large language model (LLM) inference. By utilizing low-bit quantization and caching layer input activations instead of traditional key-value (KV) caching, these methods achieve significant memory savings while maintaining high accuracy. XQuant can reduce memory consumption by up to 7.7 times with minimal accuracy loss, while XQuant-CL further exploits cross-layer similarities to achieve up to 10 times memory savings. This approach effectively addresses the memory bottleneck in LLM inference, leveraging advancements in computational power to enhance efficiency."
                },
                "zh": {
                    "title": "高效内存节省，提升LLM推理能力",
                    "desc": "XQuant和XQuant-CL通过低位量化和跨层相似性利用，显著减少了大规模语言模型（LLM）推理中的内存消耗，同时保持了较小的准确性损失。它们通过量化和缓存层输入激活值，而不是使用标准的键值缓存，从而实现了内存的显著节省。XQuant在内存消耗上实现了高达7.7倍的节省，而XQuant-CL则利用跨层相似性，达到了高达10倍的内存节省。这些方法充分利用了现代硬件的计算能力，解决了内存瓶颈问题。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.11116",
            "title": "PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical\n  Register Indexing",
            "url": "https://huggingface.co/papers/2508.11116",
            "abstract": "PaperRegister enhances paper search by using hierarchical indexing and adaptive retrieval, supporting flexible and fine-grained queries beyond traditional abstract-based systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Paper search is an important activity for researchers, typically involving using a query with description of a topic to find relevant papers. As research deepens, paper search requirements may become more flexible, sometimes involving specific details such as module configuration rather than being limited to coarse-grained topics. However, previous paper search systems are unable to meet these flexible-grained requirements, as these systems mainly collect paper abstracts to construct index of corpus, which lack detailed information to support retrieval by finer-grained queries. In this work, we propose PaperRegister, consisted of offline hierarchical indexing and online adaptive retrieval, transforming traditional abstract-based index into hierarchical index tree for paper search, thereby supporting queries at flexible granularity. Experiments on paper search tasks across a range of granularity demonstrate that PaperRegister achieves the state-of-the-art performance, and particularly excels in fine-grained scenarios, highlighting the good potential as an effective solution for flexible-grained paper search in real-world applications. Code for this work is in https://github.com/Li-Z-Q/PaperRegister.",
            "score": 7,
            "issue_id": 5394,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "f0b9bc66fdef20ad",
            "authors": [
                "Zhuoqun Li",
                "Xuanang Chen",
                "Hongyu Lin",
                "Yaojie Lu",
                "Xianpei Han",
                "Le Sun"
            ],
            "affiliations": [
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11116.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Гибкий поиск научных статей на любом уровне детализации",
                    "desc": "PaperRegister - это новая система поиска научных статей, использующая иерархическую индексацию и адаптивный поиск. Она поддерживает гибкие и детализированные запросы, выходящие за рамки традиционных систем на основе аннотаций. PaperRegister преобразует обычный индекс на основе аннотаций в иерархическое дерево индексов, что позволяет выполнять поиск с различной степенью детализации. Эксперименты показали, что PaperRegister достигает наилучших результатов, особенно в сценариях с высокой степенью детализации запросов."
                },
                "en": {
                    "title": "Revolutionizing Paper Search with Hierarchical Indexing",
                    "desc": "PaperRegister is a novel system designed to improve the search for academic papers by utilizing hierarchical indexing and adaptive retrieval methods. Unlike traditional systems that rely solely on abstracts, PaperRegister allows for more detailed and flexible queries, accommodating specific research needs. The system organizes papers into a hierarchical index tree, enabling users to perform searches at various levels of granularity. Experimental results show that PaperRegister outperforms existing methods, particularly in scenarios requiring fine-grained search capabilities."
                },
                "zh": {
                    "title": "PaperRegister：灵活细粒度论文搜索的新方法",
                    "desc": "PaperRegister 是一种改进论文搜索的方法，它通过层次索引和自适应检索来增强搜索体验。传统的论文搜索系统主要依赖摘要来构建索引，无法满足细粒度查询的需求。PaperRegister 通过将传统的基于摘要的索引转变为层次索引树，支持更灵活的查询方式。实验结果表明，PaperRegister 在各种细粒度的论文搜索任务中表现优异，展示了其在实际应用中的良好潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.11203",
            "title": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image\n  Translation",
            "url": "https://huggingface.co/papers/2508.11203",
            "abstract": "StyleMM constructs stylized 3DMMs from text descriptions using a diffusion model for image-to-image translation while preserving facial attributes.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce StyleMM, a novel framework that can construct a stylized 3D Morphable Model (3DMM) based on user-defined text descriptions specifying a target style. Building upon a pre-trained mesh deformation network and a texture generator for original 3DMM-based realistic human faces, our approach fine-tunes these models using stylized facial images generated via text-guided image-to-image (i2i) translation with a diffusion model, which serve as stylization targets for the rendered mesh. To prevent undesired changes in identity, facial alignment, or expressions during i2i translation, we introduce a stylization method that explicitly preserves the facial attributes of the source image. By maintaining these critical attributes during image stylization, the proposed approach ensures consistent 3D style transfer across the 3DMM parameter space through image-based training. Once trained, StyleMM enables feed-forward generation of stylized face meshes with explicit control over shape, expression, and texture parameters, producing meshes with consistent vertex connectivity and animatability. Quantitative and qualitative evaluations demonstrate that our approach outperforms state-of-the-art methods in terms of identity-level facial diversity and stylization capability. The code and videos are available at [kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).",
            "score": 6,
            "issue_id": 5395,
            "pub_date": "2025-08-15",
            "pub_date_card": {
                "ru": "15 августа",
                "en": "August 15",
                "zh": "8月15日"
            },
            "hash": "9af71c9ddc545447",
            "authors": [
                "Seungmi Lee",
                "Kwan Yun",
                "Junyong Noh"
            ],
            "affiliations": [
                "KAIST, Visual Media Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11203.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d",
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Стильные 3D-лица из текста: новый уровень контроля и реализма",
                    "desc": "StyleMM - это новая система для создания стилизованных 3D-моделей лиц на основе текстовых описаний. Она использует диффузионную модель для преобразования изображений, сохраняя при этом ключевые черты лица. StyleMM обучается на стилизованных изображениях лиц и позволяет генерировать 3D-модели с контролем над формой, выражением и текстурой. Эксперименты показывают, что этот метод превосходит современные аналоги по разнообразию и возможностям стилизации лиц."
                },
                "en": {
                    "title": "Transforming Text to Stylized 3D Faces with Precision",
                    "desc": "StyleMM is a new framework that creates stylized 3D Morphable Models (3DMMs) from text descriptions, using a diffusion model for image-to-image translation. It fine-tunes a pre-trained mesh deformation network and a texture generator to ensure that the generated 3D faces maintain their original facial attributes while adopting new styles. The method prevents changes in identity and expression during the stylization process, allowing for consistent 3D style transfer. Ultimately, StyleMM enables the generation of customizable and animatable stylized face meshes, outperforming existing methods in facial diversity and stylization quality."
                },
                "zh": {
                    "title": "风格化3D模型的智能生成",
                    "desc": "StyleMM是一个新颖的框架，可以根据用户定义的文本描述构建风格化的3D可变形模型（3DMM）。该方法利用预训练的网格变形网络和纹理生成器，通过扩散模型进行图像到图像的翻译，生成风格化的面部图像作为目标。为了保持面部特征不变，我们引入了一种显式保留源图像面部属性的风格化方法。经过训练后，StyleMM能够生成具有明确形状、表情和纹理参数控制的风格化面部网格，确保在3DMM参数空间中的一致性风格转移。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.11255",
            "title": "FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for\n  Audio-Driven Portrait Animation",
            "url": "https://huggingface.co/papers/2508.11255",
            "abstract": "A multimodal reward model and adaptive preference optimization framework improve audio-driven portrait animation by aligning with human preferences across multiple dimensions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in audio-driven portrait animation have demonstrated impressive capabilities. However, existing methods struggle to align with fine-grained human preferences across multiple dimensions, such as motion naturalness, lip-sync accuracy, and visual quality. This is due to the difficulty of optimizing among competing preference objectives, which often conflict with one another, and the scarcity of large-scale, high-quality datasets with multidimensional preference annotations. To address these, we first introduce Talking-Critic, a multimodal reward model that learns human-aligned reward functions to quantify how well generated videos satisfy multidimensional expectations. Leveraging this model, we curate Talking-NSQ, a large-scale multidimensional human preference dataset containing 410K preference pairs. Finally, we propose Timestep-Layer adaptive multi-expert Preference Optimization (TLPO), a novel framework for aligning diffusion-based portrait animation models with fine-grained, multidimensional preferences. TLPO decouples preferences into specialized expert modules, which are then fused across timesteps and network layers, enabling comprehensive, fine-grained enhancement across all dimensions without mutual interference. Experiments demonstrate that Talking-Critic significantly outperforms existing methods in aligning with human preference ratings. Meanwhile, TLPO achieves substantial improvements over baseline models in lip-sync accuracy, motion naturalness, and visual quality, exhibiting superior performance in both qualitative and quantitative evaluations. Ours project page: https://fantasy-amap.github.io/fantasy-talking2/",
            "score": 5,
            "issue_id": 5398,
            "pub_date": "2025-08-15",
            "pub_date_card": {
                "ru": "15 августа",
                "en": "August 15",
                "zh": "8月15日"
            },
            "hash": "aff56c9a04ada350",
            "authors": [
                "MengChao Wang",
                "Qiang Wang",
                "Fan Jiang",
                "Mu Xu"
            ],
            "affiliations": [
                "AMAP, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11255.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#optimization",
                    "#multimodal",
                    "#diffusion",
                    "#alignment",
                    "#dataset"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Многомерная оптимизация анимации портретов с учетом человеческих предпочтений",
                    "desc": "Статья представляет новый подход к улучшению анимации портретов, управляемой аудио. Авторы разработали мультимодальную модель вознаграждения Talking-Critic, которая учится оценивать качество генерируемых видео по нескольким измерениям. На основе этой модели был создан большой датасет Talking-NSQ с многомерными оценками предпочтений. Предложен фреймворк TLPO для точной настройки диффузионных моделей анимации портретов в соответствии с многомерными предпочтениями."
                },
                "en": {
                    "title": "Enhancing Portrait Animation with Human-Aligned Preferences",
                    "desc": "This paper presents a new approach to improve audio-driven portrait animation by focusing on human preferences. It introduces a multimodal reward model called Talking-Critic, which quantifies how well generated animations meet various human expectations like lip-sync accuracy and visual quality. Additionally, the authors create a large dataset, Talking-NSQ, with 410,000 preference pairs to train their model effectively. They also propose a novel optimization framework, TLPO, that allows for better alignment of animation models with these preferences by using specialized expert modules to enhance multiple dimensions simultaneously."
                },
                "zh": {
                    "title": "提升音频驱动肖像动画的多模态优化",
                    "desc": "本论文提出了一种多模态奖励模型和自适应偏好优化框架，以改善音频驱动的肖像动画。通过引入Talking-Critic模型，研究者能够量化生成视频在多个维度上与人类偏好的对齐程度。为了支持这一模型，研究团队还构建了一个包含41万个偏好对的大规模多维人类偏好数据集Talking-NSQ。最后，提出的TLPO框架通过将偏好解耦为专门的专家模块，实现了在不同时间步和网络层之间的融合，从而在不相互干扰的情况下全面提升动画的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10868",
            "title": "TexVerse: A Universe of 3D Objects with High-Resolution Textures",
            "url": "https://huggingface.co/papers/2508.10868",
            "abstract": "TexVerse is a large-scale 3D dataset with high-resolution textures, including PBR materials, rigged models, and animated models, suitable for texture synthesis, PBR material development, and 3D vision tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce TexVerse, a large-scale 3D dataset featuring high-resolution textures. While recent advances in large-scale 3D datasets have enhanced high-resolution geometry generation, creating high-resolution textures end-to-end remains underexplored due to the lack of suitable datasets. TexVerse fills this gap with a curated collection of over 858K unique high-resolution 3D models sourced from Sketchfab, including more than 158K models with physically based rendering (PBR) materials. Each model encompasses all of its high-resolution variants, bringing the total to 1.6M 3D instances. TexVerse also includes specialized subsets: TexVerse-Skeleton, with 69K rigged models, and TexVerse-Animation, with 54K animated models, both preserving original skeleton and animation data uploaded by the user. We also provide detailed model annotations describing overall characteristics, structural components, and intricate features. TexVerse offers a high-quality data resource with wide-ranging potential applications in texture synthesis, PBR material development, animation, and various 3D vision and graphics tasks.",
            "score": 5,
            "issue_id": 5399,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "92f536825f2fff8b",
            "authors": [
                "Yibo Zhang",
                "Li Zhang",
                "Rui Ma",
                "Nan Cao"
            ],
            "affiliations": [
                "Fudan University",
                "Jilin University",
                "Shanghai Innovation Institute",
                "Tongji University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10868.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "TexVerse: Революция в мире 3D-текстур и материалов",
                    "desc": "TexVerse - это крупномасштабный набор данных 3D-моделей с высококачественными текстурами, включая PBR-материалы, риггированные и анимированные модели. Датасет содержит более 858 тысяч уникальных 3D-моделей с высоким разрешением, из которых более 158 тысяч имеют физически корректные материалы (PBR). TexVerse включает специализированные подмножества: TexVerse-Skeleton с 69 тысячами риггированных моделей и TexVerse-Animation с 54 тысячами анимированных моделей. Этот набор данных предоставляет широкие возможности для применения в задачах синтеза текстур, разработки PBR-материалов, анимации и различных задач компьютерного зрения и графики."
                },
                "en": {
                    "title": "TexVerse: Elevating 3D Graphics with High-Resolution Textures",
                    "desc": "TexVerse is a comprehensive 3D dataset that provides high-resolution textures and models, addressing the need for quality data in texture synthesis and PBR material development. It includes over 858,000 unique 3D models, with a significant portion featuring physically based rendering materials, making it ideal for advanced graphics tasks. The dataset also contains specialized subsets for rigged and animated models, ensuring that users have access to a variety of 3D instances with detailed annotations. This resource is designed to enhance machine learning applications in 3D vision and graphics, facilitating better model training and development."
                },
                "zh": {
                    "title": "TexVerse：高分辨率3D纹理的未来",
                    "desc": "TexVerse是一个大规模的3D数据集，包含高分辨率的纹理和PBR材料。该数据集汇集了超过858,000个独特的3D模型，其中包括158,000个具有物理基础渲染（PBR）材料的模型。TexVerse还提供了特定子集，如TexVerse-Skeleton和TexVerse-Animation，分别包含69,000个绑定模型和54,000个动画模型，保留了用户上传的原始骨骼和动画数据。这个数据集为纹理合成、PBR材料开发和3D视觉任务提供了高质量的数据资源。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.11616",
            "title": "Controlling Multimodal LLMs via Reward-guided Decoding",
            "url": "https://huggingface.co/papers/2508.11616",
            "abstract": "A reward-guided decoding method for Multimodal Large Language Models (MLLMs) improves visual grounding by controlling object precision and recall, offering dynamic trade-offs between compute and grounding quality.  \t\t\t\t\tAI-generated summary \t\t\t\t As Multimodal Large Language Models (MLLMs) gain widespread applicability, it is becoming increasingly desirable to adapt them for diverse user needs. In this paper, we study the adaptation of MLLMs through controlled decoding. To achieve this, we introduce the first method for reward-guided decoding of MLLMs and demonstrate its application in improving their visual grounding. Our method involves building reward models for visual grounding and using them to guide the MLLM's decoding process. Concretely, we build two separate reward models to independently control the degree of object precision and recall in the model's output. Our approach enables on-the-fly controllability of an MLLM's inference process in two ways: first, by giving control over the relative importance of each reward function during decoding, allowing a user to dynamically trade off object precision for recall in image captioning tasks; second, by giving control over the breadth of the search during decoding, allowing the user to control the trade-off between the amount of test-time compute and the degree of visual grounding. We evaluate our method on standard object hallucination benchmarks, showing that it provides significant controllability over MLLM inference, while consistently outperforming existing hallucination mitigation methods.",
            "score": 3,
            "issue_id": 5396,
            "pub_date": "2025-08-15",
            "pub_date_card": {
                "ru": "15 августа",
                "en": "August 15",
                "zh": "8月15日"
            },
            "hash": "a8b7ff59eca3faf3",
            "authors": [
                "Oscar Mañas",
                "Pierluca D'Oro",
                "Koustuv Sinha",
                "Adriana Romero-Soriano",
                "Michal Drozdzal",
                "Aishwarya Agrawal"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "McGill University",
                "Meta FAIR",
                "Mila - Quebec AI Institute",
                "Universite de Montreal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11616.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#alignment",
                    "#multimodal",
                    "#benchmark",
                    "#inference"
                ],
                "emoji": "🎛️",
                "ru": {
                    "title": "Управляемое декодирование MLLM для точной визуальной привязки",
                    "desc": "Статья представляет метод управляемого декодирования для мультимодальных больших языковых моделей (MLLM), улучшающий визуальную привязку. Авторы разработали модели вознаграждения для контроля точности и полноты распознавания объектов. Метод позволяет динамически регулировать баланс между вычислительными затратами и качеством визуальной привязки. Эксперименты показали превосходство предложенного подхода над существующими методами снижения галлюцинаций в MLLM."
                },
                "en": {
                    "title": "Dynamic Control for Enhanced Visual Grounding in MLLMs",
                    "desc": "This paper presents a novel reward-guided decoding method for Multimodal Large Language Models (MLLMs) that enhances visual grounding by managing object precision and recall. The authors introduce reward models that guide the decoding process, allowing users to adjust the importance of precision versus recall dynamically. This method not only improves the quality of image captioning tasks but also offers flexibility in balancing computational resources with grounding accuracy. The results demonstrate that this approach significantly outperforms existing methods for reducing object hallucination in MLLMs."
                },
                "zh": {
                    "title": "动态控制多模态语言模型的视觉定位",
                    "desc": "本文提出了一种基于奖励引导的解码方法，用于多模态大型语言模型（MLLMs），旨在提高视觉定位的精确度和召回率。我们构建了两个独立的奖励模型，分别控制模型输出中对象的精确度和召回率，从而实现动态的权衡。该方法允许用户在解码过程中实时调整奖励函数的重要性，以适应不同的任务需求。通过在标准对象幻觉基准上进行评估，我们的方法在控制MLLM推理方面表现出显著优势，超越了现有的幻觉缓解方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10461",
            "title": "X-Node: Self-Explanation is All We Need",
            "url": "https://huggingface.co/papers/2508.10461",
            "abstract": "X-Node is a self-explaining GNN framework that generates per-node explanations by encoding local topology features and integrating them into the message-passing pipeline, maintaining accuracy while enhancing interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a \"text-injection\" mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.",
            "score": 3,
            "issue_id": 5399,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "a9f5f6075fa0c601",
            "authors": [
                "Prajit Sengupta",
                "Islem Rekik"
            ],
            "affiliations": [
                "BASIRA Lab, Imperial-X (I-X) and Department of Computing, Imperial College London, London, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10461.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#interpretability",
                    "#healthcare",
                    "#graphs",
                    "#dataset",
                    "#architecture"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "X-Node: Самообъясняющиеся графовые нейронные сети для интерпретируемого анализа данных",
                    "desc": "X-Node - это фреймворк для графовых нейронных сетей (GNN), который генерирует объяснения для каждого узла, кодируя особенности локальной топологии и интегрируя их в процесс передачи сообщений. Он создает структурированный вектор контекста, кодирующий интерпретируемые признаки, такие как степень, центральность и кластеризация узла. Легковесный модуль Reasoner преобразует этот контекст в компактный вектор объяснения, который используется для реконструкции скрытого представления узла, генерации текстового объяснения с помощью языковой модели и улучшения работы самой GNN. X-Node поддерживает конкурентоспособную точность классификации, одновременно предоставляя объяснения для каждого узла."
                },
                "en": {
                    "title": "X-Node: Empowering GNNs with Self-Explanations for Trustworthy AI",
                    "desc": "X-Node is a novel framework for graph neural networks (GNNs) that enhances interpretability by providing explanations for each node's predictions. It encodes local topology features, such as degree and centrality, into a structured context vector that informs the decision-making process. A Reasoner module then translates this context into a compact explanation vector, which is used to reconstruct the node's embedding and generate natural language explanations. This approach allows X-Node to maintain high classification accuracy while offering clear insights into individual node decisions, making it suitable for critical applications like medical diagnostics."
                },
                "zh": {
                    "title": "X-Node：自解释的图神经网络框架",
                    "desc": "X-Node是一个自解释的图神经网络（GNN）框架，能够为每个节点生成解释。它通过编码局部拓扑特征并将其整合到消息传递过程中，既保持了准确性，又增强了可解释性。与传统的后置全局解释技术不同，X-Node为每个节点提供了个性化的解释，帮助理解节点的决策过程。我们在MedMNIST和MorphoMNIST两个图数据集上评估了X-Node，结果表明它在分类准确性上具有竞争力，同时生成了可信的节点解释。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.06429",
            "title": "SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via\n  Class-Conditioned Image Translation",
            "url": "https://huggingface.co/papers/2508.06429",
            "abstract": "A GAN-based semi-supervised learning framework improves medical image classification with minimal labeled data by integrating specialized neural networks and ensemble-based pseudo-labeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep learning has revolutionized medical imaging, but its effectiveness is severely limited by insufficient labeled training data. This paper introduces a novel GAN-based semi-supervised learning framework specifically designed for low labeled-data regimes, evaluated across settings with 5 to 50 labeled samples per class. Our approach integrates three specialized neural networks -- a generator for class-conditioned image translation, a discriminator for authenticity assessment and classification, and a dedicated classifier -- within a three-phase training framework. The method alternates between supervised training on limited labeled data and unsupervised learning that leverages abundant unlabeled images through image-to-image translation rather than generation from noise. We employ ensemble-based pseudo-labeling that combines confidence-weighted predictions from the discriminator and classifier with temporal consistency through exponential moving averaging, enabling reliable label estimation for unlabeled data. Comprehensive evaluation across eleven MedMNIST datasets demonstrates that our approach achieves statistically significant improvements over six state-of-the-art GAN-based semi-supervised methods, with particularly strong performance in the extreme 5-shot setting where the scarcity of labeled data is most challenging. The framework maintains its superiority across all evaluated settings (5, 10, 20, and 50 shots per class). Our approach offers a practical solution for medical imaging applications where annotation costs are prohibitive, enabling robust classification performance even with minimal labeled data. Code is available at https://github.com/GuidoManni/SPARSE.",
            "score": 1,
            "issue_id": 5395,
            "pub_date": "2025-08-08",
            "pub_date_card": {
                "ru": "8 августа",
                "en": "August 8",
                "zh": "8月8日"
            },
            "hash": "f1862f0be07f6b3d",
            "authors": [
                "Guido Manni",
                "Clemente Lauretti",
                "Loredana Zollo",
                "Paolo Soda"
            ],
            "affiliations": [
                "Unit of Advanced Robotics and Human-Centred Technologies, Department of Engineering, Università Campus Bio-Medico di Roma, Rome, Italy",
                "Unit of Artificial Intelligence and Computer Systems, Department of Engineering, Università Campus Bio-Medico di Roma, Rome, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.06429.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#dataset",
                    "#healthcare",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "Эффективная классификация медицинских изображений при минимуме размеченных данных",
                    "desc": "Статья представляет новый полу-контролируемый метод обучения на основе генеративно-состязательных сетей (GAN) для классификации медицинских изображений при ограниченном количестве размеченных данных. Подход включает три специализированные нейронные сети: генератор для условного преобразования изображений, дискриминатор для оценки подлинности и классификации, и отдельный классификатор. Метод использует ансамблевое псевдо-маркирование, комбинируя взвешенные по уверенности предсказания дискриминатора и классификатора. Эксперименты на 11 наборах данных MedMNIST показали статистически значимое улучшение по сравнению с шестью современными GAN-методами, особенно в условиях крайне малого количества размеченных примеров."
                },
                "en": {
                    "title": "Enhancing Medical Image Classification with Minimal Labels Using GANs",
                    "desc": "This paper presents a GAN-based semi-supervised learning framework that enhances medical image classification using very few labeled samples. It combines three specialized neural networks: a generator for transforming images based on class conditions, a discriminator for assessing image authenticity and classification, and a classifier for final predictions. The training process alternates between supervised learning on limited labeled data and unsupervised learning using abundant unlabeled images, employing image-to-image translation techniques. The method also utilizes ensemble-based pseudo-labeling to improve label estimation, demonstrating significant performance gains across various datasets, especially in scenarios with extremely limited labeled data."
                },
                "zh": {
                    "title": "基于GAN的半监督学习提升医疗图像分类",
                    "desc": "本文提出了一种基于生成对抗网络（GAN）的半监督学习框架，旨在改善医疗图像分类，尤其是在标注数据稀缺的情况下。该框架结合了三种专门的神经网络，包括用于图像转换的生成器、用于真实性评估和分类的判别器，以及专用分类器。通过在有限标注数据上进行监督训练和利用大量未标注图像进行无监督学习，该方法实现了有效的标签估计。实验结果表明，该方法在多个数据集上显著优于现有的半监督方法，尤其在标注样本极少的情况下表现出色。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10894",
            "title": "MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and\n  Multispectral Earth Observation Data",
            "url": "https://huggingface.co/papers/2508.10894",
            "abstract": "MAESTRO, an adapted Masked Autoencoder with optimized fusion strategies and spectral prior normalization, achieves state-of-the-art performance on multitemporal Earth observation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-supervised learning holds great promise for remote sensing, but standard self-supervised methods must be adapted to the unique characteristics of Earth observation data. We take a step in this direction by conducting a comprehensive benchmark of fusion strategies and reconstruction target normalization schemes for multimodal, multitemporal, and multispectral Earth observation data. Based on our findings, we propose MAESTRO, a novel adaptation of the Masked Autoencoder, featuring optimized fusion strategies and a tailored target normalization scheme that introduces a spectral prior as a self-supervisory signal. Evaluated on four Earth observation datasets, MAESTRO sets a new state-of-the-art on tasks that strongly rely on multitemporal dynamics, while remaining highly competitive on tasks dominated by a single mono-temporal modality. Code to reproduce all our experiments is available at https://github.com/ignf/maestro.",
            "score": 0,
            "issue_id": 5400,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "2dc651b08b24751f",
            "authors": [
                "Antoine Labatie",
                "Michael Vaccaro",
                "Nina Lardiere",
                "Anatol Garioud",
                "Nicolas Gonthier"
            ],
            "affiliations": [
                "Institut national de linformation géographique et forestière (IGN), France",
                "Univ Gustave Eiffel, ENSG, IGN, LASTIG, France"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10894.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#multimodal",
                    "#open_source",
                    "#dataset",
                    "#cv"
                ],
                "emoji": "🛰️",
                "ru": {
                    "title": "MAESTRO: прорыв в самообучении для анализа мультивременных спутниковых данных",
                    "desc": "MAESTRO - это адаптированный маскированный автоэнкодер для задач мультивременного дистанционного зондирования Земли. Он использует оптимизированные стратегии слияния и схему нормализации с учетом спектрального приора. MAESTRO достигает наилучших результатов на задачах, сильно зависящих от мультивременной динамики. Модель была протестирована на четырех наборах данных дистанционного зондирования Земли."
                },
                "en": {
                    "title": "MAESTRO: Revolutionizing Earth Observation with Self-Supervised Learning",
                    "desc": "MAESTRO is a new machine learning model designed for analyzing Earth observation data using self-supervised learning. It adapts the Masked Autoencoder architecture by incorporating optimized fusion strategies and a unique spectral prior normalization method. This approach allows MAESTRO to effectively handle multimodal and multitemporal data, improving performance on tasks that require understanding changes over time. The model has been tested on multiple datasets and has achieved state-of-the-art results, demonstrating its effectiveness in remote sensing applications."
                },
                "zh": {
                    "title": "MAESTRO：地球观测的自监督学习新突破",
                    "desc": "MAESTRO是一种改进的掩码自编码器，采用优化的融合策略和光谱先验归一化，能够在多时相地球观测任务中实现最先进的性能。该研究针对地球观测数据的独特特性，进行了全面的融合策略和重建目标归一化方案的基准测试。MAESTRO引入了光谱先验作为自监督信号，优化了多模态、多时相和多光谱数据的处理。经过在四个地球观测数据集上的评估，MAESTRO在依赖多时相动态的任务中设定了新的最先进水平，同时在单一时相模态主导的任务中也保持了高度竞争力。"
                }
            }
        }
    ],
    "link_prev": "2025-08-15.html",
    "link_next": "2025-08-19.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "15.08",
        "en": "08/15",
        "zh": "8月15日"
    },
    "short_date_next": {
        "ru": "19.08",
        "en": "08/19",
        "zh": "8月19日"
    },
    "categories": {
        "#dataset": 7,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 2,
        "#cv": 5,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 2,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}