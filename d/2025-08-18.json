{
    "date": {
        "ru": "18 августа",
        "en": "August 18",
        "zh": "8月18日"
    },
    "time_utc": "2025-08-18 08:18",
    "weekday": 0,
    "issue_id": 5399,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2508.11630",
            "title": "Thyme: Think Beyond Images",
            "url": "https://huggingface.co/papers/2508.11630",
            "abstract": "Thyme, a novel paradigm, enables MLLMs to autonomously perform image manipulations and computations, enhancing performance in perception and reasoning tasks through a two-stage training strategy and GRPO-ATS algorithm.  \t\t\t\t\tAI-generated summary \t\t\t\t Following OpenAI's introduction of the ``thinking with images'' concept, recent efforts have explored stimulating the use of visual information in the reasoning process to enhance model performance in perception and reasoning tasks. However, to the best of our knowledge, no open-source work currently offers a feature set as rich as proprietary models (O3), which can perform diverse image manipulations and simultaneously enhance logical reasoning capabilities through code. In this paper, we make a preliminary attempt in this direction by introducing Thyme (Think Beyond Images), a novel paradigm for enabling MLLMs to transcend existing ``think with images'' approaches by autonomously generating and executing diverse image processing and computational operations via executable code. This approach not only facilitates a rich, on-the-fly set of image manipulations (e.g., cropping, rotation, contrast enhancement) but also allows for mathematical computations, all while maintaining high autonomy in deciding when and how to apply these operations. We activate this capability through a two-stage training strategy: an initial SFT on a curated dataset of 500K samples to teach code generation, followed by a RL phase to refine decision-making. For the RL stage, we manually collect and design high-resolution question-answer pairs to increase the learning difficulty, and we propose GRPO-ATS (Group Relative Policy Optimization with Adaptive Temperature Sampling), an algorithm that applies distinct temperatures to text and code generation to balance reasoning exploration with code execution precision. We conduct extensive experimental analysis and ablation studies. Comprehensive evaluations on nearly 20 benchmarks show that Thyme yields significant and consistent performance gains, particularly in challenging high-resolution perception and complex reasoning tasks.",
            "score": 21,
            "issue_id": 5394,
            "pub_date": "2025-08-15",
            "pub_date_card": {
                "ru": "15 августа",
                "en": "August 15",
                "zh": "8月15日"
            },
            "hash": "9df408fb7ce360bf",
            "authors": [
                "Yi-Fan Zhang",
                "Xingyu Lu",
                "Shukang Yin",
                "Chaoyou Fu",
                "Wei Chen",
                "Xiao Hu",
                "Bin Wen",
                "Kaiyu Jiang",
                "Changyi Liu",
                "Tianke Zhang",
                "Haonan Fan",
                "Kaibing Chen",
                "Jiankang Chen",
                "Haojie Ding",
                "Kaiyu Tang",
                "Zhang Zhang",
                "Liang Wang",
                "Fan Yang",
                "Tingting Gao",
                "Guorui Zhou"
            ],
            "affiliations": [
                "CASIA",
                "Kwai Keye",
                "NJU",
                "THU",
                "USTC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11630.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#training",
                    "#optimization",
                    "#agents",
                    "#cv",
                    "#rl"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Тайм: новый уровень мышления ИИ с изображениями",
                    "desc": "Тайм (Thyme) - это новая парадигма, позволяющая мультимодальным языковым моделям (MLLM) автономно выполнять манипуляции с изображениями и вычисления. Она использует двухэтапную стратегию обучения и алгоритм GRPO-ATS для улучшения восприятия и рассуждений. Тайм позволяет моделям самостоятельно генерировать и выполнять разнообразные операции обработки изображений и вычисления через исполняемый код. Эксперименты на почти 20 бенчмарках показали значительное и последовательное улучшение производительности, особенно в сложных задачах восприятия изображений высокого разрешения и комплексных рассуждений."
                },
                "en": {
                    "title": "Think Beyond Images with Thyme!",
                    "desc": "Thyme is a new approach that allows Multi-Modal Language Models (MLLMs) to independently perform various image manipulations and computations, improving their ability to understand and reason about images. It uses a two-stage training method, starting with supervised fine-tuning on a large dataset to teach the model how to generate code for image processing. The second stage involves reinforcement learning to enhance the model's decision-making skills when applying these operations. The GRPO-ATS algorithm is introduced to optimize the balance between exploring reasoning tasks and executing code accurately, leading to better performance on numerous benchmarks."
                },
                "zh": {
                    "title": "超越图像思维的自主处理能力",
                    "desc": "Thyme是一种新颖的范式，旨在使多模态大语言模型（MLLMs）能够自主执行图像处理和计算操作，从而提升感知和推理任务的表现。该方法采用两阶段训练策略，首先在一个包含50万样本的精心策划数据集上进行监督微调（SFT），然后通过强化学习（RL）阶段来优化决策过程。我们提出的GRPO-ATS算法通过对文本和代码生成应用不同的温度，平衡推理探索与代码执行的精确性。实验结果表明，Thyme在近20个基准测试中表现出显著且一致的性能提升，尤其是在高分辨率感知和复杂推理任务中。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.11203",
            "title": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image\n  Translation",
            "url": "https://huggingface.co/papers/2508.11203",
            "abstract": "StyleMM constructs stylized 3DMMs from text descriptions using a diffusion model for image-to-image translation while preserving facial attributes.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce StyleMM, a novel framework that can construct a stylized 3D Morphable Model (3DMM) based on user-defined text descriptions specifying a target style. Building upon a pre-trained mesh deformation network and a texture generator for original 3DMM-based realistic human faces, our approach fine-tunes these models using stylized facial images generated via text-guided image-to-image (i2i) translation with a diffusion model, which serve as stylization targets for the rendered mesh. To prevent undesired changes in identity, facial alignment, or expressions during i2i translation, we introduce a stylization method that explicitly preserves the facial attributes of the source image. By maintaining these critical attributes during image stylization, the proposed approach ensures consistent 3D style transfer across the 3DMM parameter space through image-based training. Once trained, StyleMM enables feed-forward generation of stylized face meshes with explicit control over shape, expression, and texture parameters, producing meshes with consistent vertex connectivity and animatability. Quantitative and qualitative evaluations demonstrate that our approach outperforms state-of-the-art methods in terms of identity-level facial diversity and stylization capability. The code and videos are available at [kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).",
            "score": 4,
            "issue_id": 5395,
            "pub_date": "2025-08-15",
            "pub_date_card": {
                "ru": "15 августа",
                "en": "August 15",
                "zh": "8月15日"
            },
            "hash": "9af71c9ddc545447",
            "authors": [
                "Seungmi Lee",
                "Kwan Yun",
                "Junyong Noh"
            ],
            "affiliations": [
                "KAIST, Visual Media Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11203.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#3d",
                    "#diffusion",
                    "#open_source"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Стильные 3D-лица из текста: новый уровень контроля и реализма",
                    "desc": "StyleMM - это новая система для создания стилизованных 3D-моделей лиц на основе текстовых описаний. Она использует диффузионную модель для преобразования изображений, сохраняя при этом ключевые черты лица. StyleMM обучается на стилизованных изображениях лиц и позволяет генерировать 3D-модели с контролем над формой, выражением и текстурой. Эксперименты показывают, что этот метод превосходит современные аналоги по разнообразию и возможностям стилизации лиц."
                },
                "en": {
                    "title": "Transforming Text to Stylized 3D Faces with Precision",
                    "desc": "StyleMM is a new framework that creates stylized 3D Morphable Models (3DMMs) from text descriptions, using a diffusion model for image-to-image translation. It fine-tunes a pre-trained mesh deformation network and a texture generator to ensure that the generated 3D faces maintain their original facial attributes while adopting new styles. The method prevents changes in identity and expression during the stylization process, allowing for consistent 3D style transfer. Ultimately, StyleMM enables the generation of customizable and animatable stylized face meshes, outperforming existing methods in facial diversity and stylization quality."
                },
                "zh": {
                    "title": "风格化3D模型的智能生成",
                    "desc": "StyleMM是一个新颖的框架，可以根据用户定义的文本描述构建风格化的3D可变形模型（3DMM）。该方法利用预训练的网格变形网络和纹理生成器，通过扩散模型进行图像到图像的翻译，生成风格化的面部图像作为目标。为了保持面部特征不变，我们引入了一种显式保留源图像面部属性的风格化方法。经过训练后，StyleMM能够生成具有明确形状、表情和纹理参数控制的风格化面部网格，确保在3DMM参数空间中的一致性风格转移。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.11116",
            "title": "PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical\n  Register Indexing",
            "url": "https://huggingface.co/papers/2508.11116",
            "abstract": "PaperRegister enhances paper search by using hierarchical indexing and adaptive retrieval, supporting flexible and fine-grained queries beyond traditional abstract-based systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Paper search is an important activity for researchers, typically involving using a query with description of a topic to find relevant papers. As research deepens, paper search requirements may become more flexible, sometimes involving specific details such as module configuration rather than being limited to coarse-grained topics. However, previous paper search systems are unable to meet these flexible-grained requirements, as these systems mainly collect paper abstracts to construct index of corpus, which lack detailed information to support retrieval by finer-grained queries. In this work, we propose PaperRegister, consisted of offline hierarchical indexing and online adaptive retrieval, transforming traditional abstract-based index into hierarchical index tree for paper search, thereby supporting queries at flexible granularity. Experiments on paper search tasks across a range of granularity demonstrate that PaperRegister achieves the state-of-the-art performance, and particularly excels in fine-grained scenarios, highlighting the good potential as an effective solution for flexible-grained paper search in real-world applications. Code for this work is in https://github.com/Li-Z-Q/PaperRegister.",
            "score": 3,
            "issue_id": 5394,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "f0b9bc66fdef20ad",
            "authors": [
                "Zhuoqun Li",
                "Xuanang Chen",
                "Hongyu Lin",
                "Yaojie Lu",
                "Xianpei Han",
                "Le Sun"
            ],
            "affiliations": [
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11116.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Гибкий поиск научных статей на любом уровне детализации",
                    "desc": "PaperRegister - это новая система поиска научных статей, использующая иерархическую индексацию и адаптивный поиск. Она поддерживает гибкие и детализированные запросы, выходящие за рамки традиционных систем на основе аннотаций. PaperRegister преобразует обычный индекс на основе аннотаций в иерархическое дерево индексов, что позволяет выполнять поиск с различной степенью детализации. Эксперименты показали, что PaperRegister достигает наилучших результатов, особенно в сценариях с высокой степенью детализации запросов."
                },
                "en": {
                    "title": "Revolutionizing Paper Search with Hierarchical Indexing",
                    "desc": "PaperRegister is a novel system designed to improve the search for academic papers by utilizing hierarchical indexing and adaptive retrieval methods. Unlike traditional systems that rely solely on abstracts, PaperRegister allows for more detailed and flexible queries, accommodating specific research needs. The system organizes papers into a hierarchical index tree, enabling users to perform searches at various levels of granularity. Experimental results show that PaperRegister outperforms existing methods, particularly in scenarios requiring fine-grained search capabilities."
                },
                "zh": {
                    "title": "PaperRegister：灵活细粒度论文搜索的新方法",
                    "desc": "PaperRegister 是一种改进论文搜索的方法，它通过层次索引和自适应检索来增强搜索体验。传统的论文搜索系统主要依赖摘要来构建索引，无法满足细粒度查询的需求。PaperRegister 通过将传统的基于摘要的索引转变为层次索引树，支持更灵活的查询方式。实验结果表明，PaperRegister 在各种细粒度的论文搜索任务中表现优异，展示了其在实际应用中的良好潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.11616",
            "title": "Controlling Multimodal LLMs via Reward-guided Decoding",
            "url": "https://huggingface.co/papers/2508.11616",
            "abstract": "A reward-guided decoding method for Multimodal Large Language Models (MLLMs) improves visual grounding by controlling object precision and recall, offering dynamic trade-offs between compute and grounding quality.  \t\t\t\t\tAI-generated summary \t\t\t\t As Multimodal Large Language Models (MLLMs) gain widespread applicability, it is becoming increasingly desirable to adapt them for diverse user needs. In this paper, we study the adaptation of MLLMs through controlled decoding. To achieve this, we introduce the first method for reward-guided decoding of MLLMs and demonstrate its application in improving their visual grounding. Our method involves building reward models for visual grounding and using them to guide the MLLM's decoding process. Concretely, we build two separate reward models to independently control the degree of object precision and recall in the model's output. Our approach enables on-the-fly controllability of an MLLM's inference process in two ways: first, by giving control over the relative importance of each reward function during decoding, allowing a user to dynamically trade off object precision for recall in image captioning tasks; second, by giving control over the breadth of the search during decoding, allowing the user to control the trade-off between the amount of test-time compute and the degree of visual grounding. We evaluate our method on standard object hallucination benchmarks, showing that it provides significant controllability over MLLM inference, while consistently outperforming existing hallucination mitigation methods.",
            "score": 2,
            "issue_id": 5396,
            "pub_date": "2025-08-15",
            "pub_date_card": {
                "ru": "15 августа",
                "en": "August 15",
                "zh": "8月15日"
            },
            "hash": "a8b7ff59eca3faf3",
            "authors": [
                "Oscar Mañas",
                "Pierluca D'Oro",
                "Koustuv Sinha",
                "Adriana Romero-Soriano",
                "Michal Drozdzal",
                "Aishwarya Agrawal"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "McGill University",
                "Meta FAIR",
                "Mila - Quebec AI Institute",
                "Universite de Montreal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11616.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#alignment",
                    "#multimodal",
                    "#benchmark",
                    "#inference"
                ],
                "emoji": "🎛️",
                "ru": {
                    "title": "Управляемое декодирование MLLM для точной визуальной привязки",
                    "desc": "Статья представляет метод управляемого декодирования для мультимодальных больших языковых моделей (MLLM), улучшающий визуальную привязку. Авторы разработали модели вознаграждения для контроля точности и полноты распознавания объектов. Метод позволяет динамически регулировать баланс между вычислительными затратами и качеством визуальной привязки. Эксперименты показали превосходство предложенного подхода над существующими методами снижения галлюцинаций в MLLM."
                },
                "en": {
                    "title": "Dynamic Control for Enhanced Visual Grounding in MLLMs",
                    "desc": "This paper presents a novel reward-guided decoding method for Multimodal Large Language Models (MLLMs) that enhances visual grounding by managing object precision and recall. The authors introduce reward models that guide the decoding process, allowing users to adjust the importance of precision versus recall dynamically. This method not only improves the quality of image captioning tasks but also offers flexibility in balancing computational resources with grounding accuracy. The results demonstrate that this approach significantly outperforms existing methods for reducing object hallucination in MLLMs."
                },
                "zh": {
                    "title": "动态控制多模态语言模型的视觉定位",
                    "desc": "本文提出了一种基于奖励引导的解码方法，用于多模态大型语言模型（MLLMs），旨在提高视觉定位的精确度和召回率。我们构建了两个独立的奖励模型，分别控制模型输出中对象的精确度和召回率，从而实现动态的权衡。该方法允许用户在解码过程中实时调整奖励函数的重要性，以适应不同的任务需求。通过在标准对象幻觉基准上进行评估，我们的方法在控制MLLM推理方面表现出显著优势，超越了现有的幻觉缓解方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.11255",
            "title": "FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for\n  Audio-Driven Portrait Animation",
            "url": "https://huggingface.co/papers/2508.11255",
            "abstract": "A multimodal reward model and adaptive preference optimization framework improve audio-driven portrait animation by aligning with human preferences across multiple dimensions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in audio-driven portrait animation have demonstrated impressive capabilities. However, existing methods struggle to align with fine-grained human preferences across multiple dimensions, such as motion naturalness, lip-sync accuracy, and visual quality. This is due to the difficulty of optimizing among competing preference objectives, which often conflict with one another, and the scarcity of large-scale, high-quality datasets with multidimensional preference annotations. To address these, we first introduce Talking-Critic, a multimodal reward model that learns human-aligned reward functions to quantify how well generated videos satisfy multidimensional expectations. Leveraging this model, we curate Talking-NSQ, a large-scale multidimensional human preference dataset containing 410K preference pairs. Finally, we propose Timestep-Layer adaptive multi-expert Preference Optimization (TLPO), a novel framework for aligning diffusion-based portrait animation models with fine-grained, multidimensional preferences. TLPO decouples preferences into specialized expert modules, which are then fused across timesteps and network layers, enabling comprehensive, fine-grained enhancement across all dimensions without mutual interference. Experiments demonstrate that Talking-Critic significantly outperforms existing methods in aligning with human preference ratings. Meanwhile, TLPO achieves substantial improvements over baseline models in lip-sync accuracy, motion naturalness, and visual quality, exhibiting superior performance in both qualitative and quantitative evaluations. Ours project page: https://fantasy-amap.github.io/fantasy-talking2/",
            "score": 1,
            "issue_id": 5398,
            "pub_date": "2025-08-15",
            "pub_date_card": {
                "ru": "15 августа",
                "en": "August 15",
                "zh": "8月15日"
            },
            "hash": "aff56c9a04ada350",
            "authors": [
                "MengChao Wang",
                "Qiang Wang",
                "Fan Jiang",
                "Mu Xu"
            ],
            "affiliations": [
                "AMAP, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.11255.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#training",
                    "#optimization",
                    "#multimodal",
                    "#diffusion",
                    "#alignment",
                    "#dataset"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "Многомерная оптимизация анимации портретов с учетом человеческих предпочтений",
                    "desc": "Статья представляет новый подход к улучшению анимации портретов, управляемой аудио. Авторы разработали мультимодальную модель вознаграждения Talking-Critic, которая учится оценивать качество генерируемых видео по нескольким измерениям. На основе этой модели был создан большой датасет Talking-NSQ с многомерными оценками предпочтений. Предложен фреймворк TLPO для точной настройки диффузионных моделей анимации портретов в соответствии с многомерными предпочтениями."
                },
                "en": {
                    "title": "Enhancing Portrait Animation with Human-Aligned Preferences",
                    "desc": "This paper presents a new approach to improve audio-driven portrait animation by focusing on human preferences. It introduces a multimodal reward model called Talking-Critic, which quantifies how well generated animations meet various human expectations like lip-sync accuracy and visual quality. Additionally, the authors create a large dataset, Talking-NSQ, with 410,000 preference pairs to train their model effectively. They also propose a novel optimization framework, TLPO, that allows for better alignment of animation models with these preferences by using specialized expert modules to enhance multiple dimensions simultaneously."
                },
                "zh": {
                    "title": "提升音频驱动肖像动画的多模态优化",
                    "desc": "本论文提出了一种多模态奖励模型和自适应偏好优化框架，以改善音频驱动的肖像动画。通过引入Talking-Critic模型，研究者能够量化生成视频在多个维度上与人类偏好的对齐程度。为了支持这一模型，研究团队还构建了一个包含41万个偏好对的大规模多维人类偏好数据集Talking-NSQ。最后，提出的TLPO框架通过将偏好解耦为专门的专家模块，实现了在不同时间步和网络层之间的融合，从而在不相互干扰的情况下全面提升动画的质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10868",
            "title": "TexVerse: A Universe of 3D Objects with High-Resolution Textures",
            "url": "https://huggingface.co/papers/2508.10868",
            "abstract": "TexVerse is a large-scale 3D dataset with high-resolution textures, including PBR materials, rigged models, and animated models, suitable for texture synthesis, PBR material development, and 3D vision tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce TexVerse, a large-scale 3D dataset featuring high-resolution textures. While recent advances in large-scale 3D datasets have enhanced high-resolution geometry generation, creating high-resolution textures end-to-end remains underexplored due to the lack of suitable datasets. TexVerse fills this gap with a curated collection of over 858K unique high-resolution 3D models sourced from Sketchfab, including more than 158K models with physically based rendering (PBR) materials. Each model encompasses all of its high-resolution variants, bringing the total to 1.6M 3D instances. TexVerse also includes specialized subsets: TexVerse-Skeleton, with 69K rigged models, and TexVerse-Animation, with 54K animated models, both preserving original skeleton and animation data uploaded by the user. We also provide detailed model annotations describing overall characteristics, structural components, and intricate features. TexVerse offers a high-quality data resource with wide-ranging potential applications in texture synthesis, PBR material development, animation, and various 3D vision and graphics tasks.",
            "score": 1,
            "issue_id": 5399,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "92f536825f2fff8b",
            "authors": [
                "Yibo Zhang",
                "Li Zhang",
                "Rui Ma",
                "Nan Cao"
            ],
            "affiliations": [
                "Fudan University",
                "Jilin University",
                "Shanghai Innovation Institute",
                "Tongji University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10868.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#dataset"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "TexVerse: Революция в мире 3D-текстур и материалов",
                    "desc": "TexVerse - это крупномасштабный набор данных 3D-моделей с высококачественными текстурами, включая PBR-материалы, риггированные и анимированные модели. Датасет содержит более 858 тысяч уникальных 3D-моделей с высоким разрешением, из которых более 158 тысяч имеют физически корректные материалы (PBR). TexVerse включает специализированные подмножества: TexVerse-Skeleton с 69 тысячами риггированных моделей и TexVerse-Animation с 54 тысячами анимированных моделей. Этот набор данных предоставляет широкие возможности для применения в задачах синтеза текстур, разработки PBR-материалов, анимации и различных задач компьютерного зрения и графики."
                },
                "en": {
                    "title": "TexVerse: Elevating 3D Graphics with High-Resolution Textures",
                    "desc": "TexVerse is a comprehensive 3D dataset that provides high-resolution textures and models, addressing the need for quality data in texture synthesis and PBR material development. It includes over 858,000 unique 3D models, with a significant portion featuring physically based rendering materials, making it ideal for advanced graphics tasks. The dataset also contains specialized subsets for rigged and animated models, ensuring that users have access to a variety of 3D instances with detailed annotations. This resource is designed to enhance machine learning applications in 3D vision and graphics, facilitating better model training and development."
                },
                "zh": {
                    "title": "TexVerse：高分辨率3D纹理的未来",
                    "desc": "TexVerse是一个大规模的3D数据集，包含高分辨率的纹理和PBR材料。该数据集汇集了超过858,000个独特的3D模型，其中包括158,000个具有物理基础渲染（PBR）材料的模型。TexVerse还提供了特定子集，如TexVerse-Skeleton和TexVerse-Animation，分别包含69,000个绑定模型和54,000个动画模型，保留了用户上传的原始骨骼和动画数据。这个数据集为纹理合成、PBR材料开发和3D视觉任务提供了高质量的数据资源。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.10461",
            "title": "X-Node: Self-Explanation is All We Need",
            "url": "https://huggingface.co/papers/2508.10461",
            "abstract": "X-Node is a self-explaining GNN framework that generates per-node explanations by encoding local topology features and integrating them into the message-passing pipeline, maintaining accuracy while enhancing interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a \"text-injection\" mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.",
            "score": 1,
            "issue_id": 5399,
            "pub_date": "2025-08-14",
            "pub_date_card": {
                "ru": "14 августа",
                "en": "August 14",
                "zh": "8月14日"
            },
            "hash": "a9f5f6075fa0c601",
            "authors": [
                "Prajit Sengupta",
                "Islem Rekik"
            ],
            "affiliations": [
                "BASIRA Lab, Imperial-X (I-X) and Department of Computing, Imperial College London, London, United Kingdom"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.10461.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#interpretability",
                    "#healthcare",
                    "#graphs",
                    "#dataset",
                    "#architecture"
                ],
                "emoji": "🕸️",
                "ru": {
                    "title": "X-Node: Самообъясняющиеся графовые нейронные сети для интерпретируемого анализа данных",
                    "desc": "X-Node - это фреймворк для графовых нейронных сетей (GNN), который генерирует объяснения для каждого узла, кодируя особенности локальной топологии и интегрируя их в процесс передачи сообщений. Он создает структурированный вектор контекста, кодирующий интерпретируемые признаки, такие как степень, центральность и кластеризация узла. Легковесный модуль Reasoner преобразует этот контекст в компактный вектор объяснения, который используется для реконструкции скрытого представления узла, генерации текстового объяснения с помощью языковой модели и улучшения работы самой GNN. X-Node поддерживает конкурентоспособную точность классификации, одновременно предоставляя объяснения для каждого узла."
                },
                "en": {
                    "title": "X-Node: Empowering GNNs with Self-Explanations for Trustworthy AI",
                    "desc": "X-Node is a novel framework for graph neural networks (GNNs) that enhances interpretability by providing explanations for each node's predictions. It encodes local topology features, such as degree and centrality, into a structured context vector that informs the decision-making process. A Reasoner module then translates this context into a compact explanation vector, which is used to reconstruct the node's embedding and generate natural language explanations. This approach allows X-Node to maintain high classification accuracy while offering clear insights into individual node decisions, making it suitable for critical applications like medical diagnostics."
                },
                "zh": {
                    "title": "X-Node：自解释的图神经网络框架",
                    "desc": "X-Node是一个自解释的图神经网络（GNN）框架，能够为每个节点生成解释。它通过编码局部拓扑特征并将其整合到消息传递过程中，既保持了准确性，又增强了可解释性。与传统的后置全局解释技术不同，X-Node为每个节点提供了个性化的解释，帮助理解节点的决策过程。我们在MedMNIST和MorphoMNIST两个图数据集上评估了X-Node，结果表明它在分类准确性上具有竞争力，同时生成了可信的节点解释。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.06429",
            "title": "SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via\n  Class-Conditioned Image Translation",
            "url": "https://huggingface.co/papers/2508.06429",
            "abstract": "A GAN-based semi-supervised learning framework improves medical image classification with minimal labeled data by integrating specialized neural networks and ensemble-based pseudo-labeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep learning has revolutionized medical imaging, but its effectiveness is severely limited by insufficient labeled training data. This paper introduces a novel GAN-based semi-supervised learning framework specifically designed for low labeled-data regimes, evaluated across settings with 5 to 50 labeled samples per class. Our approach integrates three specialized neural networks -- a generator for class-conditioned image translation, a discriminator for authenticity assessment and classification, and a dedicated classifier -- within a three-phase training framework. The method alternates between supervised training on limited labeled data and unsupervised learning that leverages abundant unlabeled images through image-to-image translation rather than generation from noise. We employ ensemble-based pseudo-labeling that combines confidence-weighted predictions from the discriminator and classifier with temporal consistency through exponential moving averaging, enabling reliable label estimation for unlabeled data. Comprehensive evaluation across eleven MedMNIST datasets demonstrates that our approach achieves statistically significant improvements over six state-of-the-art GAN-based semi-supervised methods, with particularly strong performance in the extreme 5-shot setting where the scarcity of labeled data is most challenging. The framework maintains its superiority across all evaluated settings (5, 10, 20, and 50 shots per class). Our approach offers a practical solution for medical imaging applications where annotation costs are prohibitive, enabling robust classification performance even with minimal labeled data. Code is available at https://github.com/GuidoManni/SPARSE.",
            "score": 1,
            "issue_id": 5395,
            "pub_date": "2025-08-08",
            "pub_date_card": {
                "ru": "8 августа",
                "en": "August 8",
                "zh": "8月8日"
            },
            "hash": "f1862f0be07f6b3d",
            "authors": [
                "Guido Manni",
                "Clemente Lauretti",
                "Loredana Zollo",
                "Paolo Soda"
            ],
            "affiliations": [
                "Unit of Advanced Robotics and Human-Centred Technologies, Department of Engineering, Università Campus Bio-Medico di Roma, Rome, Italy",
                "Unit of Artificial Intelligence and Computer Systems, Department of Engineering, Università Campus Bio-Medico di Roma, Rome, Italy"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.06429.jpg",
            "data": {
                "categories": [
                    "#low_resource",
                    "#dataset",
                    "#healthcare",
                    "#training",
                    "#synthetic"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "Эффективная классификация медицинских изображений при минимуме размеченных данных",
                    "desc": "Статья представляет новый полу-контролируемый метод обучения на основе генеративно-состязательных сетей (GAN) для классификации медицинских изображений при ограниченном количестве размеченных данных. Подход включает три специализированные нейронные сети: генератор для условного преобразования изображений, дискриминатор для оценки подлинности и классификации, и отдельный классификатор. Метод использует ансамблевое псевдо-маркирование, комбинируя взвешенные по уверенности предсказания дискриминатора и классификатора. Эксперименты на 11 наборах данных MedMNIST показали статистически значимое улучшение по сравнению с шестью современными GAN-методами, особенно в условиях крайне малого количества размеченных примеров."
                },
                "en": {
                    "title": "Enhancing Medical Image Classification with Minimal Labels Using GANs",
                    "desc": "This paper presents a GAN-based semi-supervised learning framework that enhances medical image classification using very few labeled samples. It combines three specialized neural networks: a generator for transforming images based on class conditions, a discriminator for assessing image authenticity and classification, and a classifier for final predictions. The training process alternates between supervised learning on limited labeled data and unsupervised learning using abundant unlabeled images, employing image-to-image translation techniques. The method also utilizes ensemble-based pseudo-labeling to improve label estimation, demonstrating significant performance gains across various datasets, especially in scenarios with extremely limited labeled data."
                },
                "zh": {
                    "title": "基于GAN的半监督学习提升医疗图像分类",
                    "desc": "本文提出了一种基于生成对抗网络（GAN）的半监督学习框架，旨在改善医疗图像分类，尤其是在标注数据稀缺的情况下。该框架结合了三种专门的神经网络，包括用于图像转换的生成器、用于真实性评估和分类的判别器，以及专用分类器。通过在有限标注数据上进行监督训练和利用大量未标注图像进行无监督学习，该方法实现了有效的标签估计。实验结果表明，该方法在多个数据集上显著优于现有的半监督方法，尤其在标注样本极少的情况下表现出色。"
                }
            }
        }
    ],
    "link_prev": "2025-08-15.html",
    "link_next": "2025-08-19.html",
    "link_month": "2025-08.html",
    "short_date_prev": {
        "ru": "15.08",
        "en": "08/15",
        "zh": "8月15日"
    },
    "short_date_next": {
        "ru": "19.08",
        "en": "08/19",
        "zh": "8月19日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 3,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 2,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 2,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    }
}