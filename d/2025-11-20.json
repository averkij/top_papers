{
    "date": {
        "ru": "20 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        "en": "November 20",
        "zh": "11æœˆ20æ—¥"
    },
    "time_utc": "2025-11-20 09:00",
    "weekday": 3,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2025-11-20",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2511.14993",
            "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation",
            "url": "https://huggingface.co/papers/2511.14993",
            "abstract": "Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.",
            "score": 222,
            "issue_id": 1,
            "pub_date": "2025-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "69648feecbbe5248",
            "authors": [
                "Vladimir Arkhipkin",
                "Vladimir Korviakov",
                "Nikolai Gerasimenko",
                "Denis Parkhomenko",
                "Viacheslav Vasilev",
                "Alexey Letunovskiy",
                "Nikolai Vaulin",
                "Maria Kovaleva",
                "Ivan Kirillov",
                "Lev Novitskiy",
                "Denis Koposov",
                "Nikita Kiselev",
                "Alexander Varlamov",
                "Dmitrii Mikhailov",
                "Vladimir Polovnikov",
                "Andrey Shutkin",
                "Julia Agafonova",
                "Ilya Vasiliev",
                "Anastasiia Kargapoltseva",
                "Anna Dmitrienko",
                "Anastasia Maltseva",
                "Anna Averchenkova",
                "Olga Kim",
                "Tatiana Nikulina",
                "Denis Dimitrov"
            ],
            "affiliations": [
                "Kandinsky Lab"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.14993.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#open_source",
                    "#video",
                    "#data",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Kandinsky 5.0: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑĞµĞ¼ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Kandinsky 5.0 â€” ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ»Ğ¸Ğ½ĞµĞµĞº Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (6B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ (2B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²), Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ° (19B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: ÑĞ±Ğ¾Ñ€, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ, Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ reinforcement learning Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸."
                },
                "en": {
                    "title": "Kandinsky 5.0: Revolutionizing High-Quality Image and Video Generation",
                    "desc": "Kandinsky 5.0 is a cutting-edge generative model designed for creating high-resolution images and short videos. It includes three main versions: Image Lite for image generation, Video Lite for quick text-to-video tasks, and Video Pro for high-quality video generation. The model benefits from a robust training pipeline that utilizes advanced techniques like self-supervised fine-tuning and reinforcement learning to enhance output quality. With its open-source release, Kandinsky 5.0 aims to support researchers in developing high-quality generative applications."
                },
                "zh": {
                    "title": "Kandinsky 5.0ï¼šé«˜è´¨é‡ç”Ÿæˆæ¨¡å‹çš„æ–°æ—¶ä»£",
                    "desc": "Kandinsky 5.0 æ˜¯ä¸€ç³»åˆ—å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ï¼Œä¸“æ³¨äºé«˜åˆ†è¾¨ç‡å›¾åƒå’ŒçŸ­è§†é¢‘çš„åˆæˆã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ç§æ ¸å¿ƒæ¨¡å‹ï¼šKandinsky 5.0 Image Liteï¼Œå…·æœ‰6Bå‚æ•°çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼›Kandinsky 5.0 Video Liteï¼Œå¿«é€Ÿè½»é‡çš„2Bå‚æ•°æ–‡æœ¬åˆ°è§†é¢‘å’Œå›¾åƒåˆ°è§†é¢‘æ¨¡å‹ï¼›ä»¥åŠKandinsky 5.0 Video Proï¼Œå…·æœ‰19Bå‚æ•°çš„é«˜è´¨é‡è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚æŠ¥å‘Šè¯¦ç»†ä»‹ç»äº†æ•°æ®ç­–åˆ’ç”Ÿå‘½å‘¨æœŸï¼ŒåŒ…æ‹¬æ•°æ®æ”¶é›†ã€å¤„ç†ã€è¿‡æ»¤å’Œèšç±»ï¼Œä»¥åŠå¤šé˜¶æ®µè®­ç»ƒç®¡é“ä¸­çš„è´¨é‡æå‡æŠ€æœ¯ï¼Œå¦‚è‡ªç›‘ç£å¾®è°ƒå’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒã€‚Kandinsky 5.0 é€šè¿‡åˆ›æ–°çš„æ¶æ„å’Œè®­ç»ƒä¼˜åŒ–ï¼Œå®ç°äº†é«˜ç”Ÿæˆé€Ÿåº¦å’Œå“è¶Šçš„æ€§èƒ½ï¼Œé€‚ç”¨äºå¤šç§ç”Ÿæˆåº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.15065",
            "title": "Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks",
            "url": "https://huggingface.co/papers/2511.15065",
            "abstract": "VR-Bench evaluates video models' spatial reasoning capabilities through maze-solving tasks, demonstrating that these models excel in spatial perception and reasoning, outperforming VLMs and benefiting from diverse sampling during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.",
            "score": 74,
            "issue_id": 1,
            "pub_date": "2025-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "09e9785ac42b8ebc",
            "authors": [
                "Cheng Yang",
                "Haiyuan Wan",
                "Yiran Peng",
                "Xin Cheng",
                "Zhaoyang Yu",
                "Jiayi Zhang",
                "Junchi Yu",
                "Xinlei Yu",
                "Xiawu Zheng",
                "Dongzhan Zhou",
                "Chenglin Wu"
            ],
            "affiliations": [
                "DeepWisdom",
                "Hong Kong University of Science and Technology (GuangZhou)",
                "National University of Singapore",
                "Renmin University of China",
                "Shanghai Artificial Intelligence Laboratory",
                "Tsinghua University",
                "University of Oxford",
                "Xiamen University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.15065.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#video",
                    "#games",
                    "#reasoning"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ VR-Bench â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ğ±Ğ¸Ñ€Ğ¸Ğ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğµ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 10â€“20 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Spatial Reasoning in Video Models with VR-Bench",
                    "desc": "The paper introduces VR-Bench, a benchmark designed to evaluate the spatial reasoning capabilities of video models through maze-solving tasks. It demonstrates that these models excel in spatial perception and reasoning, outperforming traditional vision-language models (VLMs). The study reveals that video models can effectively perform reasoning via video generation, leveraging explicit spatial layouts and temporal continuity. Additionally, it finds that diverse sampling during inference significantly enhances the reliability of reasoning outcomes by 10-20%."
                },
                "zh": {
                    "title": "è§†é¢‘æ¨¡å‹ï¼šç©ºé—´æ¨ç†çš„æ–°å‰æ²¿",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†VR-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªè¯„ä¼°è§†é¢‘æ¨¡å‹ç©ºé—´æ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œä¸»è¦é€šè¿‡è¿·å®«æ±‚è§£ä»»åŠ¡è¿›è¡Œæµ‹è¯•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè§†é¢‘æ¨¡å‹åœ¨ç©ºé—´æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚é€šè¿‡å¤šæ ·åŒ–çš„é‡‡æ ·æ–¹æ³•ï¼Œè§†é¢‘æ¨¡å‹åœ¨æ¨ç†æ—¶çš„å¯é æ€§æé«˜äº†10%åˆ°20%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†è§†é¢‘ç”Ÿæˆåœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­çš„ç‹¬ç‰¹æ½œåŠ›å’Œå¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.15593",
            "title": "What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity",
            "url": "https://huggingface.co/papers/2511.15593",
            "abstract": "Ideation diversity significantly enhances the performance of AI research agents across various models and scaffolds on the MLE-bench benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.",
            "score": 55,
            "issue_id": 1,
            "pub_date": "2025-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "bee400ab5faf8c87",
            "authors": [
                "Alexis Audran-Reiss",
                "Jordi Armengol-EstapÃ©",
                "Karen Hambardzumyan",
                "Amar Budhiraja",
                "Martin Josifoski",
                "Edan Toledo",
                "Rishi Hazra",
                "Despoina Magka",
                "Michael Shvartsman",
                "Parth Pathak",
                "Justine T Kao",
                "Lucia Cipolina-Kun",
                "Bhavul Gauri",
                "Jean-Christophe Gagnon-Audet",
                "Emanuel Tewolde",
                "Jenny Zhang",
                "Taco Cohen",
                "Yossi Adi",
                "Tatiana Shavrina",
                "Yoram Bachrach"
            ],
            "affiliations": [
                "FAIR at Meta",
                "Meta SuperIntelligence Labs",
                "University College London",
                "University of British Columbia"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.15593.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents"
                ],
                "emoji": "ğŸ’¡",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸Ğ´ĞµĞ¹ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒÑĞ¿ĞµÑ…Ñƒ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ñ€Ğ¾Ğ»ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸Ğ´ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MLE-bench Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ´ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¸Ğ´ĞµĞ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Boosting AI Performance through Ideation Diversity",
                    "desc": "This paper investigates how ideation diversity affects the performance of AI research agents in machine learning. By analyzing agent trajectories on the MLE-bench benchmark, the authors find that agents with greater ideation diversity tend to perform better. They conduct controlled experiments to show that increasing ideation diversity leads to improved outcomes in various models and agent scaffolds. Additionally, the study confirms these results using multiple performance metrics, reinforcing the importance of ideation diversity in enhancing AI research capabilities."
                },
                "zh": {
                    "title": "åˆ›æ„å¤šæ ·æ€§æå‡AIç ”ç©¶ä»£ç†æ€§èƒ½",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åˆ›æ„å¤šæ ·æ€§å¯¹äººå·¥æ™ºèƒ½ç ”ç©¶ä»£ç†æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬åœ¨MLE-benchåŸºå‡†ä¸Šåˆ†æäº†ä¸åŒæ¨¡å‹å’Œä»£ç†æ¡†æ¶çš„ä»£ç†è½¨è¿¹ï¼Œå‘ç°åˆ›æ„å¤šæ ·æ€§ä¸ä»£ç†çš„è¡¨ç°æœ‰æ˜¾è‘—å…³è”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ›æ„å¤šæ ·æ€§è¶Šé«˜ï¼Œä»£ç†çš„æ€§èƒ½è¶Šå¼ºã€‚æˆ‘ä»¬è¿˜é€šè¿‡å…¶ä»–è¯„ä¼°æŒ‡æ ‡éªŒè¯äº†è¿™ä¸€å‘ç°ï¼Œè¿›ä¸€æ­¥æ”¯æŒäº†åˆ›æ„å¤šæ ·æ€§åœ¨AIç ”ç©¶ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.15661",
            "title": "VisPlay: Self-Evolving Vision-Language Models from Images",
            "url": "https://huggingface.co/papers/2511.15661",
            "abstract": "VisPlay, a self-evolving RL framework, uses unlabeled image data to enhance VLMs' reasoning, generalization, and response quality through two interacting roles and GRPO.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/",
            "score": 42,
            "issue_id": 1,
            "pub_date": "2025-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "a19c6b5e20cad57d",
            "authors": [
                "Yicheng He",
                "Chengsong Huang",
                "Zongxia Li",
                "Jiaxin Huang",
                "Yonghui Yang"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Illinois Urbana-Champaign",
                "University of Maryland",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.15661.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#rl",
                    "#hallucinations",
                    "#rlhf",
                    "#reasoning"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ”Ğ²Ğ¾Ğ¹Ğ½Ğ°Ñ Ğ¸Ğ³Ñ€Ğ°: ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸",
                    "desc": "VisPlay â€” ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ°ÑÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ³Ğ´Ğµ Ğ¾Ğ´Ğ½Ğ° Ñ€Ğ¾Ğ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼, Ğ° Ğ´Ñ€ÑƒĞ³Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ…Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Empowering VLMs with Self-Evolving Reinforcement Learning",
                    "desc": "VisPlay is a novel self-evolving reinforcement learning (RL) framework designed to enhance Vision-Language Models (VLMs) by utilizing large amounts of unlabeled image data. It introduces two key roles: an Image-Conditioned Questioner that creates challenging visual questions, and a Multimodal Reasoner that provides responses to these questions. The framework employs Group Relative Policy Optimization (GRPO) to optimize the training process, balancing the complexity of questions with the quality of answers. By applying VisPlay to different model families, it demonstrates significant improvements in visual reasoning and generalization across multiple benchmarks."
                },
                "zh": {
                    "title": "è‡ªæˆ‘è¿›åŒ–çš„è§†è§‰è¯­è¨€æ¨¡å‹æå‡æ¨ç†èƒ½åŠ›",
                    "desc": "VisPlayæ˜¯ä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¤§é‡æœªæ ‡è®°çš„å›¾åƒæ•°æ®æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å°†æ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªç›¸äº’ä½œç”¨çš„è§’è‰²ï¼šå›¾åƒæ¡ä»¶æé—®è€…å’Œå¤šæ¨¡æ€æ¨ç†è€…ï¼Œå‰è€…æå‡ºå…·æœ‰æŒ‘æˆ˜æ€§çš„è§†è§‰é—®é¢˜ï¼Œåè€…ç”Ÿæˆç›¸åº”çš„ç­”æ¡ˆã€‚é€šè¿‡ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼ŒVisPlayåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹³è¡¡äº†é—®é¢˜çš„å¤æ‚æ€§å’Œç­”æ¡ˆçš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisPlayåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è§†è§‰æ¨ç†å’Œç»„åˆæ³›åŒ–çš„ä¸€è‡´æ€§æå‡ï¼Œå±•ç¤ºäº†è‡ªæˆ‘è¿›åŒ–å¤šæ¨¡æ€æ™ºèƒ½çš„å¯æ‰©å±•è·¯å¾„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.15186",
            "title": "Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset",
            "url": "https://huggingface.co/papers/2511.15186",
            "abstract": "A new instruction-guided lesion segmentation paradigm using a large-scale dataset and a vision-language model enables diverse CXR lesion segmentation with simple instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding.",
            "score": 25,
            "issue_id": 1,
            "pub_date": "2025-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "3030d26b32d3ec42",
            "authors": [
                "Geon Choi",
                "Hangyul Yoon",
                "Hyunju Shin",
                "Hyunki Park",
                "Sang Hoon Seo",
                "Eunho Yang",
                "Edward Choi"
            ],
            "affiliations": [
                "AITRICS",
                "KAIST",
                "Samsung Medical Center"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.15186.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#multimodal",
                    "#open_source",
                    "#dataset",
                    "#cv",
                    "#synthetic",
                    "#healthcare"
                ],
                "emoji": "ğŸ«",
                "ru": {
                    "title": "Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ĞµĞ³ĞºĞ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ… Ğ³Ñ€ÑƒĞ´Ğ½Ğ¾Ğ¹ ĞºĞ»ĞµÑ‚ĞºĞ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ MIMIC-ILS â€” ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ 1,1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ğ°Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 192K Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ğ¾Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ»ĞµĞ³ĞºĞ¸Ñ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ROSALIA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ Ğ¸ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Simplifying Lesion Segmentation with User-Friendly Instructions",
                    "desc": "This paper presents a new method for segmenting lesions in chest X-rays (CXRs) using simple instructions, overcoming the limitations of previous models that required complex inputs. The authors introduce a large-scale dataset called MIMIC-ILS, which includes 1.1 million instruction-answer pairs and covers various lesion types. They also develop a vision-language model named ROSALIA, which is fine-tuned on this dataset to accurately segment lesions and provide explanations based on user instructions. The results demonstrate that this approach significantly improves the accessibility and effectiveness of lesion segmentation in medical imaging."
                },
                "zh": {
                    "title": "ç®€å•æŒ‡ä»¤å®ç°å¤šæ ·åŒ–ç—…ç¶åˆ†å‰²",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æŒ‡ä»¤å¼•å¯¼ç—…ç¶åˆ†å‰²èŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡ç®€å•çš„ç”¨æˆ·æŒ‡ä»¤å®ç°å¤šæ ·åŒ–çš„èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰ç—…ç¶åˆ†å‰²ã€‚æˆ‘ä»¬æ„å»ºäº†MIMIC-ILSï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„æŒ‡ä»¤-ç­”æ¡ˆæ•°æ®é›†ï¼ŒåŒ…å«110ä¸‡ä¸ªæŒ‡ä»¤-ç­”æ¡ˆå¯¹ï¼Œæ¶µç›–ä¸ƒç§ä¸»è¦ç—…ç¶ç±»å‹ã€‚é€šè¿‡ä½¿ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ROSALIAï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ¨¡å‹åœ¨ç—…ç¶åˆ†å‰²å’Œæ–‡æœ¬è§£é‡Šæ–¹é¢çš„é«˜å‡†ç¡®æ€§ã€‚è¯¥ç ”ç©¶ä¸ºåƒç´ çº§CXRç—…ç¶å®šä½æä¾›äº†æœ‰æ•ˆçš„åŸºç¡€èµ„æºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.14349",
            "title": "ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries",
            "url": "https://huggingface.co/papers/2511.14349",
            "abstract": "ARC-Chapter is a large-scale video chaptering model that improves performance through extensive training data and a new evaluation metric, demonstrating superior results and transferability.  \t\t\t\t\tAI-generated summary \t\t\t\t The proliferation of hour-long videos (e.g., lectures, podcasts, documentaries) has intensified demand for efficient content structuring. However, existing approaches are constrained by small-scale training with annotations that are typical short and coarse, restricting generalization to nuanced transitions in long videos. We introduce ARC-Chapter, the first large-scale video chaptering model trained on over million-level long video chapters, featuring bilingual, temporally grounded, and hierarchical chapter annotations. To achieve this goal, we curated a bilingual English-Chinese chapter dataset via a structured pipeline that unifies ASR transcripts, scene texts, visual captions into multi-level annotations, from short title to long summaries. We demonstrate clear performance improvements with data scaling, both in data volume and label intensity. Moreover, we design a new evaluation metric termed GRACE, which incorporates many-to-one segment overlaps and semantic similarity, better reflecting real-world chaptering flexibility. Extensive experiments demonstrate that ARC-Chapter establishes a new state-of-the-art by a significant margin, outperforming the previous best by 14.0% in F1 score and 11.3% in SODA score. Moreover, ARC-Chapter shows excellent transferability, improving the state-of-the-art on downstream tasks like dense video captioning on YouCook2.",
            "score": 16,
            "issue_id": 1,
            "pub_date": "2025-11-18",
            "pub_date_card": {
                "ru": "18 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 18",
                "zh": "11æœˆ18æ—¥"
            },
            "hash": "60b706cd356bf7d3",
            "authors": [
                "Junfu Pu",
                "Teng Wang",
                "Yixiao Ge",
                "Yuying Ge",
                "Chen Li",
                "Ying Shan"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.14349.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#open_source",
                    "#transfer_learning",
                    "#video",
                    "#dataset",
                    "#multilingual"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ³Ğ»Ğ°Ğ²Ñ‹ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸",
                    "desc": "ARC-Chapter â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ³Ğ»Ğ°Ğ²Ñ‹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‡Ğ°ÑĞ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸, Ñ‚ĞµĞºÑÑ‚ ÑĞ¾ ÑÑ†ĞµĞ½ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ² Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ GRACE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶ĞµÑÑ‚ÑŒ, Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 14% Ğ¿Ğ¾ F1-score Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Video Chaptering with ARC-Chapter!",
                    "desc": "ARC-Chapter is a novel video chaptering model that leverages a large-scale dataset to enhance the structuring of long videos. It addresses limitations of previous methods by utilizing over a million long video chapters with detailed bilingual annotations. The model introduces a new evaluation metric called GRACE, which better captures the nuances of chaptering by considering segment overlaps and semantic similarity. As a result, ARC-Chapter achieves significant performance improvements, setting new benchmarks in F1 and SODA scores, and demonstrates strong transferability to related tasks like dense video captioning."
                },
                "zh": {
                    "title": "ARC-Chapterï¼šè§†é¢‘ç« èŠ‚åˆ’åˆ†çš„æ–°æ ‡æ†",
                    "desc": "ARC-Chapteræ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„è§†é¢‘ç« èŠ‚æ¨¡å‹ï¼Œé€šè¿‡å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œæ–°çš„è¯„ä¼°æŒ‡æ ‡æ¥æé«˜æ€§èƒ½ã€‚è¯¥æ¨¡å‹ä½¿ç”¨äº†è¶…è¿‡ç™¾ä¸‡çº§çš„é•¿è§†é¢‘ç« èŠ‚è¿›è¡Œè®­ç»ƒï¼Œå…·æœ‰åŒè¯­ã€æ—¶é—´åŸºç¡€å’Œå±‚æ¬¡åŒ–çš„ç« èŠ‚æ³¨é‡Šã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡GRACEï¼Œæ›´å¥½åœ°åæ˜ äº†å®é™…ç« èŠ‚åˆ’åˆ†çš„çµæ´»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARC-Chapteråœ¨F1åˆ†æ•°å’ŒSODAåˆ†æ•°ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³æ¨¡å‹ï¼Œå±•ç¤ºäº†å‡ºè‰²çš„è¿ç§»èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.15586",
            "title": "MHR: Momentum Human Rig",
            "url": "https://huggingface.co/papers/2511.15586",
            "abstract": "MHR combines ATLAS's skeleton/shape paradigm with a modern rig to provide expressive, anatomically plausible human animation for AR/VR and graphics.  \t\t\t\t\tAI-generated summary \t\t\t\t We present MHR, a parametric human body model that combines the decoupled skeleton/shape paradigm of ATLAS with a flexible, modern rig and pose corrective system inspired by the Momentum library. Our model enables expressive, anatomically plausible human animation, supporting non-linear pose correctives, and is designed for robust integration in AR/VR and graphics pipelines.",
            "score": 13,
            "issue_id": 1,
            "pub_date": "2025-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "c5f90b0500ba4bb9",
            "authors": [
                "Aaron Ferguson",
                "Ahmed A. A. Osman",
                "Berta Bescos",
                "Carsten Stoll",
                "Chris Twigg",
                "Christoph Lassner",
                "David Otte",
                "Eric Vignola",
                "Fabian Prada",
                "Federica Bogo",
                "Igor Santesteban",
                "Javier Romero",
                "Jenna Zarate",
                "Jeongseok Lee",
                "Jinhyung Park",
                "Jinlong Yang",
                "John Doublestein",
                "Kishore Venkateshan",
                "Kris Kitani",
                "Ladislav Kavan",
                "Marco Dal Farra",
                "Matthew Hu",
                "Matthew Cioffi",
                "Michael Fabris",
                "Michael Ranieri",
                "Mohammad Modarres",
                "Petr Kadlecek",
                "Rawal Khirodkar",
                "Rinat Abdrashitov",
                "Romain PrÃ©vost",
                "Roman Rajbhandari",
                "Ronald Mallet",
                "Russell Pearsall",
                "Sandy Kao",
                "Sanjeev Kumar",
                "Scott Parrish",
                "Shoou-I Yu",
                "Shunsuke Saito",
                "Takaaki Shiratori",
                "Te-Li Wang",
                "Tony Tung",
                "Yichen Xu",
                "Yuan Dong",
                "Yuhua Chen",
                "Yuanlu Xu",
                "Yuting Ye",
                "Zhongshi Jiang"
            ],
            "affiliations": [
                "Meta"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.15586.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ§‘â€ğŸ¦¾",
                "ru": {
                    "title": "ĞĞ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²ĞµÑ€Ğ½Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² AR/VR",
                    "desc": "MHR â€” ÑÑ‚Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµĞ»Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ñ‘Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ÑĞºĞµĞ»ĞµÑ‚Ğ° Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ATLAS Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ Ñ€Ğ¸Ğ³-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ° Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¾Ğ¹ Momentum Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ·, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¸ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½ÑƒÑ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ñ AR/VR Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "Realistic Human Animation for AR/VR with MHR",
                    "desc": "MHR is a new model for creating realistic human animations in augmented and virtual reality. It merges the skeleton and shape concepts from the ATLAS model with a modern rig that allows for flexible movements. The model includes a pose corrective system that enhances the realism of animations by adjusting poses dynamically. This makes MHR suitable for use in various graphics applications, ensuring that animations look natural and anatomically correct."
                },
                "zh": {
                    "title": "MHRï¼šè§£å‰–å­¦ä¸è¡¨ç°åŠ›çš„å®Œç¾ç»“åˆ",
                    "desc": "MHRæ˜¯ä¸€ç§å‚æ•°åŒ–çš„äººä½“æ¨¡å‹ï¼Œå®ƒç»“åˆäº†ATLASçš„è§£è€¦éª¨æ¶/å½¢çŠ¶èŒƒå¼å’Œç°ä»£çš„çµæ´»è£…é…ç³»ç»Ÿã€‚è¯¥æ¨¡å‹æ”¯æŒéçº¿æ€§å§¿æ€ä¿®æ­£ï¼Œèƒ½å¤Ÿå®ç°å¯Œæœ‰è¡¨ç°åŠ›ä¸”ç¬¦åˆè§£å‰–å­¦çš„åŠ¨ç”»æ•ˆæœã€‚MHRç‰¹åˆ«é€‚ç”¨äºå¢å¼ºç°å®ï¼ˆARï¼‰ã€è™šæ‹Ÿç°å®ï¼ˆVRï¼‰å’Œå›¾å½¢å¤„ç†çš„é›†æˆã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒMHRä¸ºäººç±»åŠ¨ç”»æä¾›äº†æ›´é«˜çš„çœŸå®æ„Ÿå’Œè¡¨ç°åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.12207",
            "title": "Mixture of States: Routing Token-Level Dynamics for Multimodal Generation",
            "url": "https://huggingface.co/papers/2511.12207",
            "abstract": "MoS, a novel multimodal diffusion model fusion paradigm, achieves state-of-the-art results in text-to-image generation and editing with minimal parameters and computational overhead by using a learnable, token-wise router for modality interaction.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-k hidden states and is trained with an Îµ-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to 4times larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2025-11-15",
            "pub_date_card": {
                "ru": "15 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 15",
                "zh": "11æœˆ15æ—¥"
            },
            "hash": "732aff547d48321d",
            "authors": [
                "Haozhe Liu",
                "Ding Liu",
                "Mingchen Zhuge",
                "Zijian Zhou",
                "Tian Xie",
                "Sen He",
                "Yukang Yang",
                "Shuming Liu",
                "Yuren Cong",
                "Jiadong Guo",
                "Hongyu Xu",
                "Ke Xu",
                "Kam-Woh Ng",
                "Juan C. PÃ©rez",
                "Juan-Manuel~PÃ©rez-RÃºa",
                "Tao Xiang",
                "Wei Liu",
                "Shikun Liu",
                "JÃ¼rgen Schmidhuber"
            ],
            "affiliations": [
                "KAUST",
                "Meta AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.12207.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#small_models",
                    "#inference",
                    "#architecture"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹",
                    "desc": "MoS Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞœĞ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Îµ-Ğ¶Ğ°Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. MoS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑƒÑ€Ğ¾Ğ²Ğ½Ñ state-of-the-art Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² 4 Ñ€Ğ°Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 3-5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "MoS: Efficient Multimodal Fusion for Superior Image Generation",
                    "desc": "The paper presents MoS, a new approach for combining different types of data (multimodal) in diffusion models, which are used for generating and editing images from text. MoS uses a smart router that learns to connect different data types at a very detailed level, allowing it to work efficiently with fewer resources. This method focuses on selecting the most relevant features from the data, which helps in achieving high-quality results without needing a lot of computational power. The experiments show that MoS can produce better or similar results compared to larger models, making it a promising option for future applications in text-to-image tasks."
                },
                "zh": {
                    "title": "MoSï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹èåˆ",
                    "desc": "MoSæ˜¯ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹èåˆèŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­å®ç°æœ€å…ˆè¿›çš„ç»“æœã€‚å®ƒé€šè¿‡ä¸€ä¸ªå¯å­¦ä¹ çš„ã€é€ä¸ªæ ‡è®°çš„è·¯ç”±å™¨æ¥å®ç°æ¨¡æ€ä¹‹é—´çš„äº¤äº’ï¼Œä»è€Œå‡å°‘å‚æ•°å’Œè®¡ç®—å¼€é”€ã€‚è¯¥è·¯ç”±å™¨æ ¹æ®å»å™ªæ—¶é—´æ­¥å’Œè¾“å…¥åŠ¨æ€åœ°é€‰æ‹©æœ€ä¼˜çš„éšè—çŠ¶æ€ï¼Œç²¾ç¡®å¯¹é½æ ‡è®°çº§ç‰¹å¾ä¸æ‰©æ•£è½¨è¿¹ã€‚MoSæ¨¡å‹ä»…éœ€3Båˆ°5Bçš„å‚æ•°ï¼Œä¾¿èƒ½ä¸æœ€å¤§å››å€å‚æ•°çš„æ¨¡å‹ç›¸åª²ç¾æˆ–è¶…è¶Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.15706",
            "title": "RoMa v2: Harder Better Faster Denser Feature Matching",
            "url": "https://huggingface.co/papers/2511.15706",
            "abstract": "A novel dense feature matching model using a custom architecture and loss, combined with DINOv3, achieves state-of-the-art accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "3bb41368dddfca61",
            "authors": [
                "Johan Edstedt",
                "David NordstrÃ¶m",
                "Yushan Zhang",
                "Georg BÃ¶kman",
                "Jonathan Astermark",
                "Viktor Larsson",
                "Anders Heyden",
                "Fredrik Kahl",
                "MÃ¥rten WadenbÃ¤ck",
                "Michael Felsberg"
            ],
            "affiliations": [
                "Centre for Mathematical Sciences, Lund University",
                "Chalmers University of Technology",
                "Linkoping University",
                "University of Amsterdam"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.15706.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ DINOv3",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºÑƒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ 3D ÑÑ†ĞµĞ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğµ Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ DINOv3, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒÑÑ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€Ğ¸Ğ»Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑĞ´Ñ€Ğ¾Ğ¼ CUDA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ»Ğ¾ÑÑĞ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ° ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Dense Feature Matching with Custom Architecture and DINOv3",
                    "desc": "This paper presents a new dense feature matching model that improves the accuracy and efficiency of matching features between images of 3D scenes. The authors introduce a custom architecture and loss function, along with a diverse training dataset, to enhance the model's performance on challenging matching tasks. They also implement a two-stage pipeline that separates matching and refinement processes, which speeds up training and reduces memory usage. By integrating the DINOv3 foundation model, the proposed matcher achieves state-of-the-art results, outperforming previous models in accuracy and robustness."
                },
                "zh": {
                    "title": "æ–°å‹ç¨ å¯†ç‰¹å¾åŒ¹é…æ¨¡å‹ï¼Œç²¾å‡†é«˜æ•ˆï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç¨ å¯†ç‰¹å¾åŒ¹é…æ¨¡å‹ï¼Œé‡‡ç”¨è‡ªå®šä¹‰æ¶æ„å’ŒæŸå¤±å‡½æ•°ï¼Œå¹¶ç»“åˆDINOv3ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ç¨ å¯†ç‰¹å¾åŒ¹é…æ—¨åœ¨ä¼°è®¡ä¸‰ç»´åœºæ™¯ä¸­ä¸¤å¹…å›¾åƒä¹‹é—´çš„æ‰€æœ‰å¯¹åº”å…³ç³»ï¼Œå› å…¶é«˜å‡†ç¡®æ€§å’Œé²æ£’æ€§è€Œè¢«å¹¿æ³›è®¤å¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç¨ å¯†åŒ¹é…å™¨åœ¨è®¸å¤šå¤æ‚çš„ç°å®åœºæ™¯ä¸­ä»ç„¶è¡¨ç°ä¸ä½³ï¼Œä¸”é«˜ç²¾åº¦æ¨¡å‹é€šå¸¸é€Ÿåº¦è¾ƒæ…¢ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚é€šè¿‡ä¸€ç³»åˆ—ç³»ç»Ÿæ€§çš„æ”¹è¿›ï¼Œæœ¬æ–‡æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œæ„å»ºäº†æ–°çš„åŒ¹é…æ¶æ„å’ŒæŸå¤±å‡½æ•°ï¼Œå¹¶é€šè¿‡è§£è€¦çš„ä¸¤é˜¶æ®µåŒ¹é…ä¸ç²¾ç‚¼æµç¨‹åŠ å¿«äº†è®­ç»ƒé€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.13524",
            "title": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI",
            "url": "https://huggingface.co/papers/2511.13524",
            "abstract": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2025-11-17",
            "pub_date_card": {
                "ru": "17 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 17",
                "zh": "11æœˆ17æ—¥"
            },
            "hash": "7fb7fdf95694c72d",
            "authors": [
                "Yuhang Peng",
                "Yizhou Pan",
                "Xinning He",
                "Jihaoyu Yang",
                "Xinyu Yin",
                "Han Wang",
                "Xiaoji Zheng",
                "Chao Gao",
                "Jiangtao Gong"
            ],
            "affiliations": [
                "Institute for AI Industry Research, Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.13524.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#agents",
                    "#cv",
                    "#synthetic"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ’Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ embodied Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ FreeAskWorld â€” Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Vision-and-Language Navigation Ğ² Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ÑˆĞµÑÑ‚ÑŒÑ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 17 Ñ‡Ğ°ÑĞ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ embodied AI ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° FreeAskWorld, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼."
                },
                "en": {
                    "title": "Empowering AI with Socially Grounded Interaction in FreeAskWorld",
                    "desc": "This paper presents FreeAskWorld, a new simulation framework designed to enhance embodied intelligence in AI by integrating large language models (LLMs) for better behavior planning and social interaction. The framework allows for realistic human-agent simulations and includes a data generation pipeline for various tasks. It extends the Vision-and-Language Navigation (VLN) task to include interactive elements, enabling agents to seek and interpret navigational guidance. The results show that models trained on FreeAskWorld significantly improve their understanding and interaction skills, highlighting the importance of social context in AI development."
                },
                "zh": {
                    "title": "æ¨åŠ¨å…·èº«æ™ºèƒ½çš„ç¤¾ä¼šåŸºç¡€æ¨¡æ‹Ÿæ¡†æ¶",
                    "desc": "éšç€å…·èº«æ™ºèƒ½æˆä¸ºäººå·¥æ™ºèƒ½ç ”ç©¶çš„æ ¸å¿ƒå‰æ²¿ï¼Œæ¨¡æ‹Ÿå¹³å°å¿…é¡»è¶…è¶Šä½çº§ç‰©ç†äº¤äº’ï¼Œä»¥æ•æ‰å¤æ‚çš„äººæœ¬ç¤¾ä¼šè¡Œä¸ºã€‚æˆ‘ä»¬ä»‹ç»äº†FreeAskWorldï¼Œè¿™æ˜¯ä¸€ä¸ªäº’åŠ¨æ¨¡æ‹Ÿæ¡†æ¶ï¼Œæ•´åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”¨äºé«˜å±‚æ¬¡è¡Œä¸ºè§„åˆ’å’Œè¯­ä¹‰åŸºç¡€çš„äº¤äº’ï¼Œå—æ„å›¾å’Œç¤¾ä¼šè®¤çŸ¥ç†è®ºçš„å¯å‘ã€‚è¯¥æ¡†æ¶æ”¯æŒå¯æ‰©å±•ã€çœŸå®çš„äººæœºæ¨¡æ‹Ÿï¼Œå¹¶åŒ…æ‹¬ä¸€ä¸ªæ¨¡å—åŒ–çš„æ•°æ®ç”Ÿæˆç®¡é“ï¼Œé€‚ç”¨äºå¤šæ ·åŒ–çš„å…·èº«ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨FreeAskWorldä¸Šå¾®è°ƒçš„æ¨¡å‹åœ¨è¯­ä¹‰ç†è§£å’Œäº¤äº’èƒ½åŠ›ä¸Šä¼˜äºåŸå§‹æ¨¡å‹ï¼Œå¼ºè°ƒäº†ç¤¾ä¼šåŸºç¡€æ¨¡æ‹Ÿæ¡†æ¶åœ¨æ¨åŠ¨å…·èº«AIç³»ç»Ÿå‘å¤æ‚é«˜å±‚æ¬¡è§„åˆ’å’Œæ›´è‡ªç„¶çš„äººæœºäº¤äº’æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.15038",
            "title": "Aligning Generative Music AI with Human Preferences: Methods and Challenges",
            "url": "https://huggingface.co/papers/2511.15038",
            "abstract": "Preference alignment techniques, such as those in MusicRL and DiffRhythm+, are proposed to enhance music generation by addressing human preferences and unique challenges like temporal coherence and harmonic consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in generative AI for music have achieved remarkable fidelity and stylistic diversity, yet these systems often fail to align with nuanced human preferences due to the specific loss functions they use. This paper advocates for the systematic application of preference alignment techniques to music generation, addressing the fundamental gap between computational optimization and human musical appreciation. Drawing on recent breakthroughs including MusicRL's large-scale preference learning, multi-preference alignment frameworks like diffusion-based preference optimization in DiffRhythm+, and inference-time optimization techniques like Text2midi-InferAlign, we discuss how these techniques can address music's unique challenges: temporal coherence, harmonic consistency, and subjective quality assessment. We identify key research challenges including scalability to long-form compositions, reliability amongst others in preference modelling. Looking forward, we envision preference-aligned music generation enabling transformative applications in interactive composition tools and personalized music services. This work calls for sustained interdisciplinary research combining advances in machine learning, music-theory to create music AI systems that truly serve human creative and experiential needs.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-11-19",
            "pub_date_card": {
                "ru": "19 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 19",
                "zh": "11æœˆ19æ—¥"
            },
            "hash": "29b2b4d2bbcae1a3",
            "authors": [
                "Dorien Herremans",
                "Abhinaba Roy"
            ],
            "affiliations": [
                "AMAAI Lab, Singapore University of Technology and Design"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.15038.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#diffusion",
                    "#training",
                    "#audio",
                    "#rlhf"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº MusicRL Ğ¸ DiffRhythm+. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€ĞµÑˆĞ°ÑÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ - Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ³Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Aligning AI Music with Human Preferences",
                    "desc": "This paper discusses how preference alignment techniques can improve AI-generated music by better matching human tastes. It highlights the limitations of current generative models, which often do not consider the subtleties of human musical preferences due to their loss functions. The authors propose using methods like MusicRL and DiffRhythm+ to enhance aspects such as temporal coherence and harmonic consistency in music generation. They also identify challenges in scaling these techniques for longer compositions and emphasize the need for interdisciplinary research to create music AI that meets human creative needs."
                },
                "zh": {
                    "title": "éŸ³ä¹ç”Ÿæˆä¸­çš„åå¥½å¯¹é½ï¼šæå‡åˆ›ä½œä½“éªŒ",
                    "desc": "æœ¬æ–‡æå‡ºäº†åå¥½å¯¹é½æŠ€æœ¯ï¼Œä»¥æé«˜éŸ³ä¹ç”Ÿæˆçš„è´¨é‡ï¼Œç‰¹åˆ«æ˜¯è§£å†³äººç±»åå¥½å’ŒéŸ³ä¹ç”Ÿæˆä¸­çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¦‚æ—¶é—´ä¸€è‡´æ€§å’Œå’Œå£°ä¸€è‡´æ€§ã€‚å°½ç®¡ç”Ÿæˆå¼AIåœ¨éŸ³ä¹åˆ›ä½œä¸­å–å¾—äº†æ˜¾è‘—çš„é£æ ¼å¤šæ ·æ€§å’Œä¿çœŸåº¦ï¼Œä½†ç”±äºç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œè¿™äº›ç³»ç»Ÿå¾€å¾€æ— æ³•æ»¡è¶³ç»†è‡´çš„äººç±»åå¥½ã€‚æˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•é€šè¿‡å¤§è§„æ¨¡åå¥½å­¦ä¹ å’Œå¤šåå¥½å¯¹é½æ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå¹¶æå‡ºäº†åœ¨æ¨ç†æ—¶ä¼˜åŒ–çš„æŠ€æœ¯ã€‚æœªæ¥ï¼Œæˆ‘ä»¬å¸Œæœ›åå¥½å¯¹é½çš„éŸ³ä¹ç”Ÿæˆèƒ½å¤Ÿåœ¨äº’åŠ¨åˆ›ä½œå·¥å…·å’Œä¸ªæ€§åŒ–éŸ³ä¹æœåŠ¡ä¸­å®ç°å˜é©æ€§åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.13001",
            "title": "Medal S: Spatio-Textual Prompt Model for Medical Segmentation",
            "url": "https://huggingface.co/papers/2511.13001",
            "abstract": "Medal S is a medical segmentation foundation model that integrates spatial and textual prompts for efficient, high-accuracy multi-class segmentation across various imaging modalities.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-17",
            "pub_date_card": {
                "ru": "17 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 17",
                "zh": "11æœˆ17æ—¥"
            },
            "hash": "2d0a2011c58fda12",
            "authors": [
                "Pengcheng Shi",
                "Jiawei Chen",
                "Jiaqi Liu",
                "Xinglin Zhang",
                "Tao Chen",
                "Lei Li"
            ],
            "affiliations": [
                "Medical Image Insights, Shanghai, China",
                "University of Washington, Seattle, WA, USA",
                "University of Waterloo, Waterloo, ON, Canada",
                "Xian Jiaotong University, Xian, China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.13001.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#multimodal",
                    "#inference",
                    "#open_source",
                    "#dataset",
                    "#cv",
                    "#healthcare"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚ Ğ²Ğ¼ĞµÑÑ‚Ğµ",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Medal S â€” Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğ¼ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ›Ñ‘Ğ³ĞºĞ¸Ğ¹ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ ÑĞ²Ñ‘Ñ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ²Ğ¾ĞºÑĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ´Ğ¾ 243 ĞºĞ»Ğ°ÑÑĞ¾Ğ² Ğ½Ğ° ĞšĞ¢, ĞœĞ Ğ¢, ĞŸĞ­Ğ¢, ÑƒĞ»ÑŒÑ‚Ñ€Ğ°Ğ·Ğ²ÑƒĞºĞµ Ğ¸ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ğ¸. ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 90% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ»Ğ°ÑÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Medal S: Revolutionizing Medical Segmentation with Spatial and Textual Prompts",
                    "desc": "Medal S is a cutting-edge medical segmentation model that combines spatial and textual prompts to enhance multi-class segmentation accuracy across various imaging types. It uniquely aligns volumetric prompts with text embeddings, addressing resolution mismatches that can lead to errors in segmentation. The model processes multiple segmentation masks simultaneously while maintaining full 3D context, significantly improving performance and reducing inference time. With its innovative prompting modes and advanced data augmentation techniques, Medal S sets a new standard in medical image analysis, outperforming existing models in both efficiency and accuracy."
                },
                "zh": {
                    "title": "Medal Sï¼šé«˜æ•ˆç²¾å‡†çš„åŒ»ç–—åˆ†å‰²æ–°æ¨¡å‹",
                    "desc": "Medal S æ˜¯ä¸€ç§åŒ»ç–—åˆ†å‰²åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆä¸”é«˜ç²¾åº¦åœ°è¿›è¡Œå¤šç±»åˆ†å‰²ï¼Œæ”¯æŒç©ºé—´å’Œæ–‡æœ¬æç¤ºçš„ç»“åˆã€‚ä¸ä»…ä½¿ç”¨æ–‡æœ¬çš„æ–¹æ³•ä¸åŒï¼ŒMedal S å®ç°äº†ä½“ç§¯æç¤ºä¸æ–‡æœ¬åµŒå…¥ä¹‹é—´çš„é€šé“å¯¹é½ï¼Œå‡å°‘äº†åˆ†è¾¨ç‡ä¸åŒ¹é…å¸¦æ¥çš„ä¸å‡†ç¡®æ€§ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå¹¶è¡Œå¤„ç†å¤šä¸ªåŸç”Ÿåˆ†è¾¨ç‡çš„æ©è†œï¼Œæå‡äº†å¤šç±»åˆ†å‰²çš„æ€§èƒ½ã€‚é€šè¿‡åŠ¨æ€é‡é‡‡æ ·å’Œä¼˜åŒ–çš„æ–‡æœ¬é¢„å¤„ç†ï¼ŒMedal S åœ¨å¤šä¸ªæˆåƒæ¨¡æ€ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨åŒ»ç–—åˆ†å‰²ä»»åŠ¡ä¸­çš„é«˜æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-11-19.html",
    "link_next": "2025-11-21.html",
    "link_month": "2025-11.html",
    "short_date_prev": {
        "ru": "19.11",
        "en": "11/19",
        "zh": "11æœˆ19æ—¥"
    },
    "short_date_next": {
        "ru": "21.11",
        "en": "11/21",
        "zh": "11æœˆ21æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 1,
        "#benchmark": 5,
        "#agents": 2,
        "#cv": 4,
        "#rl": 1,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 0,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 7,
        "#math": 0,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 2,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 6,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    }
}