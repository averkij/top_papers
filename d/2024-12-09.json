{
    "date": {
        "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 9",
        "zh": "12æœˆ9æ—¥"
    },
    "time_utc": "2024-12-09 03:32",
    "weekday": 0,
    "issue_id": 1011,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.04445",
            "title": "Moto: Latent Motion Token as the Bridging Language for Robot Manipulation",
            "url": "https://huggingface.co/papers/2412.04445",
            "abstract": "Recent developments in Large Language Models pre-trained on extensive corpora have shown significant success in various natural language processing tasks with minimal fine-tuning. This success offers new promise for robotics, which has long been constrained by the high cost of action-labeled data. We ask: given the abundant video data containing interaction-related knowledge available as a rich \"corpus\", can a similar generative pre-training approach be effectively applied to enhance robot learning? The key challenge is to identify an effective representation for autoregressive pre-training that benefits robot manipulation tasks. Inspired by the way humans learn new skills through observing dynamic environments, we propose that effective robotic learning should emphasize motion-related knowledge, which is closely tied to low-level actions and is hardware-agnostic, facilitating the transfer of learned motions to actual robot actions. To this end, we introduce Moto, which converts video content into latent Motion Token sequences by a Latent Motion Tokenizer, learning a bridging \"language\" of motion from videos in an unsupervised manner. We pre-train Moto-GPT through motion token autoregression, enabling it to capture diverse visual motion knowledge. After pre-training, Moto-GPT demonstrates the promising ability to produce semantically interpretable motion tokens, predict plausible motion trajectories, and assess trajectory rationality through output likelihood. To transfer learned motion priors to real robot actions, we implement a co-fine-tuning strategy that seamlessly bridges latent motion token prediction and real robot control. Extensive experiments show that the fine-tuned Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks, underscoring its effectiveness in transferring knowledge from video data to downstream visual manipulation tasks.",
            "score": 11,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 5",
                "zh": "12æœˆ5æ—¥"
            },
            "hash": "a5ac6d786500ef9f",
            "authors": [
                "Yi Chen",
                "Yuying Ge",
                "Yizhuo Li",
                "Yixiao Ge",
                "Mingyu Ding",
                "Ying Shan",
                "Xihui Liu"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "The University of Hong Kong",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04445.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#transfer_learning",
                    "#games",
                    "#robotics",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Moto, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Latent Motion Tokenizer. Moto-GPT Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Moto-GPT Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Bridging Video Knowledge to Robot Actions with Moto-GPT",
                    "desc": "This paper explores the application of Large Language Models to enhance robot learning by leveraging abundant video data. It introduces Moto, a system that converts video content into Motion Token sequences, allowing robots to learn motion-related knowledge in an unsupervised manner. The approach emphasizes the importance of motion understanding for effective robotic manipulation, enabling the transfer of learned skills to real-world actions. Through extensive experiments, Moto-GPT demonstrates improved performance in robot manipulation tasks, showcasing its ability to bridge the gap between video knowledge and practical robot control."
                },
                "zh": {
                    "title": "åˆ©ç”¨è§†é¢‘æ•°æ®æå‡æœºå™¨äººå­¦ä¹ èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§è§„æ¨¡è§†é¢‘æ•°æ®æ¥æå‡æœºå™¨äººå­¦ä¹ èƒ½åŠ›ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºMotoçš„æ¨¡å‹ï¼Œé€šè¿‡å°†è§†é¢‘å†…å®¹è½¬æ¢ä¸ºæ½œåœ¨çš„è¿åŠ¨æ ‡è®°åºåˆ—ï¼Œæ¥å­¦ä¹ è¿åŠ¨çŸ¥è¯†ã€‚Moto-GPTç»è¿‡é¢„è®­ç»ƒåï¼Œèƒ½å¤Ÿç”Ÿæˆå¯è§£é‡Šçš„è¿åŠ¨æ ‡è®°ï¼Œå¹¶é¢„æµ‹åˆç†çš„è¿åŠ¨è½¨è¿¹ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„Moto-GPTåœ¨æœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†ä»è§†é¢‘æ•°æ®è½¬ç§»çŸ¥è¯†çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04440",
            "title": "GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration",
            "url": "https://huggingface.co/papers/2412.04440",
            "abstract": "Text-to-video generation models have shown significant progress in the recent years. However, they still struggle with generating complex dynamic scenes based on compositional text prompts, such as attribute binding for multiple objects, temporal dynamics associated with different objects, and interactions between objects. Our key motivation is that complex tasks can be decomposed into simpler ones, each handled by a role-specialized MLLM agent. Multiple agents can collaborate together to achieve collective intelligence for complex goals. We propose GenMAC, an iterative, multi-agent framework that enables compositional text-to-video generation. The collaborative workflow includes three stages: Design, Generation, and Redesign, with an iterative loop between the Generation and Redesign stages to progressively verify and refine the generated videos. The Redesign stage is the most challenging stage that aims to verify the generated videos, suggest corrections, and redesign the text prompts, frame-wise layouts, and guidance scales for the next iteration of generation. To avoid hallucination of a single MLLM agent, we decompose this stage to four sequentially-executed MLLM-based agents: verification agent, suggestion agent, correction agent, and output structuring agent. Furthermore, to tackle diverse scenarios of compositional text-to-video generation, we design a self-routing mechanism to adaptively select the proper correction agent from a collection of correction agents each specialized for one scenario. Extensive experiments demonstrate the effectiveness of GenMAC, achieving state-of-the art performance in compositional text-to-video generation.",
            "score": 8,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 5",
                "zh": "12æœˆ5æ—¥"
            },
            "hash": "28dc2191ba71c4ea",
            "authors": [
                "Kaiyi Huang",
                "Yukun Huang",
                "Xuefei Ning",
                "Zinan Lin",
                "Yu Wang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Microsoft Research",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04440.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#agents",
                    "#games",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GenMAC - Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ†Ğ¸ĞºĞ»Ğ¾Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°. GenMAC Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Collaborative Intelligence for Text-to-Video Mastery",
                    "desc": "This paper introduces GenMAC, a multi-agent framework designed to improve text-to-video generation by breaking down complex tasks into simpler components. It utilizes specialized machine learning agents that collaborate through three main stages: Design, Generation, and Redesign, with an iterative process to refine video outputs. The Redesign stage is particularly crucial as it involves multiple agents that verify, suggest corrections, and restructure prompts to enhance the generated videos. The framework also includes a self-routing mechanism to select the most suitable correction agent for various scenarios, leading to significant advancements in generating dynamic scenes from text prompts."
                },
                "zh": {
                    "title": "GenMACï¼šåä½œç”Ÿæˆå¤æ‚è§†é¢‘çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç”Ÿæˆå¤æ‚åŠ¨æ€åœºæ™¯æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºGenMACçš„å¤šä»£ç†æ¡†æ¶ï¼Œé€šè¿‡å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºç®€å•ä»»åŠ¡æ¥å®ç°åä½œç”Ÿæˆã€‚è¯¥æ¡†æ¶åŒ…æ‹¬è®¾è®¡ã€ç”Ÿæˆå’Œé‡æ–°è®¾è®¡ä¸‰ä¸ªé˜¶æ®µï¼Œç”Ÿæˆå’Œé‡æ–°è®¾è®¡ä¹‹é—´å­˜åœ¨è¿­ä»£å¾ªç¯ï¼Œä»¥é€æ­¥éªŒè¯å’Œä¼˜åŒ–ç”Ÿæˆçš„è§†é¢‘ã€‚é€šè¿‡è‡ªé€‚åº”é€‰æ‹©ä¸“é—¨åŒ–çš„ä¿®æ­£ä»£ç†ï¼ŒGenMACèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å¤šæ ·åŒ–çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆåœºæ™¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04862",
            "title": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases",
            "url": "https://huggingface.co/papers/2412.04862",
            "abstract": "This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from https://huggingface.co/LGAI-EXAONE. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.",
            "score": 1,
            "issue_id": 1011,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 6",
                "zh": "12æœˆ6æ—¥"
            },
            "hash": "83e6f957e42ebb5e",
            "authors": [
                "LG AI Research",
                "Soyoung An",
                "Kyunghoon Bae",
                "Eunbi Choi",
                "Kibong Choi",
                "Stanley Jungkyu Choi",
                "Seokhee Hong",
                "Junwon Hwang",
                "Hyojin Jeon",
                "Gerrard Jeongwon Jo",
                "Hyunjik Jo",
                "Jiyeon Jung",
                "Yountae Jung",
                "Hyosang Kim",
                "Joonkee Kim",
                "Seonghwan Kim",
                "Soyeon Kim",
                "Sunkyoung Kim",
                "Yireun Kim",
                "Yongil Kim",
                "Youchul Kim",
                "Edward Hwayoung Lee",
                "Haeju Lee",
                "Honglak Lee",
                "Jinsik Lee",
                "Kyungmin Lee",
                "Woohyung Lim",
                "Sangha Park",
                "Sooyoun Park",
                "Yongmin Park",
                "Sihoon Yang",
                "Heuiyeen Yeen",
                "Hyeongu Yun"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.04862.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#open_source",
                    "#small_models",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "EXAONE 3.5: ĞĞ¾Ğ²Ğ¾Ğµ ÑĞ»Ğ¾Ğ²Ğ¾ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¾Ñ‚ LG AI Research",
                    "desc": "ĞšĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ LG AI Research Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ EXAONE 3.5, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ² Ñ‚Ñ€ĞµÑ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ…: 32B, 7.8B Ğ¸ 2.4B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ EXAONE 3.5 Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Hugging Face."
                },
                "en": {
                    "title": "EXAONE 3.5: Leading the Way in Instruction-Tuned Language Models",
                    "desc": "The EXAONE 3.5 language models, created by LG AI Research, are advanced instruction-tuned models available in three sizes: 32B, 7.8B, and 2.4B. They excel in following instructions in real-world applications, achieving top scores on seven different benchmarks. Additionally, these models demonstrate superior long-context understanding, ranking first in four benchmarks. They are accessible for research purposes and show competitive performance against leading open models of similar sizes across nine general benchmarks."
                },
                "zh": {
                    "title": "EXAONE 3.5ï¼šæŒ‡ä»¤è·Ÿéšä¸é•¿ä¸Šä¸‹æ–‡ç†è§£çš„å…ˆé”‹",
                    "desc": "EXAONE 3.5 æ˜¯ç”± LG AI ç ”ç©¶æ‰€å¼€å‘çš„æŒ‡ä»¤è°ƒä¼˜è¯­è¨€æ¨¡å‹ï¼Œæä¾›ä¸‰ç§é…ç½®ï¼š32Bã€7.8B å’Œ 2.4Bã€‚è¿™äº›æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­å…·æœ‰å“è¶Šçš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€é«˜åˆ†ã€‚å®ƒä»¬åœ¨é•¿ä¸Šä¸‹æ–‡ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸åŒç±»å¼€æºæ¨¡å‹ç›¸æ¯”ï¼ŒEXAONE 3.5 åœ¨ä¹ä¸ªé€šç”¨åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿå±•ç°äº†ç«äº‰åŠ›çš„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04301",
            "title": "SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion",
            "url": "https://huggingface.co/papers/2412.04301",
            "abstract": "Recent advances in text-guided image editing enable users to perform image edits through simple text inputs, leveraging the extensive priors of multi-step diffusion-based text-to-image models. However, these methods often fall short of the speed demands required for real-world and on-device applications due to the costly multi-step inversion and sampling process involved. In response to this, we introduce SwiftEdit, a simple yet highly efficient editing tool that achieve instant text-guided image editing (in 0.23s). The advancement of SwiftEdit lies in its two novel contributions: a one-step inversion framework that enables one-step image reconstruction via inversion and a mask-guided editing technique with our proposed attention rescaling mechanism to perform localized image editing. Extensive experiments are provided to demonstrate the effectiveness and efficiency of SwiftEdit. In particular, SwiftEdit enables instant text-guided image editing, which is extremely faster than previous multi-step methods (at least 50 times faster) while maintain a competitive performance in editing results. Our project page is at: https://swift-edit.github.io/",
            "score": 1,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 5",
                "zh": "12æœˆ5æ—¥"
            },
            "hash": "a4cea89a59a9a3c0",
            "authors": [
                "Trong-Tung Nguyen",
                "Quang Nguyen",
                "Khoi Nguyen",
                "Anh Tran",
                "Cuong Pham"
            ],
            "affiliations": [
                "Posts & Telecom. Inst. of Tech., Vietnam",
                "VinAI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04301.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "SwiftEdit - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², SwiftEdit Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾ - Ğ·Ğ° 0.23 ÑĞµĞºÑƒĞ½Ğ´Ñ‹. SwiftEdit Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ Ğ² 50 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "SwiftEdit: Instant Text-Guided Image Editing at Lightning Speed!",
                    "desc": "This paper presents SwiftEdit, a new tool for fast text-guided image editing that significantly improves the speed of image modifications. Traditional methods rely on multi-step diffusion processes, which can be slow and inefficient for real-time applications. SwiftEdit introduces a one-step inversion framework that allows for quick image reconstruction and a mask-guided editing technique that uses attention rescaling for precise edits. The results show that SwiftEdit is at least 50 times faster than previous methods while still delivering high-quality editing outcomes."
                },
                "zh": {
                    "title": "SwiftEditï¼šç¬æ—¶æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘çš„é©å‘½",
                    "desc": "æœ€è¿‘çš„æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘æŠ€æœ¯ä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡ç®€å•çš„æ–‡æœ¬è¾“å…¥è¿›è¡Œå›¾åƒç¼–è¾‘ï¼Œåˆ©ç”¨äº†å¤šæ­¥æ‰©æ•£æ¨¡å‹çš„ä¸°å¯Œå…ˆéªŒçŸ¥è¯†ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­å¾€å¾€é€Ÿåº¦è¾ƒæ…¢ï¼Œæ— æ³•æ»¡è¶³å®æ—¶å’Œè®¾å¤‡ç«¯çš„éœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SwiftEditï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„ç¼–è¾‘å·¥å…·ï¼Œå¯ä»¥å®ç°ç¬æ—¶çš„æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘ï¼Œé€Ÿåº¦è¾¾åˆ°0.23ç§’ã€‚SwiftEditçš„åˆ›æ–°åœ¨äºå…¶ä¸€ä½“åŒ–çš„åæ¼”æ¡†æ¶å’ŒåŸºäºæ©ç çš„ç¼–è¾‘æŠ€æœ¯ï¼Œèƒ½å¤Ÿå¿«é€Ÿè¿›è¡Œå±€éƒ¨å›¾åƒç¼–è¾‘ï¼ŒåŒæ—¶ä¿æŒä¸ä¼ ç»Ÿå¤šæ­¥æ–¹æ³•ç›¸å½“çš„ç¼–è¾‘æ•ˆæœã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-06.html",
    "link_next": "2024-12-10.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "06.12",
        "en": "12/06",
        "zh": "12æœˆ6æ—¥"
    },
    "short_date_next": {
        "ru": "10.12",
        "en": "12/10",
        "zh": "12æœˆ10æ—¥"
    },
    "categories": {
        "#dataset": 0,
        "#data": 0,
        "#benchmark": 1,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 3,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 0,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "æœ€è¿‘çš„è§†è§‰-è¯­è¨€æ¨¡å‹é€šè¿‡å¢åŠ è§†è§‰æ ‡è®°çš„é•¿åº¦æé«˜äº†æ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å‘ç°ï¼Œæµè¡Œçš„è§†è§‰ç¼–ç å™¨ç”Ÿæˆçš„è§†è§‰æ ‡è®°å­˜åœ¨å¤§é‡å†—ä½™ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VisionZipï¼Œä¸€ç§é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„è§†è§‰æ ‡è®°çš„æ–¹æ³•ï¼Œå‡å°‘å†—ä½™å¹¶æé«˜æ•ˆç‡ã€‚VisionZipå¯åº”ç”¨äºå›¾åƒå’Œè§†é¢‘ç†è§£ä»»åŠ¡ï¼Œé€‚ç”¨äºå¤šè½®å¯¹è¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVisionZipåœ¨å‡ ä¹æ‰€æœ‰è®¾ç½®ä¸­éƒ½ä¼˜äºä¹‹å‰çš„æœ€ä½³æ–¹æ³•ï¼Œå¹¶æ˜¾è‘—æé«˜äº†æ¨¡å‹æ¨ç†é€Ÿåº¦ã€‚",
        "title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models",
        "pinyin": "æœ€è¿‘çš„è§†è§‰-è¯­è¨€æ¨¡å‹é€šè¿‡å¢åŠ è§†è§‰æ ‡è®°çš„é•¿åº¦æé«˜äº†æ€§èƒ½ï¼Œä½†ä¹Ÿå¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å‘ç°ï¼Œæµè¡Œçš„è§†è§‰ç¼–ç å™¨ç”Ÿæˆçš„è§†è§‰æ ‡è®°å­˜åœ¨å¤§é‡å†—ä½™ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VisionZipï¼Œä¸€ç§é€‰æ‹©ä¿¡æ¯ä¸°å¯Œçš„è§†è§‰æ ‡è®°çš„æ–¹æ³•ï¼Œå‡å°‘å†—ä½™å¹¶æé«˜æ•ˆç‡ã€‚VisionZipå¯åº”ç”¨äºå›¾åƒå’Œè§†é¢‘ç†è§£ä»»åŠ¡ï¼Œé€‚ç”¨äºå¤šè½®å¯¹è¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVisionZipåœ¨å‡ ä¹æ‰€æœ‰è®¾ç½®ä¸­éƒ½ä¼˜äºä¹‹å‰çš„æœ€ä½³æ–¹æ³•ï¼Œå¹¶æ˜¾è‘—æé«˜äº†æ¨¡å‹æ¨ç†é€Ÿåº¦ã€‚\n\nZuÃ¬jÃ¬n de shÃ¬juÃ©-yÇ”yÃ¡n mÃ³xÃ­ng tÅngguÃ² zÄ“ngjiÄ shÃ¬juÃ© biÄojÃ¬ de chÃ¡ngdÃ¹ tÃ­gÄo le xÃ¬ngnÃ©ng, dÃ n yÄ› zÄ“ngjiÄ le jÃ¬suÃ n chÃ©ngbÄ›n. WÇ’men fÄxiÃ n, liÃºxÃ­ng de shÃ¬juÃ© biÄnmÇqÃ¬ shÄ“ngchÃ©ng de shÃ¬juÃ© biÄojÃ¬ cÃºnzÃ i dÃ liÃ ng rÇ’ngyÃ¹. WÃ¨i jiÄ›juÃ© zhÃ¨gÃ¨ wÃ¨ntÃ­, wÇ’men yÇnrÃ¹ le VisionZip, yÄ«zhÇ’ng xuÇnzÃ© xÃ¬nxÄ« fÄ“ngfÃ¹ de shÃ¬juÃ© biÄojÃ¬ de fÄngfÇ, jiÇnshÇo rÇ’ngyÃ¹ bÃ¬ng tÃ­gÄo xiÃ olÇœ. VisionZip kÄ› yÃ¬ngyÃ²ng yÃº tÃºxiÃ ng hÃ© shÃ¬pÇn lÇjiÄ› rÃ¨nwÃ¹, shÃ¬yÃ²ng yÃº duÅlÃºn duÃ¬huÃ . ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, VisionZip zÃ i jÄ«huÄ suÇ’yÇ’u shÃ¨zhÃ¬ zhÅng dÅu yÅu zhÄ«qiÃ¡n de zuÃ¬jiÄ fÄngfÇ, bÃ¬ng xiÇnzhÃ¹ tÃ­gÄo le mÃ³xÃ­ng tuÄ«lÇ sÃ¹dÃ¹.",
        "vocab": "[{'word': 'è§†è§‰', 'pinyin': 'shÃ¬juÃ©', 'trans': 'vision'}, {'word': 'è¯­è¨€', 'pinyin': 'yÇ”yÃ¡n', 'trans': 'language'}, {'word': 'æ¨¡å‹', 'pinyin': 'mÃ³xÃ­ng', 'trans': 'model'}, {'word': 'å¢åŠ ', 'pinyin': 'zÄ“ngjiÄ', 'trans': 'increase'}, {'word': 'é•¿åº¦', 'pinyin': 'chÃ¡ngdÃ¹', 'trans': 'length'}, {'word': 'æ€§èƒ½', 'pinyin': 'xÃ¬ngnÃ©ng', 'trans': 'performance'}, {'word': 'è®¡ç®—', 'pinyin': 'jÃ¬suÃ n', 'trans': 'calculation'}, {'word': 'æˆæœ¬', 'pinyin': 'chÃ©ngbÄ›n', 'trans': 'cost'}, {'word': 'å‘ç°', 'pinyin': 'fÄxiÃ n', 'trans': 'discover'}, {'word': 'æµè¡Œ', 'pinyin': 'liÃºxÃ­ng', 'trans': 'popular'}, {'word': 'ç¼–ç å™¨', 'pinyin': 'biÄnmÇqÃ¬', 'trans': 'encoder'}, {'word': 'ç”Ÿæˆ', 'pinyin': 'shÄ“ngchÃ©ng', 'trans': 'generate'}, {'word': 'æ ‡è®°', 'pinyin': 'biÄojÃ¬', 'trans': 'token'}, {'word': 'å­˜åœ¨', 'pinyin': 'cÃºnzÃ i', 'trans': 'exist'}, {'word': 'å†—ä½™', 'pinyin': 'rÃ³ngyÃº', 'trans': 'redundancy'}, {'word': 'è§£å†³', 'pinyin': 'jiÄ›juÃ©', 'trans': 'solve'}, {'word': 'å¼•å…¥', 'pinyin': 'yÇnrÃ¹', 'trans': 'introduce'}, {'word': 'é€‰æ‹©', 'pinyin': 'xuÇnzÃ©', 'trans': 'select'}, {'word': 'ä¸°å¯Œ', 'pinyin': 'fÄ“ngfÃ¹', 'trans': 'rich'}, {'word': 'æ–¹æ³•', 'pinyin': 'fÄngfÇ', 'trans': 'method'}, {'word': 'å‡å°‘', 'pinyin': 'jiÇnshÇo', 'trans': 'reduce'}, {'word': 'æ•ˆç‡', 'pinyin': 'xiÃ olÇœ', 'trans': 'efficiency'}, {'word': 'åº”ç”¨', 'pinyin': 'yÃ¬ngyÃ²ng', 'trans': 'apply'}, {'word': 'å›¾åƒ', 'pinyin': 'tÃºxiÃ ng', 'trans': 'image'}, {'word': 'è§†é¢‘', 'pinyin': 'shÃ¬pÃ­n', 'trans': 'video'}, {'word': 'ç†è§£', 'pinyin': 'lÇjiÄ›', 'trans': 'understanding'}, {'word': 'ä»»åŠ¡', 'pinyin': 'rÃ¨nwÃ¹', 'trans': 'task'}, {'word': 'é€‚ç”¨', 'pinyin': 'shÃ¬yÃ²ng', 'trans': 'applicable'}, {'word': 'å¤šè½®', 'pinyin': 'duÅlÃºn', 'trans': 'multi-turn'}, {'word': 'å¯¹è¯', 'pinyin': 'duÃ¬huÃ ', 'trans': 'dialogue'}, {'word': 'å®éªŒ', 'pinyin': 'shÃ­yÃ n', 'trans': 'experiment'}, {'word': 'ç»“æœ', 'pinyin': 'jiÃ©guÇ’', 'trans': 'result'}, {'word': 'æ˜¾ç¤º', 'pinyin': 'xiÇnshÃ¬', 'trans': 'show'}, {'word': 'ä¼˜äº', 'pinyin': 'yÅuyÃº', 'trans': 'superior to'}, {'word': 'ä¹‹å‰', 'pinyin': 'zhÄ«qiÃ¡n', 'trans': 'previous'}, {'word': 'æœ€ä½³', 'pinyin': 'zuÃ¬jiÄ', 'trans': 'best'}, {'word': 'è®¾ç½®', 'pinyin': 'shÃ¨zhÃ¬', 'trans': 'setting'}, {'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnzhÃ¹', 'trans': 'significant'}, {'word': 'æé«˜', 'pinyin': 'tÃ­gÄo', 'trans': 'improve'}, {'word': 'æ¨ç†', 'pinyin': 'tuÄ«lÇ', 'trans': 'inference'}, {'word': 'é€Ÿåº¦', 'pinyin': 'sÃ¹dÃ¹', 'trans': 'speed'}]",
        "trans": "Recent vision-language models have improved performance by increasing the length of visual tokens, but this has also increased computational costs. We have discovered that popular visual encoders generate visual tokens with a significant amount of redundancy. To address this issue, we introduce VisionZip, a method for selecting information-rich visual tokens to reduce redundancy and enhance efficiency. VisionZip can be applied to both image and video understanding tasks and is suitable for multi-turn conversations. Experimental results show that VisionZip outperforms previous best methods in almost all settings and significantly speeds up model inference.",
        "update_ts": "2024-12-08 12:42"
    }
}