{
    "date": {
        "ru": "9 декабря",
        "en": "December 9",
        "zh": "12月9日"
    },
    "time_utc": "2024-12-09 06:16",
    "weekday": 0,
    "issue_id": 1014,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.05271",
            "title": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
            "url": "https://huggingface.co/papers/2412.05271",
            "abstract": "We introduce InternVL 2.5, an advanced multimodal large language model (MLLM) series that builds upon InternVL 2.0, maintaining its core model architecture while introducing significant enhancements in training and testing strategies as well as data quality. In this work, we delve into the relationship between model scaling and performance, systematically exploring the performance trends in vision encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations on a wide range of benchmarks, including multi-discipline reasoning, document understanding, multi-image / video understanding, real-world comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processing, InternVL 2.5 exhibits competitive performance, rivaling leading commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a 3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing strong potential for test-time scaling. We hope this model contributes to the open-source community by setting new standards for developing and applying multimodal AI systems. HuggingFace demo see https://huggingface.co/spaces/OpenGVLab/InternVL",
            "score": 24,
            "issue_id": 1013,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "81590cc90bda9173",
            "authors": [
                "Zhe Chen",
                "Weiyun Wang",
                "Yue Cao",
                "Yangzhou Liu",
                "Zhangwei Gao",
                "Erfei Cui",
                "Jinguo Zhu",
                "Shenglong Ye",
                "Hao Tian",
                "Zhaoyang Liu",
                "Lixin Gu",
                "Xuehui Wang",
                "Qingyun Li",
                "Yimin Ren",
                "Zixuan Chen",
                "Jiapeng Luo",
                "Jiahao Wang",
                "Tan Jiang",
                "Bo Wang",
                "Conghui He",
                "Botian Shi",
                "Xingcheng Zhang",
                "Han Lv",
                "Yi Wang",
                "Wenqi Shao",
                "Pei Chu",
                "Zhongying Tu",
                "Tong He",
                "Zhiyong Wu",
                "Huipeng Deng",
                "Jiaye Ge",
                "Kai Chen",
                "Min Dou",
                "Lewei Lu",
                "Xizhou Zhu",
                "Tong Lu",
                "Dahua Lin",
                "Yu Qiao",
                "Jifeng Dai",
                "Wenhai Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Nanjing University",
                "SenseTime Research",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05271.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#hallucinations",
                    "#reasoning",
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Прорыв в мультимодальном ИИ: InternVL 2.5 устанавливает новые стандарты",
                    "desc": "InternVL 2.5 - это усовершенствованная мультимодальная большая языковая модель (MLLM), развивающая архитектуру InternVL 2.0. Исследователи изучили связь между масштабированием модели и её производительностью, анализируя тренды в визуальных энкодерах, языковых моделях и размерах датасетов. Модель продемонстрировала конкурентоспособную производительность на различных бенчмарках, включая мультидисциплинарные рассуждения и понимание документов. InternVL 2.5 стала первой открытой MLLM, преодолевшей порог в 70% на бенчмарке MMMU, показав потенциал для масштабирования во время тестирования."
                },
                "en": {
                    "title": "InternVL 2.5: Setting New Standards in Multimodal AI",
                    "desc": "InternVL 2.5 is a state-of-the-art multimodal large language model that enhances its predecessor, InternVL 2.0, by improving training methods and data quality. The paper investigates how increasing the model size affects its performance across various tasks, including reasoning, document understanding, and multimodal comprehension. Extensive testing shows that InternVL 2.5 competes effectively with top commercial models, achieving significant benchmarks like surpassing 70% on the MMMU benchmark. This model aims to advance the open-source community by establishing new benchmarks for multimodal AI applications."
                },
                "zh": {
                    "title": "开创多模态AI新标准的InternVL 2.5",
                    "desc": "我们介绍了InternVL 2.5，这是一个先进的多模态大型语言模型系列，基于InternVL 2.0进行改进。该模型在训练和测试策略以及数据质量上进行了显著增强，并系统地探讨了模型规模与性能之间的关系。通过在多个基准测试上的广泛评估，InternVL 2.5展现了与领先商业模型如GPT-4o和Claude-3.5-Sonnet相媲美的竞争性能。我们的模型首次在MMMU基准上超过70%，并通过链式思维推理实现了3.7点的提升，展示了在测试时扩展的强大潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04814",
            "title": "LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment",
            "url": "https://huggingface.co/papers/2412.04814",
            "abstract": "Recent advancements in text-to-video (T2V) generative models have shown impressive capabilities. However, these models are still inadequate in aligning synthesized videos with human preferences (e.g., accurately reflecting text descriptions), which is particularly difficult to address, as human preferences are inherently subjective and challenging to formalize as objective functions. Therefore, this paper proposes LiFT, a novel fine-tuning method leveraging human feedback for T2V model alignment. Specifically, we first construct a Human Rating Annotation dataset, LiFT-HRA, consisting of approximately 10k human annotations, each including a score and its corresponding rationale. Based on this, we train a reward model LiFT-Critic to learn reward function effectively, which serves as a proxy for human judgment, measuring the alignment between given videos and human expectations. Lastly, we leverage the learned reward function to align the T2V model by maximizing the reward-weighted likelihood. As a case study, we apply our pipeline to CogVideoX-2B, showing that the fine-tuned model outperforms the CogVideoX-5B across all 16 metrics, highlighting the potential of human feedback in improving the alignment and quality of synthesized videos.",
            "score": 19,
            "issue_id": 1013,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "3ac10cfceba4368e",
            "authors": [
                "Yibin Wang",
                "Zhiyu Tan",
                "Junyan Wang",
                "Xiaomeng Yang",
                "Cheng Jin",
                "Hao Li"
            ],
            "affiliations": [
                "Australian Institute for Machine Learning, The University of Adelaide",
                "Fudan University",
                "Shanghai Academy of Artificial Intelligence for Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04814.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#dataset",
                    "#training",
                    "#alignment",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Улучшение генерации видео с помощью человеческих оценок",
                    "desc": "Статья представляет LiFT - новый метод дообучения моделей генерации видео по тексту с использованием обратной связи от людей. Авторы создали датасет LiFT-HRA с 10 тысячами человеческих оценок видео и их обоснованиями. На основе этих данных была обучена модель-критик LiFT-Critic, оценивающая соответствие видео ожиданиям людей. Метод был применен к модели CogVideoX-2B, что позволило превзойти более крупную CogVideoX-5B по 16 метрикам."
                },
                "en": {
                    "title": "Aligning Videos with Human Preferences Using Feedback",
                    "desc": "This paper introduces LiFT, a new method for improving text-to-video (T2V) generative models by incorporating human feedback. The authors create a dataset called LiFT-HRA, which contains around 10,000 human annotations that provide scores and rationales for video quality. They develop a reward model, LiFT-Critic, to quantify how well generated videos align with human expectations, effectively serving as a stand-in for human judgment. By maximizing the reward-weighted likelihood using this model, they demonstrate that their approach significantly enhances the performance of T2V models, as shown in their case study with CogVideoX-2B."
                },
                "zh": {
                    "title": "利用人类反馈提升文本到视频生成模型的对齐性",
                    "desc": "最近，文本到视频生成模型（T2V）取得了显著进展，但在将生成的视频与人类偏好对齐方面仍然存在不足。由于人类偏好具有主观性，难以形式化为客观函数，因此本文提出了一种新颖的微调方法LiFT，利用人类反馈来改善T2V模型的对齐。我们构建了一个包含约1万条人类评分及其理由的注释数据集LiFT-HRA，并基于此训练了一个奖励模型LiFT-Critic，以有效学习奖励函数。通过最大化奖励加权的似然性，我们成功地将T2V模型与人类期望对齐，实验结果表明，微调后的模型在各项指标上均优于原模型，展示了人类反馈在提升生成视频质量方面的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.05237",
            "title": "MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale",
            "url": "https://huggingface.co/papers/2412.05237",
            "abstract": "Open-source multimodal large language models (MLLMs) have shown significant potential in a broad range of multimodal tasks. However, their reasoning capabilities remain constrained by existing instruction-tuning datasets, which were predominately repurposed from academic datasets such as VQA, AI2D, and ChartQA. These datasets target simplistic tasks, and only provide phrase-level answers without any intermediate rationales. To address these challenges, we introduce a scalable and cost-effective method to construct a large-scale multimodal instruction-tuning dataset with rich intermediate rationales designed to elicit CoT reasoning. Using only open models, we create a dataset containing 12M instruction-response pairs to cover diverse, reasoning-intensive tasks with detailed and faithful rationales. Experiments demonstrate that training MLLMs on this dataset significantly improves reasoning capabilities, achieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates notable improvements of up to 4% on non-reasoning-based benchmarks. Ablation studies further highlight the importance of key components, such as rewriting and self-filtering, in the dataset construction process.",
            "score": 15,
            "issue_id": 1012,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "1e3d5645afd61cf2",
            "authors": [
                "Jarvis Guo",
                "Tuney Zheng",
                "Yuelin Bai",
                "Bo Li",
                "Yubo Wang",
                "King Zhu",
                "Yizhi Li",
                "Graham Neubig",
                "Wenhu Chen",
                "Xiang Yue"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Nanyang Technological University",
                "The University of Manchester",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05237.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#reasoning",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение рассуждений мультимодальных ИИ через обучение на масштабном наборе данных с промежуточными выводами",
                    "desc": "Статья представляет новый метод создания масштабного набора данных для обучения мультимодальных языковых моделей с открытым исходным кодом. В отличие от существующих наборов данных, новый подход включает подробные промежуточные рассуждения, что способствует развитию навыков рассуждения у моделей. Авторы создали набор данных из 12 миллионов пар инструкций и ответов, охватывающих разнообразные задачи, требующие интенсивных рассуждений. Эксперименты показали значительное улучшение способностей моделей к рассуждению, достигая лучших результатов на нескольких бенчмарках."
                },
                "en": {
                    "title": "Enhancing Reasoning in MLLMs with Rich Rationales",
                    "desc": "This paper presents a new method for creating a large-scale multimodal instruction-tuning dataset aimed at improving the reasoning abilities of multimodal large language models (MLLMs). The authors identify that existing datasets are limited as they mainly provide simple answers without detailed reasoning. To overcome this, they developed a dataset with 12 million instruction-response pairs that include rich intermediate rationales, promoting chain-of-thought (CoT) reasoning. Experiments show that training on this dataset enhances MLLMs' reasoning performance significantly, achieving state-of-the-art results on various benchmarks."
                },
                "zh": {
                    "title": "提升多模态模型推理能力的新方法",
                    "desc": "这篇论文介绍了一种构建大规模多模态指令调优数据集的方法，以提高多模态大语言模型（MLLMs）的推理能力。现有的数据集主要来自学术研究，任务简单，缺乏中间推理过程的详细解释。我们的方法创建了一个包含1200万对指令和响应的数据集，涵盖了多样化且需要推理的任务，并提供了丰富的推理依据。实验结果表明，使用该数据集训练的MLLMs在多个基准测试中显著提高了推理能力，达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04445",
            "title": "Moto: Latent Motion Token as the Bridging Language for Robot Manipulation",
            "url": "https://huggingface.co/papers/2412.04445",
            "abstract": "Recent developments in Large Language Models pre-trained on extensive corpora have shown significant success in various natural language processing tasks with minimal fine-tuning. This success offers new promise for robotics, which has long been constrained by the high cost of action-labeled data. We ask: given the abundant video data containing interaction-related knowledge available as a rich \"corpus\", can a similar generative pre-training approach be effectively applied to enhance robot learning? The key challenge is to identify an effective representation for autoregressive pre-training that benefits robot manipulation tasks. Inspired by the way humans learn new skills through observing dynamic environments, we propose that effective robotic learning should emphasize motion-related knowledge, which is closely tied to low-level actions and is hardware-agnostic, facilitating the transfer of learned motions to actual robot actions. To this end, we introduce Moto, which converts video content into latent Motion Token sequences by a Latent Motion Tokenizer, learning a bridging \"language\" of motion from videos in an unsupervised manner. We pre-train Moto-GPT through motion token autoregression, enabling it to capture diverse visual motion knowledge. After pre-training, Moto-GPT demonstrates the promising ability to produce semantically interpretable motion tokens, predict plausible motion trajectories, and assess trajectory rationality through output likelihood. To transfer learned motion priors to real robot actions, we implement a co-fine-tuning strategy that seamlessly bridges latent motion token prediction and real robot control. Extensive experiments show that the fine-tuned Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks, underscoring its effectiveness in transferring knowledge from video data to downstream visual manipulation tasks.",
            "score": 14,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "a5ac6d786500ef9f",
            "authors": [
                "Yi Chen",
                "Yuying Ge",
                "Yizhuo Li",
                "Yixiao Ge",
                "Mingyu Ding",
                "Ying Shan",
                "Xihui Liu"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "The University of Hong Kong",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04445.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#transfer_learning",
                    "#games",
                    "#robotics",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Обучение роботов движению через предобучение на видеоданных",
                    "desc": "В статье представлен новый подход к обучению роботов на основе предобученных языковых моделей. Авторы предлагают метод Moto, который преобразует видеоконтент в последовательности латентных токенов движения с помощью Latent Motion Tokenizer. Moto-GPT обучается на основе авторегрессии токенов движения, что позволяет ей захватывать разнообразные знания о визуальном движении. После предобучения и дообучения Moto-GPT демонстрирует высокую эффективность в задачах манипуляции роботов."
                },
                "en": {
                    "title": "Bridging Video Knowledge to Robot Actions with Moto-GPT",
                    "desc": "This paper explores the application of Large Language Models to enhance robot learning by leveraging abundant video data. It introduces Moto, a system that converts video content into Motion Token sequences, allowing robots to learn motion-related knowledge in an unsupervised manner. The approach emphasizes the importance of motion understanding for effective robotic manipulation, enabling the transfer of learned skills to real-world actions. Through extensive experiments, Moto-GPT demonstrates improved performance in robot manipulation tasks, showcasing its ability to bridge the gap between video knowledge and practical robot control."
                },
                "zh": {
                    "title": "利用视频数据提升机器人学习能力",
                    "desc": "这篇论文探讨了如何利用大规模视频数据来提升机器人学习能力。研究者提出了一种名为Moto的模型，通过将视频内容转换为潜在的运动标记序列，来学习运动知识。Moto-GPT经过预训练后，能够生成可解释的运动标记，并预测合理的运动轨迹。最终，研究表明，经过微调的Moto-GPT在机器人操作基准测试中表现出色，证明了从视频数据转移知识的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04440",
            "title": "GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration",
            "url": "https://huggingface.co/papers/2412.04440",
            "abstract": "Text-to-video generation models have shown significant progress in the recent years. However, they still struggle with generating complex dynamic scenes based on compositional text prompts, such as attribute binding for multiple objects, temporal dynamics associated with different objects, and interactions between objects. Our key motivation is that complex tasks can be decomposed into simpler ones, each handled by a role-specialized MLLM agent. Multiple agents can collaborate together to achieve collective intelligence for complex goals. We propose GenMAC, an iterative, multi-agent framework that enables compositional text-to-video generation. The collaborative workflow includes three stages: Design, Generation, and Redesign, with an iterative loop between the Generation and Redesign stages to progressively verify and refine the generated videos. The Redesign stage is the most challenging stage that aims to verify the generated videos, suggest corrections, and redesign the text prompts, frame-wise layouts, and guidance scales for the next iteration of generation. To avoid hallucination of a single MLLM agent, we decompose this stage to four sequentially-executed MLLM-based agents: verification agent, suggestion agent, correction agent, and output structuring agent. Furthermore, to tackle diverse scenarios of compositional text-to-video generation, we design a self-routing mechanism to adaptively select the proper correction agent from a collection of correction agents each specialized for one scenario. Extensive experiments demonstrate the effectiveness of GenMAC, achieving state-of-the art performance in compositional text-to-video generation.",
            "score": 10,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "28dc2191ba71c4ea",
            "authors": [
                "Kaiyi Huang",
                "Yukun Huang",
                "Xuefei Ning",
                "Zinan Lin",
                "Yu Wang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Microsoft Research",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04440.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#agents",
                    "#games",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Коллективный интеллект агентов для создания сложных видео по тексту",
                    "desc": "Статья представляет GenMAC - итеративную мультиагентную систему для генерации видео по текстовому описанию. Система использует несколько специализированных агентов на основе мультимодальных языковых моделей (MLLM) для декомпозиции сложных задач. Процесс включает этапы проектирования, генерации и перепроектирования, с итеративным циклом между генерацией и перепроектированием для постепенного улучшения результата. GenMAC демонстрирует передовые результаты в генерации композиционных видео по текстовому описанию."
                },
                "en": {
                    "title": "Collaborative Intelligence for Text-to-Video Mastery",
                    "desc": "This paper introduces GenMAC, a multi-agent framework designed to improve text-to-video generation by breaking down complex tasks into simpler components. It utilizes specialized machine learning agents that collaborate through three main stages: Design, Generation, and Redesign, with an iterative process to refine video outputs. The Redesign stage is particularly crucial as it involves multiple agents that verify, suggest corrections, and restructure prompts to enhance the generated videos. The framework also includes a self-routing mechanism to select the most suitable correction agent for various scenarios, leading to significant advancements in generating dynamic scenes from text prompts."
                },
                "zh": {
                    "title": "GenMAC：协作生成复杂视频的智能框架",
                    "desc": "文本到视频生成模型近年来取得了显著进展，但在生成复杂动态场景时仍面临挑战。我们提出了一种名为GenMAC的多代理框架，通过将复杂任务分解为简单任务来实现协作生成。该框架包括设计、生成和重新设计三个阶段，生成和重新设计之间存在迭代循环，以逐步验证和优化生成的视频。通过自适应选择专门化的修正代理，GenMAC能够有效应对多样化的文本到视频生成场景。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.05270",
            "title": "APOLLO: SGD-like Memory, AdamW-level Performance",
            "url": "https://huggingface.co/papers/2412.05270",
            "abstract": "Large language models (LLMs) are notoriously memory-intensive during training, particularly with the popular AdamW optimizer. This memory burden necessitates using more or higher-end GPUs or reducing batch sizes, limiting training scalability and throughput. To address this, various memory-efficient optimizers have been proposed to reduce optimizer memory usage. However, they face critical challenges: (i) reliance on costly SVD operations; (ii) significant performance trade-offs compared to AdamW; and (iii) still substantial optimizer memory overhead to maintain competitive performance.   In this work, we identify that AdamW's learning rate adaptation rule can be effectively coarsened as a structured learning rate update. Based on this insight, we propose Approximated Gradient Scaling for Memory-Efficient LLM Optimization (APOLLO), which approximates learning rate scaling using an auxiliary low-rank optimizer state based on pure random projection. This structured learning rate update rule makes APOLLO highly tolerant to further memory reductions while delivering comparable pre-training performance. Even its rank-1 variant, APOLLO-Mini, achieves superior pre-training performance compared to AdamW with SGD-level memory costs.   Extensive experiments demonstrate that the APOLLO series performs on-par with or better than AdamW, while achieving greater memory savings by nearly eliminating the optimization states of AdamW. These savings provide significant system-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB setup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model Scalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without system-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training LLaMA-7B on a single GPU using less than 12 GB of memory with weight quantization.",
            "score": 9,
            "issue_id": 1012,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "14bb480f5fe29bae",
            "authors": [
                "Hanqing Zhu",
                "Zhenyu Zhang",
                "Wenyan Cong",
                "Xi Liu",
                "Sem Park",
                "Vikas Chandra",
                "Bo Long",
                "David Z. Pan",
                "Zhangyang Wang",
                "Jinwon Lee"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.05270.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "APOLLO: эффективное обучение LLM с минимальными затратами памяти",
                    "desc": "Статья представляет новый оптимизатор APOLLO для обучения больших языковых моделей (LLM). APOLLO использует структурированное обновление скорости обучения, основанное на случайном проецировании, что позволяет значительно снизить потребление памяти по сравнению с популярным оптимизатором AdamW. Эксперименты показывают, что APOLLO достигает сопоставимой или лучшей производительности, чем AdamW, при существенной экономии памяти. Это позволяет увеличить пропускную способность, масштабируемость модели и делает возможным обучение LLM даже на GPU среднего уровня."
                },
                "en": {
                    "title": "APOLLO: Memory-Efficient Optimization for Large Language Models",
                    "desc": "This paper addresses the high memory requirements of training large language models (LLMs) using the AdamW optimizer. It introduces a new method called APOLLO, which simplifies the learning rate adaptation process to reduce memory usage while maintaining performance. APOLLO utilizes a low-rank optimizer state based on random projection, allowing for significant memory savings without sacrificing training efficiency. The results show that APOLLO can achieve comparable or better performance than AdamW, enabling larger batch sizes and making LLM training feasible on lower-end GPUs."
                },
                "zh": {
                    "title": "APOLLO：高效内存优化的未来",
                    "desc": "大型语言模型（LLMs）在训练过程中对内存的需求非常高，尤其是使用流行的AdamW优化器时。为了解决这个问题，研究者们提出了多种内存高效的优化器，但它们面临着依赖昂贵的SVD操作和性能折衷等挑战。本文提出了一种名为APOLLO的优化方法，通过近似学习率缩放来减少内存使用，同时保持与AdamW相当的预训练性能。实验结果表明，APOLLO系列在内存节省方面表现优异，能够在较低的内存成本下实现更高的训练吞吐量和模型可扩展性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04862",
            "title": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases",
            "url": "https://huggingface.co/papers/2412.04862",
            "abstract": "This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from https://huggingface.co/LGAI-EXAONE. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.",
            "score": 6,
            "issue_id": 1011,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "83e6f957e42ebb5e",
            "authors": [
                "LG AI Research",
                "Soyoung An",
                "Kyunghoon Bae",
                "Eunbi Choi",
                "Kibong Choi",
                "Stanley Jungkyu Choi",
                "Seokhee Hong",
                "Junwon Hwang",
                "Hyojin Jeon",
                "Gerrard Jeongwon Jo",
                "Hyunjik Jo",
                "Jiyeon Jung",
                "Yountae Jung",
                "Hyosang Kim",
                "Joonkee Kim",
                "Seonghwan Kim",
                "Soyeon Kim",
                "Sunkyoung Kim",
                "Yireun Kim",
                "Yongil Kim",
                "Youchul Kim",
                "Edward Hwayoung Lee",
                "Haeju Lee",
                "Honglak Lee",
                "Jinsik Lee",
                "Kyungmin Lee",
                "Woohyung Lim",
                "Sangha Park",
                "Sooyoun Park",
                "Yongmin Park",
                "Sihoon Yang",
                "Heuiyeen Yeen",
                "Hyeongu Yun"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2412.04862.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#open_source",
                    "#small_models",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "EXAONE 3.5: Новое слово в языковых моделях от LG AI Research",
                    "desc": "Компания LG AI Research представила языковые модели EXAONE 3.5, обученные выполнению инструкций. Модели доступны в трех конфигурациях: 32B, 7.8B и 2.4B параметров. Они демонстрируют исключительные способности в следовании инструкциям, понимании длинного контекста и показывают конкурентоспособные результаты по сравнению с современными открытыми моделями аналогичного размера. Модели EXAONE 3.5 доступны для исследовательских целей и могут быть загружены с платформы Hugging Face."
                },
                "en": {
                    "title": "EXAONE 3.5: Leading the Way in Instruction-Tuned Language Models",
                    "desc": "The EXAONE 3.5 language models, created by LG AI Research, are advanced instruction-tuned models available in three sizes: 32B, 7.8B, and 2.4B. They excel in following instructions in real-world applications, achieving top scores on seven different benchmarks. Additionally, these models demonstrate superior long-context understanding, ranking first in four benchmarks. They are accessible for research purposes and show competitive performance against leading open models of similar sizes across nine general benchmarks."
                },
                "zh": {
                    "title": "EXAONE 3.5：指令跟随与长上下文理解的先锋",
                    "desc": "EXAONE 3.5 是由 LG AI 研究所开发的指令调优语言模型，提供三种配置：32B、7.8B 和 2.4B。这些模型在实际场景中具有卓越的指令跟随能力，在七个基准测试中取得了最高分。它们在长上下文理解方面表现出色，在四个基准测试中也达到了最佳性能。此外，与同类开源模型相比，EXAONE 3.5 在九个通用基准测试中也展现了竞争力的结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03428",
            "title": "2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction",
            "url": "https://huggingface.co/papers/2412.03428",
            "abstract": "The reconstruction of indoor scenes remains challenging due to the inherent complexity of spatial structures and the prevalence of textureless regions. Recent advancements in 3D Gaussian Splatting have improved novel view synthesis with accelerated processing but have yet to deliver comparable performance in surface reconstruction. In this paper, we introduce 2DGS-Room, a novel method leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction. Specifically, we employ a seed-guided mechanism to control the distribution of 2D Gaussians, with the density of seed points dynamically optimized through adaptive growth and pruning mechanisms. To further improve geometric accuracy, we incorporate monocular depth and normal priors to provide constraints for details and textureless regions respectively. Additionally, multi-view consistency constraints are employed to mitigate artifacts and further enhance reconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasets demonstrate that our method achieves state-of-the-art performance in indoor scene reconstruction.",
            "score": 3,
            "issue_id": 1013,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "ead3f67b9be4d52b",
            "authors": [
                "Wanting Zhang",
                "Haodong Xiang",
                "Zhichao Liao",
                "Xiansong Lai",
                "Xinghui Li",
                "Long Zeng"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03428.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "🏠",
                "ru": {
                    "title": "2DGS-Room: Революция в реконструкции интерьеров с помощью 2D гауссовского сплаттинга",
                    "desc": "Эта статья представляет новый метод 2DGS-Room для реконструкции интерьеров с использованием 2D гауссовского сплаттинга. Авторы применяют механизм управления распределением 2D гауссианов с помощью семян, оптимизируя их плотность через адаптивный рост и отсечение. Для улучшения геометрической точности используются монокулярные глубинные и нормальные приоры, а также ограничения многоракурсной согласованности. Эксперименты на наборах данных ScanNet и ScanNet++ показывают, что метод достигает наилучших результатов в реконструкции интерьеров."
                },
                "en": {
                    "title": "Revolutionizing Indoor Scene Reconstruction with 2D Gaussian Splatting",
                    "desc": "This paper presents 2DGS-Room, a new approach for reconstructing indoor scenes using 2D Gaussian Splatting. The method introduces a seed-guided mechanism that optimizes the distribution of 2D Gaussians, enhancing the reconstruction process. By incorporating monocular depth and normal priors, the approach improves geometric accuracy, especially in areas lacking texture. The use of multi-view consistency constraints helps reduce artifacts, leading to high-fidelity results in indoor scene reconstruction, as demonstrated by experiments on ScanNet and ScanNet++ datasets."
                },
                "zh": {
                    "title": "高保真室内场景重建的新方法",
                    "desc": "室内场景的重建因空间结构复杂和无纹理区域的普遍存在而具有挑战性。本文提出了一种新方法2DGS-Room，利用2D高斯点云实现高保真度的室内场景重建。我们采用种子引导机制来控制2D高斯的分布，并通过自适应生长和修剪机制动态优化种子点的密度。通过结合单目深度和法线先验，我们进一步提高了几何精度，并使用多视图一致性约束来减少伪影，提升重建质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04301",
            "title": "SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion",
            "url": "https://huggingface.co/papers/2412.04301",
            "abstract": "Recent advances in text-guided image editing enable users to perform image edits through simple text inputs, leveraging the extensive priors of multi-step diffusion-based text-to-image models. However, these methods often fall short of the speed demands required for real-world and on-device applications due to the costly multi-step inversion and sampling process involved. In response to this, we introduce SwiftEdit, a simple yet highly efficient editing tool that achieve instant text-guided image editing (in 0.23s). The advancement of SwiftEdit lies in its two novel contributions: a one-step inversion framework that enables one-step image reconstruction via inversion and a mask-guided editing technique with our proposed attention rescaling mechanism to perform localized image editing. Extensive experiments are provided to demonstrate the effectiveness and efficiency of SwiftEdit. In particular, SwiftEdit enables instant text-guided image editing, which is extremely faster than previous multi-step methods (at least 50 times faster) while maintain a competitive performance in editing results. Our project page is at: https://swift-edit.github.io/",
            "score": 3,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "a4cea89a59a9a3c0",
            "authors": [
                "Trong-Tung Nguyen",
                "Quang Nguyen",
                "Khoi Nguyen",
                "Anh Tran",
                "Cuong Pham"
            ],
            "affiliations": [
                "Posts & Telecom. Inst. of Tech., Vietnam",
                "VinAI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04301.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Мгновенное редактирование изображений текстом",
                    "desc": "SwiftEdit - это новый инструмент для быстрого редактирования изображений с помощью текстовых запросов. В отличие от существующих многошаговых методов, SwiftEdit использует одношаговую инверсию и механизм масштабирования внимания для локального редактирования. Это позволяет выполнять изменения изображений практически мгновенно - за 0.23 секунды. SwiftEdit работает как минимум в 50 раз быстрее аналогов, сохраняя при этом высокое качество результатов."
                },
                "en": {
                    "title": "SwiftEdit: Instant Text-Guided Image Editing at Lightning Speed!",
                    "desc": "This paper presents SwiftEdit, a new tool for fast text-guided image editing that significantly improves the speed of image modifications. Traditional methods rely on multi-step diffusion processes, which can be slow and inefficient for real-time applications. SwiftEdit introduces a one-step inversion framework that allows for quick image reconstruction and a mask-guided editing technique that uses attention rescaling for precise edits. The results show that SwiftEdit is at least 50 times faster than previous methods while still delivering high-quality editing outcomes."
                },
                "zh": {
                    "title": "SwiftEdit：瞬时文本引导图像编辑的革命",
                    "desc": "最近的文本引导图像编辑技术使用户能够通过简单的文本输入进行图像编辑，利用了多步扩散模型的丰富先验知识。然而，这些方法在实际应用中往往速度较慢，无法满足实时和设备端的需求。为此，我们提出了SwiftEdit，这是一种简单而高效的编辑工具，可以实现瞬时的文本引导图像编辑，速度达到0.23秒。SwiftEdit的创新在于其一体化的反演框架和基于掩码的编辑技术，能够快速进行局部图像编辑，同时保持与传统多步方法相当的编辑效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.05263",
            "title": "Mind the Time: Temporally-Controlled Multi-Event Video Generation",
            "url": "https://huggingface.co/papers/2412.05263",
            "abstract": "Real-world videos consist of sequences of events. Generating such sequences with precise temporal control is infeasible with existing video generators that rely on a single paragraph of text as input. When tasked with generating multiple events described using a single prompt, such methods often ignore some of the events or fail to arrange them in the correct order. To address this limitation, we present MinT, a multi-event video generator with temporal control. Our key insight is to bind each event to a specific period in the generated video, which allows the model to focus on one event at a time. To enable time-aware interactions between event captions and video tokens, we design a time-based positional encoding method, dubbed ReRoPE. This encoding helps to guide the cross-attention operation. By fine-tuning a pre-trained video diffusion transformer on temporally grounded data, our approach produces coherent videos with smoothly connected events. For the first time in the literature, our model offers control over the timing of events in generated videos. Extensive experiments demonstrate that MinT outperforms existing open-source models by a large margin.",
            "score": 2,
            "issue_id": 1014,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "826bb588770d5c27",
            "authors": [
                "Ziyi Wu",
                "Aliaksandr Siarohin",
                "Willi Menapace",
                "Ivan Skorokhodov",
                "Yuwei Fang",
                "Varnith Chordia",
                "Igor Gilitschenski",
                "Sergey Tulyakov"
            ],
            "affiliations": [
                "Snap Research",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05263.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#architecture",
                    "#diffusion",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "⏱️",
                "ru": {
                    "title": "Точный контроль времени в генерации видео с несколькими событиями",
                    "desc": "Статья представляет MinT - генератор видео с множеством событий и временным контролем. Метод привязывает каждое событие к определенному периоду в генерируемом видео, что позволяет модели фокусироваться на одном событии за раз. Авторы разработали метод временного позиционного кодирования ReRoPE для управления взаимодействием между описаниями событий и видеотокенами. Модель, дообученная на темпорально размеченных данных, создает согласованные видео с плавно соединенными событиями, превосходя существующие открытые модели."
                },
                "en": {
                    "title": "MinT: Mastering Multi-Event Video Generation with Temporal Precision",
                    "desc": "This paper introduces MinT, a novel multi-event video generator that allows for precise temporal control over the events depicted in generated videos. Unlike traditional models that struggle with sequencing multiple events from a single text prompt, MinT binds each event to a specific time period, ensuring that all events are accurately represented and ordered. The authors implement a time-based positional encoding method called ReRoPE, which enhances the model's ability to manage interactions between event descriptions and video frames. By fine-tuning a pre-trained video diffusion transformer on data with temporal grounding, MinT achieves superior performance in generating coherent videos with well-timed events."
                },
                "zh": {
                    "title": "MinT：精准控制视频事件时序的生成器",
                    "desc": "本论文提出了一种名为MinT的多事件视频生成器，旨在解决现有视频生成器在生成多个事件时的时间控制问题。通过将每个事件绑定到生成视频的特定时间段，MinT能够逐个关注事件，从而提高生成视频的连贯性。我们设计了一种基于时间的位置信息编码方法ReRoPE，以增强事件描述与视频帧之间的交互。实验结果表明，MinT在生成视频的时间控制方面优于现有的开源模型。"
                }
            }
        }
    ],
    "link_prev": "2024-12-06.html",
    "link_next": "2024-12-10.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "06.12",
        "en": "12/06",
        "zh": "12月6日"
    },
    "short_date_next": {
        "ru": "10.12",
        "en": "12/10",
        "zh": "12月10日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 3,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "最近的视觉-语言模型通过增加视觉标记的长度提高了性能，但也增加了计算成本。我们发现，流行的视觉编码器生成的视觉标记存在大量冗余。为解决这个问题，我们引入了VisionZip，一种选择信息丰富的视觉标记的方法，减少冗余并提高效率。VisionZip可应用于图像和视频理解任务，适用于多轮对话。实验结果显示，VisionZip在几乎所有设置中都优于之前的最佳方法，并显著提高了模型推理速度。",
        "title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models",
        "pinyin": "最近的视觉-语言模型通过增加视觉标记的长度提高了性能，但也增加了计算成本。我们发现，流行的视觉编码器生成的视觉标记存在大量冗余。为解决这个问题，我们引入了VisionZip，一种选择信息丰富的视觉标记的方法，减少冗余并提高效率。VisionZip可应用于图像和视频理解任务，适用于多轮对话。实验结果显示，VisionZip在几乎所有设置中都优于之前的最佳方法，并显著提高了模型推理速度。\n\nZuìjìn de shìjué-yǔyán móxíng tōngguò zēngjiā shìjué biāojì de chángdù tígāo le xìngnéng, dàn yě zēngjiā le jìsuàn chéngběn. Wǒmen fāxiàn, liúxíng de shìjué biānmǎqì shēngchéng de shìjué biāojì cúnzài dàliàng rǒngyù. Wèi jiějué zhègè wèntí, wǒmen yǐnrù le VisionZip, yīzhǒng xuǎnzé xìnxī fēngfù de shìjué biāojì de fāngfǎ, jiǎnshǎo rǒngyù bìng tígāo xiàolǜ. VisionZip kě yìngyòng yú túxiàng hé shìpǐn lǐjiě rènwù, shìyòng yú duōlún duìhuà. Shíyàn jiéguǒ xiǎnshì, VisionZip zài jīhuā suǒyǒu shèzhì zhōng dōu yōu zhīqián de zuìjiā fāngfǎ, bìng xiǎnzhù tígāo le móxíng tuīlǐ sùdù.",
        "vocab": "[{'word': '视觉', 'pinyin': 'shìjué', 'trans': 'vision'}, {'word': '语言', 'pinyin': 'yǔyán', 'trans': 'language'}, {'word': '模型', 'pinyin': 'móxíng', 'trans': 'model'}, {'word': '增加', 'pinyin': 'zēngjiā', 'trans': 'increase'}, {'word': '长度', 'pinyin': 'chángdù', 'trans': 'length'}, {'word': '性能', 'pinyin': 'xìngnéng', 'trans': 'performance'}, {'word': '计算', 'pinyin': 'jìsuàn', 'trans': 'calculation'}, {'word': '成本', 'pinyin': 'chéngběn', 'trans': 'cost'}, {'word': '发现', 'pinyin': 'fāxiàn', 'trans': 'discover'}, {'word': '流行', 'pinyin': 'liúxíng', 'trans': 'popular'}, {'word': '编码器', 'pinyin': 'biānmǎqì', 'trans': 'encoder'}, {'word': '生成', 'pinyin': 'shēngchéng', 'trans': 'generate'}, {'word': '标记', 'pinyin': 'biāojì', 'trans': 'token'}, {'word': '存在', 'pinyin': 'cúnzài', 'trans': 'exist'}, {'word': '冗余', 'pinyin': 'róngyú', 'trans': 'redundancy'}, {'word': '解决', 'pinyin': 'jiějué', 'trans': 'solve'}, {'word': '引入', 'pinyin': 'yǐnrù', 'trans': 'introduce'}, {'word': '选择', 'pinyin': 'xuǎnzé', 'trans': 'select'}, {'word': '丰富', 'pinyin': 'fēngfù', 'trans': 'rich'}, {'word': '方法', 'pinyin': 'fāngfǎ', 'trans': 'method'}, {'word': '减少', 'pinyin': 'jiǎnshǎo', 'trans': 'reduce'}, {'word': '效率', 'pinyin': 'xiàolǜ', 'trans': 'efficiency'}, {'word': '应用', 'pinyin': 'yìngyòng', 'trans': 'apply'}, {'word': '图像', 'pinyin': 'túxiàng', 'trans': 'image'}, {'word': '视频', 'pinyin': 'shìpín', 'trans': 'video'}, {'word': '理解', 'pinyin': 'lǐjiě', 'trans': 'understanding'}, {'word': '任务', 'pinyin': 'rènwù', 'trans': 'task'}, {'word': '适用', 'pinyin': 'shìyòng', 'trans': 'applicable'}, {'word': '多轮', 'pinyin': 'duōlún', 'trans': 'multi-turn'}, {'word': '对话', 'pinyin': 'duìhuà', 'trans': 'dialogue'}, {'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'}, {'word': '结果', 'pinyin': 'jiéguǒ', 'trans': 'result'}, {'word': '显示', 'pinyin': 'xiǎnshì', 'trans': 'show'}, {'word': '优于', 'pinyin': 'yōuyú', 'trans': 'superior to'}, {'word': '之前', 'pinyin': 'zhīqián', 'trans': 'previous'}, {'word': '最佳', 'pinyin': 'zuìjiā', 'trans': 'best'}, {'word': '设置', 'pinyin': 'shèzhì', 'trans': 'setting'}, {'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'}, {'word': '提高', 'pinyin': 'tígāo', 'trans': 'improve'}, {'word': '推理', 'pinyin': 'tuīlǐ', 'trans': 'inference'}, {'word': '速度', 'pinyin': 'sùdù', 'trans': 'speed'}]",
        "trans": "Recent vision-language models have improved performance by increasing the length of visual tokens, but this has also increased computational costs. We have discovered that popular visual encoders generate visual tokens with a significant amount of redundancy. To address this issue, we introduce VisionZip, a method for selecting information-rich visual tokens to reduce redundancy and enhance efficiency. VisionZip can be applied to both image and video understanding tasks and is suitable for multi-turn conversations. Experimental results show that VisionZip outperforms previous best methods in almost all settings and significantly speeds up model inference.",
        "update_ts": "2024-12-08 12:42"
    }
}