{
    "date": {
        "ru": "9 декабря",
        "en": "December 9",
        "zh": "12月9日"
    },
    "time_utc": "2024-12-09 14:10",
    "weekday": 0,
    "issue_id": 1023,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.05271",
            "title": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
            "url": "https://huggingface.co/papers/2412.05271",
            "abstract": "We introduce InternVL 2.5, an advanced multimodal large language model (MLLM) series that builds upon InternVL 2.0, maintaining its core model architecture while introducing significant enhancements in training and testing strategies as well as data quality. In this work, we delve into the relationship between model scaling and performance, systematically exploring the performance trends in vision encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations on a wide range of benchmarks, including multi-discipline reasoning, document understanding, multi-image / video understanding, real-world comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processing, InternVL 2.5 exhibits competitive performance, rivaling leading commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a 3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing strong potential for test-time scaling. We hope this model contributes to the open-source community by setting new standards for developing and applying multimodal AI systems. HuggingFace demo see https://huggingface.co/spaces/OpenGVLab/InternVL",
            "score": 45,
            "issue_id": 1013,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "81590cc90bda9173",
            "authors": [
                "Zhe Chen",
                "Weiyun Wang",
                "Yue Cao",
                "Yangzhou Liu",
                "Zhangwei Gao",
                "Erfei Cui",
                "Jinguo Zhu",
                "Shenglong Ye",
                "Hao Tian",
                "Zhaoyang Liu",
                "Lixin Gu",
                "Xuehui Wang",
                "Qingyun Li",
                "Yimin Ren",
                "Zixuan Chen",
                "Jiapeng Luo",
                "Jiahao Wang",
                "Tan Jiang",
                "Bo Wang",
                "Conghui He",
                "Botian Shi",
                "Xingcheng Zhang",
                "Han Lv",
                "Yi Wang",
                "Wenqi Shao",
                "Pei Chu",
                "Zhongying Tu",
                "Tong He",
                "Zhiyong Wu",
                "Huipeng Deng",
                "Jiaye Ge",
                "Kai Chen",
                "Min Dou",
                "Lewei Lu",
                "Xizhou Zhu",
                "Tong Lu",
                "Dahua Lin",
                "Yu Qiao",
                "Jifeng Dai",
                "Wenhai Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Nanjing University",
                "SenseTime Research",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05271.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#hallucinations",
                    "#reasoning",
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Прорыв в мультимодальном ИИ: InternVL 2.5 устанавливает новые стандарты",
                    "desc": "InternVL 2.5 - это усовершенствованная мультимодальная большая языковая модель (MLLM), развивающая архитектуру InternVL 2.0. Исследователи изучили связь между масштабированием модели и её производительностью, анализируя тренды в визуальных энкодерах, языковых моделях и размерах датасетов. Модель продемонстрировала конкурентоспособную производительность на различных бенчмарках, включая мультидисциплинарные рассуждения и понимание документов. InternVL 2.5 стала первой открытой MLLM, преодолевшей порог в 70% на бенчмарке MMMU, показав потенциал для масштабирования во время тестирования."
                },
                "en": {
                    "title": "InternVL 2.5: Setting New Standards in Multimodal AI",
                    "desc": "InternVL 2.5 is a state-of-the-art multimodal large language model that enhances its predecessor, InternVL 2.0, by improving training methods and data quality. The paper investigates how increasing the model size affects its performance across various tasks, including reasoning, document understanding, and multimodal comprehension. Extensive testing shows that InternVL 2.5 competes effectively with top commercial models, achieving significant benchmarks like surpassing 70% on the MMMU benchmark. This model aims to advance the open-source community by establishing new benchmarks for multimodal AI applications."
                },
                "zh": {
                    "title": "开创多模态AI新标准的InternVL 2.5",
                    "desc": "我们介绍了InternVL 2.5，这是一个先进的多模态大型语言模型系列，基于InternVL 2.0进行改进。该模型在训练和测试策略以及数据质量上进行了显著增强，并系统地探讨了模型规模与性能之间的关系。通过在多个基准测试上的广泛评估，InternVL 2.5展现了与领先商业模型如GPT-4o和Claude-3.5-Sonnet相媲美的竞争性能。我们的模型首次在MMMU基准上超过70%，并通过链式思维推理实现了3.7点的提升，展示了在测试时扩展的强大潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04814",
            "title": "LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment",
            "url": "https://huggingface.co/papers/2412.04814",
            "abstract": "Recent advancements in text-to-video (T2V) generative models have shown impressive capabilities. However, these models are still inadequate in aligning synthesized videos with human preferences (e.g., accurately reflecting text descriptions), which is particularly difficult to address, as human preferences are inherently subjective and challenging to formalize as objective functions. Therefore, this paper proposes LiFT, a novel fine-tuning method leveraging human feedback for T2V model alignment. Specifically, we first construct a Human Rating Annotation dataset, LiFT-HRA, consisting of approximately 10k human annotations, each including a score and its corresponding rationale. Based on this, we train a reward model LiFT-Critic to learn reward function effectively, which serves as a proxy for human judgment, measuring the alignment between given videos and human expectations. Lastly, we leverage the learned reward function to align the T2V model by maximizing the reward-weighted likelihood. As a case study, we apply our pipeline to CogVideoX-2B, showing that the fine-tuned model outperforms the CogVideoX-5B across all 16 metrics, highlighting the potential of human feedback in improving the alignment and quality of synthesized videos.",
            "score": 30,
            "issue_id": 1013,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "3ac10cfceba4368e",
            "authors": [
                "Yibin Wang",
                "Zhiyu Tan",
                "Junyan Wang",
                "Xiaomeng Yang",
                "Cheng Jin",
                "Hao Li"
            ],
            "affiliations": [
                "Australian Institute for Machine Learning, The University of Adelaide",
                "Fudan University",
                "Shanghai Academy of Artificial Intelligence for Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04814.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#dataset",
                    "#training",
                    "#alignment",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Улучшение генерации видео с помощью человеческих оценок",
                    "desc": "Статья представляет LiFT - новый метод дообучения моделей генерации видео по тексту с использованием обратной связи от людей. Авторы создали датасет LiFT-HRA с 10 тысячами человеческих оценок видео и их обоснованиями. На основе этих данных была обучена модель-критик LiFT-Critic, оценивающая соответствие видео ожиданиям людей. Метод был применен к модели CogVideoX-2B, что позволило превзойти более крупную CogVideoX-5B по 16 метрикам."
                },
                "en": {
                    "title": "Aligning Videos with Human Preferences Using Feedback",
                    "desc": "This paper introduces LiFT, a new method for improving text-to-video (T2V) generative models by incorporating human feedback. The authors create a dataset called LiFT-HRA, which contains around 10,000 human annotations that provide scores and rationales for video quality. They develop a reward model, LiFT-Critic, to quantify how well generated videos align with human expectations, effectively serving as a stand-in for human judgment. By maximizing the reward-weighted likelihood using this model, they demonstrate that their approach significantly enhances the performance of T2V models, as shown in their case study with CogVideoX-2B."
                },
                "zh": {
                    "title": "利用人类反馈提升文本到视频生成模型的对齐性",
                    "desc": "最近，文本到视频生成模型（T2V）取得了显著进展，但在将生成的视频与人类偏好对齐方面仍然存在不足。由于人类偏好具有主观性，难以形式化为客观函数，因此本文提出了一种新颖的微调方法LiFT，利用人类反馈来改善T2V模型的对齐。我们构建了一个包含约1万条人类评分及其理由的注释数据集LiFT-HRA，并基于此训练了一个奖励模型LiFT-Critic，以有效学习奖励函数。通过最大化奖励加权的似然性，我们成功地将T2V模型与人类期望对齐，实验结果表明，微调后的模型在各项指标上均优于原模型，展示了人类反馈在提升生成视频质量方面的潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.05237",
            "title": "MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale",
            "url": "https://huggingface.co/papers/2412.05237",
            "abstract": "Open-source multimodal large language models (MLLMs) have shown significant potential in a broad range of multimodal tasks. However, their reasoning capabilities remain constrained by existing instruction-tuning datasets, which were predominately repurposed from academic datasets such as VQA, AI2D, and ChartQA. These datasets target simplistic tasks, and only provide phrase-level answers without any intermediate rationales. To address these challenges, we introduce a scalable and cost-effective method to construct a large-scale multimodal instruction-tuning dataset with rich intermediate rationales designed to elicit CoT reasoning. Using only open models, we create a dataset containing 12M instruction-response pairs to cover diverse, reasoning-intensive tasks with detailed and faithful rationales. Experiments demonstrate that training MLLMs on this dataset significantly improves reasoning capabilities, achieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates notable improvements of up to 4% on non-reasoning-based benchmarks. Ablation studies further highlight the importance of key components, such as rewriting and self-filtering, in the dataset construction process.",
            "score": 26,
            "issue_id": 1012,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "1e3d5645afd61cf2",
            "authors": [
                "Jarvis Guo",
                "Tuney Zheng",
                "Yuelin Bai",
                "Bo Li",
                "Yubo Wang",
                "King Zhu",
                "Yizhi Li",
                "Graham Neubig",
                "Wenhu Chen",
                "Xiang Yue"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Nanyang Technological University",
                "The University of Manchester",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05237.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#reasoning",
                    "#dataset",
                    "#training"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение рассуждений мультимодальных ИИ через обучение на масштабном наборе данных с промежуточными выводами",
                    "desc": "Статья представляет новый метод создания масштабного набора данных для обучения мультимодальных языковых моделей с открытым исходным кодом. В отличие от существующих наборов данных, новый подход включает подробные промежуточные рассуждения, что способствует развитию навыков рассуждения у моделей. Авторы создали набор данных из 12 миллионов пар инструкций и ответов, охватывающих разнообразные задачи, требующие интенсивных рассуждений. Эксперименты показали значительное улучшение способностей моделей к рассуждению, достигая лучших результатов на нескольких бенчмарках."
                },
                "en": {
                    "title": "Enhancing Reasoning in MLLMs with Rich Rationales",
                    "desc": "This paper presents a new method for creating a large-scale multimodal instruction-tuning dataset aimed at improving the reasoning abilities of multimodal large language models (MLLMs). The authors identify that existing datasets are limited as they mainly provide simple answers without detailed reasoning. To overcome this, they developed a dataset with 12 million instruction-response pairs that include rich intermediate rationales, promoting chain-of-thought (CoT) reasoning. Experiments show that training on this dataset enhances MLLMs' reasoning performance significantly, achieving state-of-the-art results on various benchmarks."
                },
                "zh": {
                    "title": "提升多模态模型推理能力的新方法",
                    "desc": "这篇论文介绍了一种构建大规模多模态指令调优数据集的方法，以提高多模态大语言模型（MLLMs）的推理能力。现有的数据集主要来自学术研究，任务简单，缺乏中间推理过程的详细解释。我们的方法创建了一个包含1200万对指令和响应的数据集，涵盖了多样化且需要推理的任务，并提供了丰富的推理依据。实验结果表明，使用该数据集训练的MLLMs在多个基准测试中显著提高了推理能力，达到了最先进的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04862",
            "title": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases",
            "url": "https://huggingface.co/papers/2412.04862",
            "abstract": "This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from https://huggingface.co/LGAI-EXAONE. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.",
            "score": 23,
            "issue_id": 1011,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "83e6f957e42ebb5e",
            "authors": [
                "LG AI Research",
                "Soyoung An",
                "Kyunghoon Bae",
                "Eunbi Choi",
                "Kibong Choi",
                "Stanley Jungkyu Choi",
                "Seokhee Hong",
                "Junwon Hwang",
                "Hyojin Jeon",
                "Gerrard Jeongwon Jo",
                "Hyunjik Jo",
                "Jiyeon Jung",
                "Yountae Jung",
                "Hyosang Kim",
                "Joonkee Kim",
                "Seonghwan Kim",
                "Soyeon Kim",
                "Sunkyoung Kim",
                "Yireun Kim",
                "Yongil Kim",
                "Youchul Kim",
                "Edward Hwayoung Lee",
                "Haeju Lee",
                "Honglak Lee",
                "Jinsik Lee",
                "Kyungmin Lee",
                "Woohyung Lim",
                "Sangha Park",
                "Sooyoun Park",
                "Yongmin Park",
                "Sihoon Yang",
                "Heuiyeen Yeen",
                "Hyeongu Yun"
            ],
            "affiliations": [
                "LG AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04862.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#open_source",
                    "#small_models",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "EXAONE 3.5: Новое слово в языковых моделях от LG AI Research",
                    "desc": "Компания LG AI Research представила языковые модели EXAONE 3.5, обученные выполнению инструкций. Модели доступны в трех конфигурациях: 32B, 7.8B и 2.4B параметров. Они демонстрируют исключительные способности в следовании инструкциям, понимании длинного контекста и показывают конкурентоспособные результаты по сравнению с современными открытыми моделями аналогичного размера. Модели EXAONE 3.5 доступны для исследовательских целей и могут быть загружены с платформы Hugging Face."
                },
                "en": {
                    "title": "EXAONE 3.5: Leading the Way in Instruction-Tuned Language Models",
                    "desc": "The EXAONE 3.5 language models, created by LG AI Research, are advanced instruction-tuned models available in three sizes: 32B, 7.8B, and 2.4B. They excel in following instructions in real-world applications, achieving top scores on seven different benchmarks. Additionally, these models demonstrate superior long-context understanding, ranking first in four benchmarks. They are accessible for research purposes and show competitive performance against leading open models of similar sizes across nine general benchmarks."
                },
                "zh": {
                    "title": "EXAONE 3.5：指令跟随与长上下文理解的先锋",
                    "desc": "EXAONE 3.5 是由 LG AI 研究所开发的指令调优语言模型，提供三种配置：32B、7.8B 和 2.4B。这些模型在实际场景中具有卓越的指令跟随能力，在七个基准测试中取得了最高分。它们在长上下文理解方面表现出色，在四个基准测试中也达到了最佳性能。此外，与同类开源模型相比，EXAONE 3.5 在九个通用基准测试中也展现了竞争力的结果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04445",
            "title": "Moto: Latent Motion Token as the Bridging Language for Robot Manipulation",
            "url": "https://huggingface.co/papers/2412.04445",
            "abstract": "Recent developments in Large Language Models pre-trained on extensive corpora have shown significant success in various natural language processing tasks with minimal fine-tuning. This success offers new promise for robotics, which has long been constrained by the high cost of action-labeled data. We ask: given the abundant video data containing interaction-related knowledge available as a rich \"corpus\", can a similar generative pre-training approach be effectively applied to enhance robot learning? The key challenge is to identify an effective representation for autoregressive pre-training that benefits robot manipulation tasks. Inspired by the way humans learn new skills through observing dynamic environments, we propose that effective robotic learning should emphasize motion-related knowledge, which is closely tied to low-level actions and is hardware-agnostic, facilitating the transfer of learned motions to actual robot actions. To this end, we introduce Moto, which converts video content into latent Motion Token sequences by a Latent Motion Tokenizer, learning a bridging \"language\" of motion from videos in an unsupervised manner. We pre-train Moto-GPT through motion token autoregression, enabling it to capture diverse visual motion knowledge. After pre-training, Moto-GPT demonstrates the promising ability to produce semantically interpretable motion tokens, predict plausible motion trajectories, and assess trajectory rationality through output likelihood. To transfer learned motion priors to real robot actions, we implement a co-fine-tuning strategy that seamlessly bridges latent motion token prediction and real robot control. Extensive experiments show that the fine-tuned Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks, underscoring its effectiveness in transferring knowledge from video data to downstream visual manipulation tasks.",
            "score": 18,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "a5ac6d786500ef9f",
            "authors": [
                "Yi Chen",
                "Yuying Ge",
                "Yizhuo Li",
                "Yixiao Ge",
                "Mingyu Ding",
                "Ying Shan",
                "Xihui Liu"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "The University of Hong Kong",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04445.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#transfer_learning",
                    "#games",
                    "#robotics",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Обучение роботов движению через предобучение на видеоданных",
                    "desc": "В статье представлен новый подход к обучению роботов на основе предобученных языковых моделей. Авторы предлагают метод Moto, который преобразует видеоконтент в последовательности латентных токенов движения с помощью Latent Motion Tokenizer. Moto-GPT обучается на основе авторегрессии токенов движения, что позволяет ей захватывать разнообразные знания о визуальном движении. После предобучения и дообучения Moto-GPT демонстрирует высокую эффективность в задачах манипуляции роботов."
                },
                "en": {
                    "title": "Bridging Video Knowledge to Robot Actions with Moto-GPT",
                    "desc": "This paper explores the application of Large Language Models to enhance robot learning by leveraging abundant video data. It introduces Moto, a system that converts video content into Motion Token sequences, allowing robots to learn motion-related knowledge in an unsupervised manner. The approach emphasizes the importance of motion understanding for effective robotic manipulation, enabling the transfer of learned skills to real-world actions. Through extensive experiments, Moto-GPT demonstrates improved performance in robot manipulation tasks, showcasing its ability to bridge the gap between video knowledge and practical robot control."
                },
                "zh": {
                    "title": "利用视频数据提升机器人学习能力",
                    "desc": "这篇论文探讨了如何利用大规模视频数据来提升机器人学习能力。研究者提出了一种名为Moto的模型，通过将视频内容转换为潜在的运动标记序列，来学习运动知识。Moto-GPT经过预训练后，能够生成可解释的运动标记，并预测合理的运动轨迹。最终，研究表明，经过微调的Moto-GPT在机器人操作基准测试中表现出色，证明了从视频数据转移知识的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.05270",
            "title": "APOLLO: SGD-like Memory, AdamW-level Performance",
            "url": "https://huggingface.co/papers/2412.05270",
            "abstract": "Large language models (LLMs) are notoriously memory-intensive during training, particularly with the popular AdamW optimizer. This memory burden necessitates using more or higher-end GPUs or reducing batch sizes, limiting training scalability and throughput. To address this, various memory-efficient optimizers have been proposed to reduce optimizer memory usage. However, they face critical challenges: (i) reliance on costly SVD operations; (ii) significant performance trade-offs compared to AdamW; and (iii) still substantial optimizer memory overhead to maintain competitive performance.   In this work, we identify that AdamW's learning rate adaptation rule can be effectively coarsened as a structured learning rate update. Based on this insight, we propose Approximated Gradient Scaling for Memory-Efficient LLM Optimization (APOLLO), which approximates learning rate scaling using an auxiliary low-rank optimizer state based on pure random projection. This structured learning rate update rule makes APOLLO highly tolerant to further memory reductions while delivering comparable pre-training performance. Even its rank-1 variant, APOLLO-Mini, achieves superior pre-training performance compared to AdamW with SGD-level memory costs.   Extensive experiments demonstrate that the APOLLO series performs on-par with or better than AdamW, while achieving greater memory savings by nearly eliminating the optimization states of AdamW. These savings provide significant system-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB setup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model Scalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without system-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training LLaMA-7B on a single GPU using less than 12 GB of memory with weight quantization.",
            "score": 15,
            "issue_id": 1012,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "14bb480f5fe29bae",
            "authors": [
                "Hanqing Zhu",
                "Zhenyu Zhang",
                "Wenyan Cong",
                "Xi Liu",
                "Sem Park",
                "Vikas Chandra",
                "Bo Long",
                "David Z. Pan",
                "Zhangyang Wang",
                "Jinwon Lee"
            ],
            "affiliations": [
                "Department of Electrical and Computer Engineering, The University of Texas at Austin",
                "Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05270.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "APOLLO: эффективное обучение LLM с минимальными затратами памяти",
                    "desc": "Статья представляет новый оптимизатор APOLLO для обучения больших языковых моделей (LLM). APOLLO использует структурированное обновление скорости обучения, основанное на случайном проецировании, что позволяет значительно снизить потребление памяти по сравнению с популярным оптимизатором AdamW. Эксперименты показывают, что APOLLO достигает сопоставимой или лучшей производительности, чем AdamW, при существенной экономии памяти. Это позволяет увеличить пропускную способность, масштабируемость модели и делает возможным обучение LLM даже на GPU среднего уровня."
                },
                "en": {
                    "title": "APOLLO: Memory-Efficient Optimization for Large Language Models",
                    "desc": "This paper addresses the high memory requirements of training large language models (LLMs) using the AdamW optimizer. It introduces a new method called APOLLO, which simplifies the learning rate adaptation process to reduce memory usage while maintaining performance. APOLLO utilizes a low-rank optimizer state based on random projection, allowing for significant memory savings without sacrificing training efficiency. The results show that APOLLO can achieve comparable or better performance than AdamW, enabling larger batch sizes and making LLM training feasible on lower-end GPUs."
                },
                "zh": {
                    "title": "APOLLO：高效内存优化的未来",
                    "desc": "大型语言模型（LLMs）在训练过程中对内存的需求非常高，尤其是使用流行的AdamW优化器时。为了解决这个问题，研究者们提出了多种内存高效的优化器，但它们面临着依赖昂贵的SVD操作和性能折衷等挑战。本文提出了一种名为APOLLO的优化方法，通过近似学习率缩放来减少内存使用，同时保持与AdamW相当的预训练性能。实验结果表明，APOLLO系列在内存节省方面表现优异，能够在较低的内存成本下实现更高的训练吞吐量和模型可扩展性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04301",
            "title": "SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion",
            "url": "https://huggingface.co/papers/2412.04301",
            "abstract": "Recent advances in text-guided image editing enable users to perform image edits through simple text inputs, leveraging the extensive priors of multi-step diffusion-based text-to-image models. However, these methods often fall short of the speed demands required for real-world and on-device applications due to the costly multi-step inversion and sampling process involved. In response to this, we introduce SwiftEdit, a simple yet highly efficient editing tool that achieve instant text-guided image editing (in 0.23s). The advancement of SwiftEdit lies in its two novel contributions: a one-step inversion framework that enables one-step image reconstruction via inversion and a mask-guided editing technique with our proposed attention rescaling mechanism to perform localized image editing. Extensive experiments are provided to demonstrate the effectiveness and efficiency of SwiftEdit. In particular, SwiftEdit enables instant text-guided image editing, which is extremely faster than previous multi-step methods (at least 50 times faster) while maintain a competitive performance in editing results. Our project page is at: https://swift-edit.github.io/",
            "score": 13,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "a4cea89a59a9a3c0",
            "authors": [
                "Trong-Tung Nguyen",
                "Quang Nguyen",
                "Khoi Nguyen",
                "Anh Tran",
                "Cuong Pham"
            ],
            "affiliations": [
                "Posts & Telecom. Inst. of Tech., Vietnam",
                "VinAI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04301.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Мгновенное редактирование изображений текстом",
                    "desc": "SwiftEdit - это новый инструмент для быстрого редактирования изображений с помощью текстовых запросов. В отличие от существующих многошаговых методов, SwiftEdit использует одношаговую инверсию и механизм масштабирования внимания для локального редактирования. Это позволяет выполнять изменения изображений практически мгновенно - за 0.23 секунды. SwiftEdit работает как минимум в 50 раз быстрее аналогов, сохраняя при этом высокое качество результатов."
                },
                "en": {
                    "title": "SwiftEdit: Instant Text-Guided Image Editing at Lightning Speed!",
                    "desc": "This paper presents SwiftEdit, a new tool for fast text-guided image editing that significantly improves the speed of image modifications. Traditional methods rely on multi-step diffusion processes, which can be slow and inefficient for real-time applications. SwiftEdit introduces a one-step inversion framework that allows for quick image reconstruction and a mask-guided editing technique that uses attention rescaling for precise edits. The results show that SwiftEdit is at least 50 times faster than previous methods while still delivering high-quality editing outcomes."
                },
                "zh": {
                    "title": "SwiftEdit：瞬时文本引导图像编辑的革命",
                    "desc": "最近的文本引导图像编辑技术使用户能够通过简单的文本输入进行图像编辑，利用了多步扩散模型的丰富先验知识。然而，这些方法在实际应用中往往速度较慢，无法满足实时和设备端的需求。为此，我们提出了SwiftEdit，这是一种简单而高效的编辑工具，可以实现瞬时的文本引导图像编辑，速度达到0.23秒。SwiftEdit的创新在于其一体化的反演框架和基于掩码的编辑技术，能够快速进行局部图像编辑，同时保持与传统多步方法相当的编辑效果。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04440",
            "title": "GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration",
            "url": "https://huggingface.co/papers/2412.04440",
            "abstract": "Text-to-video generation models have shown significant progress in the recent years. However, they still struggle with generating complex dynamic scenes based on compositional text prompts, such as attribute binding for multiple objects, temporal dynamics associated with different objects, and interactions between objects. Our key motivation is that complex tasks can be decomposed into simpler ones, each handled by a role-specialized MLLM agent. Multiple agents can collaborate together to achieve collective intelligence for complex goals. We propose GenMAC, an iterative, multi-agent framework that enables compositional text-to-video generation. The collaborative workflow includes three stages: Design, Generation, and Redesign, with an iterative loop between the Generation and Redesign stages to progressively verify and refine the generated videos. The Redesign stage is the most challenging stage that aims to verify the generated videos, suggest corrections, and redesign the text prompts, frame-wise layouts, and guidance scales for the next iteration of generation. To avoid hallucination of a single MLLM agent, we decompose this stage to four sequentially-executed MLLM-based agents: verification agent, suggestion agent, correction agent, and output structuring agent. Furthermore, to tackle diverse scenarios of compositional text-to-video generation, we design a self-routing mechanism to adaptively select the proper correction agent from a collection of correction agents each specialized for one scenario. Extensive experiments demonstrate the effectiveness of GenMAC, achieving state-of-the art performance in compositional text-to-video generation.",
            "score": 12,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 декабря",
                "en": "December 5",
                "zh": "12月5日"
            },
            "hash": "28dc2191ba71c4ea",
            "authors": [
                "Kaiyi Huang",
                "Yukun Huang",
                "Xuefei Ning",
                "Zinan Lin",
                "Yu Wang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Microsoft Research",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04440.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#agents",
                    "#games",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Коллективный интеллект агентов для создания сложных видео по тексту",
                    "desc": "Статья представляет GenMAC - итеративную мультиагентную систему для генерации видео по текстовому описанию. Система использует несколько специализированных агентов на основе мультимодальных языковых моделей (MLLM) для декомпозиции сложных задач. Процесс включает этапы проектирования, генерации и перепроектирования, с итеративным циклом между генерацией и перепроектированием для постепенного улучшения результата. GenMAC демонстрирует передовые результаты в генерации композиционных видео по текстовому описанию."
                },
                "en": {
                    "title": "Collaborative Intelligence for Text-to-Video Mastery",
                    "desc": "This paper introduces GenMAC, a multi-agent framework designed to improve text-to-video generation by breaking down complex tasks into simpler components. It utilizes specialized machine learning agents that collaborate through three main stages: Design, Generation, and Redesign, with an iterative process to refine video outputs. The Redesign stage is particularly crucial as it involves multiple agents that verify, suggest corrections, and restructure prompts to enhance the generated videos. The framework also includes a self-routing mechanism to select the most suitable correction agent for various scenarios, leading to significant advancements in generating dynamic scenes from text prompts."
                },
                "zh": {
                    "title": "GenMAC：协作生成复杂视频的智能框架",
                    "desc": "文本到视频生成模型近年来取得了显著进展，但在生成复杂动态场景时仍面临挑战。我们提出了一种名为GenMAC的多代理框架，通过将复杂任务分解为简单任务来实现协作生成。该框架包括设计、生成和重新设计三个阶段，生成和重新设计之间存在迭代循环，以逐步验证和优化生成的视频。通过自适应选择专门化的修正代理，GenMAC能够有效应对多样化的文本到视频生成场景。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.05263",
            "title": "Mind the Time: Temporally-Controlled Multi-Event Video Generation",
            "url": "https://huggingface.co/papers/2412.05263",
            "abstract": "Real-world videos consist of sequences of events. Generating such sequences with precise temporal control is infeasible with existing video generators that rely on a single paragraph of text as input. When tasked with generating multiple events described using a single prompt, such methods often ignore some of the events or fail to arrange them in the correct order. To address this limitation, we present MinT, a multi-event video generator with temporal control. Our key insight is to bind each event to a specific period in the generated video, which allows the model to focus on one event at a time. To enable time-aware interactions between event captions and video tokens, we design a time-based positional encoding method, dubbed ReRoPE. This encoding helps to guide the cross-attention operation. By fine-tuning a pre-trained video diffusion transformer on temporally grounded data, our approach produces coherent videos with smoothly connected events. For the first time in the literature, our model offers control over the timing of events in generated videos. Extensive experiments demonstrate that MinT outperforms existing open-source models by a large margin.",
            "score": 6,
            "issue_id": 1014,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "826bb588770d5c27",
            "authors": [
                "Ziyi Wu",
                "Aliaksandr Siarohin",
                "Willi Menapace",
                "Ivan Skorokhodov",
                "Yuwei Fang",
                "Varnith Chordia",
                "Igor Gilitschenski",
                "Sergey Tulyakov"
            ],
            "affiliations": [
                "Snap Research",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05263.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#architecture",
                    "#diffusion",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "⏱️",
                "ru": {
                    "title": "Точный контроль времени в генерации видео с несколькими событиями",
                    "desc": "Статья представляет MinT - генератор видео с множеством событий и временным контролем. Метод привязывает каждое событие к определенному периоду в генерируемом видео, что позволяет модели фокусироваться на одном событии за раз. Авторы разработали метод временного позиционного кодирования ReRoPE для управления взаимодействием между описаниями событий и видеотокенами. Модель, дообученная на темпорально размеченных данных, создает согласованные видео с плавно соединенными событиями, превосходя существующие открытые модели."
                },
                "en": {
                    "title": "MinT: Mastering Multi-Event Video Generation with Temporal Precision",
                    "desc": "This paper introduces MinT, a novel multi-event video generator that allows for precise temporal control over the events depicted in generated videos. Unlike traditional models that struggle with sequencing multiple events from a single text prompt, MinT binds each event to a specific time period, ensuring that all events are accurately represented and ordered. The authors implement a time-based positional encoding method called ReRoPE, which enhances the model's ability to manage interactions between event descriptions and video frames. By fine-tuning a pre-trained video diffusion transformer on data with temporal grounding, MinT achieves superior performance in generating coherent videos with well-timed events."
                },
                "zh": {
                    "title": "MinT：精准控制视频事件时序的生成器",
                    "desc": "本论文提出了一种名为MinT的多事件视频生成器，旨在解决现有视频生成器在生成多个事件时的时间控制问题。通过将每个事件绑定到生成视频的特定时间段，MinT能够逐个关注事件，从而提高生成视频的连贯性。我们设计了一种基于时间的位置信息编码方法ReRoPE，以增强事件描述与视频帧之间的交互。实验结果表明，MinT在生成视频的时间控制方面优于现有的开源模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03428",
            "title": "2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction",
            "url": "https://huggingface.co/papers/2412.03428",
            "abstract": "The reconstruction of indoor scenes remains challenging due to the inherent complexity of spatial structures and the prevalence of textureless regions. Recent advancements in 3D Gaussian Splatting have improved novel view synthesis with accelerated processing but have yet to deliver comparable performance in surface reconstruction. In this paper, we introduce 2DGS-Room, a novel method leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction. Specifically, we employ a seed-guided mechanism to control the distribution of 2D Gaussians, with the density of seed points dynamically optimized through adaptive growth and pruning mechanisms. To further improve geometric accuracy, we incorporate monocular depth and normal priors to provide constraints for details and textureless regions respectively. Additionally, multi-view consistency constraints are employed to mitigate artifacts and further enhance reconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasets demonstrate that our method achieves state-of-the-art performance in indoor scene reconstruction.",
            "score": 5,
            "issue_id": 1013,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 декабря",
                "en": "December 4",
                "zh": "12月4日"
            },
            "hash": "ead3f67b9be4d52b",
            "authors": [
                "Wanting Zhang",
                "Haodong Xiang",
                "Zhichao Liao",
                "Xiansong Lai",
                "Xinghui Li",
                "Long Zeng"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03428.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "🏠",
                "ru": {
                    "title": "2DGS-Room: Революция в реконструкции интерьеров с помощью 2D гауссовского сплаттинга",
                    "desc": "Эта статья представляет новый метод 2DGS-Room для реконструкции интерьеров с использованием 2D гауссовского сплаттинга. Авторы применяют механизм управления распределением 2D гауссианов с помощью семян, оптимизируя их плотность через адаптивный рост и отсечение. Для улучшения геометрической точности используются монокулярные глубинные и нормальные приоры, а также ограничения многоракурсной согласованности. Эксперименты на наборах данных ScanNet и ScanNet++ показывают, что метод достигает наилучших результатов в реконструкции интерьеров."
                },
                "en": {
                    "title": "Revolutionizing Indoor Scene Reconstruction with 2D Gaussian Splatting",
                    "desc": "This paper presents 2DGS-Room, a new approach for reconstructing indoor scenes using 2D Gaussian Splatting. The method introduces a seed-guided mechanism that optimizes the distribution of 2D Gaussians, enhancing the reconstruction process. By incorporating monocular depth and normal priors, the approach improves geometric accuracy, especially in areas lacking texture. The use of multi-view consistency constraints helps reduce artifacts, leading to high-fidelity results in indoor scene reconstruction, as demonstrated by experiments on ScanNet and ScanNet++ datasets."
                },
                "zh": {
                    "title": "高保真室内场景重建的新方法",
                    "desc": "室内场景的重建因空间结构复杂和无纹理区域的普遍存在而具有挑战性。本文提出了一种新方法2DGS-Room，利用2D高斯点云实现高保真度的室内场景重建。我们采用种子引导机制来控制2D高斯的分布，并通过自适应生长和修剪机制动态优化种子点的密度。通过结合单目深度和法线先验，我们进一步提高了几何精度，并使用多视图一致性约束来减少伪影，提升重建质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04905",
            "title": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling",
            "url": "https://huggingface.co/papers/2412.04905",
            "abstract": "Large language models (LLMs) have made dialogue one of the central modes of human-machine interaction, leading to the accumulation of vast amounts of conversation logs and increasing demand for dialogue generation. A conversational life-cycle spans from the Prelude through the Interlocution to the Epilogue, encompassing various elements. Despite the existence of numerous dialogue-related studies, there is a lack of benchmarks that encompass comprehensive dialogue elements, hindering precise modeling and systematic evaluation. To bridge this gap, we introduce an innovative research task Dialogue Element MOdeling, including Element Awareness and Dialogue Agent Interaction, and propose a novel benchmark, DEMO, designed for a comprehensive dialogue modeling and assessment. Inspired by imitation learning, we further build the agent which possesses the adept ability to model dialogue elements based on the DEMO benchmark. Extensive experiments indicate that existing LLMs still exhibit considerable potential for enhancement, and our DEMO agent has superior performance in both in-domain and out-of-domain tasks.",
            "score": 4,
            "issue_id": 1017,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 декабря",
                "en": "December 6",
                "zh": "12月6日"
            },
            "hash": "4b12d357252b671f",
            "authors": [
                "Minzheng Wang",
                "Xinghua Zhang",
                "Kun Chen",
                "Nan Xu",
                "Haiyang Yu",
                "Fei Huang",
                "Wenji Mao",
                "Yongbin Li"
            ],
            "affiliations": [
                "MAIS, Institute of Automation, Chinese Academy of Sciences",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04905.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dialogue_generation",
                    "#agents"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "DEMO: Новый стандарт для комплексной оценки диалоговых систем",
                    "desc": "Эта статья представляет новую задачу исследования под названием 'Моделирование элементов диалога' и соответствующий бенчмарк DEMO. Авторы отмечают нехватку комплексных бенчмарков для оценки диалоговых систем, охватывающих все элементы разговора. Используя принципы имитационного обучения, исследователи создали агента на основе DEMO, способного эффективно моделировать элементы диалога. Эксперименты показали, что существующие языковые модели (LLM) все еще имеют значительный потенциал для улучшения в этой области."
                },
                "en": {
                    "title": "Enhancing Dialogue Generation with Comprehensive Element Modeling",
                    "desc": "This paper addresses the growing need for effective dialogue generation in human-machine interactions, particularly with large language models (LLMs). It identifies a gap in existing research due to the lack of comprehensive benchmarks that cover all aspects of dialogue, which hampers accurate modeling and evaluation. To tackle this issue, the authors introduce a new task called Dialogue Element MOdeling (DEMO), which focuses on understanding dialogue elements and how agents interact within conversations. Their experiments show that while current LLMs have room for improvement, the proposed DEMO agent outperforms them in various tasks, demonstrating its effectiveness in dialogue modeling."
                },
                "zh": {
                    "title": "全面对话建模的新基准DEMO",
                    "desc": "大型语言模型（LLMs）使对话成为人机交互的主要方式，导致大量对话日志的积累和对对话生成的需求增加。对话的生命周期包括前奏、对话和结尾，涵盖了多个元素。尽管已有许多与对话相关的研究，但缺乏全面的基准，限制了精确建模和系统评估。为了解决这个问题，我们提出了对话元素建模的新任务，并设计了一个新的基准DEMO，以便进行全面的对话建模和评估。"
                }
            }
        }
    ],
    "link_prev": "2024-12-06.html",
    "link_next": "2024-12-10.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "06.12",
        "en": "12/06",
        "zh": "12月6日"
    },
    "short_date_next": {
        "ru": "10.12",
        "en": "12/10",
        "zh": "12月10日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 2,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0,
        "#dialogue_generation": 1
    },
    "zh": {
        "text": "我们介绍了 InternVL 2.5，这是一个基于 InternVL 2.0 的先进多模态大语言模型系列。它保留了核心模型架构，但在训练和测试策略以及数据质量方面进行了显著改进。我们研究了模型扩展与性能之间的关系，系统地探讨了视觉编码器、语言模型、数据集大小和测试时配置的性能趋势。通过广泛的评估，InternVL 2.5 在多个基准测试中表现出色，包括多学科推理、文档理解、多图像/视频理解、实际理解、多模态幻觉检测、视觉定位、多语言能力和纯语言处理。我们的模型是第一个在 MMMU 基准测试中超过 70% 的开源多模态大语言模型，并展示了强大的测试时扩展潜力。我们希望这个模型能为开源社区贡献新的标准，用于开发和应用多模态人工智能系统。HuggingFace 演示见 https://huggingface.co/spaces/OpenGVLab/InternVL。",
        "title": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
        "pinyin": "Wǒmen jièshào le InternVL 2.5, zhè shì yīgè jīyú InternVL 2.0 de xiānjìn duō móshì dà yǔyán móxíng xìliè. Tā bǎoliúle héxīn móxíng jiàgòu, dàn zài xùnliàn hé cèshì cèlüè yǐjiǎ shùjù zhìliàng fāngmiàn jìnxíng le xiǎnzhù gǎijìn. Wǒmen yánjiū le móxíng kuòzhǎn yǔ xìngnéng zhījiān de guānxì, xìtǒng de tàntào le shìjuān biānmǎqì, yǔyán móxíng, shùjùjí dàxìng hé cèshì shí pèizhì de xìngnéng qūshì. Tōngguò guǎngfàn de pínggū, InternVL 2.5 zài duōgè jīzhǔn cèshì zhōng biǎoxiàn chūsè, bāokuò duō xuékē tuīlǐ, wénjiàn lǐjiě, duō túxiàng/shìpín lǐjiě, shíjì lǐjiě, duō móshì huànjué jiǎncè, shìjuān dìngwèi, duō yǔyán nénglì hé chún yǔyán chǔlǐ. Wǒmen de móxíng shì dì-yīgè zài MMMU jīzhǔn cèshì zhōng chāoguò 70% de kāiyuán duō móshì dà yǔyán móxíng, bìng zhǎnshì le qiángdà de cèshì shí kuòzhǎn qiánlì. Wǒmen xīwàng zhègè móxíng néng wèi kāiyuán shèqū gòngxiàn xīn de biāozhǔn, yòngyú kāifā hé yìngyòng duō móshì réngōng zhìnéng xìtǒng. HuggingFace yǎnshì jiàn https://huggingface.co/spaces/OpenGVLab/InternVL.",
        "vocab": "[\n    {\"word\": \"介绍\", \"pinyin\": \"jiè shào\", \"trans\": \"introduce\"},\n    {\"word\": \"先进\", \"pinyin\": \"xiān jìn\", \"trans\": \"advanced\"},\n    {\"word\": \"多模态\", \"pinyin\": \"duō mó tài\", \"trans\": \"multimodal\"},\n    {\"word\": \"保留\", \"pinyin\": \"bǎo liú\", \"trans\": \"retain\"},\n    {\"word\": \"核心\", \"pinyin\": \"hé xīn\", \"trans\": \"core\"},\n    {\"word\": \"架构\", \"pinyin\": \"jià gòu\", \"trans\": \"architecture\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"改进\", \"pinyin\": \"gǎi jìn\", \"trans\": \"improvement\"},\n    {\"word\": \"扩展\", \"pinyin\": \"kuò zhǎn\", \"trans\": \"expand\"},\n    {\"word\": \"性能\", \"pinyin\": \"xìng néng\", \"trans\": \"performance\"},\n    {\"word\": \"系统\", \"pinyin\": \"xì tǒng\", \"trans\": \"system\"},\n    {\"word\": \"探讨\", \"pinyin\": \"tàn tǎo\", \"trans\": \"explore\"},\n    {\"word\": \"视觉\", \"pinyin\": \"shì jué\", \"trans\": \"visual\"},\n    {\"word\": \"编码器\", \"pinyin\": \"biān mǎ qì\", \"trans\": \"encoder\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shù jù jí\", \"trans\": \"dataset\"},\n    {\"word\": \"配置\", \"pinyin\": \"pèi zhì\", \"trans\": \"configuration\"},\n    {\"word\": \"趋势\", \"pinyin\": \"qū shì\", \"trans\": \"trend\"},\n    {\"word\": \"评估\", \"pinyin\": \"píng gū\", \"trans\": \"evaluate\"},\n    {\"word\": \"基准\", \"pinyin\": \"jī zhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"表现\", \"pinyin\": \"biǎo xiàn\", \"trans\": \"performance\"},\n    {\"word\": \"出色\", \"pinyin\": \"chū sè\", \"trans\": \"outstanding\"},\n    {\"word\": \"多学科\", \"pinyin\": \"duō xué kē\", \"trans\": \"multidisciplinary\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"reasoning\"},\n    {\"word\": \"文档\", \"pinyin\": \"wén dàng\", \"trans\": \"document\"},\n    {\"word\": \"理解\", \"pinyin\": \"lǐ jiě\", \"trans\": \"understanding\"},\n    {\"word\": \"多图像\", \"pinyin\": \"duō tú xiàng\", \"trans\": \"multi-image\"},\n    {\"word\": \"视频\", \"pinyin\": \"shì pìn\", \"trans\": \"video\"},\n    {\"word\": \"实际\", \"pinyin\": \"shí jì\", \"trans\": \"practical\"},\n    {\"word\": \"幻觉\", \"pinyin\": \"huàn jué\", \"trans\": \"hallucination\"},\n    {\"word\": \"检测\", \"pinyin\": \"jiǎn cè\", \"trans\": \"detection\"},\n    {\"word\": \"定位\", \"pinyin\": \"dìng wèi\", \"trans\": \"localization\"},\n    {\"word\": \"多语言\", \"pinyin\": \"duō yǔ yán\", \"trans\": \"multilingual\"},\n    {\"word\": \"能力\", \"pinyin\": \"néng lì\", \"trans\": \"ability\"},\n    {\"word\": \"纯语言\", \"pinyin\": \"chún yǔ yán\", \"trans\": \"pure language\"},\n    {\"word\": \"处理\", \"pinyin\": \"chǔ lǐ\", \"trans\": \"process\"},\n    {\"word\": \"开源\", \"pinyin\": \"kāi yuán\", \"trans\": \"open-source\"},\n    {\"word\": \"社区\", \"pinyin\": \"shè qū\", \"trans\": \"community\"},\n    {\"word\": \"贡献\", \"pinyin\": \"gòng xiàn\", \"trans\": \"contribute\"},\n    {\"word\": \"标准\", \"pinyin\": \"biāo zhǔn\", \"trans\": \"standard\"},\n    {\"word\": \"开发\", \"pinyin\": \"kāi fā\", \"trans\": \"develop\"},\n    {\"word\": \"应用\", \"pinyin\": \"yìng yòng\", \"trans\": \"apply\"},\n    {\"word\": \"人工智能\", \"pinyin\": \"rén gōng zhì néng\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"系统\", \"pinyin\": \"xì tǒng\", \"trans\": \"system\"},\n    {\"word\": \"演示\", \"pinyin\": \"yǎn shì\", \"trans\": \"demonstration\"},\n    {\"word\": \"见\", \"pinyin\": \"jiàn\", \"trans\": \"see\"}\n]",
        "trans": "We introduced InternVL 2.5, an advanced multimodal large language model series based on InternVL 2.0. It retains the core model architecture but features significant improvements in training and testing strategies, as well as data quality. We studied the relationship between model scaling and performance, systematically exploring performance trends in visual encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations, InternVL 2.5 has demonstrated outstanding performance across multiple benchmarks, including multidisciplinary reasoning, document understanding, multi-image/video understanding, practical understanding, multimodal hallucination detection, visual localization, multilingual capabilities, and pure language processing. Our model is the first open-source multimodal large language model to exceed 70% on the MMMU benchmark and has shown strong test-time scaling potential. We hope this model will contribute new standards to the open-source community for developing and applying multimodal AI systems. See the HuggingFace demo at https://huggingface.co/spaces/OpenGVLab/InternVL.",
        "update_ts": "2024-12-09 09:12"
    }
}