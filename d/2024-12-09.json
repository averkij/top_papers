{
    "date": {
        "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 9",
        "zh": "12æœˆ9æ—¥"
    },
    "time_utc": "2024-12-09 14:10",
    "weekday": 0,
    "issue_id": 1023,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2412.05271",
            "title": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
            "url": "https://huggingface.co/papers/2412.05271",
            "abstract": "We introduce InternVL 2.5, an advanced multimodal large language model (MLLM) series that builds upon InternVL 2.0, maintaining its core model architecture while introducing significant enhancements in training and testing strategies as well as data quality. In this work, we delve into the relationship between model scaling and performance, systematically exploring the performance trends in vision encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations on a wide range of benchmarks, including multi-discipline reasoning, document understanding, multi-image / video understanding, real-world comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processing, InternVL 2.5 exhibits competitive performance, rivaling leading commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a 3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing strong potential for test-time scaling. We hope this model contributes to the open-source community by setting new standards for developing and applying multimodal AI systems. HuggingFace demo see https://huggingface.co/spaces/OpenGVLab/InternVL",
            "score": 45,
            "issue_id": 1013,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 6",
                "zh": "12æœˆ6æ—¥"
            },
            "hash": "81590cc90bda9173",
            "authors": [
                "Zhe Chen",
                "Weiyun Wang",
                "Yue Cao",
                "Yangzhou Liu",
                "Zhangwei Gao",
                "Erfei Cui",
                "Jinguo Zhu",
                "Shenglong Ye",
                "Hao Tian",
                "Zhaoyang Liu",
                "Lixin Gu",
                "Xuehui Wang",
                "Qingyun Li",
                "Yimin Ren",
                "Zixuan Chen",
                "Jiapeng Luo",
                "Jiahao Wang",
                "Tan Jiang",
                "Bo Wang",
                "Conghui He",
                "Botian Shi",
                "Xingcheng Zhang",
                "Han Lv",
                "Yi Wang",
                "Wenqi Shao",
                "Pei Chu",
                "Zhongying Tu",
                "Tong He",
                "Zhiyong Wu",
                "Huipeng Deng",
                "Jiaye Ge",
                "Kai Chen",
                "Min Dou",
                "Lewei Lu",
                "Xizhou Zhu",
                "Tong Lu",
                "Dahua Lin",
                "Yu Qiao",
                "Jifeng Dai",
                "Wenhai Wang"
            ],
            "affiliations": [
                "Fudan University",
                "Nanjing University",
                "SenseTime Research",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05271.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#hallucinations",
                    "#reasoning",
                    "#training",
                    "#multimodal",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜: InternVL 2.5 ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹",
                    "desc": "InternVL 2.5 - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM), Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ InternVL 2.0. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞµÑ‘ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ñ…, ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². InternVL 2.5 ÑÑ‚Ğ°Ğ»Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ MLLM, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²ÑˆĞµĞ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ² 70% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMMU, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "InternVL 2.5: Setting New Standards in Multimodal AI",
                    "desc": "InternVL 2.5 is a state-of-the-art multimodal large language model that enhances its predecessor, InternVL 2.0, by improving training methods and data quality. The paper investigates how increasing the model size affects its performance across various tasks, including reasoning, document understanding, and multimodal comprehension. Extensive testing shows that InternVL 2.5 competes effectively with top commercial models, achieving significant benchmarks like surpassing 70% on the MMMU benchmark. This model aims to advance the open-source community by establishing new benchmarks for multimodal AI applications."
                },
                "zh": {
                    "title": "å¼€åˆ›å¤šæ¨¡æ€AIæ–°æ ‡å‡†çš„InternVL 2.5",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†InternVL 2.5ï¼Œè¿™æ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼ŒåŸºäºInternVL 2.0è¿›è¡Œæ”¹è¿›ã€‚è¯¥æ¨¡å‹åœ¨è®­ç»ƒå’Œæµ‹è¯•ç­–ç•¥ä»¥åŠæ•°æ®è´¨é‡ä¸Šè¿›è¡Œäº†æ˜¾è‘—å¢å¼ºï¼Œå¹¶ç³»ç»Ÿåœ°æ¢è®¨äº†æ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»ã€‚é€šè¿‡åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼ŒInternVL 2.5å±•ç°äº†ä¸é¢†å…ˆå•†ä¸šæ¨¡å‹å¦‚GPT-4oå’ŒClaude-3.5-Sonnetç›¸åª²ç¾çš„ç«äº‰æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¨¡å‹é¦–æ¬¡åœ¨MMMUåŸºå‡†ä¸Šè¶…è¿‡70%ï¼Œå¹¶é€šè¿‡é“¾å¼æ€ç»´æ¨ç†å®ç°äº†3.7ç‚¹çš„æå‡ï¼Œå±•ç¤ºäº†åœ¨æµ‹è¯•æ—¶æ‰©å±•çš„å¼ºå¤§æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04814",
            "title": "LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment",
            "url": "https://huggingface.co/papers/2412.04814",
            "abstract": "Recent advancements in text-to-video (T2V) generative models have shown impressive capabilities. However, these models are still inadequate in aligning synthesized videos with human preferences (e.g., accurately reflecting text descriptions), which is particularly difficult to address, as human preferences are inherently subjective and challenging to formalize as objective functions. Therefore, this paper proposes LiFT, a novel fine-tuning method leveraging human feedback for T2V model alignment. Specifically, we first construct a Human Rating Annotation dataset, LiFT-HRA, consisting of approximately 10k human annotations, each including a score and its corresponding rationale. Based on this, we train a reward model LiFT-Critic to learn reward function effectively, which serves as a proxy for human judgment, measuring the alignment between given videos and human expectations. Lastly, we leverage the learned reward function to align the T2V model by maximizing the reward-weighted likelihood. As a case study, we apply our pipeline to CogVideoX-2B, showing that the fine-tuned model outperforms the CogVideoX-5B across all 16 metrics, highlighting the potential of human feedback in improving the alignment and quality of synthesized videos.",
            "score": 30,
            "issue_id": 1013,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 6",
                "zh": "12æœˆ6æ—¥"
            },
            "hash": "3ac10cfceba4368e",
            "authors": [
                "Yibin Wang",
                "Zhiyu Tan",
                "Junyan Wang",
                "Xiaomeng Yang",
                "Cheng Jin",
                "Hao Li"
            ],
            "affiliations": [
                "Australian Institute for Machine Learning, The University of Adelaide",
                "Fudan University",
                "Shanghai Academy of Artificial Intelligence for Science"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04814.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#dataset",
                    "#training",
                    "#alignment",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LiFT - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LiFT-HRA Ñ 10 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸Ñ… Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº LiFT-Critic, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸ÑĞ¼ Ğ»ÑĞ´ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CogVideoX-2B, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½ÑƒÑ CogVideoX-5B Ğ¿Ğ¾ 16 Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "Aligning Videos with Human Preferences Using Feedback",
                    "desc": "This paper introduces LiFT, a new method for improving text-to-video (T2V) generative models by incorporating human feedback. The authors create a dataset called LiFT-HRA, which contains around 10,000 human annotations that provide scores and rationales for video quality. They develop a reward model, LiFT-Critic, to quantify how well generated videos align with human expectations, effectively serving as a stand-in for human judgment. By maximizing the reward-weighted likelihood using this model, they demonstrate that their approach significantly enhances the performance of T2V models, as shown in their case study with CogVideoX-2B."
                },
                "zh": {
                    "title": "åˆ©ç”¨äººç±»åé¦ˆæå‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å¯¹é½æ€§",
                    "desc": "æœ€è¿‘ï¼Œæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆT2Vï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å°†ç”Ÿæˆçš„è§†é¢‘ä¸äººç±»åå¥½å¯¹é½æ–¹é¢ä»ç„¶å­˜åœ¨ä¸è¶³ã€‚ç”±äºäººç±»åå¥½å…·æœ‰ä¸»è§‚æ€§ï¼Œéš¾ä»¥å½¢å¼åŒ–ä¸ºå®¢è§‚å‡½æ•°ï¼Œå› æ­¤æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¾®è°ƒæ–¹æ³•LiFTï¼Œåˆ©ç”¨äººç±»åé¦ˆæ¥æ”¹å–„T2Væ¨¡å‹çš„å¯¹é½ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«çº¦1ä¸‡æ¡äººç±»è¯„åˆ†åŠå…¶ç†ç”±çš„æ³¨é‡Šæ•°æ®é›†LiFT-HRAï¼Œå¹¶åŸºäºæ­¤è®­ç»ƒäº†ä¸€ä¸ªå¥–åŠ±æ¨¡å‹LiFT-Criticï¼Œä»¥æœ‰æ•ˆå­¦ä¹ å¥–åŠ±å‡½æ•°ã€‚é€šè¿‡æœ€å¤§åŒ–å¥–åŠ±åŠ æƒçš„ä¼¼ç„¶æ€§ï¼Œæˆ‘ä»¬æˆåŠŸåœ°å°†T2Væ¨¡å‹ä¸äººç±»æœŸæœ›å¯¹é½ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåçš„æ¨¡å‹åœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸæ¨¡å‹ï¼Œå±•ç¤ºäº†äººç±»åé¦ˆåœ¨æå‡ç”Ÿæˆè§†é¢‘è´¨é‡æ–¹é¢çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.05237",
            "title": "MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale",
            "url": "https://huggingface.co/papers/2412.05237",
            "abstract": "Open-source multimodal large language models (MLLMs) have shown significant potential in a broad range of multimodal tasks. However, their reasoning capabilities remain constrained by existing instruction-tuning datasets, which were predominately repurposed from academic datasets such as VQA, AI2D, and ChartQA. These datasets target simplistic tasks, and only provide phrase-level answers without any intermediate rationales. To address these challenges, we introduce a scalable and cost-effective method to construct a large-scale multimodal instruction-tuning dataset with rich intermediate rationales designed to elicit CoT reasoning. Using only open models, we create a dataset containing 12M instruction-response pairs to cover diverse, reasoning-intensive tasks with detailed and faithful rationales. Experiments demonstrate that training MLLMs on this dataset significantly improves reasoning capabilities, achieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates notable improvements of up to 4% on non-reasoning-based benchmarks. Ablation studies further highlight the importance of key components, such as rewriting and self-filtering, in the dataset construction process.",
            "score": 26,
            "issue_id": 1012,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 6",
                "zh": "12æœˆ6æ—¥"
            },
            "hash": "1e3d5645afd61cf2",
            "authors": [
                "Jarvis Guo",
                "Tuney Zheng",
                "Yuelin Bai",
                "Bo Li",
                "Yubo Wang",
                "King Zhu",
                "Yizhi Li",
                "Graham Neubig",
                "Wenhu Chen",
                "Xiang Yue"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Nanyang Technological University",
                "The University of Manchester",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05237.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#open_source",
                    "#reasoning",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 12 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Reasoning in MLLMs with Rich Rationales",
                    "desc": "This paper presents a new method for creating a large-scale multimodal instruction-tuning dataset aimed at improving the reasoning abilities of multimodal large language models (MLLMs). The authors identify that existing datasets are limited as they mainly provide simple answers without detailed reasoning. To overcome this, they developed a dataset with 12 million instruction-response pairs that include rich intermediate rationales, promoting chain-of-thought (CoT) reasoning. Experiments show that training on this dataset enhances MLLMs' reasoning performance significantly, achieving state-of-the-art results on various benchmarks."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ„å»ºå¤§è§„æ¨¡å¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†çš„æ–¹æ³•ï¼Œä»¥æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç°æœ‰çš„æ•°æ®é›†ä¸»è¦æ¥è‡ªå­¦æœ¯ç ”ç©¶ï¼Œä»»åŠ¡ç®€å•ï¼Œç¼ºä¹ä¸­é—´æ¨ç†è¿‡ç¨‹çš„è¯¦ç»†è§£é‡Šã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«1200ä¸‡å¯¹æŒ‡ä»¤å’Œå“åº”çš„æ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šæ ·åŒ–ä¸”éœ€è¦æ¨ç†çš„ä»»åŠ¡ï¼Œå¹¶æä¾›äº†ä¸°å¯Œçš„æ¨ç†ä¾æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ•°æ®é›†è®­ç»ƒçš„MLLMsåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨ç†èƒ½åŠ›ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04862",
            "title": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases",
            "url": "https://huggingface.co/papers/2412.04862",
            "abstract": "This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from https://huggingface.co/LGAI-EXAONE. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.",
            "score": 23,
            "issue_id": 1011,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 6",
                "zh": "12æœˆ6æ—¥"
            },
            "hash": "83e6f957e42ebb5e",
            "authors": [
                "LG AI Research",
                "Soyoung An",
                "Kyunghoon Bae",
                "Eunbi Choi",
                "Kibong Choi",
                "Stanley Jungkyu Choi",
                "Seokhee Hong",
                "Junwon Hwang",
                "Hyojin Jeon",
                "Gerrard Jeongwon Jo",
                "Hyunjik Jo",
                "Jiyeon Jung",
                "Yountae Jung",
                "Hyosang Kim",
                "Joonkee Kim",
                "Seonghwan Kim",
                "Soyeon Kim",
                "Sunkyoung Kim",
                "Yireun Kim",
                "Yongil Kim",
                "Youchul Kim",
                "Edward Hwayoung Lee",
                "Haeju Lee",
                "Honglak Lee",
                "Jinsik Lee",
                "Kyungmin Lee",
                "Woohyung Lim",
                "Sangha Park",
                "Sooyoun Park",
                "Yongmin Park",
                "Sihoon Yang",
                "Heuiyeen Yeen",
                "Hyeongu Yun"
            ],
            "affiliations": [
                "LG AI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04862.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#open_source",
                    "#small_models",
                    "#long_context",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "EXAONE 3.5: ĞĞ¾Ğ²Ğ¾Ğµ ÑĞ»Ğ¾Ğ²Ğ¾ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¾Ñ‚ LG AI Research",
                    "desc": "ĞšĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ LG AI Research Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ EXAONE 3.5, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ² Ñ‚Ñ€ĞµÑ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ…: 32B, 7.8B Ğ¸ 2.4B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ EXAONE 3.5 Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Hugging Face."
                },
                "en": {
                    "title": "EXAONE 3.5: Leading the Way in Instruction-Tuned Language Models",
                    "desc": "The EXAONE 3.5 language models, created by LG AI Research, are advanced instruction-tuned models available in three sizes: 32B, 7.8B, and 2.4B. They excel in following instructions in real-world applications, achieving top scores on seven different benchmarks. Additionally, these models demonstrate superior long-context understanding, ranking first in four benchmarks. They are accessible for research purposes and show competitive performance against leading open models of similar sizes across nine general benchmarks."
                },
                "zh": {
                    "title": "EXAONE 3.5ï¼šæŒ‡ä»¤è·Ÿéšä¸é•¿ä¸Šä¸‹æ–‡ç†è§£çš„å…ˆé”‹",
                    "desc": "EXAONE 3.5 æ˜¯ç”± LG AI ç ”ç©¶æ‰€å¼€å‘çš„æŒ‡ä»¤è°ƒä¼˜è¯­è¨€æ¨¡å‹ï¼Œæä¾›ä¸‰ç§é…ç½®ï¼š32Bã€7.8B å’Œ 2.4Bã€‚è¿™äº›æ¨¡å‹åœ¨å®é™…åœºæ™¯ä¸­å…·æœ‰å“è¶Šçš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œåœ¨ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€é«˜åˆ†ã€‚å®ƒä»¬åœ¨é•¿ä¸Šä¸‹æ–‡ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¹Ÿè¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸åŒç±»å¼€æºæ¨¡å‹ç›¸æ¯”ï¼ŒEXAONE 3.5 åœ¨ä¹ä¸ªé€šç”¨åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿå±•ç°äº†ç«äº‰åŠ›çš„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04445",
            "title": "Moto: Latent Motion Token as the Bridging Language for Robot Manipulation",
            "url": "https://huggingface.co/papers/2412.04445",
            "abstract": "Recent developments in Large Language Models pre-trained on extensive corpora have shown significant success in various natural language processing tasks with minimal fine-tuning. This success offers new promise for robotics, which has long been constrained by the high cost of action-labeled data. We ask: given the abundant video data containing interaction-related knowledge available as a rich \"corpus\", can a similar generative pre-training approach be effectively applied to enhance robot learning? The key challenge is to identify an effective representation for autoregressive pre-training that benefits robot manipulation tasks. Inspired by the way humans learn new skills through observing dynamic environments, we propose that effective robotic learning should emphasize motion-related knowledge, which is closely tied to low-level actions and is hardware-agnostic, facilitating the transfer of learned motions to actual robot actions. To this end, we introduce Moto, which converts video content into latent Motion Token sequences by a Latent Motion Tokenizer, learning a bridging \"language\" of motion from videos in an unsupervised manner. We pre-train Moto-GPT through motion token autoregression, enabling it to capture diverse visual motion knowledge. After pre-training, Moto-GPT demonstrates the promising ability to produce semantically interpretable motion tokens, predict plausible motion trajectories, and assess trajectory rationality through output likelihood. To transfer learned motion priors to real robot actions, we implement a co-fine-tuning strategy that seamlessly bridges latent motion token prediction and real robot control. Extensive experiments show that the fine-tuned Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks, underscoring its effectiveness in transferring knowledge from video data to downstream visual manipulation tasks.",
            "score": 18,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 5",
                "zh": "12æœˆ5æ—¥"
            },
            "hash": "a5ac6d786500ef9f",
            "authors": [
                "Yi Chen",
                "Yuying Ge",
                "Yizhuo Li",
                "Yixiao Ge",
                "Mingyu Ding",
                "Ying Shan",
                "Xihui Liu"
            ],
            "affiliations": [
                "ARC Lab, Tencent PCG",
                "The University of Hong Kong",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04445.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#transfer_learning",
                    "#games",
                    "#robotics",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Moto, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Latent Motion Tokenizer. Moto-GPT Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸. ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Moto-GPT Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Bridging Video Knowledge to Robot Actions with Moto-GPT",
                    "desc": "This paper explores the application of Large Language Models to enhance robot learning by leveraging abundant video data. It introduces Moto, a system that converts video content into Motion Token sequences, allowing robots to learn motion-related knowledge in an unsupervised manner. The approach emphasizes the importance of motion understanding for effective robotic manipulation, enabling the transfer of learned skills to real-world actions. Through extensive experiments, Moto-GPT demonstrates improved performance in robot manipulation tasks, showcasing its ability to bridge the gap between video knowledge and practical robot control."
                },
                "zh": {
                    "title": "åˆ©ç”¨è§†é¢‘æ•°æ®æå‡æœºå™¨äººå­¦ä¹ èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§è§„æ¨¡è§†é¢‘æ•°æ®æ¥æå‡æœºå™¨äººå­¦ä¹ èƒ½åŠ›ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºMotoçš„æ¨¡å‹ï¼Œé€šè¿‡å°†è§†é¢‘å†…å®¹è½¬æ¢ä¸ºæ½œåœ¨çš„è¿åŠ¨æ ‡è®°åºåˆ—ï¼Œæ¥å­¦ä¹ è¿åŠ¨çŸ¥è¯†ã€‚Moto-GPTç»è¿‡é¢„è®­ç»ƒåï¼Œèƒ½å¤Ÿç”Ÿæˆå¯è§£é‡Šçš„è¿åŠ¨æ ‡è®°ï¼Œå¹¶é¢„æµ‹åˆç†çš„è¿åŠ¨è½¨è¿¹ã€‚æœ€ç»ˆï¼Œç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„Moto-GPTåœ¨æœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†ä»è§†é¢‘æ•°æ®è½¬ç§»çŸ¥è¯†çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.05270",
            "title": "APOLLO: SGD-like Memory, AdamW-level Performance",
            "url": "https://huggingface.co/papers/2412.05270",
            "abstract": "Large language models (LLMs) are notoriously memory-intensive during training, particularly with the popular AdamW optimizer. This memory burden necessitates using more or higher-end GPUs or reducing batch sizes, limiting training scalability and throughput. To address this, various memory-efficient optimizers have been proposed to reduce optimizer memory usage. However, they face critical challenges: (i) reliance on costly SVD operations; (ii) significant performance trade-offs compared to AdamW; and (iii) still substantial optimizer memory overhead to maintain competitive performance.   In this work, we identify that AdamW's learning rate adaptation rule can be effectively coarsened as a structured learning rate update. Based on this insight, we propose Approximated Gradient Scaling for Memory-Efficient LLM Optimization (APOLLO), which approximates learning rate scaling using an auxiliary low-rank optimizer state based on pure random projection. This structured learning rate update rule makes APOLLO highly tolerant to further memory reductions while delivering comparable pre-training performance. Even its rank-1 variant, APOLLO-Mini, achieves superior pre-training performance compared to AdamW with SGD-level memory costs.   Extensive experiments demonstrate that the APOLLO series performs on-par with or better than AdamW, while achieving greater memory savings by nearly eliminating the optimization states of AdamW. These savings provide significant system-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB setup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model Scalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without system-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training LLaMA-7B on a single GPU using less than 12 GB of memory with weight quantization.",
            "score": 15,
            "issue_id": 1012,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 6",
                "zh": "12æœˆ6æ—¥"
            },
            "hash": "14bb480f5fe29bae",
            "authors": [
                "Hanqing Zhu",
                "Zhenyu Zhang",
                "Wenyan Cong",
                "Xi Liu",
                "Sem Park",
                "Vikas Chandra",
                "Bo Long",
                "David Z. Pan",
                "Zhangyang Wang",
                "Jinwon Lee"
            ],
            "affiliations": [
                "Department of Electrical and Computer Engineering, The University of Texas at Austin",
                "Meta"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05270.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "APOLLO: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ APOLLO Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). APOLLO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ AdamW. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ APOLLO Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‡ĞµĞ¼ AdamW, Ğ¿Ñ€Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ´Ğ°Ğ¶Ğµ Ğ½Ğ° GPU ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ."
                },
                "en": {
                    "title": "APOLLO: Memory-Efficient Optimization for Large Language Models",
                    "desc": "This paper addresses the high memory requirements of training large language models (LLMs) using the AdamW optimizer. It introduces a new method called APOLLO, which simplifies the learning rate adaptation process to reduce memory usage while maintaining performance. APOLLO utilizes a low-rank optimizer state based on random projection, allowing for significant memory savings without sacrificing training efficiency. The results show that APOLLO can achieve comparable or better performance than AdamW, enabling larger batch sizes and making LLM training feasible on lower-end GPUs."
                },
                "zh": {
                    "title": "APOLLOï¼šé«˜æ•ˆå†…å­˜ä¼˜åŒ–çš„æœªæ¥",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹å†…å­˜çš„éœ€æ±‚éå¸¸é«˜ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨æµè¡Œçš„AdamWä¼˜åŒ–å™¨æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…ä»¬æå‡ºäº†å¤šç§å†…å­˜é«˜æ•ˆçš„ä¼˜åŒ–å™¨ï¼Œä½†å®ƒä»¬é¢ä¸´ç€ä¾èµ–æ˜‚è´µçš„SVDæ“ä½œå’Œæ€§èƒ½æŠ˜è¡·ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºAPOLLOçš„ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡è¿‘ä¼¼å­¦ä¹ ç‡ç¼©æ”¾æ¥å‡å°‘å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒä¸AdamWç›¸å½“çš„é¢„è®­ç»ƒæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAPOLLOç³»åˆ—åœ¨å†…å­˜èŠ‚çœæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿåœ¨è¾ƒä½çš„å†…å­˜æˆæœ¬ä¸‹å®ç°æ›´é«˜çš„è®­ç»ƒååé‡å’Œæ¨¡å‹å¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04301",
            "title": "SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion",
            "url": "https://huggingface.co/papers/2412.04301",
            "abstract": "Recent advances in text-guided image editing enable users to perform image edits through simple text inputs, leveraging the extensive priors of multi-step diffusion-based text-to-image models. However, these methods often fall short of the speed demands required for real-world and on-device applications due to the costly multi-step inversion and sampling process involved. In response to this, we introduce SwiftEdit, a simple yet highly efficient editing tool that achieve instant text-guided image editing (in 0.23s). The advancement of SwiftEdit lies in its two novel contributions: a one-step inversion framework that enables one-step image reconstruction via inversion and a mask-guided editing technique with our proposed attention rescaling mechanism to perform localized image editing. Extensive experiments are provided to demonstrate the effectiveness and efficiency of SwiftEdit. In particular, SwiftEdit enables instant text-guided image editing, which is extremely faster than previous multi-step methods (at least 50 times faster) while maintain a competitive performance in editing results. Our project page is at: https://swift-edit.github.io/",
            "score": 13,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 5",
                "zh": "12æœˆ5æ—¥"
            },
            "hash": "a4cea89a59a9a3c0",
            "authors": [
                "Trong-Tung Nguyen",
                "Quang Nguyen",
                "Khoi Nguyen",
                "Anh Tran",
                "Cuong Pham"
            ],
            "affiliations": [
                "Posts & Telecom. Inst. of Tech., Vietnam",
                "VinAI Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04301.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "SwiftEdit - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², SwiftEdit Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²ÑƒÑ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾ - Ğ·Ğ° 0.23 ÑĞµĞºÑƒĞ½Ğ´Ñ‹. SwiftEdit Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ Ğ² 50 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "SwiftEdit: Instant Text-Guided Image Editing at Lightning Speed!",
                    "desc": "This paper presents SwiftEdit, a new tool for fast text-guided image editing that significantly improves the speed of image modifications. Traditional methods rely on multi-step diffusion processes, which can be slow and inefficient for real-time applications. SwiftEdit introduces a one-step inversion framework that allows for quick image reconstruction and a mask-guided editing technique that uses attention rescaling for precise edits. The results show that SwiftEdit is at least 50 times faster than previous methods while still delivering high-quality editing outcomes."
                },
                "zh": {
                    "title": "SwiftEditï¼šç¬æ—¶æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘çš„é©å‘½",
                    "desc": "æœ€è¿‘çš„æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘æŠ€æœ¯ä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡ç®€å•çš„æ–‡æœ¬è¾“å…¥è¿›è¡Œå›¾åƒç¼–è¾‘ï¼Œåˆ©ç”¨äº†å¤šæ­¥æ‰©æ•£æ¨¡å‹çš„ä¸°å¯Œå…ˆéªŒçŸ¥è¯†ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­å¾€å¾€é€Ÿåº¦è¾ƒæ…¢ï¼Œæ— æ³•æ»¡è¶³å®æ—¶å’Œè®¾å¤‡ç«¯çš„éœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SwiftEditï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„ç¼–è¾‘å·¥å…·ï¼Œå¯ä»¥å®ç°ç¬æ—¶çš„æ–‡æœ¬å¼•å¯¼å›¾åƒç¼–è¾‘ï¼Œé€Ÿåº¦è¾¾åˆ°0.23ç§’ã€‚SwiftEditçš„åˆ›æ–°åœ¨äºå…¶ä¸€ä½“åŒ–çš„åæ¼”æ¡†æ¶å’ŒåŸºäºæ©ç çš„ç¼–è¾‘æŠ€æœ¯ï¼Œèƒ½å¤Ÿå¿«é€Ÿè¿›è¡Œå±€éƒ¨å›¾åƒç¼–è¾‘ï¼ŒåŒæ—¶ä¿æŒä¸ä¼ ç»Ÿå¤šæ­¥æ–¹æ³•ç›¸å½“çš„ç¼–è¾‘æ•ˆæœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04440",
            "title": "GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration",
            "url": "https://huggingface.co/papers/2412.04440",
            "abstract": "Text-to-video generation models have shown significant progress in the recent years. However, they still struggle with generating complex dynamic scenes based on compositional text prompts, such as attribute binding for multiple objects, temporal dynamics associated with different objects, and interactions between objects. Our key motivation is that complex tasks can be decomposed into simpler ones, each handled by a role-specialized MLLM agent. Multiple agents can collaborate together to achieve collective intelligence for complex goals. We propose GenMAC, an iterative, multi-agent framework that enables compositional text-to-video generation. The collaborative workflow includes three stages: Design, Generation, and Redesign, with an iterative loop between the Generation and Redesign stages to progressively verify and refine the generated videos. The Redesign stage is the most challenging stage that aims to verify the generated videos, suggest corrections, and redesign the text prompts, frame-wise layouts, and guidance scales for the next iteration of generation. To avoid hallucination of a single MLLM agent, we decompose this stage to four sequentially-executed MLLM-based agents: verification agent, suggestion agent, correction agent, and output structuring agent. Furthermore, to tackle diverse scenarios of compositional text-to-video generation, we design a self-routing mechanism to adaptively select the proper correction agent from a collection of correction agents each specialized for one scenario. Extensive experiments demonstrate the effectiveness of GenMAC, achieving state-of-the art performance in compositional text-to-video generation.",
            "score": 12,
            "issue_id": 1011,
            "pub_date": "2024-12-05",
            "pub_date_card": {
                "ru": "5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 5",
                "zh": "12æœˆ5æ—¥"
            },
            "hash": "28dc2191ba71c4ea",
            "authors": [
                "Kaiyi Huang",
                "Yukun Huang",
                "Xuefei Ning",
                "Zinan Lin",
                "Yu Wang",
                "Xihui Liu"
            ],
            "affiliations": [
                "Microsoft Research",
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04440.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#agents",
                    "#games",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GenMAC - Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ†Ğ¸ĞºĞ»Ğ¾Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°. GenMAC Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Collaborative Intelligence for Text-to-Video Mastery",
                    "desc": "This paper introduces GenMAC, a multi-agent framework designed to improve text-to-video generation by breaking down complex tasks into simpler components. It utilizes specialized machine learning agents that collaborate through three main stages: Design, Generation, and Redesign, with an iterative process to refine video outputs. The Redesign stage is particularly crucial as it involves multiple agents that verify, suggest corrections, and restructure prompts to enhance the generated videos. The framework also includes a self-routing mechanism to select the most suitable correction agent for various scenarios, leading to significant advancements in generating dynamic scenes from text prompts."
                },
                "zh": {
                    "title": "GenMACï¼šåä½œç”Ÿæˆå¤æ‚è§†é¢‘çš„æ™ºèƒ½æ¡†æ¶",
                    "desc": "æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç”Ÿæˆå¤æ‚åŠ¨æ€åœºæ™¯æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºGenMACçš„å¤šä»£ç†æ¡†æ¶ï¼Œé€šè¿‡å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºç®€å•ä»»åŠ¡æ¥å®ç°åä½œç”Ÿæˆã€‚è¯¥æ¡†æ¶åŒ…æ‹¬è®¾è®¡ã€ç”Ÿæˆå’Œé‡æ–°è®¾è®¡ä¸‰ä¸ªé˜¶æ®µï¼Œç”Ÿæˆå’Œé‡æ–°è®¾è®¡ä¹‹é—´å­˜åœ¨è¿­ä»£å¾ªç¯ï¼Œä»¥é€æ­¥éªŒè¯å’Œä¼˜åŒ–ç”Ÿæˆçš„è§†é¢‘ã€‚é€šè¿‡è‡ªé€‚åº”é€‰æ‹©ä¸“é—¨åŒ–çš„ä¿®æ­£ä»£ç†ï¼ŒGenMACèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å¤šæ ·åŒ–çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆåœºæ™¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.05263",
            "title": "Mind the Time: Temporally-Controlled Multi-Event Video Generation",
            "url": "https://huggingface.co/papers/2412.05263",
            "abstract": "Real-world videos consist of sequences of events. Generating such sequences with precise temporal control is infeasible with existing video generators that rely on a single paragraph of text as input. When tasked with generating multiple events described using a single prompt, such methods often ignore some of the events or fail to arrange them in the correct order. To address this limitation, we present MinT, a multi-event video generator with temporal control. Our key insight is to bind each event to a specific period in the generated video, which allows the model to focus on one event at a time. To enable time-aware interactions between event captions and video tokens, we design a time-based positional encoding method, dubbed ReRoPE. This encoding helps to guide the cross-attention operation. By fine-tuning a pre-trained video diffusion transformer on temporally grounded data, our approach produces coherent videos with smoothly connected events. For the first time in the literature, our model offers control over the timing of events in generated videos. Extensive experiments demonstrate that MinT outperforms existing open-source models by a large margin.",
            "score": 6,
            "issue_id": 1014,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 6",
                "zh": "12æœˆ6æ—¥"
            },
            "hash": "826bb588770d5c27",
            "authors": [
                "Ziyi Wu",
                "Aliaksandr Siarohin",
                "Willi Menapace",
                "Ivan Skorokhodov",
                "Yuwei Fang",
                "Varnith Chordia",
                "Igor Gilitschenski",
                "Sergey Tulyakov"
            ],
            "affiliations": [
                "Snap Research",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.05263.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#training",
                    "#architecture",
                    "#diffusion",
                    "#video",
                    "#multimodal"
                ],
                "emoji": "â±ï¸",
                "ru": {
                    "title": "Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MinT - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğµ Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ñƒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¸ Ğ·Ğ° Ñ€Ğ°Ğ·. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ReRoPE Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "MinT: Mastering Multi-Event Video Generation with Temporal Precision",
                    "desc": "This paper introduces MinT, a novel multi-event video generator that allows for precise temporal control over the events depicted in generated videos. Unlike traditional models that struggle with sequencing multiple events from a single text prompt, MinT binds each event to a specific time period, ensuring that all events are accurately represented and ordered. The authors implement a time-based positional encoding method called ReRoPE, which enhances the model's ability to manage interactions between event descriptions and video frames. By fine-tuning a pre-trained video diffusion transformer on data with temporal grounding, MinT achieves superior performance in generating coherent videos with well-timed events."
                },
                "zh": {
                    "title": "MinTï¼šç²¾å‡†æ§åˆ¶è§†é¢‘äº‹ä»¶æ—¶åºçš„ç”Ÿæˆå™¨",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºMinTçš„å¤šäº‹ä»¶è§†é¢‘ç”Ÿæˆå™¨ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è§†é¢‘ç”Ÿæˆå™¨åœ¨ç”Ÿæˆå¤šä¸ªäº‹ä»¶æ—¶çš„æ—¶é—´æ§åˆ¶é—®é¢˜ã€‚é€šè¿‡å°†æ¯ä¸ªäº‹ä»¶ç»‘å®šåˆ°ç”Ÿæˆè§†é¢‘çš„ç‰¹å®šæ—¶é—´æ®µï¼ŒMinTèƒ½å¤Ÿé€ä¸ªå…³æ³¨äº‹ä»¶ï¼Œä»è€Œæé«˜ç”Ÿæˆè§†é¢‘çš„è¿è´¯æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºæ—¶é—´çš„ä½ç½®ä¿¡æ¯ç¼–ç æ–¹æ³•ReRoPEï¼Œä»¥å¢å¼ºäº‹ä»¶æè¿°ä¸è§†é¢‘å¸§ä¹‹é—´çš„äº¤äº’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMinTåœ¨ç”Ÿæˆè§†é¢‘çš„æ—¶é—´æ§åˆ¶æ–¹é¢ä¼˜äºç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.03428",
            "title": "2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction",
            "url": "https://huggingface.co/papers/2412.03428",
            "abstract": "The reconstruction of indoor scenes remains challenging due to the inherent complexity of spatial structures and the prevalence of textureless regions. Recent advancements in 3D Gaussian Splatting have improved novel view synthesis with accelerated processing but have yet to deliver comparable performance in surface reconstruction. In this paper, we introduce 2DGS-Room, a novel method leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction. Specifically, we employ a seed-guided mechanism to control the distribution of 2D Gaussians, with the density of seed points dynamically optimized through adaptive growth and pruning mechanisms. To further improve geometric accuracy, we incorporate monocular depth and normal priors to provide constraints for details and textureless regions respectively. Additionally, multi-view consistency constraints are employed to mitigate artifacts and further enhance reconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasets demonstrate that our method achieves state-of-the-art performance in indoor scene reconstruction.",
            "score": 5,
            "issue_id": 1013,
            "pub_date": "2024-12-04",
            "pub_date_card": {
                "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 4",
                "zh": "12æœˆ4æ—¥"
            },
            "hash": "ead3f67b9be4d52b",
            "authors": [
                "Wanting Zhang",
                "Haodong Xiang",
                "Zhichao Liao",
                "Xiansong Lai",
                "Xinghui Li",
                "Long Zeng"
            ],
            "affiliations": [
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.03428.jpg",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ğŸ ",
                "ru": {
                    "title": "2DGS-Room: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 2D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ°",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ 2DGS-Room Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 2D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ 2D Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞµĞ¼ÑĞ½, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€Ğ¾ÑÑ‚ Ğ¸ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ğµ. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ScanNet Ğ¸ ScanNet++ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Indoor Scene Reconstruction with 2D Gaussian Splatting",
                    "desc": "This paper presents 2DGS-Room, a new approach for reconstructing indoor scenes using 2D Gaussian Splatting. The method introduces a seed-guided mechanism that optimizes the distribution of 2D Gaussians, enhancing the reconstruction process. By incorporating monocular depth and normal priors, the approach improves geometric accuracy, especially in areas lacking texture. The use of multi-view consistency constraints helps reduce artifacts, leading to high-fidelity results in indoor scene reconstruction, as demonstrated by experiments on ScanNet and ScanNet++ datasets."
                },
                "zh": {
                    "title": "é«˜ä¿çœŸå®¤å†…åœºæ™¯é‡å»ºçš„æ–°æ–¹æ³•",
                    "desc": "å®¤å†…åœºæ™¯çš„é‡å»ºå› ç©ºé—´ç»“æ„å¤æ‚å’Œæ— çº¹ç†åŒºåŸŸçš„æ™®éå­˜åœ¨è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•2DGS-Roomï¼Œåˆ©ç”¨2Dé«˜æ–¯ç‚¹äº‘å®ç°é«˜ä¿çœŸåº¦çš„å®¤å†…åœºæ™¯é‡å»ºã€‚æˆ‘ä»¬é‡‡ç”¨ç§å­å¼•å¯¼æœºåˆ¶æ¥æ§åˆ¶2Dé«˜æ–¯çš„åˆ†å¸ƒï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”ç”Ÿé•¿å’Œä¿®å‰ªæœºåˆ¶åŠ¨æ€ä¼˜åŒ–ç§å­ç‚¹çš„å¯†åº¦ã€‚é€šè¿‡ç»“åˆå•ç›®æ·±åº¦å’Œæ³•çº¿å…ˆéªŒï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æé«˜äº†å‡ ä½•ç²¾åº¦ï¼Œå¹¶ä½¿ç”¨å¤šè§†å›¾ä¸€è‡´æ€§çº¦æŸæ¥å‡å°‘ä¼ªå½±ï¼Œæå‡é‡å»ºè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.04905",
            "title": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling",
            "url": "https://huggingface.co/papers/2412.04905",
            "abstract": "Large language models (LLMs) have made dialogue one of the central modes of human-machine interaction, leading to the accumulation of vast amounts of conversation logs and increasing demand for dialogue generation. A conversational life-cycle spans from the Prelude through the Interlocution to the Epilogue, encompassing various elements. Despite the existence of numerous dialogue-related studies, there is a lack of benchmarks that encompass comprehensive dialogue elements, hindering precise modeling and systematic evaluation. To bridge this gap, we introduce an innovative research task Dialogue Element MOdeling, including Element Awareness and Dialogue Agent Interaction, and propose a novel benchmark, DEMO, designed for a comprehensive dialogue modeling and assessment. Inspired by imitation learning, we further build the agent which possesses the adept ability to model dialogue elements based on the DEMO benchmark. Extensive experiments indicate that existing LLMs still exhibit considerable potential for enhancement, and our DEMO agent has superior performance in both in-domain and out-of-domain tasks.",
            "score": 4,
            "issue_id": 1017,
            "pub_date": "2024-12-06",
            "pub_date_card": {
                "ru": "6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 6",
                "zh": "12æœˆ6æ—¥"
            },
            "hash": "4b12d357252b671f",
            "authors": [
                "Minzheng Wang",
                "Xinghua Zhang",
                "Kun Chen",
                "Nan Xu",
                "Haiyang Yu",
                "Fei Huang",
                "Wenji Mao",
                "Yongbin Li"
            ],
            "affiliations": [
                "MAIS, Institute of Automation, Chinese Academy of Sciences",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.04905.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dialogue_generation",
                    "#agents"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "DEMO: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'ĞœĞ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°' Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº DEMO. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºÑƒ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ²ÑĞµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ DEMO, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing Dialogue Generation with Comprehensive Element Modeling",
                    "desc": "This paper addresses the growing need for effective dialogue generation in human-machine interactions, particularly with large language models (LLMs). It identifies a gap in existing research due to the lack of comprehensive benchmarks that cover all aspects of dialogue, which hampers accurate modeling and evaluation. To tackle this issue, the authors introduce a new task called Dialogue Element MOdeling (DEMO), which focuses on understanding dialogue elements and how agents interact within conversations. Their experiments show that while current LLMs have room for improvement, the proposed DEMO agent outperforms them in various tasks, demonstrating its effectiveness in dialogue modeling."
                },
                "zh": {
                    "title": "å…¨é¢å¯¹è¯å»ºæ¨¡çš„æ–°åŸºå‡†DEMO",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½¿å¯¹è¯æˆä¸ºäººæœºäº¤äº’çš„ä¸»è¦æ–¹å¼ï¼Œå¯¼è‡´å¤§é‡å¯¹è¯æ—¥å¿—çš„ç§¯ç´¯å’Œå¯¹å¯¹è¯ç”Ÿæˆçš„éœ€æ±‚å¢åŠ ã€‚å¯¹è¯çš„ç”Ÿå‘½å‘¨æœŸåŒ…æ‹¬å‰å¥ã€å¯¹è¯å’Œç»“å°¾ï¼Œæ¶µç›–äº†å¤šä¸ªå…ƒç´ ã€‚å°½ç®¡å·²æœ‰è®¸å¤šä¸å¯¹è¯ç›¸å…³çš„ç ”ç©¶ï¼Œä½†ç¼ºä¹å…¨é¢çš„åŸºå‡†ï¼Œé™åˆ¶äº†ç²¾ç¡®å»ºæ¨¡å’Œç³»ç»Ÿè¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¯¹è¯å…ƒç´ å»ºæ¨¡çš„æ–°ä»»åŠ¡ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªæ–°çš„åŸºå‡†DEMOï¼Œä»¥ä¾¿è¿›è¡Œå…¨é¢çš„å¯¹è¯å»ºæ¨¡å’Œè¯„ä¼°ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-06.html",
    "link_next": "2024-12-10.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "06.12",
        "en": "12/06",
        "zh": "12æœˆ6æ—¥"
    },
    "short_date_next": {
        "ru": "10.12",
        "en": "12/10",
        "zh": "12æœˆ10æ—¥"
    },
    "categories": {
        "#dataset": 3,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 2,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 4,
        "#multimodal": 6,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 7,
        "#robotics": 1,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0,
        "#dialogue_generation": 1
    },
    "zh": {
        "text": "æˆ‘ä»¬ä»‹ç»äº† InternVL 2.5ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº InternVL 2.0 çš„å…ˆè¿›å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ã€‚å®ƒä¿ç•™äº†æ ¸å¿ƒæ¨¡å‹æ¶æ„ï¼Œä½†åœ¨è®­ç»ƒå’Œæµ‹è¯•ç­–ç•¥ä»¥åŠæ•°æ®è´¨é‡æ–¹é¢è¿›è¡Œäº†æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬ç ”ç©¶äº†æ¨¡å‹æ‰©å±•ä¸æ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œç³»ç»Ÿåœ°æ¢è®¨äº†è§†è§‰ç¼–ç å™¨ã€è¯­è¨€æ¨¡å‹ã€æ•°æ®é›†å¤§å°å’Œæµ‹è¯•æ—¶é…ç½®çš„æ€§èƒ½è¶‹åŠ¿ã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼ŒInternVL 2.5 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å¤šå­¦ç§‘æ¨ç†ã€æ–‡æ¡£ç†è§£ã€å¤šå›¾åƒ/è§†é¢‘ç†è§£ã€å®é™…ç†è§£ã€å¤šæ¨¡æ€å¹»è§‰æ£€æµ‹ã€è§†è§‰å®šä½ã€å¤šè¯­è¨€èƒ½åŠ›å’Œçº¯è¯­è¨€å¤„ç†ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ˜¯ç¬¬ä¸€ä¸ªåœ¨ MMMU åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡ 70% çš„å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„æµ‹è¯•æ—¶æ‰©å±•æ½œåŠ›ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªæ¨¡å‹èƒ½ä¸ºå¼€æºç¤¾åŒºè´¡çŒ®æ–°çš„æ ‡å‡†ï¼Œç”¨äºå¼€å‘å’Œåº”ç”¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚HuggingFace æ¼”ç¤ºè§ https://huggingface.co/spaces/OpenGVLab/InternVLã€‚",
        "title": "Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling",
        "pinyin": "WÇ’men jiÃ¨shÃ o le InternVL 2.5, zhÃ¨ shÃ¬ yÄ«gÃ¨ jÄ«yÃº InternVL 2.0 de xiÄnjÃ¬n duÅ mÃ³shÃ¬ dÃ  yÇ”yÃ¡n mÃ³xÃ­ng xÃ¬liÃ¨. TÄ bÇoliÃºle hÃ©xÄ«n mÃ³xÃ­ng jiÃ gÃ²u, dÃ n zÃ i xÃ¹nliÃ n hÃ© cÃ¨shÃ¬ cÃ¨lÃ¼Ã¨ yÇjiÇ shÃ¹jÃ¹ zhÃ¬liÃ ng fÄngmiÃ n jÃ¬nxÃ­ng le xiÇnzhÃ¹ gÇijÃ¬n. WÇ’men yÃ¡njiÅ« le mÃ³xÃ­ng kuÃ²zhÇn yÇ” xÃ¬ngnÃ©ng zhÄ«jiÄn de guÄnxÃ¬, xÃ¬tÇ’ng de tÃ ntÃ o le shÃ¬juÄn biÄnmÇqÃ¬, yÇ”yÃ¡n mÃ³xÃ­ng, shÃ¹jÃ¹jÃ­ dÃ xÃ¬ng hÃ© cÃ¨shÃ¬ shÃ­ pÃ¨izhÃ¬ de xÃ¬ngnÃ©ng qÅ«shÃ¬. TÅngguÃ² guÇngfÃ n de pÃ­nggÅ«, InternVL 2.5 zÃ i duÅgÃ¨ jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng biÇoxiÃ n chÅ«sÃ¨, bÄokuÃ² duÅ xuÃ©kÄ“ tuÄ«lÇ, wÃ©njiÃ n lÇjiÄ›, duÅ tÃºxiÃ ng/shÃ¬pÃ­n lÇjiÄ›, shÃ­jÃ¬ lÇjiÄ›, duÅ mÃ³shÃ¬ huÃ njuÃ© jiÇncÃ¨, shÃ¬juÄn dÃ¬ngwÃ¨i, duÅ yÇ”yÃ¡n nÃ©nglÃ¬ hÃ© chÃºn yÇ”yÃ¡n chÇ”lÇ. WÇ’men de mÃ³xÃ­ng shÃ¬ dÃ¬-yÄ«gÃ¨ zÃ i MMMU jÄ«zhÇ”n cÃ¨shÃ¬ zhÅng chÄoguÃ² 70% de kÄiyuÃ¡n duÅ mÃ³shÃ¬ dÃ  yÇ”yÃ¡n mÃ³xÃ­ng, bÃ¬ng zhÇnshÃ¬ le qiÃ¡ngdÃ  de cÃ¨shÃ¬ shÃ­ kuÃ²zhÇn qiÃ¡nlÃ¬. WÇ’men xÄ«wÃ ng zhÃ¨gÃ¨ mÃ³xÃ­ng nÃ©ng wÃ¨i kÄiyuÃ¡n shÃ¨qÅ« gÃ²ngxiÃ n xÄ«n de biÄozhÇ”n, yÃ²ngyÃº kÄifÄ hÃ© yÃ¬ngyÃ²ng duÅ mÃ³shÃ¬ rÃ©ngÅng zhÃ¬nÃ©ng xÃ¬tÇ’ng. HuggingFace yÇnshÃ¬ jiÃ n https://huggingface.co/spaces/OpenGVLab/InternVL.",
        "vocab": "[\n    {\"word\": \"ä»‹ç»\", \"pinyin\": \"jiÃ¨ shÃ o\", \"trans\": \"introduce\"},\n    {\"word\": \"å…ˆè¿›\", \"pinyin\": \"xiÄn jÃ¬n\", \"trans\": \"advanced\"},\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"ä¿ç•™\", \"pinyin\": \"bÇo liÃº\", \"trans\": \"retain\"},\n    {\"word\": \"æ ¸å¿ƒ\", \"pinyin\": \"hÃ© xÄ«n\", \"trans\": \"core\"},\n    {\"word\": \"æ¶æ„\", \"pinyin\": \"jiÃ  gÃ²u\", \"trans\": \"architecture\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"æ”¹è¿›\", \"pinyin\": \"gÇi jÃ¬n\", \"trans\": \"improvement\"},\n    {\"word\": \"æ‰©å±•\", \"pinyin\": \"kuÃ² zhÇn\", \"trans\": \"expand\"},\n    {\"word\": \"æ€§èƒ½\", \"pinyin\": \"xÃ¬ng nÃ©ng\", \"trans\": \"performance\"},\n    {\"word\": \"ç³»ç»Ÿ\", \"pinyin\": \"xÃ¬ tÇ’ng\", \"trans\": \"system\"},\n    {\"word\": \"æ¢è®¨\", \"pinyin\": \"tÃ n tÇo\", \"trans\": \"explore\"},\n    {\"word\": \"è§†è§‰\", \"pinyin\": \"shÃ¬ juÃ©\", \"trans\": \"visual\"},\n    {\"word\": \"ç¼–ç å™¨\", \"pinyin\": \"biÄn mÇ qÃ¬\", \"trans\": \"encoder\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹ jÃ¹ jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"é…ç½®\", \"pinyin\": \"pÃ¨i zhÃ¬\", \"trans\": \"configuration\"},\n    {\"word\": \"è¶‹åŠ¿\", \"pinyin\": \"qÅ« shÃ¬\", \"trans\": \"trend\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­ng gÅ«\", \"trans\": \"evaluate\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ« zhÇ”n\", \"trans\": \"benchmark\"},\n    {\"word\": \"è¡¨ç°\", \"pinyin\": \"biÇo xiÃ n\", \"trans\": \"performance\"},\n    {\"word\": \"å‡ºè‰²\", \"pinyin\": \"chÅ« sÃ¨\", \"trans\": \"outstanding\"},\n    {\"word\": \"å¤šå­¦ç§‘\", \"pinyin\": \"duÅ xuÃ© kÄ“\", \"trans\": \"multidisciplinary\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"reasoning\"},\n    {\"word\": \"æ–‡æ¡£\", \"pinyin\": \"wÃ©n dÃ ng\", \"trans\": \"document\"},\n    {\"word\": \"ç†è§£\", \"pinyin\": \"lÇ jiÄ›\", \"trans\": \"understanding\"},\n    {\"word\": \"å¤šå›¾åƒ\", \"pinyin\": \"duÅ tÃº xiÃ ng\", \"trans\": \"multi-image\"},\n    {\"word\": \"è§†é¢‘\", \"pinyin\": \"shÃ¬ pÃ¬n\", \"trans\": \"video\"},\n    {\"word\": \"å®é™…\", \"pinyin\": \"shÃ­ jÃ¬\", \"trans\": \"practical\"},\n    {\"word\": \"å¹»è§‰\", \"pinyin\": \"huÃ n juÃ©\", \"trans\": \"hallucination\"},\n    {\"word\": \"æ£€æµ‹\", \"pinyin\": \"jiÇn cÃ¨\", \"trans\": \"detection\"},\n    {\"word\": \"å®šä½\", \"pinyin\": \"dÃ¬ng wÃ¨i\", \"trans\": \"localization\"},\n    {\"word\": \"å¤šè¯­è¨€\", \"pinyin\": \"duÅ yÇ” yÃ¡n\", \"trans\": \"multilingual\"},\n    {\"word\": \"èƒ½åŠ›\", \"pinyin\": \"nÃ©ng lÃ¬\", \"trans\": \"ability\"},\n    {\"word\": \"çº¯è¯­è¨€\", \"pinyin\": \"chÃºn yÇ” yÃ¡n\", \"trans\": \"pure language\"},\n    {\"word\": \"å¤„ç†\", \"pinyin\": \"chÇ” lÇ\", \"trans\": \"process\"},\n    {\"word\": \"å¼€æº\", \"pinyin\": \"kÄi yuÃ¡n\", \"trans\": \"open-source\"},\n    {\"word\": \"ç¤¾åŒº\", \"pinyin\": \"shÃ¨ qÅ«\", \"trans\": \"community\"},\n    {\"word\": \"è´¡çŒ®\", \"pinyin\": \"gÃ²ng xiÃ n\", \"trans\": \"contribute\"},\n    {\"word\": \"æ ‡å‡†\", \"pinyin\": \"biÄo zhÇ”n\", \"trans\": \"standard\"},\n    {\"word\": \"å¼€å‘\", \"pinyin\": \"kÄi fÄ\", \"trans\": \"develop\"},\n    {\"word\": \"åº”ç”¨\", \"pinyin\": \"yÃ¬ng yÃ²ng\", \"trans\": \"apply\"},\n    {\"word\": \"äººå·¥æ™ºèƒ½\", \"pinyin\": \"rÃ©n gÅng zhÃ¬ nÃ©ng\", \"trans\": \"artificial intelligence\"},\n    {\"word\": \"ç³»ç»Ÿ\", \"pinyin\": \"xÃ¬ tÇ’ng\", \"trans\": \"system\"},\n    {\"word\": \"æ¼”ç¤º\", \"pinyin\": \"yÇn shÃ¬\", \"trans\": \"demonstration\"},\n    {\"word\": \"è§\", \"pinyin\": \"jiÃ n\", \"trans\": \"see\"}\n]",
        "trans": "We introduced InternVL 2.5, an advanced multimodal large language model series based on InternVL 2.0. It retains the core model architecture but features significant improvements in training and testing strategies, as well as data quality. We studied the relationship between model scaling and performance, systematically exploring performance trends in visual encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations, InternVL 2.5 has demonstrated outstanding performance across multiple benchmarks, including multidisciplinary reasoning, document understanding, multi-image/video understanding, practical understanding, multimodal hallucination detection, visual localization, multilingual capabilities, and pure language processing. Our model is the first open-source multimodal large language model to exceed 70% on the MMMU benchmark and has shown strong test-time scaling potential. We hope this model will contribute new standards to the open-source community for developing and applying multimodal AI systems. See the HuggingFace demo at https://huggingface.co/spaces/OpenGVLab/InternVL.",
        "update_ts": "2024-12-09 09:12"
    }
}