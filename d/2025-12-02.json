{
    "date": {
        "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 2",
        "zh": "12æœˆ2æ—¥"
    },
    "time_utc": "2025-12-02 09:00",
    "weekday": 1,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2025-12-02",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2511.18538",
            "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence",
            "url": "https://huggingface.co/papers/2511.18538",
            "abstract": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.",
            "score": 256,
            "issue_id": 1,
            "pub_date": "2025-11-23",
            "pub_date_card": {
                "ru": "23 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 23",
                "zh": "11æœˆ23æ—¥"
            },
            "hash": "3b469030942f9dcc",
            "authors": [
                "Jian Yang",
                "Xianglong Liu",
                "Weifeng Lv",
                "Ken Deng",
                "Shawn Guo",
                "Lin Jing",
                "Yizhi Li",
                "Shark Liu",
                "Xianzhen Luo",
                "Yuyu Luo",
                "Changzai Pan",
                "Ensheng Shi",
                "Yingshui Tan",
                "Renshuai Tao",
                "Jiajun Wu",
                "Xianjie Wu",
                "Zhenhe Wu",
                "Daoguang Zan",
                "Chenchen Zhang",
                "Wei Zhang",
                "He Zhu",
                "Terry Yue Zhuo",
                "Kerui Cao",
                "Xianfu Cheng",
                "Jun Dong",
                "Shengjie Fang",
                "Zhiwei Fei",
                "Xiangyuan Guan",
                "Qipeng Guo",
                "Zhiguang Han",
                "Joseph James",
                "Tianqi Luo",
                "Renyuan Li",
                "Yuhang Li",
                "Yiming Liang",
                "Congnan Liu",
                "Jiaheng Liu",
                "Qian Liu",
                "Ruitong Liu",
                "Tyler Loakman",
                "Xiangxin Meng",
                "Chuang Peng",
                "Tianhao Peng",
                "Jiajun Shi",
                "Mingjie Tang",
                "Boyang Wang",
                "Haowen Wang",
                "Yunli Wang",
                "Fanglin Xu",
                "Zihan Xu",
                "Fei Yuan",
                "Ge Zhang",
                "Jiayi Zhang",
                "Xinhao Zhang",
                "Wangchunshu Zhou",
                "Hualei Zhu",
                "King Zhu",
                "Bryan Dai",
                "Aishan Liu",
                "Zhoujun Li",
                "Chenghua Lin",
                "Tianyu Liu",
                "Chao Peng",
                "Kai Shen",
                "Libo Qin",
                "Shuangyong Song",
                "Zizheng Zhan",
                "Jiajun Zhang",
                "Jie Zhang",
                "Zhaoxiang Zhang",
                "Bo Zheng"
            ],
            "affiliations": [
                "Alibaba",
                "BIT",
                "BJTU",
                "BUAA-SKLCCSE",
                "BUPT",
                "ByteDance",
                "CASIA",
                "CSU",
                "HIT",
                "HKUST (GZ)",
                "HNU",
                "Huawei Cloud",
                "Kuaishou",
                "M-A-P",
                "Manchester",
                "Monash/CSIRO",
                "NJU",
                "NTU",
                "NUS",
                "OPPO",
                "PKU",
                "SCU",
                "Shanghai AI Lab",
                "StepFun",
                "TeleAI",
                "Tencent",
                "Ubiquant",
                "UoS",
                "ZJU"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.18538.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#optimization",
                    "#agents",
                    "#plp",
                    "#open_source",
                    "#benchmark",
                    "#data",
                    "#training",
                    "#survey",
                    "#rlhf"
                ],
                "emoji": "ğŸ’»",
                "ru": {
                    "title": "ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ³Ğ°Ğ¹Ğ´ Ğ¿Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°: Ğ¾Ñ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° (Code LLMs), Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ. ĞĞ½Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (GPT-4, Claude, LLaMA), Ñ‚Ğ°Ğº Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ° (StarCoder, Code LLaMA, DeepSeek-Coder), Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ Ğ² Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ (Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸) Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ (ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ´Ğ°, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ°Ğ¼Ğ¸). Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞµÑ€Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Unlocking the Power of Code LLMs: From Data to Deployment",
                    "desc": "This paper provides a detailed overview of the lifecycle of code large language models (LLMs), from data curation to deployment. It highlights the evolution of LLMs from rule-based systems to advanced Transformer architectures, showcasing their significant performance improvements in generating functional code from natural language. The authors analyze both general and code-specialized LLMs, discussing their capabilities, design choices, and the trade-offs involved in their development. Additionally, the paper addresses the gap between academic research and practical applications, emphasizing the importance of code correctness, security, and integration into existing workflows."
                },
                "zh": {
                    "title": "å…¨é¢è§£æä»£ç å¤§å‹è¯­è¨€æ¨¡å‹çš„ç”Ÿå‘½å‘¨æœŸ",
                    "desc": "è¿™ç¯‡è®ºæ–‡å…¨é¢ä»‹ç»äº†ä»£ç å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç”Ÿå‘½å‘¨æœŸï¼Œä»æ•°æ®æ•´ç†åˆ°éƒ¨ç½²ï¼Œæ¶µç›–äº†æŠ€æœ¯ã€æƒè¡¡å’Œç ”ç©¶ä¸å®è·µä¹‹é—´çš„å·®è·ã€‚å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å°†è‡ªç„¶è¯­è¨€æè¿°ç›´æ¥è½¬æ¢ä¸ºåŠŸèƒ½ä»£ç ï¼Œæå¤§åœ°æ”¹å˜äº†è‡ªåŠ¨åŒ–è½¯ä»¶å¼€å‘ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†é€šç”¨LLMså’Œä¸“é—¨é’ˆå¯¹ä»£ç çš„LLMsçš„èƒ½åŠ›ï¼Œæ¢è®¨äº†æ¨¡å‹è®­ç»ƒä¸­çš„å„ç§æŠ€æœ¯å’Œè®¾è®¡å†³ç­–ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒåˆ†æäº†ä»£ç é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„æ•ˆæœï¼Œæ­ç¤ºäº†å­¦æœ¯ç ”ç©¶ä¸å®é™…åº”ç”¨ä¹‹é—´çš„å·®è·ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.20785",
            "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling",
            "url": "https://huggingface.co/papers/2511.20785",
            "abstract": "LongVT, an end-to-end framework, enhances long video reasoning by interleaving global and local analysis using multimodal tools, outperforming existing methods on challenging benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .",
            "score": 150,
            "issue_id": 1,
            "pub_date": "2025-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "85fa1ecaaa8fcb34",
            "authors": [
                "Zuhao Yang",
                "Sudong Wang",
                "Kaichen Zhang",
                "Keming Wu",
                "Sicong Leng",
                "Yifan Zhang",
                "Bo Li",
                "Chengwei Qin",
                "Shijian Lu",
                "Xingxuan Li",
                "Lidong Bing"
            ],
            "affiliations": [
                "HKUST(GZ)",
                "LMMs-Lab Team",
                "MiroMind AI",
                "NTU",
                "THU"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.20785.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#agents",
                    "#long_context",
                    "#multimodal",
                    "#hallucinations",
                    "#open_source",
                    "#synthetic",
                    "#benchmark",
                    "#video",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ´ĞµĞ¾-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° LongVT â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ (Chain-of-Tool-Thought). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²ÑĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VideoSIAH Ñ 247.9K Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ 1,280 Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Enhancing Long Video Reasoning with Global-Local Analysis",
                    "desc": "LongVT is a novel framework designed to improve reasoning about long videos by combining global and local analysis through multimodal tools. It mimics human understanding by first skimming the entire video and then focusing on specific clips for detailed examination. The framework leverages large multimodal models' ability to ground temporal information, allowing it to effectively zoom in on relevant video segments. Additionally, LongVT addresses the lack of fine-grained question-answering data by introducing a comprehensive dataset, VideoSIAH, which supports both training and evaluation of the model."
                },
                "zh": {
                    "title": "é•¿è§†é¢‘æ¨ç†çš„æ–°çªç ´ï¼šLongVT",
                    "desc": "LongVTæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œé€šè¿‡äº¤æ›¿è¿›è¡Œå…¨å±€å’Œå±€éƒ¨åˆ†æï¼Œå¢å¼ºäº†é•¿è§†é¢‘æ¨ç†çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šæ¨¡æ€å·¥å…·ï¼Œæ¨¡ä»¿äººç±»ç†è§£é•¿è§†é¢‘çš„æ–¹å¼ï¼Œå…ˆè¿›è¡Œå…¨å±€æµè§ˆï¼Œå†æ·±å…¥ç›¸å…³ç‰‡æ®µè¿›è¡Œç»†èŠ‚åˆ†æã€‚LongVTåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„æ—¶é—´åŸºç¡€èƒ½åŠ›ï¼Œä½œä¸ºè§†é¢‘å‰ªè¾‘å·¥å…·ï¼Œèšç„¦äºç‰¹å®šè§†é¢‘ç‰‡æ®µå¹¶é‡æ–°é‡‡æ ·æ›´ç»†ç²’åº¦çš„å¸§ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒLongVTåœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿è§†é¢‘ç†è§£å’Œæ¨ç†åŸºå‡†ä¸Šï¼Œå§‹ç»ˆä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01816",
            "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights",
            "url": "https://huggingface.co/papers/2512.01816",
            "abstract": "A benchmark for chained text-to-multi-image generation assesses models' ability to model dynamic causal processes and world knowledge, revealing that unified multimodal models outperform specialized ones but still struggle with spatiotemporal consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.",
            "score": 88,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "c54687160c19944b",
            "authors": [
                "Juanxi Tian",
                "Siyuan Li",
                "Conghui He",
                "Lijun Wu",
                "Cheng Tan"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01816.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#multimodal",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ğ¸Ñ€Ñƒ: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Envision Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Envision-Score, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºÑƒ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 1000 Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¸Ñ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ T2I Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºĞ¾Ğ¹, Ğ½Ğ¾ Ğ½Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾ Ğ¼Ğ¸Ñ€Ğµ, Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ² Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ÑÑ‘ ĞµÑ‰Ñ‘ Ğ½Ğµ Ñ€ĞµÑˆĞ¸Ğ»Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºĞ°Ğ´Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "Envision: Advancing Multi-Image Generation with Causal Understanding",
                    "desc": "This paper introduces a benchmark called Envision for evaluating models that generate multiple images from text prompts, focusing on their ability to understand and represent dynamic processes over time. It highlights that while unified multimodal models perform better than specialized text-to-image models in creating coherent narratives, they still face challenges with maintaining consistency across time and space. The study proposes a new evaluation metric, Envision-Score, which assesses models on their consistency, physical realism, and aesthetic quality across multiple frames. The findings suggest that current training methods, which often emphasize single images, hinder models' capabilities to effectively model complex, evolving scenarios in the real world."
                },
                "zh": {
                    "title": "åŠ¨æ€å› æœå»ºæ¨¡çš„æ–°åŸºå‡†",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨é“¾å¼æ–‡æœ¬åˆ°å¤šå›¾åƒç”Ÿæˆä¸­çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åŠ¨æ€å› æœè¿‡ç¨‹å’Œä¸–ç•ŒçŸ¥è¯†çš„å»ºæ¨¡èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹åœ¨å› æœå™äº‹ä¸€è‡´æ€§æ–¹é¢ä¼˜äºä¸“é—¨çš„æ¨¡å‹ï¼Œä½†åœ¨æ—¶ç©ºä¸€è‡´æ€§æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡å¼•å…¥äº†EnvisionåŸºå‡†å’ŒEnvision-ScoreæŒ‡æ ‡ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨å¤šå¸§ç”Ÿæˆä¸­çš„è¡¨ç°ã€‚é€šè¿‡å¯¹15ä¸ªæ¨¡å‹çš„ç»¼åˆè¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œå°½ç®¡ç»Ÿä¸€æ¨¡å‹åœ¨å› æœå™äº‹ä¸Šè¡¨ç°æ›´å¥½ï¼Œä½†ä»ç„¶æ— æ³•å®Œå…¨å…‹æœæ—¶ç©ºä¸€è‡´æ€§çš„æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01374",
            "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices",
            "url": "https://huggingface.co/papers/2512.01374",
            "abstract": "The paper provides a theoretical foundation for optimizing sequence-level rewards in reinforcement learning using token-level objectives, highlighting the importance of techniques like importance sampling correction, clipping, and Routing Replay for stabilizing training, especially with large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.",
            "score": 85,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "c2bfee42fb0d45e8",
            "authors": [
                "Chujie Zheng",
                "Kai Dang",
                "Bowen Yu",
                "Mingze Li",
                "Huiqiang Jiang",
                "Junrong Lin",
                "Yuqiong Liu",
                "Hao Lin",
                "Chencan Wu",
                "Feng Hu",
                "An Yang",
                "Jingren Zhou",
                "Junyang Lin"
            ],
            "affiliations": [
                "Alibaba Inc."
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01374.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#alignment",
                    "#architecture",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "ĞÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğº Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼: Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· token-level objectives Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ½Ğ°Ñ Ñ†ĞµĞ»ĞµĞ²Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ¾Ğ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ°ĞºĞ¸Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº ĞºĞ°Ğº importance sampling correction, clipping Ğ¸ Routing Replay Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ 30B MoE Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ."
                },
                "en": {
                    "title": "Optimizing Sequence Rewards with Token-Level Objectives in RL",
                    "desc": "This paper presents a new approach to reinforcement learning (RL) that focuses on optimizing rewards at the sequence level using objectives at the token level. It emphasizes the significance of minimizing discrepancies between training and inference, as well as reducing policy staleness to ensure the effectiveness of the surrogate objective in policy gradient methods. The authors highlight key techniques such as importance sampling correction, clipping, and Routing Replay, which are essential for stabilizing training, especially in large language models. Through extensive experiments, they demonstrate that these methods lead to improved training stability and performance in RL tasks."
                },
                "zh": {
                    "title": "ä¼˜åŒ–åºåˆ—çº§å¥–åŠ±ï¼Œç¨³å®šå¼ºåŒ–å­¦ä¹ è®­ç»ƒ",
                    "desc": "æœ¬æ–‡ä¸ºå¼ºåŒ–å­¦ä¹ ä¸­çš„åºåˆ—çº§å¥–åŠ±ä¼˜åŒ–æä¾›äº†ç†è®ºåŸºç¡€ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ä»£ç†çš„æ ‡è®°çº§ç›®æ ‡å¯ä»¥ä¼˜åŒ–çœŸå®çš„åºåˆ—çº§å¥–åŠ±ï¼Œå°¤å…¶æ˜¯åœ¨æœ€å°åŒ–è®­ç»ƒä¸æ¨ç†ä¹‹é—´çš„å·®å¼‚å’Œç­–ç•¥è¿‡æ—¶çš„æƒ…å†µä¸‹ã€‚æ–‡ç« å¼ºè°ƒäº†é‡è¦æ€§é‡‡æ ·æ ¡æ­£ã€å‰ªåˆ‡å’Œè·¯ç”±é‡æ”¾ç­‰æŠ€æœ¯åœ¨ç¨³å®šè®­ç»ƒè¿‡ç¨‹ä¸­çš„é‡è¦æ€§ã€‚é€šè¿‡å¯¹30B MoEæ¨¡å‹çš„å¹¿æ³›å®éªŒï¼Œè¯æ˜äº†è¿™äº›æŠ€æœ¯åœ¨æé«˜è®­ç»ƒç¨³å®šæ€§å’ŒåŠ é€Ÿæ”¶æ•›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02014",
            "title": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
            "url": "https://huggingface.co/papers/2512.02014",
            "abstract": "TUNA, a unified multimodal model, uses a cascaded VAE and representation encoder for end-to-end multimodal understanding and generation, outperforming decoupled models and achieving state-of-the-art results across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.",
            "score": 60,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "8ea919f109941f5d",
            "authors": [
                "Zhiheng Liu",
                "Weiming Ren",
                "Haozhe Liu",
                "Zijian Zhou",
                "Shoufa Chen",
                "Haonan Qiu",
                "Xiaoke Huang",
                "Zhaochong An",
                "Fanny Yang",
                "Aditya Patel",
                "Viktar Atliha",
                "Tony Ng",
                "Xiao Han",
                "Chuyan Zhu",
                "Chenyang Zhang",
                "Ding Liu",
                "Juan-Manuel Perez-Rua",
                "Sen He",
                "JÃ¼rgen Schmidhuber",
                "Wenhu Chen",
                "Ping Luo",
                "Wei Liu",
                "Tao Xiang",
                "Jonas Schult",
                "Yuren Cong"
            ],
            "affiliations": [
                "HKU",
                "KAUST",
                "Meta BizAI",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02014.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01948",
            "title": "How Far Are We from Genuinely Useful Deep Research Agents?",
            "url": "https://huggingface.co/papers/2512.01948",
            "abstract": "FINDER is a benchmark for deep research agents with standardized human-curated tasks and DEFT is a failure taxonomy revealing that DRAs struggle with evidence integration, verification, and reasoning-resilient planning.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.",
            "score": 52,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "6008ac881bc21d5a",
            "authors": [
                "Dingling Zhang",
                "He Zhu",
                "Jincheng Ren",
                "Kangqi Song",
                "Xinran Zhou",
                "Boyu Feng",
                "Shudong Liu",
                "Jiabin Luo",
                "Weihao Xie",
                "Zhaohui Wang",
                "Tianrui Qin",
                "King Zhu",
                "Yuqing Wang",
                "Qianben Chen",
                "Yuchen Eleanor Jiang",
                "Wei Wang",
                "Jiaheng Liu",
                "Wangchunshu Zhou"
            ],
            "affiliations": [
                "Nanjing University",
                "OPPO",
                "OPPO AI Agent Team"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01948.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.00425",
            "title": "What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards",
            "url": "https://huggingface.co/papers/2512.00425",
            "abstract": "A physics-grounded post-training framework using verifiable rewards improves physical realism and motion quality in video diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent video diffusion models can synthesize visually compelling clips, yet often violate basic physical laws-objects float, accelerations drift, and collisions behave inconsistently-revealing a persistent gap between visual realism and physical realism. We propose NewtonRewards, the first physics-grounded post-training framework for video generation based on verifiable rewards. Instead of relying on human or VLM feedback, NewtonRewards extracts measurable proxies from generated videos using frozen utility models: optical flow serves as a proxy for velocity, while high-level appearance features serve as a proxy for mass. These proxies enable explicit enforcement of Newtonian structure through two complementary rewards: a Newtonian kinematic constraint enforcing constant-acceleration dynamics, and a mass conservation reward preventing trivial, degenerate solutions. We evaluate NewtonRewards on five Newtonian Motion Primitives (free fall, horizontal/parabolic throw, and ramp sliding down/up) using our newly constructed large-scale benchmark, NewtonBench-60K. Across all primitives in visual and physics metrics, NewtonRewards consistently improves physical plausibility, motion smoothness, and temporal coherence over prior post-training methods. It further maintains strong performance under out-of-distribution shifts in height, speed, and friction. Our results show that physics-grounded verifiable rewards offer a scalable path toward physics-aware video generation.",
            "score": 47,
            "issue_id": 1,
            "pub_date": "2025-11-29",
            "pub_date_card": {
                "ru": "29 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 29",
                "zh": "11æœˆ29æ—¥"
            },
            "hash": "5805666c28d224e2",
            "authors": [
                "Minh-Quan Le",
                "Yuanzhi Zhu",
                "Vicky Kalogeiton",
                "Dimitris Samaras"
            ],
            "affiliations": [
                "LIX, Ecole Polytechnique, CNRS, IPP",
                "Stony Brook University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.00425.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#benchmark",
                    "#diffusion",
                    "#video",
                    "#training",
                    "#science"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸ĞºĞ° Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº NewtonRewards Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸Ğ·Ğ¼ĞµÑ€Ğ¸Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ğ¸-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸, Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑÑ‹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²Ğ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ° Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°: Ğ½ÑŒÑÑ‚Ğ¾Ğ½Ğ¾Ğ²ÑĞºĞ¾Ğµ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñƒ Ğ·Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑÑÑ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ NewtonBench-60K Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ĞºĞ¾Ğ³ĞµÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Video Realism with Physics-Grounded Rewards",
                    "desc": "This paper introduces NewtonRewards, a novel framework that enhances video diffusion models by incorporating physics-based rewards to improve the realism of generated videos. It addresses common issues in video generation, such as unrealistic object behavior, by using measurable proxies like optical flow for velocity and appearance features for mass. The framework applies two key rewards: one that enforces constant-acceleration dynamics and another that ensures mass conservation, leading to more physically plausible motion. Evaluations on various motion scenarios demonstrate that NewtonRewards significantly enhances both visual quality and adherence to physical laws compared to previous methods."
                },
                "zh": {
                    "title": "åŸºäºç‰©ç†çš„å¥–åŠ±æå‡è§†é¢‘ç”ŸæˆçœŸå®æ„Ÿ",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç‰©ç†çš„åè®­ç»ƒæ¡†æ¶ï¼Œç§°ä¸ºNewtonRewardsï¼Œæ—¨åœ¨æé«˜è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç‰©ç†çœŸå®æ„Ÿå’Œè¿åŠ¨è´¨é‡ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯éªŒè¯çš„å¥–åŠ±æœºåˆ¶ï¼Œåˆ©ç”¨å…‰æµå’Œå¤–è§‚ç‰¹å¾ç­‰å¯æµ‹é‡çš„ä»£ç†ï¼Œæ¥å¼ºåˆ¶æ‰§è¡Œç‰›é¡¿è¿åŠ¨å®šå¾‹ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒNewtonRewardsåœ¨å¤šä¸ªç‰›é¡¿è¿åŠ¨åŸå‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æ”¹å–„äº†ç‰©ç†åˆç†æ€§ã€è¿åŠ¨å¹³æ»‘æ€§å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒåŸºäºç‰©ç†çš„å¯éªŒè¯å¥–åŠ±ä¸ºç‰©ç†æ„ŸçŸ¥çš„è§†é¢‘ç”Ÿæˆæä¾›äº†ä¸€æ¡å¯æ‰©å±•çš„è·¯å¾„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.20649",
            "title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout",
            "url": "https://huggingface.co/papers/2511.20649",
            "abstract": "A unified inference-time framework addresses core limitations of autoregressive video diffusion models, enabling infinite-horizon, controllable, and cinematic video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce infty-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish infty-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that infty-RoPE consistently surpasses previous autoregressive models in overall VBench scores.",
            "score": 45,
            "issue_id": 1,
            "pub_date": "2025-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "779895effebefaf5",
            "authors": [
                "Hidir Yesiltepe",
                "Tuna Han Salih Meral",
                "Adil Kaan Akan",
                "Kaan Oktay",
                "Pinar Yanardag"
            ],
            "affiliations": [
                "Virginia Tech"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.20649.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.00590",
            "title": "Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models",
            "url": "https://huggingface.co/papers/2512.00590",
            "abstract": "Wikontic is a multi-stage pipeline that constructs high-quality, ontology-consistent knowledge graphs from open-domain text, achieving state-of-the-art performance in information retention and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3times fewer than AriGraph and <1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.",
            "score": 38,
            "issue_id": 1,
            "pub_date": "2025-11-29",
            "pub_date_card": {
                "ru": "29 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 29",
                "zh": "11æœˆ29æ—¥"
            },
            "hash": "a5037d08b7ba5b54",
            "authors": [
                "Alla Chepurova",
                "Aydar Bulatov",
                "Yuri Kuratov",
                "Mikhail Burtsev"
            ],
            "affiliations": [
                "Cognitive AI Systems Lab, Moscow, Russia",
                "London Institute for Mathematical Sciences, London, UK",
                "Moscow Independent Research Institute of Artificial Intelligence, Moscow, Russia"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.00590.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark",
                    "#dataset",
                    "#graphs"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ½Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Wikontic â€” ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ñ‹ Ñ ĞºĞ²Ğ°Ğ»Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Wikidata. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¾Ğ½Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ°Ñ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑŒ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ² 96% ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ², Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MuSiQue Ğ¸ HotpotQA ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ° Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ¿Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Wikontic Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ: Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ğ° Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ² 3 Ñ€Ğ°Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡ĞµĞ¼ AriGraph, Ğ¸ Ğ² 20 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ GraphRAG."
                },
                "en": {
                    "title": "Building Better Knowledge Graphs Efficiently with Wikontic",
                    "desc": "Wikontic is a novel multi-stage pipeline designed to create high-quality knowledge graphs (KGs) from open-domain text. It extracts candidate triplets while applying constraints based on Wikidata to ensure ontology consistency and reduce entity duplication. The resulting KGs are compact and well-connected, achieving impressive performance metrics on benchmarks like MuSiQue and HotpotQA. This approach not only enhances the intrinsic quality of KGs but also demonstrates significant efficiency in construction compared to existing methods."
                },
                "zh": {
                    "title": "Wikonticï¼šæ„å»ºé«˜è´¨é‡çŸ¥è¯†å›¾è°±çš„åˆ›æ–°ç®¡é“",
                    "desc": "Wikontic æ˜¯ä¸€ä¸ªå¤šé˜¶æ®µçš„ç®¡é“ï¼Œæ—¨åœ¨ä»å¼€æ”¾é¢†åŸŸæ–‡æœ¬ä¸­æ„å»ºé«˜è´¨é‡ä¸”ç¬¦åˆæœ¬ä½“çš„çŸ¥è¯†å›¾è°±ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–å€™é€‰ä¸‰å…ƒç»„ã€å¼ºåˆ¶æ‰§è¡ŒåŸºäºWikidataçš„ç±»å‹å’Œå…³ç³»çº¦æŸï¼Œå¹¶è§„èŒƒåŒ–å®ä½“æ¥å‡å°‘é‡å¤ï¼Œä»è€Œæé«˜çŸ¥è¯†å›¾è°±çš„è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWikontic åœ¨ä¿¡æ¯ä¿ç•™å’Œæ•ˆç‡æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ MuSiQue å’Œ HotpotQA æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚è¯¥ç®¡é“ä¸ä»…æé«˜äº†ç”ŸæˆçŸ¥è¯†å›¾è°±çš„è´¨é‡ï¼Œè¿˜ä¸ºå¤§è¯­è¨€æ¨¡å‹æä¾›äº†å¯æ‰©å±•çš„ç»“æ„åŒ–çŸ¥è¯†è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.20614",
            "title": "The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment",
            "url": "https://huggingface.co/papers/2511.20614",
            "abstract": "ImageCritic addresses detail inconsistency in image generation through reference-guided post-editing, using attention alignment loss and a detail encoder.  \t\t\t\t\tAI-generated summary \t\t\t\t Previous works have explored various customized generation tasks given a reference image, but they still face limitations in generating consistent fine-grained details. In this paper, our aim is to solve the inconsistency problem of generated images by applying a reference-guided post-editing approach and present our ImageCritic. We first construct a dataset of reference-degraded-target triplets obtained via VLM-based selection and explicit degradation, which effectively simulates the common inaccuracies or inconsistencies observed in existing generation models. Furthermore, building on a thorough examination of the model's attention mechanisms and intrinsic representations, we accordingly devise an attention alignment loss and a detail encoder to precisely rectify inconsistencies. ImageCritic can be integrated into an agent framework to automatically detect inconsistencies and correct them with multi-round and local editing in complex scenarios. Extensive experiments demonstrate that ImageCritic can effectively resolve detail-related issues in various customized generation scenarios, providing significant improvements over existing methods.",
            "score": 38,
            "issue_id": 1,
            "pub_date": "2025-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "77a13a3e501505e1",
            "authors": [
                "Ziheng Ouyang",
                "Yiren Song",
                "Yaoli Liu",
                "Shihao Zhu",
                "Qibin Hou",
                "Ming-Ming Cheng",
                "Mike Zheng Shou"
            ],
            "affiliations": [
                "Show Lab, National University of Singapore",
                "State Key Laboratory of CAD&CG, Zhejiang University",
                "VCIP, Nankai University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.20614.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'dict'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.23404",
            "title": "LFM2 Technical Report",
            "url": "https://huggingface.co/papers/2511.23404",
            "abstract": "LFM2, a family of compact foundation models, achieves high efficiency and performance on-device through hardware-in-the-loop architecture search and advanced training techniques, supporting various tasks including multimodal applications.  \t\t\t\t\tAI-generated summary \t\t\t\t We present LFM2, a family of Liquid Foundation Models designed for efficient on-device deployment and strong task capabilities. Using hardware-in-the-loop architecture search under edge latency and memory constraints, we obtain a compact hybrid backbone that combines gated short convolutions with a small number of grouped query attention blocks, delivering up to 2x faster prefill and decode on CPUs compared to similarly sized models. The LFM2 family covers 350M-8.3B parameters, including dense models (350M, 700M, 1.2B, 2.6B) and a mixture-of-experts variant (8.3B total, 1.5B active), all with 32K context length. LFM2's training pipeline includes a tempered, decoupled Top-K knowledge distillation objective that avoids support mismatch; curriculum learning with difficulty-ordered data; and a three-stage post-training recipe of supervised fine-tuning, length-normalized preference optimization, and model merging. Pre-trained on 10-12T tokens, LFM2 models achieve strong results across diverse benchmarks; for example, LFM2-2.6B reaches 79.56% on IFEval and 82.41% on GSM8K. We further build multimodal and retrieval variants: LFM2-VL for vision-language tasks, LFM2-Audio for speech, and LFM2-ColBERT for retrieval. LFM2-VL supports tunable accuracy-latency tradeoffs via token-efficient visual processing, while LFM2-Audio separates audio input and output pathways to enable real-time speech-to-speech interaction competitive with models 3x larger. LFM2-ColBERT provides a low-latency encoder for queries and documents, enabling high-performance retrieval across multiple languages. All models are released with open weights and deployment packages for ExecuTorch, llama.cpp, and vLLM, making LFM2 a practical base for edge applications that need fast, memory-efficient inference and strong task capabilities.",
            "score": 34,
            "issue_id": 1,
            "pub_date": "2025-11-28",
            "pub_date_card": {
                "ru": "28 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 28",
                "zh": "11æœˆ28æ—¥"
            },
            "hash": "7efffacd214da4e1",
            "authors": [
                "Alexander Amini",
                "Anna Banaszak",
                "Harold Benoit",
                "Arthur BÃ¶Ã¶k",
                "Tarek Dakhran",
                "Song Duong",
                "Alfred Eng",
                "Fernando Fernandes",
                "Marc HÃ¤rkÃ¶nen",
                "Anne Harrington",
                "Ramin Hasani",
                "Saniya Karwa",
                "Yuri Khrustalev",
                "Maxime Labonne",
                "Mathias Lechner",
                "Valentine Lechner",
                "Simon Lee",
                "Zetian Li",
                "Noel Loo",
                "Jacob Marks",
                "Edoardo Mosca",
                "Samuel J. Paech",
                "Paul Pak",
                "Rom N. Parnichkun",
                "Alex Quach",
                "Ryan Rogers",
                "Daniela Rus",
                "Nayan Saxena",
                "Bettina Schlager",
                "Tim Seyde",
                "Jimmy T. H. Smith",
                "Aditya Tadimeti",
                "Neehal Tumma"
            ],
            "affiliations": [
                "Liquid AI"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.23404.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#open_source",
                    "#small_models",
                    "#multimodal",
                    "#multilingual",
                    "#audio",
                    "#transfer_learning",
                    "#rag",
                    "#architecture",
                    "#training",
                    "#cv"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…",
                    "desc": "LFM2 â€” ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¿ĞµÑ€Ğ¸Ñ„ĞµÑ€Ğ¸Ğ¹Ğ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ğ·Ğ°Ñ‚Ğ²Ğ¾Ñ€ĞµĞ½Ñ‹Ğµ ÑĞ²Ñ‘Ñ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ğ´Ğ²Ğ¾Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸: Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, curriculum learning Ğ¸ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ supervised fine-tuning Ğ¸ preference optimization. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ğµ, Ñ€ĞµÑ‡ÑŒ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞº, Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ…."
                },
                "en": {
                    "title": "LFM2: Efficient Foundation Models for On-Device Performance",
                    "desc": "LFM2 is a series of Liquid Foundation Models designed for efficient deployment on devices while maintaining high performance across various tasks. It utilizes a hardware-in-the-loop architecture search to create a compact model that combines gated convolutions and grouped query attention, achieving significant speed improvements on CPUs. The training process incorporates advanced techniques like knowledge distillation and curriculum learning, resulting in models that excel in benchmarks with up to 8.3 billion parameters. Additionally, LFM2 includes specialized variants for multimodal tasks, such as vision-language and speech processing, all optimized for low-latency and memory-efficient inference."
                },
                "zh": {
                    "title": "LFM2ï¼šé«˜æ•ˆç´§å‡‘çš„åŸºç¡€æ¨¡å‹",
                    "desc": "LFM2æ˜¯ä¸€ç³»åˆ—ç´§å‡‘å‹åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆçš„è®¾å¤‡ç«¯éƒ¨ç½²å’Œå¼ºå¤§çš„ä»»åŠ¡èƒ½åŠ›ã€‚é€šè¿‡ç¡¬ä»¶å¾ªç¯æ¶æ„æœç´¢ï¼ŒLFM2åœ¨è¾¹ç¼˜å»¶è¿Ÿå’Œå†…å­˜é™åˆ¶ä¸‹ï¼Œç»“åˆäº†é—¨æ§çŸ­å·ç§¯å’Œå°‘é‡åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›å—ï¼Œæ˜¾è‘—æé«˜äº†CPUä¸Šçš„é¢„å¡«å……å’Œè§£ç é€Ÿåº¦ã€‚è¯¥æ¨¡å‹å®¶æ—æ¶µç›–äº†350Måˆ°8.3Bå‚æ•°çš„å¤šç§å˜ä½“ï¼Œå¹¶åœ¨å¤šæ¨¡æ€åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ã€‚LFM2çš„è®­ç»ƒæµç¨‹åŒ…æ‹¬çŸ¥è¯†è’¸é¦ã€è¯¾ç¨‹å­¦ä¹ å’Œåè®­ç»ƒä¼˜åŒ–ï¼Œç¡®ä¿äº†åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¼˜å¼‚çš„ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.20549",
            "title": "Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning",
            "url": "https://huggingface.co/papers/2511.20549",
            "abstract": "Flash-DMD is a framework that combines efficient timestep-aware distillation and reinforcement learning to accelerate and stabilize the training of generative diffusion models, achieving high generation quality with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only 2.1% its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and text-image alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon.",
            "score": 24,
            "issue_id": 1,
            "pub_date": "2025-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "6fd00b19703b1770",
            "authors": [
                "Guanjie Chen",
                "Shirui Huang",
                "Kai Liu",
                "Jianchen Zhu",
                "Xiaoye Qu",
                "Peng Chen",
                "Yu Cheng",
                "Yifu Sun"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Shanghai Jiao Tong University",
                "Tencent",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.20549.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01925",
            "title": "Rectifying LLM Thought from Lens of Optimization",
            "url": "https://huggingface.co/papers/2512.01925",
            "abstract": "RePro, a novel process-level reward mechanism, enhances LLM reasoning by refining the optimization process underlying chain-of-thought prompting, thereby improving performance and reducing suboptimal behaviors.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.",
            "score": 23,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "577c6bc117eb679e",
            "authors": [
                "Junnan Liu",
                "Hongwei Liu",
                "Songyang Zhang",
                "Kai Chen"
            ],
            "affiliations": [
                "Monash University",
                "Shanghai AI Laboratory"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01925.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#rl",
                    "#alignment",
                    "#benchmark",
                    "#training",
                    "#rlhf"
                ],
                "emoji": "âš™ï¸",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "RePro â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¿ÑƒÑĞºĞ°, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¿ÑƒÑ‚Ğ¸ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. RePro Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ½ÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ· Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸, ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒĞº Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with RePro: A New Reward Mechanism",
                    "desc": "This paper introduces RePro, a new reward mechanism designed to improve the reasoning abilities of large language models (LLMs) during chain-of-thought (CoT) prompting. By viewing the reasoning process as an optimization problem, RePro refines how LLMs learn from their reasoning steps, aiming to reduce issues like overthinking and lengthy reasoning chains. The method employs a dual scoring system to evaluate the effectiveness and stability of the reasoning process, which is then used to create a composite reward for reinforcement learning. Experimental results show that RePro significantly boosts reasoning performance across various tasks, including mathematics and coding, while minimizing suboptimal behaviors."
                },
                "zh": {
                    "title": "ReProï¼šä¼˜åŒ–æ¨ç†è¿‡ç¨‹çš„åˆ›æ–°æœºåˆ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºReProçš„æ–°å‹è¿‡ç¨‹çº§å¥–åŠ±æœºåˆ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–é“¾å¼æ€ç»´æç¤ºçš„è¿‡ç¨‹æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡LLMåœ¨æ¨ç†èƒ½åŠ›ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†é•¿é“¾å¼æ€ç»´æç¤ºçš„æ¨¡å‹å¸¸å¸¸è¡¨ç°å‡ºä¸ç†æƒ³çš„æ¨ç†è¡Œä¸ºï¼Œå¦‚è¿‡åº¦æ€è€ƒå’Œæ¨ç†é“¾è¿‡é•¿ã€‚ReProé€šè¿‡å®šä¹‰ä¸€ä¸ªæ›¿ä»£ç›®æ ‡å‡½æ•°æ¥è¯„ä¼°é“¾å¼æ€ç»´çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶åˆ©ç”¨åŒé‡è¯„åˆ†æœºåˆ¶é‡åŒ–æ¨ç†çš„å¼ºåº¦å’Œç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReProåœ¨å¤šä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œä¸åŒLLMä¸Šå‡èƒ½æœ‰æ•ˆæå‡æ¨ç†æ€§èƒ½ï¼Œå‡å°‘ä¸ç†æƒ³çš„æ¨ç†è¡Œä¸ºã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01801",
            "title": "GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation",
            "url": "https://huggingface.co/papers/2512.01801",
            "abstract": "GR-RL enhances a vision-language-action policy for long-horizon dexterous manipulation through a multi-stage training pipeline that filters, augments, and refines demonstrations using reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present GR-RL, a robotic learning framework that turns a generalist vision-language-action (VLA) policy into a highly capable specialist for long-horizon dexterous manipulation. Assuming the optimality of human demonstrations is core to existing VLA policies. However, we claim that in highly dexterous and precise manipulation tasks, human demonstrations are noisy and suboptimal. GR-RL proposes a multi-stage training pipeline that filters, augments, and reinforces the demonstrations by reinforcement learning. First, GR-RL learns a vision-language-conditioned task progress, filters the demonstration trajectories, and only keeps the transitions that contribute positively to the progress. Specifically, we show that by directly applying offline RL with sparse reward, the resulting Q-values can be treated as a robust progress function. Next, we introduce morphological symmetry augmentation that greatly improves the generalization and performance of GR-RL. Lastly, to better align the VLA policy with its deployment behaviors for high-precision control, we perform online RL by learning a latent space noise predictor. With this pipeline, GR-RL is, to our knowledge, the first learning-based policy that can autonomously lace up a shoe by threading shoelaces through multiple eyelets with an 83.3% success rate, a task requiring long-horizon reasoning, millimeter-level precision, and compliant soft-body interaction. We hope GR-RL provides a step toward enabling generalist robot foundations models to specialize into reliable real-world experts.",
            "score": 23,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "4e3f0d787261c330",
            "authors": [
                "Yunfei Li",
                "Xiao Ma",
                "Jiafeng Xu",
                "Yu Cui",
                "Zhongren Cui",
                "Zhigang Han",
                "Liqun Huang",
                "Tao Kong",
                "Yuxiao Liu",
                "Hao Niu",
                "Wanli Peng",
                "Jingchao Qiao",
                "Zeyu Ren",
                "Haixin Shi",
                "Zhi Su",
                "Jiawen Tian",
                "Yuyang Xiao",
                "Shenyu Zhang",
                "Liwei Zheng",
                "Hang Li",
                "Yonghui Wu"
            ],
            "affiliations": [
                "bytedance.com"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01801.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01031",
            "title": "VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference",
            "url": "https://huggingface.co/papers/2512.01031",
            "abstract": "VLASH is an asynchronous inference framework for Vision-Language-Action models that achieves high speed and low latency without sacrificing accuracy, enabling precise robotic tasks like ping-pong and whack-a-mole.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action models (VLAs) are becoming increasingly capable across diverse robotic tasks. However, their real-world deployment remains slow and inefficient: demonstration videos are often sped up by 5-10x to appear smooth, with noticeable action stalls and delayed reactions to environmental changes. Asynchronous inference offers a promising solution to achieve continuous and low-latency control by enabling robots to execute actions and perform inference simultaneously. However, because the robot and environment continue to evolve during inference, a temporal misalignment arises between the prediction and execution intervals. This leads to significant action instability, while existing methods either degrade accuracy or introduce runtime overhead to mitigate it. We propose VLASH, a general asynchronous inference framework for VLAs that delivers smooth, accurate, and fast reaction control without additional overhead or architectural changes. VLASH estimates the future execution-time state by rolling the robot state forward with the previously generated action chunk, thereby bridging the gap between prediction and execution. Experiments show that VLASH achieves up to 2.03x speedup and reduces reaction latency by up to 17.4x compared to synchronous inference while fully preserving the original accuracy. Moreover, it empowers VLAs to handle fast-reaction, high-precision tasks such as playing ping-pong and playing whack-a-mole, where traditional synchronous inference fails. Code is available at https://github.com/mit-han-lab/vlash",
            "score": 23,
            "issue_id": 1,
            "pub_date": "2025-11-30",
            "pub_date_card": {
                "ru": "30 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 30",
                "zh": "11æœˆ30æ—¥"
            },
            "hash": "f1b7cfa9cbe8c74e",
            "authors": [
                "Jiaming Tang",
                "Yufei Sun",
                "Yilong Zhao",
                "Shang Yang",
                "Yujun Lin",
                "Zhuoyang Zhang",
                "James Hou",
                "Yao Lu",
                "Zhijian Liu",
                "Song Han"
            ],
            "affiliations": [
                "Caltech",
                "MIT",
                "NVIDIA",
                "Tsinghua University",
                "UC Berkeley",
                "UCSD"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01031.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01030",
            "title": "Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model",
            "url": "https://huggingface.co/papers/2512.01030",
            "abstract": "A two-stage deterministic framework, Lotus-2, leverages diffusion models' world priors for high-quality geometric inference, achieving state-of-the-art results in monocular depth estimation and competitive surface normal prediction with limited training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures. While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning. Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions. In this work, we propose Lotus-2, a two-stage deterministic framework for stable, accurate and fine-grained geometric dense prediction, aiming to provide an optimal adaption protocol to fully exploit the pre-trained generative priors. Specifically, in the first stage, the core predictor employs a single-step deterministic formulation with a clean-data objective and a lightweight local continuity module (LCM) to generate globally coherent structures without grid artifacts. In the second stage, the detail sharpener performs a constrained multi-step rectified-flow refinement within the manifold defined by the core predictor, enhancing fine-grained geometry through noise-free deterministic flow matching. Using only 59K training samples, less than 1% of existing large-scale datasets, Lotus-2 establishes new state-of-the-art results in monocular depth estimation and highly competitive surface normal prediction. These results demonstrate that diffusion models can serve as deterministic world priors, enabling high-quality geometric reasoning beyond traditional discriminative and generative paradigms.",
            "score": 17,
            "issue_id": 1,
            "pub_date": "2025-11-30",
            "pub_date_card": {
                "ru": "30 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 30",
                "zh": "11æœˆ30æ—¥"
            },
            "hash": "1fcea2ab96740203",
            "authors": [
                "Jing He",
                "Haodong Li",
                "Mingzhi Sheng",
                "Ying-Cong Chen"
            ],
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01030.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22989",
            "title": "MultiBanana: A Challenging Benchmark for Multi-Reference Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2511.22989",
            "abstract": "MultiBanana is a benchmark dataset for evaluating multi-reference text-to-image generation models across various challenging conditions, providing insights into model strengths and weaknesses.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent text-to-image generation models have acquired the ability of multi-reference generation and editing; the ability to inherit the appearance of subjects from multiple reference images and re-render them under new contexts. However, the existing benchmark datasets often focus on the generation with single or a few reference images, which prevents us from measuring the progress on how model performance advances or pointing out their weaknesses, under different multi-reference conditions. In addition, their task definitions are still vague, typically limited to axes such as \"what to edit\" or \"how many references are given\", and therefore fail to capture the intrinsic difficulty of multi-reference settings. To address this gap, we introduce MultiBanana, which is carefully designed to assesses the edge of model capabilities by widely covering multi-reference-specific problems at scale: (1) varying the number of references, (2) domain mismatch among references (e.g., photo vs. anime), (3) scale mismatch between reference and target scenes, (4) references containing rare concepts (e.g., a red banana), and (5) multilingual textual references for rendering. Our analysis among a variety of text-to-image models reveals their superior performances, typical failure modes, and areas for improvement. MultiBanana will be released as an open benchmark to push the boundaries and establish a standardized basis for fair comparison in multi-reference image generation. Our data and code are available at https://github.com/matsuolab/multibanana .",
            "score": 15,
            "issue_id": 1,
            "pub_date": "2025-11-28",
            "pub_date_card": {
                "ru": "28 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 28",
                "zh": "11æœˆ28æ—¥"
            },
            "hash": "9ad12b8ab3c5249e",
            "authors": [
                "Yuta Oshima",
                "Daiki Miyake",
                "Kohsei Matsutani",
                "Yusuke Iwasawa",
                "Masahiro Suzuki",
                "Yutaka Matsuo",
                "Hiroki Furuta"
            ],
            "affiliations": [
                "Google DeepMind",
                "The University of Tokyo"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22989.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#multimodal",
                    "#multilingual",
                    "#benchmark"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ°Ğ¼Ğ¸",
                    "desc": "MultiBanana â€” ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑÑ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ°Ğ±Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ñ‚Ğ°Ğº ĞºĞ°Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ°Ñ…. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸, Ñ€ĞµĞ´ĞºĞ¸Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… text-to-image Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ¸Ñ… Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ñ‡ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "MultiBanana: Elevating Multi-Reference Image Generation Evaluation",
                    "desc": "MultiBanana is a new benchmark dataset designed to evaluate text-to-image generation models that can use multiple reference images. It addresses the limitations of existing datasets by including various challenging conditions, such as different numbers of references and mismatches in domains and scales. This dataset helps identify the strengths and weaknesses of models in multi-reference scenarios, providing a clearer understanding of their performance. By offering a standardized way to assess these models, MultiBanana aims to advance research in multi-reference image generation."
                },
                "zh": {
                    "title": "MultiBananaï¼šå¤šå‚è€ƒç”Ÿæˆçš„æ ‡å‡†åŸºå‡†",
                    "desc": "MultiBananaæ˜¯ä¸€ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å¤šå‚è€ƒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å„ç§æŒ‘æˆ˜æ¡ä»¶ä¸‹çš„è¡¨ç°ã€‚è¯¥æ•°æ®é›†è§£å†³äº†ç°æœ‰åŸºå‡†æ•°æ®é›†åªå…³æ³¨å•ä¸€æˆ–å°‘é‡å‚è€ƒå›¾åƒçš„é—®é¢˜ï¼Œèƒ½å¤Ÿæ›´å…¨é¢åœ°è¡¡é‡æ¨¡å‹åœ¨å¤šå‚è€ƒæ¡ä»¶ä¸‹çš„æ€§èƒ½è¿›å±•å’Œå¼±ç‚¹ã€‚MultiBananaè®¾è®¡äº†å¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬å‚è€ƒæ•°é‡å˜åŒ–ã€å‚è€ƒä¹‹é—´çš„é¢†åŸŸä¸åŒ¹é…ã€å‚è€ƒä¸ç›®æ ‡åœºæ™¯çš„è§„æ¨¡ä¸åŒ¹é…ç­‰ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›æé™ã€‚é€šè¿‡å¯¹å¤šç§æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„åˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†å®ƒä»¬çš„ä¼˜è¶Šè¡¨ç°ã€å…¸å‹å¤±è´¥æ¨¡å¼å’Œæ”¹è¿›ç©ºé—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01342",
            "title": "InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision",
            "url": "https://huggingface.co/papers/2512.01342",
            "abstract": "InternVideo-Next uses a two-stage pretraining scheme with an Encoder-Predictor-Decoder framework to achieve state-of-the-art video representation learning by combining pixel-level fidelity and high-level semantics.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physical cues. In contrast, masked video modeling (MVM) directly exploits spatiotemporal structures but trails text-supervised methods on general tasks. We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning. To address these, we disentangle the traditional encoder-decoder design into an Encoder-Predictor-Decoder (EPD) framework, where the predictor acts as a latent world model, and propose InternVideo-Next, a two-stage pretraining scheme that builds a semantically consistent yet detail-preserving latent space for this world model. First, conventional linear decoder in pixel MVM enforces the predictor output latent to be linearly projected to, thus separable in pixel space, causing the conflict with semantic abstraction. Our Stage 1 proposes a conditional diffusion decoder and injects reliable image-level semantic priors to enhance semantics and convergence, thus bridging pixel-level fidelity with high-level semantic abstraction. Stage 2 further learns world knowledge by predicting frozen Stage 1 targets within this space, mitigating shortcut learning. Trained on public, unlabeled videos, InternVideo-Next achieves state-of-the-art results across benchmarks and provides a scalable path toward general video representation learning.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "28cef6cacc767f41",
            "authors": [
                "Chenting Wang",
                "Yuhan Zhu",
                "Yicheng Xu",
                "Jiange Yang",
                "Ziang Yan",
                "Yali Wang",
                "Yi Wang",
                "Limin Wang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "Shenzhen Institutes of Advanced Technology, China",
                "State Key Laboratory for Novel Software Technology, Nanjing University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01342.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#synthetic",
                    "#diffusion",
                    "#video",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞœĞ¸Ñ€ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ: Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ EPD",
                    "desc": "InternVideo-Next Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Encoder-Predictor-Decoder Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾ Ğ¼Ğ¸Ñ€Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Bridging Pixel Fidelity and Semantic Understanding in Video Learning",
                    "desc": "InternVideo-Next introduces a novel Encoder-Predictor-Decoder (EPD) framework for video representation learning that enhances both pixel fidelity and semantic understanding. The two-stage pretraining process first improves semantic consistency using a conditional diffusion decoder, which helps bridge the gap between low-level pixel details and high-level semantics. In the second stage, the model learns world knowledge by predicting targets from the first stage, reducing the risk of shortcut learning. This approach allows InternVideo-Next to achieve state-of-the-art performance on various benchmarks while being trained on large-scale, unlabeled video data."
                },
                "zh": {
                    "title": "è§†é¢‘è¡¨ç¤ºå­¦ä¹ çš„æ–°çªç ´",
                    "desc": "InternVideo-Nexté‡‡ç”¨äº†ä¸¤é˜¶æ®µçš„é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œç»“åˆäº†ç¼–ç å™¨-é¢„æµ‹å™¨-è§£ç å™¨æ¡†æ¶ï¼Œä»¥å®ç°å…ˆè¿›çš„è§†é¢‘è¡¨ç¤ºå­¦ä¹ ã€‚è¯¥æ–¹æ³•é€šè¿‡åƒç´ çº§çš„ä¿çœŸåº¦å’Œé«˜å±‚æ¬¡çš„è¯­ä¹‰ç»“åˆï¼Œå…‹æœäº†ä¼ ç»Ÿè§†é¢‘å»ºæ¨¡ä¸­çš„ä¸€äº›é—®é¢˜ã€‚ç¬¬ä¸€é˜¶æ®µå¼•å…¥æ¡ä»¶æ‰©æ•£è§£ç å™¨ï¼Œå¢å¼ºäº†è¯­ä¹‰ä¿¡æ¯å’Œæ”¶æ•›æ€§ï¼Œç¬¬äºŒé˜¶æ®µåˆ™é€šè¿‡é¢„æµ‹ç¬¬ä¸€é˜¶æ®µçš„ç›®æ ‡æ¥å­¦ä¹ ä¸–ç•ŒçŸ¥è¯†ï¼Œå‡å°‘äº†æ·å¾„å­¦ä¹ çš„å½±å“ã€‚æœ€ç»ˆï¼ŒInternVideo-Nextåœ¨å…¬å…±æ— æ ‡ç­¾è§†é¢‘ä¸Šè®­ç»ƒï¼Œè¾¾åˆ°äº†å„é¡¹åŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›ç»“æœã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.00891",
            "title": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression",
            "url": "https://huggingface.co/papers/2512.00891",
            "abstract": "STC, a hierarchical framework, optimizes streaming VideoLLMs by reducing ViT encoding and LLM pre-filling latency through token caching and pruning, without significantly impacting accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose Streaming Token Compression (STC), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: STC-Cacher, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and STC-Pruner, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to 99\\% of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by 24.5\\% and 45.3\\%.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2025-11-30",
            "pub_date_card": {
                "ru": "30 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 30",
                "zh": "11æœˆ30æ—¥"
            },
            "hash": "d0a9394aad841d34",
            "authors": [
                "Yiyu Wang",
                "Xuyang Liu",
                "Xiyan Gui",
                "Xinying Lin",
                "Boxue Yang",
                "Chenfei Liao",
                "Tailai Chen",
                "Linfeng Zhang"
            ],
            "affiliations": [
                "EPIC Lab, Shanghai Jiao Tong University",
                "Hong Kong University of Science and Technology (Guangzhou)",
                "Huazhong University of Science and Technology",
                "Sichuan University",
                "Sun Yat-sen University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.00891.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.00722",
            "title": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs",
            "url": "https://huggingface.co/papers/2512.00722",
            "abstract": "SpeContext leverages a distilled language model for efficient long-context reasoning, reducing parameters and improving throughput with minimal accuracy loss in both cloud and edge environments.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2025-11-30",
            "pub_date_card": {
                "ru": "30 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 30",
                "zh": "11æœˆ30æ—¥"
            },
            "hash": "ad2a9e6128a8e0f8",
            "authors": [
                "Jiaming Xu",
                "Jiayi Pan",
                "Hanzhen Wang",
                "Yongkang Zhou",
                "Jiancai Ye",
                "Yu Wang",
                "Guohao Dai"
            ],
            "affiliations": [
                "Infinigence-AI",
                "SII Shanghai",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.00722.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'list'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.23342",
            "title": "Flow Straighter and Faster: Efficient One-Step Generative Modeling via MeanFlow on Rectified Trajectories",
            "url": "https://huggingface.co/papers/2511.23342",
            "abstract": "Rectified MeanFlow enables efficient one-step sampling for flow-based generative models by modeling mean velocity fields with a single reflow step and a truncation heuristic, improving both sample quality and training efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Flow-based generative models have recently demonstrated strong performance, yet sampling typically relies on expensive numerical integration of ordinary differential equations (ODEs). Rectified Flow enables one-step sampling by learning nearly straight probability paths, but achieving such straightness requires multiple computationally intensive reflow iterations. MeanFlow achieves one-step generation by directly modeling the average velocity over time; however, when trained on highly curved flows, it suffers from slow convergence and noisy supervision. To address these limitations, we propose Rectified MeanFlow, a framework that models the mean velocity field along the rectified trajectory using only a single reflow step. This eliminates the need for perfectly straightened trajectories while enabling efficient training. Furthermore, we introduce a simple yet effective truncation heuristic that aims to reduce residual curvature and further improve performance. Extensive experiments on ImageNet at 64, 256, and 512 resolutions show that Re-MeanFlow consistently outperforms prior one-step flow distillation and Rectified Flow methods in both sample quality and training efficiency. Code is available at https://github.com/Xinxi-Zhang/Re-MeanFlow.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2025-11-28",
            "pub_date_card": {
                "ru": "28 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 28",
                "zh": "11æœˆ28æ—¥"
            },
            "hash": "082bdf59d8d5a59b",
            "authors": [
                "Xinxi Zhang",
                "Shiwei Tan",
                "Quang Nguyen",
                "Quan Dao",
                "Ligong Han",
                "Xiaoxiao He",
                "Tunyu Zhang",
                "Alen Mrdovic",
                "Dimitris Metaxas"
            ],
            "affiliations": [
                "Red Hat AI Innovation",
                "Rutgers University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.23342.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization",
                    "#open_source",
                    "#diffusion",
                    "#training"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞĞ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Rectified MeanFlow â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¹ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ€ĞµĞ´Ğ½ĞµĞµ Ğ¿Ğ¾Ğ»Ğµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ´Ğ¾Ğ»ÑŒ ÑĞ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºÑƒ ÑƒÑĞµÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ ĞºÑ€Ğ¸Ğ²Ğ¸Ğ·Ğ½Ñƒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ImageNet Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² ĞºĞ°Ğº Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Efficient One-Step Sampling with Rectified MeanFlow",
                    "desc": "Rectified MeanFlow is a novel approach for improving flow-based generative models by enabling efficient one-step sampling. It achieves this by modeling the mean velocity fields with a single reflow step, which simplifies the sampling process and enhances training efficiency. The method also incorporates a truncation heuristic to minimize residual curvature, leading to better sample quality. Experimental results demonstrate that Rectified MeanFlow outperforms existing methods in generating high-quality samples across various resolutions."
                },
                "zh": {
                    "title": "é«˜æ•ˆä¸€æ­¥é‡‡æ ·ï¼Œæå‡ç”Ÿæˆæ¨¡å‹æ€§èƒ½",
                    "desc": "Rectified MeanFlow æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åŸºäºæµçš„ç”Ÿæˆæ¨¡å‹çš„é‡‡æ ·æ•ˆç‡ã€‚å®ƒé€šè¿‡ä»…ä½¿ç”¨ä¸€æ­¥é‡æµæ­¥éª¤æ¥å»ºæ¨¡å¹³å‡é€Ÿåº¦åœºï¼Œä»è€Œå®ç°é«˜è´¨é‡çš„æ ·æœ¬ç”Ÿæˆã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨é«˜åº¦å¼¯æ›²æµåŠ¨ä¸­è®­ç»ƒæ—¶çš„æ”¶æ•›ç¼“æ…¢å’Œå™ªå£°ç›‘ç£é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRectified MeanFlow åœ¨æ ·æœ¬è´¨é‡å’Œè®­ç»ƒæ•ˆç‡ä¸Šå‡ä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.17282",
            "title": "Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation",
            "url": "https://huggingface.co/papers/2511.17282",
            "abstract": "Current multilingual text-to-image models often generate culturally neutral or English-biased images, and two proposed alignment strategies improve cultural consistency without compromising fidelity and diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Multilingual text-to-image (T2I) models have advanced rapidly in terms of visual realism and semantic alignment, and are now widely utilized. Yet outputs vary across cultural contexts: because language carries cultural connotations, images synthesized from multilingual prompts should preserve cross-lingual cultural consistency. We conduct a comprehensive analysis showing that current T2I models often produce culturally neutral or English-biased results under multilingual prompts. Analyses of two representative models indicate that the issue stems not from missing cultural knowledge but from insufficient activation of culture-related representations. We propose a probing method that localizes culture-sensitive signals to a small set of neurons in a few fixed layers. Guided by this finding, we introduce two complementary alignment strategies: (1) inference-time cultural activation that amplifies the identified neurons without backbone fine-tuned; and (2) layer-targeted cultural enhancement that updates only culturally relevant layers. Experiments on our CultureBench demonstrate consistent improvements over strong baselines in cultural consistency while preserving fidelity and diversity.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2025-11-21",
            "pub_date_card": {
                "ru": "21 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 21",
                "zh": "11æœˆ21æ—¥"
            },
            "hash": "e200da6a2c8bffac",
            "authors": [
                "Chuancheng Shi",
                "Shangze Li",
                "Shiming Guo",
                "Simiao Xie",
                "Wenhua Wu",
                "Jingtong Dou",
                "Chao Wu",
                "Canran Xiao",
                "Cong Wang",
                "Zifeng Cheng",
                "Fei Shen",
                "Tat-Seng Chua"
            ],
            "affiliations": [
                "Central South University",
                "Nanjing University",
                "Nanjing University of Science and Technology",
                "National University of Singapore",
                "The University of Sydney"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.17282.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01949",
            "title": "Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2512.01949",
            "abstract": "Script, a plug-and-play token pruning method for multimodal large language models, improves efficiency and accuracy by removing redundant and irrelevant visual tokens without retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "01abe4e9fb13ed8d",
            "authors": [
                "Zhongyu Yang",
                "Dannong Xu",
                "Wei Pang",
                "Yingfang Yuan"
            ],
            "affiliations": [
                "BCML, Heriot-Watt University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01949.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01420",
            "title": "PromptBridge: Cross-Model Prompt Transfer for Large Language Models",
            "url": "https://huggingface.co/papers/2512.01420",
            "abstract": "PromptBridge is a framework that facilitates prompt transfer between different LLMs to maintain performance without re-optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "09e6c31ee61b1a21",
            "authors": [
                "Yaxuan Wang",
                "Quan Liu",
                "Zhenting Wang",
                "Zichao Li",
                "Wei Wei",
                "Yang Liu",
                "Yujia Bao"
            ],
            "affiliations": [
                "Center for Advanced AI, Accenture",
                "University of California, Santa Cruz"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01420.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'list'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.00466",
            "title": "SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling",
            "url": "https://huggingface.co/papers/2512.00466",
            "abstract": "SCALE selectively allocates computational resources for large language models based on sub-problem difficulty, improving performance and resource utilization.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time compute scaling has emerged as a powerful paradigm for enhancing mathematical reasoning in large language models (LLMs) by allocating additional computational resources during inference. However, current methods employ uniform resource distribution across all reasoning sub-problems, creating fundamental bottlenecks where challenging sub-problems receive insufficient attention while routine operations consume disproportionate resources. This uniform allocation creates performance bottlenecks where additional computational resources yield diminishing returns. Inspired by dual-process theory, we propose SCALE (Selective Resource Allocation), a framework that selectively allocates computational resources based on sub-problem difficulty. SCALE operates through four stages: (1) problem decomposition into sequential reasoning sub-problems, (2) difficulty assessment of each sub-problem to distinguish between routine operations and computationally challenging sub-problems, (3) selective processing mode assignment between System 1 for simple sub-problems and System 2 for complex ones, and (4) sequential execution with context propagation. By concentrating resources on challenging sub-problems while processing routine operations efficiently, SCALE achieves substantial performance improvements with superior resource utilization. Extensive experiments demonstrate that SCALE significantly outperforms uniform scaling baselines, achieving accuracy improvements of up to 13.75 percentage points (57.50% to 71.25% on AIME25) while reducing computational costs by 33%-53%, representing a major advance in test-time scaling that addresses fundamental limitations of current approaches.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2025-11-29",
            "pub_date_card": {
                "ru": "29 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 29",
                "zh": "11æœˆ29æ—¥"
            },
            "hash": "39c06c8894ca58f8",
            "authors": [
                "Yang Xiao",
                "Chunpu Xu",
                "Ruifeng Yuan",
                "Jiashuo Wang",
                "Wenjie Li",
                "Pengfei Liu"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "The Hong Kong Polytechnic University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.00466.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸",
                    "desc": "SCALE â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸Ğ· Ğ½Ğ¸Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸: Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ…. Ğ’Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸ĞµĞ¹ Ğ´Ğ²Ğ¾Ğ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ½Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ…, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 13.75 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° 33-53 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "SCALE: Smart Resource Allocation for Better AI Performance",
                    "desc": "The paper introduces SCALE, a framework designed to enhance the performance of large language models (LLMs) by selectively allocating computational resources based on the difficulty of reasoning sub-problems. Unlike traditional methods that distribute resources uniformly, SCALE assesses each sub-problem's difficulty and assigns processing modes accordingly, optimizing resource use. This approach allows for more attention to be given to complex sub-problems while efficiently handling simpler ones. Experimental results show that SCALE significantly improves accuracy and reduces computational costs compared to existing uniform scaling methods."
                },
                "zh": {
                    "title": "æ ¹æ®éš¾åº¦é€‰æ‹©æ€§åˆ†é…èµ„æºï¼Œæå‡æ¨¡å‹æ€§èƒ½",
                    "desc": "SCALEï¼ˆé€‰æ‹©æ€§èµ„æºåˆ†é…ï¼‰æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æ ¹æ®å­é—®é¢˜çš„éš¾åº¦æ¥åˆ†é…è®¡ç®—èµ„æºï¼Œä»è€Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å’Œèµ„æºåˆ©ç”¨ç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºé¡ºåºæ¨ç†å­é—®é¢˜ï¼Œå¹¶è¯„ä¼°æ¯ä¸ªå­é—®é¢˜çš„éš¾åº¦ï¼Œæ¥åŒºåˆ†å¸¸è§„æ“ä½œå’Œè®¡ç®—ä¸Šå…·æœ‰æŒ‘æˆ˜æ€§çš„å­é—®é¢˜ã€‚SCALEé‡‡ç”¨ä¸¤ç§å¤„ç†æ¨¡å¼ï¼Œç®€å•å­é—®é¢˜ä½¿ç”¨ç³»ç»Ÿ1ï¼Œå¤æ‚å­é—®é¢˜ä½¿ç”¨ç³»ç»Ÿ2ï¼Œä»è€Œåœ¨å¤„ç†å¸¸è§„æ“ä½œçš„åŒæ—¶é›†ä¸­èµ„æºäºå…·æœ‰æŒ‘æˆ˜æ€§çš„å­é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSCALEåœ¨å‡†ç¡®æ€§å’Œè®¡ç®—æˆæœ¬æ–¹é¢å‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å‡åŒ€èµ„æºåˆ†é…æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01707",
            "title": "StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos",
            "url": "https://huggingface.co/papers/2512.01707",
            "abstract": "StreamGaze is a benchmark that evaluates how effectively models use gaze signals for temporal and proactive reasoning in streaming videos, revealing limitations in current MLLM capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "530f337a057a59f1",
            "authors": [
                "Daeun Lee",
                "Subhojyoti Mukherjee",
                "Branislav Kveton",
                "Ryan A. Rossi",
                "Viet Dac Lai",
                "Seunghyun Yoon",
                "Trung Bui",
                "Franck Dernoncourt",
                "Mohit Bansal"
            ],
            "affiliations": [
                "Adobe Research",
                "UNC Chapel Hill"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01707.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#open_source",
                    "#multimodal",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "Ğ’Ğ·Ğ³Ğ»ÑĞ´ ĞºĞ°Ğº ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ",
                    "desc": "StreamGaze â€” ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ° Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ° Ğ¸Ğ· ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ MLLM Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ° Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğµ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "StreamGaze: Enhancing Video Understanding with Gaze Signals",
                    "desc": "StreamGaze is a new benchmark that tests how well machine learning models can use gaze signals to understand and predict actions in streaming videos. It focuses on the ability of models to not only analyze video frames as they come in but also to anticipate what a user might want to do next, which is important for applications like augmented reality. The benchmark includes tasks that require models to track where a person is looking and to infer their intentions based on previous and current video frames. The results show that current models have significant gaps in performance compared to humans, highlighting the need for improvements in gaze-based reasoning and intention prediction."
                },
                "zh": {
                    "title": "StreamGazeï¼šè¯„ä¼°æ³¨è§†ä¿¡å·åœ¨æµåª’ä½“è§†é¢‘ç†è§£ä¸­çš„åº”ç”¨",
                    "desc": "StreamGazeæ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨æµåª’ä½“è§†é¢‘ä¸­å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨æ³¨è§†ä¿¡å·è¿›è¡Œæ—¶é—´æ¨ç†å’Œä¸»åŠ¨æ¨ç†ã€‚è¯¥åŸºå‡†å¡«è¡¥äº†ç°æœ‰å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨ç†è§£äººç±»æ³¨è§†ä¿¡å·æ–¹é¢çš„ç©ºç™½ã€‚StreamGazeè®¾è®¡äº†æ³¨è§†å¼•å¯¼çš„ä»»åŠ¡ï¼Œå…¨é¢è¯„ä¼°æ¨¡å‹åœ¨æµåª’ä½“è§†é¢‘ç†è§£ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰çš„MLLMåœ¨åŸºäºæ³¨è§†çš„æ—¶é—´æ¨ç†å’Œæ„å›¾å»ºæ¨¡æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œæ­ç¤ºäº†å…¶åŸºæœ¬å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22891",
            "title": "ORION: Teaching Language Models to Reason Efficiently in the Language of Thought",
            "url": "https://huggingface.co/papers/2511.22891",
            "abstract": "ORION models enhance reasoning efficiency and cost-effectiveness by compressing reasoning steps into ultra-compressed structured tokens, reducing latency and training costs while maintaining high accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose \"thinking\" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.",
            "score": 7,
            "issue_id": 1,
            "pub_date": "2025-11-28",
            "pub_date_card": {
                "ru": "28 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 28",
                "zh": "11æœˆ28æ—¥"
            },
            "hash": "01e5f56ba2c22228",
            "authors": [
                "Kumar Tanmay",
                "Kriti Aggarwal",
                "Paul Pu Liang",
                "Subhabrata Mukherjee"
            ],
            "affiliations": [
                "Harvard University",
                "Hippocratic AI",
                "Massachusetts Institute of Technology"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22891.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#rlhf",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ: Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ, ĞºĞ°Ğº ÑĞ·Ñ‹Ğº Ğ¼Ğ°ÑˆĞ¸Ğ½Ñ‹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ORION, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ğ¾Ğ¹ Language of Thought, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ»ÑŒÑ‚Ñ€Ğ°ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Mentalese, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO) - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ÑĞµÑ‚ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ORION Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² 4-16 Ñ€Ğ°Ğ·, ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 5 Ñ€Ğ°Ğ· Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² 7-9 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ DeepSeek R1, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ 90-98% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ Mentalese Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Efficient Reasoning with Ultra-Compressed Tokens",
                    "desc": "The ORION models improve reasoning efficiency in machine learning by using ultra-compressed structured tokens, which significantly reduce the number of reasoning steps needed. This approach minimizes latency and training costs while still achieving high accuracy in tasks like mathematics and code generation. By implementing a reinforcement learning method called SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), the models are trained to favor concise reasoning that remains correct, allowing for a balance between brevity and detail. The results demonstrate that ORION models can produce reasoning traces with fewer tokens and lower inference latency, outperforming existing models while maintaining a high level of accuracy."
                },
                "zh": {
                    "title": "ORIONæ¨¡å‹ï¼šé«˜æ•ˆæ¨ç†çš„æ–°çªç ´",
                    "desc": "ORIONæ¨¡å‹é€šè¿‡å°†æ¨ç†æ­¥éª¤å‹ç¼©ä¸ºè¶…å‹ç¼©çš„ç»“æ„åŒ–æ ‡è®°ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡å’Œæˆæœ¬æ•ˆç›Šã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†å»¶è¿Ÿå’Œè®­ç»ƒæˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†é«˜å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æå‡ºçš„çŸ­é•¿åº¦åå¥½ä¼˜åŒ–ï¼ˆSLPOï¼‰æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå¥–åŠ±ç®€æ´ä¸”æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œä»è€Œå®ç°æ›´é«˜çš„å‹ç¼©ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒORIONæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ¨ç†è¿‡ç¨‹ä¸­çš„æ ‡è®°æ•°é‡å‡å°‘äº†4-16å€ï¼Œæ¨ç†å»¶è¿Ÿé™ä½äº†5å€ï¼Œè®­ç»ƒæˆæœ¬å‡å°‘äº†7-9å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01827",
            "title": "CauSight: Learning to Supersense for Visual Causal Discovery",
            "url": "https://huggingface.co/papers/2512.01827",
            "abstract": "A novel vision-language model, CauSight, performs visual causal discovery by inferring cause-and-effect relationships in images, outperforming GPT-4.1 with a significant performance boost.  \t\t\t\t\tAI-generated summary \t\t\t\t Causal thinking enables humans to understand not just what is seen, but why it happens. To replicate this capability in modern AI systems, we introduce the task of visual causal discovery. It requires models to infer cause-and-effect relations among visual entities across diverse scenarios instead of merely perceiving their presence. To this end, we first construct the Visual Causal Graph dataset (VCG-32K), a large-scale collection of over 32,000 images annotated with entity-level causal graphs, and further develop CauSight, a novel vision-language model to perform visual causal discovery through causally aware reasoning. Our training recipe integrates three components: (1) training data curation from VCG-32K, (2) Tree-of-Causal-Thought (ToCT) for synthesizing reasoning trajectories, and (3) reinforcement learning with a designed causal reward to refine the reasoning policy. Experiments show that CauSight outperforms GPT-4.1 on visual causal discovery, achieving over a threefold performance boost (21% absolute gain). Our code, model, and dataset are fully open-sourced at project page: https://github.com/OpenCausaLab/CauSight.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "6b358e1fca62d57e",
            "authors": [
                "Yize Zhang",
                "Meiqi Chen",
                "Sirui Chen",
                "Bo Peng",
                "Yanxi Zhang",
                "Tianyu Li",
                "Chaochao Lu"
            ],
            "affiliations": [
                "Peking University",
                "Shanghai AI Laboratory",
                "Shanghai Innovation Institute",
                "Shanghai Jiao Tong University",
                "Tongji University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01827.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01763",
            "title": "HiconAgent: History Context-aware Policy Optimization for GUI Agents",
            "url": "https://huggingface.co/papers/2512.01763",
            "abstract": "HiconAgent, a GUI agent using HCPO, efficiently utilizes historical context for better performance in navigation tasks with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical User Interface (GUI) agents require effective use of historical context to perform sequential navigation tasks. While incorporating past actions and observations can improve decision making, naive use of full history leads to excessive computational overhead and distraction from irrelevant information. To address this, we introduce HiconAgent, a GUI agent trained with History Context-aware Policy Optimization (HCPO) for efficient and effective utilization of historical information. HCPO optimizes history usage in both sampling and policy updates through two complementary components: (1) Dynamic Context Sampling (DCS) presents the agent with variable length histories during sampling, enabling adaptive use of the most relevant context; (2) Anchor-guided History Compression (AHC) refines the policy update phase with a dual branch strategy where the compressed branch removes history observations while keeping history actions as information flow anchors. The compressed and uncompressed branches are coupled through a history-enhanced alignment loss to enforce consistent history usage while maintaining efficiency. Experiments on mainstream GUI navigation benchmarks demonstrate strong performance. Despite being smaller, HiconAgent-3B outperforms GUI-R1-7B by +8.46 percent grounding accuracy and +11.32 percent step success rate on GUI-Odyssey, while achieving comparable results on AndroidControl and AITW with up to 2.47x computational speedup and 60 percent FLOPs reduction.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "a6467c6d25d1a401",
            "authors": [
                "Xurui Zhou",
                "Gongwei Chen",
                "Yuquan Xie",
                "Zaijing Li",
                "Kaiwen Zhou",
                "Shuai Wang",
                "Shuo Yang",
                "Zhuotao Tian",
                "Rui Shao"
            ],
            "affiliations": [
                "Harbin Institute of Technology, Shenzhen",
                "Huawei Noahs Ark Lab"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01763.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#agents",
                    "#small_models",
                    "#long_context",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ HiconAgent â€” Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ History Context-aware Policy Optimization (HCPO). ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ³Ñ€ÑƒĞ·Ğ¾Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹: Dynamic Context Sampling Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¸ Anchor-guided History Compression Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ HiconAgent Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Efficient Navigation with Historical Context Optimization",
                    "desc": "HiconAgent is a graphical user interface (GUI) agent that leverages historical context to enhance navigation tasks while minimizing computational costs. It employs History Context-aware Policy Optimization (HCPO) to optimize how historical information is used during both sampling and policy updates. The agent utilizes Dynamic Context Sampling (DCS) to adaptively select relevant historical data and Anchor-guided History Compression (AHC) to streamline policy updates by compressing unnecessary historical observations. Experimental results show that HiconAgent outperforms larger models in accuracy and efficiency, achieving significant improvements in grounding accuracy and computational speed."
                },
                "zh": {
                    "title": "é«˜æ•ˆåˆ©ç”¨å†å²ä¸Šä¸‹æ–‡çš„å¯¼èˆªä»£ç†",
                    "desc": "HiconAgentæ˜¯ä¸€ç§ä½¿ç”¨å†å²ä¸Šä¸‹æ–‡ä¼˜åŒ–çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ä»£ç†ï¼Œæ—¨åœ¨æé«˜å¯¼èˆªä»»åŠ¡çš„æ€§èƒ½å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚é€šè¿‡å†å²ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ï¼ˆHCPOï¼‰ï¼ŒHiconAgentæœ‰æ•ˆåˆ©ç”¨å†å²ä¿¡æ¯ï¼Œé¿å…äº†ç®€å•ä½¿ç”¨å®Œæ•´å†å²æ‰€å¸¦æ¥çš„è®¡ç®—å¼€é”€å’Œæ— å…³ä¿¡æ¯çš„å¹²æ‰°ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬åŠ¨æ€ä¸Šä¸‹æ–‡é‡‡æ ·ï¼ˆDCSï¼‰å’Œé”šå¼•å¯¼å†å²å‹ç¼©ï¼ˆAHCï¼‰ï¼Œå‰è€…åœ¨é‡‡æ ·æ—¶æä¾›å¯å˜é•¿åº¦çš„å†å²ï¼Œåè€…åœ¨ç­–ç•¥æ›´æ–°é˜¶æ®µé€šè¿‡åŒåˆ†æ”¯ç­–ç•¥ä¼˜åŒ–å†å²ä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHiconAgentåœ¨å¤šä¸ªä¸»æµå¯¼èˆªåŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°½ç®¡æ¨¡å‹è¾ƒå°ï¼Œä½†åœ¨å‡†ç¡®æ€§å’ŒæˆåŠŸç‡ä¸Šå‡è¶…è¶Šäº†æ›´å¤§çš„å¯¹æ‰‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01103",
            "title": "Learning Eigenstructures of Unstructured Data Manifolds",
            "url": "https://huggingface.co/papers/2512.01103",
            "abstract": "A new deep learning framework learns spectral bases directly from unstructured data, eliminating the need for traditional operator selection and eigendecomposition, and providing a data-driven approach to geometry processing.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce a novel framework that directly learns a spectral basis for shape and manifold analysis from unstructured data, eliminating the need for traditional operator selection, discretization, and eigensolvers. Grounded in optimal-approximation theory, we train a network to decompose an implicit approximation operator by minimizing the reconstruction error in the learned basis over a chosen distribution of probe functions. For suitable distributions, they can be seen as an approximation of the Laplacian operator and its eigendecomposition, which are fundamental in geometry processing. Furthermore, our method recovers in a unified manner not only the spectral basis, but also the implicit metric's sampling density and the eigenvalues of the underlying operator. Notably, our unsupervised method makes no assumption on the data manifold, such as meshing or manifold dimensionality, allowing it to scale to arbitrary datasets of any dimension. On point clouds lying on surfaces in 3D and high-dimensional image manifolds, our approach yields meaningful spectral bases, that can resemble those of the Laplacian, without explicit construction of an operator. By replacing the traditional operator selection, construction, and eigendecomposition with a learning-based approach, our framework offers a principled, data-driven alternative to conventional pipelines. This opens new possibilities in geometry processing for unstructured data, particularly in high-dimensional spaces.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-30",
            "pub_date_card": {
                "ru": "30 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 30",
                "zh": "11æœˆ30æ—¥"
            },
            "hash": "a3726da8955fb5b3",
            "authors": [
                "Roy Velich",
                "Arkadi Piven",
                "David BensaÃ¯d",
                "Daniel Cremers",
                "Thomas DagÃ¨s",
                "Ron Kimmel"
            ],
            "affiliations": [
                "Munich Center for Machine Learning",
                "Technical University of Munich",
                "Technion - Israel Institute of Technology"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01103.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#architecture",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸŒŠ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸ĞµÑÑ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¸ÑÑ‹: Ğ¾Ñ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ²",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¸ÑĞ¾Ğ² Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ñ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµĞ»ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹. ĞĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ² Ğ²Ñ‹ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ±Ğ°Ğ·Ğ¸ÑĞµ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ›Ğ°Ğ¿Ğ»Ğ°ÑĞ° Ğ¸ ĞµĞ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ½ĞµÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… - Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞµÑ‚ĞºĞ¸ Ğ¸Ğ»Ğ¸ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¾Ğ±Ğ»Ğ°ĞºĞ°Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Learning Spectral Bases from Data: A New Era in Geometry Processing",
                    "desc": "This paper presents a new deep learning framework that learns spectral bases directly from unstructured data, bypassing traditional methods like operator selection and eigendecomposition. The framework is based on optimal-approximation theory, where a neural network is trained to minimize reconstruction error in the learned basis. It effectively approximates the Laplacian operator and its eigenvalues, which are crucial for geometry processing. This unsupervised method is versatile, as it does not rely on assumptions about the data's structure, making it applicable to various datasets, including high-dimensional point clouds and image manifolds."
                },
                "zh": {
                    "title": "ä»æ•°æ®ä¸­å­¦ä¹ è°±åŸºï¼Œé©æ–°å‡ ä½•å¤„ç†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥ç›´æ¥ä»éç»“æ„åŒ–æ•°æ®ä¸­å­¦ä¹ è°±åŸºï¼Œçœå»äº†ä¼ ç»Ÿçš„ç®—å­é€‰æ‹©å’Œç‰¹å¾åˆ†è§£çš„æ­¥éª¤ã€‚è¿™ç§æ–¹æ³•åŸºäºæœ€ä¼˜è¿‘ä¼¼ç†è®ºï¼Œé€šè¿‡æœ€å°åŒ–åœ¨å­¦ä¹ åŸºä¸Šçš„é‡æ„è¯¯å·®æ¥è®­ç»ƒç½‘ç»œï¼Œä»è€Œåˆ†è§£éšå¼è¿‘ä¼¼ç®—å­ã€‚æˆ‘ä»¬çš„æ— ç›‘ç£æ–¹æ³•ä¸å¯¹æ•°æ®æµå½¢åšä»»ä½•å‡è®¾ï¼Œèƒ½å¤Ÿé€‚åº”ä»»æ„ç»´åº¦çš„æ•°æ®é›†ï¼Œå¹¶åœ¨ä¸‰ç»´ç‚¹äº‘å’Œé«˜ç»´å›¾åƒæµå½¢ä¸Šäº§ç”Ÿæœ‰æ„ä¹‰çš„è°±åŸºã€‚é€šè¿‡ç”¨å­¦ä¹ æ–¹æ³•æ›¿ä»£ä¼ ç»Ÿçš„ç®—å­é€‰æ‹©å’Œç‰¹å¾åˆ†è§£ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸ºéç»“æ„åŒ–æ•°æ®çš„å‡ ä½•å¤„ç†æä¾›äº†ä¸€ç§æ–°çš„æ•°æ®é©±åŠ¨çš„è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.00762",
            "title": "Seeing the Wind from a Falling Leaf",
            "url": "https://huggingface.co/papers/2512.00762",
            "abstract": "An end-to-end differentiable inverse graphics framework recovers force representations from video observations, enabling physics-based video generation and editing.  \t\t\t\t\tAI-generated summary \t\t\t\t A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing a leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap between vision and physics. Please check more video results in our https://chaoren2357.github.io/seeingthewind/{project page}.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-30",
            "pub_date_card": {
                "ru": "30 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 30",
                "zh": "11æœˆ30æ—¥"
            },
            "hash": "832644c20249e539",
            "authors": [
                "Zhiyuan Gao",
                "Jiageng Mao",
                "Hong-Xing Yu",
                "Haozhe Lou",
                "Emily Yue-Ting Jia",
                "Jernej Barbic",
                "Jiajun Wu",
                "Yue Wang"
            ],
            "affiliations": [
                "Stanford University",
                "University of Southern California"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.00762.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'dict'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.00369",
            "title": "POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models",
            "url": "https://huggingface.co/papers/2512.00369",
            "abstract": "POLARIS, a novel approach based on diffusion models, minimizes noise approximation errors during image inversion by treating the guidance scale as a step-wise variable, thus enhancing the quality and accuracy of image editing and restoration tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale Ï‰ as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks.",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-29",
            "pub_date_card": {
                "ru": "29 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 29",
                "zh": "11æœˆ29æ—¥"
            },
            "hash": "5adc306118272017",
            "authors": [
                "Wenshuo Chen",
                "Haosen Li",
                "Shaofeng Liang",
                "Lei Wang",
                "Haozhe Jia",
                "Kaishen Yuan",
                "Jieming Wu",
                "Bowen Tian",
                "Yutao Yue"
            ],
            "affiliations": [
                "Data61/CSIRO",
                "Griffith University",
                "The Hong Kong University of Science and Technology (Guangzhou)"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.00369.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#architecture",
                    "#optimization",
                    "#cv"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ¼ guidance Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "POLARIS â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸: Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¾Ğ², Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñƒ Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ± guidance ĞºĞ°Ğº Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²ÑƒÑ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ. POLARIS Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Image Quality with POLARIS: A Step-Wise Approach to Noise Reduction",
                    "desc": "POLARIS is a new method that improves image editing and restoration by addressing noise approximation errors in diffusion models. It identifies that these errors occur when the noise at one step is estimated based on the previous step, leading to cumulative inaccuracies. Instead of just adjusting the embeddings to correct these errors, POLARIS reformulates the problem to focus on the source of the errors. By treating the guidance scale as a variable that changes with each step, POLARIS effectively reduces noise errors and enhances the quality of the images produced with minimal additional computational cost."
                },
                "zh": {
                    "title": "POLARISï¼šæå‡å›¾åƒåæ¼”è´¨é‡çš„æ–°æ–¹æ³•",
                    "desc": "POLARISæ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å°†å¼•å¯¼å°ºåº¦è§†ä¸ºé€æ­¥å˜é‡æ¥æœ€å°åŒ–å›¾åƒåæ¼”è¿‡ç¨‹ä¸­çš„å™ªå£°è¿‘ä¼¼è¯¯å·®ã€‚è¿™ç§æ–¹æ³•æé«˜äº†å›¾åƒç¼–è¾‘å’Œä¿®å¤ä»»åŠ¡çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚æˆ‘ä»¬é‡æ–°å®¡è§†äº†åæ¼”-å»å™ªèŒƒå¼ï¼Œå‘ç°é‡å»ºé€€åŒ–çš„ä¸€ä¸ªå…³é”®å› ç´ æ˜¯è¿‘ä¼¼å™ªå£°è¯¯å·®ã€‚POLARISé€šè¿‡å°†åæ¼”é—®é¢˜é‡æ–°å®šä¹‰ä¸ºè¯¯å·®æ¥æºé—®é¢˜ï¼Œæ˜¾è‘—æ”¹å–„äº†åæ¼”æ½œåœ¨è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22396",
            "title": "Asking like Socrates: Socrates helps VLMs understand remote sensing images",
            "url": "https://huggingface.co/papers/2511.22396",
            "abstract": "RS-EoT, a language-driven iterative visual evidence-seeking paradigm, addresses pseudo reasoning in remote sensing tasks by using a self-play multi-agent system and a two-stage RL strategy, achieving state-of-the-art performance on RS VQA benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent multimodal reasoning models, inspired by DeepSeek-R1, have significantly advanced vision-language systems. However, in remote sensing (RS) tasks, we observe widespread pseudo reasoning: models narrate the process of reasoning rather than genuinely reason toward the correct answer based on visual evidence. We attribute this to the Glance Effect, where a single, coarse perception of large-scale RS imagery results in incomplete understanding and reasoning based on linguistic self-consistency instead of visual evidence. To address this, we propose RS-EoT (Remote Sensing Evidence-of-Thought), a language-driven, iterative visual evidence-seeking paradigm. To instill this paradigm, we propose SocraticAgent, a self-play multi-agent system that synthesizes reasoning traces via alternating cycles of reasoning and visual inspection. To enhance and generalize these patterns, we propose a two-stage progressive RL strategy: first, RL on fine-grained Grounding tasks to enhance RS-EoT capabilities, followed by RL on RS VQA to generalize to broader understanding scenarios. Experiments show RS-EoT achieves state-of-the-art performance on multiple RS VQA and grounding benchmarks. Analyses reveal clear iterative cycles of reasoning and evidence seeking, confirming RS-EoT mitigates the Glance Effect and enables genuine evidence-grounded reasoning. Our code, data, and models are available at https://geox-lab.github.io/Asking_like_Socrates",
            "score": 4,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "2d231be8eaf06663",
            "authors": [
                "Run Shao",
                "Ziyu Li",
                "Zhaoyang Zhang",
                "Linrui Xu",
                "Xinran He",
                "Hongyuan Yuan",
                "Bolei He",
                "Yongxing Dai",
                "Yiming Yan",
                "Yijun Chen",
                "Wang Guo",
                "Haifeng Li"
            ],
            "affiliations": [
                "Baidu Inc., Beijing, China",
                "School of Earth Sciences, Zhejiang University, Hangzhou, China",
                "School of Geosciences and Info-Physics, Central South University, Changsha, China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22396.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#open_source",
                    "#rl",
                    "#multimodal",
                    "#benchmark",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ¡Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ² Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° RS-EoT, Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ÑĞµĞ²Ğ´Ğ¾Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¸Ğ³Ñ€Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ SocraticAgent, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‚ Ñ†Ğ¸ĞºĞ»Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½ÑĞ¿ĞµĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²Ğ¾Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° RL Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¼ĞµĞ»ĞºĞ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ RL Ğ½Ğ° VQA Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RS-EoT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ±ĞµĞ³Ğ»Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²."
                },
                "en": {
                    "title": "Empowering Remote Sensing with Evidence-Driven Reasoning",
                    "desc": "The paper introduces RS-EoT, a novel approach designed to improve reasoning in remote sensing tasks by focusing on visual evidence rather than relying on superficial linguistic consistency. It utilizes a self-play multi-agent system called SocraticAgent, which iteratively alternates between reasoning and visual inspection to create more accurate reasoning traces. The method employs a two-stage reinforcement learning (RL) strategy to first refine the model's capabilities on detailed grounding tasks and then generalize its understanding to remote sensing visual question answering (RS VQA). Results demonstrate that RS-EoT outperforms existing models, effectively reducing the Glance Effect and promoting genuine evidence-based reasoning."
                },
                "zh": {
                    "title": "RS-EoTï¼šçœŸå®æ¨ç†çš„è¯æ®å¯»æ±‚æ–°èŒƒå¼",
                    "desc": "RS-EoTæ˜¯ä¸€ç§åŸºäºè¯­è¨€çš„è¿­ä»£è§†è§‰è¯æ®å¯»æ±‚èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³é¥æ„Ÿä»»åŠ¡ä¸­çš„ä¼ªæ¨ç†é—®é¢˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨è‡ªæˆ‘å¯¹å¼ˆçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå’Œä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†é¥æ„Ÿè§†è§‰é—®ç­”çš„æ€§èƒ½ã€‚é€šè¿‡SocraticAgentï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†å’Œè§†è§‰æ£€æŸ¥ä¹‹é—´äº¤æ›¿è¿›è¡Œï¼Œä»è€Œåˆæˆæ¨ç†è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRS-EoTåœ¨å¤šä¸ªé¥æ„Ÿè§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæœ‰æ•ˆç¼“è§£äº†Glance Effectï¼Œä¿ƒè¿›äº†åŸºäºè¯æ®çš„çœŸå®æ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01945",
            "title": "Agentic Policy Optimization via Instruction-Policy Co-Evolution",
            "url": "https://huggingface.co/papers/2512.01945",
            "abstract": "INSPO, a novel Instruction-Policy co-evolution framework, dynamically optimizes instructions within the reinforcement learning loop, enhancing performance in multi-turn retrieval and reasoning tasks compared to static instruction-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop. INSPO maintains a dynamic population of instruction candidates that are sampled with questions, where reward signals in RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through an on-policy reflection mechanism, where an LLM-based optimizer analyzes past experience from a replay buffer and evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward more strategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "af31cfe485fc2158",
            "authors": [
                "Han Zhou",
                "Xingchen Wan",
                "Ivan VuliÄ‡",
                "Anna Korhonen"
            ],
            "affiliations": [
                "Language Technology Lab, University of Cambridge",
                "Machine Learning Research Group, University of Oxford"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01945.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#optimization",
                    "#agents",
                    "#rl",
                    "#training"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº INSPO Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ, INSPO Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ†Ğ¸ĞºĞ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ²-Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, ÑĞ»Ğ°Ğ±Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ ÑƒĞ´Ğ°Ğ»ÑÑÑ‚ÑÑ, Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ INSPO Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Dynamic Instruction Optimization for Enhanced Learning",
                    "desc": "INSPO is a new framework that improves how instructions are used in reinforcement learning by making them adaptable. Instead of using fixed instructions, INSPO dynamically optimizes them during the learning process, which helps agents perform better in complex tasks. It does this by maintaining a pool of instruction candidates and using feedback from the agent's performance to refine these instructions. This approach leads to better reasoning and retrieval capabilities in agents, showing significant improvements over traditional static methods."
                },
                "zh": {
                    "title": "åŠ¨æ€ä¼˜åŒ–æŒ‡ä»¤ï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼",
                    "desc": "INSPOæ˜¯ä¸€ç§æ–°é¢–çš„æŒ‡ä»¤-ç­–ç•¥å…±åŒè¿›åŒ–æ¡†æ¶ï¼Œå®ƒåœ¨å¼ºåŒ–å­¦ä¹ å¾ªç¯ä¸­åŠ¨æ€ä¼˜åŒ–æŒ‡ä»¤ï¼Œä»è€Œæå‡å¤šè½®æ£€ç´¢å’Œæ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸é™æ€æŒ‡ä»¤æ–¹æ³•ç›¸æ¯”ï¼ŒINSPOé€šè¿‡ç»´æŠ¤åŠ¨æ€çš„æŒ‡ä»¤å€™é€‰ç¾¤ä½“ï¼Œè‡ªåŠ¨å°†å¥–åŠ±ä¿¡å·åˆ†é…ç»™æ¯ä¸ªæŒ‡ä»¤ï¼Œå¹¶å®šæœŸå‰”é™¤è¡¨ç°ä¸ä½³çš„æŒ‡ä»¤ã€‚æ–°çš„æŒ‡ä»¤é€šè¿‡åŸºäºç­–ç•¥çš„åæ€æœºåˆ¶ç”Ÿæˆå’ŒéªŒè¯ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹åˆ†æè¿‡å»çš„ç»éªŒï¼Œè¿›è€Œæ¼”åŒ–å‡ºæ›´æœ‰æ•ˆçš„ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒINSPOåœ¨å¤šè½®æ£€ç´¢å’Œæ¨ç†ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºä¾èµ–é™æ€æŒ‡ä»¤çš„å¼ºåŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01830",
            "title": "OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic",
            "url": "https://huggingface.co/papers/2512.01830",
            "abstract": "OpenREAD, a vision-language model-based framework, enhances autonomous driving through end-to-end reinforcement fine-tuning, improving both reasoning and planning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, two-stage fine-tuning strategies, e.g., acquiring essential driving knowledge through supervised fine-tuning (SFT) and further enhancing decision-making and planning via reinforcement fine-tuning (RFT), have shown strong potential in advancing the knowledge-driven autonomous driving (AD) paradigm. However, the learning nature of SFT still limits the generalization of reasoning, thereby constraining the full potential of driving performance. Meanwhile, current RFT approaches are primarily applied to downstream tasks, since scene understanding is an open-ended problem where corresponding rewards are difficult to quantify. To address these limitations, we propose OpenREAD, an OPEN-ended REasoning reinforced vision-language model (VLM)-based autonomous driving (AD) framework that enables end-to-end RFT across the full spectrum from high-level reasoning to low-level trajectory planning. Specifically, we begin by constructing large-scale Chain-of-Thought (CoT) annotations on open-source driving-related knowledge datasets, and employ the powerful Qwen3 large language model (LLM) as the critic in RFT to quantify reasoning quality for open-ended questions during reward modeling. Extensive experiments confirm that joint end-to-end RFT yields substantial improvements in both upstream and downstream tasks, enabling OpenREAD to achieve state-of-the-art performance on reasoning and planning benchmarks.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "3a64799531f1b41a",
            "authors": [
                "Songyan Zhang",
                "Wenhui Huang",
                "Zhan Chen",
                "Chua Jiahao Collister",
                "Qihang Huang",
                "Chen Lv"
            ],
            "affiliations": [
                "Harvard University, USA",
                "Nanyang Technological University, Singapore"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01830.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#rl",
                    "#multimodal",
                    "#benchmark",
                    "#training",
                    "#rlhf",
                    "#cv"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "ĞÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğº Ğ¼Ğ°Ğ½ĞµĞ²Ñ€Ğ°Ğ¼: end-to-end Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "OpenREAD â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ-ÑĞ·Ñ‹ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· end-to-end Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RFT). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ supervised fine-tuning (SFT) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ reinforcement fine-tuning Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Chain-of-Thought Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ LLM Qwen3 Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ end-to-end Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Autonomous Driving with OpenREAD: Enhanced Reasoning and Planning",
                    "desc": "OpenREAD is a novel framework that integrates vision and language models to enhance autonomous driving through a unique end-to-end reinforcement fine-tuning approach. It addresses the limitations of traditional supervised fine-tuning by allowing for improved reasoning and planning capabilities in driving scenarios. By utilizing large-scale Chain-of-Thought annotations and the Qwen3 large language model as a critic, OpenREAD effectively quantifies reasoning quality for complex driving tasks. Experimental results demonstrate that this framework significantly boosts performance in both high-level reasoning and low-level trajectory planning, setting new benchmarks in the field."
                },
                "zh": {
                    "title": "OpenREADï¼šæå‡è‡ªåŠ¨é©¾é©¶çš„æ¨ç†ä¸è§„åˆ’èƒ½åŠ›",
                    "desc": "OpenREADæ˜¯ä¸€ä¸ªåŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œé€šè¿‡ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ¥æå‡è‡ªåŠ¨é©¾é©¶çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å¾®è°ƒï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨æ¨ç†å’Œè§„åˆ’æ–¹é¢çš„å±€é™æ€§ã€‚OpenREADåˆ©ç”¨å¤§è§„æ¨¡çš„æ€ç»´é“¾æ³¨é‡Šå’Œå¼ºå¤§çš„è¯­è¨€æ¨¡å‹æ¥é‡åŒ–æ¨ç†è´¨é‡ï¼Œä»è€Œæ”¹å–„å†³ç­–è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOpenREADåœ¨æ¨ç†å’Œè§„åˆ’åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02008",
            "title": "The Art of Scaling Test-Time Compute for Large Language Models",
            "url": "https://huggingface.co/papers/2512.02008",
            "abstract": "Systematic study of test-time scaling strategies in large language models reveals distinct performance trends based on problem difficulty, model type, and compute budget.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling (TTS) -- the dynamic allocation of compute during inference -- is a promising direction for improving reasoning in large language models (LLMs). However, a systematic comparison of well-known TTS strategies under identical conditions is missing, and the influence of model type and problem difficulty on performance remains unclear. To address these gaps, we conduct the first large-scale study of TTS, spanning over thirty billion tokens generated using eight open-source LLMs (7B to 235B parameters), across four reasoning datasets. We observe three consistent trends: (1) no single TTS strategy universally dominates; (2) reasoning models exhibit distinct trace-quality patterns across problem difficulty and trace length, forming short-horizon and long-horizon categories; and (3) for a given model type, the optimal TTS performance scales monotonically with compute budget. Based on these insights, we provide a practical recipe for selecting the best TTS strategy, considering problem difficulty, model type, and compute budget, providing a practical guide to effective inference-time scaling.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "f5a5b9a0022d308d",
            "authors": [
                "Aradhye Agarwal",
                "Ayan Sengupta",
                "Tanmoy Chakraborty"
            ],
            "affiliations": [
                "Indian Institute of Technology Delhi",
                "Microsoft Research"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02008.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#reasoning",
                    "#optimization",
                    "#open_source",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (TTS) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ»Ğ¸ Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ñ€Ğ¸Ğ´Ñ†Ğ°Ñ‚Ğ¸ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²Ğ¾Ğº Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ğ¸Ğ¿Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Optimizing Inference: Tailoring TTS Strategies for LLMs",
                    "desc": "This paper investigates test-time scaling (TTS) strategies for large language models (LLMs) to enhance their reasoning capabilities during inference. The study analyzes over thirty billion tokens from eight different LLMs, focusing on how model type and problem difficulty affect performance. The findings reveal that no single TTS strategy is superior across all scenarios, and that reasoning models show varying performance patterns based on the complexity of the tasks. Additionally, the research provides guidelines for selecting the most effective TTS strategy based on specific conditions, such as compute budget and model characteristics."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ¨ç†çš„åŠ¨æ€è®¡ç®—åˆ†é…ç­–ç•¥",
                    "desc": "æœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹åœ¨æµ‹è¯•æ—¶çš„ç¼©æ”¾ç­–ç•¥ï¼Œæ­ç¤ºäº†åŸºäºé—®é¢˜éš¾åº¦ã€æ¨¡å‹ç±»å‹å’Œè®¡ç®—é¢„ç®—çš„ä¸åŒæ€§èƒ½è¶‹åŠ¿ã€‚æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰æ˜¯æŒ‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œè¿™ä¸€æ–¹å‘æœ‰åŠ©äºæå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œæ²¡æœ‰å•ä¸€çš„TTSç­–ç•¥èƒ½å¤Ÿæ™®éé€‚ç”¨ï¼Œæ¨ç†æ¨¡å‹åœ¨ä¸åŒé—®é¢˜éš¾åº¦å’Œè½¨è¿¹é•¿åº¦ä¸‹è¡¨ç°å‡ºæ˜æ˜¾çš„è´¨é‡æ¨¡å¼ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæœ¬æ–‡æä¾›äº†ä¸€ç§å®ç”¨çš„æ–¹æ³•ï¼Œå¸®åŠ©é€‰æ‹©æœ€ä½³çš„TTSç­–ç•¥ï¼Œä»¥åº”å¯¹ä¸åŒçš„è®¡ç®—é¢„ç®—å’Œé—®é¢˜éš¾åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01481",
            "title": "ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling",
            "url": "https://huggingface.co/papers/2512.01481",
            "abstract": "ChronosObserver generates high-fidelity, 3D-consistent, and time-synchronized multi-view videos using a training-free approach that leverages World State Hyperspace and Hyperspace Guided Sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is a pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, a training-free method including World State Hyperspace to represent the spatiotemporal constraints of a 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3D-consistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "d3c1d6d18de15fce",
            "authors": [
                "Qisen Wang",
                "Yifan Zhao",
                "Peisen Shen",
                "Jialu Li",
                "Jia Li"
            ],
            "affiliations": [
                "State Key Laboratory of Virtual Reality Technology and Systems, SCSE, Beihang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01481.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ 4D Ğ¼Ğ¸Ñ€Ğ°",
                    "desc": "ChronosObserver â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ World State Hyperspace Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ 4D ÑÑ†ĞµĞ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Hyperspace Guided Sampling Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ, Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾-ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Generate Stunning 3D Videos Without Training!",
                    "desc": "ChronosObserver is a novel method for generating high-quality, 3D-consistent, and time-synchronized multi-view videos without the need for training. It utilizes a concept called World State Hyperspace to effectively capture the spatial and temporal aspects of a 4D scene. Additionally, it employs Hyperspace Guided Sampling to ensure that the video generation process is synchronized across different views. The results show that this approach can produce impressive video outputs while overcoming the limitations of traditional training-based models."
                },
                "zh": {
                    "title": "ChronosObserverï¼šæ— éœ€è®­ç»ƒçš„é«˜ä¿çœŸå¤šè§†è§’è§†é¢‘ç”Ÿæˆ",
                    "desc": "ChronosObserveræ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸã€ä¸‰ç»´ä¸€è‡´ä¸”æ—¶é—´åŒæ­¥çš„å¤šè§†è§’è§†é¢‘ã€‚å®ƒåˆ©ç”¨ä¸–ç•ŒçŠ¶æ€è¶…ç©ºé—´æ¥è¡¨ç¤ºå››ç»´ä¸–ç•Œåœºæ™¯çš„æ—¶ç©ºçº¦æŸï¼Œå¹¶é€šè¿‡è¶…ç©ºé—´å¼•å¯¼é‡‡æ ·æ¥åŒæ­¥å¤šä¸ªè§†è§’çš„æ‰©æ•£é‡‡æ ·è½¨è¿¹ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºç›¸æœºæ§åˆ¶çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒChronosObserverå…‹æœäº†æ¨¡å‹æ³›åŒ–å’Œå¯æ‰©å±•æ€§çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆå¤šè§†è§’è§†é¢‘æ—¶è¡¨ç°å‡ºè‰²ï¼Œæ— éœ€è®­ç»ƒæˆ–å¾®è°ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01191",
            "title": "Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks",
            "url": "https://huggingface.co/papers/2512.01191",
            "abstract": "Generalist LLMs outperform specialized clinical AI assistants in a multi-task benchmark, highlighting the need for independent evaluation of clinical decision support tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "b3b75153da1bf791",
            "authors": [
                "Krithik Vishwanath",
                "Mrigayu Ghosh",
                "Anton Alyakin",
                "Daniel Alexander Alber",
                "Yindalon Aphinyanaphongs",
                "Eric Karl Oermann"
            ],
            "affiliations": [
                "Center for Data Science, New York University",
                "Department of Aerospace Engineering & Engineering Mechanics, The University of Texas at Austin",
                "Department of Biomedical Engineering, The University of Texas at Austin",
                "Department of Mathematics, The University of Texas at Austin",
                "Department of Medicine, NYU Langone Health",
                "Department of Molecular Biosciences, The University of Texas at Austin",
                "Department of Neurological Surgery, NYU Langone Health",
                "Department of Neurosurgery, Washington University School of Medicine in St. Louis",
                "Department of Population Health, NYU Langone Health",
                "Department of Radiology, NYU Langone Health",
                "Global AI Frontier Lab, New York University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01191.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#healthcare",
                    "#ethics",
                    "#science"
                ],
                "emoji": "âš•ï¸",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ AI Ğ² Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ñ‹ Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 1000 Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-5 Ğ¸ Claude, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… AI-Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞ´ Ğ¸Ñ… Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºÑƒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Generalist LLMs: The Superior Choice for Clinical Decision Support",
                    "desc": "This paper evaluates the performance of generalist large language models (LLMs) against specialized clinical AI assistants in a medical context. The study reveals that generalist models, like GPT-5, outperform clinical tools such as OpenEvidence and UpToDate in a benchmark designed to assess medical knowledge and clinician alignment. The findings indicate that specialized clinical AI systems often lack completeness, communication quality, and context awareness compared to their generalist counterparts. This highlights the necessity for independent evaluations of clinical decision support tools to ensure their reliability and effectiveness in medical practice."
                },
                "zh": {
                    "title": "é€šç”¨æ¨¡å‹è¶…è¶Šä¸“ç”¨ä¸´åºŠAIï¼Œéœ€ç‹¬ç«‹è¯„ä¼°ï¼",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä¸“é—¨çš„ä¸´åºŠäººå·¥æ™ºèƒ½åŠ©æ‰‹çš„ç°è±¡ã€‚ç ”ç©¶è¯„ä¼°äº†ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„ä¸´åºŠAIç³»ç»Ÿä¸ä¸‰ç§æœ€å…ˆè¿›çš„é€šç”¨LLMsï¼Œç»“æœæ˜¾ç¤ºé€šç”¨æ¨¡å‹åœ¨åŒ»ç–—çŸ¥è¯†å’Œä¸´åºŠå¯¹é½ä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³ã€‚ç‰¹åˆ«æ˜¯GPT-5çš„å¾—åˆ†æœ€é«˜ï¼Œè€ŒOpenEvidenceå’ŒUpToDateåœ¨å®Œæ•´æ€§ã€æ²Ÿé€šè´¨é‡ã€ä¸Šä¸‹æ–‡æ„è¯†å’Œç³»ç»Ÿå®‰å…¨æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨ä¸´åºŠå†³ç­–æ”¯æŒå·¥å…·æŠ•å…¥ä½¿ç”¨å‰ï¼Œè¿›è¡Œç‹¬ç«‹è¯„ä¼°çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.00387",
            "title": "WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing",
            "url": "https://huggingface.co/papers/2512.00387",
            "abstract": "WiseEdit is a benchmark for evaluating cognition- and creativity-informed image editing models by decomposing the process into Awareness, Interpretation, and Imagination tasks, assessing them on declarative, procedural, and metacognitive knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-11-29",
            "pub_date_card": {
                "ru": "29 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 29",
                "zh": "11æœˆ29æ—¥"
            },
            "hash": "c10b081c2c354fcb",
            "authors": [
                "Kaihang Pan",
                "Weile Chen",
                "Haiyi Qiu",
                "Qifan Yu",
                "Wendong Bu",
                "Zehan Wang",
                "Yun Zhu",
                "Juncheng Li",
                "Siliang Tang"
            ],
            "affiliations": [
                "Shanghai Artificial Intelligence Laboratory",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.00387.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.00333",
            "title": "IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages",
            "url": "https://huggingface.co/papers/2512.00333",
            "abstract": "IndicParam benchmark evaluates multiple-choice question performance of LLMs across 11 Indic languages, revealing limitations in cross-lingual transfer and grammatical proficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language models excel on high-resource multilingual tasks, low- and extremely low-resource Indic languages remain severely under-evaluated. We present IndicParam, a human-curated benchmark of over 13,000 multiple-choice questions covering 11 such languages (Nepali, Gujarati, Marathi, Odia as low-resource; Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani as extremely low-resource) plus Sanskrit-English code-mixed set. We evaluated 19 LLMs, both proprietary and open-weights, which reveals that even the top-performing GPT-5 reaches only 45.0% average accuracy, followed by DeepSeek-3.2 (43.1) and Claude-4.5 (42.7). We additionally label each question as knowledge-oriented or purely linguistic to discriminate factual recall from grammatical proficiency. Further, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. IndicParam provides insights into limitations of cross-lingual transfer and establishes a challenging benchmark for Indic languages. The dataset is available at https://huggingface.co/datasets/bharatgenai/IndicParam. Scripts to run benchmark are present at https://github.com/ayushbits/IndicParam.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2025-11-29",
            "pub_date_card": {
                "ru": "29 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 29",
                "zh": "11æœˆ29æ—¥"
            },
            "hash": "d7f83b04f2bf8f7d",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#multilingual",
                    "#benchmark",
                    "#low_resource"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ’Ñ‹Ğ·Ğ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸: ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ LLM Ğ½Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº IndicParam Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ° 11 Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ Ğ¸ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 13 000 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-5, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 45% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ²Ğ¸Ğ´ĞµÑ‚ĞµĞ»ÑŒÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¾ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑÑ… ĞºÑ€Ğ¾ÑÑÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° Ğ´Ğ²Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸: Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ‡Ğ¸ÑÑ‚ÑƒÑ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸ĞºÑƒ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ‹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ­Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸."
                },
                "en": {
                    "title": "Bridging the Gap: Evaluating LLMs in Indic Languages",
                    "desc": "The IndicParam benchmark evaluates the performance of large language models (LLMs) on multiple-choice questions across 11 Indic languages, highlighting their limitations in cross-lingual transfer and grammatical skills. It includes over 13,000 questions, focusing on both low-resource and extremely low-resource languages, and categorizes questions into knowledge-oriented and linguistic types. The evaluation of 19 LLMs shows that even the best-performing model, GPT-5, achieves only 45.0% accuracy, indicating significant challenges in understanding these languages. This benchmark aims to provide a comprehensive assessment of LLM capabilities and encourages further research in underrepresented languages."
                },
                "zh": {
                    "title": "æ­ç¤ºå°åº¦è¯­è¨€æ¨¡å‹çš„å±€é™æ€§",
                    "desc": "IndicParamåŸºå‡†æµ‹è¯•è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨11ç§å°åº¦è¯­è¨€ä¸Šçš„å¤šé¡¹é€‰æ‹©é¢˜è¡¨ç°ï¼Œæ­ç¤ºäº†è·¨è¯­è¨€è¿ç§»å’Œè¯­æ³•èƒ½åŠ›çš„å±€é™æ€§ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é«˜èµ„æºå¤šè¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä½èµ„æºå’Œæä½èµ„æºçš„å°åº¦è¯­è¨€ä»ç„¶ä¸¥é‡ç¼ºä¹è¯„ä¼°ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡13,000ä¸ªå¤šé¡¹é€‰æ‹©é¢˜çš„äººç±»ç­–åˆ’åŸºå‡†ï¼Œæ¶µç›–äº†å¤šç§ä½èµ„æºå’Œæä½èµ„æºè¯­è¨€ã€‚é€šè¿‡è¯„ä¼°19ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„GPT-5ï¼Œå…¶å¹³å‡å‡†ç¡®ç‡ä¹Ÿä»…ä¸º45.0%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01686",
            "title": "DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models",
            "url": "https://huggingface.co/papers/2512.01686",
            "abstract": "DreamingComics uses a diffusion-transformer model and region-aware positional encoding to enhance story visualization with improved layout control, character consistency, and style similarity.  \t\t\t\t\tAI-generated summary \t\t\t\t Current story visualization methods tend to position subjects solely by text and face challenges in maintaining artistic consistency. To address these limitations, we introduce DreamingComics, a layout-aware story visualization framework. We build upon a pretrained video diffusion-transformer (DiT) model, leveraging its spatiotemporal priors to enhance identity and style consistency. For layout-based position control, we propose RegionalRoPE, a region-aware positional encoding scheme that re-indexes embeddings based on the target layout. Additionally, we introduce a masked condition loss to further constrain each subject's visual features to their designated region. To infer layouts from natural language scripts, we integrate an LLM-based layout generator trained to produce comic-style layouts, enabling flexible and controllable layout conditioning. We present a comprehensive evaluation of our approach, showing a 29.2% increase in character consistency and a 36.2% increase in style similarity compared to previous methods, while displaying high spatial accuracy. Our project page is available at https://yj7082126.github.io/dreamingcomics/",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "7ca284d18f02c858",
            "authors": [
                "Patrick Kwon",
                "Chen Chen"
            ],
            "affiliations": [
                "Center for Research in Computer Vision, University of Central Florida, USA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01686.jpg",
            "data": {
                "categories": [
                    "#story_generation",
                    "#multimodal",
                    "#diffusion",
                    "#video",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¸ĞºÑĞ¾Ğ²",
                    "desc": "DreamingComics â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ĞºĞ°Ğ´Ñ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ RegionalRoPE â€” Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€ÑƒĞµÑ‚ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ¼. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ LLM-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ°ĞºĞµÑ‚Ñ‹ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ ĞºĞ¾Ğ¼Ğ¸ĞºÑĞ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ: Ğ½Ğ° 29,2% Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ Ğ½Ğ° 36,2% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ° Ğ¿Ğ¾Ñ…Ğ¾Ğ¶ĞµÑÑ‚ÑŒ ÑÑ‚Ğ¸Ğ»Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Comic Visualization with AI: Consistency and Control",
                    "desc": "DreamingComics is a novel framework that enhances story visualization by using a diffusion-transformer model combined with region-aware positional encoding. This approach addresses the common issues of artistic consistency and layout control in AI-generated comics. By leveraging a pretrained video diffusion-transformer, it improves character identity and style consistency while allowing for precise layout positioning through a new encoding scheme. The integration of a large language model (LLM) for layout generation further enables flexible and controllable comic-style layouts, resulting in significant improvements in character consistency and style similarity."
                },
                "zh": {
                    "title": "æå‡æ•…äº‹å¯è§†åŒ–çš„ä¸€ä½“åŒ–è§£å†³æ–¹æ¡ˆ",
                    "desc": "DreamingComics æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£-å˜æ¢å™¨æ¨¡å‹çš„æ•…äº‹å¯è§†åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„å¸ƒå±€æ§åˆ¶ã€è§’è‰²ä¸€è‡´æ€§å’Œé£æ ¼ç›¸ä¼¼æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£-å˜æ¢å™¨æ¨¡å‹ï¼Œåˆ©ç”¨å…¶æ—¶ç©ºå…ˆéªŒæ¥å¢å¼ºè§’è‰²å’Œé£æ ¼çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬æå‡ºäº†åŒºåŸŸæ„ŸçŸ¥ä½ç½®ç¼–ç ï¼ˆRegionalRoPEï¼‰ï¼Œä»¥ä¾¿æ ¹æ®ç›®æ ‡å¸ƒå±€é‡æ–°ç´¢å¼•åµŒå…¥ï¼Œä»è€Œå®ç°åŸºäºå¸ƒå±€çš„æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†æ©ç æ¡ä»¶æŸå¤±ï¼Œä»¥è¿›ä¸€æ­¥çº¦æŸæ¯ä¸ªè§’è‰²çš„è§†è§‰ç‰¹å¾ä¸å…¶æŒ‡å®šåŒºåŸŸçš„åŒ¹é…ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2511.22448",
            "title": "Structured Extraction from Business Process Diagrams Using Vision-Language Models",
            "url": "https://huggingface.co/papers/2511.22448",
            "abstract": "Business Process Model and Notation (BPMN) is a widely adopted standard for representing complex business workflows. While BPMN diagrams are often exchanged as visual images, existing methods primarily rely on XML representations for computational analysis. In this work, we present a pipeline that leverages Vision-Language Models (VLMs) to extract structured JSON representations of BPMN diagrams directly from images, without requiring source model files or textual annotations. We also incorporate optical character recognition (OCR) for textual enrichment and evaluate the generated element lists against ground truth data derived from the source XML files. Our approach enables robust component extraction in scenarios where original source files are unavailable. We benchmark multiple VLMs and observe performance improvements in several models when OCR is used for text enrichment. In addition, we conducted extensive statistical analyses of OCR-based enrichment methods and prompt ablation studies, providing a clearer understanding of their impact on model performance.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-27",
            "pub_date_card": {
                "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 27",
                "zh": "11æœˆ27æ—¥"
            },
            "hash": "8e4b23f956c1a99c",
            "authors": [
                "Pritam Deka",
                "Barry Devereux"
            ],
            "affiliations": [
                "Queens University Belfast"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2511.22448.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'dict'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.00077",
            "title": "A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs",
            "url": "https://huggingface.co/papers/2512.00077",
            "abstract": "A hierarchical control architecture combining learning-based locomotion and model-based balancing enhances humanoid stability with supernumerary limbs.  \t\t\t\t\tAI-generated summary \t\t\t\t The integration of Supernumerary Limbs (SLs) on humanoid robots poses a significant stability challenge due to the dynamic perturbations they introduce. This thesis addresses this issue by designing a novel hierarchical control architecture to improve humanoid locomotion stability with SLs. The core of this framework is a decoupled strategy that combines learning-based locomotion with model-based balancing. The low-level component consists of a walking gait for a Unitree H1 humanoid through imitation learning and curriculum learning. The high-level component actively utilizes the SLs for dynamic balancing. The effectiveness of the system is evaluated in a physics-based simulation under three conditions: baseline gait for an unladen humanoid (baseline walking), walking with a static SL payload (static payload), and walking with the active dynamic balancing controller (dynamic balancing). Our evaluation shows that the dynamic balancing controller improves stability. Compared to the static payload condition, the balancing strategy yields a gait pattern closer to the baseline and decreases the Dynamic Time Warping (DTW) distance of the CoM trajectory by 47\\%. The balancing controller also improves the re-stabilization within gait cycles and achieves a more coordinated anti-phase pattern of Ground Reaction Forces (GRF). The results demonstrate that a decoupled, hierarchical design can effectively mitigate the internal dynamic disturbances arising from the mass and movement of the SLs, enabling stable locomotion for humanoids equipped with functional limbs. Code and videos are available here: https://github.com/heyzbw/HuSLs.",
            "score": 1,
            "issue_id": 1,
            "pub_date": "2025-11-25",
            "pub_date_card": {
                "ru": "25 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 25",
                "zh": "11æœˆ25æ—¥"
            },
            "hash": "7e64e5a19ad7c7a8",
            "authors": [
                "Bowen Zhi"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.00077.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.02015",
            "title": "Generative Video Motion Editing with 3D Point Tracks",
            "url": "https://huggingface.co/papers/2512.02015",
            "abstract": "A track-conditioned video-to-video framework enables precise joint editing of camera and object motion using 3D point tracks to maintain spatiotemporal coherence and handle occlusions.  \t\t\t\t\tAI-generated summary \t\t\t\t Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.",
            "score": 0,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "0287df9768d48c05",
            "authors": [
                "Yao-Chih Lee",
                "Zhoutong Zhang",
                "Jiahui Huang",
                "Jui-Hsien Wang",
                "Joon-Young Lee",
                "Jia-Bin Huang",
                "Eli Shechtman",
                "Zhengqi Li"
            ],
            "affiliations": [
                "Adobe",
                "Adobe Research",
                "University of Maryland College Park"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.02015.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#synthetic",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞºĞ¸ Ñ‚Ğ¾Ñ‡ĞµĞº",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞºĞ¾Ğ² Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ° Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸: ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ñƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½ĞµĞ¶Ñ‘ÑÑ‚ĞºÑƒÑ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Video Editing with 3D Motion Control",
                    "desc": "This paper introduces a track-conditioned video-to-video (V2V) framework that enhances the editing of camera and object motions in videos. By utilizing 3D point tracks, the framework maintains spatiotemporal coherence and effectively manages occlusions during the editing process. Unlike traditional methods that struggle with full-scene context, this approach allows for precise control over complex motion edits. The model is trained in two stages, enabling a variety of creative edits such as joint manipulation and motion transfer, thus expanding the possibilities in video editing."
                },
                "zh": {
                    "title": "ç²¾ç¡®è”åˆç¼–è¾‘ç›¸æœºä¸ç‰©ä½“è¿åŠ¨çš„è½¨è¿¹æ¡ä»¶æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è½¨è¿¹æ¡ä»¶çš„è§†é¢‘åˆ°è§†é¢‘ï¼ˆV2Vï¼‰æ¡†æ¶ï¼Œèƒ½å¤Ÿç²¾ç¡®åœ°è”åˆç¼–è¾‘ç›¸æœºå’Œç‰©ä½“çš„è¿åŠ¨ã€‚é€šè¿‡ä½¿ç”¨3Dç‚¹è½¨è¿¹ï¼Œè¯¥æ–¹æ³•ä¿æŒäº†æ—¶ç©ºä¸€è‡´æ€§ï¼Œå¹¶æœ‰æ•ˆå¤„ç†äº†é®æŒ¡é—®é¢˜ã€‚ä¸ä¼ ç»Ÿçš„å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæä¾›æ›´ä¸°å¯Œçš„åœºæ™¯ä¸Šä¸‹æ–‡ï¼Œä»è€Œå®ç°æ›´ç»†è‡´çš„è¿åŠ¨æ§åˆ¶ã€‚ç»è¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒåï¼Œè¯¥æ¨¡å‹æ”¯æŒå¤šç§è¿åŠ¨ç¼–è¾‘ï¼ŒåŒ…æ‹¬ç›¸æœºå’Œç‰©ä½“çš„è”åˆæ“ä½œã€è¿åŠ¨è½¬ç§»å’Œéåˆšæ€§å˜å½¢ï¼Œæ‹“å±•äº†è§†é¢‘ç¼–è¾‘çš„åˆ›ä½œæ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.01443",
            "title": "MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification",
            "url": "https://huggingface.co/papers/2512.01443",
            "abstract": "The Conformer-based decoders adapted for MEG signals achieved high performance in Speech Detection and Phoneme Classification tasks using task-specific augmentations and normalization techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Conformer-based decoders for the LibriBrain 2025 PNPL competition, targeting two foundational MEG tasks: Speech Detection and Phoneme Classification. Our approach adapts a compact Conformer to raw 306-channel MEG signals, with a lightweight convolutional projection layer and task-specific heads. For Speech Detection, a MEG-oriented SpecAugment provided a first exploration of MEG-specific augmentation. For Phoneme Classification, we used inverse-square-root class weighting and a dynamic grouping loader to handle 100-sample averaged examples. In addition, a simple instance-level normalization proved critical to mitigate distribution shifts on the holdout split. Using the official Standard track splits and F1-macro for model selection, our best systems achieved 88.9% (Speech) and 65.8% (Phoneme) on the leaderboard, surpassing the competition baselines and ranking within the top-10 in both tasks. For further implementation details, the technical documentation, source code, and checkpoints are available at https://github.com/neural2speech/libribrain-experiments.",
            "score": 0,
            "issue_id": 1,
            "pub_date": "2025-12-01",
            "pub_date_card": {
                "ru": "1 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 1",
                "zh": "12æœˆ1æ—¥"
            },
            "hash": "bfca7069232f4198",
            "authors": [
                "Xabier de Zuazo",
                "Ibon Saratxaga",
                "Eva Navas"
            ],
            "affiliations": [
                "HiTZ Center, Dept. of Communications Engineering, School of Engineering, University of the Basque Country (UPV/EHU), Bilbao, Spain"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.01443.jpg",
            "data": {
                "error": "can only concatenate list (not \"dict\") to list"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.00639",
            "title": "Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation",
            "url": "https://huggingface.co/papers/2512.00639",
            "abstract": "YOLOv5 algorithms enhance thyroid nodule segmentation accuracy in ultrasound images, particularly when incorporating doppler images, offering a real-time solution for clinical diagnostics.  \t\t\t\t\tAI-generated summary \t\t\t\t The increasing prevalence of thyroid cancer globally has led to the development of various computer-aided detection methods. Accurate segmentation of thyroid nodules is a critical first step in the development of AI-assisted clinical decision support systems. This study focuses on instance segmentation of thyroid nodules using YOLOv5 algorithms on ultrasound images. We evaluated multiple YOLOv5 variants (Nano, Small, Medium, Large, and XLarge) across two dataset versions, with and without doppler images. The YOLOv5-Large algorithm achieved the highest performance with a dice score of 91\\% and mAP of 0.87 on the dataset including doppler images. Notably, our results demonstrate that doppler images, typically excluded by physicians, can significantly improve segmentation performance. The YOLOv5-Small model achieved 79\\% dice score when doppler images were excluded, while including them improved performance across all model variants. These findings suggest that instance segmentation with YOLOv5 provides an effective real-time approach for thyroid nodule detection, with potential clinical applications in automated diagnostic systems.",
            "score": 0,
            "issue_id": 1,
            "pub_date": "2025-11-29",
            "pub_date_card": {
                "ru": "29 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 29",
                "zh": "11æœˆ29æ—¥"
            },
            "hash": "fcebbcb9e9d9cedc",
            "authors": [
                "Mahmoud El Hussieni"
            ],
            "affiliations": [
                "Department of Computer Engineering, Istanbul Medipol University, Istanbul, Turkey"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.00639.jpg",
            "data": {
                "error": "unsupported operand type(s) for +: 'dict' and 'list'"
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.00234",
            "title": "OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion",
            "url": "https://huggingface.co/papers/2512.00234",
            "abstract": "A multimodal translation system, OmniFusion, integrates pretrained multimodal foundation models and translation LLMs for simultaneous speech translation and improved translation quality by leveraging audio and visual inputs.  \t\t\t\t\tAI-generated summary \t\t\t\t There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce a novel fusion strategy that connects hidden states from multiple layers of a pretrained MMFM to a translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves a 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation qualityCode is available at https://github.com/saikoneru/OmniFusion.",
            "score": 0,
            "issue_id": 1,
            "pub_date": "2025-11-28",
            "pub_date_card": {
                "ru": "28 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 28",
                "zh": "11æœˆ28æ—¥"
            },
            "hash": "89533e483d59e505",
            "authors": [
                "Sai Koneru",
                "Matthias Huck",
                "Jan Niehues"
            ],
            "affiliations": [
                "Karlsruhe Institute of Technology",
                "SAP SE"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2512.00234.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#multimodal",
                    "#multilingual",
                    "#translation",
                    "#audio",
                    "#architecture",
                    "#training"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ€ĞµÑ‡Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° OmniFusion Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ LLM Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑ‡ÑŒ, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° (Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ + Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´), Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¿Ñ€Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ. ĞĞ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° ÑĞ»Ğ¾Ñ‘Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº LLM Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñ‡Ğ¸ĞºÑƒ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ OmniFusion ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ½Ğ° 1 ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ğ² ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ‰ĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "OmniFusion: Revolutionizing Simultaneous Speech Translation with Multimodal Inputs",
                    "desc": "The paper presents OmniFusion, a multimodal translation system that combines pretrained multimodal foundation models (MMFMs) with translation large language models (LLMs) for enhanced speech translation. Unlike traditional methods that use separate steps for speech recognition and translation, OmniFusion allows for simultaneous processing, reducing latency significantly. By integrating audio and visual inputs, the system improves translation quality and disambiguation, leveraging the strengths of both modalities. The novel fusion strategy enables joint training, resulting in a more efficient and effective translation model capable of handling various input types."
                },
                "zh": {
                    "title": "OmniFusionï¼šå¤šæ¨¡æ€ç¿»è¯‘çš„æ–°çªç ´",
                    "desc": "OmniFusionæ˜¯ä¸€ç§å¤šæ¨¡æ€ç¿»è¯‘ç³»ç»Ÿï¼Œç»“åˆäº†é¢„è®­ç»ƒçš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å’Œç¿»è¯‘å¤§è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†åŒæ—¶è¯­éŸ³ç¿»è¯‘ã€‚è¯¥ç³»ç»Ÿé€šè¿‡éŸ³é¢‘å’Œè§†è§‰è¾“å…¥æ¥æé«˜ç¿»è¯‘è´¨é‡ï¼Œå…‹æœäº†ä¼ ç»Ÿè¯­éŸ³ç¿»è¯‘ä¸­å­˜åœ¨çš„å»¶è¿Ÿé—®é¢˜ã€‚OmniFusioné‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„èåˆç­–ç•¥ï¼Œå°†å¤šå±‚éšè—çŠ¶æ€è¿æ¥åˆ°ç¿»è¯‘å¤§è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†è”åˆç«¯åˆ°ç«¯è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒOmniFusionåœ¨åŒæ—¶è¯­éŸ³ç¿»è¯‘ä¸­æœ‰æ•ˆåˆ©ç”¨éŸ³é¢‘å’Œè§†è§‰è¾“å…¥ï¼Œæ˜¾è‘—é™ä½äº†å»¶è¿Ÿå¹¶æå‡äº†ç¿»è¯‘æ•ˆæœã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-01.html",
    "link_next": "2025-12-03.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "01.12",
        "en": "12/01",
        "zh": "12æœˆ1æ—¥"
    },
    "short_date_next": {
        "ru": "03.12",
        "en": "12/03",
        "zh": "12æœˆ3æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 1,
        "#benchmark": 14,
        "#agents": 5,
        "#cv": 4,
        "#rl": 5,
        "#rlhf": 5,
        "#rag": 2,
        "#plp": 1,
        "#inference": 5,
        "#3d": 3,
        "#audio": 2,
        "#video": 8,
        "#multimodal": 13,
        "#math": 2,
        "#multilingual": 4,
        "#architecture": 7,
        "#healthcare": 1,
        "#training": 17,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 10,
        "#transfer_learning": 1,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 14,
        "#survey": 1,
        "#diffusion": 6,
        "#alignment": 2,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 11,
        "#small_models": 2,
        "#science": 3,
        "#low_resource": 1,
        "#translation": 1
    }
}