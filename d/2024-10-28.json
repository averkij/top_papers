{
    "date": {
        "ru": "28 октября",
        "en": "October 28",
        "zh": "10月28日"
    },
    "time_utc": "2024-10-28 06:18",
    "weekday": 0,
    "issue_id": 307,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.17856",
            "title": "ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting",
            "url": "https://huggingface.co/papers/2410.17856",
            "abstract": "Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. A key issue is the difficulty in smoothly connecting individual entities in low-level observations with abstract concepts required for planning. A common approach to address this problem is through the use of hierarchical agents, where VLMs serve as high-level reasoners that break down tasks into executable sub-tasks, typically specified using language and imagined observations. However, language often fails to effectively convey spatial information, while generating future images with sufficient accuracy remains challenging. To address these limitations, we propose visual-temporal context prompting, a novel communication protocol between VLMs and policy models. This protocol leverages object segmentation from both past and present observations to guide policy-environment interactions. Using this approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual observations and segmentation masks, with real-time object tracking provided by SAM-2. Our method unlocks the full potential of VLMs visual-language reasoning abilities, enabling them to solve complex creative tasks, especially those heavily reliant on spatial understanding. Experiments in Minecraft demonstrate that our approach allows agents to accomplish previously unattainable tasks, highlighting the effectiveness of visual-temporal context prompting in embodied decision-making. Codes and demos will be available on the project page: https://craftjarvis.github.io/ROCKET-1.",
            "score": 16,
            "issue_id": 302,
            "pub_date": "2024-10-23",
            "pub_date_card": {
                "ru": "23 октября",
                "en": "October 23",
                "zh": "10月23日"
            },
            "hash": "0a477d46d035f1d4",
            "data": {
                "categories": [
                    "#agents",
                    "#cv",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "Визуально-временной контекст открывает новые горизонты для VLM в воплощенном ИИ",
                    "desc": "Статья представляет новый подход к использованию визуально-языковых моделей (VLM) в задачах принятия решений в открытых средах. Авторы предлагают протокол визуально-временного контекстного промптинга для улучшения взаимодействия между VLM и моделями политик. Метод использует сегментацию объектов из прошлых и текущих наблюдений для управления взаимодействием политики и среды. Эксперименты в Minecraft показывают, что подход позволяет агентам решать ранее недостижимые задачи, особенно требующие пространственного понимания."
                },
                "en": {
                    "title": "Unlocking Spatial Reasoning in Decision-Making with VLMs",
                    "desc": "This paper addresses the challenges of using vision-language models (VLMs) for decision-making in dynamic environments. It highlights the difficulty of linking low-level visual data with high-level abstract concepts necessary for planning. The authors introduce a new method called visual-temporal context prompting, which enhances communication between VLMs and policy models by utilizing object segmentation from past and present observations. Their approach, implemented in the ROCKET-1 model, demonstrates improved performance in complex tasks that require spatial reasoning, as shown in experiments conducted in Minecraft."
                },
                "zh": {
                    "title": "视觉-时间上下文提示：提升具身决策能力的关键",
                    "desc": "视觉语言模型（VLMs）在多模态任务中表现出色，但在开放世界环境中进行具身决策时面临挑战。主要问题是如何将低级观察中的个体实体与规划所需的抽象概念顺利连接。为了解决这个问题，本文提出了一种视觉-时间上下文提示的新通信协议，利用过去和现在观察中的物体分割来指导策略与环境的交互。通过这种方法，我们训练了ROCKET-1，一个基于视觉观察和分割掩码预测动作的低级策略，展示了在复杂创意任务中，尤其是依赖空间理解的任务中的有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19008",
            "title": "Teach Multimodal LLMs to Comprehend Electrocardiographic Images",
            "url": "https://huggingface.co/papers/2410.19008",
            "abstract": "The electrocardiogram (ECG) is an essential non-invasive diagnostic tool for assessing cardiac conditions. Existing automatic interpretation methods suffer from limited generalizability, focusing on a narrow range of cardiac conditions, and typically depend on raw physiological signals, which may not be readily available in resource-limited settings where only printed or digital ECG images are accessible. Recent advancements in multimodal large language models (MLLMs) present promising opportunities for addressing these challenges. However, the application of MLLMs to ECG image interpretation remains challenging due to the lack of instruction tuning datasets and well-established ECG image benchmarks for quantitative evaluation. To address these challenges, we introduce ECGInstruct, a comprehensive ECG image instruction tuning dataset of over one million samples, covering a wide range of ECG-related tasks from diverse data sources. Using ECGInstruct, we develop PULSE, an MLLM tailored for ECG image comprehension. In addition, we curate ECGBench, a new evaluation benchmark covering four key ECG image interpretation tasks across nine different datasets. Our experiments show that PULSE sets a new state-of-the-art, outperforming general MLLMs with an average accuracy improvement of 15% to 30%. This work highlights the potential of PULSE to enhance ECG interpretation in clinical practice.",
            "score": 8,
            "issue_id": 300,
            "pub_date": "2024-10-21",
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            },
            "hash": "85936de603f8cc7a",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#medicine"
                ],
                "emoji": "❤️",
                "ru": {
                    "title": "PULSE: Революция в автоматическом анализе ЭКГ с помощью мультимодальных языковых моделей",
                    "desc": "Статья представляет новый подход к автоматической интерпретации ЭКГ с использованием мультимодальных больших языковых моделей (MLLM). Авторы создали ECGInstruct - набор данных из более чем миллиона образцов ЭКГ для обучения моделей, и разработали PULSE - специализированную MLLM для понимания изображений ЭКГ. Также был создан бенчмарк ECGBench для оценки моделей по четырем ключевым задачам интерпретации ЭКГ. Эксперименты показали, что PULSE превосходит общие MLLM на 15-30% по точности, демонстрируя потенциал для улучшения интерпретации ЭКГ в клинической практике."
                },
                "en": {
                    "title": "Revolutionizing ECG Interpretation with PULSE and ECGInstruct",
                    "desc": "This paper presents ECGInstruct, a large dataset designed for instruction tuning of multimodal large language models (MLLMs) specifically for interpreting ECG images. The authors introduce PULSE, an MLLM that leverages ECGInstruct to improve the understanding of ECG images, addressing the limitations of existing methods that rely on raw signals. Additionally, they create ECGBench, a benchmark for evaluating ECG image interpretation across various tasks and datasets. The results demonstrate that PULSE significantly enhances accuracy in ECG interpretation, showcasing its potential for clinical applications."
                },
                "zh": {
                    "title": "PULSE：提升心电图解读的新方法",
                    "desc": "心电图（ECG）是评估心脏状况的重要非侵入性诊断工具。现有的自动解读方法在通用性上存在局限，通常只关注少数心脏疾病，并依赖于生理信号，这在资源有限的环境中难以获取。为了解决这些问题，我们提出了ECGInstruct，这是一个包含超过一百万个样本的心电图图像指令调优数据集，涵盖了多种心电图相关任务。基于ECGInstruct，我们开发了PULSE，一个专为心电图图像理解而设计的大型多模态语言模型，实验结果显示PULSE在准确性上超越了通用模型，提升幅度达到15%到30%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.18558",
            "title": "Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data",
            "url": "https://huggingface.co/papers/2410.18558",
            "abstract": "Vision-Language Models (VLMs) have recently made significant progress, but the limited scale and quality of open-source instruction data hinder their performance compared to closed-source models. In this work, we address this limitation by introducing Infinity-MM, a large-scale multimodal instruction dataset with 40 million samples, enhanced through rigorous quality filtering and deduplication. We also propose a synthetic instruction generation method based on open-source VLMs, using detailed image annotations and diverse question generation. Using this data, we trained a 2-billion-parameter VLM, Aquila-VL-2B, achieving state-of-the-art (SOTA) performance for models of similar scale. This demonstrates that expanding instruction data and generating synthetic data can significantly improve the performance of open-source models.",
            "score": 6,
            "issue_id": 300,
            "pub_date": "2024-10-24",
            "pub_date_card": {
                "ru": "24 октября",
                "en": "October 24",
                "zh": "10月24日"
            },
            "hash": "18e760a965f56e6d",
            "data": {
                "categories": [
                    "#dataset",
                    "#data",
                    "#multimodal"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Большие данные - ключ к улучшению открытых визуально-языковых моделей",
                    "desc": "Исследователи представили Infinity-MM - крупномасштабный мультимодальный набор данных с инструкциями, содержащий 40 миллионов образцов. Они также предложили метод генерации синтетических инструкций на основе открытых визуально-языковых моделей. Используя эти данные, они обучили 2-миллиардную модель Aquila-VL-2B, достигшую наилучших результатов среди моделей аналогичного масштаба. Это демонстрирует, что расширение обучающих данных и генерация синтетических данных могут значительно улучшить производительность открытых моделей."
                },
                "en": {
                    "title": "Unlocking VLM Potential with Infinity-MM Dataset",
                    "desc": "This paper presents Infinity-MM, a large-scale multimodal instruction dataset containing 40 million samples, aimed at improving the performance of Vision-Language Models (VLMs). The authors highlight the challenges faced by open-source VLMs due to limited instruction data quality and scale. They introduce a synthetic instruction generation method that leverages detailed image annotations and diverse question generation to enhance the dataset. Training their 2-billion-parameter model, Aquila-VL-2B, on this enriched dataset resulted in state-of-the-art performance, showcasing the potential of expanded and synthetic instruction data for open-source models."
                },
                "zh": {
                    "title": "扩展数据，提升开源模型性能！",
                    "desc": "本文介绍了一种新的多模态指令数据集Infinity-MM，包含4000万条样本，旨在提升开源视觉语言模型（VLM）的性能。通过严格的质量过滤和去重，该数据集的质量得到了显著提高。我们还提出了一种基于开源VLM的合成指令生成方法，利用详细的图像注释和多样的问题生成。最终，我们训练了一个具有20亿参数的VLM模型Aquila-VL-2B，达到了同规模模型的最新性能，证明了扩展指令数据和生成合成数据的重要性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19168",
            "title": "MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark",
            "url": "https://huggingface.co/papers/2410.19168",
            "abstract": "The ability to comprehend audio--which includes speech, non-speech sounds, and music--is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex reasoning. MMAU comprises 10k carefully curated audio clips paired with human-annotated natural language questions and answers spanning speech, environmental sounds, and music. It includes information extraction and reasoning questions, requiring models to demonstrate 27 distinct skills across unique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes advanced perception and reasoning with domain-specific knowledge, challenging models to tackle tasks akin to those faced by experts. We assess 18 open-source and proprietary (Large) Audio-Language Models, demonstrating the significant challenges posed by MMAU. Notably, even the most advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves only 52.50%, highlighting considerable room for improvement. We believe MMAU will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.",
            "score": 4,
            "issue_id": 300,
            "pub_date": "2024-10-24",
            "pub_date_card": {
                "ru": "24 октября",
                "en": "October 24",
                "zh": "10月24日"
            },
            "hash": "7a747d9a9b0717cc",
            "data": {
                "categories": [
                    "#benchmark",
                    "#audio",
                    "#multimodal"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "MMAU: новый рубеж в понимании аудио для ИИ",
                    "desc": "MMAU - это новый эталонный тест для оценки моделей мультимодального понимания аудио. Он включает 10 тысяч аудиоклипов с вопросами и ответами, охватывающими речь, звуки окружающей среды и музыку. MMAU требует от моделей демонстрации 27 различных навыков и экспертных знаний в области аудио. Даже самые продвинутые модели, такие как Gemini Pro v1.5 и Qwen2-Audio, достигают точности лишь около 53%, что указывает на значительное пространство для улучшения в этой области."
                },
                "en": {
                    "title": "MMAU: Raising the Bar for Audio Understanding in AI",
                    "desc": "The paper introduces MMAU, a new benchmark for evaluating multimodal audio understanding models, which is essential for AI to interpret various audio types like speech and music. It consists of 10,000 audio clips with corresponding human-annotated questions and answers that require advanced reasoning and domain-specific knowledge. The benchmark tests models on 27 distinct skills through complex tasks, pushing the limits of current audio understanding capabilities. Results show that even leading models struggle, indicating a need for further advancements in the field."
                },
                "zh": {
                    "title": "MMAU：推动音频理解模型的进步",
                    "desc": "MMAU是一个新颖的基准，旨在评估多模态音频理解模型在需要专家级知识和复杂推理的任务上的表现。它包含1万个精心策划的音频片段，并配有人工标注的自然语言问题和答案，涵盖了语音、环境声音和音乐。MMAU要求模型展示27种不同的技能，处理独特且具有挑战性的任务，强调高级感知和推理能力。通过评估18个开源和专有的音频语言模型，结果显示即使是最先进的模型也面临显著挑战，表明在音频理解领域还有很大的改进空间。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19355",
            "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality",
            "url": "https://huggingface.co/papers/2410.19355",
            "abstract": "In this paper, we present \\textit{FasterCache}, a novel training-free strategy designed to accelerate the inference of video diffusion models with high-quality generation. By analyzing existing cache-based methods, we observe that directly reusing adjacent-step features degrades video quality due to the loss of subtle variations. We further perform a pioneering investigation of the acceleration potential of classifier-free guidance (CFG) and reveal significant redundancy between conditional and unconditional features within the same timestep. Capitalizing on these observations, we introduce FasterCache to substantially accelerate diffusion-based video generation. Our key contributions include a dynamic feature reuse strategy that preserves both feature distinction and temporal continuity, and CFG-Cache which optimizes the reuse of conditional and unconditional outputs to further enhance inference speed without compromising video quality. We empirically evaluate FasterCache on recent video diffusion models. Experimental results show that FasterCache can significantly accelerate video generation (\\eg 1.67times speedup on Vchitect-2.0) while keeping video quality comparable to the baseline, and consistently outperform existing methods in both inference speed and video quality.",
            "score": 2,
            "issue_id": 301,
            "pub_date": "2024-10-25",
            "pub_date_card": {
                "ru": "25 октября",
                "en": "October 25",
                "zh": "10月25日"
            },
            "hash": "2de0b0700140bc7e",
            "data": {
                "categories": [
                    "#video",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "🚀",
                "ru": {
                    "title": "FasterCache: Ускорение видео-диффузии без потери качества",
                    "desc": "FasterCache - это новая стратегия для ускорения вывода видео-моделей диффузии без дополнительного обучения. Метод анализирует существующие кэш-подходы и оптимизирует повторное использование условных и безусловных признаков. FasterCache включает динамическую стратегию переиспользования признаков и CFG-кэш для сохранения качества видео. Эксперименты показали значительное ускорение генерации видео (например, в 1,67 раза для Vchitect-2.0) при сохранении качества на уровне базовой модели."
                },
                "en": {
                    "title": "Accelerating Video Generation with FasterCache!",
                    "desc": "This paper introduces FasterCache, a new method to speed up video generation using diffusion models without needing extra training. The authors found that reusing features from adjacent steps can hurt video quality by losing important details. They also discovered that there is a lot of overlap between features used for conditional and unconditional generation, which can be optimized. FasterCache uses a smart way to reuse features that maintains quality and speed, achieving significant improvements in video generation speed while keeping the quality high."
                },
                "zh": {
                    "title": "FasterCache：加速视频生成的新策略",
                    "desc": "本文提出了一种名为FasterCache的新策略，旨在加速视频扩散模型的推理过程，同时保持高质量的生成效果。通过分析现有的基于缓存的方法，我们发现直接重用相邻步骤的特征会导致视频质量下降，因为细微变化会丢失。我们还首次研究了无分类器引导（CFG）的加速潜力，揭示了同一时间步内条件特征和无条件特征之间的显著冗余。FasterCache通过动态特征重用策略和优化条件与无条件输出的缓存，显著提高了扩散视频生成的速度，同时保持了视频质量。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19290",
            "title": "Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning",
            "url": "https://huggingface.co/papers/2410.19290",
            "abstract": "Recent studies have identified one aggravating factor of LLM hallucinations as the knowledge inconsistency between pre-training and fine-tuning, where unfamiliar fine-tuning data mislead the LLM to fabricate plausible but wrong outputs. In this paper, we propose a novel fine-tuning strategy called Prereq-Tune to address this knowledge inconsistency and reduce hallucinations. Fundamentally, Prereq-Tune disentangles the learning of skills and knowledge, so the model learns only the task skills without being impacted by the knowledge inconsistency. To achieve this, Prereq-Tune introduces an additional prerequisite learning stage to learn the necessary knowledge for SFT, allowing subsequent SFT to focus only on task skills. Prereq-Tune can also be combined with fictitious synthetic data to enhance the grounding of LLM outputs to their internal knowledge. Experiments show that Prereq-Tune outperforms existing baselines in improving LLM's factuality across short QA and long-form generation tasks. It also opens new possibilities for knowledge-controlled generation in LLMs. Our code is available at https://github.com/UCSB-NLP-Chang/Prereq_tune.git.",
            "score": 1,
            "issue_id": 302,
            "pub_date": "2024-10-25",
            "pub_date_card": {
                "ru": "25 октября",
                "en": "October 25",
                "zh": "10月25日"
            },
            "hash": "a11e3c6587db25fc",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#alignment"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Prereq-Tune: Уменьшение галлюцинаций в языковых моделях путем разделения обучения навыкам и знаниям",
                    "desc": "В статье представлена новая стратегия дообучения языковых моделей под названием Prereq-Tune, направленная на уменьшение галлюцинаций. Метод разделяет обучение навыкам и знаниям, вводя дополнительный этап предварительного обучения необходимым знаниям перед основным дообучением. Prereq-Tune также может использоваться с синтетическими данными для улучшения привязки выходных данных модели к ее внутренним знаниям. Эксперименты показывают, что Prereq-Tune превосходит существующие базовые методы в повышении фактической точности языковых моделей при решении задач вопросно-ответных систем и генерации длинных текстов."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "解决LLM幻觉的知识不一致性",
                    "desc": "最近的研究发现，LLM（大型语言模型）幻觉的一个加重因素是预训练和微调之间的知识不一致，这导致模型在面对不熟悉的微调数据时产生错误的输出。为了解决这个问题，本文提出了一种新的微调策略，称为Prereq-Tune，旨在减少这种知识不一致性。Prereq-Tune通过引入额外的先决学习阶段，使模型在微调之前先学习必要的知识，从而专注于任务技能的学习。实验表明，Prereq-Tune在提高LLM的事实性方面优于现有的基线方法，并为知识控制生成开辟了新的可能性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19133",
            "title": "Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback",
            "url": "https://huggingface.co/papers/2410.19133",
            "abstract": "Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, directly collecting human preferences can be expensive, time-consuming, and can have high variance. An appealing alternative is to distill preferences from LMs as a source of synthetic annotations as they are more consistent, cheaper, and scale better than human annotation; however, they are also prone to biases and errors. In this work, we introduce a routing framework that combines inputs from humans and LMs to achieve better annotation quality, while reducing the total cost of human annotation. The crux of our approach is to identify preference instances that will benefit from human annotations. We formulate this as an optimization problem: given a preference dataset and an evaluation metric, we train a performance prediction model to predict a reward model's performance on an arbitrary combination of human and LM annotations and employ a routing strategy that selects a combination that maximizes predicted performance. We train the performance prediction model on MultiPref, a new preference dataset with 10K instances paired with human and LM labels. We show that the selected hybrid mixture of LM and direct human preferences using our routing framework achieves better reward model performance compared to using either one exclusively. We simulate selective human preference collection on three other datasets and show that our method generalizes well to all three. We analyze features from the routing model to identify characteristics of instances that can benefit from human feedback, e.g., prompts with a moderate safety concern or moderate intent complexity. We release the dataset, annotation platform, and source code used in this study to foster more efficient and accurate preference collection in the future.",
            "score": 1,
            "issue_id": 301,
            "pub_date": "2024-10-24",
            "pub_date_card": {
                "ru": "24 октября",
                "en": "October 24",
                "zh": "10月24日"
            },
            "hash": "f6e6f7e5b9e467fe",
            "data": {
                "categories": [
                    "#rlhf",
                    "#dataset",
                    "#data",
                    "#optimization"
                ],
                "emoji": "🔀",
                "ru": {
                    "title": "Оптимальное сочетание человеческого и машинного интеллекта для обучения ИИ",
                    "desc": "Статья представляет новый подход к обучению языковых моделей на основе предпочтений. Авторы предлагают гибридный метод, сочетающий аннотации от людей и языковых моделей для повышения качества обучения. Ключевая идея заключается в использовании модели маршрутизации, которая определяет, какие примеры требуют человеческой оценки. Эксперименты показывают, что такой подход позволяет добиться лучших результатов по сравнению с использованием только человеческих или только машинных аннотаций."
                },
                "en": {
                    "title": "Optimizing Language Model Annotations with Human and Synthetic Feedback",
                    "desc": "This paper presents a novel routing framework that enhances the quality of annotations for language models by intelligently combining human feedback and synthetic annotations from other language models. The authors address the challenges of collecting human preferences, which can be costly and inconsistent, by proposing a method that identifies which instances would benefit most from human input. They introduce a performance prediction model trained on a new dataset called MultiPref, which helps optimize the selection of human and LM annotations to maximize overall performance. The results demonstrate that this hybrid approach outperforms using either source of annotations alone, and the authors provide tools and datasets to support future research in this area."
                },
                "zh": {
                    "title": "结合人类与模型，提升注释质量！",
                    "desc": "本研究提出了一种新的路由框架，旨在结合人类和语言模型（LM）的输入，以提高注释质量并降低人类注释的总成本。我们通过优化问题来识别需要人类注释的偏好实例，并训练一个性能预测模型来预测奖励模型在不同人类和LM注释组合下的表现。实验结果表明，使用我们的路由框架选择的混合注释组合在奖励模型性能上优于单独使用人类或LM注释。我们还分析了路由模型的特征，以识别哪些实例更能从人类反馈中受益。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.18076",
            "title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration",
            "url": "https://huggingface.co/papers/2410.18076",
            "abstract": "Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement. In this work, we study how unlabeled prior trajectory data can be leveraged to learn efficient exploration strategies. While prior data can be used to pretrain a set of low-level skills, or as additional off-policy data for online RL, it has been unclear how to combine these ideas effectively for online exploration. Our method SUPE (Skills from Unlabeled Prior data for Exploration) demonstrates that a careful combination of these ideas compounds their benefits. Our method first extracts low-level skills using a variational autoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an optimistic reward model, transforming prior data into high-level, task-relevant examples. Finally, SUPE uses these transformed examples as additional off-policy data for online RL to learn a high-level policy that composes pretrained low-level skills to explore efficiently. We empirically show that SUPE reliably outperforms prior strategies, successfully solving a suite of long-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.",
            "score": 1,
            "issue_id": 300,
            "pub_date": "2024-10-23",
            "pub_date_card": {
                "ru": "23 октября",
                "en": "October 23",
                "zh": "10月23日"
            },
            "hash": "4c364a800e98d62f",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Эффективное исследование в RL с помощью непомеченных данных",
                    "desc": "Эта статья представляет метод SUPE для эффективного исследования в обучении с подкреплением, используя непомеченные предварительные данные траекторий. SUPE сначала извлекает низкоуровневые навыки с помощью вариационного автоэнкодера (VAE), затем псевдо-размечает непомеченные траектории с использованием оптимистичной модели вознаграждения. Затем метод использует эти преобразованные примеры как дополнительные офф-полиси данные для онлайн-обучения с подкреплением. SUPE превосходит предыдущие стратегии в решении задач с долгосрочным горизонтом и редкими вознаграждениями."
                },
                "en": {
                    "title": "Unlocking Exploration with Unlabeled Data in RL",
                    "desc": "This paper introduces a method called SUPE, which stands for Skills from Unlabeled Prior data for Exploration, aimed at improving exploration strategies in reinforcement learning (RL). The approach utilizes unlabeled trajectory data to pretrain low-level skills using a variational autoencoder (VAE) and then pseudo-labels this data with an optimistic reward model. By transforming prior data into high-level, task-relevant examples, SUPE enhances the learning process in online RL by providing additional off-policy data. The results demonstrate that SUPE outperforms existing methods in solving complex tasks with long horizons and sparse rewards."
                },
                "zh": {
                    "title": "利用未标记数据提升强化学习探索效率",
                    "desc": "这篇论文探讨了如何利用未标记的先前轨迹数据来学习有效的探索策略，特别是在强化学习（RL）中。研究者提出了一种名为SUPE的方法，通过变分自编码器（VAE）提取低级技能，并使用乐观奖励模型对未标记的轨迹进行伪标签化，从而将先前数据转化为高层次的任务相关示例。SUPE将这些转化后的示例作为额外的离线数据，用于在线强化学习，以学习一个高层次的策略，从而有效地组合预训练的低级技能进行探索。实验结果表明，SUPE在解决一系列长时间跨度和稀疏奖励的任务中，表现优于之前的策略。"
                }
            }
        }
    ],
    "link_prev": "2024-10-25.html",
    "link_next": "2024-10-29.html",
    "short_date_prev": {
        "ru": "25.10",
        "en": "10/25",
        "zh": "10月25日"
    },
    "short_date_next": {
        "ru": "29.10",
        "en": "10/29",
        "zh": "10月29日"
    },
    "categories": {
        "#dataset": 3,
        "#data": 2,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 0,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#medicine": 1,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#quantum": 0,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种对比损失的计算策略。传统方法受限于GPU内存消耗的增加。作者提出了一种基于块的计算策略，避免了相似矩阵的完全实例化。此外，他们还引入了多级块策略，利用分布式系统的层次结构。实验结果表明，该方法可以显著扩大批量大小，并保持准确性。",
        "title": "Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss",
        "pinyin": "Zhè piān wénzhāng jièshào le yī zhǒng duìbǐ sǔnshī de jìsuàn cèlüè. Chuántǒng fāngfǎ shòuxiàn yú GPU nèicùn xiāohào de zēngjiā. Zuòzhě tíchū le yī zhǒng jīyú kuài de jìsuàn cèlüè, bìmiǎn le xiāngsì jǔzhèn de wánquán shílìhuà. Cǐwài, tāmen hái yǐnrù le duō jí kuài cèlüè, lìyòng fēnbùshì xìtǒng de céngcì jiégòu. Shíyàn jiéguǒ biǎomíng, gāi fāngfǎ kěyǐ xiǎnzhù kuòdà pīliàng dàxiao, bìng bǎochí zhǔnquèxìng.\n\nHere is the pinyin transcription for the given text.",
        "vocab": "[\n    {\"word\": \"对比损失\", \"pinyin\": \"duìbǐ sǔnshī\", \"trans\": \"contrastive loss\"},\n    {\"word\": \"计算策略\", \"pinyin\": \"jìsuàn cèlüè\", \"trans\": \"computational strategy\"},\n    {\"word\": \"传统方法\", \"pinyin\": \"chuántǒng fāngfǎ\", \"trans\": \"traditional method\"},\n    {\"word\": \"受限于\", \"pinyin\": \"shòu xiàn yú\", \"trans\": \"limited by\"},\n    {\"word\": \"GPU内存消耗\", \"pinyin\": \"GPU nèicún xiāohào\", \"trans\": \"GPU memory consumption\"},\n    {\"word\": \"提出\", \"pinyin\": \"tíchū\", \"trans\": \"propose\"},\n    {\"word\": \"基于块的计算策略\", \"pinyin\": \"jīyú kuài de jìsuàn cèlüè\", \"trans\": \"block-based computational strategy\"},\n    {\"word\": \"避免\", \"pinyin\": \"bìmiǎn\", \"trans\": \"avoid\"},\n    {\"word\": \"相似矩阵\", \"pinyin\": \"xiāngsì jǔzhèn\", \"trans\": \"similarity matrix\"},\n    {\"word\": \"完全实例化\", \"pinyin\": \"wánquán shílìhuà\", \"trans\": \"fully instantiate\"},\n    {\"word\": \"多级块策略\", \"pinyin\": \"duōjí kuài cèlüè\", \"trans\": \"multi-level block strategy\"},\n    {\"word\": \"利用\", \"pinyin\": \"lìyòng\", \"trans\": \"utilize\"},\n    {\"word\": \"分布式系统\", \"pinyin\": \"fēnbùshì xìtǒng\", \"trans\": \"distributed system\"},\n    {\"word\": \"层次结构\", \"pinyin\": \"céngcì jiégòu\", \"trans\": \"hierarchical structure\"},\n    {\"word\": \"实验结果\", \"pinyin\": \"shíyàn jiéguǒ\", \"trans\": \"experimental results\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎnzhù\", \"trans\": \"significant\"},\n    {\"word\": \"扩大\", \"pinyin\": \"kuòdà\", \"trans\": \"expand\"},\n    {\"word\": \"批量大小\", \"pinyin\": \"pīliàng dàxiaǒ\", \"trans\": \"batch size\"},\n    {\"word\": \"准确性\", \"pinyin\": \"zhǔnquèxìng\", \"trans\": \"accuracy\"}\n]",
        "trans": "This article introduces a strategy for computing contrastive loss. Traditional methods are limited by the increase in GPU memory consumption. The authors propose a block-based computing strategy that avoids the complete instantiation of similarity matrices. Additionally, they introduce a multi-level block strategy that leverages the hierarchical structure of distributed systems. Experimental results demonstrate that this method can significantly increase batch size while maintaining accuracy.",
        "update_ts": "2024-10-27 10:11"
    }
}