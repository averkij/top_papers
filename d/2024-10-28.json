{
    "date": {
        "ru": "28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 28",
        "zh": "10æœˆ28æ—¥"
    },
    "time_utc": "2024-10-28 09:00",
    "weekday": 0,
    "issue_id": 319,
    "home_page_url": "https://huggingface.co/papers?date=2024-10-28",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.17856",
            "title": "ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting",
            "url": "https://huggingface.co/papers/2410.17856",
            "abstract": "Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. A key issue is the difficulty in smoothly connecting individual entities in low-level observations with abstract concepts required for planning. A common approach to address this problem is through the use of hierarchical agents, where VLMs serve as high-level reasoners that break down tasks into executable sub-tasks, typically specified using language and imagined observations. However, language often fails to effectively convey spatial information, while generating future images with sufficient accuracy remains challenging. To address these limitations, we propose visual-temporal context prompting, a novel communication protocol between VLMs and policy models. This protocol leverages object segmentation from both past and present observations to guide policy-environment interactions. Using this approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual observations and segmentation masks, with real-time object tracking provided by SAM-2. Our method unlocks the full potential of VLMs visual-language reasoning abilities, enabling them to solve complex creative tasks, especially those heavily reliant on spatial understanding. Experiments in Minecraft demonstrate that our approach allows agents to accomplish previously unattainable tasks, highlighting the effectiveness of visual-temporal context prompting in embodied decision-making. Codes and demos will be available on the project page: https://craftjarvis.github.io/ROCKET-1.",
            "score": 48,
            "issue_id": 302,
            "pub_date": "2024-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "0a477d46d035f1d4",
            "data": {
                "categories": [
                    "#agents",
                    "#cv",
                    "#multimodal",
                    "#rl"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ´Ğ»Ñ VLM Ğ² Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ VLM Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… Ğ¸ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ ÑÑ€ĞµĞ´Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Minecraft Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ€Ğ°Ğ½ĞµĞµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ¸Ğ¶Ğ¸Ğ¼Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Spatial Reasoning in Decision-Making with VLMs",
                    "desc": "This paper addresses the challenges of using vision-language models (VLMs) for decision-making in dynamic environments. It highlights the difficulty of linking low-level visual data with high-level abstract concepts necessary for planning. The authors introduce a new method called visual-temporal context prompting, which enhances communication between VLMs and policy models by utilizing object segmentation from past and present observations. Their approach, implemented in the ROCKET-1 model, demonstrates improved performance in complex tasks that require spatial reasoning, as shown in experiments conducted in Minecraft."
                },
                "zh": {
                    "title": "è§†è§‰-æ—¶é—´ä¸Šä¸‹æ–‡æç¤ºï¼šæå‡å…·èº«å†³ç­–èƒ½åŠ›çš„å…³é”®",
                    "desc": "è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­è¿›è¡Œå…·èº«å†³ç­–æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸»è¦é—®é¢˜æ˜¯å¦‚ä½•å°†ä½çº§è§‚å¯Ÿä¸­çš„ä¸ªä½“å®ä½“ä¸è§„åˆ’æ‰€éœ€çš„æŠ½è±¡æ¦‚å¿µé¡ºåˆ©è¿æ¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è§†è§‰-æ—¶é—´ä¸Šä¸‹æ–‡æç¤ºçš„æ–°é€šä¿¡åè®®ï¼Œåˆ©ç”¨è¿‡å»å’Œç°åœ¨è§‚å¯Ÿä¸­çš„ç‰©ä½“åˆ†å‰²æ¥æŒ‡å¯¼ç­–ç•¥ä¸ç¯å¢ƒçš„äº¤äº’ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ROCKET-1ï¼Œä¸€ä¸ªåŸºäºè§†è§‰è§‚å¯Ÿå’Œåˆ†å‰²æ©ç é¢„æµ‹åŠ¨ä½œçš„ä½çº§ç­–ç•¥ï¼Œå±•ç¤ºäº†åœ¨å¤æ‚åˆ›æ„ä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯ä¾èµ–ç©ºé—´ç†è§£çš„ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.16048",
            "title": "Continuous Speech Synthesis using per-token Latent Diffusion",
            "url": "https://huggingface.co/papers/2410.16048",
            "abstract": "The success of autoregressive transformer models with discrete tokens has inspired quantization-based approaches for continuous modalities, though these often limit reconstruction quality. We therefore introduce SALAD, a per-token latent diffusion model for zero-shot text-to-speech, that operates on continuous representations. SALAD builds upon the recently proposed expressive diffusion head for image generation, and extends it to generate variable-length outputs. Our approach utilizes semantic tokens for providing contextual information and determining the stopping condition. We suggest three continuous variants for our method, extending popular discrete speech synthesis techniques. Additionally, we implement discrete baselines for each variant and conduct a comparative analysis of discrete versus continuous speech modeling techniques. Our results demonstrate that both continuous and discrete approaches are highly competent, and that SALAD achieves a superior intelligibility score while obtaining speech quality and speaker similarity on par with the ground-truth audio.",
            "score": 28,
            "issue_id": 308,
            "pub_date": "2024-10-21",
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            },
            "hash": "8d8228e9ec5fdf77",
            "data": {
                "categories": [
                    "#audio",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "SALAD: Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸",
                    "desc": "SALAD - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "SALAD: Revolutionizing Text-to-Speech with Continuous Representations",
                    "desc": "This paper presents SALAD, a novel per-token latent diffusion model designed for zero-shot text-to-speech synthesis using continuous representations. Unlike traditional methods that rely on discrete tokens, SALAD leverages semantic tokens to enhance contextual understanding and manage output length. The model builds on recent advancements in diffusion techniques for image generation, adapting them for speech synthesis. Comparative analysis shows that SALAD not only matches the quality of existing discrete methods but also excels in intelligibility, making it a promising approach for high-quality speech generation."
                },
                "zh": {
                    "title": "SALADï¼šæå‡æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSALADçš„æ¨¡å‹ï¼Œå®ƒæ˜¯ä¸€ç§åŸºäºæ¯ä¸ªæ ‡è®°çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°é›¶-shotæ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ã€‚SALADåˆ©ç”¨è¿ç»­è¡¨ç¤ºï¼Œå…‹æœäº†ä¼ ç»Ÿç¦»æ•£æ ‡è®°æ¨¡å‹åœ¨é‡å»ºè´¨é‡ä¸Šçš„é™åˆ¶ã€‚è¯¥æ¨¡å‹å€Ÿé‰´äº†ç”¨äºå›¾åƒç”Ÿæˆçš„æ‰©æ•£å¤´ï¼Œå¹¶æ‰©å±•åˆ°ç”Ÿæˆå¯å˜é•¿åº¦çš„è¾“å‡ºã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒSALADåœ¨è¯­éŸ³å¯æ‡‚åº¦ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒåŒæ—¶åœ¨è¯­éŸ³è´¨é‡å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢ä¸çœŸå®éŸ³é¢‘ç›¸å½“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19008",
            "title": "Teach Multimodal LLMs to Comprehend Electrocardiographic Images",
            "url": "https://huggingface.co/papers/2410.19008",
            "abstract": "The electrocardiogram (ECG) is an essential non-invasive diagnostic tool for assessing cardiac conditions. Existing automatic interpretation methods suffer from limited generalizability, focusing on a narrow range of cardiac conditions, and typically depend on raw physiological signals, which may not be readily available in resource-limited settings where only printed or digital ECG images are accessible. Recent advancements in multimodal large language models (MLLMs) present promising opportunities for addressing these challenges. However, the application of MLLMs to ECG image interpretation remains challenging due to the lack of instruction tuning datasets and well-established ECG image benchmarks for quantitative evaluation. To address these challenges, we introduce ECGInstruct, a comprehensive ECG image instruction tuning dataset of over one million samples, covering a wide range of ECG-related tasks from diverse data sources. Using ECGInstruct, we develop PULSE, an MLLM tailored for ECG image comprehension. In addition, we curate ECGBench, a new evaluation benchmark covering four key ECG image interpretation tasks across nine different datasets. Our experiments show that PULSE sets a new state-of-the-art, outperforming general MLLMs with an average accuracy improvement of 15% to 30%. This work highlights the potential of PULSE to enhance ECG interpretation in clinical practice.",
            "score": 22,
            "issue_id": 300,
            "pub_date": "2024-10-21",
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            },
            "hash": "85936de603f8cc7a",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#medicine",
                    "#multimodal"
                ],
                "emoji": "â¤ï¸",
                "ru": {
                    "title": "PULSE: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ­ĞšĞ“ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ­ĞšĞ“ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ECGInstruct - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ­ĞšĞ“ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ PULSE - ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ MLLM Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ­ĞšĞ“. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ECGBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ­ĞšĞ“. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ PULSE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ MLLM Ğ½Ğ° 15-30% Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ­ĞšĞ“ Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ."
                },
                "en": {
                    "title": "Revolutionizing ECG Interpretation with PULSE and ECGInstruct",
                    "desc": "This paper presents ECGInstruct, a large dataset designed for instruction tuning of multimodal large language models (MLLMs) specifically for interpreting ECG images. The authors introduce PULSE, an MLLM that leverages ECGInstruct to improve the understanding of ECG images, addressing the limitations of existing methods that rely on raw signals. Additionally, they create ECGBench, a benchmark for evaluating ECG image interpretation across various tasks and datasets. The results demonstrate that PULSE significantly enhances accuracy in ECG interpretation, showcasing its potential for clinical applications."
                },
                "zh": {
                    "title": "PULSEï¼šæå‡å¿ƒç”µå›¾è§£è¯»çš„æ–°æ–¹æ³•",
                    "desc": "å¿ƒç”µå›¾ï¼ˆECGï¼‰æ˜¯è¯„ä¼°å¿ƒè„çŠ¶å†µçš„é‡è¦éä¾µå…¥æ€§è¯Šæ–­å·¥å…·ã€‚ç°æœ‰çš„è‡ªåŠ¨è§£è¯»æ–¹æ³•åœ¨é€šç”¨æ€§ä¸Šå­˜åœ¨å±€é™ï¼Œé€šå¸¸åªå…³æ³¨å°‘æ•°å¿ƒè„ç–¾ç—…ï¼Œå¹¶ä¾èµ–äºç”Ÿç†ä¿¡å·ï¼Œè¿™åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­éš¾ä»¥è·å–ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ECGInstructï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è¶…è¿‡ä¸€ç™¾ä¸‡ä¸ªæ ·æœ¬çš„å¿ƒç”µå›¾å›¾åƒæŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šç§å¿ƒç”µå›¾ç›¸å…³ä»»åŠ¡ã€‚åŸºäºECGInstructï¼Œæˆ‘ä»¬å¼€å‘äº†PULSEï¼Œä¸€ä¸ªä¸“ä¸ºå¿ƒç”µå›¾å›¾åƒç†è§£è€Œè®¾è®¡çš„å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œå®éªŒç»“æœæ˜¾ç¤ºPULSEåœ¨å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†é€šç”¨æ¨¡å‹ï¼Œæå‡å¹…åº¦è¾¾åˆ°15%åˆ°30%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19355",
            "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality",
            "url": "https://huggingface.co/papers/2410.19355",
            "abstract": "In this paper, we present \\textit{FasterCache}, a novel training-free strategy designed to accelerate the inference of video diffusion models with high-quality generation. By analyzing existing cache-based methods, we observe that directly reusing adjacent-step features degrades video quality due to the loss of subtle variations. We further perform a pioneering investigation of the acceleration potential of classifier-free guidance (CFG) and reveal significant redundancy between conditional and unconditional features within the same timestep. Capitalizing on these observations, we introduce FasterCache to substantially accelerate diffusion-based video generation. Our key contributions include a dynamic feature reuse strategy that preserves both feature distinction and temporal continuity, and CFG-Cache which optimizes the reuse of conditional and unconditional outputs to further enhance inference speed without compromising video quality. We empirically evaluate FasterCache on recent video diffusion models. Experimental results show that FasterCache can significantly accelerate video generation (\\eg 1.67times speedup on Vchitect-2.0) while keeping video quality comparable to the baseline, and consistently outperform existing methods in both inference speed and video quality.",
            "score": 20,
            "issue_id": 301,
            "pub_date": "2024-10-25",
            "pub_date_card": {
                "ru": "25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 25",
                "zh": "10æœˆ25æ—¥"
            },
            "hash": "2de0b0700140bc7e",
            "data": {
                "categories": [
                    "#diffusion",
                    "#inference",
                    "#video"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "FasterCache: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "FasterCache - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ĞºÑÑˆ-Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². FasterCache Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ CFG-ĞºÑÑˆ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ² 1,67 Ñ€Ğ°Ğ·Ğ° Ğ´Ğ»Ñ Vchitect-2.0) Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Accelerating Video Generation with FasterCache!",
                    "desc": "This paper introduces FasterCache, a new method to speed up video generation using diffusion models without needing extra training. The authors found that reusing features from adjacent steps can hurt video quality by losing important details. They also discovered that there is a lot of overlap between features used for conditional and unconditional generation, which can be optimized. FasterCache uses a smart way to reuse features that maintains quality and speed, achieving significant improvements in video generation speed while keeping the quality high."
                },
                "zh": {
                    "title": "FasterCacheï¼šåŠ é€Ÿè§†é¢‘ç”Ÿæˆçš„æ–°ç­–ç•¥",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFasterCacheçš„æ–°ç­–ç•¥ï¼Œæ—¨åœ¨åŠ é€Ÿè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡çš„ç”Ÿæˆæ•ˆæœã€‚é€šè¿‡åˆ†æç°æœ‰çš„åŸºäºç¼“å­˜çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å‘ç°ç›´æ¥é‡ç”¨ç›¸é‚»æ­¥éª¤çš„ç‰¹å¾ä¼šå¯¼è‡´è§†é¢‘è´¨é‡ä¸‹é™ï¼Œå› ä¸ºç»†å¾®å˜åŒ–ä¼šä¸¢å¤±ã€‚æˆ‘ä»¬è¿˜é¦–æ¬¡ç ”ç©¶äº†æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰çš„åŠ é€Ÿæ½œåŠ›ï¼Œæ­ç¤ºäº†åŒä¸€æ—¶é—´æ­¥å†…æ¡ä»¶ç‰¹å¾å’Œæ— æ¡ä»¶ç‰¹å¾ä¹‹é—´çš„æ˜¾è‘—å†—ä½™ã€‚FasterCacheé€šè¿‡åŠ¨æ€ç‰¹å¾é‡ç”¨ç­–ç•¥å’Œä¼˜åŒ–æ¡ä»¶ä¸æ— æ¡ä»¶è¾“å‡ºçš„ç¼“å­˜ï¼Œæ˜¾è‘—æé«˜äº†æ‰©æ•£è§†é¢‘ç”Ÿæˆçš„é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒäº†è§†é¢‘è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19168",
            "title": "MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark",
            "url": "https://huggingface.co/papers/2410.19168",
            "abstract": "The ability to comprehend audio--which includes speech, non-speech sounds, and music--is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex reasoning. MMAU comprises 10k carefully curated audio clips paired with human-annotated natural language questions and answers spanning speech, environmental sounds, and music. It includes information extraction and reasoning questions, requiring models to demonstrate 27 distinct skills across unique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes advanced perception and reasoning with domain-specific knowledge, challenging models to tackle tasks akin to those faced by experts. We assess 18 open-source and proprietary (Large) Audio-Language Models, demonstrating the significant challenges posed by MMAU. Notably, even the most advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves only 52.50%, highlighting considerable room for improvement. We believe MMAU will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.",
            "score": 19,
            "issue_id": 300,
            "pub_date": "2024-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "7a747d9a9b0717cc",
            "data": {
                "categories": [
                    "#audio",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "MMAU: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ´Ğ»Ñ Ğ˜Ğ˜",
                    "desc": "MMAU - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 10 Ñ‚Ñ‹ÑÑÑ‡ Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑ‡ÑŒ, Ğ·Ğ²ÑƒĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ. MMAU Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ 27 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾. Ğ”Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Gemini Pro v1.5 Ğ¸ Qwen2-Audio, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸ÑˆÑŒ Ğ¾ĞºĞ¾Ğ»Ğ¾ 53%, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "MMAU: Raising the Bar for Audio Understanding in AI",
                    "desc": "The paper introduces MMAU, a new benchmark for evaluating multimodal audio understanding models, which is essential for AI to interpret various audio types like speech and music. It consists of 10,000 audio clips with corresponding human-annotated questions and answers that require advanced reasoning and domain-specific knowledge. The benchmark tests models on 27 distinct skills through complex tasks, pushing the limits of current audio understanding capabilities. Results show that even leading models struggle, indicating a need for further advancements in the field."
                },
                "zh": {
                    "title": "MMAUï¼šæ¨åŠ¨éŸ³é¢‘ç†è§£æ¨¡å‹çš„è¿›æ­¥",
                    "desc": "MMAUæ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€éŸ³é¢‘ç†è§£æ¨¡å‹åœ¨éœ€è¦ä¸“å®¶çº§çŸ¥è¯†å’Œå¤æ‚æ¨ç†çš„ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚å®ƒåŒ…å«1ä¸‡ä¸ªç²¾å¿ƒç­–åˆ’çš„éŸ³é¢‘ç‰‡æ®µï¼Œå¹¶é…æœ‰äººå·¥æ ‡æ³¨çš„è‡ªç„¶è¯­è¨€é—®é¢˜å’Œç­”æ¡ˆï¼Œæ¶µç›–äº†è¯­éŸ³ã€ç¯å¢ƒå£°éŸ³å’ŒéŸ³ä¹ã€‚MMAUè¦æ±‚æ¨¡å‹å±•ç¤º27ç§ä¸åŒçš„æŠ€èƒ½ï¼Œå¤„ç†ç‹¬ç‰¹ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¼ºè°ƒé«˜çº§æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚é€šè¿‡è¯„ä¼°18ä¸ªå¼€æºå’Œä¸“æœ‰çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿé¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ï¼Œè¡¨æ˜åœ¨éŸ³é¢‘ç†è§£é¢†åŸŸè¿˜æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.18558",
            "title": "Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data",
            "url": "https://huggingface.co/papers/2410.18558",
            "abstract": "Vision-Language Models (VLMs) have recently made significant progress, but the limited scale and quality of open-source instruction data hinder their performance compared to closed-source models. In this work, we address this limitation by introducing Infinity-MM, a large-scale multimodal instruction dataset with 40 million samples, enhanced through rigorous quality filtering and deduplication. We also propose a synthetic instruction generation method based on open-source VLMs, using detailed image annotations and diverse question generation. Using this data, we trained a 2-billion-parameter VLM, Aquila-VL-2B, achieving state-of-the-art (SOTA) performance for models of similar scale. This demonstrates that expanding instruction data and generating synthetic data can significantly improve the performance of open-source models.",
            "score": 17,
            "issue_id": 300,
            "pub_date": "2024-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "18e760a965f56e6d",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#multimodal",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ - ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Infinity-MM - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 40 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ². ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¾Ğ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ 2-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Aquila-VL-2B, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ÑˆÑƒÑ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking VLM Potential with Infinity-MM Dataset",
                    "desc": "This paper presents Infinity-MM, a large-scale multimodal instruction dataset containing 40 million samples, aimed at improving the performance of Vision-Language Models (VLMs). The authors highlight the challenges faced by open-source VLMs due to limited instruction data quality and scale. They introduce a synthetic instruction generation method that leverages detailed image annotations and diverse question generation to enhance the dataset. Training their 2-billion-parameter model, Aquila-VL-2B, on this enriched dataset resulted in state-of-the-art performance, showcasing the potential of expanded and synthetic instruction data for open-source models."
                },
                "zh": {
                    "title": "æ‰©å±•æ•°æ®ï¼Œæå‡å¼€æºæ¨¡å‹æ€§èƒ½ï¼",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®é›†Infinity-MMï¼ŒåŒ…å«4000ä¸‡æ¡æ ·æœ¬ï¼Œæ—¨åœ¨æå‡å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ€§èƒ½ã€‚é€šè¿‡ä¸¥æ ¼çš„è´¨é‡è¿‡æ»¤å’Œå»é‡ï¼Œè¯¥æ•°æ®é›†çš„è´¨é‡å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºå¼€æºVLMçš„åˆæˆæŒ‡ä»¤ç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨è¯¦ç»†çš„å›¾åƒæ³¨é‡Šå’Œå¤šæ ·çš„é—®é¢˜ç”Ÿæˆã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå…·æœ‰20äº¿å‚æ•°çš„VLMæ¨¡å‹Aquila-VL-2Bï¼Œè¾¾åˆ°äº†åŒè§„æ¨¡æ¨¡å‹çš„æœ€æ–°æ€§èƒ½ï¼Œè¯æ˜äº†æ‰©å±•æŒ‡ä»¤æ•°æ®å’Œç”Ÿæˆåˆæˆæ•°æ®çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19123",
            "title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design",
            "url": "https://huggingface.co/papers/2410.19123",
            "abstract": "The proliferation of large language models (LLMs) has led to the adoption of Mixture-of-Experts (MoE) architectures that dynamically leverage specialized subnetworks for improved efficiency and performance. Despite their benefits, MoE models face significant challenges during inference, including inefficient memory management and suboptimal batching, due to misaligned design choices between the model architecture and the system policies. Furthermore, the conventional approach of training MoEs from scratch is increasingly prohibitive in terms of cost. In this paper, we propose a novel framework Read-ME that transforms pre-trained dense LLMs into smaller MoE models (in contrast to \"upcycling\" generalist MoEs), avoiding the high costs of ground-up training. Our approach employs activation sparsity to extract experts. To compose experts, we examine the widely-adopted layer-wise router design and show its redundancy, and thus we introduce the pre-gating router decoupled from the MoE backbone that facilitates system-friendly pre-computing and lookahead scheduling, enhancing expert-aware batching and caching. Our codesign therefore addresses critical gaps on both the algorithmic and system fronts, establishing a scalable and efficient alternative for LLM inference in resource-constrained settings. Read-ME outperforms other popular open-source dense models of similar scales, achieving improvements of up to 10.1% on MMLU, and improving mean end-to-end latency up to 6.1%. Codes are available at: https://github.com/VITA-Group/READ-ME.",
            "score": 15,
            "issue_id": 319,
            "pub_date": "2024-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "94c8bf1991abe8b0",
            "data": {
                "categories": [
                    "#architecture",
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² MoE Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Read-ME, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Mixture-of-Experts (MoE). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾ÑƒÑ‚Ğ¸Ğ½Ğ³Ğ°, Ğ¾Ñ‚Ğ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ MoE. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ğ°ĞºĞµÑ‚Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¸ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Read-ME Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 10.1% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MMLU Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 6.1%."
                },
                "en": {
                    "title": "Transforming Dense Models into Efficient MoE with Read-ME",
                    "desc": "This paper introduces Read-ME, a framework that converts pre-trained dense large language models (LLMs) into smaller Mixture-of-Experts (MoE) models, which enhances efficiency and performance. The authors address challenges in MoE models during inference, such as memory management and batching issues, by proposing a new pre-gating router design that allows for better system integration. By utilizing activation sparsity, Read-ME effectively extracts and composes experts, leading to improved resource utilization. The results show that Read-ME significantly outperforms existing dense models, achieving notable gains in accuracy and reduced latency."
                },
                "zh": {
                    "title": "Read-MEï¼šé«˜æ•ˆçš„æ··åˆä¸“å®¶æ¨¡å‹è½¬æ¢æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶Read-MEï¼Œæ—¨åœ¨å°†é¢„è®­ç»ƒçš„å¯†é›†å‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è½¬åŒ–ä¸ºæ›´å°çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Œä»è€Œé¿å…ä»å¤´è®­ç»ƒçš„é«˜æˆæœ¬ã€‚æˆ‘ä»¬åˆ©ç”¨æ¿€æ´»ç¨€ç–æ€§æ¥æå–ä¸“å®¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä¸MoEä¸»å¹²è§£è€¦çš„é¢„é—¨æ§è·¯ç”±å™¨ï¼Œä»¥ä¼˜åŒ–ç³»ç»Ÿå‹å¥½çš„é¢„è®¡ç®—å’Œå‰ç»è°ƒåº¦ã€‚è¯¥æ–¹æ³•è§£å†³äº†æ¨¡å‹æ¶æ„ä¸ç³»ç»Ÿç­–ç•¥ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ï¼Œæé«˜äº†ä¸“å®¶æ„ŸçŸ¥çš„æ‰¹å¤„ç†å’Œç¼“å­˜æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRead-MEåœ¨MMLUä¸Šæ¯”å…¶ä»–æµè¡Œçš„å¼€æºå¯†é›†æ¨¡å‹æé«˜äº†10.1%çš„æ€§èƒ½ï¼Œå¹¶å°†å¹³å‡ç«¯åˆ°ç«¯å»¶è¿Ÿé™ä½äº†6.1%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.18889",
            "title": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance",
            "url": "https://huggingface.co/papers/2410.18889",
            "abstract": "NLP benchmarks rely on standardized datasets for training and evaluating models and are crucial for advancing the field. Traditionally, expert annotations ensure high-quality labels; however, the cost of expert annotation does not scale well with the growing demand for larger datasets required by modern models. While crowd-sourcing provides a more scalable solution, it often comes at the expense of annotation precision and consistency. Recent advancements in large language models (LLMs) offer new opportunities to enhance the annotation process, particularly for detecting label errors in existing datasets. In this work, we consider the recent approach of LLM-as-a-judge, leveraging an ensemble of LLMs to flag potentially mislabeled examples. Through a case study of four datasets from the TRUE benchmark, covering different tasks and domains, we empirically analyze the labeling quality of existing datasets, and compare expert, crowd-sourced, and our LLM-based annotations in terms of agreement, label quality, and efficiency, demonstrating the strengths and limitations of each annotation method. Our findings reveal a substantial number of label errors, which, when corrected, induce a significant upward shift in reported model performance. This suggests that many of the LLMs so-called mistakes are due to label errors rather than genuine model failures. Additionally, we discuss the implications of mislabeled data and propose methods to mitigate them in training to improve model performance.",
            "score": 15,
            "issue_id": 315,
            "pub_date": "2024-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "9deca7d98b0025ca",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#dataset"
                ],
                "emoji": "ğŸ·ï¸",
                "ru": {
                    "title": "LLM ĞºĞ°Ğº ÑÑƒĞ´ÑŒÑ: Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ² NLP-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ NLP-Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ, ĞºÑ€Ğ°ÑƒĞ´ÑĞ¾Ñ€ÑĞ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¸ LLM-Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° TRUE. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚ĞºĞ°Ñ…, Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing NLP Dataset Quality with LLMs",
                    "desc": "This paper addresses the challenges of labeling quality in NLP datasets, which are essential for training and evaluating machine learning models. It highlights the limitations of traditional expert annotations and the trade-offs of crowd-sourced labeling, which can compromise precision. The authors propose using large language models (LLMs) as a tool to identify mislabeled data, demonstrating their effectiveness through a case study on various datasets. The results indicate that correcting label errors can significantly enhance model performance, suggesting that many perceived model failures may actually stem from poor labeling rather than model deficiencies."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå‡æ•°æ®é›†æ ‡æ³¨è´¨é‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åŸºå‡†æµ‹è¯•ä¸­æ•°æ®é›†æ ‡æ³¨çš„é‡è¦æ€§ã€‚ä¼ ç»Ÿä¸Šï¼Œä¸“å®¶æ ‡æ³¨ç¡®ä¿äº†é«˜è´¨é‡çš„æ ‡ç­¾ï¼Œä½†éšç€æ•°æ®é›†éœ€æ±‚çš„å¢åŠ ï¼Œä¸“å®¶æ ‡æ³¨çš„æˆæœ¬éš¾ä»¥æ‰©å±•ã€‚è™½ç„¶ä¼—åŒ…æä¾›äº†æ›´å…·å¯æ‰©å±•æ€§çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å¸¸å¸¸ç‰ºç‰²äº†æ ‡æ³¨çš„ç²¾ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„åˆ¤è€…ï¼Œé€šè¿‡å¤šä¸ªLLMçš„ç»„åˆæ¥è¯†åˆ«æ½œåœ¨çš„é”™è¯¯æ ‡ç­¾ï¼Œä»è€Œæé«˜æ ‡æ³¨è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19133",
            "title": "Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback",
            "url": "https://huggingface.co/papers/2410.19133",
            "abstract": "Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, directly collecting human preferences can be expensive, time-consuming, and can have high variance. An appealing alternative is to distill preferences from LMs as a source of synthetic annotations as they are more consistent, cheaper, and scale better than human annotation; however, they are also prone to biases and errors. In this work, we introduce a routing framework that combines inputs from humans and LMs to achieve better annotation quality, while reducing the total cost of human annotation. The crux of our approach is to identify preference instances that will benefit from human annotations. We formulate this as an optimization problem: given a preference dataset and an evaluation metric, we train a performance prediction model to predict a reward model's performance on an arbitrary combination of human and LM annotations and employ a routing strategy that selects a combination that maximizes predicted performance. We train the performance prediction model on MultiPref, a new preference dataset with 10K instances paired with human and LM labels. We show that the selected hybrid mixture of LM and direct human preferences using our routing framework achieves better reward model performance compared to using either one exclusively. We simulate selective human preference collection on three other datasets and show that our method generalizes well to all three. We analyze features from the routing model to identify characteristics of instances that can benefit from human feedback, e.g., prompts with a moderate safety concern or moderate intent complexity. We release the dataset, annotation platform, and source code used in this study to foster more efficient and accurate preference collection in the future.",
            "score": 11,
            "issue_id": 301,
            "pub_date": "2024-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "f6e6f7e5b9e467fe",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#optimization",
                    "#rlhf",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚, ĞºĞ°ĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾Ğ±Ğ¸Ñ‚ÑŒÑÑ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ»Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Optimizing Language Model Annotations with Human and Synthetic Feedback",
                    "desc": "This paper presents a novel routing framework that enhances the quality of annotations for language models by intelligently combining human feedback and synthetic annotations from other language models. The authors address the challenges of collecting human preferences, which can be costly and inconsistent, by proposing a method that identifies which instances would benefit most from human input. They introduce a performance prediction model trained on a new dataset called MultiPref, which helps optimize the selection of human and LM annotations to maximize overall performance. The results demonstrate that this hybrid approach outperforms using either source of annotations alone, and the authors provide tools and datasets to support future research in this area."
                },
                "zh": {
                    "title": "ç»“åˆäººç±»ä¸æ¨¡å‹ï¼Œæå‡æ³¨é‡Šè´¨é‡ï¼",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è·¯ç”±æ¡†æ¶ï¼Œæ—¨åœ¨ç»“åˆäººç±»å’Œè¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„è¾“å…¥ï¼Œä»¥æé«˜æ³¨é‡Šè´¨é‡å¹¶é™ä½äººç±»æ³¨é‡Šçš„æ€»æˆæœ¬ã€‚æˆ‘ä»¬é€šè¿‡ä¼˜åŒ–é—®é¢˜æ¥è¯†åˆ«éœ€è¦äººç±»æ³¨é‡Šçš„åå¥½å®ä¾‹ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªæ€§èƒ½é¢„æµ‹æ¨¡å‹æ¥é¢„æµ‹å¥–åŠ±æ¨¡å‹åœ¨ä¸åŒäººç±»å’ŒLMæ³¨é‡Šç»„åˆä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„è·¯ç”±æ¡†æ¶é€‰æ‹©çš„æ··åˆæ³¨é‡Šç»„åˆåœ¨å¥–åŠ±æ¨¡å‹æ€§èƒ½ä¸Šä¼˜äºå•ç‹¬ä½¿ç”¨äººç±»æˆ–LMæ³¨é‡Šã€‚æˆ‘ä»¬è¿˜åˆ†æäº†è·¯ç”±æ¨¡å‹çš„ç‰¹å¾ï¼Œä»¥è¯†åˆ«å“ªäº›å®ä¾‹æ›´èƒ½ä»äººç±»åé¦ˆä¸­å—ç›Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19730",
            "title": "Counting Ability of Large Language Models and Impact of Tokenization",
            "url": "https://huggingface.co/papers/2410.19730",
            "abstract": "Transformers, the backbone of modern large language models (LLMs), face inherent architectural limitations that impede their reasoning capabilities. Unlike recurrent networks, Transformers lack recurrent connections, confining them to constant-depth computation. This restriction places them in the complexity class TC^0, making them theoretically incapable of solving tasks that demand increasingly deep reasoning as input length grows. Counting, a fundamental component of many reasoning tasks, also requires reasoning depth to grow linearly to be performed inductively. While previous studies have established the upper limits of counting ability in Transformer-based expert models (i.e., models specifically trained for counting tasks), these findings do not directly extend to general-purpose LLMs due to differences in reasoning mechanisms. Recent work has highlighted how Chain of Thought (CoT) reasoning can help alleviate some of the architectural limitations of Transformers in counting tasks. However, little attention has been paid to the role of tokenization in these models. Unlike expert models that often use character-level tokenization, LLMs typically rely on byte-level (BPE) tokenizers, which fundamentally alters the way reasoning is processed. Our work investigates the impact of tokenization on the counting abilities of LLMs, uncovering substantial performance variations based on input tokenization differences. We provide both theoretical and experimental analyses, offering insights into how tokenization choices can undermine models' theoretical computability, thereby inspiring the design of new tokenization methods to enhance reasoning in LLMs.",
            "score": 10,
            "issue_id": 308,
            "pub_date": "2024-10-25",
            "pub_date_card": {
                "ru": "25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 25",
                "zh": "10æœˆ25æ—¥"
            },
            "hash": "103500d45390097e",
            "data": {
                "categories": [
                    "#architecture",
                    "#reasoning"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑÑÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ, ĞºĞ°Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ¾Ñ€Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking Reasoning: The Impact of Tokenization on LLMs",
                    "desc": "This paper explores the limitations of Transformers, which are widely used in large language models (LLMs), particularly in their reasoning capabilities. It highlights that Transformers, unlike recurrent networks, cannot perform deep reasoning due to their constant-depth computation structure. The study focuses on how tokenization methods, specifically byte-level tokenization, affect the counting abilities of LLMs, revealing significant performance differences. The authors propose that understanding these tokenization impacts can lead to better design choices that enhance reasoning in LLMs."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ ‡è®°åŒ–ä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å˜æ¢å™¨æ¶æ„çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢ã€‚å˜æ¢å™¨ç¼ºä¹é€’å½’è¿æ¥ï¼Œå¯¼è‡´å…¶è®¡ç®—æ·±åº¦å—é™ï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†éœ€è¦æ·±åº¦æ¨ç†çš„ä»»åŠ¡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®¡æ•°ä»»åŠ¡çš„æ¨ç†æ·±åº¦éœ€è¦çº¿æ€§å¢é•¿ï¼Œè€Œå˜æ¢å™¨åœ¨è¿™æ–¹é¢å­˜åœ¨ç†è®ºä¸Šçš„ä¸è¶³ã€‚æˆ‘ä»¬åˆ†æäº†è¾“å…¥çš„æ ‡è®°åŒ–æ–¹å¼å¦‚ä½•å½±å“LLMsçš„è®¡æ•°èƒ½åŠ›ï¼Œå¹¶æå‡ºæ–°çš„æ ‡è®°åŒ–æ–¹æ³•ä»¥æå‡æ¨ç†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.19290",
            "title": "Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning",
            "url": "https://huggingface.co/papers/2410.19290",
            "abstract": "Recent studies have identified one aggravating factor of LLM hallucinations as the knowledge inconsistency between pre-training and fine-tuning, where unfamiliar fine-tuning data mislead the LLM to fabricate plausible but wrong outputs. In this paper, we propose a novel fine-tuning strategy called Prereq-Tune to address this knowledge inconsistency and reduce hallucinations. Fundamentally, Prereq-Tune disentangles the learning of skills and knowledge, so the model learns only the task skills without being impacted by the knowledge inconsistency. To achieve this, Prereq-Tune introduces an additional prerequisite learning stage to learn the necessary knowledge for SFT, allowing subsequent SFT to focus only on task skills. Prereq-Tune can also be combined with fictitious synthetic data to enhance the grounding of LLM outputs to their internal knowledge. Experiments show that Prereq-Tune outperforms existing baselines in improving LLM's factuality across short QA and long-form generation tasks. It also opens new possibilities for knowledge-controlled generation in LLMs. Our code is available at https://github.com/UCSB-NLP-Chang/Prereq_tune.git.",
            "score": 10,
            "issue_id": 302,
            "pub_date": "2024-10-25",
            "pub_date_card": {
                "ru": "25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 25",
                "zh": "10æœˆ25æ—¥"
            },
            "hash": "a11e3c6587db25fc",
            "data": {
                "categories": [
                    "#alignment",
                    "#hallucinations",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Prereq-Tune: Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Prereq-Tune, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼ Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼, Ğ²Ğ²Ğ¾Ğ´Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼. Prereq-Tune Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ĞµĞµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Prereq-Tune Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing LLM Factuality with Prereq-Tune",
                    "desc": "This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation."
                },
                "zh": {
                    "title": "è§£å†³LLMå¹»è§‰çš„çŸ¥è¯†ä¸ä¸€è‡´æ€§",
                    "desc": "æœ€è¿‘çš„ç ”ç©¶å‘ç°ï¼ŒLLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰å¹»è§‰çš„ä¸€ä¸ªåŠ é‡å› ç´ æ˜¯é¢„è®­ç»ƒå’Œå¾®è°ƒä¹‹é—´çš„çŸ¥è¯†ä¸ä¸€è‡´ï¼Œè¿™å¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹ä¸ç†Ÿæ‚‰çš„å¾®è°ƒæ•°æ®æ—¶äº§ç”Ÿé”™è¯¯çš„è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¾®è°ƒç­–ç•¥ï¼Œç§°ä¸ºPrereq-Tuneï¼Œæ—¨åœ¨å‡å°‘è¿™ç§çŸ¥è¯†ä¸ä¸€è‡´æ€§ã€‚Prereq-Tuneé€šè¿‡å¼•å…¥é¢å¤–çš„å…ˆå†³å­¦ä¹ é˜¶æ®µï¼Œä½¿æ¨¡å‹åœ¨å¾®è°ƒä¹‹å‰å…ˆå­¦ä¹ å¿…è¦çš„çŸ¥è¯†ï¼Œä»è€Œä¸“æ³¨äºä»»åŠ¡æŠ€èƒ½çš„å­¦ä¹ ã€‚å®éªŒè¡¨æ˜ï¼ŒPrereq-Tuneåœ¨æé«˜LLMçš„äº‹å®æ€§æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸ºçŸ¥è¯†æ§åˆ¶ç”Ÿæˆå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.16090",
            "title": "Analysing the Residual Stream of Language Models Under Knowledge Conflicts",
            "url": "https://huggingface.co/papers/2410.16090",
            "abstract": "Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model behaviour, such as reliance on outdated or incorrect information. In this work, we investigate whether LLMs can identify knowledge conflicts and whether it is possible to know which source of knowledge the model will rely on by analysing the residual stream of the LLM. Through probing tasks, we find that LLMs can internally register the signal of knowledge conflict in the residual stream, which can be accurately detected by probing the intermediate model activations. This allows us to detect conflicts within the residual stream before generating the answers without modifying the input or model parameters. Moreover, we find that the residual stream shows significantly different patterns when the model relies on contextual knowledge versus parametric knowledge to resolve conflicts. This pattern can be employed to estimate the behaviour of LLMs when conflict happens and prevent unexpected answers before producing the answers. Our analysis offers insights into how LLMs internally manage knowledge conflicts and provides a foundation for developing methods to control the knowledge selection processes.",
            "score": 6,
            "issue_id": 316,
            "pub_date": "2024-10-21",
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            },
            "hash": "cddfaa4779e86373",
            "data": {
                "categories": [
                    "#alignment",
                    "#hallucinations",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ñ…Ñ€Ğ°Ğ½ÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ Ğ² Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ñ…, Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ£Ñ‡ĞµĞ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğµ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¾ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ±Ñ‹Ğ»Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² LLM."
                },
                "en": {
                    "title": "Understanding and Managing Knowledge Conflicts in LLMs",
                    "desc": "This paper explores how large language models (LLMs) manage conflicting information between their stored knowledge and the context they are given. It reveals that LLMs can detect these knowledge conflicts by analyzing the residual stream, which is the internal signal that reflects the model's processing. By using probing tasks, the authors demonstrate that different patterns in the residual stream indicate whether the model is relying on its internal knowledge or the contextual information. This understanding can help in predicting model behavior during conflicts and in developing strategies to improve knowledge selection in LLMs."
                },
                "zh": {
                    "title": "è¯†åˆ«çŸ¥è¯†å†²çªï¼Œä¼˜åŒ–æ¨¡å‹è¡Œä¸º",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿåœ¨å…¶å‚æ•°ä¸­å­˜å‚¨å¤§é‡çš„äº‹å®çŸ¥è¯†ã€‚ç„¶è€Œï¼Œè¿™äº›å‚æ•°çŸ¥è¯†å¯èƒ½ä¸ä¸Šä¸‹æ–‡ä¸­æä¾›çš„ä¿¡æ¯å‘ç”Ÿå†²çªã€‚è¿™ç§å†²çªå¯èƒ½å¯¼è‡´æ¨¡å‹äº§ç”Ÿä¸ç†æƒ³çš„è¡Œä¸ºï¼Œä¾‹å¦‚ä¾èµ–è¿‡æ—¶æˆ–ä¸æ­£ç¡®çš„ä¿¡æ¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMsèƒ½å¤Ÿè¯†åˆ«çŸ¥è¯†å†²çªï¼Œå¹¶é€šè¿‡åˆ†ææ®‹å·®æµæ¥äº†è§£æ¨¡å‹å°†ä¾èµ–äºå“ªä¸ªçŸ¥è¯†æ¥æºï¼Œä»è€Œåœ¨ç”Ÿæˆç­”æ¡ˆä¹‹å‰æ£€æµ‹åˆ°å†²çªã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.18912",
            "title": "Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling",
            "url": "https://huggingface.co/papers/2410.18912",
            "abstract": "Videos of robots interacting with objects encode rich information about the objects' dynamics. However, existing video prediction approaches typically do not explicitly account for the 3D information from videos, such as robot actions and objects' 3D states, limiting their use in real-world robotic applications. In this work, we introduce a framework to learn object dynamics directly from multi-view RGB videos by explicitly considering the robot's action trajectories and their effects on scene dynamics. We utilize the 3D Gaussian representation of 3D Gaussian Splatting (3DGS) to train a particle-based dynamics model using Graph Neural Networks. This model operates on sparse control particles downsampled from the densely tracked 3D Gaussian reconstructions. By learning the neural dynamics model on offline robot interaction data, our method can predict object motions under varying initial configurations and unseen robot actions. The 3D transformations of Gaussians can be interpolated from the motions of control particles, enabling the rendering of predicted future object states and achieving action-conditioned video prediction. The dynamics model can also be applied to model-based planning frameworks for object manipulation tasks. We conduct experiments on various kinds of deformable materials, including ropes, clothes, and stuffed animals, demonstrating our framework's ability to model complex shapes and dynamics. Our project page is available at https://gs-dynamics.github.io.",
            "score": 6,
            "issue_id": 313,
            "pub_date": "2024-10-24",
            "pub_date_card": {
                "ru": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 24",
                "zh": "10æœˆ24æ—¥"
            },
            "hash": "2fd905b5809fea40",
            "data": {
                "categories": [
                    "#3d",
                    "#cv",
                    "#robots"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "3D-Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… RGB-Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ† Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑÑ… Ğ¸ Ğ½ĞµĞ²Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ½ĞµĞµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Predicting Object Dynamics in Robot Interactions Using 3D Video Analysis",
                    "desc": "This paper presents a new framework for predicting how objects move when robots interact with them, using videos from multiple angles. It focuses on understanding the 3D aspects of these interactions, such as the robot's actions and the 3D states of the objects. The authors employ a particle-based dynamics model trained with Graph Neural Networks, which allows for accurate predictions of object motions based on robot actions. Their approach is tested on various deformable materials, showing its effectiveness in modeling complex dynamics and shapes in real-world scenarios."
                },
                "zh": {
                    "title": "é€šè¿‡3DåŠ¨æ€å»ºæ¨¡æå‡æœºå™¨äººäº¤äº’èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œé€šè¿‡å¤šè§†è§’RGBè§†é¢‘ç›´æ¥å­¦ä¹ ç‰©ä½“çš„åŠ¨æ€ï¼Œç‰¹åˆ«å…³æ³¨æœºå™¨äººåŠ¨ä½œè½¨è¿¹åŠå…¶å¯¹åœºæ™¯åŠ¨æ€çš„å½±å“ã€‚æˆ‘ä»¬åˆ©ç”¨3Dé«˜æ–¯è¡¨ç¤ºæ³•å’Œå›¾ç¥ç»ç½‘ç»œè®­ç»ƒåŸºäºç²’å­çš„åŠ¨æ€æ¨¡å‹ï¼Œä»ç¨€ç–æ§åˆ¶ç²’å­ä¸­æå–ä¿¡æ¯ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒåˆå§‹é…ç½®å’Œæœªè§è¿‡çš„æœºå™¨äººåŠ¨ä½œä¸‹é¢„æµ‹ç‰©ä½“è¿åŠ¨ï¼Œå¹¶å®ç°åŸºäºåŠ¨ä½œçš„æœªæ¥çŠ¶æ€æ¸²æŸ“ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å»ºæ¨¡å¤æ‚å½¢çŠ¶å’ŒåŠ¨æ€æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œé€‚ç”¨äºå„ç§å¯å˜å½¢ææ–™çš„æ“ä½œä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.17655",
            "title": "Mapping the Media Landscape: Predicting Factual Reporting and Political Bias Through Web Interactions",
            "url": "https://huggingface.co/papers/2410.17655",
            "abstract": "Bias assessment of news sources is paramount for professionals, organizations, and researchers who rely on truthful evidence for information gathering and reporting. While certain bias indicators are discernible from content analysis, descriptors like political bias and fake news pose greater challenges. In this paper, we propose an extension to a recently presented news media reliability estimation method that focuses on modeling outlets and their longitudinal web interactions. Concretely, we assess the classification performance of four reinforcement learning strategies on a large news media hyperlink graph. Our experiments, targeting two challenging bias descriptors, factual reporting and political bias, showed a significant performance improvement at the source media level. Additionally, we validate our methods on the CLEF 2023 CheckThat! Lab challenge, outperforming the reported results in both, F1-score and the official MAE metric. Furthermore, we contribute by releasing the largest annotated dataset of news source media, categorized with factual reporting and political bias labels. Our findings suggest that profiling news media sources based on their hyperlink interactions over time is feasible, offering a bird's-eye view of evolving media landscapes.",
            "score": 5,
            "issue_id": 316,
            "pub_date": "2024-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "0fce9f46807b7f5b",
            "data": {
                "categories": [
                    "#benchmark",
                    "#data",
                    "#dataset",
                    "#rl"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ğ°Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ³Ğ¸Ğ¿ĞµÑ€ÑÑÑ‹Ğ»ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ°Ğ¹Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ñ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing News Bias Detection through Web Interaction Analysis",
                    "desc": "This paper focuses on assessing bias in news sources, which is crucial for accurate information gathering. It introduces an enhanced method for estimating the reliability of news media by analyzing their web interactions over time. The authors evaluate four reinforcement learning strategies on a large graph of news media hyperlinks, achieving improved classification of factual reporting and political bias. They also provide a new, extensive dataset of news sources labeled by bias, demonstrating that tracking hyperlink interactions can effectively profile media bias."
                },
                "zh": {
                    "title": "åŸºäºè¶…é“¾æ¥äº’åŠ¨çš„æ–°é—»åª’ä½“åè§è¯„ä¼°",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†æ–°é—»æ¥æºçš„åè§è¯„ä¼°ï¼Œå¼ºè°ƒäº†å¯¹çœŸå®ä¿¡æ¯æ”¶é›†å’ŒæŠ¥å‘Šçš„é‡è¦æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ‰©å±•çš„æ–°é—»åª’ä½“å¯é æ€§ä¼°è®¡æ–¹æ³•ï¼Œä¸“æ³¨äºå»ºæ¨¡åª’ä½“åŠå…¶é•¿æœŸçš„ç½‘ç»œäº’åŠ¨ã€‚é€šè¿‡å¯¹å¤§å‹æ–°é—»åª’ä½“è¶…é“¾æ¥å›¾çš„å®éªŒï¼Œæˆ‘ä»¬è¯„ä¼°äº†å››ç§å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„åˆ†ç±»æ€§èƒ½ï¼Œç‰¹åˆ«é’ˆå¯¹äº‹å®æŠ¥é“å’Œæ”¿æ²»åè§è¿™ä¸¤ä¸ªæŒ‘æˆ˜æ€§åè§æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºè¶…é“¾æ¥äº’åŠ¨çš„æ–°é—»åª’ä½“æºåˆ†ææ˜¯å¯è¡Œçš„ï¼Œèƒ½å¤Ÿæä¾›å¯¹åª’ä½“ç¯å¢ƒæ¼”å˜çš„å…¨å±€è§†è§’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.16270",
            "title": "Reflection-Bench: probing AI intelligence with reflection",
            "url": "https://huggingface.co/papers/2410.16270",
            "abstract": "The ability to adapt beliefs or behaviors in response to unexpected outcomes, reflection, is fundamental to intelligent systems' interaction with the world. From a cognitive science perspective, this serves as a core principle of intelligence applicable to both human and AI systems. To address the debate on the intelligence of large language models (LLMs), we propose Reflection-Bench, a comprehensive benchmark comprising 7 tasks spanning core cognitive functions crucial for reflection, including perception, memory, belief updating, decision-making, prediction, counterfactual thinking, and meta-reflection. We evaluate the performances of 13 prominent LLMs such as OpenAI o1, GPT-4, Claude 3.5 Sonnet, etc. The results indicate that current LLMs still lack satisfactory reflection ability. We discuss the underlying causes of these results and suggest potential avenues for future research. In conclusion, Reflection-Bench offers both evaluation tools and inspiration for developing AI capable of reliably interacting with the environment. Our data and code are available at https://github.com/YabYum/ReflectionBench.",
            "score": 5,
            "issue_id": 310,
            "pub_date": "2024-10-21",
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            },
            "hash": "2e170064cac1a857",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "ğŸ¤”",
                "ru": {
                    "title": "Reflection-Bench: Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ˜Ğ˜ Ğº ÑĞ°Ğ¼Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Reflection-Bench - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 7 Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµÑ‚Ğ°Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 13 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ½Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğº Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Enhancing AI Reflection: Introducing Reflection-Bench",
                    "desc": "This paper introduces Reflection-Bench, a new benchmark designed to evaluate the reflection capabilities of large language models (LLMs). Reflection is a key cognitive function that allows intelligent systems to adapt their beliefs and behaviors based on unexpected outcomes. The benchmark includes seven tasks that assess various cognitive functions such as memory, decision-making, and counterfactual thinking. The evaluation of 13 leading LLMs reveals that they currently struggle with reflection, highlighting areas for improvement and future research in AI development."
                },
                "zh": {
                    "title": "åæ€èƒ½åŠ›ï¼šæ™ºèƒ½ç³»ç»Ÿçš„æ ¸å¿ƒ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºReflection-Benchçš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åæ€èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ã€‚åæ€èƒ½åŠ›æ˜¯æ™ºèƒ½ç³»ç»Ÿä¸ä¸–ç•Œäº’åŠ¨çš„æ ¸å¿ƒåŸåˆ™ï¼Œæ¶‰åŠæ„ŸçŸ¥ã€è®°å¿†ã€ä¿¡å¿µæ›´æ–°ã€å†³ç­–ã€é¢„æµ‹ã€åäº‹å®æ€ç»´å’Œå…ƒåæ€ç­‰è®¤çŸ¥åŠŸèƒ½ã€‚é€šè¿‡å¯¹13ä¸ªçŸ¥åLLMçš„è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå½“å‰çš„LLMåœ¨åæ€èƒ½åŠ›ä¸Šä»ç„¶ä¸è¶³ã€‚æˆ‘ä»¬è®¨è®ºäº†è¿™äº›ç»“æœçš„åŸå› ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.18076",
            "title": "Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration",
            "url": "https://huggingface.co/papers/2410.18076",
            "abstract": "Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement. In this work, we study how unlabeled prior trajectory data can be leveraged to learn efficient exploration strategies. While prior data can be used to pretrain a set of low-level skills, or as additional off-policy data for online RL, it has been unclear how to combine these ideas effectively for online exploration. Our method SUPE (Skills from Unlabeled Prior data for Exploration) demonstrates that a careful combination of these ideas compounds their benefits. Our method first extracts low-level skills using a variational autoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an optimistic reward model, transforming prior data into high-level, task-relevant examples. Finally, SUPE uses these transformed examples as additional off-policy data for online RL to learn a high-level policy that composes pretrained low-level skills to explore efficiently. We empirically show that SUPE reliably outperforms prior strategies, successfully solving a suite of long-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.",
            "score": 4,
            "issue_id": 300,
            "pub_date": "2024-10-23",
            "pub_date_card": {
                "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 23",
                "zh": "10æœˆ23æ—¥"
            },
            "hash": "4c364a800e98d62f",
            "data": {
                "categories": [
                    "#rl",
                    "#rlhf"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² RL Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SUPE Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. SUPE ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° (VAE), Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ Ğ½ĞµĞ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ ĞºĞ°Ğº Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ„Ñ„-Ğ¿Ğ¾Ğ»Ğ¸ÑĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. SUPE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ¼ Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Unlocking Exploration with Unlabeled Data in RL",
                    "desc": "This paper introduces a method called SUPE, which stands for Skills from Unlabeled Prior data for Exploration, aimed at improving exploration strategies in reinforcement learning (RL). The approach utilizes unlabeled trajectory data to pretrain low-level skills using a variational autoencoder (VAE) and then pseudo-labels this data with an optimistic reward model. By transforming prior data into high-level, task-relevant examples, SUPE enhances the learning process in online RL by providing additional off-policy data. The results demonstrate that SUPE outperforms existing methods in solving complex tasks with long horizons and sparse rewards."
                },
                "zh": {
                    "title": "åˆ©ç”¨æœªæ ‡è®°æ•°æ®æå‡å¼ºåŒ–å­¦ä¹ æ¢ç´¢æ•ˆç‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨æœªæ ‡è®°çš„å…ˆå‰è½¨è¿¹æ•°æ®æ¥å­¦ä¹ æœ‰æ•ˆçš„æ¢ç´¢ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºSUPEçš„æ–¹æ³•ï¼Œé€šè¿‡å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æå–ä½çº§æŠ€èƒ½ï¼Œå¹¶ä½¿ç”¨ä¹è§‚å¥–åŠ±æ¨¡å‹å¯¹æœªæ ‡è®°çš„è½¨è¿¹è¿›è¡Œä¼ªæ ‡ç­¾åŒ–ï¼Œä»è€Œå°†å…ˆå‰æ•°æ®è½¬åŒ–ä¸ºé«˜å±‚æ¬¡çš„ä»»åŠ¡ç›¸å…³ç¤ºä¾‹ã€‚SUPEå°†è¿™äº›è½¬åŒ–åçš„ç¤ºä¾‹ä½œä¸ºé¢å¤–çš„ç¦»çº¿æ•°æ®ï¼Œç”¨äºåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œä»¥å­¦ä¹ ä¸€ä¸ªé«˜å±‚æ¬¡çš„ç­–ç•¥ï¼Œä»è€Œæœ‰æ•ˆåœ°ç»„åˆé¢„è®­ç»ƒçš„ä½çº§æŠ€èƒ½è¿›è¡Œæ¢ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSUPEåœ¨è§£å†³ä¸€ç³»åˆ—é•¿æ—¶é—´è·¨åº¦å’Œç¨€ç–å¥–åŠ±çš„ä»»åŠ¡ä¸­ï¼Œè¡¨ç°ä¼˜äºä¹‹å‰çš„ç­–ç•¥ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-10-25.html",
    "link_next": "2024-10-29.html",
    "link_month": "2024-10.html",
    "short_date_prev": {
        "ru": "25.10",
        "en": "10/25",
        "zh": "10æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "29.10",
        "en": "10/29",
        "zh": "10æœˆ29æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 4,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 2,
        "#rl": 3,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 2,
        "#video": 1,
        "#multimodal": 4,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 2,
        "#medicine": 1,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 2,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 2,
        "#long_context": 0,
        "#synthetic": 3,
        "#translation": 0,
        "#robots": 0
    }
}