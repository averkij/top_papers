{
    "date": {
        "ru": "1 Ğ¼Ğ°Ñ",
        "en": "May 1",
        "zh": "5æœˆ1æ—¥"
    },
    "time_utc": "2025-05-01 05:12",
    "weekday": 3,
    "issue_id": 3528,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.21776",
            "title": "WebThinker: Empowering Large Reasoning Models with Deep Research\n  Capability",
            "url": "https://huggingface.co/papers/2504.21776",
            "abstract": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose WebThinker, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a Deep Web Explorer module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an Autonomous Think-Search-and-Draft strategy, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an RL-based training strategy via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.",
            "score": 14,
            "issue_id": 3525,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 30",
                "zh": "4æœˆ30æ—¥"
            },
            "hash": "61ce82abe42f584a",
            "authors": [
                "Xiaoxi Li",
                "Jiajie Jin",
                "Guanting Dong",
                "Hongjin Qian",
                "Yutao Zhu",
                "Yongkang Wu",
                "Ji-Rong Wen",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "BAAI",
                "Huawei Poisson Lab",
                "Renmin University of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21776.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#science",
                    "#agents",
                    "#rlhf",
                    "#optimization",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ•¸ï¸",
                "ru": {
                    "title": "WebThinker: Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ²ĞµĞ±-Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "WebThinker - ÑÑ‚Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ°Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Deep Web Explorer Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ¾Ğ². WebThinker Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (DPO) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ WebThinker Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Empowering LRMs with Real-Time Web Research Capabilities",
                    "desc": "This paper introduces WebThinker, a deep research agent designed to enhance large reasoning models (LRMs) by enabling them to autonomously search the web for information. Traditional LRMs struggle with complex tasks due to their static internal knowledge, but WebThinker allows them to dynamically gather and synthesize information in real-time. It features a Deep Web Explorer module for navigating web pages and an Autonomous Think-Search-and-Draft strategy that integrates reasoning with information retrieval and report writing. The proposed RL-based training strategy improves the model's performance on complex reasoning benchmarks and scientific report generation tasks, demonstrating significant advancements over existing methods."
                },
                "zh": {
                    "title": "WebThinkerï¼šè®©æ¨ç†æ¨¡å‹æ›´æ™ºèƒ½çš„ç ”ç©¶åŠ©æ‰‹",
                    "desc": "å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å¦‚OpenAI-o1å’ŒDeepSeek-R1åœ¨é•¿æ—¶é—´æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ä¾èµ–é™æ€å†…éƒ¨çŸ¥è¯†ï¼Œé™åˆ¶äº†åœ¨å¤æ‚çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†WebThinkerï¼Œä¸€ä¸ªæ·±åº¦ç ”ç©¶ä»£ç†ï¼Œèƒ½å¤Ÿè®©LRMsè‡ªä¸»æœç´¢ç½‘ç»œã€æµè§ˆç½‘é¡µå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ’°å†™ç ”ç©¶æŠ¥å‘Šã€‚WebThinkeré›†æˆäº†æ·±ç½‘æ¢ç´¢æ¨¡å—ï¼Œä½¿LRMsåœ¨é‡åˆ°çŸ¥è¯†ç©ºç™½æ—¶èƒ½å¤ŸåŠ¨æ€æœç´¢å’Œæå–ä¿¡æ¯ã€‚é€šè¿‡å¼•å…¥åŸºäºå¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜WebThinkeråœ¨å¤æ‚æ¨ç†åŸºå‡†å’Œç§‘å­¦æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21233",
            "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language\n  Models in Math",
            "url": "https://huggingface.co/papers/2504.21233",
            "abstract": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving reasoning in Small Language Models (SLMs) remains challenging due to their limited model capacity. Recent work by Deepseek-R1 demonstrates that distillation from LLM-generated synthetic data can substantially improve the reasoning ability of SLM. However, the detailed modeling recipe is not disclosed. In this work, we present a systematic training recipe for SLMs that consists of four steps: (1) large-scale mid-training on diverse distilled long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3) Rollout DPO leveraging a carefully curated preference dataset, and (4) Reinforcement Learning (RL) with Verifiable Reward. We apply our method on Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning model exceeds, on math reasoning tasks, much larger reasoning models, e.g., outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate that a carefully designed training recipe, with large-scale high-quality CoT data, is effective to unlock strong reasoning capabilities even in resource-constrained small models.",
            "score": 10,
            "issue_id": 3525,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 30",
                "zh": "4æœˆ30æ—¥"
            },
            "hash": "0b800a9195884db4",
            "authors": [
                "Haoran Xu",
                "Baolin Peng",
                "Hany Awadalla",
                "Dongdong Chen",
                "Yen-Chun Chen",
                "Mei Gao",
                "Young Jin Kim",
                "Yunsheng Li",
                "Liliang Ren",
                "Yelong Shen",
                "Shuohang Wang",
                "Weijian Xu",
                "Jianfeng Gao",
                "Weizhu Chen"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.21233.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#transfer_learning",
                    "#small_models",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (SLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Phi-4-Mini (3.8 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²) Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ¶Ğµ Ğ² Ñ€ĞµÑÑƒÑ€ÑĞ½Ğ¾-Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Unlocking Reasoning Power in Small Models",
                    "desc": "This paper discusses how to improve the reasoning abilities of Small Language Models (SLMs) using a systematic training approach. It introduces a four-step recipe that includes mid-training on diverse data, fine-tuning on high-quality data, preference-based training, and reinforcement learning with verifiable rewards. The authors demonstrate that their method significantly enhances the reasoning performance of a compact model, Phi-4-Mini, surpassing larger models in math reasoning tasks. This work highlights the potential of well-structured training strategies to boost the capabilities of smaller models in machine learning."
                },
                "zh": {
                    "title": "å°æ¨¡å‹ä¹Ÿèƒ½å¼ºæ¨ç†ï¼",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¥æå‡å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†å°å‹æ¨¡å‹ç”±äºå®¹é‡é™åˆ¶ï¼Œæå‡æ¨ç†èƒ½åŠ›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ä»LLMç”Ÿæˆçš„åˆæˆæ•°æ®è¿›è¡Œè’¸é¦ï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„SLMçš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„è®­ç»ƒæ–¹æ¡ˆï¼ŒåŒ…æ‹¬å››ä¸ªæ­¥éª¤ï¼Œæœ€ç»ˆåœ¨Phi-4-Miniæ¨¡å‹ä¸Šå®ç°äº†è¶…è¶Šæ›´å¤§æ¨¡å‹çš„æ¨ç†è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.20966",
            "title": "Softpick: No Attention Sink, No Massive Activations with Rectified\n  Softmax",
            "url": "https://huggingface.co/papers/2504.20966",
            "abstract": "We introduce softpick, a rectified, not sum-to-one, drop-in replacement for softmax in transformer attention mechanisms that eliminates attention sink and massive activations. Our experiments with 340M parameter models demonstrate that softpick maintains performance parity with softmax on standard benchmarks while achieving 0% sink rate. The softpick transformer produces hidden states with significantly lower kurtosis (340 vs 33,510) and creates sparse attention maps (46.97% sparsity). Models using softpick consistently outperform softmax when quantized, with particularly pronounced advantages at lower bit precisions. Our analysis and discussion shows how softpick has the potential to open new possibilities for quantization, low-precision training, sparsity optimization, pruning, and interpretability. Our code is available at https://github.com/zaydzuhri/softpick-attention.",
            "score": 7,
            "issue_id": 3526,
            "pub_date": "2025-04-29",
            "pub_date_card": {
                "ru": "29 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 29",
                "zh": "4æœˆ29æ—¥"
            },
            "hash": "cb610c1427bdf307",
            "pdf_title_img": "img/title_stub.png",
            "data": {
                "categories": [
                    "#interpretability",
                    "#architecture",
                    "#inference",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Softpick: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ softpick - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…, Ğ·Ğ°Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¹ softmax. Softpick ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ 'attention sink' Ğ¸ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ softmax. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ softpick ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ÑĞºÑÑ†ĞµÑÑĞ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ñ softpick Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ softmax Ğ¿Ñ€Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Softpick: A Smarter Alternative to Softmax for Transformers",
                    "desc": "This paper presents softpick, a new alternative to the softmax function used in transformer attention mechanisms. Unlike softmax, softpick does not require outputs to sum to one, which helps to avoid issues like attention sink and excessive activations. The authors show that softpick maintains similar performance to softmax while achieving a 0% sink rate and significantly lower kurtosis in hidden states. Additionally, softpick enhances model performance during quantization, especially at lower bit precisions, and offers benefits for sparsity optimization and interpretability."
                },
                "zh": {
                    "title": "softpickï¼šæå‡Transformeræ³¨æ„åŠ›çš„æ–°é€‰æ‹©",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºsoftpickçš„æ–°æ–¹æ³•ï¼Œå®ƒå¯ä»¥æ›¿ä»£transformeræ³¨æ„åŠ›æœºåˆ¶ä¸­çš„softmaxã€‚softpickä¸éœ€è¦å°†æƒé‡å½’ä¸€åŒ–ä¸º1ï¼Œèƒ½å¤Ÿæ¶ˆé™¤æ³¨æ„åŠ›æ²‰æ²¡å’Œå¤§è§„æ¨¡æ¿€æ´»ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨softpickçš„æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ä¸softmaxè¡¨ç°ç›¸å½“ï¼Œä½†æ³¨æ„åŠ›æ²‰æ²¡ç‡ä¸º0%ï¼Œå¹¶ä¸”ç”Ÿæˆçš„éšè—çŠ¶æ€å…·æœ‰æ›´ä½çš„å³°åº¦ã€‚softpickåœ¨é‡åŒ–å’Œä½ç²¾åº¦è®­ç»ƒä¸­è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶åœ¨è¾ƒä½ä½æ•°ç²¾åº¦ä¸‹å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œå±•ç¤ºäº†å…¶åœ¨ç¨€ç–æ€§ä¼˜åŒ–å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21318",
            "title": "Phi-4-reasoning Technical Report",
            "url": "https://huggingface.co/papers/2504.21318",
            "abstract": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.",
            "score": 5,
            "issue_id": 3525,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 30",
                "zh": "4æœˆ30æ—¥"
            },
            "hash": "7004ae060f674e9a",
            "authors": [
                "Marah Abdin",
                "Sahaj Agarwal",
                "Ahmed Awadallah",
                "Vidhisha Balachandran",
                "Harkirat Behl",
                "Lingjiao Chen",
                "Gustavo de Rosa",
                "Suriya Gunasekar",
                "Mojan Javaheripi",
                "Neel Joshi",
                "Piero Kauffmann",
                "Yash Lara",
                "Caio CÃ©sar Teodoro Mendes",
                "Arindam Mitra",
                "Besmira Nushi",
                "Dimitris Papailiopoulos",
                "Olli Saarikivi",
                "Shital Shah",
                "Vaishnavi Shrivastava",
                "Vibhav Vineet",
                "Yue Wu",
                "Safoora Yousefi",
                "Guoqing Zheng"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.21318.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#dataset",
                    "#math",
                    "#transfer_learning",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ¾Ñ‰Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Phi-4-reasoning - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Phi-4-reasoning Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ, Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Unlocking Complex Reasoning with Phi-4-Reasoning",
                    "desc": "The paper presents Phi-4-reasoning, a large-scale reasoning model with 14 billion parameters that excels in complex reasoning tasks. It is trained using supervised fine-tuning on a diverse set of carefully selected prompts, which helps it generate detailed reasoning chains during inference. An enhanced version, Phi-4-reasoning-plus, incorporates reinforcement learning to improve performance by producing longer reasoning traces. The models demonstrate superior performance compared to larger models and show significant improvements across various reasoning benchmarks, highlighting the importance of data curation and training methodologies in developing effective reasoning models."
                },
                "zh": {
                    "title": "æ¨ç†æ¨¡å‹çš„æ–°çªç ´ï¼šPhi-4-reasoning",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Phi-4-reasoningï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰140äº¿å‚æ•°çš„æ¨ç†æ¨¡å‹ï¼Œåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹ç²¾å¿ƒæŒ‘é€‰çš„â€œå¯æ•™â€æç¤ºè¿›è¡Œç›‘ç£å¾®è°ƒè®­ç»ƒï¼Œç”Ÿæˆè¯¦ç»†çš„æ¨ç†é“¾ï¼Œæœ‰æ•ˆåˆ©ç”¨æ¨ç†æ—¶çš„è®¡ç®—èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†Phi-4-reasoning-plusï¼Œé€šè¿‡åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥å¢å¼ºï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´é•¿çš„æ¨ç†è½¨è¿¹ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨æ•°å­¦ã€ç§‘å­¦æ¨ç†ã€ç¼–ç ç­‰å¤šä¸ªä»»åŠ¡ä¸Šå‡ä¼˜äºæ›´å¤§çš„å¼€æ”¾æƒé‡æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.19720",
            "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving",
            "url": "https://huggingface.co/papers/2504.19720",
            "abstract": "Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.",
            "score": 5,
            "issue_id": 3526,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 28",
                "zh": "4æœˆ28æ—¥"
            },
            "hash": "e74f8b7af65e09fd",
            "authors": [
                "Ranran Zhen",
                "Juntao Li",
                "Yixin Ji",
                "Zhenlin Yang",
                "Tong Liu",
                "Qingrong Xia",
                "Xinyu Duan",
                "Zhefeng Wang",
                "Baoxing Huai",
                "Min Zhang"
            ],
            "affiliations": [
                "Huawei Cloud",
                "Soochow University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.19720.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ñ‚ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ° Ğ´Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ LLM Ğ¸ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Optimizing LLMs: Balancing Performance and Efficiency in Generative AI",
                    "desc": "This paper surveys the advancements in optimizing Large Language Models (LLMs) for Generative AI, focusing on reducing memory and computational demands during inference. It discusses instance-level strategies like model placement and request scheduling, as well as cluster-level solutions such as GPU deployment and load balancing. The paper also highlights emerging scenarios and niche areas that require attention for improving LLM performance. Additionally, it suggests future research directions to enhance the efficiency of LLM inference services."
                },
                "zh": {
                    "title": "æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æœåŠ¡çš„ç ”ç©¶è¿›å±•",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶åºå¤§çš„å‚æ•°é‡å’Œæ³¨æ„åŠ›æœºåˆ¶çš„é«˜è®¡ç®—éœ€æ±‚å¯¼è‡´äº†å†…å­˜å¼€é”€å¤§ï¼Œå½±å“äº†æ¨ç†æœåŠ¡çš„ä½å»¶è¿Ÿå’Œé«˜ååé‡ã€‚æœ¬æ–‡å…¨é¢è°ƒæŸ¥äº†åº”å¯¹è¿™äº›æŒ‘æˆ˜çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å®ä¾‹çº§å’Œé›†ç¾¤çº§çš„ç­–ç•¥ï¼Œä»¥åŠæ–°å…´åœºæ™¯çš„æ–¹å‘ã€‚æˆ‘ä»¬è®¨è®ºäº†æ¨¡å‹éƒ¨ç½²ã€è¯·æ±‚è°ƒåº¦ã€è§£ç é•¿åº¦é¢„æµ‹ç­‰å®ä¾‹çº§æ–¹æ³•ï¼Œä»¥åŠGPUé›†ç¾¤éƒ¨ç½²å’Œå¤šå®ä¾‹è´Ÿè½½å‡è¡¡ç­‰é›†ç¾¤çº§ç­–ç•¥ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ½œåœ¨æ–¹å‘ï¼Œä»¥æ¨åŠ¨LLMæ¨ç†æœåŠ¡çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.18904",
            "title": "RoboVerse: Towards a Unified Platform, Dataset and Benchmark for\n  Scalable and Generalizable Robot Learning",
            "url": "https://huggingface.co/papers/2504.18904",
            "abstract": "Data scaling and standardized evaluation benchmarks have driven significant advances in natural language processing and computer vision. However, robotics faces unique challenges in scaling data and establishing evaluation protocols. Collecting real-world data is resource-intensive and inefficient, while benchmarking in real-world scenarios remains highly complex. Synthetic data and simulation offer promising alternatives, yet existing efforts often fall short in data quality, diversity, and benchmark standardization. To address these challenges, we introduce RoboVerse, a comprehensive framework comprising a simulation platform, a synthetic dataset, and unified benchmarks. Our simulation platform supports multiple simulators and robotic embodiments, enabling seamless transitions between different environments. The synthetic dataset, featuring high-fidelity physics and photorealistic rendering, is constructed through multiple approaches. Additionally, we propose unified benchmarks for imitation learning and reinforcement learning, enabling evaluation across different levels of generalization. At the core of the simulation platform is MetaSim, an infrastructure that abstracts diverse simulation environments into a universal interface. It restructures existing simulation environments into a simulator-agnostic configuration system, as well as an API aligning different simulator functionalities, such as launching simulation environments, loading assets with initial states, stepping the physics engine, etc. This abstraction ensures interoperability and extensibility. Comprehensive experiments demonstrate that RoboVerse enhances the performance of imitation learning, reinforcement learning, world model learning, and sim-to-real transfer. These results validate the reliability of our dataset and benchmarks, establishing RoboVerse as a robust solution for advancing robot learning.",
            "score": 5,
            "issue_id": 3525,
            "pub_date": "2025-04-26",
            "pub_date_card": {
                "ru": "26 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 26",
                "zh": "4æœˆ26æ—¥"
            },
            "hash": "c724dcb5ceb5df7b",
            "authors": [
                "Haoran Geng",
                "Feishi Wang",
                "Songlin Wei",
                "Yuyang Li",
                "Bangjun Wang",
                "Boshi An",
                "Charlie Tianyue Cheng",
                "Haozhe Lou",
                "Peihao Li",
                "Yen-Jen Wang",
                "Yutong Liang",
                "Dylan Goetting",
                "Chaoyi Xu",
                "Haozhe Chen",
                "Yuxi Qian",
                "Yiran Geng",
                "Jiageng Mao",
                "Weikang Wan",
                "Mingtong Zhang",
                "Jiangran Lyu",
                "Siheng Zhao",
                "Jiazhao Zhang",
                "Jialiang Zhang",
                "Chengyang Zhao",
                "Haoran Lu",
                "Yufei Ding",
                "Ran Gong",
                "Yuran Wang",
                "Yuxuan Kuang",
                "Ruihai Wu",
                "Baoxiong Jia",
                "Carlo Sferrazza",
                "Hao Dong",
                "Siyuan Huang",
                "Yue Wang",
                "Jitendra Malik",
                "Pieter Abbeel"
            ],
            "affiliations": [
                "BIGAI",
                "CMU",
                "PKU",
                "Stanford",
                "UC Berkeley",
                "UCLA",
                "UIUC",
                "UMich",
                "USC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.18904.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#benchmark",
                    "#synthetic",
                    "#optimization",
                    "#transfer_learning",
                    "#robotics"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "RoboVerse: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²",
                    "desc": "RoboVerse - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ, ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ€ĞµĞ´Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ¾Ğ¼. RoboVerse Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "RoboVerse: Advancing Robotics with Unified Simulations and Benchmarks",
                    "desc": "This paper presents RoboVerse, a new framework designed to improve robotics research by addressing the challenges of data scaling and evaluation. It includes a simulation platform that allows for easy switching between different robotic environments and a synthetic dataset that offers high-quality, diverse data. The framework also introduces unified benchmarks for testing various learning methods, such as imitation learning and reinforcement learning, ensuring consistent evaluation across different scenarios. Overall, RoboVerse aims to enhance robot learning performance and facilitate better research outcomes in the field."
                },
                "zh": {
                    "title": "RoboVerseï¼šæ¨åŠ¨æœºå™¨äººå­¦ä¹ çš„å¼ºå¤§æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†RoboVerseï¼Œè¿™æ˜¯ä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººé¢†åŸŸæ•°æ®æ”¶é›†å’Œè¯„ä¼°çš„æŒ‘æˆ˜ã€‚RoboVerseåŒ…æ‹¬ä¸€ä¸ªæ¨¡æ‹Ÿå¹³å°ã€ä¸€ä¸ªåˆæˆæ•°æ®é›†å’Œç»Ÿä¸€çš„åŸºå‡†æµ‹è¯•ï¼Œæ”¯æŒå¤šç§æ¨¡æ‹Ÿå™¨å’Œæœºå™¨äººå½¢æ€ã€‚é€šè¿‡é«˜ä¿çœŸç‰©ç†å’Œé€¼çœŸçš„æ¸²æŸ“ï¼Œåˆæˆæ•°æ®é›†æä¾›äº†é«˜è´¨é‡å’Œå¤šæ ·æ€§çš„æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoboVerseæ˜¾è‘—æå‡äº†æ¨¡ä»¿å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ å’Œä»æ¨¡æ‹Ÿåˆ°ç°å®çš„è½¬ç§»æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æ•°æ®é›†å’ŒåŸºå‡†çš„å¯é æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21850",
            "title": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning",
            "url": "https://huggingface.co/papers/2504.21850",
            "abstract": "Multimodal Large Language Models (MLLMs) excel at simple vision-language tasks but struggle when faced with complex tasks that require multiple capabilities, such as simultaneously recognizing objects, counting them, and understanding their spatial relationships. This might be partially the result of the fact that Visual Instruction Tuning (VIT), a critical training step for MLLMs, has traditionally focused on scaling data volume, but not the compositional complexity of training examples. We propose COMPACT (COMPositional Atomic-to-complex visual Capability Tuning), which generates a training dataset explicitly controlling for the compositional complexity of the training examples. The data from COMPACT allows MLLMs to train on combinations of atomic capabilities to learn complex capabilities more efficiently. Across all benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT while using less than 10% of its data budget, and even outperforms it on several, especially those involving complex multi-capability tasks. For example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0% improvement on MM-Vet compared to the full-scale VIT on particularly complex questions that require four or more atomic capabilities. COMPACT offers a scalable, data-efficient, visual compositional tuning recipe to improve on complex visual-language tasks.",
            "score": 4,
            "issue_id": 3525,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 30",
                "zh": "4æœˆ30æ—¥"
            },
            "hash": "3db97f245360deb4",
            "authors": [
                "Xindi Wu",
                "Hee Seung Hwang",
                "Polina Kirichenko",
                "Olga Russakovsky"
            ],
            "affiliations": [
                "Meta AI",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21850.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#data",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "COMPACT: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ MLLM ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ COMPACT. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². COMPACT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ MLLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ LLaVA-665k, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ 10% Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞµĞ³Ğ¾ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Unlocking Complex Tasks with Efficient Compositional Training",
                    "desc": "This paper introduces COMPACT, a new method for training Multimodal Large Language Models (MLLMs) to handle complex vision-language tasks more effectively. Traditional training methods focused on increasing data volume but neglected the complexity of the tasks, leading to limitations in MLLMs' performance. COMPACT generates a training dataset that emphasizes the compositional complexity of examples, allowing MLLMs to learn how to combine simpler skills into more complex capabilities. The results show that COMPACT not only matches the performance of existing methods with significantly less data but also excels in tasks requiring multiple skills, demonstrating its efficiency and effectiveness."
                },
                "zh": {
                    "title": "æå‡å¤æ‚è§†è§‰è¯­è¨€ä»»åŠ¡çš„èƒ½åŠ›",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç®€å•çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦å¤šç§èƒ½åŠ›çš„å¤æ‚ä»»åŠ¡ä¸­å´é¢ä¸´æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„è§†è§‰æŒ‡ä»¤è°ƒä¼˜ï¼ˆVITï¼‰ä¸»è¦å…³æ³¨æ•°æ®é‡çš„æ‰©å¤§ï¼Œè€Œå¿½è§†äº†è®­ç»ƒç¤ºä¾‹çš„ç»„åˆå¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†COMPACTï¼ˆç»„åˆåŸå­åˆ°å¤æ‚è§†è§‰èƒ½åŠ›è°ƒä¼˜ï¼‰ï¼Œå®ƒç”Ÿæˆä¸€ä¸ªæ˜ç¡®æ§åˆ¶è®­ç»ƒç¤ºä¾‹ç»„åˆå¤æ‚æ€§çš„è®­ç»ƒæ•°æ®é›†ã€‚COMPACTä½¿å¾—MLLMsèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°å­¦ä¹ å¤æ‚èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨æ¶‰åŠå¤æ‚å¤šèƒ½åŠ›ä»»åŠ¡æ—¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21855",
            "title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D\n  Physics Modeling for Complex Motion and Interaction",
            "url": "https://huggingface.co/papers/2504.21855",
            "abstract": "In recent years, video generation has seen significant advancements. However, challenges still persist in generating complex motions and interactions. To address these challenges, we introduce ReVision, a plug-and-play framework that explicitly integrates parameterized 3D physical knowledge into a pretrained conditional video generation model, significantly enhancing its ability to generate high-quality videos with complex motion and interactions. Specifically, ReVision consists of three stages. First, a video diffusion model is used to generate a coarse video. Next, we extract a set of 2D and 3D features from the coarse video to construct a 3D object-centric representation, which is then refined by our proposed parameterized physical prior model to produce an accurate 3D motion sequence. Finally, this refined motion sequence is fed back into the same video diffusion model as additional conditioning, enabling the generation of motion-consistent videos, even in scenarios involving complex actions and interactions. We validate the effectiveness of our approach on Stable Video Diffusion, where ReVision significantly improves motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even outperforms a state-of-the-art video generation model with over 13B parameters on complex video generation by a substantial margin. Our results suggest that, by incorporating 3D physical knowledge, even a relatively small video diffusion model can generate complex motions and interactions with greater realism and controllability, offering a promising solution for physically plausible video generation.",
            "score": 2,
            "issue_id": 3525,
            "pub_date": "2025-04-30",
            "pub_date_card": {
                "ru": "30 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 30",
                "zh": "4æœˆ30æ—¥"
            },
            "hash": "5d8989ce0c77aa23",
            "authors": [
                "Qihao Liu",
                "Ju He",
                "Qihang Yu",
                "Liang-Chieh Chen",
                "Alan Yuille"
            ],
            "affiliations": [
                "Independent Researcher",
                "Johns Hopkins University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21855.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#games",
                    "#video",
                    "#small_models"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ReVision: Ñ„Ğ¸Ğ·Ğ¸ĞºĞ° Ğ² Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒ Ğ˜Ğ˜ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "ReVision - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ 2D Ğ¸ 3D Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ReVision Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "ReVision: Enhancing Video Generation with 3D Physical Knowledge",
                    "desc": "The paper presents ReVision, a novel framework that enhances video generation by integrating 3D physical knowledge into a pretrained model. It operates in three stages: first, it generates a rough video using a diffusion model; second, it extracts 2D and 3D features to create a detailed 3D object-centric representation; and finally, it refines this representation to produce a coherent motion sequence. This refined sequence is then used to condition the video generation process, resulting in videos that exhibit complex motions and interactions with improved fidelity. The results demonstrate that ReVision, with only 1.5 billion parameters, surpasses a leading model with over 13 billion parameters, showcasing the effectiveness of incorporating physical principles in video generation."
                },
                "zh": {
                    "title": "é€šè¿‡3Dç‰©ç†çŸ¥è¯†æå‡è§†é¢‘ç”Ÿæˆçš„çœŸå®æ„Ÿä¸å¯æ§æ€§",
                    "desc": "è¿‘å¹´æ¥ï¼Œè§†é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç”Ÿæˆå¤æ‚åŠ¨ä½œå’Œäº¤äº’æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReVisionï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ’æ‹”çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå°†å‚æ•°åŒ–çš„ä¸‰ç»´ç‰©ç†çŸ¥è¯†é›†æˆåˆ°é¢„è®­ç»ƒçš„æ¡ä»¶è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä»è€Œæ˜¾è‘—æå‡ç”Ÿæˆé«˜è´¨é‡è§†é¢‘çš„èƒ½åŠ›ã€‚ReVisionåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šé¦–å…ˆä½¿ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ç”Ÿæˆç²—ç•¥è§†é¢‘ï¼Œç„¶åæå–2Då’Œ3Dç‰¹å¾æ„å»ºä¸‰ç»´ç‰©ä½“ä¸­å¿ƒè¡¨ç¤ºï¼Œæœ€åé€šè¿‡å‚æ•°åŒ–ç‰©ç†å…ˆéªŒæ¨¡å‹ç²¾ç‚¼è¿åŠ¨åºåˆ—ï¼Œåé¦ˆåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ä»¥ç”Ÿæˆä¸€è‡´çš„è¿åŠ¨è§†é¢‘ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒReVisionåœ¨å¤æ‚è§†é¢‘ç”Ÿæˆä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³ä»¥è¾ƒå°‘çš„å‚æ•°è¶…è¶Šäº†å¤§å‹æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.21039",
            "title": "Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report",
            "url": "https://huggingface.co/papers/2504.21039",
            "abstract": "As transformer-based large language models (LLMs) increasingly permeate society, they have revolutionized domains such as software engineering, creative writing, and digital arts. However, their adoption in cybersecurity remains limited due to challenges like scarcity of specialized training data and complexity of representing cybersecurity-specific knowledge. To address these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on the Llama 3.1 architecture and enhanced through continued pretraining on a carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across both established and new cybersecurity benchmarks, showing that it matches Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By releasing our model to the public, we aim to accelerate progress and adoption of AI-driven tools in both public and private cybersecurity contexts.",
            "score": 2,
            "issue_id": 3528,
            "pub_date": "2025-04-28",
            "pub_date_card": {
                "ru": "28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 28",
                "zh": "4æœˆ28æ—¥"
            },
            "hash": "a66fbdd0c4cc7250",
            "authors": [
                "Paul Kassianik",
                "Baturay Saglam",
                "Alexander Chen",
                "Blaine Nelson",
                "Anu Vellore",
                "Massimo Aufiero",
                "Fraser Burch",
                "Dhruv Kedia",
                "Avi Zohary",
                "Sajana Weerawardhena",
                "Aman Priyanshu",
                "Adam Swanda",
                "Amy Chang",
                "Hyrum Anderson",
                "Kojin Oshiba",
                "Omar Santos",
                "Yaron Singer",
                "Amin Karbasi"
            ],
            "affiliations": [
                "Foundation AI Cisco Systems Inc.",
                "Security & Trust Organization Cisco Systems Inc.",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.21039.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#data",
                    "#open_source",
                    "#training",
                    "#dataset",
                    "#benchmark",
                    "#security"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Foundation-Sec-8B - ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Llama 3.1 Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¿Ğ¾ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Foundation-Sec-8B Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Llama 3.1-70B Ğ¸ GPT-4o-mini Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑÑ„ĞµÑ€Ğµ ĞºĞ¸Ğ±ĞµÑ€Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Empowering Cybersecurity with Foundation-Sec-8B",
                    "desc": "This paper introduces Foundation-Sec-8B, a large language model specifically designed for cybersecurity applications. Built on the Llama 3.1 architecture, it has been further trained on a specialized dataset focused on cybersecurity knowledge. The model is evaluated against existing benchmarks and demonstrates competitive performance compared to other leading models like Llama 3.1-70B and GPT-4o-mini in cybersecurity tasks. By making this model publicly available, the authors aim to enhance the use of AI tools in cybersecurity for both public and private sectors."
                },
                "zh": {
                    "title": "æ¨åŠ¨ç½‘ç»œå®‰å…¨çš„AIå·¥å…·è¿›æ­¥",
                    "desc": "éšç€åŸºäºå˜æ¢å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¤¾ä¼šä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå®ƒä»¬åœ¨è½¯ä»¶å·¥ç¨‹ã€åˆ›æ„å†™ä½œå’Œæ•°å­—è‰ºæœ¯ç­‰é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹ä¸“ä¸šçš„è®­ç»ƒæ•°æ®å’Œè¡¨ç¤ºç½‘ç»œå®‰å…¨ç‰¹å®šçŸ¥è¯†çš„å¤æ‚æ€§ï¼Œå®ƒä»¬åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Foundation-Sec-8Bï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºç½‘ç»œå®‰å…¨çš„LLMï¼ŒåŸºäºLlama 3.1æ¶æ„ï¼Œå¹¶é€šè¿‡åœ¨ç²¾å¿ƒç­–åˆ’çš„ç½‘ç»œå®‰å…¨è¯­æ–™åº“ä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒæ¥å¢å¼ºã€‚æˆ‘ä»¬åœ¨å¤šä¸ªç½‘ç»œå®‰å…¨åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†Foundation-Sec-8Bï¼Œç»“æœæ˜¾ç¤ºå®ƒåœ¨æŸäº›ç½‘ç»œå®‰å…¨ç‰¹å®šä»»åŠ¡ä¸Šä¸Llama 3.1-70Bå’ŒGPT-4o-miniç›¸åŒ¹é…ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-30.html",
    "link_next": "2025-05-02.html",
    "link_month": "2025-05.html",
    "short_date_prev": {
        "ru": "30.04",
        "en": "04/30",
        "zh": "4æœˆ30æ—¥"
    },
    "short_date_next": {
        "ru": "02.05",
        "en": "05/02",
        "zh": "5æœˆ2æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 1,
        "#cv": 0,
        "#rl": 4,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 2,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 2,
        "#science": 1,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œç§°ä¸ºUniversalRAGã€‚å®ƒèƒ½ä»ä¸åŒæ¨¡æ€å’Œç²’åº¦çš„çŸ¥è¯†æºä¸­æ£€ç´¢å’Œæ•´åˆä¿¡æ¯ã€‚ç°æœ‰çš„RAGæ–¹æ³•ä¸»è¦å±€é™äºå•ä¸€æ¨¡æ€çš„è¯­æ–™åº“ï¼Œè€ŒUniversalRAGé€šè¿‡æ¨¡æ€æ„ŸçŸ¥è·¯ç”±æœºåˆ¶åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„æ¨¡æ€ç‰¹å®šè¯­æ–™åº“è¿›è¡Œæ£€ç´¢ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æ ¹æ®æŸ¥è¯¢çš„å¤æ‚æ€§å’ŒèŒƒå›´ï¼Œå¯¹æ¯ç§æ¨¡æ€è¿›è¡Œå¤šçº§ç²’åº¦çš„ç»„ç»‡ã€‚ç ”ç©¶åœ¨8ä¸ªå¤šæ¨¡æ€åŸºå‡†ä¸ŠéªŒè¯äº†UniversalRAGçš„ä¼˜è¶Šæ€§ã€‚",
        "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with\n  Diverse Modalities and Granularities",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œç§°ä¸ºUniversalRAGã€‚å®ƒèƒ½ä»ä¸åŒæ¨¡æ€å’Œç²’åº¦çš„çŸ¥è¯†æºä¸­æ£€ç´¢å’Œæ•´åˆä¿¡æ¯ã€‚ç°æœ‰çš„RAGæ–¹æ³•ä¸»è¦å±€é™äºå•ä¸€æ¨¡æ€çš„è¯­æ–™åº“ï¼Œè€ŒUniversalRAGé€šè¿‡æ¨¡æ€æ„ŸçŸ¥è·¯ç”±æœºåˆ¶åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„æ¨¡æ€ç‰¹å®šè¯­æ–™åº“è¿›è¡Œæ£€ç´¢ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æ ¹æ®æŸ¥è¯¢çš„å¤æ‚æ€§å’ŒèŒƒå›´ï¼Œå¯¹æ¯ç§æ¨¡æ€è¿›è¡Œå¤šçº§ç²’åº¦çš„ç»„ç»‡ã€‚ç ”ç©¶åœ¨8ä¸ªå¤šæ¨¡æ€åŸºå‡†ä¸ŠéªŒè¯äº†UniversalRAGçš„ä¼˜è¶Šæ€§ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng xÄ«n de jiÇn suÇ’ zÄ“ng qiÃ¡ng shÄ“ng chÃ©ng (RAG) kuÃ ng jiÃ , chÄ“ng wÃ©i UniversalRAG. tÄ nÃ©ng cÃ³ng bÃ¹ tÃ³ng mÃ³ tÃ i hÃ© lÃ¬ dÃ¹ de zhÄ« shi yuÃ¡n zhÅng jiÇn suÇ’ hÃ© zhÄ›ng hÃ© xÃ¬n xÄ«. xiÃ n yÇ’u de RAG fÄng fÇ zhÇ” yÃ o jÃº xiÃ n yÄ« dÃ n yÄ« mÃ³ tÃ i de yÇ” liÃ o kÃ¹, Ã©r UniversalRAG tÅng guÃ² mÃ³ tÃ i gÇn juÃ© lÃ¹ yÃ³u jÄ« zhÃ¬ dÃ²ng tÃ i xuÇn zÃ© zuÃ¬ hÃ© shÃ¬ de mÃ³ tÃ i tÃ¨ dÃ¬ng yÇ” liÃ o kÃ¹ jÃ¬n xÃ­ng jiÇn suÇ’. cÇ wÃ i, tÄ hÃ¡i gÄ“n jÃ¹ chÃ¡ xÃºn de fÃº zÃ  xÃ¬ng hÃ© fÃ n wÃ©i, duÃ¬ mÄ›i zhÇ’ng mÃ³ tÃ i jÃ¬n xÃ­ng duÅ jÃ­ lÃ¬ dÃ¹ de zÇ” zhÄ«. yÃ¡n jiÅ« zÃ i 8 gÃ¨ duÅ mÃ³ tÃ i bÇ zhÇ”n shÃ ng yÃ n zhÃ¨ng le UniversalRAG de yÅu yuÃ¨ xÃ¬ng.",
        "vocab": "[\n    {\"word\": \"æ£€ç´¢\", \"pinyin\": \"jiÇnsuÇ’\", \"trans\": \"retrieval\"},\n    {\"word\": \"å¢å¼º\", \"pinyin\": \"zÄ“ngqiÃ¡ng\", \"trans\": \"enhancement\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ngchÃ©ng\", \"trans\": \"generation\"},\n    {\"word\": \"æ¡†æ¶\", \"pinyin\": \"kuÃ ngjiÃ \", \"trans\": \"framework\"},\n    {\"word\": \"æ¨¡æ€\", \"pinyin\": \"mÃ³ tÃ i\", \"trans\": \"modality\"},\n    {\"word\": \"ç²’åº¦\", \"pinyin\": \"lÃ¬ dÃ¹\", \"trans\": \"granularity\"},\n    {\"word\": \"çŸ¥è¯†æº\", \"pinyin\": \"zhÄ«shi yuÃ¡n\", \"trans\": \"knowledge source\"},\n    {\"word\": \"æ•´åˆ\", \"pinyin\": \"zhÄ›nghÃ©\", \"trans\": \"integration\"},\n    {\"word\": \"å±€é™äº\", \"pinyin\": \"jÃº xiÃ n yÃº\", \"trans\": \"limited to\"},\n    {\"word\": \"å•ä¸€\", \"pinyin\": \"dÄn yÄ«\", \"trans\": \"single\"},\n    {\"word\": \"è¯­æ–™åº“\", \"pinyin\": \"yÇ” liÃ o kÃ¹\", \"trans\": \"corpus\"},\n    {\"word\": \"æ„ŸçŸ¥\", \"pinyin\": \"gÇnzhÄ«\", \"trans\": \"perception\"},\n    {\"word\": \"è·¯ç”±\", \"pinyin\": \"lÃ¹ yÃ³u\", \"trans\": \"routing\"},\n    {\"word\": \"æœºåˆ¶\", \"pinyin\": \"jÄ«zhÃ¬\", \"trans\": \"mechanism\"},\n    {\"word\": \"åŠ¨æ€\", \"pinyin\": \"dÃ²ngtÃ i\", \"trans\": \"dynamic\"},\n    {\"word\": \"é€‰æ‹©\", \"pinyin\": \"xuÇnzÃ©\", \"trans\": \"selection\"},\n    {\"word\": \"åˆé€‚\", \"pinyin\": \"hÃ©shÃ¬\", \"trans\": \"appropriate\"},\n    {\"word\": \"ç‰¹å®š\", \"pinyin\": \"tÃ¨dÃ¬ng\", \"trans\": \"specific\"},\n    {\"word\": \"æŸ¥è¯¢\", \"pinyin\": \"chÃ¡xÃºn\", \"trans\": \"query\"},\n    {\"word\": \"å¤æ‚æ€§\", \"pinyin\": \"fÃ¹zÃ¡xÃ¬ng\", \"trans\": \"complexity\"},\n    {\"word\": \"èŒƒå›´\", \"pinyin\": \"fÃ nwÃ©i\", \"trans\": \"scope\"},\n    {\"word\": \"å¤šçº§\", \"pinyin\": \"duÅjÃ­\", \"trans\": \"multi-level\"},\n    {\"word\": \"ç»„ç»‡\", \"pinyin\": \"zÇ”zhÄ«\", \"trans\": \"organization\"},\n    {\"word\": \"ç ”ç©¶\", \"pinyin\": \"yÃ¡njiÅ«\", \"trans\": \"research\"},\n    {\"word\": \"éªŒè¯\", \"pinyin\": \"yÃ nzhÃ¨ng\", \"trans\": \"validation\"},\n    {\"word\": \"ä¼˜è¶Šæ€§\", \"pinyin\": \"yÅuyuÃ¨xÃ¬ng\", \"trans\": \"superiority\"},\n    {\"word\": \"åŸºå‡†\", \"pinyin\": \"jÄ«zhÇ”n\", \"trans\": \"benchmark\"}\n]",
        "trans": "This article introduces a new Retrieval-Augmented Generation (RAG) framework called UniversalRAG. It can retrieve and integrate information from knowledge sources of different modalities and granularities. Existing RAG methods are mainly limited to single-modality corpora, while UniversalRAG dynamically selects the most suitable modality-specific corpus for retrieval through a modality-aware routing mechanism. Additionally, it organizes each modality at multiple granularity levels based on the complexity and scope of the query. The superiority of UniversalRAG has been validated on 8 multimodal benchmarks.",
        "update_ts": "2025-04-30 09:12"
    }
}