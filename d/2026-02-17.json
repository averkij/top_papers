{
    "date": {
        "ru": "17 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 17",
        "zh": "2æœˆ17æ—¥"
    },
    "time_utc": "2026-02-17 21:28",
    "weekday": 1,
    "issue_id": 1100,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2602.10809",
            "title": "DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories",
            "url": "https://huggingface.co/papers/2602.10809",
            "abstract": "DeepImageSearch presents an agentic approach to image retrieval that addresses limitations of traditional semantic matching by enabling multi-step reasoning over visual histories through a modular agent framework with dual-memory system.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing multimodal retrieval systems excel at semantic matching but implicitly assume that query-image relevance can be measured in isolation. This paradigm overlooks the rich dependencies inherent in realistic visual streams, where information is distributed across temporal sequences rather than confined to single snapshots. To bridge this gap, we introduce DeepImageSearch, a novel agentic paradigm that reformulates image retrieval as an autonomous exploration task. Models must plan and perform multi-step reasoning over raw visual histories to locate targets based on implicit contextual cues. We construct DISBench, a challenging benchmark built on interconnected visual data. To address the scalability challenge of creating context-dependent queries, we propose a human-model collaborative pipeline that employs vision-language models to mine latent spatiotemporal associations, effectively offloading intensive context discovery before human verification. Furthermore, we build a robust baseline using a modular agent framework equipped with fine-grained tools and a dual-memory system for long-horizon navigation. Extensive experiments demonstrate that DISBench poses significant challenges to state-of-the-art models, highlighting the necessity of incorporating agentic reasoning into next-generation retrieval systems.",
            "score": 25,
            "issue_id": 1092,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "4a13bf8cc67f759c",
            "authors": [
                "Chenlong Deng",
                "Mengjie Deng",
                "Junjie Wu",
                "Dun Zeng",
                "Teng Wang",
                "Qingsong Xie",
                "Jiadeng Huang",
                "Shengjie Ma",
                "Changwang Zhang",
                "Zhaoxiang Wang",
                "Jun Wang",
                "Yutao Zhu",
                "Zhicheng Dou"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "OPPO Research Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.10809.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#reasoning",
                    "#long_context",
                    "#dataset",
                    "#cv",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "DeepImageSearch Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸, Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ DISBench â€” ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Image Retrieval with Agentic Reasoning",
                    "desc": "DeepImageSearch introduces a new way to find images by allowing models to think and plan like agents. Instead of just matching images based on their meanings, it enables multi-step reasoning over sequences of images, capturing the context that traditional methods miss. The system uses a dual-memory framework to help navigate through complex visual histories and find relevant images based on implicit cues. By creating DISBench, a benchmark for testing these capabilities, the research shows that incorporating agentic reasoning is essential for improving image retrieval systems."
                },
                "zh": {
                    "title": "ä¸»åŠ¨æ¨ç†ï¼Œæå‡å›¾åƒæ£€ç´¢èƒ½åŠ›",
                    "desc": "DeepImageSearchæå‡ºäº†ä¸€ç§ä¸»åŠ¨çš„å›¾åƒæ£€ç´¢æ–¹æ³•ï¼Œå…‹æœäº†ä¼ ç»Ÿè¯­ä¹‰åŒ¹é…çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¨¡å—åŒ–ä»£ç†æ¡†æ¶å’ŒåŒé‡è®°å¿†ç³»ç»Ÿï¼Œæ”¯æŒå¯¹è§†è§‰å†å²è¿›è¡Œå¤šæ­¥æ¨ç†ã€‚ä¸ç°æœ‰çš„å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿä¸åŒï¼ŒDeepImageSearchå°†å›¾åƒæ£€ç´¢è§†ä¸ºä¸€ç§è‡ªä¸»æ¢ç´¢ä»»åŠ¡ï¼Œèƒ½å¤Ÿæ ¹æ®éšå«çš„ä¸Šä¸‹æ–‡çº¿ç´¢å®šä½ç›®æ ‡ã€‚é€šè¿‡æ„å»ºDISBenchåŸºå‡†ï¼Œç ”ç©¶è¡¨æ˜ï¼Œä¸‹ä¸€ä»£æ£€ç´¢ç³»ç»Ÿéœ€è¦æ•´åˆä¸»åŠ¨æ¨ç†èƒ½åŠ›ï¼Œä»¥åº”å¯¹å¤æ‚çš„è§†è§‰æ•°æ®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.13949",
            "title": "Experiential Reinforcement Learning",
            "url": "https://huggingface.co/papers/2602.13949",
            "abstract": "Experiential Reinforcement Learning introduces an explicit experience-reflection-consolidation loop that improves learning efficiency and performance in sparse-reward environments by enabling structured behavioral revision without additional inference costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.",
            "score": 23,
            "issue_id": 1087,
            "pub_date": "2026-02-15",
            "pub_date_card": {
                "ru": "15 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 15",
                "zh": "2æœˆ15æ—¥"
            },
            "hash": "076438305a62c51d",
            "authors": [
                "Taiwei Shi",
                "Sihao Chen",
                "Bowen Jiang",
                "Linxin Song",
                "Longqi Yang",
                "Jieyu Zhao"
            ],
            "affiliations": [
                "Microsoft",
                "University of Pennsylvania",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.13949.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#training",
                    "#rl"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ ĞºĞ°Ğº Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ ÑĞ²Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ¿Ñ‹Ñ‚-Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ-ĞºĞ¾Ğ½ÑĞ¾Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºÑƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ¸ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²Ñ‚Ğ¾Ñ€ÑƒÑ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºÑƒ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ERL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ - Ğ´Ğ¾ 81% Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ´Ğ¾ 11% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Transforming Feedback into Learning: The Power of Reflection in ERL",
                    "desc": "Experiential Reinforcement Learning (ERL) introduces a structured loop of experience, reflection, and consolidation to enhance learning in environments with sparse rewards. This method allows models to generate initial actions, receive feedback, and reflect on their performance to refine future attempts. By embedding this explicit reflection process, ERL improves exploration and stabilizes the optimization of policies without incurring extra inference costs. The results demonstrate significant improvements in learning efficiency and performance, particularly in complex tasks, highlighting the value of self-reflection in reinforcement learning."
                },
                "zh": {
                    "title": "ä½“éªŒå¼ºåŒ–å­¦ä¹ ï¼šæå‡ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­çš„å­¦ä¹ æ•ˆç‡",
                    "desc": "ä½“éªŒå¼ºåŒ–å­¦ä¹ ï¼ˆERLï¼‰å¼•å…¥äº†ä¸€ä¸ªæ˜ç¡®çš„ç»éªŒ-åæ€-å·©å›ºå¾ªç¯ï¼Œæ—¨åœ¨æé«˜ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­çš„å­¦ä¹ æ•ˆç‡å’Œæ€§èƒ½ã€‚è¯¥æ–¹æ³•å…è®¸æ¨¡å‹åœ¨æ¥æ”¶åˆ°ç¯å¢ƒåé¦ˆåè¿›è¡Œç»“æ„åŒ–çš„è¡Œä¸ºä¿®æ­£ï¼Œè€Œæ— éœ€é¢å¤–çš„æ¨ç†æˆæœ¬ã€‚é€šè¿‡ç”Ÿæˆåˆå§‹å°è¯•ã€æ¥æ”¶åé¦ˆå¹¶è¿›è¡Œåæ€ï¼Œæ¨¡å‹èƒ½å¤ŸæŒ‡å¯¼æ›´ç²¾ç»†çš„ç¬¬äºŒæ¬¡å°è¯•ï¼Œä»è€Œå°†æˆåŠŸç»éªŒå†…åŒ–åˆ°åŸºç¡€ç­–ç•¥ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒERLåœ¨å¤æ‚çš„å¤šæ­¥éª¤ç¯å¢ƒå’Œå·¥å…·ä½¿ç”¨æ¨ç†ä»»åŠ¡ä¸­ï¼Œå­¦ä¹ æ•ˆç‡å’Œæœ€ç»ˆæ€§èƒ½å‡æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14234",
            "title": "REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents",
            "url": "https://huggingface.co/papers/2602.14234",
            "abstract": "REDSearcher presents a unified framework for optimizing search agents through improved task synthesis, tool-augmented queries, midtraining capability enhancement, and simulated environments to address challenges in long-horizon search tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.",
            "score": 17,
            "issue_id": 1084,
            "pub_date": "2026-02-15",
            "pub_date_card": {
                "ru": "15 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 15",
                "zh": "2æœˆ15æ—¥"
            },
            "hash": "6258e5ec798dd7f7",
            "authors": [
                "Zheng Chu",
                "Xiao Wang",
                "Jack Hong",
                "Huiming Fan",
                "Yuqi Huang",
                "Yue Yang",
                "Guohai Xu",
                "Chenxiao Zhao",
                "Cheng Xiang",
                "Shengchao Hu",
                "Dongdong Kuang",
                "Ming Liu",
                "Bing Qin",
                "Xing Yu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2602.14234.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#synthetic",
                    "#multimodal",
                    "#reasoning",
                    "#open_source",
                    "#agents",
                    "#benchmark",
                    "#dataset",
                    "#long_context",
                    "#training"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "REDSearcher Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ±ÑƒĞ´ĞµÑ‚ ÑĞ¾Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ¶Ğ´Ğ°Ñ‚ÑŒÑÑ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Optimizing Search Agents with REDSearcher: A Unified Approach",
                    "desc": "REDSearcher is a new framework designed to improve the performance of search agents in complex tasks. It tackles the problem of sparse high-quality search trajectories by optimizing task synthesis and enhancing training processes. The framework introduces innovative methods like dual-constrained task synthesis and tool-augmented queries to promote effective tool usage. Additionally, it provides a simulated environment for efficient reinforcement learning, leading to state-of-the-art results in both text and multimodal search tasks."
                },
                "zh": {
                    "title": "REDSearcherï¼šä¼˜åŒ–æœç´¢ä»£ç†çš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "REDSearcheræ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ”¹è¿›ä»»åŠ¡åˆæˆã€å·¥å…·å¢å¼ºæŸ¥è¯¢ã€ä¸­æœŸè®­ç»ƒèƒ½åŠ›æå‡å’Œæ¨¡æ‹Ÿç¯å¢ƒæ¥ä¼˜åŒ–æœç´¢ä»£ç†ï¼Œä»¥åº”å¯¹é•¿æ—¶é—´æœç´¢ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åŒçº¦æŸä¼˜åŒ–æ¥ç²¾ç¡®æ§åˆ¶ä»»åŠ¡éš¾åº¦ï¼Œä»è€Œå¯æ‰©å±•åœ°ç”Ÿæˆå¤æ‚çš„é«˜è´¨é‡ä»»åŠ¡ã€‚REDSearcherè¿˜å¼•å…¥äº†å·¥å…·å¢å¼ºæŸ¥è¯¢ï¼Œé¼“åŠ±ä¸»åŠ¨ä½¿ç”¨å·¥å…·ï¼Œè€Œä¸æ˜¯è¢«åŠ¨å›å¿†ã€‚åœ¨ä¸­æœŸè®­ç»ƒä¸­ï¼ŒREDSearcheræ˜¾è‘—å¢å¼ºäº†æ ¸å¿ƒåŸå­èƒ½åŠ›ï¼Œé™ä½äº†æ”¶é›†é«˜è´¨é‡è½¨è¿¹çš„æˆæœ¬ï¼Œä»è€Œæé«˜äº†ä¸‹æ¸¸è®­ç»ƒçš„æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14265",
            "title": "STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts",
            "url": "https://huggingface.co/papers/2602.14265",
            "abstract": "STATe presents an interpretable inference-time compute method that uses discrete textual interventions to generate diverse, high-quality, and explainable text by searching over reasoning patterns rather than relying on stochastic sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t Inference-Time-Compute (ITC) methods like Best-of-N and Tree-of-Thoughts are meant to produce output candidates that are both high-quality and diverse, but their use of high-temperature sampling often fails to achieve meaningful output diversity. Moreover, existing ITC methods offer limited control over how to perform reasoning, which in turn limits their explainability. We present STATe-of-Thoughts (STATe), an interpretable ITC method that searches over high-level reasoning patterns. STATe replaces stochastic sampling with discrete and interpretable textual interventions: a controller selects actions encoding high-level reasoning choices, a generator produces reasoning steps conditioned on those choices, and an evaluator scores candidates to guide search. This structured approach yields three main advantages. First, action-guided textual interventions produce greater response diversity than temperature-based sampling. Second, in a case study on argument generation, STATe's explicit action sequences capture interpretable features that are highly predictive of output quality. Third, estimating the association between performance and action choices allows us to identify promising yet unexplored regions of the action space and steer generation directly toward them. Together, these results establish STATe as a practical framework for generating high-quality, diverse, and interpretable text. Our framework is available at https://github.com/zbambergerNLP/state-of-thoughts.",
            "score": 16,
            "issue_id": 1095,
            "pub_date": "2026-02-15",
            "pub_date_card": {
                "ru": "15 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 15",
                "zh": "2æœˆ15æ—¥"
            },
            "hash": "747054a7692ef63d",
            "authors": [
                "Zachary Bamberger",
                "Till R. Saenger",
                "Gilad Morad",
                "Ofra Amir",
                "Brandon M. Stewart",
                "Amir Feder"
            ],
            "affiliations": [
                "Hebrew University of Jerusalem",
                "Princeton University",
                "Technion Israel Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14265.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#open_source",
                    "#interpretability"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "STATe Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸, Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ñƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ temperature-based ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ²Ğ½Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ´Ğ¸Ñ‚ÑŒ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "STATe: Structured Reasoning for Diverse and Explainable Text Generation",
                    "desc": "STATe introduces a new method for generating text that is both diverse and explainable by using structured reasoning patterns instead of random sampling. This method employs discrete textual interventions where a controller selects high-level reasoning actions, a generator creates reasoning steps based on those actions, and an evaluator scores the outputs. This approach enhances output diversity and allows for better interpretability of the reasoning process, particularly in tasks like argument generation. By analyzing the relationship between action choices and performance, STATe can effectively guide the text generation process towards high-quality results."
                },
                "zh": {
                    "title": "STATeï¼šç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–ä¸”å¯è§£é‡Šæ–‡æœ¬çš„æ–°æ–¹æ³•",
                    "desc": "STATeæ˜¯ä¸€ç§å¯è§£é‡Šçš„æ¨ç†æ—¶è®¡ç®—æ–¹æ³•ï¼Œé€šè¿‡ç¦»æ•£çš„æ–‡æœ¬å¹²é¢„ç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡ä¸”å¯è§£é‡Šçš„æ–‡æœ¬ã€‚ä¸ä¼ ç»Ÿçš„éšæœºé‡‡æ ·æ–¹æ³•ä¸åŒï¼ŒSTATeé€šè¿‡æœç´¢é«˜å±‚æ¬¡çš„æ¨ç†æ¨¡å¼æ¥ç”Ÿæˆæ–‡æœ¬ï¼Œé¿å…äº†è¾“å‡ºå¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•çš„æ§åˆ¶å™¨é€‰æ‹©ç¼–ç é«˜å±‚æ¬¡æ¨ç†é€‰æ‹©çš„åŠ¨ä½œï¼Œç”Ÿæˆå™¨æ ¹æ®è¿™äº›é€‰æ‹©ç”Ÿæˆæ¨ç†æ­¥éª¤ï¼Œè¯„ä¼°å™¨åˆ™å¯¹å€™é€‰ç»“æœè¿›è¡Œè¯„åˆ†ä»¥æŒ‡å¯¼æœç´¢ã€‚é€šè¿‡è¿™ç§ç»“æ„åŒ–çš„æ–¹æ³•ï¼ŒSTATeèƒ½å¤Ÿæé«˜å“åº”çš„å¤šæ ·æ€§ï¼Œå¹¶ä¸”åœ¨ç”Ÿæˆè®ºè¯æ—¶ï¼Œæ˜ç¡®çš„åŠ¨ä½œåºåˆ—èƒ½å¤Ÿæ•æ‰åˆ°ä¸è¾“å‡ºè´¨é‡é«˜åº¦ç›¸å…³çš„å¯è§£é‡Šç‰¹å¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14492",
            "title": "Query as Anchor: Scenario-Adaptive User Representation via Large Language Model",
            "url": "https://huggingface.co/papers/2602.14492",
            "abstract": "A novel framework called Query-as-Anchor is introduced that transforms user modeling from static encoding to dynamic, query-aware synthesis using large language models with specialized architectures and training methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.",
            "score": 15,
            "issue_id": 1086,
            "pub_date": "2026-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "b95785a3057a9ff5",
            "authors": [
                "Jiahao Yuan",
                "Yike Xu",
                "Jinyong Wen",
                "Baokun Wang",
                "Ziyi Gao",
                "Xiaotong Lin",
                "Yun Liu",
                "Xing Fu",
                "Yu Cheng",
                "Yongchao Liu",
                "Weiqiang Wang",
                "Zhongle Xie"
            ],
            "affiliations": [
                "Ant Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14492.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#architecture",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "âš“",
                "ru": {
                    "title": "ĞÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğ¼: Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğº Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Query-as-Anchor, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ·, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ UserU Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Q-Anchor Embedding Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°Ğ¼Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾-Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ”Ğ»Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Cluster-based Soft Prompt Tuning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 10 Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Alipay Ğ¸ Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğµ Ğ¸ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ñ Ğ² production-ÑÑ€ĞµĞ´Ğµ."
                },
                "en": {
                    "title": "Dynamic User Modeling with Query-as-Anchor",
                    "desc": "The paper presents a new framework called Query-as-Anchor that enhances user modeling by shifting from static to dynamic, query-aware representations using large language models (LLMs). It addresses the limitations of existing methods that produce static embeddings, which often fail to meet the diverse needs of various tasks. The framework utilizes a specially constructed dataset, UserU, and a unique Q-Anchor Embedding architecture to create user representations that are sensitive to specific queries. The results demonstrate superior performance on multiple benchmarks and effective deployment in real-world applications, showcasing the framework's scalability and practical utility."
                },
                "zh": {
                    "title": "åŠ¨æ€æŸ¥è¯¢é©±åŠ¨çš„ç”¨æˆ·å»ºæ¨¡æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºQuery-as-Anchorçš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å°†ç”¨æˆ·å»ºæ¨¡ä»é™æ€ç¼–ç è½¬å˜ä¸ºåŠ¨æ€çš„ã€åŸºäºæŸ¥è¯¢çš„åˆæˆã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œä¸“é—¨çš„æ¶æ„ï¼Œé€šè¿‡æ„å»ºUserUæ•°æ®é›†æ¥å¢å¼ºå¯¹ç”¨æˆ·è¡Œä¸ºçš„ç†è§£ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†Q-AnchoråµŒå…¥æ¶æ„ï¼Œç»“åˆäº†å±‚æ¬¡åŒ–çš„ç²—ç»†ç¼–ç å™¨ï¼Œä»¥å®ç°æŸ¥è¯¢æ„ŸçŸ¥çš„ç”¨æˆ·è¡¨ç¤ºã€‚é€šè¿‡åœ¨å®é™…åº”ç”¨ä¸­è¿›è¡Œå¤§è§„æ¨¡åœ¨çº¿A/Bæµ‹è¯•ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14699",
            "title": "Qute: Towards Quantum-Native Database",
            "url": "https://huggingface.co/papers/2602.14699",
            "abstract": "This paper envisions a quantum database (Qute) that treats quantum computation as a first-class execution option. Unlike prior simulation-based methods that either run quantum algorithms on classical machines or adapt existing databases for quantum simulation, Qute instead (i) compiles an extended form of SQL into gate-efficient quantum circuits, (ii) employs a hybrid optimizer to dynamically select between quantum and classical execution plans, (iii) introduces selective quantum indexing, and (iv) designs fidelity-preserving storage to mitigate current qubit constraints. We also present a three-stage evolution roadmap toward quantum-native database. Finally, by deploying Qute on a real quantum processor (origin_wukong), we show that it outperforms a classical baseline at scale, and we release an open-source prototype at https://github.com/weAIDB/Qute.",
            "score": 13,
            "issue_id": 1084,
            "pub_date": "2026-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "b7c34d2ef559622e",
            "authors": [
                "Muzhi Chen",
                "Xuanhe Zhou",
                "Wei Zhou",
                "Bangrui Xu",
                "Surui Tang",
                "Guoliang Li",
                "Bingsheng He",
                "Yeye He",
                "Yitong Song",
                "Fan Wu"
            ],
            "affiliations": [
                "Hong Kong Baptist University",
                "Microsoft Corporation",
                "National University of Singapore",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14699.jpg",
            "data": {
                "categories": [],
                "emoji": "âš›ï¸",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞ¿Ğ¾Ñ…Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ñ Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Qute, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ SQL Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ½ĞµĞ´Ñ€ÑÑÑ‚ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ ĞºÑƒĞ±Ğ¸Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Qute: Revolutionizing Databases with Quantum Computing",
                    "desc": "This paper introduces Qute, a quantum database that integrates quantum computation directly into database operations. It innovatively compiles SQL into quantum circuits, allowing for efficient execution of quantum algorithms. Qute also features a hybrid optimizer that intelligently chooses between quantum and classical processing, along with selective quantum indexing to enhance performance. The authors demonstrate Qute's effectiveness on a real quantum processor, showing superior performance compared to classical methods, and provide an open-source prototype for further exploration."
                },
                "zh": {
                    "title": "é‡å­æ•°æ®åº“ï¼šé‡å­è®¡ç®—çš„æ–°é€‰æ‹©",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é‡å­æ•°æ®åº“ï¼ˆQuteï¼‰ï¼Œå°†é‡å­è®¡ç®—è§†ä¸ºä¸€ç§ä¼˜å…ˆæ‰§è¡Œé€‰é¡¹ã€‚ä¸ä»¥å¾€åœ¨ç»å…¸æœºå™¨ä¸Šè¿è¡Œé‡å­ç®—æ³•æˆ–è°ƒæ•´ç°æœ‰æ•°æ®åº“è¿›è¡Œé‡å­æ¨¡æ‹Ÿçš„æ–¹æ³•ä¸åŒï¼ŒQuteé€šè¿‡å°†æ‰©å±•çš„SQLç¼–è¯‘ä¸ºé«˜æ•ˆçš„é‡å­ç”µè·¯ï¼ŒåŠ¨æ€é€‰æ‹©é‡å­å’Œç»å…¸æ‰§è¡Œè®¡åˆ’ï¼Œå¹¶å¼•å…¥é€‰æ‹©æ€§é‡å­ç´¢å¼•æ¥ä¼˜åŒ–æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒQuteè®¾è®¡äº†ä¿çœŸåº¦å­˜å‚¨ä»¥åº”å¯¹å½“å‰é‡å­æ¯”ç‰¹çš„é™åˆ¶ï¼Œå¹¶å±•ç¤ºäº†åœ¨çœŸå®é‡å­å¤„ç†å™¨ä¸Šéƒ¨ç½²çš„æ•ˆæœè¶…è¶Šç»å…¸åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14367",
            "title": "InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem",
            "url": "https://huggingface.co/papers/2602.14367",
            "abstract": "InnoEval is a deep innovation evaluation framework that emulates human-level idea assessment through knowledge-grounded, multi-perspective reasoning with heterogeneous deep knowledge search and multi-dimensional decoupled evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts.",
            "score": 13,
            "issue_id": 1089,
            "pub_date": "2026-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "ac112d6375b44dc7",
            "authors": [
                "Shuofei Qiao",
                "Yunxiang Wei",
                "Xuehai Wang",
                "Bin Wu",
                "Boyang Xue",
                "Ningyu Zhang",
                "Hossein A. Rahmani",
                "Yanshan Wang",
                "Qiang Zhang",
                "Keyan Ding",
                "Jeff Z. Pan",
                "Huajun Chen",
                "Emine Yilmaz"
            ],
            "affiliations": [
                "The Chinese University of Hong Kong",
                "The University of Edinburgh",
                "University College London",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14367.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#benchmark",
                    "#dataset",
                    "#open_source",
                    "#reasoning",
                    "#science"
                ],
                "emoji": "ğŸ’¡",
                "ru": {
                    "title": "ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·ÑƒĞ¼ Ğ´Ğ»Ñ Ñ‡ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "InnoEval â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸Ğ´ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑƒĞ·ĞºĞ¸Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´imensional Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ğ¼Ğ¸ÑÑĞ¸Ñ Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ InnoEval Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing Idea Evaluation with InnoEval",
                    "desc": "InnoEval is a novel framework designed to evaluate scientific ideas by mimicking human-level assessment through advanced reasoning techniques. It utilizes a heterogeneous deep knowledge search to gather diverse evidence from various online sources, ensuring a well-rounded evaluation. The framework incorporates a multi-dimensional approach by involving reviewers from different academic backgrounds, which helps in achieving consensus and reducing bias. Experiments show that InnoEval outperforms traditional evaluation methods, aligning closely with expert human judgments across various evaluation tasks."
                },
                "zh": {
                    "title": "åˆ›æ–°è¯„ä¼°çš„æ–°æ ‡å‡†ï¼šInnoEval",
                    "desc": "InnoEvalæ˜¯ä¸€ä¸ªæ·±åº¦åˆ›æ–°è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿäººç±»çš„åˆ›æ„è¯„ä¼°èƒ½åŠ›ã€‚å®ƒé€šè¿‡çŸ¥è¯†é©±åŠ¨çš„å¤šè§’åº¦æ¨ç†ï¼Œç»“åˆå¼‚æ„æ·±åº¦çŸ¥è¯†æœç´¢å’Œå¤šç»´è§£è€¦è¯„ä¼°ï¼Œæ¥è§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ¥è‡ªä¸åŒåœ¨çº¿æ¥æºçš„åŠ¨æ€è¯æ®ï¼Œç¡®ä¿è¯„ä¼°çš„å…¨é¢æ€§å’Œå‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒInnoEvalåœ¨å¤šç§è¯„ä¼°ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»ŸåŸºå‡†ï¼Œè¯„åˆ¤æ¨¡å¼ä¸äººç±»ä¸“å®¶é«˜åº¦ä¸€è‡´ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14041",
            "title": "BitDance: Scaling Autoregressive Generative Models with Binary Tokens",
            "url": "https://huggingface.co/papers/2602.14041",
            "abstract": "BitDance is a scalable autoregressive image generator that uses binary visual tokens and diffusion-based methods to achieve efficient high-resolution image generation with improved speed and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We present BitDance, a scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents, BitDance lets each token represent up to 2^{256} states, yielding a compact yet highly expressive discrete representation. Sampling from such a huge token space is difficult with standard classification. To resolve this, BitDance uses a binary diffusion head: instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion, a new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256x256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion, BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4x fewer parameters (260M) and achieving 8.7x speedup. For text-to-image generation, BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 1024x1024 images, BitDance achieves a speedup of over 30x compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code and models are available at: https://github.com/shallowdream204/BitDance.",
            "score": 13,
            "issue_id": 1085,
            "pub_date": "2026-02-15",
            "pub_date_card": {
                "ru": "15 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 15",
                "zh": "2æœˆ15æ—¥"
            },
            "hash": "81405516666aafa5",
            "authors": [
                "Yuang Ai",
                "Jiaming Han",
                "Shaobin Zhuang",
                "Weijia Mao",
                "Xuefeng Hu",
                "Ziyan Yang",
                "Zhenheng Yang",
                "Huaibo Huang",
                "Xiangyu Yue",
                "Hao Chen"
            ],
            "affiliations": [
                "ByteDance",
                "Institute of Automation, Chinese Academy of Sciences",
                "MMLab, The Chinese University of Hong Kong",
                "National University of Singapore",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14041.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#multimodal",
                    "#open_source",
                    "#diffusion",
                    "#small_models",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ‘Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "BitDance â€” ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸Ğ½Ğ´ĞµĞºÑĞ¾Ğ² ĞºĞ¾Ğ´Ğ±ÑƒĞºĞ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ´Ğ¾ 2^256 ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ next-patch diffusion Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 30 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ñ€ĞµÑˆÑ‘Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "BitDance: Fast and Efficient Image Generation with Binary Tokens",
                    "desc": "BitDance is an innovative autoregressive image generator that utilizes binary visual tokens for efficient high-resolution image creation. By employing high-entropy binary latents, it allows each token to represent a vast number of states, leading to a compact and expressive representation. The model introduces a binary diffusion head to generate these tokens, enhancing the sampling process compared to traditional methods. Additionally, its next-patch diffusion technique enables parallel token prediction, significantly improving inference speed and performance while using fewer parameters than existing models."
                },
                "zh": {
                    "title": "BitDanceï¼šé«˜æ•ˆçš„è‡ªå›å½’å›¾åƒç”Ÿæˆæ–°æ–¹æ³•",
                    "desc": "BitDanceæ˜¯ä¸€ç§å¯æ‰©å±•çš„è‡ªå›å½’å›¾åƒç”Ÿæˆå™¨ï¼Œä½¿ç”¨äºŒè¿›åˆ¶è§†è§‰æ ‡è®°å’Œæ‰©æ•£æ–¹æ³•æ¥å®ç°é«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆã€‚å®ƒé€šè¿‡é«˜ç†µçš„äºŒè¿›åˆ¶æ½œå˜é‡ï¼Œä½¿æ¯ä¸ªæ ‡è®°èƒ½å¤Ÿè¡¨ç¤ºå¤šè¾¾2^{256}ç§çŠ¶æ€ï¼Œä»è€Œæä¾›ç´§å‡‘è€Œå¯Œæœ‰è¡¨ç°åŠ›çš„ç¦»æ•£è¡¨ç¤ºã€‚ä¸ºäº†å…‹æœä»åºå¤§æ ‡è®°ç©ºé—´ä¸­é‡‡æ ·çš„å›°éš¾ï¼ŒBitDanceé‡‡ç”¨äº†äºŒè¿›åˆ¶æ‰©æ•£å¤´ï¼Œé€šè¿‡è¿ç»­ç©ºé—´æ‰©æ•£ç”ŸæˆäºŒè¿›åˆ¶æ ‡è®°ï¼Œè€Œä¸æ˜¯ä½¿ç”¨softmaxé¢„æµ‹ç´¢å¼•ã€‚æ­¤å¤–ï¼ŒBitDanceå¼•å…¥äº†ä¸‹ä¸€è¡¥ä¸æ‰©æ•£çš„æ–°è§£ç æ–¹æ³•ï¼Œèƒ½å¤Ÿå¹¶è¡Œé¢„æµ‹å¤šä¸ªæ ‡è®°ï¼Œæ˜¾è‘—åŠ å¿«æ¨ç†é€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07824",
            "title": "Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training",
            "url": "https://huggingface.co/papers/2602.07824",
            "abstract": "Data Darwinism presents a systematic framework for data-model co-evolution through a ten-level taxonomy, demonstrating that advanced processing techniques significantly improve foundation model performance on scientific text.  \t\t\t\t\tAI-generated summary \t\t\t\t Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.   To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.",
            "score": 13,
            "issue_id": 1089,
            "pub_date": "2026-02-08",
            "pub_date_card": {
                "ru": "8 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 8",
                "zh": "2æœˆ8æ—¥"
            },
            "hash": "0b9f58872c1786bc",
            "authors": [
                "Yiwei Qin",
                "Zhen Huang",
                "Tiantian Mi",
                "Weiye Si",
                "Chenyang Zhou",
                "Qipeng Guo",
                "Siyuan Feng",
                "Pengfei Liu"
            ],
            "affiliations": [
                "FDU",
                "GAIR",
                "SII",
                "SJTU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07824.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#open_source",
                    "#synthetic"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Data Darwinism â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´ĞµÑÑÑ‚Ğ¸ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ³Ğ´Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ LLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Darwin-Science, ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ¸Ğ· 900 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ ÑÑ‹Ñ€Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ baseline Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ daVinci-origin Ñ Ğ½ÑƒĞ»Ñ, Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡Ğ¸ÑÑ‚Ğ¾Ñ‚Ñƒ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ÑÑ‰ĞµĞ³Ğ¾ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 600 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Darwin-Science Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 20 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚Ğ° Ğ´Ğ¾ +8.40 Ñ‚Ğ¾Ñ‡ĞµĞº Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Unlocking Model Potential through Data Evolution",
                    "desc": "The paper introduces 'Data Darwinism', a framework that outlines how data and models can evolve together through a ten-level taxonomy. It highlights the importance of data quality in enhancing the performance of foundation models, particularly in the context of scientific literature. By creating the Darwin-Science corpus, which consists of 900 billion tokens, the authors demonstrate that advanced processing techniques like Generative Refinement and Cognitive Completion can significantly improve model learnability. The results show that models trained on this refined data outperform traditional baselines, confirming that systematic data processing can unlock greater value in machine learning applications."
                },
                "zh": {
                    "title": "æ•°æ®ä¸æ¨¡å‹çš„å…±åŒè¿›åŒ–ä¹‹é“",
                    "desc": "æ•°æ®è¾¾å°”æ–‡ä¸»ä¹‰æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿæ¡†æ¶ï¼Œç”¨äºæ•°æ®ä¸æ¨¡å‹çš„å…±åŒè¿›åŒ–ï¼ŒåŒ…å«åä¸ªå±‚çº§çš„åˆ†ç±»æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå…ˆè¿›çš„å¤„ç†æŠ€æœ¯æ˜¾è‘—æå‡äº†åŸºç¡€æ¨¡å‹åœ¨ç§‘å­¦æ–‡æœ¬ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬é€šè¿‡æ„å»ºDarwin-Scienceï¼Œä¸€ä¸ªåŒ…å«9000äº¿ä¸ªæ ‡è®°çš„è¯­æ–™åº“ï¼ŒéªŒè¯äº†è¿™ä¸€ç†è®ºï¼Œå¹¶å‘ç°åŸå§‹ç§‘å­¦æ–‡æœ¬å­˜åœ¨å¯å­¦ä¹ æ€§å·®è·ã€‚é€šè¿‡ä½¿ç”¨å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç”Ÿæˆæ€§ç²¾ç‚¼å’Œè®¤çŸ¥è¡¥å…¨ï¼Œæˆ‘ä»¬æˆåŠŸå¼¥è¡¥äº†è¿™ä¸€å·®è·ï¼Œæå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.13367",
            "title": "Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts",
            "url": "https://huggingface.co/papers/2602.13367",
            "abstract": "Nanbeige4.1-3B is a 3B-parameter unified language model that demonstrates superior performance in agentic behavior, code generation, and reasoning compared to larger models through advanced reward modeling and training techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.",
            "score": 11,
            "issue_id": 1091,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "997eb846d353137d",
            "authors": [
                "Chen Yang",
                "Guangyue Peng",
                "Jiaying Zhu",
                "Ran Le",
                "Ruixiang Feng",
                "Tao Zhang",
                "Xiyun Xu",
                "Yang Song",
                "Yiming Jia",
                "Yuntao Wen",
                "Yunzhi Xu",
                "Zekai Wang",
                "Zhenwei An",
                "Zhicong Sun",
                "Zongchao Chen"
            ],
            "affiliations": [
                "Boss Zhipin",
                "Nanbeige LLM Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.13367.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#small_models",
                    "#rlhf",
                    "#synthetic",
                    "#rl",
                    "#open_source",
                    "#reasoning",
                    "#optimization",
                    "#training",
                    "#agents",
                    "#plp"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞœĞ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ…: ÑƒĞ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ğ´Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² 3B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ñ…",
                    "desc": "Nanbeige4.1-3B â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¾Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ (pointwise Ğ¸ pairwise). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°Ğº Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ´Ğ¾ 600 Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´Ñ€ÑĞ´ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Small Model, Big Impact: Nanbeige4.1-3B Redefines Language Model Potential",
                    "desc": "Nanbeige4.1-3B is a unified language model with 3 billion parameters that excels in agentic behavior, code generation, and reasoning tasks. It is the first open-source small language model to achieve such a high level of versatility. The model employs advanced reward modeling techniques, including point-wise and pair-wise rewards, to enhance reasoning and ensure responses align with human preferences. Additionally, it utilizes complexity-aware rewards in Reinforcement Learning for code generation, allowing it to perform complex problem-solving with stable tool interactions."
                },
                "zh": {
                    "title": "å°æ¨¡å‹ä¹Ÿèƒ½å¤§ä½œä¸ºï¼",
                    "desc": "Nanbeige4.1-3B æ˜¯ä¸€ä¸ªæ‹¥æœ‰ 30 äº¿å‚æ•°çš„ç»Ÿä¸€è¯­è¨€æ¨¡å‹ï¼Œå±•ç°å‡ºåœ¨æ™ºèƒ½è¡Œä¸ºã€ä»£ç ç”Ÿæˆå’Œæ¨ç†æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚é€šè¿‡å…ˆè¿›çš„å¥–åŠ±å»ºæ¨¡å’Œè®­ç»ƒæŠ€æœ¯ï¼Œå®ƒåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šäº†æ›´å¤§çš„æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆäº†ç‚¹å¯¹ç‚¹å’Œæˆå¯¹å¥–åŠ±å»ºæ¨¡ï¼Œä»¥æé«˜æ¨ç†å’Œåå¥½å¯¹é½çš„è´¨é‡ï¼ŒåŒæ—¶åœ¨å¼ºåŒ–å­¦ä¹ ä¸­è®¾è®¡äº†å¤æ‚åº¦æ„ŸçŸ¥çš„å¥–åŠ±ï¼Œä»¥ä¼˜åŒ–ä»£ç ç”Ÿæˆçš„æ­£ç¡®æ€§å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNanbeige4.1-3B åœ¨ä¸åŒç±»è§„æ¨¡çš„æ¨¡å‹æ¯”è¾ƒæ—¶è¡¨ç°æ˜¾è‘—ä¼˜è¶Šï¼Œé‡æ–°å®šä¹‰äº† 30 äº¿å‚æ•°æ¨¡å‹çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14178",
            "title": "UniWeTok: An Unified Binary Tokenizer with Codebook Size 2^{128} for Unified Multimodal Large Language Model",
            "url": "https://huggingface.co/papers/2602.14178",
            "abstract": "UniWeTok introduces a unified discrete tokenizer with a massive binary codebook and novel training techniques to achieve superior performance in image generation and multimodal tasks while reducing computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook (2^{128}). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.",
            "score": 8,
            "issue_id": 1084,
            "pub_date": "2026-02-15",
            "pub_date_card": {
                "ru": "15 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 15",
                "zh": "2æœˆ15æ—¥"
            },
            "hash": "7c417dc23afb0abb",
            "authors": [
                "Shaobin Zhuang",
                "Yuang Ai",
                "Jiaming Han",
                "Weijia Mao",
                "Xiaohui Li",
                "Fangyikang Wang",
                "Xiao Wang",
                "Yan Li",
                "Shanchuan Lin",
                "Kun Xu",
                "Zhenheng Yang",
                "Huaibo Huang",
                "Xiangyu Yue",
                "Hao Chen",
                "Yali Wang"
            ],
            "affiliations": [
                "ByteDance",
                "Institute of Automation, Chinese Academy of Sciences",
                "MMLab, The Chinese University of Hong Kong",
                "National University of Singapore",
                "Shanghai Jiao Tong University",
                "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14178.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#architecture",
                    "#multimodal",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "UniWeTok Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ñ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ğ¼ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ‘Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 2^128, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¸Ğ³Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Pre-Post Distillation Ğ¸ Generative-Aware Prior, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ²Ñ‘Ñ€Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ SigLu, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ loss Ğ¿Ñ€Ğ¸Ğ²ĞµÑ€Ğ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¢Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ…ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğº Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼ Ñ Ğ»Ğ¸Ñ†Ğ°Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. UniWeTok Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "UniWeTok: Bridging the Gap in Multimodal Tokenization",
                    "desc": "UniWeTok is a novel discrete tokenizer that enhances image generation and multimodal tasks by utilizing a large binary codebook and innovative training methods. It addresses the challenges of high-fidelity reconstruction and complex semantic extraction within a single framework. The paper introduces a unique training approach called Pre-Post Distillation and a Generative-Aware Prior, which improve the performance of discrete tokens. Additionally, a convolution-attention hybrid architecture with the SigLu activation function is proposed to optimize the training process and adaptability across various scenarios."
                },
                "zh": {
                    "title": "ç»Ÿä¸€ç¦»æ•£åˆ†è¯å™¨ï¼Œæå‡å¤šæ¨¡æ€ä»»åŠ¡æ€§èƒ½",
                    "desc": "UniWeTokæ˜¯ä¸€ç§ç»Ÿä¸€çš„ç¦»æ•£åˆ†è¯å™¨ï¼Œé‡‡ç”¨äº†ä¸€ä¸ªå·¨å¤§çš„äºŒè¿›åˆ¶ä»£ç æœ¬å’Œæ–°é¢–çš„è®­ç»ƒæŠ€æœ¯ï¼Œä»¥åœ¨å›¾åƒç”Ÿæˆå’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­å®ç°å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶é™ä½è®¡ç®—éœ€æ±‚ã€‚å®ƒé€šè¿‡å¼•å…¥é¢„åè’¸é¦å’Œç”Ÿæˆæ„ŸçŸ¥å…ˆéªŒæ¥å¢å¼ºç¦»æ•£æ ‡è®°çš„è¯­ä¹‰æå–å’Œç”Ÿæˆèƒ½åŠ›ã€‚UniWeTokçš„æ¨¡å‹æ¶æ„ç»“åˆäº†å·ç§¯å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶ä½¿ç”¨SigLuæ¿€æ´»å‡½æ•°æ¥ç¨³å®šè¯­ä¹‰è’¸é¦è¿‡ç¨‹ã€‚ç»è¿‡ä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶çš„ä¼˜åŒ–ï¼ŒUniWeTokåœ¨å„ç§å›¾åƒåˆ†è¾¨ç‡å’Œæ„ŸçŸ¥æ•æ„Ÿåœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨ImageNetä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å›¾åƒç”Ÿæˆæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.13823",
            "title": "Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings",
            "url": "https://huggingface.co/papers/2602.13823",
            "abstract": "A reasoning-driven universal multimodal embedding framework integrates embedder-guided reinforcement learning with traceability chain-of-thought to enhance cross-modal semantic consistency and retrieval performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks. Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose a reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner, ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment, effectively strengthens cross-modal semantic consistency and boosts the fine-grained matching capability of the model as well as the generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development.",
            "score": 6,
            "issue_id": 1084,
            "pub_date": "2026-02-14",
            "pub_date_card": {
                "ru": "14 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 14",
                "zh": "2æœˆ14æ—¥"
            },
            "hash": "5066c0e4ca817367",
            "authors": [
                "Haonan Jiang",
                "Yuji Wang",
                "Yongjie Zhu",
                "Xin Lu",
                "Wenyu Qin",
                "Meng Wang",
                "Pengfei Wan",
                "Yansong Tang"
            ],
            "affiliations": [
                "Kling Team, Kuaishou Technology",
                "Tsinghua Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.13823.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#multimodal",
                    "#reasoning",
                    "#rag",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚æ¡†æ¶EG-RL, Ğ³Ğ´Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´ĞµÑ€ ÑĞ²Ğ½Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ĞµÑ‘ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (T-CoT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ…, Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… MMEB-V2 Ğ¸ UVRB, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºÑ€Ğ¾ÑÑĞ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ."
                },
                "en": {
                    "title": "Enhancing Multimodal Retrieval with Reasoning-Driven Embeddings",
                    "desc": "This paper presents a new framework for Universal Multimodal Embeddings (UME) that enhances the consistency and performance of retrieving information across different types of data, like text and images. It combines Embedder-Guided Reinforcement Learning (EG-RL) with a novel reasoning approach called Traceability Chain-of-Thought (T-CoT) to improve how models understand and relate different modalities. The framework allows the reasoning process to be guided by the embedder, ensuring that the generated reasoning is relevant to the retrieval tasks. Overall, this approach shows significant improvements in embedding quality and retrieval effectiveness on benchmark datasets, demonstrating the power of structured reasoning in multimodal contexts."
                },
                "zh": {
                    "title": "æ¨ç†é©±åŠ¨çš„å¤šæ¨¡æ€åµŒå…¥æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨ç†çš„é€šç”¨å¤šæ¨¡æ€åµŒå…¥æ¡†æ¶ï¼Œç»“åˆäº†åµŒå…¥å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ å’Œå¯è¿½æº¯çš„æ€ç»´é“¾ï¼Œä»¥å¢å¼ºè·¨æ¨¡æ€çš„è¯­ä¹‰ä¸€è‡´æ€§å’Œæ£€ç´¢æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥ç”Ÿæˆçš„æ€ç»´é“¾æ¨ç†ï¼Œæ˜¾è‘—æå‡äº†ä»»åŠ¡ç‰¹å®šçš„è¡¨ç¤ºèƒ½åŠ›ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªEG-RLæ¡†æ¶ï¼Œä½¿åµŒå…¥å™¨ä¸ºæ¨ç†å™¨æä¾›æ˜ç¡®çš„ç›‘ç£ï¼Œç¡®ä¿ç”Ÿæˆçš„æ€ç»´é“¾ä¸åµŒå…¥ä»»åŠ¡å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹ï¼Œæå‡äº†æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸­çš„åŒ¹é…èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12876",
            "title": "BrowseComp-V^3: A Visual, Vertical, and Verifiable Benchmark for Multimodal Browsing Agents",
            "url": "https://huggingface.co/papers/2602.12876",
            "abstract": "A new benchmark called BrowseComp-V3 challenges multimodal large language models with complex, multi-hop reasoning tasks requiring deep search across text and visual modalities, revealing significant gaps in current capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs), equipped with increasingly advanced planning and tool-use capabilities, are evolving into autonomous agents capable of performing multimodal web browsing and deep search in open-world environments. However, existing benchmarks for multimodal browsing remain limited in task complexity, evidence accessibility, and evaluation granularity, hindering comprehensive and reproducible assessments of deep search capabilities. To address these limitations, we introduce BrowseComp-V^3, a novel benchmark consisting of 300 carefully curated and challenging questions spanning diverse domains. The benchmark emphasizes deep, multi-level, and cross-modal multi-hop reasoning, where critical evidence is interleaved across textual and visual modalities within and across web pages. All supporting evidence is strictly required to be publicly searchable, ensuring fairness and reproducibility. Beyond final-answer accuracy, we incorporate an expert-validated, subgoal-driven process evaluation mechanism that enables fine-grained analysis of intermediate reasoning behaviors and systematic characterization of capability boundaries. In addition, we propose OmniSeeker, a unified multimodal browsing agent framework integrating diverse web search and visual perception tools. Comprehensive experiments demonstrate that even state-of-the-art models achieve only 36% accuracy on our benchmark, revealing critical bottlenecks in multimodal information integration and fine-grained perception. Our results highlight a fundamental gap between current model capabilities and robust multimodal deep search in real-world settings.",
            "score": 6,
            "issue_id": 1084,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "1bff688eadc0be15",
            "authors": [
                "Huanyao Zhang",
                "Jiepeng Zhou",
                "Bo Li",
                "Bowen Zhou",
                "Yanzhe Dan",
                "Haishan Lu",
                "Zhiyong Cao",
                "Jiaoyang Chen",
                "Yuqian Han",
                "Zinan Sheng",
                "Zhengwei Tao",
                "Hao Liang",
                "Jialong Wu",
                "Yang Shi",
                "Yuanpeng He",
                "Jiaye Lin",
                "Qintong Zhang",
                "Guochen Yan",
                "Runhao Zhao",
                "Zhengpin Li",
                "Xiaohan Yu",
                "Lang Mei",
                "Chong Chen",
                "Wentao Zhang",
                "Bin Cui"
            ],
            "affiliations": [
                "CASIA",
                "HITSZ",
                "HKUST(GZ)",
                "Huawei Cloud BU",
                "OUC",
                "PKU",
                "THU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12876.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞºĞµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº BrowseComp-V3 Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 36% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° OmniSeeker â€” ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Bridging the Gap in Multimodal Reasoning with BrowseComp-V3",
                    "desc": "The paper introduces BrowseComp-V3, a new benchmark designed to test multimodal large language models (MLLMs) on complex reasoning tasks that require deep searching across both text and visual information. It highlights the limitations of existing benchmarks in evaluating the capabilities of MLLMs, particularly in terms of task complexity and evidence accessibility. BrowseComp-V3 consists of 300 challenging questions that necessitate multi-hop reasoning across different modalities, ensuring that all evidence is publicly searchable for fairness. The study reveals that even the best current models only achieve 36% accuracy, indicating significant gaps in their ability to integrate multimodal information effectively."
                },
                "zh": {
                    "title": "æŒ‘æˆ˜å¤šæ¨¡æ€æ¨ç†çš„æ–°åŸºå‡†æµ‹è¯•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•BrowseComp-V3ï¼Œæ—¨åœ¨æŒ‘æˆ˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤æ‚çš„å¤šè·³æ¨ç†ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚è¿™äº›ä»»åŠ¡éœ€è¦åœ¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€ä¸­è¿›è¡Œæ·±å…¥æœç´¢ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹èƒ½åŠ›çš„æ˜¾è‘—å·®è·ã€‚æ–°åŸºå‡†åŒ…å«300ä¸ªç²¾å¿ƒç­–åˆ’çš„é—®é¢˜ï¼Œå¼ºè°ƒæ·±å±‚æ¬¡çš„å¤šçº§è·¨æ¨¡æ€æ¨ç†ï¼Œå¹¶è¦æ±‚æ‰€æœ‰è¯æ®å¿…é¡»æ˜¯å…¬å¼€å¯æœç´¢çš„ï¼Œä»¥ç¡®ä¿å…¬å¹³æ€§å’Œå¯é‡å¤æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†OmniSeekeræ¡†æ¶ï¼Œæ•´åˆäº†å¤šç§ç½‘ç»œæœç´¢å’Œè§†è§‰æ„ŸçŸ¥å·¥å…·ï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œä¸­çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11574",
            "title": "Learning to Configure Agentic AI Systems",
            "url": "https://huggingface.co/papers/2602.11574",
            "abstract": "Learning per-query agent configurations through reinforcement learning improves task accuracy while reducing computational costs compared to fixed templates and hand-tuned heuristics.  \t\t\t\t\tAI-generated summary \t\t\t\t Configuring LLM-based agent systems involves choosing workflows, tools, token budgets, and prompts from a large combinatorial design space, and is typically handled today by fixed large templates or hand-tuned heuristics. This leads to brittle behavior and unnecessary compute, since the same cumbersome configuration is often applied to both easy and hard input queries. We formulate agent configuration as a query-wise decision problem and introduce ARC (Agentic Resource & Configuration learner), which learns a light-weight hierarchical policy using reinforcement learning to dynamically tailor these configurations. Across multiple benchmarks spanning reasoning and tool-augmented question answering, the learned policy consistently outperforms strong hand-designed and other baselines, achieving up to 25% higher task accuracy while also reducing token and runtime costs. These results demonstrate that learning per-query agent configurations is a powerful alternative to \"one size fits all\" designs.",
            "score": 6,
            "issue_id": 1098,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "72e2aea860ebab16",
            "authors": [
                "Aditya Taparia",
                "Som Sagar",
                "Ransalu Senanayake"
            ],
            "affiliations": [
                "School of Computing and Augmented Intelligence, Arizona State University, Tempe, United States of America"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11574.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#benchmark",
                    "#optimization",
                    "#reasoning",
                    "#rl"
                ],
                "emoji": "âš™ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°, Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ARC, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ€ÑƒÑ‡Ğ½Ñ‹Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹ Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ½Ğ° 25% Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ´ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑÑ…ĞµĞ¼Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Dynamic Configurations for Enhanced Task Performance",
                    "desc": "This paper presents a method for configuring agent systems using reinforcement learning, which adapts to each specific query instead of relying on fixed templates. The authors introduce ARC (Agentic Resource & Configuration learner), a lightweight hierarchical policy that dynamically adjusts configurations based on the complexity of the input. By treating agent configuration as a decision problem, ARC improves task accuracy by up to 25% while also reducing computational costs. The results show that personalized configurations are more effective than traditional one-size-fits-all approaches."
                },
                "zh": {
                    "title": "é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä»£ç†é…ç½®ï¼Œæå‡ä»»åŠ¡å‡†ç¡®æ€§ï¼",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥å­¦ä¹ æ¯ä¸ªæŸ¥è¯¢çš„ä»£ç†é…ç½®çš„æ–¹æ³•ï¼Œä»è€Œæé«˜ä»»åŠ¡çš„å‡†ç¡®æ€§å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚ä¼ ç»Ÿä¸Šï¼Œä»£ç†ç³»ç»Ÿçš„é…ç½®ä¾èµ–äºå›ºå®šçš„å¤§æ¨¡æ¿æˆ–æ‰‹åŠ¨è°ƒä¼˜çš„å¯å‘å¼æ–¹æ³•ï¼Œè¿™å¯¼è‡´äº†ä¸ç¨³å®šçš„è¡¨ç°å’Œä¸å¿…è¦çš„è®¡ç®—èµ„æºæµªè´¹ã€‚æˆ‘ä»¬å°†ä»£ç†é…ç½®è§†ä¸ºä¸€ä¸ªæŸ¥è¯¢çº§çš„å†³ç­–é—®é¢˜ï¼Œæå‡ºäº†ARCï¼ˆAgentic Resource & Configuration learnerï¼‰ï¼Œå®ƒä½¿ç”¨å¼ºåŒ–å­¦ä¹ åŠ¨æ€è°ƒæ•´é…ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒARCåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä»»åŠ¡å‡†ç¡®ç‡æé«˜äº†25%ï¼ŒåŒæ—¶å‡å°‘äº†ä»¤ç‰Œå’Œè¿è¡Œæ—¶é—´çš„æˆæœ¬ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14721",
            "title": "WebWorld: A Large-Scale World Model for Web Agent Training",
            "url": "https://huggingface.co/papers/2602.14721",
            "abstract": "WebWorld is an open-web simulator trained on over one million interactions that supports long-horizon reasoning and multi-format data, achieving performance comparable to advanced models like Gemini-3-Pro and GPT-4o.  \t\t\t\t\tAI-generated summary \t\t\t\t Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce WebWorld series, the first open-web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation, we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation, Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search, outperforming GPT-5 as a world model. Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.",
            "score": 5,
            "issue_id": 1085,
            "pub_date": "2026-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "52090d1e0e85eb26",
            "authors": [
                "Zikai Xiao",
                "Jianhong Tu",
                "Chuhang Zou",
                "Yuxin Zuo",
                "Zhi Li",
                "Peng Wang",
                "Bowen Yu",
                "Fei Huang",
                "Junyang Lin",
                "Zuozhu Liu"
            ],
            "affiliations": [
                "Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14721.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#multimodal",
                    "#synthetic",
                    "#agents",
                    "#open_source",
                    "#reasoning",
                    "#long_context",
                    "#training"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ²ĞµĞ±-ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "WebWorld â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ²ĞµĞ±-ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ. ĞĞ½ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ¾ 30+ ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ LLM Ğ²Ñ€Ğ¾Ğ´Ğµ Gemini-3-Pro Ğ¸ GPT-4o, Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ±ĞµĞ· Ñ€Ğ¸ÑĞºĞ¾Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ÑĞµÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ²ĞµĞ±-Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğ¾ Ğ¸ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑÑ€ĞµĞ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "WebWorld: Revolutionizing AI Training with Open-Web Simulations",
                    "desc": "WebWorld is a large-scale open-web simulator designed to enhance the training of AI agents by utilizing over one million interactions. It supports long-horizon reasoning and can handle multi-format data, allowing for simulations that extend beyond 30 steps. The simulator's performance is on par with advanced models like Gemini-3-Pro and GPT-4o, demonstrating its effectiveness in both intrinsic and extrinsic evaluations. Additionally, WebWorld shows the ability to generalize across different domains, including code and gaming environments, making it a versatile tool for developing world models."
                },
                "zh": {
                    "title": "WebWorldï¼šå¼€æ”¾ç½‘ç»œæ¨¡æ‹Ÿçš„æœªæ¥",
                    "desc": "WebWorldæ˜¯ä¸€ä¸ªå¼€æ”¾ç½‘ç»œæ¨¡æ‹Ÿå™¨ï¼Œç»è¿‡è¶…è¿‡ä¸€ç™¾ä¸‡æ¬¡äº¤äº’çš„è®­ç»ƒï¼Œæ”¯æŒé•¿æ—¶é—´æ¨ç†å’Œå¤šç§æ ¼å¼çš„æ•°æ®ã€‚ä¸ç°æœ‰çš„å°é—­ç¯å¢ƒæ¨¡æ‹Ÿå™¨ä¸åŒï¼ŒWebWorldåˆ©ç”¨å¯æ‰©å±•çš„æ•°æ®ç®¡é“è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒï¼Œèƒ½å¤Ÿè¿›è¡Œè¶…è¿‡30æ­¥çš„é•¿æ—¶é—´æ¨¡æ‹Ÿã€‚é€šè¿‡å¼•å…¥WebWorld-Benchè¿›è¡Œå†…åœ¨è¯„ä¼°ï¼ŒWebWorldåœ¨ä¹ä¸ªç»´åº¦ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ¨¡æ‹Ÿæ€§èƒ½ä¸Gemini-3-Proç›¸å½“ã€‚WebWorldä¸ä»…åœ¨ç½‘ç»œæ¨¡æ‹Ÿä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¿˜èƒ½è·¨é¢†åŸŸæ³›åŒ–åˆ°ä»£ç ã€å›¾å½¢ç”¨æˆ·ç•Œé¢å’Œæ¸¸æˆç¯å¢ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.13195",
            "title": "Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision",
            "url": "https://huggingface.co/papers/2602.13195",
            "abstract": "Conversational image segmentation addresses functional and physical reasoning tasks by introducing a new benchmark and model that combines segmentation priors with language understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., \"left-most apple\") and overlooks functional and physical reasoning (e.g., \"where can I safely store the knife?\"). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding, and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/",
            "score": 4,
            "issue_id": 1087,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "a8bc4ef47d1edcb3",
            "authors": [
                "Aadarsh Sahoo",
                "Georgia Gkioxari"
            ],
            "affiliations": [
                "Caltech"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.13195.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#open_source",
                    "#reasoning",
                    "#cv",
                    "#dataset",
                    "#multimodal",
                    "#benchmark"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ”Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ: Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğº Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ConverSeg, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ConverSeg-Net, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€ 'Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ-Ğ¼Ğ°ÑĞºĞ°' Ğ±ĞµĞ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ."
                },
                "en": {
                    "title": "Bridging Language and Vision for Smarter Image Segmentation",
                    "desc": "This paper introduces Conversational Image Segmentation (CIS), a new approach that combines image segmentation with language understanding to enhance functional and physical reasoning tasks. The authors present a benchmark called ConverSeg, which evaluates models on various aspects such as spatial relations and safety considerations. They propose ConverSeg-Net, a model that integrates advanced segmentation techniques with natural language processing to create accurate pixel masks based on conversational queries. The results show that existing models struggle with CIS tasks, while ConverSeg-Net demonstrates improved performance on both the new benchmark and traditional segmentation tasks."
                },
                "zh": {
                    "title": "å¯¹è¯å¼å›¾åƒåˆ†å‰²ï¼šç»“åˆè¯­è¨€ç†è§£ä¸åˆ†å‰²å…ˆéªŒ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¯¹è¯å¼å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³åŠŸèƒ½å’Œç‰©ç†æ¨ç†ä»»åŠ¡ã€‚æˆ‘ä»¬å¼•å…¥äº†å¯¹è¯å¼å›¾åƒåˆ†å‰²ï¼ˆCISï¼‰å’ŒConverSegåŸºå‡†ï¼Œç»“åˆäº†å®ä½“ã€ç©ºé—´å…³ç³»ã€æ„å›¾å’Œå®‰å…¨æ€§ç­‰æ–¹é¢çš„æ¨ç†ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ConverSeg-Netæ¨¡å‹ï¼Œå®ƒå°†å¼ºå¤§çš„åˆ†å‰²å…ˆéªŒä¸è¯­è¨€ç†è§£ç›¸ç»“åˆï¼Œå¹¶é€šè¿‡AIé©±åŠ¨çš„æ•°æ®å¼•æ“ç”Ÿæˆæ— ç›‘ç£çš„æç¤º-æ©ç å¯¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„è¯­è¨€å¼•å¯¼åˆ†å‰²æ¨¡å‹åœ¨CISä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³ï¼Œè€ŒConverSeg-Netåœ¨æˆ‘ä»¬çš„æ•°æ®å¼•æ“ä¸Šè®­ç»ƒåï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14147",
            "title": "LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models",
            "url": "https://huggingface.co/papers/2602.14147",
            "abstract": "LaViDa-R1 is a multimodal reasoning diffusion language model that unifies supervised fine-tuning and multi-task reinforcement learning with novel training techniques for enhanced performance across visual reasoning and generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing, tree search, and complementary likelihood estimation, to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks, including visual math reasoning, reason-intensive grounding, and image editing.",
            "score": 3,
            "issue_id": 1084,
            "pub_date": "2026-02-15",
            "pub_date_card": {
                "ru": "15 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 15",
                "zh": "2æœˆ15æ—¥"
            },
            "hash": "457ae835cce3e314",
            "authors": [
                "Shufan Li",
                "Yuchen Zhu",
                "Jiuxiang Gu",
                "Kangning Liu",
                "Zhe Lin",
                "Yongxin Chen",
                "Molei Tao",
                "Aditya Grover",
                "Jason Kuen"
            ],
            "affiliations": [
                "Adobe",
                "Georgia Tech",
                "UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14147.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#architecture",
                    "#multimodal",
                    "#reasoning",
                    "#training",
                    "#diffusion"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "LaViDa-R1 â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚, Ğ¾Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ. LaViDa-R1 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…: Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unifying Multimodal Reasoning with LaViDa-R1",
                    "desc": "LaViDa-R1 is a new type of language model that combines different ways of learning to improve how machines understand and generate information from both text and images. It uses a unique training approach that merges supervised fine-tuning with multi-task reinforcement learning, allowing it to tackle various tasks at once. This model introduces innovative techniques like answer-forcing and tree search to boost its performance and adaptability. Experiments show that LaViDa-R1 excels in complex tasks such as visual reasoning and image editing, making it a powerful tool for multimodal applications."
                },
                "zh": {
                    "title": "LaViDa-R1ï¼šå¤šæ¨¡æ€æ¨ç†çš„ç»Ÿä¸€è§£å†³æ–¹æ¡ˆ",
                    "desc": "LaViDa-R1æ˜¯ä¸€ç§å¤šæ¨¡æ€æ¨ç†æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†ç›‘ç£å¾®è°ƒå’Œå¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ çš„æ–°è®­ç»ƒæŠ€æœ¯ï¼Œä»¥æé«˜è§†è§‰æ¨ç†å’Œç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚ä¸ç°æœ‰çš„é€šè¿‡ç‰¹å®šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æ„å»ºæ¨ç†æ‰©æ•£è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼ŒLaViDa-R1ä»¥ç»Ÿä¸€çš„æ–¹å¼æ•´åˆäº†å¤šç§å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„ç»Ÿä¸€åè®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿæ— ç¼ç»“åˆç›‘ç£å¾®è°ƒå’Œå¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ã€‚é€šè¿‡å¤šé¡¹å®éªŒï¼ŒLaViDa-R1åœ¨è§†è§‰æ•°å­¦æ¨ç†ã€æ¨ç†å¯†é›†çš„åŸºç¡€å’Œå›¾åƒç¼–è¾‘ç­‰å¤šç§å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.13344",
            "title": "FireRed-Image-Edit-1.0 Techinical Report",
            "url": "https://huggingface.co/papers/2602.13344",
            "abstract": "FireRed-Image-Edit uses a diffusion transformer with optimized data curation and training methods to achieve state-of-the-art performance in instruction-based image editing, supported by a comprehensive benchmark and novel techniques for data efficiency and optimization stability.  \t\t\t\t\tAI-generated summary \t\t\t\t We present FireRed-Image-Edit, a diffusion transformer for instruction-based image editing that achieves state-of-the-art performance through systematic optimization of data curation, training methodology, and evaluation design. We construct a 1.6B-sample training corpus, comprising 900M text-to-image and 700M image editing pairs from diverse sources. After rigorous cleaning, stratification, auto-labeling, and two-stage filtering, we retain over 100M high-quality samples balanced between generation and editing, ensuring strong semantic coverage and instruction alignment. Our multi-stage training pipeline progressively builds editing capability via pre-training, supervised fine-tuning, and reinforcement learning. To improve data efficiency, we introduce a Multi-Condition Aware Bucket Sampler for variable-resolution batching and Stochastic Instruction Alignment with dynamic prompt re-indexing. To stabilize optimization and enhance controllability, we propose Asymmetric Gradient Optimization for DPO, DiffusionNFT with layout-aware OCR rewards for text editing, and a differentiable Consistency Loss for identity preservation. We further establish REDEdit-Bench, a comprehensive benchmark spanning 15 editing categories, including newly introduced beautification and low-level enhancement tasks. Extensive experiments on REDEdit-Bench and public benchmarks (ImgEdit and GEdit) demonstrate competitive or superior performance against both open-source and proprietary systems. We release code, models, and the benchmark suite to support future research.",
            "score": 3,
            "issue_id": 1084,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "1f15cbb364c88559",
            "authors": [
                "Super Intelligence Team",
                "Changhao Qiao",
                "Chao Hui",
                "Chen Li",
                "Cunzheng Wang",
                "Dejia Song",
                "Jiale Zhang",
                "Jing Li",
                "Qiang Xiang",
                "Runqi Wang",
                "Shuang Sun",
                "Wei Zhu",
                "Xu Tang",
                "Yao Hu",
                "Yibo Chen",
                "Yuhao Huang",
                "Yuxuan Duan",
                "Zhiyi Chen",
                "Ziyuan Guo"
            ],
            "affiliations": [
                "Xiaohongshu Inc."
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.13344.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#rlhf",
                    "#open_source",
                    "#data",
                    "#benchmark",
                    "#dataset",
                    "#training",
                    "#cv"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹",
                    "desc": "ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° FireRed-Image-Edit â€” Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ¸Ğ· 1.6 Ğ¼Ğ»Ñ€Ğ´ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¾Ğ¹ Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ² Ğ±Ğ¾Ğ»ĞµĞµ 100 Ğ¼Ğ»Ğ½ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¹ ÑÑĞ¼Ğ¿Ğ»ĞµÑ€ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° REDEdit-Bench Ñ 15 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Revolutionizing Image Editing with FireRed-Image-Edit",
                    "desc": "FireRed-Image-Edit is a cutting-edge diffusion transformer designed for instruction-based image editing, achieving top performance through advanced data curation and training techniques. It utilizes a massive training dataset of 1.6 billion samples, ensuring high-quality and diverse image editing capabilities. The model employs a multi-stage training approach, incorporating pre-training, supervised fine-tuning, and reinforcement learning to enhance its editing skills. Additionally, it introduces innovative methods for data efficiency and optimization stability, validated through extensive benchmarking against existing systems."
                },
                "zh": {
                    "title": "åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ–°çªç ´",
                    "desc": "FireRed-Image-Edit æ˜¯ä¸€ç§åŸºäºæ‰©æ•£å˜æ¢å™¨çš„å›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œä¸“æ³¨äºæŒ‡ä»¤é©±åŠ¨çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¼˜åŒ–æ•°æ®æ•´ç†å’Œè®­ç»ƒæ–¹æ³•ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«16äº¿æ ·æœ¬çš„è®­ç»ƒæ•°æ®é›†ï¼Œç¡®ä¿äº†é«˜è´¨é‡æ ·æœ¬çš„å¹³è¡¡å’Œè¯­ä¹‰è¦†ç›–ã€‚ä¸ºäº†æé«˜æ•°æ®æ•ˆç‡ï¼Œæ¨¡å‹å¼•å…¥äº†å¤šæ¡ä»¶æ„ŸçŸ¥çš„æ¡¶é‡‡æ ·å™¨å’ŒåŠ¨æ€æç¤ºé‡ç´¢å¼•æŠ€æœ¯ã€‚é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒæµç¨‹å’Œåˆ›æ–°çš„ä¼˜åŒ–æŠ€æœ¯ï¼ŒFireRed-Image-Edit åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†è®¸å¤šå¼€æºå’Œä¸“æœ‰ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14534",
            "title": "MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation",
            "url": "https://huggingface.co/papers/2602.14534",
            "abstract": "A unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards improves human motion understanding and generation through semantic alignment, reasoning coherence, and physical plausibility.  \t\t\t\t\tAI-generated summary \t\t\t\t Human motion understanding and generation are crucial for vision and robotics but remain limited in reasoning capability and test-time planning. We propose MoRL, a unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards. Our task-specific reward design combines semantic alignment and reasoning coherence for understanding with physical plausibility and text-motion consistency for generation, improving both logical reasoning and perceptual realism. To further enhance inference, we introduce Chain-of-Motion (CoM), a test-time reasoning method that enables step-by-step planning and reflection. We also construct two large-scale CoT datasets, MoUnd-CoT-140K and MoGen-CoT-140K, to align motion sequences with reasoning traces and action descriptions. Experiments on HumanML3D and KIT-ML show that MoRL achieves significant gains over state-of-the-art baselines. Code: https://github.com/AIGeeksGroup/MoRL. Website: https://aigeeksgroup.github.io/MoRL.",
            "score": 2,
            "issue_id": 1085,
            "pub_date": "2026-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "9e1c007f7c019e5b",
            "authors": [
                "Hongpeng Wang",
                "Zeyu Zhang",
                "Wenhao Li",
                "Hao Tang"
            ],
            "affiliations": [
                "Nanyang Technological University",
                "Peking University",
                "The University of Sydney"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14534.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#open_source",
                    "#rl",
                    "#reasoning",
                    "#rlhf",
                    "#cv",
                    "#robotics",
                    "#optimization"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ›Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ MoRL â€” ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Chain-of-Motion Ğ´Ğ»Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Human Motion with Unified Multimodal Learning",
                    "desc": "This paper presents MoRL, a unified multimodal motion model that enhances human motion understanding and generation. It employs supervised fine-tuning and reinforcement learning with verifiable rewards to improve reasoning and physical plausibility. The model's reward design focuses on semantic alignment and reasoning coherence for understanding, while ensuring text-motion consistency for generation. Additionally, the authors introduce Chain-of-Motion (CoM) for step-by-step planning during inference, supported by two large-scale datasets for better alignment of motion sequences with reasoning and action descriptions."
                },
                "zh": {
                    "title": "ç»Ÿä¸€å¤šæ¨¡æ€è¿åŠ¨æ¨¡å‹æå‡äººç±»è¿åŠ¨ç†è§£ä¸ç”Ÿæˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€è¿åŠ¨æ¨¡å‹MoRLï¼Œè¯¥æ¨¡å‹é€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œæ—¨åœ¨æé«˜äººç±»è¿åŠ¨çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬è®¾è®¡äº†ç‰¹å®šä»»åŠ¡çš„å¥–åŠ±æœºåˆ¶ï¼Œç»“åˆäº†è¯­ä¹‰å¯¹é½å’Œæ¨ç†ä¸€è‡´æ€§ï¼Œä»¥å¢å¼ºç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶ç¡®ä¿ç”Ÿæˆçš„è¿åŠ¨åœ¨ç‰©ç†ä¸Šåˆç†ä¸”ä¸æ–‡æœ¬ä¸€è‡´ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†é€æ­¥è§„åˆ’å’Œåæ€çš„Chain-of-Motionï¼ˆCoMï¼‰æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoRLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14060",
            "title": "LM-Lexicon: Improving Definition Modeling via Harmonizing Semantic Experts",
            "url": "https://huggingface.co/papers/2602.14060",
            "abstract": "LM-Lexicon improves definition modeling through data clustering, semantic expert learning, and sparse mixture-of-experts architecture, achieving higher BLEU scores and better expert specialization.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce LM-Lexicon, an innovative definition modeling approach that incorporates data clustering, semantic expert learning, and model merging using a sparse mixture-of-experts architecture. By decomposing the definition modeling task into specialized semantic domains, where small language models are trained as domain experts, LM-Lexicon achieves substantial improvements (+7% BLEU score compared with the prior state-of-the-art model) over existing methods on five widely used benchmarks. Empirically, we demonstrate that 1) the clustering strategy enables fine-grained expert specialization with nearly 10% improvement in definition quality; 2) the semantic-aware domain-level routing mechanism achieves higher expert efficacy (+1%) than conventional token-level routing; and 3) further performance gains can be obtained through test-time compute and semantic expert scaling. Our work advances definition modeling while providing insights into the development of efficient language models for semantic-intensive applications.",
            "score": 2,
            "issue_id": 1094,
            "pub_date": "2026-02-15",
            "pub_date_card": {
                "ru": "15 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 15",
                "zh": "2æœˆ15æ—¥"
            },
            "hash": "d65dbe92a521f10f",
            "authors": [
                "Yang Liu",
                "Jiaye Yang",
                "Weikang Li",
                "Jiahui Liang",
                "Yang Li",
                "Lingyong Yan"
            ],
            "affiliations": [
                "BIGAI",
                "Baidu Inc.",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14060.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#small_models",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹",
                    "desc": "LM-Lexicon â€” ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ³Ğ´Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñƒ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 7% Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ BLEU Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Definition Modeling with Expert Specialization",
                    "desc": "LM-Lexicon is a new approach for modeling definitions that uses data clustering and a specialized architecture called sparse mixture-of-experts. This method trains smaller language models as experts in specific semantic areas, leading to better performance in generating definitions. The paper shows that this approach improves definition quality significantly, achieving a 7% increase in BLEU scores compared to previous models. Additionally, it highlights the benefits of clustering for expert specialization and a routing mechanism that enhances expert effectiveness."
                },
                "zh": {
                    "title": "LM-Lexiconï¼šå®šä¹‰å»ºæ¨¡çš„æ–°çªç ´",
                    "desc": "LM-Lexiconæ˜¯ä¸€ç§åˆ›æ–°çš„å®šä¹‰å»ºæ¨¡æ–¹æ³•ï¼Œç»“åˆäº†æ•°æ®èšç±»ã€è¯­ä¹‰ä¸“å®¶å­¦ä¹ å’Œç¨€ç–ä¸“å®¶æ··åˆæ¶æ„ã€‚é€šè¿‡å°†å®šä¹‰å»ºæ¨¡ä»»åŠ¡åˆ†è§£ä¸ºä¸“é—¨çš„è¯­ä¹‰é¢†åŸŸï¼Œå°å‹è¯­è¨€æ¨¡å‹è¢«è®­ç»ƒä¸ºé¢†åŸŸä¸“å®¶ï¼Œä»è€Œåœ¨äº”ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œèšç±»ç­–ç•¥ä½¿å¾—ä¸“å®¶ä¸“ä¸šåŒ–å¾—ä»¥ç»†åŒ–ï¼Œå®šä¹‰è´¨é‡æé«˜äº†è¿‘10%ã€‚æ­¤å¤–ï¼Œè¯­ä¹‰æ„ŸçŸ¥çš„é¢†åŸŸçº§è·¯ç”±æœºåˆ¶æ¯”ä¼ ç»Ÿçš„ä»¤ç‰Œçº§è·¯ç”±æé«˜äº†ä¸“å®¶çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09185",
            "title": "AIDev: Studying AI Coding Agents on GitHub",
            "url": "https://huggingface.co/papers/2602.09185",
            "abstract": "AIDev is a large-scale dataset of agent-authored pull requests from real-world GitHub repositories that captures AI coding agent usage in practical software development scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t AI coding agents are rapidly transforming software engineering by performing tasks such as feature development, debugging, and testing. Despite their growing impact, the research community lacks a comprehensive dataset capturing how these agents are used in real-world projects. To address this gap, we introduce AIDev, a large-scale dataset focused on agent-authored pull requests (Agentic-PRs) in real-world GitHub repositories. AIDev aggregates 932,791 Agentic-PRs produced by five agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These PRs span 116,211 repositories and involve 72,189 developers. In addition, AIDev includes a curated subset of 33,596 Agentic-PRs from 2,807 repositories with over 100 stars, providing further information such as comments, reviews, commits, and related issues. This dataset offers a foundation for future research on AI adoption, developer productivity, and human-AI collaboration in the new era of software engineering.   > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Agentic Software Engineering, Agentic Engineering",
            "score": 2,
            "issue_id": 1084,
            "pub_date": "2026-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "5c7714c0798e6fe5",
            "authors": [
                "Hao Li",
                "Haoxiang Zhang",
                "Ahmed E. Hassan"
            ],
            "affiliations": [
                "Queens University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09185.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#open_source",
                    "#agents",
                    "#dataset",
                    "#plp"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞºĞ¾Ğ´Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AIDev â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 932 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ pull requests, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ñ… Ğ½Ğ° GitHub. ĞĞ°Ğ±Ğ¾Ñ€ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ Ğ¿ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² (Codex, Devin, GitHub Copilot, Cursor Ğ¸ Claude Code) Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 116 Ñ‚Ñ‹ÑÑÑ‡ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ĞºĞ¾Ğ¼Ğ¼Ğ¸Ñ‚Ğ°Ñ…, ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ÑÑ…, Ñ€ĞµĞ²ÑŒÑ Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… issues Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ AI Ğ² Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ. Ğ­Ñ‚Ğ¾Ñ‚ Ñ€ĞµÑÑƒÑ€Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜, Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ¾Ğ¹ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking AI's Role in Software Development with AIDev",
                    "desc": "AIDev is a comprehensive dataset that focuses on pull requests authored by AI coding agents in real-world software development. It includes 932,791 Agentic-PRs from five different AI agents, showcasing their contributions across 116,211 GitHub repositories. This dataset not only highlights the usage of AI in coding tasks like feature development and debugging but also provides insights into developer interactions with these agents. AIDev serves as a valuable resource for studying AI's impact on software engineering, developer productivity, and collaboration between humans and AI."
                },
                "zh": {
                    "title": "AIDevï¼šAIç¼–ç ä»£ç†çš„çœŸå®ä¸–ç•Œæ•°æ®é›†",
                    "desc": "AIDevæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œä¸“æ³¨äºçœŸå®ä¸–ç•ŒGitHubä»“åº“ä¸­ç”±AIç¼–å†™çš„æ‹‰å–è¯·æ±‚ï¼ˆAgentic-PRsï¼‰ã€‚è¯¥æ•°æ®é›†æ±‡æ€»äº†932,791ä¸ªç”±äº”ä¸ªä¸åŒçš„AIç¼–ç ä»£ç†ç”Ÿæˆçš„æ‹‰å–è¯·æ±‚ï¼Œæ¶µç›–äº†116,211ä¸ªä»“åº“å’Œ72,189åå¼€å‘è€…ã€‚AIDevè¿˜åŒ…å«ä¸€ä¸ªç»è¿‡æ•´ç†çš„å­é›†ï¼Œæä¾›äº†æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œå¦‚è¯„è®ºã€å®¡æŸ¥ã€æäº¤å’Œç›¸å…³é—®é¢˜ã€‚è¿™ä¸€æ•°æ®é›†ä¸ºæœªæ¥å…³äºAIåº”ç”¨ã€å¼€å‘è€…ç”Ÿäº§åŠ›å’Œäººæœºåä½œçš„ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.15031",
            "title": "EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing",
            "url": "https://huggingface.co/papers/2602.15031",
            "abstract": "Efficient video inpainting framework that focuses computation on masked regions while maintaining global context consistency through a lightweight embedder.  \t\t\t\t\tAI-generated summary \t\t\t\t High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.",
            "score": 1,
            "issue_id": 1095,
            "pub_date": "2026-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "52b65be2df4c9168",
            "authors": [
                "Yehonathan Litman",
                "Shikun Liu",
                "Dario Seyb",
                "Nicholas Milef",
                "Yang Zhou",
                "Carl Marshall",
                "Shubham Tulsiani",
                "Caleb Leak"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Meta AI",
                "Meta Reality Labs"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.15031.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#inference",
                    "#architecture"
                ],
                "emoji": "âœ‚ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾-ÑÑ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° EditCtrl â€” ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ°Ğ´Ñ€Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñƒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¼ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¼ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 10-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ³Ğ¾."
                },
                "en": {
                    "title": "Efficient Video Inpainting: Focused Computation for Enhanced Editing",
                    "desc": "This paper presents EditCtrl, a novel framework for video inpainting that optimizes computational efficiency by concentrating resources on masked regions rather than the entire video. By utilizing a local video context module, the framework processes only the necessary parts of the video, making it ten times more efficient than existing methods. Additionally, a lightweight global context embedder maintains overall video consistency while minimizing computational overhead. The approach not only enhances editing quality but also introduces new functionalities like multi-region editing and autoregressive content propagation."
                },
                "zh": {
                    "title": "é«˜æ•ˆè§†é¢‘ä¿®å¤ï¼Œèšç„¦å…³é”®åŒºåŸŸ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„è§†é¢‘ä¿®å¤æ¡†æ¶EditCtrlï¼Œä¸“æ³¨äºåœ¨æ©ç åŒºåŸŸè¿›è¡Œè®¡ç®—ï¼ŒåŒæ—¶ä¿æŒå…¨å±€ä¸Šä¸‹æ–‡çš„ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸€ä¸ªè½»é‡çº§çš„æ—¶é—´å…¨å±€ä¸Šä¸‹æ–‡åµŒå…¥å™¨ï¼Œç¡®ä¿è§†é¢‘çš„æ•´ä½“ä¸€è‡´æ€§ï¼Œè®¡ç®—æˆæœ¬ä¸ç¼–è¾‘åŒºåŸŸçš„å¤§å°æˆæ­£æ¯”ã€‚ä¸ç°æœ‰çš„ç”Ÿæˆç¼–è¾‘æ–¹æ³•ç›¸æ¯”ï¼ŒEditCtrlçš„è®¡ç®—æ•ˆç‡æé«˜äº†10å€ï¼Œå¹¶ä¸”åœ¨ç¼–è¾‘è´¨é‡ä¸Šä¹Ÿæœ‰æ‰€æå‡ã€‚è¯¥æ¡†æ¶è¿˜æ”¯æŒå¤šåŒºåŸŸç¼–è¾‘å’Œè‡ªå›å½’å†…å®¹ä¼ æ’­ç­‰æ–°åŠŸèƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14941",
            "title": "AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories",
            "url": "https://huggingface.co/papers/2602.14941",
            "abstract": "AnchorWeave addresses long-term video generation consistency by replacing global 3D scene reconstruction with multiple local geometric memories and a multi-anchor weaving controller to reconcile cross-view inconsistencies.  \t\t\t\t\tAI-generated summary \t\t\t\t Maintaining spatial world consistency over long horizons remains a central challenge for camera-controllable video generation. Existing memory-based approaches often condition generation on globally reconstructed 3D scenes by rendering anchor videos from the reconstructed geometry in the history. However, reconstructing a global 3D scene from multiple views inevitably introduces cross-view misalignment, as pose and depth estimation errors cause the same surfaces to be reconstructed at slightly different 3D locations across views. When fused, these inconsistencies accumulate into noisy geometry that contaminates the conditioning signals and degrades generation quality. We introduce AnchorWeave, a memory-augmented video generation framework that replaces a single misaligned global memory with multiple clean local geometric memories and learns to reconcile their cross-view inconsistencies. To this end, AnchorWeave performs coverage-driven local memory retrieval aligned with the target trajectory and integrates the selected local memories through a multi-anchor weaving controller during generation. Extensive experiments demonstrate that AnchorWeave significantly improves long-term scene consistency while maintaining strong visual quality, with ablation and analysis studies further validating the effectiveness of local geometric conditioning, multi-anchor control, and coverage-driven retrieval.",
            "score": 1,
            "issue_id": 1095,
            "pub_date": "2026-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "b71ab76cdc1b6693",
            "authors": [
                "Zun Wang",
                "Han Lin",
                "Jaehong Yoon",
                "Jaemin Cho",
                "Yue Zhang",
                "Mohit Bansal"
            ],
            "affiliations": [
                "AI2",
                "Nanyang Technological University, Singapore",
                "University of North Carolina, Chapel Hill"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14941.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#3d",
                    "#multimodal",
                    "#long_context"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ…Ğ°Ğ¾ÑĞ°: ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞºĞ¾Ñ€Ğ½Ğ¾Ğµ Ğ¿Ğ»ĞµÑ‚ĞµĞ½Ğ¸Ğµ",
                    "desc": "AnchorWeave Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾sequences, Ğ·Ğ°Ğ¼ĞµĞ½ÑÑ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ 3D ÑÑ†ĞµĞ½Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ°Ğ¼ÑÑ‚ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞºĞ¾Ñ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ¸Ğ·-Ğ·Ğ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ·Ñ‹ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹, Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AnchorWeave Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ†ĞµĞ½Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾."
                },
                "en": {
                    "title": "Achieving Long-Term Video Consistency with Local Memories",
                    "desc": "AnchorWeave is a novel framework for generating videos that maintains consistency over long periods. It replaces the traditional method of using a single global 3D scene with multiple local geometric memories, which helps to avoid errors caused by cross-view misalignment. By using a multi-anchor weaving controller, it effectively reconciles inconsistencies between different views during video generation. Experiments show that AnchorWeave enhances scene consistency and visual quality compared to existing methods."
                },
                "zh": {
                    "title": "AnchorWeaveï¼šæå‡è§†é¢‘ç”Ÿæˆçš„ä¸€è‡´æ€§",
                    "desc": "AnchorWeave æ˜¯ä¸€ç§è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é•¿æœŸè§†é¢‘ç”Ÿæˆä¸­çš„ä¸€è‡´æ€§é—®é¢˜ã€‚å®ƒé€šè¿‡ä½¿ç”¨å¤šä¸ªå±€éƒ¨å‡ ä½•è®°å¿†æ›¿ä»£å•ä¸€çš„å…¨å±€3Dåœºæ™¯é‡å»ºï¼Œä»è€Œå‡å°‘è§†è§’é—´çš„ä¸ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šé”šç¼–ç»‡æ§åˆ¶å™¨æ¥æ•´åˆä¸åŒè§†è§’çš„å±€éƒ¨è®°å¿†ï¼Œç¡®ä¿ç”Ÿæˆçš„è§†é¢‘åœ¨ç©ºé—´ä¸Šä¿æŒä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAnchorWeave åœ¨ä¿æŒè§†è§‰è´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†é•¿æœŸåœºæ™¯çš„ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14689",
            "title": "Exposing the Systematic Vulnerability of Open-Weight Models to Prefill Attacks",
            "url": "https://huggingface.co/papers/2602.14689",
            "abstract": "Prefill attacks represent a significant and underexplored vulnerability in open-weight language models, affecting major contemporary models despite some resistance from large reasoning models.  \t\t\t\t\tAI-generated summary \t\t\t\t As the capabilities of large language models continue to advance, so does their potential for misuse. While closed-source models typically rely on external defenses, open-weight models must primarily depend on internal safeguards to mitigate harmful behavior. Prior red-teaming research has largely focused on input-based jailbreaking and parameter-level manipulations. However, open-weight models also natively support prefilling, which allows an attacker to predefine initial response tokens before generation begins. Despite its potential, this attack vector has received little systematic attention. We present the largest empirical study to date of prefill attacks, evaluating over 20 existing and novel strategies across multiple model families and state-of-the-art open-weight models. Our results show that prefill attacks are consistently effective against all major contemporary open-weight models, revealing a critical and previously underexplored vulnerability with significant implications for deployment. While certain large reasoning models exhibit some robustness against generic prefilling, they remain vulnerable to tailored, model-specific strategies. Our findings underscore the urgent need for model developers to prioritize defenses against prefill attacks in open-weight LLMs.",
            "score": 1,
            "issue_id": 1088,
            "pub_date": "2026-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "d64473e8857a332a",
            "authors": [
                "Lukas Struppek",
                "Adam Gleave",
                "Kellin Pelrine"
            ],
            "affiliations": [
                "FAR.AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14689.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#security"
                ],
                "emoji": "ğŸ”“",
                "ru": {
                    "title": "ĞŸÑ€ĞµÑ„Ğ¸Ğ»Ğ»Ğ¸Ğ½Ğ³-Ğ°Ñ‚Ğ°ĞºĞ¸: ÑĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ°Ñ Ñ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿ Ğ¿Ñ€ĞµÑ„Ğ¸Ğ»Ğ»Ğ¸Ğ½Ğ³Ğ° â€” ĞºĞ¾Ğ³Ğ´Ğ° Ğ·Ğ»Ğ¾ÑƒĞ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¸Ğº Ğ¿Ñ€ĞµĞ´Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ´Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ñ‡Ğ½Ñ‘Ñ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ 20 ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ°Ñ‚Ğ°ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ²ÑĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¥Ğ¾Ñ‚Ñ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµÑ„Ğ¸Ğ»Ğ»Ğ¸Ğ½Ğ³-Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼, Ğ¾Ğ½Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ñ‹ Ğ¿ĞµÑ€ĞµĞ´ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑĞ° Ğ°Ñ‚Ğ°Ğº Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM Ğ² production."
                },
                "en": {
                    "title": "Uncovering Vulnerabilities: The Threat of Prefill Attacks on Open-Weight Models",
                    "desc": "This paper investigates a new type of attack called prefill attacks on open-weight language models, which are models that allow users to see and modify their internal workings. The authors conducted a comprehensive study, testing over 20 different strategies for these attacks on various state-of-the-art models. They found that prefill attacks are effective against all major open-weight models, highlighting a serious vulnerability that has not been adequately addressed. The study emphasizes the importance of developing stronger defenses against these attacks to ensure the safe deployment of open-weight language models."
                },
                "zh": {
                    "title": "é¢„å¡«æ”»å‡»ï¼šå¼€æ”¾æƒé‡æ¨¡å‹çš„éšç§˜å¨èƒ",
                    "desc": "é¢„å¡«æ”»å‡»æ˜¯ä¸€ç§é‡è¦ä½†å°šæœªå……åˆ†ç ”ç©¶çš„æ¼æ´ï¼Œå½±å“å¼€æ”¾æƒé‡è¯­è¨€æ¨¡å‹ã€‚å°½ç®¡ä¸€äº›å¤§å‹æ¨ç†æ¨¡å‹å¯¹è¿™ç§æ”»å‡»æœ‰ä¸€å®šæŠµæŠ—åŠ›ï¼Œä½†å¼€æ”¾æƒé‡æ¨¡å‹ä¸»è¦ä¾èµ–å†…éƒ¨ä¿æŠ¤æªæ–½æ¥å‡è½»æœ‰å®³è¡Œä¸ºã€‚æˆ‘ä»¬è¿›è¡Œäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„é¢„å¡«æ”»å‡»å®è¯ç ”ç©¶ï¼Œè¯„ä¼°äº†20å¤šç§ç°æœ‰å’Œæ–°é¢–çš„ç­–ç•¥ï¼Œç»“æœæ˜¾ç¤ºé¢„å¡«æ”»å‡»å¯¹æ‰€æœ‰ä¸»è¦çš„å¼€æ”¾æƒé‡æ¨¡å‹éƒ½æœ‰æ•ˆã€‚è¿™ä¸€å‘ç°å¼ºè°ƒäº†æ¨¡å‹å¼€å‘è€…éœ€è¦ä¼˜å…ˆè€ƒè™‘å¯¹å¼€æ”¾æƒé‡å¤§è¯­è¨€æ¨¡å‹çš„é¢„å¡«æ”»å‡»é˜²å¾¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12586",
            "title": "Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models",
            "url": "https://huggingface.co/papers/2602.12586",
            "abstract": "McDiffuSE enhances Masked Diffusion Models by optimizing slot infilling order through Monte Carlo Tree Search, improving reasoning task performance through strategic exploration of generation sequences.  \t\t\t\t\tAI-generated summary \t\t\t\t While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.",
            "score": 1,
            "issue_id": 1093,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "8b9bf4b6d749f5be",
            "authors": [
                "Joshua Ong Jun Leang",
                "Yu Zhao",
                "Mihaela CÄƒtÄƒlina Stoian",
                "Wenda Li",
                "Shay B. Cohen",
                "Eleonora Giunchiglia"
            ],
            "affiliations": [
                "Imperial College London",
                "University of Edinburgh"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.12586.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#math",
                    "#diffusion",
                    "#reasoning",
                    "#optimization",
                    "#plp",
                    "#architecture"
                ],
                "emoji": "ğŸŒ³",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ McDiffuSE â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ look-ahead ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¸Ñ… Ñ„Ğ¸ĞºÑĞ°Ñ†Ğ¸ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 3.2% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ½Ğ° 8.0% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ plan-and-infill Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ½ĞµĞ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Optimizing Slot Infilling with McDiffuSE for Better Reasoning Performance",
                    "desc": "McDiffuSE is a framework that improves Masked Diffusion Models (MDMs) by optimizing the order in which slots are filled during generation tasks. It uses Monte Carlo Tree Search (MCTS) to strategically explore different sequences of slot infilling, which helps in making better decisions about the order of completions. This method leads to significant performance improvements in reasoning tasks, particularly in mathematical and code generation scenarios. The research shows that while following a sequential order is common, incorporating non-sequential strategies is crucial for achieving the best results."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ’æ§½å¡«å……é¡ºåºï¼Œæå‡æ¨ç†æ€§èƒ½",
                    "desc": "McDiffuSE æ˜¯ä¸€ç§å¢å¼ºæ©è”½æ‰©æ•£æ¨¡å‹ï¼ˆMDMsï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ä¼˜åŒ–æ’æ§½å¡«å……é¡ºåºï¼Œä»è€Œæé«˜æ¨ç†ä»»åŠ¡çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶å°†æ’æ§½é€‰æ‹©è§†ä¸ºå†³ç­–è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡å‰ç»æ€§æ¨¡æ‹Ÿè¯„ä¼°éƒ¨åˆ†å®Œæˆæƒ…å†µï¼Œç³»ç»Ÿæ€§åœ°æ¢ç´¢ç”Ÿæˆé¡ºåºçš„ç»„åˆç©ºé—´ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMcDiffuSE åœ¨è‡ªå›å½’åŸºçº¿æ¨¡å‹ä¸Šå¹³å‡æé«˜äº† 3.2%ï¼Œåœ¨åŸºçº¿è®¡åˆ’ä¸å¡«å……ä¸Šæé«˜äº† 8.0%ï¼Œåœ¨ MBPP å’Œ MATH500 æ•°æ®é›†ä¸Šåˆ†åˆ«å–å¾—äº† 19.5% å’Œ 4.9% çš„æ˜¾è‘—æå‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡ McDiffuSE ä¸»è¦éµå¾ªé¡ºåºç”Ÿæˆï¼Œä½†ç»“åˆéé¡ºåºç”Ÿæˆå¯¹äºæœ€å¤§åŒ–æ€§èƒ½è‡³å…³é‡è¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.13346",
            "title": "CellMaster: Collaborative Cell Type Annotation in Single-Cell Analysis",
            "url": "https://huggingface.co/papers/2602.13346",
            "abstract": "CellMaster uses LLM-encoded knowledge for zero-shot cell-type annotation in single-cell RNA sequencing, improving accuracy over existing tools through interpretable rationales without pre-training.  \t\t\t\t\tAI-generated summary \t\t\t\t Single-cell RNA-seq (scRNA-seq) enables atlas-scale profiling of complex tissues, revealing rare lineages and transient states. Yet, assigning biologically valid cell identities remains a bottleneck because markers are tissue- and state-dependent, and novel states lack references. We present CellMaster, an AI agent that mimics expert practice for zero-shot cell-type annotation. Unlike existing automated tools, CellMaster leverages LLM-encoded knowledge (e.g., GPT-4o) to perform on-the-fly annotation with interpretable rationales, without pre-training or fixed marker databases. Across 9 datasets spanning 8 tissues, CellMaster improved accuracy by 7.1% over best-performing baselines (including CellTypist and scTab) in automatic mode. With human-in-the-loop refinement, this advantage increased to 18.6%, with a 22.1% gain on subtype populations. The system demonstrates particular strength in rare and novel cell states where baselines often fail. Source code and the web application are available at https://github.com/AnonymousGym/CellMaster{https://github.com/AnonymousGym/CellMaster}.",
            "score": 1,
            "issue_id": 1100,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "9363ab334ac39604",
            "authors": [
                "Zhen Wang",
                "Yiming Gao",
                "Jieyuan Liu",
                "Enze Ma",
                "Jefferson Chen",
                "Mark Antkowiak",
                "Mengzhou Hu",
                "JungHo Kong",
                "Dexter Pratt",
                "Zhiting Hu",
                "Wei Wang",
                "Trey Ideker",
                "Eric P. Xing"
            ],
            "affiliations": [
                "Department of Chemistry and Biochemistry, University of California San Diego",
                "Department of Electrical & Computer Engineering, Texas A&M University",
                "Department of Medicine, University of California, San Diego",
                "Halicioglu Data Science Institute, University of California, San Diego",
                "Mohamed bin Zayed University of AI",
                "Moores Cancer Center, University of California, San Diego",
                "School of Computer Science, Carnegie Mellon University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.13346.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#interpretability",
                    "#science"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "LLM Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ĞºĞ»ĞµÑ‚Ğ¾Ğº Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "CellMaster Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ·Ğ°ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM), Ğ´Ğ»Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ĞºĞ»ĞµÑ‚Ğ¾Ğº Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ´Ğ½Ğ¾ĞºĞ»ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ RNA-ÑĞµĞºĞ²ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ zero-shot, Ñ‚Ğ¾ ĞµÑÑ‚ÑŒ Ğ±ĞµĞ· Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ€ĞºĞµÑ€Ğ¾Ğ², Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. ĞĞ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 9 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞºĞ°Ğ½ĞµĞ¹ CellMaster Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 7.1% Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ¸ Ğ½Ğ° 18.6% Ğ¿Ñ€Ğ¸ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² Ñ†Ğ¸ĞºĞ»Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ¾ÑĞ¾Ğ±ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ĞºĞ»ĞµÑ‚Ğ¾Ğº, Ğ³Ğ´Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ°ÑÑ‚ÑÑ."
                },
                "en": {
                    "title": "Revolutionizing Cell Annotation with AI: Meet CellMaster!",
                    "desc": "CellMaster is a novel AI tool designed for zero-shot cell-type annotation in single-cell RNA sequencing (scRNA-seq). It utilizes large language model (LLM) encoded knowledge to provide accurate cell identity assignments without the need for pre-training or fixed marker databases. This approach allows for interpretable rationales, making the annotation process more transparent and reliable. In tests across multiple datasets, CellMaster outperformed existing tools, especially in identifying rare and novel cell types, demonstrating significant improvements in accuracy."
                },
                "zh": {
                    "title": "CellMasterï¼šé›¶æ ·æœ¬ç»†èƒç±»å‹æ³¨é‡Šçš„æ–°çªç ´",
                    "desc": "CellMaster æ˜¯ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç¼–ç çŸ¥è¯†è¿›è¡Œå•ç»†èƒ RNA æµ‹åºçš„é›¶æ ·æœ¬ç»†èƒç±»å‹æ³¨é‡Šçš„å·¥å…·ã€‚å®ƒé€šè¿‡å¯è§£é‡Šçš„æ¨ç†æ¥æé«˜å‡†ç¡®æ€§ï¼Œé¿å…äº†ä¼ ç»Ÿå·¥å…·çš„é¢„è®­ç»ƒå’Œå›ºå®šæ ‡è®°æ•°æ®åº“çš„é™åˆ¶ã€‚è¯¥ç³»ç»Ÿåœ¨ 9 ä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè‡ªåŠ¨æ¨¡å¼ä¸‹å‡†ç¡®ç‡æ¯”æœ€ä½³åŸºçº¿æé«˜äº† 7.1%ã€‚åœ¨äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸€ä¼˜åŠ¿è¿›ä¸€æ­¥æå‡è‡³ 18.6%ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨€æœ‰å’Œæ–°å‹ç»†èƒçŠ¶æ€çš„è¯†åˆ«ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.11968",
            "title": "DHPLT: large-scale multilingual diachronic corpora and word representations for semantic change modelling",
            "url": "https://huggingface.co/papers/2602.11968",
            "abstract": "In this resource paper, we present DHPLT, an open collection of diachronic corpora in 41 diverse languages. DHPLT is based on the web-crawled HPLT datasets; we use web crawl timestamps as the approximate signal of document creation time. The collection covers three time periods: 2011-2015, 2020-2021 and 2024-present (1 million documents per time period for each language). We additionally provide pre-computed word type and token embeddings and lexical substitutions for our chosen target words, while at the same time leaving it open for the other researchers to come up with their own target words using the same datasets. DHPLT aims at filling in the current lack of multilingual diachronic corpora for semantic change modelling (beyond a dozen of high-resource languages). It opens the way for a variety of new experimental setups in this field. All the resources described in this paper are available at https://data.hplt-project.org/three/diachronic/, sorted by language.",
            "score": 1,
            "issue_id": 1094,
            "pub_date": "2026-02-12",
            "pub_date_card": {
                "ru": "12 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 12",
                "zh": "2æœˆ12æ—¥"
            },
            "hash": "a7fe3240de199176",
            "authors": [
                "Mariia Fedorova",
                "Andrey Kutuzov",
                "Khonzoda Umarova"
            ],
            "affiliations": [
                "Cornell University",
                "University of Oslo"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.11968.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#multilingual",
                    "#low_resource"
                ],
                "emoji": "ğŸ“š",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ñ…Ñ€Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ñ€Ğ¿ÑƒÑÑ‹ Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ DHPLT â€” Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ Ğ´Ğ¸Ğ°Ñ…Ñ€Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ² Ğ½Ğ° 41 ÑĞ·Ñ‹ĞºĞµ, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²ĞµĞ±-ĞºÑ€Ğ°ÑƒĞ»Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… HPLT Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ²ĞµĞ±-ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² ĞºĞ°Ğº ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞšĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸Ñ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ° (2011-2015, 2020-2021, 2024-Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞµ) Ñ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ ĞµÑÑƒÑ€Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ ÑĞ»Ğ¾Ğ² Ğ¸ Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°. DHPLT Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ñ…Ñ€Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Unlocking Multilingual Semantic Change with DHPLT",
                    "desc": "This paper introduces DHPLT, a comprehensive collection of diachronic corpora spanning 41 languages, designed to facilitate research in semantic change modeling. The datasets are derived from web crawls, utilizing timestamps to estimate the creation dates of documents across three distinct time periods. Each language includes one million documents per period, along with pre-computed word embeddings and lexical substitutions for selected target words. By providing these resources, DHPLT addresses the scarcity of multilingual diachronic corpora and encourages innovative experimental approaches in the study of language evolution."
                },
                "zh": {
                    "title": "DHPLTï¼šå¤šè¯­è¨€å†æ—¶è¯­æ–™åº“çš„å¼€åˆ›ä¹‹ä½œ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†DHPLTï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«41ç§å¤šæ ·è¯­è¨€çš„å†æ—¶è¯­æ–™åº“çš„å¼€æ”¾é›†åˆã€‚DHPLTåŸºäºç½‘ç»œçˆ¬å–çš„HPLTæ•°æ®é›†ï¼Œåˆ©ç”¨ç½‘é¡µçˆ¬å–æ—¶é—´ä½œä¸ºæ–‡æ¡£åˆ›å»ºæ—¶é—´çš„è¿‘ä¼¼ä¿¡å·ã€‚è¯¥é›†åˆè¦†ç›–äº†ä¸‰ä¸ªæ—¶é—´æ®µï¼š2011-2015å¹´ã€2020-2021å¹´å’Œ2024å¹´è‡³ä»Šï¼Œæ¯ç§è¯­è¨€æ¯ä¸ªæ—¶é—´æ®µåŒ…å«100ä¸‡ä¸ªæ–‡æ¡£ã€‚DHPLTæ—¨åœ¨å¡«è¡¥å½“å‰å¤šè¯­è¨€å†æ—¶è¯­æ–™åº“åœ¨è¯­ä¹‰å˜åŒ–å»ºæ¨¡æ–¹é¢çš„ä¸è¶³ï¼Œä¸ºè¯¥é¢†åŸŸçš„æ–°å®éªŒè®¾ç½®æä¾›äº†å¯èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.12299",
            "title": "Acoustivision Pro: An Open-Source Interactive Platform for Room Impulse Response Analysis and Acoustic Characterization",
            "url": "https://huggingface.co/papers/2602.12299",
            "abstract": "Room acoustics analysis plays a central role in architectural design, audio engineering, speech intelligibility assessment, and hearing research. Despite the availability of standardized metrics such as reverberation time, clarity, and speech transmission index, accessible tools that combine rigorous signal processing with intuitive visualization remain scarce. This paper presents AcoustiVision Pro, an open-source web-based platform for comprehensive room impulse response (RIR) analysis. The system computes twelve distinct acoustic parameters from uploaded or dataset-sourced RIRs, provides interactive 3D visualizations of early reflections, generates frequency-dependent decay characteristics through waterfall plots, and checks compliance against international standards including ANSI S12.60 and ISO 3382. We introduce the accompanying RIRMega and RIRMega Speech datasets hosted on Hugging Face, containing thousands of simulated room impulse responses with full metadata. The platform supports real-time auralization through FFT-based convolution, exports detailed PDF reports suitable for engineering documentation, and provides CSV data export for further analysis. We describe the mathematical foundations underlying each acoustic metric, detail the system architecture, and present preliminary case studies demonstrating the platform's utility across diverse application domains including classroom acoustics, healthcare facility design, and recording studio evaluation.",
            "score": 1,
            "issue_id": 1086,
            "pub_date": "2026-02-11",
            "pub_date_card": {
                "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 11",
                "zh": "2æœˆ11æ—¥"
            },
            "hash": "aeb9ba614bf820af",
            "authors": [
                "Mandip Goswami"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2602.12299.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#dataset",
                    "#science",
                    "#audio"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ°ĞºÑƒÑÑ‚Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²ĞµĞ±-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° AcoustiVision Pro â€” Ğ²ĞµĞ±-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ (RIR). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ´Ğ²ĞµĞ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ€ĞµĞ¼Ñ Ñ€ĞµĞ²ĞµÑ€Ğ±ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ½Ğ´ĞµĞºÑ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ñ€ĞµÑ‡Ğ¸, Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ°Ğ¼ ANSI Ğ¸ ISO. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ° Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ·Ğ²ÑƒĞºĞ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ FFT-ÑĞ²Ñ‘Ñ€Ñ‚ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° RIRMega Ğ½Ğ° Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ Hugging Face Ñ Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ°Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Room Acoustics Analysis with AcoustiVision Pro",
                    "desc": "This paper introduces AcoustiVision Pro, a web-based platform designed for analyzing room acoustics through room impulse response (RIR) data. It computes twelve acoustic parameters and offers interactive 3D visualizations, making it easier to understand complex acoustic phenomena. The platform also includes datasets like RIRMega for training and testing, and supports real-time auralization and detailed reporting. By combining rigorous signal processing with user-friendly tools, it aims to enhance architectural design and audio engineering practices."
                },
                "zh": {
                    "title": "æˆ¿é—´å£°å­¦åˆ†æçš„åˆ›æ–°å¹³å°",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†AcoustiVision Proï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„åŸºäºç½‘ç»œçš„å¹³å°ï¼Œç”¨äºå…¨é¢åˆ†ææˆ¿é—´è„‰å†²å“åº”ï¼ˆRIRï¼‰ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿä»ä¸Šä¼ çš„RIRæ•°æ®ä¸­è®¡ç®—å‡ºåäºŒç§ä¸åŒçš„å£°å­¦å‚æ•°ï¼Œå¹¶æä¾›æ—©æœŸåå°„çš„äº¤äº’å¼3Då¯è§†åŒ–ã€‚å®ƒè¿˜ç”Ÿæˆé¢‘ç‡ä¾èµ–çš„è¡°å‡ç‰¹æ€§ï¼Œå¹¶æ£€æŸ¥æ˜¯å¦ç¬¦åˆå›½é™…æ ‡å‡†ï¼Œå¦‚ANSI S12.60å’ŒISO 3382ã€‚æ­¤å¤–ï¼Œå¹³å°æ”¯æŒå®æ—¶å¬è§‰åŒ–ï¼Œèƒ½å¤Ÿå¯¼å‡ºè¯¦ç»†çš„PDFæŠ¥å‘Šå’ŒCSVæ•°æ®ï¼Œé€‚ç”¨äºå·¥ç¨‹æ–‡æ¡£å’Œè¿›ä¸€æ­¥åˆ†æã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.09319",
            "title": "Benchmarking Knowledge-Extraction Attack and Defense on Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2602.09319",
            "abstract": "A systematic benchmark for evaluating knowledge-extraction attacks on Retrieval-Augmented Generation systems is introduced, covering diverse attack and defense strategies across multiple retrieval and generation models with standardized evaluation protocols.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) has become a cornerstone of knowledge-intensive applications, including enterprise chatbots, healthcare assistants, and agentic memory management. However, recent studies show that knowledge-extraction attacks can recover sensitive knowledge-base content through maliciously crafted queries, raising serious concerns about intellectual property theft and privacy leakage. While prior work has explored individual attack and defense techniques, the research landscape remains fragmented, spanning heterogeneous retrieval embeddings, diverse generation models, and evaluations based on non-standardized metrics and inconsistent datasets. To address this gap, we introduce the first systematic benchmark for knowledge-extraction attacks on RAG systems. Our benchmark covers a broad spectrum of attack and defense strategies, representative retrieval embedding models, and both open- and closed-source generators, all evaluated under a unified experimental framework with standardized protocols across multiple datasets. By consolidating the experimental landscape and enabling reproducible, comparable evaluation, this benchmark provides actionable insights and a practical foundation for developing privacy-preserving RAG systems in the face of emerging knowledge extraction threats. Our code is available here.",
            "score": 1,
            "issue_id": 1089,
            "pub_date": "2026-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "6eda3fa4dbc4682b",
            "authors": [
                "Zhisheng Qi",
                "Utkarsh Sahu",
                "Li Ma",
                "Haoyu Han",
                "Ryan Rossi",
                "Franck Dernoncourt",
                "Mahantesh Halappanavar",
                "Nesreen Ahmed",
                "Yushun Dong",
                "Yue Zhao",
                "Yu Zhang",
                "Yu Wang"
            ],
            "affiliations": [
                "ACM",
                "Texas A&M University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.09319.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rag",
                    "#agents"
                ],
                "emoji": "ğŸ”“",
                "ru": {
                    "title": "Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° RAG ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ñ‚ Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ benchmark Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Retrieval-Augmented Generation (RAG). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹, Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ retrieval Ğ¸ generation Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ·Ğ»Ğ¾ÑƒĞ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ğ±Ğ°Ğ·Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ crafted Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ€Ğ¸ÑĞºĞ¸ ĞºÑ€Ğ°Ğ¶Ğ° Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑ‚ĞµÑ‡ĞµĞº Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Benchmark Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ privacy-preserving RAG ÑĞ¸ÑÑ‚ĞµĞ¼."
                },
                "en": {
                    "title": "Strengthening RAG Systems Against Knowledge-Extraction Attacks",
                    "desc": "This paper presents a comprehensive benchmark designed to evaluate knowledge-extraction attacks on Retrieval-Augmented Generation (RAG) systems. It addresses the vulnerabilities of RAG applications, such as chatbots and healthcare assistants, to attacks that can extract sensitive information through crafted queries. The benchmark includes a variety of attack and defense strategies, as well as different retrieval and generation models, all assessed using standardized evaluation protocols. By providing a unified framework for testing, this work aims to enhance the security and privacy of RAG systems against emerging threats."
                },
                "zh": {
                    "title": "æ„å»ºçŸ¥è¯†æå–æ”»å‡»çš„ç³»ç»ŸåŒ–è¯„ä¼°åŸºå‡†",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªç³»ç»ŸåŒ–çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°çŸ¥è¯†æå–æ”»å‡»å¯¹å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„å½±å“ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¶æ„æŸ¥è¯¢å¯ä»¥æ¢å¤æ•æ„ŸçŸ¥è¯†åº“å†…å®¹ï¼Œå¯¼è‡´çŸ¥è¯†äº§æƒç›—çªƒå’Œéšç§æ³„éœ²çš„ä¸¥é‡é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºçš„åŸºå‡†æ¶µç›–äº†å¤šç§æ”»å‡»å’Œé˜²å¾¡ç­–ç•¥ï¼Œä»£è¡¨æ€§çš„æ£€ç´¢åµŒå…¥æ¨¡å‹ï¼Œä»¥åŠå¼€æ”¾å’Œé—­æºç”Ÿæˆå™¨ï¼Œæ‰€æœ‰è¯„ä¼°éƒ½åœ¨ç»Ÿä¸€çš„å®éªŒæ¡†æ¶ä¸‹è¿›è¡Œã€‚é€šè¿‡æ•´åˆå®éªŒç¯å¢ƒå¹¶å®ç°å¯é‡å¤ã€å¯æ¯”è¾ƒçš„è¯„ä¼°ï¼Œè¯¥åŸºå‡†ä¸ºå¼€å‘ä¿æŠ¤éšç§çš„RAGç³»ç»Ÿæä¾›äº†å®ç”¨çš„åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.07673",
            "title": "Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation",
            "url": "https://huggingface.co/papers/2602.07673",
            "abstract": "LLM judges exhibit bias toward summaries similar to their own generation, with performance deteriorating as summary overlap with human references decreases across multiple model sizes and architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.",
            "score": 1,
            "issue_id": 1089,
            "pub_date": "2026-02-07",
            "pub_date_card": {
                "ru": "7 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 7",
                "zh": "2æœˆ7æ—¥"
            },
            "hash": "cb31bcc533a68763",
            "authors": [
                "Jiangnan Fang",
                "Cheng-Tse Liu",
                "Hanieh Deilamsalehy",
                "Nesreen K. Ahmed",
                "Puneet Mathur",
                "Nedim Lipka",
                "Franck Dernoncourt",
                "Ryan A. Rossi"
            ],
            "affiliations": [
                "Adobe Research",
                "Cisco Research",
                "Independent Researchers"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.07673.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#ethics",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ¡ÑƒĞ´ÑŒĞ¸-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°ÑÑ‚ ÑĞ²Ğ¾Ğ¸Ñ…: ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ LLM Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ€ĞµĞ·ÑĞ¼Ğµ",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ LLM-ÑÑƒĞ´ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ·ÑĞ¼Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°ÑÑ‚ Ñ€ĞµĞ·ÑĞ¼Ğµ, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ LLM, Ñ€ĞµĞ·ÑĞ¼Ğµ, Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ğ¼ Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ĞºĞ¾Ğ³Ğ´Ğ° ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² (Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµĞ¼Ğ°Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ ROUGE Ğ¸ BLEU) Ğ½Ğ¸Ğ·ĞºĞ°Ñ. Ğ­Ñ‚Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¾Ñ‚ 1 Ğ´Ğ¾ 12 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° LLM ĞºĞ°Ğº ÑÑƒĞ´ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ·ÑĞ¼Ğµ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "LLM Judges Prefer Their Own: A Bias in Summary Evaluation",
                    "desc": "This paper investigates the biases of large language model (LLM) judges when evaluating summaries. It reveals that LLM judges tend to favor summaries that are similar to their own outputs, particularly as the overlap with human-written summaries decreases. The study tests nine different LLMs, showing that their performance in judging summaries declines when the similarity, measured by metrics like ROUGE and BLEU, is low. The findings suggest that LLMs may need more sophisticated evaluation techniques rather than relying solely on direct comparisons to human summaries."
                },
                "zh": {
                    "title": "LLMè¯„åˆ¤è€…çš„åè§ä¸æ‘˜è¦é‡å åº¦çš„å…³ç³»",
                    "desc": "æœ¬ç ”ç©¶åˆ†æäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„åˆ¤è€…åœ¨æ‘˜è¦ç”Ÿæˆä»»åŠ¡ä¸­çš„åè§ã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“ç”Ÿæˆçš„æ‘˜è¦ä¸äººç±»å‚è€ƒæ‘˜è¦çš„é‡å åº¦é™ä½æ—¶ï¼ŒLLMè¯„åˆ¤è€…å¯¹å…¶ä»–LLMç”Ÿæˆçš„æ‘˜è¦çš„åå¥½ä¼šå¢åŠ ã€‚é€šè¿‡æµ‹è¯•ä¹ç§ä¸åŒå‚æ•°è§„æ¨¡çš„LLMï¼Œæˆ‘ä»¬å‘ç°è¿™ç§åè§åœ¨å¤§å¤šæ•°æ¨¡å‹ä¸­æ™®éå­˜åœ¨ï¼Œå¹¶ä¸”æ¨¡å‹åœ¨è¯„åˆ¤æ‘˜è¦æ—¶å³ä½¿é‡å åº¦æœ‰é™ä¹Ÿè¡¨ç°å‡ºå›°éš¾ã€‚è¿™è¡¨æ˜ï¼Œåœ¨æ‘˜è¦é¢†åŸŸä¸­ï¼ŒLLMä½œä¸ºè¯„åˆ¤è€…çš„è¯„ä¼°æ–¹æ³•éœ€è¦è¶…è¶Šç®€å•çš„æ¯”è¾ƒæŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14696",
            "title": "A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)",
            "url": "https://huggingface.co/papers/2602.14696",
            "abstract": "Targeted instruction selection for LLM fine-tuning can be improved by systematically analyzing data representation and selection algorithms, with gradient-based representations and greedy round-robin selection performing best at low budgets.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.",
            "score": 0,
            "issue_id": 1084,
            "pub_date": "2026-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "9e682694c1439d3a",
            "authors": [
                "Nihal V. Nayak",
                "Paula Rodriguez-Diaz",
                "Neha Hulkund",
                "Sara Beery",
                "David Alvarez-Melis"
            ],
            "affiliations": [
                "Harvard University",
                "Kempner Institute",
                "MIT"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14696.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#data"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ¦ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ°, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°Ñ… Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¶Ğ°Ğ´Ğ½Ñ‹Ğ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ¼ round-robin Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ°."
                },
                "en": {
                    "title": "Optimizing Instruction Selection for Effective LLM Fine-Tuning",
                    "desc": "This paper focuses on improving the process of selecting training instructions for fine-tuning large language models (LLMs). It systematically analyzes how data representation and selection algorithms impact the effectiveness of instruction selection. The authors find that using gradient-based representations along with a greedy round-robin selection method yields the best results, especially when working with limited resources. Their work clarifies existing methods and provides a framework for better data selection practices in LLM fine-tuning."
                },
                "zh": {
                    "title": "ä¼˜åŒ–LLMå¾®è°ƒçš„æŒ‡ä»¤é€‰æ‹©ç­–ç•¥",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŒ‡ä»¤å¾®è°ƒä¸­çš„ç›®æ ‡æŒ‡ä»¤é€‰æ‹©é—®é¢˜ã€‚æˆ‘ä»¬ç³»ç»Ÿåˆ†æäº†æ•°æ®è¡¨ç¤ºå’Œé€‰æ‹©ç®—æ³•ï¼Œå‘ç°åŸºäºæ¢¯åº¦çš„æ•°æ®è¡¨ç¤ºèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹ä¸åŒæ•°æ®é›†å’Œæ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ä½é¢„ç®—æƒ…å†µä¸‹ï¼Œç»“åˆè´ªå©ªè½®è¯¢é€‰æ‹©ç®—æ³•çš„æ¢¯åº¦è¡¨ç¤ºæ–¹æ³•è¡¨ç°æœ€ä½³ï¼Œä½†åœ¨é«˜é¢„ç®—æ—¶æ•ˆæœå‡å¼±ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºLLMå¾®è°ƒä¸­çš„æ•°æ®é€‰æ‹©æä¾›äº†æ¸…æ™°çš„æŒ‡å¯¼å’Œç†è®ºåŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.14560",
            "title": "Preliminary sonification of ENSO using traditional Javanese gamelan scales",
            "url": "https://huggingface.co/papers/2602.14560",
            "abstract": "Parameter-mapping sonification of ENSO data preserves dynamical signatures through acoustic phase space analysis, revealing distinct coupling regimes in traditional musical scales.  \t\t\t\t\tAI-generated summary \t\t\t\t Sonification -- the mapping of data to non-speech audio -- offers an underexplored channel for representing complex dynamical systems. We treat El NiÃ±o-Southern Oscillation (ENSO), a canonical example of low-dimensional climate chaos, as a test case for culturally-situated sonification evaluated through complex systems diagnostics. Using parameter-mapping sonification of the NiÃ±o 3.4 sea surface temperature anomaly index (1870--2024), we encode ENSO variability into two traditional Javanese gamelan pentatonic systems (pelog and slendro) across four composition strategies, then analyze the resulting audio as trajectories in a two-dimensional acoustic phase space. Recurrence-based diagnostics, convex hull geometry, and coupling analysis reveal that the sonification pipeline preserves key dynamical signatures: alternating modes produce the highest trajectory recurrence rates, echoing ENSO's quasi-periodicity; layered polyphonic modes explore the broadest phase space regions; and the two scale families induce qualitatively distinct coupling regimes between spectral brightness and energy -- predominantly anti-phase in pelog but near-independent in slendro. Phase space trajectory analysis provides a rigorous geometric framework for comparing sonification designs within a complex systems context. Perceptual validation remains necessary; we contribute the dynamical systems methodology for evaluating such mappings.",
            "score": 0,
            "issue_id": 1086,
            "pub_date": "2026-02-16",
            "pub_date_card": {
                "ru": "16 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 16",
                "zh": "2æœˆ16æ—¥"
            },
            "hash": "31a42ec97c70307b",
            "authors": [
                "Sandy H. S. Herho",
                "Rusmawan Suwarman",
                "Nurjanna J. Trilaksono",
                "Iwan P. Anwar",
                "Faiz R. Fajary"
            ],
            "affiliations": [
                "Bandung Institute of Technology (ITB)",
                "Ronin Institute for Independent Scholarship",
                "State University of New York (SUNY), Binghamton",
                "University of California, Riverside"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.14560.jpg",
            "data": {
                "categories": [
                    "#science"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "ĞœÑƒĞ·Ñ‹ĞºĞ° ĞºĞ»Ğ¸Ğ¼Ğ°Ñ‚Ğ°: ĞºĞ°Ğº Ğ·Ğ²ÑƒĞº Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ­Ğ»ÑŒ-ĞĞ¸Ğ½ÑŒĞ¾",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ ÑĞ¾Ğ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ â€” Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ·Ğ²ÑƒĞº â€” Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ»Ğ¸Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ­Ğ»ÑŒ-ĞĞ¸Ğ½ÑŒĞ¾ (ENSO). ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ÑĞ´Ñ‹ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ĞºĞµĞ°Ğ½Ğ° Ğ² Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ²Ğ°Ğ½ÑĞºĞ¸Ğµ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿ĞµĞ»Ğ¾Ğ³ Ğ¸ ÑĞ»ĞµĞ½Ğ´Ñ€Ğ¾. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ„Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, Ğ¾Ğ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ ĞºĞ»Ğ¸Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºĞ²Ğ°Ğ·Ğ¸Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Harmonizing Climate Chaos: Sonifying ENSO Dynamics",
                    "desc": "This paper explores how sonification, the process of converting data into sound, can effectively represent complex climate systems like the El NiÃ±o-Southern Oscillation (ENSO). By mapping ENSO data to traditional Javanese musical scales, the authors analyze the resulting audio to uncover patterns and dynamics inherent in the climate data. They employ techniques such as recurrence-based diagnostics and phase space analysis to demonstrate that different musical scales can reveal distinct coupling behaviors in the data. The study emphasizes the importance of evaluating sonification methods through rigorous mathematical frameworks to enhance our understanding of complex systems."
                },
                "zh": {
                    "title": "å£°åŒ–æ•°æ®ï¼Œæ­ç¤ºENSOåŠ¨æ€ç‰¹å¾",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†æ•°æ®å£°åŒ–ï¼ˆsonificationï¼‰åœ¨è¡¨ç¤ºå¤æ‚åŠ¨æ€ç³»ç»Ÿä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å„å°”å°¼è¯º-å—æ–¹æ¶›åŠ¨ï¼ˆENSOï¼‰æ•°æ®çš„ç ”ç©¶ã€‚é€šè¿‡å°†ENSOçš„æµ·è¡¨æ¸©åº¦å¼‚å¸¸æŒ‡æ•°æ˜ å°„åˆ°ä¼ ç»Ÿçš„çˆªå“‡äº”å£°éŸ³é˜¶ä¸­ï¼Œä½œè€…åˆ†æäº†å£°åŒ–ç»“æœåœ¨å£°å­¦ç›¸ç©ºé—´ä¸­çš„è½¨è¿¹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå£°åŒ–è¿‡ç¨‹èƒ½å¤Ÿä¿ç•™ENSOçš„å…³é”®åŠ¨æ€ç‰¹å¾ï¼Œå¹¶æ­ç¤ºäº†ä¸åŒéŸ³é˜¶ä¹‹é—´çš„è€¦åˆæ¨¡å¼ã€‚æœ€åï¼Œä½œè€…æå‡ºäº†ä¸€ç§å‡ ä½•æ¡†æ¶ï¼Œç”¨äºåœ¨å¤æ‚ç³»ç»ŸèƒŒæ™¯ä¸‹æ¯”è¾ƒå£°åŒ–è®¾è®¡ï¼Œå¹¶å¼ºè°ƒäº†æ„ŸçŸ¥éªŒè¯çš„é‡è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2602.13516",
            "title": "SPILLage: Agentic Oversharing on the Web",
            "url": "https://huggingface.co/papers/2602.13516",
            "abstract": "Web agents inadvertently disclose user information through both content and behavioral traces, with behavioral oversharing being more prevalent than content oversharing, and this issue persists despite mitigation efforts.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-powered agents are beginning to automate user's tasks across the open web, often with access to user resources such as emails and calendars. Unlike standard LLMs answering questions in a controlled ChatBot setting, web agents act \"in the wild\", interacting with third parties and leaving behind an action trace. Therefore, we ask the question: how do web agents handle user resources when accomplishing tasks on their behalf across live websites? In this paper, we formalize Natural Agentic Oversharing -- the unintentional disclosure of task-irrelevant user information through an agent trace of actions on the web. We introduce SPILLage, a framework that characterizes oversharing along two dimensions: channel (content vs. behavior) and directness (explicit vs. implicit). This taxonomy reveals a critical blind spot: while prior work focuses on text leakage, web agents also overshare behaviorally through clicks, scrolls, and navigation patterns that can be monitored. We benchmark 180 tasks on live e-commerce sites with ground-truth annotations separating task-relevant from task-irrelevant attributes. Across 1,080 runs spanning two agentic frameworks and three backbone LLMs, we demonstrate that oversharing is pervasive with behavioral oversharing dominates content oversharing by 5x. This effect persists -- and can even worsen -- under prompt-level mitigation. However, removing task-irrelevant information before execution improves task success by up to 17.9%, demonstrating that reducing oversharing improves task success. Our findings underscore that protecting privacy in web agents is a fundamental challenge, requiring a broader view of \"output\" that accounts for what agents do on the web, not just what they type. Our datasets and code are available at https://github.com/jrohsc/SPILLage.",
            "score": 0,
            "issue_id": 1094,
            "pub_date": "2026-02-13",
            "pub_date_card": {
                "ru": "13 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 13",
                "zh": "2æœˆ13æ—¥"
            },
            "hash": "b7be0ff3fd243f19",
            "authors": [
                "Jaechul Roh",
                "Eugene Bagdasarian",
                "Hamed Haddadi",
                "Ali Shahin Shamsabadi"
            ],
            "affiliations": [
                "Brave Software",
                "Imperial College London",
                "University of Massachusetts Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2602.13516.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#dataset",
                    "#leakage",
                    "#agents",
                    "#benchmark",
                    "#security"
                ],
                "emoji": "ğŸ•µï¸",
                "ru": {
                    "title": "Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ: ĞºĞ°Ğº Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ²Ñ‹Ğ´Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞºĞ²Ğ¾Ğ·ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ° Ğ½Ğµ ÑĞ»Ğ¾Ğ²Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ Â«Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑÂ» â€” ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ³Ğ»Ğ°ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑÑÑ‰ĞµĞ¹ÑÑ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ñ‡ĞµÑ€ĞµĞ· ÑĞ»ĞµĞ´Ñ‹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº SPILLage, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ´Ğ²ÑƒĞ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼: ĞºĞ°Ğ½Ğ°Ğ»Ñƒ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ (ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ) Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñƒ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ (ÑĞ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğ¹). ĞĞ° Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ 180 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 1080 Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… e-commerce ÑĞ°Ğ¹Ñ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ (ĞºĞ»Ğ¸ĞºĞ¸, Ğ¿Ñ€Ğ¾ĞºÑ€ÑƒÑ‚ĞºĞ°, Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ) Ğ¿Ñ€ĞµĞ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ ÑƒÑ‚ĞµÑ‡ĞºĞ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² 5 Ñ€Ğ°Ğ· Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ÑÑ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµÑ€ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑÑÑ‰ĞµĞ¹ÑÑ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 17.9%, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Understanding and Mitigating Oversharing by Web Agents",
                    "desc": "This paper discusses how web agents, which automate tasks for users, can unintentionally share private information through their actions online. It introduces the concept of Natural Agentic Oversharing, highlighting that behavioral oversharing, such as clicks and navigation patterns, is more common than content oversharing. The authors present a framework called SPILLage to categorize this oversharing and demonstrate its prevalence through experiments on e-commerce sites. They find that reducing unnecessary information sharing can significantly improve the success of tasks performed by these agents."
                },
                "zh": {
                    "title": "ä¿æŠ¤éšç§ï¼Œå‡å°‘ç½‘ç»œä»£ç†çš„è¿‡åº¦åˆ†äº«",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†ç½‘ç»œä»£ç†åœ¨æ‰§è¡Œç”¨æˆ·ä»»åŠ¡æ—¶ï¼Œå¦‚ä½•æ— æ„ä¸­æ³„éœ²ç”¨æˆ·ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯è¡Œä¸ºä¸Šçš„è¿‡åº¦åˆ†äº«ã€‚æˆ‘ä»¬æå‡ºäº†è‡ªç„¶ä»£ç†è¿‡åº¦åˆ†äº«çš„æ¦‚å¿µï¼Œå¹¶å¼•å…¥äº†SPILLageæ¡†æ¶æ¥åˆ†ç±»è¿™ç§è¿‡åº¦åˆ†äº«ï¼Œåˆ†ä¸ºå†…å®¹ä¸è¡Œä¸ºä¸¤ä¸ªç»´åº¦ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¡Œä¸ºä¸Šçš„è¿‡åº¦åˆ†äº«æ¯”å†…å®¹ä¸Šçš„å¤šå‡ºäº”å€ï¼Œä¸”è¿™ç§ç°è±¡åœ¨ä½¿ç”¨æç¤ºçº§åˆ«çš„ç¼“è§£æªæ–½æ—¶ä»ç„¶å­˜åœ¨ã€‚é€šè¿‡å»é™¤ä¸ä»»åŠ¡æ— å…³çš„ä¿¡æ¯ï¼Œå¯ä»¥æé«˜ä»»åŠ¡æˆåŠŸç‡ï¼Œå¼ºè°ƒäº†åœ¨ç½‘ç»œä»£ç†ä¸­ä¿æŠ¤éšç§çš„é‡è¦æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2026-02-16.html",
    "link_next": "2026-02-18.html",
    "link_month": "2026-02.html",
    "short_date_prev": {
        "ru": "16.02",
        "en": "02/16",
        "zh": "2æœˆ16æ—¥"
    },
    "short_date_next": {
        "ru": "18.02",
        "en": "02/18",
        "zh": "2æœˆ18æ—¥"
    },
    "categories": {
        "#dataset": 12,
        "#data": 2,
        "#benchmark": 14,
        "#agents": 10,
        "#cv": 4,
        "#rl": 7,
        "#rlhf": 3,
        "#rag": 2,
        "#plp": 3,
        "#inference": 3,
        "#3d": 1,
        "#audio": 1,
        "#video": 2,
        "#multimodal": 13,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 8,
        "#healthcare": 0,
        "#training": 12,
        "#robotics": 1,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 3,
        "#reasoning": 12,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 2,
        "#optimization": 10,
        "#survey": 0,
        "#diffusion": 4,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 4,
        "#synthetic": 5,
        "#machine_translation": 0,
        "#leakage": 1,
        "#open_source": 17,
        "#small_models": 3,
        "#science": 6,
        "#low_resource": 1
    }
}