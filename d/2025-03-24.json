{
    "date": {
        "ru": "24 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        "en": "March 24",
        "zh": "3æœˆ24æ—¥"
    },
    "time_utc": "2025-03-24 00:52",
    "weekday": 0,
    "issue_id": 2851,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2503.13358",
            "title": "One-Step Residual Shifting Diffusion for Image Super-Resolution via\n  Distillation",
            "url": "https://huggingface.co/papers/2503.13358",
            "abstract": "Diffusion models for super-resolution (SR) produce high-quality visual results but require expensive computational costs. Despite the development of several methods to accelerate diffusion-based SR models, some (e.g., SinSR) fail to produce realistic perceptual details, while others (e.g., OSEDiff) may hallucinate non-existent structures. To overcome these issues, we present RSD, a new distillation method for ResShift, one of the top diffusion-based SR models. Our method is based on training the student network to produce such images that a new fake ResShift model trained on them will coincide with the teacher model. RSD achieves single-step restoration and outperforms the teacher by a large margin. We show that our distillation method can surpass the other distillation-based method for ResShift - SinSR - making it on par with state-of-the-art diffusion-based SR distillation methods. Compared to SR methods based on pre-trained text-to-image models, RSD produces competitive perceptual quality, provides images with better alignment to degraded input images, and requires fewer parameters and GPU memory. We provide experimental results on various real-world and synthetic datasets, including RealSR, RealSet65, DRealSR, ImageNet, and DIV2K.",
            "score": 83,
            "issue_id": 2830,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "7c30d8250da0c284",
            "authors": [
                "Daniil Selikhanovych",
                "David Li",
                "Aleksei Leonov",
                "Nikita Gushchin",
                "Sergei Kushneriuk",
                "Alexander Filippov",
                "Evgeny Burnaev",
                "Iaroslav Koshelev",
                "Alexander Korotin"
            ],
            "affiliations": [
                "AI Foundation and Algorithm Lab",
                "AIRI",
                "HSE University",
                "MIPT",
                "Skoltech",
                "Yandex Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13358.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#hallucinations",
                    "#training",
                    "#diffusion",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "RSD: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RSD. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‡ĞµÑĞºÑƒÑ ÑĞµÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ„ĞµĞ¹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ResShift, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ½Ğ¸Ñ…, ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°Ğ»Ğ° Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. RSD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, RSD Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU."
                },
                "en": {
                    "title": "RSD: Revolutionizing Super-Resolution with Efficient Distillation",
                    "desc": "This paper introduces RSD, a novel distillation method designed to enhance the performance of ResShift, a leading diffusion model for super-resolution (SR). RSD trains a student network to generate images that align closely with those produced by a teacher model, ensuring high-quality visual results while reducing computational costs. The method demonstrates significant improvements over existing techniques, such as SinSR, by maintaining realistic perceptual details and avoiding the hallucination of non-existent structures. Experimental results show that RSD not only matches but often exceeds the performance of state-of-the-art diffusion-based SR methods, while also being more efficient in terms of parameters and GPU memory usage."
                },
                "zh": {
                    "title": "RSDï¼šè¶…åˆ†è¾¨ç‡çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è’¸é¦æ–¹æ³•RSDï¼Œç”¨äºæ”¹è¿›åŸºäºæ‰©æ•£æ¨¡å‹çš„è¶…åˆ†è¾¨ç‡(SR)æŠ€æœ¯ã€‚RSDé€šè¿‡è®­ç»ƒå­¦ç”Ÿç½‘ç»œç”Ÿæˆå›¾åƒï¼Œä½¿å¾—ä¸€ä¸ªæ–°çš„å‡ResShiftæ¨¡å‹åœ¨è¿™äº›å›¾åƒä¸Šä¸æ•™å¸ˆæ¨¡å‹ä¸€è‡´ï¼Œä»è€Œå®ç°å•æ­¥æ¢å¤ã€‚ä¸å…¶ä»–è’¸é¦æ–¹æ³•ç›¸æ¯”ï¼ŒRSDåœ¨æ„ŸçŸ¥è´¨é‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åœ¨å‚æ•°å’ŒGPUå†…å­˜éœ€æ±‚ä¸Šæ›´å…·ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRSDåœ¨å¤šä¸ªçœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16416",
            "title": "Survey on Evaluation of LLM-based Agents",
            "url": "https://huggingface.co/papers/2503.16416",
            "abstract": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments. This paper provides the first comprehensive survey of evaluation methodologies for these increasingly capable agents. We systematically analyze evaluation benchmarks and frameworks across four critical dimensions: (1) fundamental agent capabilities, including planning, tool use, self-reflection, and memory; (2) application-specific benchmarks for web, software engineering, scientific, and conversational agents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating agents. Our analysis reveals emerging trends, including a shift toward more realistic, challenging evaluations with continuously updated benchmarks. We also identify critical gaps that future research must address-particularly in assessing cost-efficiency, safety, and robustness, and in developing fine-grained, and scalable evaluation methods. This survey maps the rapidly evolving landscape of agent evaluation, reveals the emerging trends in the field, identifies current limitations, and proposes directions for future research.",
            "score": 62,
            "issue_id": 2828,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "8d23d348a6ecdbce",
            "authors": [
                "Asaf Yehudai",
                "Lilach Eden",
                "Alan Li",
                "Guy Uziel",
                "Yilun Zhao",
                "Roy Bar-Haim",
                "Arman Cohan",
                "Michal Shmueli-Scheuer"
            ],
            "affiliations": [
                "IBM Research",
                "The Hebrew University of Jerusalem",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16416.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#survey",
                    "#reasoning",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼: Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸, Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ñ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM."
                },
                "en": {
                    "title": "Evaluating the Future of LLM Agents: Trends and Gaps",
                    "desc": "This paper surveys the evaluation methods for large language model (LLM)-based agents, which are AI systems capable of planning, reasoning, and interacting with their environments. It categorizes evaluation benchmarks into four key areas: fundamental capabilities, application-specific benchmarks, generalist agent benchmarks, and evaluation frameworks. The authors highlight a trend towards more realistic evaluations that adapt over time, while also pointing out significant gaps in current research, such as the need for better assessments of cost-efficiency and safety. The paper aims to provide a comprehensive overview of the evaluation landscape and suggest future research directions to enhance agent assessment."
                },
                "zh": {
                    "title": "æ™ºèƒ½ä½“è¯„ä¼°çš„æ–°è§†é‡",
                    "desc": "æœ¬è®ºæ–‡å¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“è¯„ä¼°æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ã€‚æˆ‘ä»¬åˆ†æäº†æ™ºèƒ½ä½“èƒ½åŠ›ã€åº”ç”¨ç‰¹å®šåŸºå‡†ã€é€šç”¨æ™ºèƒ½ä½“åŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ç­‰å››ä¸ªå…³é”®ç»´åº¦ã€‚ç ”ç©¶å‘ç°ï¼Œè¯„ä¼°æ–¹æ³•æ­£æœç€æ›´çœŸå®å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æ–¹å‘å‘å±•ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†åœ¨æˆæœ¬æ•ˆç‡ã€å®‰å…¨æ€§å’Œé²æ£’æ€§è¯„ä¼°æ–¹é¢çš„ç ”ç©¶ç©ºç™½ã€‚è¯¥è°ƒæŸ¥ä¸ºæ™ºèƒ½ä½“è¯„ä¼°çš„å¿«é€Ÿå‘å±•æä¾›äº†åœ°å›¾ï¼Œæ­ç¤ºäº†é¢†åŸŸä¸­çš„æ–°å…´è¶‹åŠ¿å’Œæœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16419",
            "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models",
            "url": "https://huggingface.co/papers/2503.16419",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the \"overthinking phenomenon\". In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: (1) model-based efficient reasoning, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; (2) reasoning output-based efficient reasoning, which aims to dynamically reduce reasoning steps and length during inference; (3) input prompts-based efficient reasoning, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking.",
            "score": 54,
            "issue_id": 2824,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "dc9b04a227f74af7",
            "authors": [
                "Yang Sui",
                "Yu-Neng Chuang",
                "Guanchu Wang",
                "Jiamu Zhang",
                "Tianyi Zhang",
                "Jiayi Yuan",
                "Hongyi Liu",
                "Andrew Wen",
                "Shaochen",
                "Zhong",
                "Hanjie Chen",
                "Xia Hu"
            ],
            "affiliations": [
                "Department of Computer Science, Rice University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16419.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#optimization",
                    "#reasoning",
                    "#rl",
                    "#dataset",
                    "#survey",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² LLM: Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ°Ğ¼Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¾Ğ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸Ğ·-Ğ·Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Optimizing Reasoning Efficiency in Large Language Models",
                    "desc": "This paper surveys the advancements in Large Reasoning Models (LRMs) that enhance reasoning capabilities in complex tasks like mathematics and programming. It highlights the use of supervised fine-tuning and reinforcement learning to improve Chain-of-Thought (CoT) reasoning, while addressing the computational challenges posed by longer reasoning sequences. The authors categorize existing approaches into three main strategies: optimizing reasoning models, reducing reasoning output length, and improving input prompts for efficiency. Additionally, the paper discusses the potential of small language models and introduces methods for evaluating reasoning performance."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ¨ç†ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ¢ç´¢",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­å±•ç°äº†å“è¶Šçš„èƒ½åŠ›ã€‚æœ€è¿‘ï¼Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„è¿›å±•è¿›ä¸€æ­¥æå‡äº†åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰ç³»ç»Ÿ-2æ¨ç†é¢†åŸŸçš„è¡¨ç°ï¼Œåˆ©ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æŠ€æœ¯æ¥å¢å¼ºæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ã€‚ç„¶è€Œï¼Œè¾ƒé•¿çš„CoTæ¨ç†åºåˆ—è™½ç„¶æé«˜äº†æ€§èƒ½ï¼Œå´ä¹Ÿå¸¦æ¥äº†æ˜¾è‘—çš„è®¡ç®—å¼€é”€ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºâ€œè¿‡åº¦æ€è€ƒç°è±¡â€ã€‚æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿæ€§åœ°è°ƒæŸ¥å’Œæ¢ç´¢äº†å®ç°LLMsé«˜æ•ˆæ¨ç†çš„å½“å‰è¿›å±•ï¼Œå¹¶å°†ç°æœ‰å·¥ä½œåˆ†ç±»ä¸ºå¤šä¸ªå…³é”®æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16302",
            "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
            "url": "https://huggingface.co/papers/2503.16302",
            "abstract": "3D shape generation has greatly flourished through the development of so-called \"native\" 3D diffusion, particularly through the Vecset Diffusion Model (VDM). While recent advancements have shown promising results in generating high-resolution 3D shapes, VDM still struggles with high-speed generation. Challenges exist because of difficulties not only in accelerating diffusion sampling but also VAE decoding in VDM, areas under-explored in previous works. To address these challenges, we present FlashVDM, a systematic framework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables flexible diffusion sampling with as few as 5 inference steps and comparable quality, which is made possible by stabilizing consistency distillation with our newly introduced Progressive Flow Distillation. For VAE, we introduce a lightning vecset decoder equipped with Adaptive KV Selection, Hierarchical Volume Decoding, and Efficient Network Design. By exploiting the locality of the vecset and the sparsity of shape surface in the volume, our decoder drastically lowers FLOPs, minimizing the overall decoding overhead. We apply FlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic evaluation, we show that our model significantly outperforms existing fast 3D generation methods, achieving comparable performance to the state-of-the-art while reducing inference time by over 45x for reconstruction and 32x for generation. Code and models are available at https://github.com/Tencent/FlashVDM.",
            "score": 37,
            "issue_id": 2824,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "d91e862542d76892",
            "authors": [
                "Zeqiang Lai",
                "Yunfei Zhao",
                "Zibo Zhao",
                "Haolin Liu",
                "Fuyun Wang",
                "Huiwen Shi",
                "Xianghui Yang",
                "Qinxiang Lin",
                "Jinwei Huang",
                "Yuhong Liu",
                "Jie Jiang",
                "Chunchao Guo",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "MMLab, CUHK",
                "ShanghaiTech",
                "Tencent Hunyuan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16302.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#3d",
                    "#diffusion",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "ĞœĞ¾Ğ»Ğ½Ğ¸ĞµĞ½Ğ¾ÑĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ñ„Ğ¾Ñ€Ğ¼ Ñ FlashVDM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FlashVDM - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ñ„Ğ¾Ñ€Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VDM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ VAE, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´ Progressive Flow Distillation Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ². FlashVDM Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D-Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 5 ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ€ĞµĞ¼Ñ inference. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ FlashVDM Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 32-45 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ state-of-the-art."
                },
                "en": {
                    "title": "Accelerating 3D Shape Generation with FlashVDM",
                    "desc": "This paper introduces FlashVDM, a new framework designed to enhance the speed of 3D shape generation using the Vecset Diffusion Model (VDM). It addresses the slow generation times associated with VDM by improving both the Variational Autoencoder (VAE) and the Diffusion Transformer (DiT) components. FlashVDM achieves faster diffusion sampling with fewer inference steps while maintaining high-quality outputs through a technique called Progressive Flow Distillation. Additionally, it optimizes the VAE with a new decoder that reduces computational load, resulting in significant improvements in inference speed without sacrificing performance."
                },
                "zh": {
                    "title": "åŠ é€Ÿ3Då½¢çŠ¶ç”Ÿæˆçš„FlashVDMæ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFlashVDMçš„æ¡†æ¶ï¼Œæ—¨åœ¨åŠ é€Ÿ3Då½¢çŠ¶ç”Ÿæˆä¸­çš„VAEå’ŒDiTè¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥æ¸è¿›æµè’¸é¦ï¼ŒFlashVDMå®ç°äº†çµæ´»çš„æ‰©æ•£é‡‡æ ·ï¼Œä»…éœ€5ä¸ªæ¨ç†æ­¥éª¤å³å¯è·å¾—ä¸ç°æœ‰æ–¹æ³•ç›¸å½“çš„è´¨é‡ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§é—ªç”µvecsetè§£ç å™¨ï¼Œåˆ©ç”¨è‡ªé€‚åº”KVé€‰æ‹©å’Œåˆ†å±‚ä½“ç§¯è§£ç ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFlashVDMåœ¨é‡å»ºå’Œç”Ÿæˆæ–¹é¢çš„æ¨ç†æ—¶é—´åˆ†åˆ«å‡å°‘äº†è¶…è¿‡45å€å’Œ32å€ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¿«é€Ÿ3Dç”Ÿæˆæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16219",
            "title": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What\n  Doesn't",
            "url": "https://huggingface.co/papers/2503.16219",
            "abstract": "Enhancing the reasoning capabilities of large language models (LLMs) typically relies on massive computational resources and extensive datasets, limiting accessibility for resource-constrained settings. Our study investigates the potential of reinforcement learning (RL) to improve reasoning in small LLMs, focusing on a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA A40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy Optimization (GRPO) algorithm and curating a compact, high-quality mathematical reasoning dataset, we conducted three experiments to explore model behavior and performance. Our results demonstrate rapid reasoning gains - e.g., AMC23 accuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing o1-preview - using only 7,000 samples and a $42 training cost, compared to thousands of dollars for baseline models. However, challenges such as optimization instability and length constraints emerged with prolonged training. These findings highlight the efficacy of RL-based fine-tuning for small LLMs, offering a cost-effective alternative to large-scale approaches. We release our code and datasets as open-source resources, providing insights into trade-offs and laying a foundation for scalable, reasoning-capable LLMs in resource-limited environments. All are available at https://github.com/knoveleng/open-rs.",
            "score": 31,
            "issue_id": 2829,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "b9603e3e14ebf85d",
            "authors": [
                "Quy-Anh Dang",
                "Chris Ngo"
            ],
            "affiliations": [
                "Knovel Engineering Lab, Singapore",
                "VNU University of Science, Vietnam"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16219.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#dataset",
                    "#small_models",
                    "#reasoning",
                    "#open_source",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RL",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DeepSeek-R1-Distill-Qwen-1.5B Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ GRPO Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ´Ğ¾Ğ±Ğ¸Ğ»Ğ¸ÑÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Reinforcement Learning: A Cost-Effective Boost for Small Language Models!",
                    "desc": "This paper explores how reinforcement learning (RL) can enhance the reasoning abilities of smaller language models, specifically a 1.5-billion-parameter model called DeepSeek-R1-Distill-Qwen-1.5B. The researchers used a modified Group Relative Policy Optimization (GRPO) algorithm and a carefully curated dataset to train the model efficiently on limited hardware. Their experiments showed significant improvements in reasoning accuracy, achieving notable performance gains with minimal training resources. The study emphasizes the potential of RL for fine-tuning smaller models, making advanced reasoning capabilities more accessible in resource-constrained settings."
                },
                "zh": {
                    "title": "å°å‹LLMsçš„æ¨ç†èƒ½åŠ›æå‡æ–°æ–¹æ¡ˆ",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå‡å°å‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåŒ…å«15äº¿å‚æ•°çš„æ¨¡å‹ï¼Œå¹¶åœ¨ä¸¥æ ¼çš„èµ„æºé™åˆ¶ä¸‹è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨4ä¸ªNVIDIA A40 GPUï¼Œåœ¨24å°æ—¶å†…å®Œæˆã€‚é€šè¿‡é€‚åº”ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ï¼Œå¹¶åˆ›å»ºé«˜è´¨é‡çš„æ•°å­¦æ¨ç†æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„å®éªŒæ˜¾ç¤ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ˜¾è‘—æé«˜ï¼Œè®­ç»ƒæˆæœ¬ä¹Ÿå¤§å¹…é™ä½ã€‚å°½ç®¡åœ¨é•¿æ—¶é—´è®­ç»ƒä¸­å‡ºç°äº†ä¼˜åŒ–ä¸ç¨³å®šå’Œé•¿åº¦é™åˆ¶ç­‰æŒ‘æˆ˜ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶ä¸ºèµ„æºæœ‰é™ç¯å¢ƒä¸­çš„å°å‹LLMsæä¾›äº†æœ‰æ•ˆçš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15299",
            "title": "Inside-Out: Hidden Factual Knowledge in LLMs",
            "url": "https://huggingface.co/papers/2503.15299",
            "abstract": "This work presents a framework for assessing whether large language models (LLMs) encode more factual knowledge in their parameters than what they express in their outputs. While a few studies hint at this possibility, none has clearly defined or demonstrated this phenomenon. We first propose a formal definition of knowledge, quantifying it for a given question as the fraction of correct-incorrect answer pairs where the correct one is ranked higher. This gives rise to external and internal knowledge, depending on the information used to score individual answer candidates: either the model's observable token-level probabilities or its intermediate computations. Hidden knowledge arises when internal knowledge exceeds external knowledge. We then present a case study, applying this framework to three popular open-weights LLMs in a closed-book QA setup. Our results indicate that: (1) LLMs consistently encode more factual knowledge internally than what they express externally, with an average gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a model can internally know an answer perfectly, yet fail to generate it even once, despite large-scale repeated sampling of 1,000 answers. This reveals fundamental limitations in the generation capabilities of LLMs, which (3) puts a practical constraint on scaling test-time compute via repeated answer sampling in closed-book QA: significant performance improvements remain inaccessible because some answers are practically never sampled, yet if they were, we would be guaranteed to rank them first.",
            "score": 31,
            "issue_id": 2833,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 19",
                "zh": "3æœˆ19æ—¥"
            },
            "hash": "2ca2246fe2ad8d73",
            "authors": [
                "Zorik Gekhman",
                "Eyal Ben David",
                "Hadas Orgad",
                "Eran Ofek",
                "Yonatan Belinkov",
                "Idan Szpector",
                "Jonathan Herzig",
                "Roi Reichart"
            ],
            "affiliations": [
                "Google Research",
                "Technion - Israel Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15299.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#multimodal",
                    "#data",
                    "#benchmark",
                    "#interpretability"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ±Ğ¾Ğ»ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ ĞºĞ°Ğ¶ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ Ğ²Ğ½ĞµÑˆĞ½ĞµĞµ Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ LLM ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 40% Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğµ, Ñ‡ĞµĞ¼ Ğ²Ñ‹Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ Ğ²Ğ½ĞµÑˆĞ½Ğµ. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°ÑÑ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞºÑ€Ñ‹Ñ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğµ Ğ·Ğ½Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ Ğ½Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Unveiling Hidden Knowledge in Language Models",
                    "desc": "This paper introduces a framework to evaluate the factual knowledge embedded in large language models (LLMs) compared to what they actually produce in their outputs. It defines knowledge in a quantifiable way, distinguishing between external knowledge (observable outputs) and internal knowledge (hidden computations). The study reveals that LLMs often possess more internal knowledge than they express, with a notable gap of 40% on average. Additionally, it highlights that some knowledge is so well-hidden that models can know the correct answer internally but fail to generate it, indicating limitations in their generation capabilities and affecting performance in closed-book question answering."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„éšè—çŸ¥è¯†",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å…¶å‚æ•°ä¸­ç¼–ç çš„äº‹å®çŸ¥è¯†æ˜¯å¦è¶…è¿‡å…¶è¾“å‡ºä¸­è¡¨è¾¾çš„çŸ¥è¯†ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºäº†çŸ¥è¯†çš„æ­£å¼å®šä¹‰ï¼Œé€šè¿‡æ­£ç¡®-é”™è¯¯ç­”æ¡ˆå¯¹çš„æ¯”ä¾‹æ¥é‡åŒ–çŸ¥è¯†ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMså†…éƒ¨ç¼–ç çš„äº‹å®çŸ¥è¯†é€šå¸¸æ¯”å¤–éƒ¨è¡¨è¾¾çš„å¤šï¼Œå¹³å‡å·®è·è¾¾åˆ°40%ã€‚æ­¤å¤–ï¼ŒæŸäº›çŸ¥è¯†æ·±è—ä¸éœ²ï¼Œæ¨¡å‹å¯èƒ½å†…éƒ¨å®Œå…¨çŸ¥é“ç­”æ¡ˆï¼Œä½†åœ¨å¤šæ¬¡é‡‡æ ·ä¸­å´ä»æœªç”Ÿæˆï¼Œè¿™æ­ç¤ºäº†LLMsç”Ÿæˆèƒ½åŠ›çš„åŸºæœ¬å±€é™æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16397",
            "title": "Scale-wise Distillation of Diffusion Models",
            "url": "https://huggingface.co/papers/2503.16397",
            "abstract": "We present SwD, a scale-wise distillation framework for diffusion models (DMs), which effectively employs next-scale prediction ideas for diffusion-based few-step generators. In more detail, SwD is inspired by the recent insights relating diffusion processes to the implicit spectral autoregression. We suppose that DMs can initiate generation at lower data resolutions and gradually upscale the samples at each denoising step without loss in performance while significantly reducing computational costs. SwD naturally integrates this idea into existing diffusion distillation methods based on distribution matching. Also, we enrich the family of distribution matching approaches by introducing a novel patch loss enforcing finer-grained similarity to the target distribution. When applied to state-of-the-art text-to-image diffusion models, SwD approaches the inference times of two full resolution steps and significantly outperforms the counterparts under the same computation budget, as evidenced by automated metrics and human preference studies.",
            "score": 30,
            "issue_id": 2830,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "b981ae73ba6549a8",
            "authors": [
                "Nikita Starodubcev",
                "Denis Kuznedelev",
                "Artem Babenko",
                "Dmitry Baranchuk"
            ],
            "affiliations": [
                "Yandex Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16397.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#diffusion",
                    "#cv",
                    "#optimization"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼",
                    "desc": "SwD - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸ĞµĞ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. SwD Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞŸÑ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğº ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, SwD Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ."
                },
                "en": {
                    "title": "Efficient Image Generation with Scale-Wise Distillation",
                    "desc": "The paper introduces SwD, a new framework for improving diffusion models (DMs) by using a scale-wise distillation approach. This method allows DMs to start generating images at lower resolutions and progressively enhance them at each denoising step, which reduces computational costs without sacrificing quality. SwD builds on existing diffusion distillation techniques by incorporating a novel patch loss that ensures closer alignment with the target distribution. When tested on advanced text-to-image models, SwD shows faster inference times and better performance compared to traditional methods within the same computational limits."
                },
                "zh": {
                    "title": "SwDï¼šé«˜æ•ˆçš„æ‰©æ•£æ¨¡å‹è’¸é¦æ¡†æ¶",
                    "desc": "æˆ‘ä»¬æå‡ºäº†SwDï¼Œä¸€ä¸ªé’ˆå¯¹æ‰©æ•£æ¨¡å‹çš„å°ºåº¦çº§è’¸é¦æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨ä¸‹ä¸€å°ºåº¦é¢„æµ‹çš„æ€æƒ³æ¥ç”Ÿæˆå°‘æ­¥æ‰©æ•£æ¨¡å‹ã€‚SwDçš„çµæ„Ÿæ¥æºäºå°†æ‰©æ•£è¿‡ç¨‹ä¸éšå¼è°±è‡ªå›å½’ç›¸å…³è”çš„æœ€æ–°ç ”ç©¶ã€‚æˆ‘ä»¬å‡è®¾æ‰©æ•£æ¨¡å‹å¯ä»¥åœ¨è¾ƒä½çš„æ•°æ®åˆ†è¾¨ç‡ä¸‹å¼€å§‹ç”Ÿæˆï¼Œå¹¶åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­é€æ­¥æå‡æ ·æœ¬çš„åˆ†è¾¨ç‡ï¼ŒåŒæ—¶ä¸æŸå¤±æ€§èƒ½å¹¶æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚é€šè¿‡å¼•å…¥ä¸€ç§æ–°é¢–çš„è¡¥ä¸æŸå¤±ï¼ŒSwDå¢å¼ºäº†åˆ†å¸ƒåŒ¹é…æ–¹æ³•çš„ç»†ç²’åº¦ç›¸ä¼¼æ€§ï¼Œåº”ç”¨äºæœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ—¶ï¼ŒSwDåœ¨ç›¸åŒè®¡ç®—é¢„ç®—ä¸‹æ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦å’Œæ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16365",
            "title": "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play\n  Visual Games with Keyboards and Mouse",
            "url": "https://huggingface.co/papers/2503.16365",
            "abstract": "Recently, action-based decision-making in open-world environments has gained significant attention. Visual Language Action (VLA) models, pretrained on large-scale web datasets, have shown promise in decision-making tasks. However, previous work has primarily focused on action post-training, often neglecting enhancements to the foundational model itself. In response, we introduce a novel approach, Act from Visual Language Post-Training, which refines Visual Language Models (VLMs) through visual and linguistic guidance in a self-supervised manner. This enhancement improves the models' capabilities in world knowledge, visual recognition, and spatial grounding in open-world environments. Following the above post-training paradigms, we obtain the first VLA models in Minecraft that can follow human instructions on over 1k different atomic tasks, including crafting, smelting, cooking, mining, and killing. Our experiments demonstrate that post-training on non-trajectory tasks leads to a significant 40% improvement over the best agent baseline on a diverse set of atomic tasks. Furthermore, we demonstrate that our approach surpasses traditional imitation learning-based policies in Minecraft, achieving state-of-the-art performance. We have open-sourced the code, models, and datasets to foster further research. The project page can be found in https://craftjarvis.github.io/JarvisVLA.",
            "score": 30,
            "issue_id": 2824,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "764fc9201b406ec4",
            "authors": [
                "Muyao Li",
                "Zihao Wang",
                "Kaichen He",
                "Xiaojian Ma",
                "Yitao Liang"
            ],
            "affiliations": [
                "BIGAI",
                "Peking University",
                "Team CraftJarvis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16365.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#games",
                    "#cv",
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ (VLA) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Act from Visual Language Post-Training, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ VLM Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ¼Ğ¸Ñ€Ğµ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Minecraft Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ 40% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Decision-Making in Open Worlds with Visual Language Models",
                    "desc": "This paper presents a new method called Act from Visual Language Post-Training, which enhances Visual Language Models (VLMs) for decision-making in open-world environments. The approach uses visual and linguistic guidance in a self-supervised way to improve the models' understanding of world knowledge, visual recognition, and spatial grounding. The authors demonstrate that their VLA models can effectively follow human instructions in Minecraft for over 1,000 different tasks, achieving a 40% performance boost compared to existing agents. Additionally, their method outperforms traditional imitation learning techniques, setting a new standard in the field."
                },
                "zh": {
                    "title": "æå‡å¼€æ”¾ä¸–ç•Œå†³ç­–èƒ½åŠ›çš„è§†è§‰è¯­è¨€æ¨¡å‹",
                    "desc": "æœ€è¿‘ï¼ŒåŸºäºåŠ¨ä½œçš„å†³ç­–åœ¨å¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºè§†è§‰è¯­è¨€åè®­ç»ƒï¼ˆAct from Visual Language Post-Trainingï¼‰ï¼Œé€šè¿‡è§†è§‰å’Œè¯­è¨€æŒ‡å¯¼è‡ªæˆ‘ç›‘ç£åœ°æ”¹è¿›è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚è¿™ç§å¢å¼ºæé«˜äº†æ¨¡å‹åœ¨ä¸–ç•ŒçŸ¥è¯†ã€è§†è§‰è¯†åˆ«å’Œç©ºé—´å®šä½æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨Minecraftä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œè¶…è¿‡1000ä¸ªä¸åŒçš„åŸå­ä»»åŠ¡ï¼Œå¹¶åœ¨å¤šæ ·åŒ–çš„ä»»åŠ¡ä¸Šå®ç°äº†40%çš„æ€§èƒ½æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15558",
            "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning",
            "url": "https://huggingface.co/papers/2503.15558",
            "abstract": "Physical AI systems need to perceive, understand, and perform complex actions in the physical world. In this paper, we present the Cosmos-Reason1 models that can understand the physical world and generate appropriate embodied decisions (e.g., next step action) in natural language through long chain-of-thought reasoning processes. We begin by defining key capabilities for Physical AI reasoning, with a focus on physical common sense and embodied reasoning. To represent physical common sense, we use a hierarchical ontology that captures fundamental knowledge about space, time, and physics. For embodied reasoning, we rely on a two-dimensional ontology that generalizes across different physical embodiments. Building on these capabilities, we develop two multimodal large language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data and train our models in four stages: vision pre-training, general supervised fine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL) as the post-training. To evaluate our models, we build comprehensive benchmarks for physical common sense and embodied reasoning according to our ontologies. Evaluation results show that Physical AI SFT and reinforcement learning bring significant improvements. To facilitate the development of Physical AI, we will make our code and pre-trained models available under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-reason1.",
            "score": 30,
            "issue_id": 2823,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "882bcaa2257d8251",
            "authors": [
                "NVIDIA",
                ":",
                "Alisson Azzolini",
                "Hannah Brandon",
                "Prithvijit Chattopadhyay",
                "Huayu Chen",
                "Jinju Chu",
                "Yin Cui",
                "Jenna Diamond",
                "Yifan Ding",
                "Francesco Ferroni",
                "Rama Govindaraju",
                "Jinwei Gu",
                "Siddharth Gururani",
                "Imad El Hanafi",
                "Zekun Hao",
                "Jacob Huffman",
                "Jingyi Jin",
                "Brendan Johnson",
                "Rizwan Khan",
                "George Kurian",
                "Elena Lantz",
                "Nayeon Lee",
                "Zhaoshuo Li",
                "Xuan Li",
                "Tsung-Yi Lin",
                "Yen-Chen Lin",
                "Ming-Yu Liu",
                "Andrew Mathau",
                "Yun Ni",
                "Lindsey Pavao",
                "Wei Ping",
                "David W. Romero",
                "Misha Smelyanskiy",
                "Shuran Song",
                "Lyne Tchapmi",
                "Andrew Z. Wang",
                "Boxin Wang",
                "Haoxiang Wang",
                "Fangyin Wei",
                "Jiashu Xu",
                "Yao Xu",
                "Xiaodong Yang",
                "Zhuolin Yang",
                "Xiaohui Zeng",
                "Zhe Zhang"
            ],
            "affiliations": [
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15558.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#benchmark",
                    "#open_source",
                    "#agents",
                    "#reasoning",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ¤Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ˜Ğ˜: Ğ¾Ñ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ğº Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Cosmos-Reason1, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ¸Ñ€ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ½Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ¸ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ğ½Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ğ¹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¾ÑÑŒ Ğ² Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±Ñ‰Ğ°Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ°, ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ° Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ˜Ğ˜ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ğ´Ğ¾Ğ²Ğ¾Ğ´ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ˜Ğ˜."
                },
                "en": {
                    "title": "Empowering Physical AI with Cosmos-Reason1 Models",
                    "desc": "This paper introduces the Cosmos-Reason1 models designed for Physical AI systems, which need to understand and act in the physical world. The models utilize long chain-of-thought reasoning to generate actions in natural language, focusing on physical common sense and embodied reasoning. A hierarchical ontology is employed to represent physical common sense, while a two-dimensional ontology supports various physical embodiments. The authors train their models through a structured process, including vision pre-training and reinforcement learning, and demonstrate significant performance improvements through comprehensive evaluations."
                },
                "zh": {
                    "title": "ç‰©ç†äººå·¥æ™ºèƒ½çš„æ¨ç†ä¸å†³ç­–æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†Cosmos-Reason1æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿç†è§£ç‰©ç†ä¸–ç•Œå¹¶é€šè¿‡é•¿é“¾æ¨ç†è¿‡ç¨‹ç”Ÿæˆé€‚å½“å†³ç­–çš„ç‰©ç†äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚æˆ‘ä»¬å®šä¹‰äº†ç‰©ç†äººå·¥æ™ºèƒ½æ¨ç†çš„å…³é”®èƒ½åŠ›ï¼Œé‡ç‚¹å…³æ³¨ç‰©ç†å¸¸è¯†å’Œå…·èº«æ¨ç†ã€‚ä¸ºäº†è¡¨ç¤ºç‰©ç†å¸¸è¯†ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå±‚æ¬¡æœ¬ä½“ï¼Œæ•æ‰ç©ºé—´ã€æ—¶é—´å’Œç‰©ç†å­¦çš„åŸºæœ¬çŸ¥è¯†ã€‚é€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„å¼€å‘ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å±•ç¤ºäº†ç‰©ç†äººå·¥æ™ºèƒ½åœ¨æ¨ç†å’Œå†³ç­–ä¸­çš„æ˜¾è‘—è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16418",
            "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity",
            "url": "https://huggingface.co/papers/2503.16418",
            "abstract": "Achieving flexible and high-fidelity identity-preserved image generation remains formidable, particularly with advanced Diffusion Transformers (DiTs) like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust frameworks leveraging DiTs for this task. InfU addresses significant issues of existing methods, such as insufficient identity similarity, poor text-image alignment, and low generation quality and aesthetics. Central to InfU is InfuseNet, a component that injects identity features into the DiT base model via residual connections, enhancing identity similarity while maintaining generation capabilities. A multi-stage training strategy, including pretraining and supervised fine-tuning (SFT) with synthetic single-person-multiple-sample (SPMS) data, further improves text-image alignment, ameliorates image quality, and alleviates face copy-pasting. Extensive experiments demonstrate that InfU achieves state-of-the-art performance, surpassing existing baselines. In addition, the plug-and-play design of InfU ensures compatibility with various existing methods, offering a valuable contribution to the broader community.",
            "score": 26,
            "issue_id": 2824,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "71a484a2a785f8be",
            "authors": [
                "Liming Jiang",
                "Qing Yan",
                "Yumin Jia",
                "Zichuan Liu",
                "Hao Kang",
                "Xin Lu"
            ],
            "affiliations": [
                "ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16418.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#diffusion",
                    "#architecture",
                    "#synthetic",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "InfiniteYou: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "InfiniteYou (InfU) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ (DiT). ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ InfU - InfuseNet, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DiT Ñ‡ĞµÑ€ĞµĞ· Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ InfU Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "InfiniteYou: Revolutionizing Identity-Preserved Image Generation with DiTs",
                    "desc": "This paper presents InfiniteYou (InfU), a novel framework that enhances identity-preserved image generation using advanced Diffusion Transformers (DiTs) like FLUX. InfU tackles key challenges in the field, such as improving identity similarity, aligning text with images, and enhancing overall image quality. The core innovation is InfuseNet, which integrates identity features into the DiT model through residual connections, thereby boosting identity preservation. With a multi-stage training approach that includes pretraining and supervised fine-tuning on synthetic data, InfU achieves superior performance compared to existing methods, while also being adaptable to various frameworks."
                },
                "zh": {
                    "title": "æ— é™å¯èƒ½çš„èº«ä»½ä¿ç•™å›¾åƒç”Ÿæˆ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºInfiniteYouï¼ˆInfUï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°é«˜ä¿çœŸåº¦çš„èº«ä»½ä¿ç•™å›¾åƒç”Ÿæˆã€‚InfUåˆ©ç”¨å…ˆè¿›çš„æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨èº«ä»½ç›¸ä¼¼æ€§ã€æ–‡æœ¬ä¸å›¾åƒå¯¹é½ä»¥åŠç”Ÿæˆè´¨é‡ç­‰æ–¹é¢çš„ä¸è¶³ã€‚å…¶æ ¸å¿ƒç»„ä»¶InfuseNeté€šè¿‡æ®‹å·®è¿æ¥å°†èº«ä»½ç‰¹å¾æ³¨å…¥åˆ°DiTåŸºç¡€æ¨¡å‹ä¸­ï¼Œä»è€Œå¢å¼ºèº«ä»½ç›¸ä¼¼æ€§ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼ŒInfUæ˜¾è‘—æé«˜äº†æ–‡æœ¬ä¸å›¾åƒçš„å¯¹é½åº¦å’Œå›¾åƒè´¨é‡ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶æ€§èƒ½è¶…è¶Šäº†ç°æœ‰åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14487",
            "title": "DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers",
            "url": "https://huggingface.co/papers/2503.14487",
            "abstract": "Diffusion models have demonstrated remarkable success in various image generation tasks, but their performance is often limited by the uniform processing of inputs across varying conditions and noise levels. To address this limitation, we propose a novel approach that leverages the inherent heterogeneity of the diffusion process. Our method, DiffMoE, introduces a batch-level global token pool that enables experts to access global token distributions during training, promoting specialized expert behavior. To unleash the full potential of the diffusion process, DiffMoE incorporates a capacity predictor that dynamically allocates computational resources based on noise levels and sample complexity. Through comprehensive evaluation, DiffMoE achieves state-of-the-art performance among diffusion models on ImageNet benchmark, substantially outperforming both dense architectures with 3x activated parameters and existing MoE approaches while maintaining 1x activated parameters. The effectiveness of our approach extends beyond class-conditional generation to more challenging tasks such as text-to-image generation, demonstrating its broad applicability across different diffusion model applications. Project Page: https://shiml20.github.io/DiffMoE/",
            "score": 25,
            "issue_id": 2828,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "f530713a45ca1341",
            "authors": [
                "Minglei Shi",
                "Ziyang Yuan",
                "Haotian Yang",
                "Xintao Wang",
                "Mingwu Zheng",
                "Xin Tao",
                "Wenliang Zhao",
                "Wenzhao Zheng",
                "Jie Zhou",
                "Jiwen Lu",
                "Pengfei Wan",
                "Di Zhang",
                "Kun Gai"
            ],
            "affiliations": [
                "Kuais",
                "Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14487.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "DiffMoE: Ğ£Ğ¼Ğ½Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "DiffMoE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ÑƒĞ» Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ°ĞºĞµÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. DiffMoE Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¾Ñ€ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ImageNet, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ°Ğº Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ñ‚Ğ°Ğº Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ MoE."
                },
                "en": {
                    "title": "Unlocking the Power of Diffusion with Expert Adaptation",
                    "desc": "This paper introduces DiffMoE, a new method for improving diffusion models in image generation. It addresses the issue of uniform input processing by utilizing a batch-level global token pool, allowing for specialized expert behavior during training. Additionally, DiffMoE features a capacity predictor that adjusts computational resources based on the noise levels and complexity of the samples. The results show that DiffMoE achieves top performance on the ImageNet benchmark, outperforming existing models while using fewer activated parameters, and is effective in various tasks including text-to-image generation."
                },
                "zh": {
                    "title": "é‡Šæ”¾æ‰©æ•£æ¨¡å‹çš„æ½œåŠ›ï¼ŒDiffMoEå¼•é¢†æ–°æ½®æµ",
                    "desc": "æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸åŒæ¡ä»¶å’Œå™ªå£°æ°´å¹³ä¸‹çš„è¾“å…¥å¤„ç†ä¸Šå­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºDiffMoEï¼Œåˆ©ç”¨æ‰©æ•£è¿‡ç¨‹çš„å†…åœ¨å¼‚è´¨æ€§ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªæ‰¹é‡çº§çš„å…¨å±€ä»¤ç‰Œæ± ï¼Œä½¿ä¸“å®¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿè®¿é—®å…¨å±€ä»¤ç‰Œåˆ†å¸ƒï¼Œä»è€Œä¿ƒè¿›ä¸“å®¶çš„ä¸“ä¸šåŒ–è¡Œä¸ºã€‚æ­¤å¤–ï¼ŒDiffMoEè¿˜ç»“åˆäº†ä¸€ä¸ªå®¹é‡é¢„æµ‹å™¨ï¼Œæ ¹æ®å™ªå£°æ°´å¹³å’Œæ ·æœ¬å¤æ‚æ€§åŠ¨æ€åˆ†é…è®¡ç®—èµ„æºï¼Œå……åˆ†å‘æŒ¥æ‰©æ•£è¿‡ç¨‹çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13657",
            "title": "Why Do Multi-Agent LLM Systems Fail?",
            "url": "https://huggingface.co/papers/2503.13657",
            "abstract": "Despite growing enthusiasm for Multi-Agent Systems (MAS), where multiple LLM agents collaborate to accomplish tasks, their performance gains across popular benchmarks remain minimal compared to single-agent frameworks. This gap highlights the need to analyze the challenges hindering MAS effectiveness.   In this paper, we present the first comprehensive study of MAS challenges. We analyze five popular MAS frameworks across over 150 tasks, involving six expert human annotators. We identify 14 unique failure modes and propose a comprehensive taxonomy applicable to various MAS frameworks. This taxonomy emerges iteratively from agreements among three expert annotators per study, achieving a Cohen's Kappa score of 0.88. These fine-grained failure modes are organized into 3 categories, (i) specification and system design failures, (ii) inter-agent misalignment, and (iii) task verification and termination. To support scalable evaluation, we integrate MASFT with LLM-as-a-Judge. We also explore if identified failures could be easily prevented by proposing two interventions: improved specification of agent roles and enhanced orchestration strategies. Our findings reveal that identified failures require more complex solutions, highlighting a clear roadmap for future research. We open-source our dataset and LLM annotator.",
            "score": 24,
            "issue_id": 2829,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "f3e2aec1e3948d46",
            "authors": [
                "Mert Cemri",
                "Melissa Z. Pan",
                "Shuyi Yang",
                "Lakshya A. Agrawal",
                "Bhavya Chopra",
                "Rishabh Tiwari",
                "Kurt Keutzer",
                "Aditya Parameswaran",
                "Dan Klein",
                "Kannan Ramchandran",
                "Matei Zaharia",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "affiliations": [
                "Intesa Sanpaolo",
                "UC Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13657.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#benchmark",
                    "#dataset",
                    "#agi",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼: Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ˜Ğ˜",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ (ĞœĞĞ¡) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 14 ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ² Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ÑƒÑ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°Ğ¼ ĞœĞĞ¡. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ»Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ² ĞœĞĞ¡ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 150 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ ÑˆĞµÑÑ‚Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²-Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unlocking the Potential of Multi-Agent Systems: Identifying and Overcoming Challenges",
                    "desc": "This paper investigates the challenges faced by Multi-Agent Systems (MAS) that involve multiple large language model (LLM) agents working together. The authors conducted a thorough analysis of five MAS frameworks across over 150 tasks, identifying 14 distinct failure modes that hinder performance. They categorized these failures into three main groups: specification and system design failures, inter-agent misalignment, and task verification and termination issues. Additionally, the paper proposes interventions to mitigate these failures and emphasizes the need for more sophisticated solutions to enhance MAS effectiveness."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ",
                    "desc": "å°½ç®¡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œä½†ä¸å•æ™ºèƒ½ä½“æ¡†æ¶ç›¸æ¯”ï¼Œå…¶åœ¨æµè¡ŒåŸºå‡†ä¸Šçš„æ€§èƒ½æå‡ä»ç„¶æœ‰é™ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢ç ”ç©¶äº†MASé¢ä¸´çš„æŒ‘æˆ˜ï¼Œåˆ†æäº†äº”ä¸ªæµè¡Œçš„MASæ¡†æ¶åŠå…¶åœ¨150å¤šä¸ªä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬è¯†åˆ«å‡º14ç§ç‹¬ç‰¹çš„å¤±è´¥æ¨¡å¼ï¼Œå¹¶æå‡ºäº†é€‚ç”¨äºå„ç§MASæ¡†æ¶çš„ç»¼åˆåˆ†ç±»æ³•ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè§£å†³è¿™äº›å¤±è´¥æ¨¡å¼éœ€è¦æ›´å¤æ‚çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16257",
            "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
            "url": "https://huggingface.co/papers/2503.16257",
            "abstract": "Video large language models (VideoLLMs) have demonstrated the capability to process longer video inputs and enable complex reasoning and analysis. However, due to the thousands of visual tokens from the video frames, key-value (KV) cache can significantly increase memory requirements, becoming a bottleneck for inference speed and memory usage. KV cache quantization is a widely used approach to address this problem. In this paper, we find that 2-bit KV quantization of VideoLLMs can hardly hurt the model performance, while the limit of KV cache quantization in even lower bits has not been investigated. To bridge this gap, we introduce VidKV, a plug-and-play KV cache quantization method to compress the KV cache to lower than 2 bits. Specifically, (1) for key, we propose a mixed-precision quantization strategy in the channel dimension, where we perform 2-bit quantization for anomalous channels and 1-bit quantization combined with FFT for normal channels; (2) for value, we implement 1.58-bit quantization while selectively filtering semantically salient visual tokens for targeted preservation, for a better trade-off between precision and model performance. Importantly, our findings suggest that the value cache of VideoLLMs should be quantized in a per-channel fashion instead of the per-token fashion proposed by prior KV cache quantization works for LLMs. Empirically, extensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show that VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit precision with almost no performance drop compared to the FP16 counterparts.",
            "score": 21,
            "issue_id": 2829,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "68c739951d6b3dce",
            "authors": [
                "Keda Tao",
                "Haoxuan You",
                "Yang Sui",
                "Can Qin",
                "Huan Wang"
            ],
            "affiliations": [
                "Columbia University",
                "Rice University",
                "Salesforce AI Research",
                "Westlake University",
                "Xidian University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16257.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#long_context",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "VidKV: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ĞºÑÑˆĞ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VidKV - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ (KV) Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VideoLLMs). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ÑƒÑ ÑĞ¶Ğ°Ñ‚ÑŒ ĞºÑÑˆ Ğ´Ğ¾ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ 2 Ğ±Ğ¸Ñ‚ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 2-Ğ±Ğ¸Ñ‚Ğ½ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ² ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¸ 1-Ğ±Ğ¸Ñ‚Ğ½ÑƒÑ Ñ FFT Ğ´Ğ»Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ 1.58-Ğ±Ğ¸Ñ‚Ğ½ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ VidKV Ğ² ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ ĞºÑÑˆĞ° Ğ´Ğ¾ 1.5-1.58 Ğ±Ğ¸Ñ‚ Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Efficient KV Cache Compression for VideoLLMs with VidKV",
                    "desc": "This paper introduces VidKV, a novel method for quantizing key-value (KV) caches in Video Large Language Models (VideoLLMs) to improve memory efficiency. The authors demonstrate that using 2-bit quantization does not significantly impact model performance, and they explore even lower quantization levels. VidKV employs a mixed-precision strategy for keys and a selective filtering approach for values, allowing for effective compression while maintaining accuracy. The results show that VidKV can reduce KV cache size to 1.5-bit and 1.58-bit with minimal performance loss, outperforming previous methods that used per-token quantization."
                },
                "zh": {
                    "title": "å‹ç¼©KVç¼“å­˜ï¼Œæå‡è§†é¢‘æ¨¡å‹æ€§èƒ½",
                    "desc": "è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMsï¼‰èƒ½å¤Ÿå¤„ç†æ›´é•¿çš„è§†é¢‘è¾“å…¥ï¼Œå¹¶è¿›è¡Œå¤æ‚çš„æ¨ç†å’Œåˆ†æã€‚ç„¶è€Œï¼Œç”±äºè§†é¢‘å¸§ä¸­æˆåƒä¸Šä¸‡çš„è§†è§‰æ ‡è®°ï¼Œé”®å€¼ï¼ˆKVï¼‰ç¼“å­˜ä¼šæ˜¾è‘—å¢åŠ å†…å­˜éœ€æ±‚ï¼Œæˆä¸ºæ¨ç†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨çš„ç“¶é¢ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVidKVçš„KVç¼“å­˜é‡åŒ–æ–¹æ³•ï¼Œå¯ä»¥å°†KVç¼“å­˜å‹ç¼©åˆ°ä½äº2ä½ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½å‡ ä¹ä¸å˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ··åˆç²¾åº¦é‡åŒ–ç­–ç•¥å’Œé€‰æ‹©æ€§è¿‡æ»¤è¯­ä¹‰æ˜¾è‘—çš„è§†è§‰æ ‡è®°ï¼Œå®ç°äº†æ›´å¥½çš„ç²¾åº¦ä¸æ¨¡å‹æ€§èƒ½ä¹‹é—´çš„å¹³è¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16252",
            "title": "Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning",
            "url": "https://huggingface.co/papers/2503.16252",
            "abstract": "Reasoning large language models are rapidly evolving across various domains. However, their capabilities in handling complex financial tasks still require in-depth exploration. In this paper, we introduce Fin-R1, a reasoning large language model specifically designed for the financial sector. Fin-R1 is built using a two-stage architecture, leveraging a financial reasoning dataset distilled and processed based on DeepSeek-R1. Through supervised fine-tuning (SFT) and reinforcement learning (RL) training, it demonstrates performance close to DeepSeek-R1 with a parameter size of 7 billion across a range of financial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA and ConvFinQA tasks between those LLMs in our evaluation, surpassing larger models in other tasks as well. Fin-R1 showcases strong reasoning and decision-making capabilities, providing solutions to various problems encountered in the financial domain. Our code is available at https://github.com/SUFE-AIFLM-Lab/Fin-R1.",
            "score": 21,
            "issue_id": 2823,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "f2a1e8506f9711ee",
            "authors": [
                "Zhaowei Liu",
                "Xin Guo",
                "Fangqi Lou",
                "Lingfeng Zeng",
                "Jinyi Niu",
                "Zixuan Wang",
                "Jiajie Xu",
                "Weige Cai",
                "Ziwei Yang",
                "Xueqian Zhao",
                "Chao Li",
                "Sheng Xu",
                "Dezhi Chen",
                "Yun Chen",
                "Zuo Bai",
                "Liwen Zhang"
            ],
            "affiliations": [
                "FinStep",
                "Fudan University",
                "Shanghai University of Finance and Economics"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16252.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#rl",
                    "#architecture",
                    "#reasoning",
                    "#dataset",
                    "#healthcare"
                ],
                "emoji": "ğŸ’¹",
                "ru": {
                    "title": "Fin-R1: ĞœĞ¾Ñ‰Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Fin-R1 - ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞµĞºÑ‚Ğ¾Ñ€Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ° DeepSeek-R1. Fin-R1 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ±Ğ»Ğ¸Ğ·ĞºÑƒÑ Ğº DeepSeek-R1, Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğµ Ğ² 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… FinQA Ğ¸ ConvFinQA ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ."
                },
                "en": {
                    "title": "Fin-R1: Revolutionizing Financial Reasoning with AI",
                    "desc": "This paper presents Fin-R1, a specialized reasoning large language model tailored for financial tasks. It employs a two-stage architecture and is trained on a unique financial reasoning dataset derived from DeepSeek-R1. Through techniques like supervised fine-tuning and reinforcement learning, Fin-R1 achieves competitive performance with 7 billion parameters, excelling in financial reasoning benchmarks such as FinQA and ConvFinQA. The model demonstrates advanced reasoning and decision-making skills, addressing various challenges in the financial sector."
                },
                "zh": {
                    "title": "é‡‘èé¢†åŸŸçš„æ¨ç†æ–°æ˜Ÿï¼šFin-R1",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä¸“ä¸ºé‡‘èé¢†åŸŸè®¾è®¡çš„æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹Fin-R1ã€‚Fin-R1é‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼Œåˆ©ç”¨åŸºäºDeepSeek-R1çš„é‡‘èæ¨ç†æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œå®ƒåœ¨å¤šä¸ªé‡‘èæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°æ¥è¿‘DeepSeek-R1ï¼Œå‚æ•°è§„æ¨¡ä¸º70äº¿ã€‚Fin-R1åœ¨FinQAå’ŒConvFinQAä»»åŠ¡ä¸­è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œå†³ç­–èƒ½åŠ›ï¼Œä¸ºé‡‘èé¢†åŸŸçš„å„ç§é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16420",
            "title": "SynCity: Training-Free Generation of 3D Worlds",
            "url": "https://huggingface.co/papers/2503.16420",
            "abstract": "We address the challenge of generating 3D worlds from textual descriptions. We propose SynCity, a training- and optimization-free approach, which leverages the geometric precision of pre-trained 3D generative models and the artistic versatility of 2D image generators to create large, high-quality 3D spaces. While most 3D generative models are object-centric and cannot generate large-scale worlds, we show how 3D and 2D generators can be combined to generate ever-expanding scenes. Through a tile-based approach, we allow fine-grained control over the layout and the appearance of scenes. The world is generated tile-by-tile, and each new tile is generated within its world-context and then fused with the scene. SynCity generates compelling and immersive scenes that are rich in detail and diversity.",
            "score": 18,
            "issue_id": 2836,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "2a35e5254e6b2f50",
            "authors": [
                "Paul Engstler",
                "Aleksandar Shtedritski",
                "Iro Laina",
                "Christian Rupprecht",
                "Andrea Vedaldi"
            ],
            "affiliations": [
                "Visual Geometry Group, University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16420.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#3d",
                    "#games"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "SynCity: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SynCity - Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¸Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒÑ 2D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. SynCity Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ°Ğ¹Ğ»Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºÑƒ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ ÑÑ†ĞµĞ½. ĞœĞ¸Ñ€ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ğ»Ğ¸Ñ‚ĞºĞ° Ğ·Ğ° Ğ¿Ğ»Ğ¸Ñ‚ĞºĞ¾Ğ¹, ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ»Ğ¸Ñ‚ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ÑÑ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¼Ğ¸Ñ€Ğ° Ğ¸ Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑĞ¾ ÑÑ†ĞµĞ½Ğ¾Ğ¹."
                },
                "en": {
                    "title": "Building 3D Worlds from Words with SynCity",
                    "desc": "This paper introduces SynCity, a novel method for creating 3D environments from text descriptions without the need for extensive training or optimization. It combines the strengths of pre-trained 3D generative models, which provide geometric accuracy, with the flexibility of 2D image generators to produce expansive and detailed 3D worlds. By using a tile-based approach, SynCity allows for precise control over the layout and aesthetics of the generated scenes, ensuring that each tile fits cohesively within the overall context of the world. The result is a system capable of generating immersive and richly detailed environments that enhance user experience."
                },
                "zh": {
                    "title": "ä»æ–‡æœ¬åˆ°3Dä¸–ç•Œçš„åˆ›æ–°ç”Ÿæˆ",
                    "desc": "æˆ‘ä»¬è§£å†³äº†ä»æ–‡æœ¬æè¿°ç”Ÿæˆ3Dä¸–ç•Œçš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†SynCityï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå’Œä¼˜åŒ–çš„æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„3Dç”Ÿæˆæ¨¡å‹çš„å‡ ä½•ç²¾åº¦å’Œ2Då›¾åƒç”Ÿæˆå™¨çš„è‰ºæœ¯å¤šæ ·æ€§æ¥åˆ›å»ºå¤§å‹é«˜è´¨é‡çš„3Dç©ºé—´ã€‚é€šè¿‡åŸºäºç“¦ç‰‡çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹åœºæ™¯çš„å¸ƒå±€å’Œå¤–è§‚è¿›è¡Œç»†è‡´çš„æ§åˆ¶ã€‚SynCityç”Ÿæˆçš„åœºæ™¯å¼•äººå…¥èƒœï¼Œç»†èŠ‚ä¸°å¯Œï¼Œå…·æœ‰å¤šæ ·æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16212",
            "title": "MathFusion: Enhancing Mathematic Problem-solving of LLM through\n  Instruction Fusion",
            "url": "https://huggingface.co/papers/2503.16212",
            "abstract": "Large Language Models (LLMs) have shown impressive progress in mathematical reasoning. While data augmentation is promising to enhance mathematical problem-solving ability, current approaches are predominantly limited to instance-level modifications-such as rephrasing or generating syntactic variations-which fail to capture and leverage the intrinsic relational structures inherent in mathematical knowledge. Inspired by human learning processes, where mathematical proficiency develops through systematic exposure to interconnected concepts, we introduce MathFusion, a novel framework that enhances mathematical reasoning through cross-problem instruction synthesis. MathFusion implements this through three fusion strategies: (1) sequential fusion, which chains related problems to model solution dependencies; (2) parallel fusion, which combines analogous problems to reinforce conceptual understanding; and (3) conditional fusion, which creates context-aware selective problems to enhance reasoning flexibility. By applying these strategies, we generate a new dataset, MathFusionQA, followed by fine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental results demonstrate that MathFusion achieves substantial improvements in mathematical reasoning while maintaining high data efficiency, boosting performance by 18.0 points in accuracy across diverse benchmarks while requiring only 45K additional synthetic instructions, representing a substantial improvement over traditional single-instruction approaches. Our datasets, models, and code are publicly available at https://github.com/QizhiPei/mathfusion.",
            "score": 17,
            "issue_id": 2823,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "b5997ebd979f98f0",
            "authors": [
                "Qizhi Pei",
                "Lijun Wu",
                "Zhuoshi Pan",
                "Yu Li",
                "Honglin Lin",
                "Chenlin Ming",
                "Xin Gao",
                "Conghui He",
                "Rui Yan"
            ],
            "affiliations": [
                "Gaoling School of Artificial Intelligence, Renmin University of China",
                "School of Computer Science, Wuhan University",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16212.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#synthetic",
                    "#open_source",
                    "#math",
                    "#reasoning",
                    "#dataset",
                    "#benchmark",
                    "#data"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ¡Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ĞºĞ°Ñ‡ĞºĞ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜",
                    "desc": "MathFusion - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ, Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MathFusionQA Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… LLM, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 18 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 45 Ñ‚Ñ‹ÑÑÑ‡ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Mathematical Reasoning through Conceptual Fusion",
                    "desc": "This paper presents MathFusion, a new framework designed to improve the mathematical reasoning capabilities of Large Language Models (LLMs). Unlike traditional methods that only modify individual problems, MathFusion uses three innovative fusion strategies to connect related mathematical concepts and problems. These strategies include sequential, parallel, and conditional fusion, which help models learn from the relationships between problems rather than in isolation. The results show that MathFusion significantly enhances accuracy in mathematical reasoning tasks while being efficient in data usage, outperforming previous methods with fewer additional instructions."
                },
                "zh": {
                    "title": "é€šè¿‡MathFusionæå‡æ•°å­¦æ¨ç†èƒ½åŠ›",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç°æœ‰çš„æ•°æ®å¢å¼ºæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å®ä¾‹çº§åˆ«çš„ä¿®æ”¹ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ•°å­¦çŸ¥è¯†ä¸­å›ºæœ‰çš„å…³ç³»ç»“æ„ã€‚æˆ‘ä»¬æå‡ºäº†MathFusionæ¡†æ¶ï¼Œé€šè¿‡è·¨é—®é¢˜çš„æŒ‡ä»¤åˆæˆæ¥æå‡æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸‰ç§èåˆç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.10625",
            "title": "LHM: Large Animatable Human Reconstruction Model from a Single Image in\n  Seconds",
            "url": "https://huggingface.co/papers/2503.10625",
            "abstract": "Animatable 3D human reconstruction from a single image is a challenging problem due to the ambiguity in decoupling geometry, appearance, and deformation. Recent advances in 3D human reconstruction mainly focus on static human modeling, and the reliance of using synthetic 3D scans for training limits their generalization ability. Conversely, optimization-based video methods achieve higher fidelity but demand controlled capture conditions and computationally intensive refinement processes. Motivated by the emergence of large reconstruction models for efficient static reconstruction, we propose LHM (Large Animatable Human Reconstruction Model) to infer high-fidelity avatars represented as 3D Gaussian splatting in a feed-forward pass. Our model leverages a multimodal transformer architecture to effectively encode the human body positional features and image features with attention mechanism, enabling detailed preservation of clothing geometry and texture. To further boost the face identity preservation and fine detail recovery, we propose a head feature pyramid encoding scheme to aggregate multi-scale features of the head regions. Extensive experiments demonstrate that our LHM generates plausible animatable human in seconds without post-processing for face and hands, outperforming existing methods in both reconstruction accuracy and generalization ability.",
            "score": 17,
            "issue_id": 2828,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "35de541953d40e20",
            "authors": [
                "Lingteng Qiu",
                "Xiaodong Gu",
                "Peihao Li",
                "Qi Zuo",
                "Weichao Shen",
                "Junfei Zhang",
                "Kejie Qiu",
                "Weihao Yuan",
                "Guanying Chen",
                "Zilong Dong",
                "Liefeng Bo"
            ],
            "affiliations": [
                "Tongyi Lab, Alibaba Group"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.10625.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#architecture"
                ],
                "emoji": "ğŸ§‘â€ğŸ¦°",
                "ru": {
                    "title": "Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LHM (Large Animatable Human Reconstruction Model) - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ‚ĞµĞ»Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸Ñ†Ğ° Ğ¸ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ…ĞµĞ¼Ğ° ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹. LHM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ·Ğ° ÑĞµĞºÑƒĞ½Ğ´Ñ‹ Ğ±ĞµĞ· Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing 3D Human Reconstruction with LHM",
                    "desc": "This paper presents the Large Animatable Human Reconstruction Model (LHM), which addresses the challenge of creating 3D human avatars from a single image. Unlike previous methods that rely on static models or require extensive computational resources, LHM uses a multimodal transformer architecture to efficiently encode both body and image features. The model incorporates a head feature pyramid encoding scheme to enhance the detail and identity of facial features. Experimental results show that LHM can generate high-fidelity, animatable human models quickly and accurately, surpassing existing techniques in both performance and adaptability."
                },
                "zh": {
                    "title": "é«˜æ•ˆçš„3Då¯åŠ¨ç”»äººç±»é‡å»ºæ¨¡å‹",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤§å‹å¯åŠ¨ç”»äººç±»é‡å»ºæ¨¡å‹ï¼ˆLHMï¼‰ï¼Œæ—¨åœ¨ä»å•å¼ å›¾åƒä¸­é‡å»ºé«˜ä¿çœŸåº¦çš„3Däººç±»å¤´åƒã€‚è¯¥æ¨¡å‹åˆ©ç”¨å¤šæ¨¡æ€å˜æ¢å™¨æ¶æ„ï¼Œæœ‰æ•ˆç¼–ç äººä½“ä½ç½®ç‰¹å¾å’Œå›¾åƒç‰¹å¾ï¼Œèƒ½å¤Ÿè¯¦ç»†ä¿ç•™è¡£ç‰©çš„å‡ ä½•å½¢çŠ¶å’Œçº¹ç†ã€‚ä¸ºäº†å¢å¼ºé¢éƒ¨èº«ä»½çš„ä¿ç•™å’Œç»†èŠ‚æ¢å¤ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§å¤´éƒ¨ç‰¹å¾é‡‘å­—å¡”ç¼–ç æ–¹æ¡ˆï¼Œèšåˆå¤´éƒ¨åŒºåŸŸçš„å¤šå°ºåº¦ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼ŒLHMåœ¨é‡å»ºå‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å‡ ç§’å†…ç”Ÿæˆå¯åŠ¨ç”»çš„äººç±»æ¨¡å‹ï¼Œæ— éœ€åå¤„ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16413",
            "title": "M3: 3D-Spatial MultiModal Memory",
            "url": "https://huggingface.co/papers/2503.16413",
            "abstract": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system designed to retain information about medium-sized static scenes through video sources for visual perception. By integrating 3D Gaussian Splatting techniques with foundation models, M3 builds a multimodal memory capable of rendering feature representations across granularities, encompassing a wide range of knowledge. In our exploration, we identify two key challenges in previous works on feature splatting: (1) computational constraints in storing high-dimensional features for each Gaussian primitive, and (2) misalignment or information loss between distilled features and foundation model features. To address these challenges, we propose M3 with key components of principal scene components and Gaussian memory attention, enabling efficient training and inference. To validate M3, we conduct comprehensive quantitative evaluations of feature similarity and downstream tasks, as well as qualitative visualizations to highlight the pixel trace of Gaussian memory attention. Our approach encompasses a diverse range of foundation models, including vision-language models (VLMs), perception models, and large multimodal and language models (LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy M3's feature field in indoor scenes on a quadruped robot. Notably, we claim that M3 is the first work to address the core compression challenges in 3D feature distillation.",
            "score": 13,
            "issue_id": 2823,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "963439a9c78faf82",
            "authors": [
                "Xueyan Zou",
                "Yuchen Song",
                "Ri-Zhao Qiu",
                "Xuanbin Peng",
                "Jianglong Ye",
                "Sifei Liu",
                "Xiaolong Wang"
            ],
            "affiliations": [
                "NVIDIA",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16413.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#architecture",
                    "#optimization",
                    "#games",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ 3D Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ 3D ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ĞœÑƒĞ»ÑŒÑ‚Ğ¸ĞœĞ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ĞŸĞ°Ğ¼ÑÑ‚ÑŒ (M3) - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ… ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. M3 Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ° Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° M3 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Scene Memory with M3: Efficient 3D Feature Distillation",
                    "desc": "The paper introduces the 3D Spatial MultiModal Memory (M3), a novel memory system that captures and retains information about static scenes using video inputs for enhanced visual perception. M3 combines 3D Gaussian Splatting with foundation models to create a multimodal memory that effectively represents features at various levels of detail. It tackles two significant challenges in previous methods: the difficulty of storing high-dimensional features and the misalignment of distilled features with those from foundation models. The authors validate M3 through extensive evaluations and demonstrate its practical use in real-world scenarios, such as deploying it on a quadruped robot in indoor environments."
                },
                "zh": {
                    "title": "3Då¤šæ¨¡æ€è®°å¿†ï¼šè§£å†³ç‰¹å¾å‹ç¼©æŒ‘æˆ˜çš„åˆ›æ–°",
                    "desc": "æˆ‘ä»¬æå‡ºäº†3Dç©ºé—´å¤šæ¨¡æ€è®°å¿†ç³»ç»Ÿï¼ˆM3ï¼‰ï¼Œæ—¨åœ¨é€šè¿‡è§†é¢‘æºä¿ç•™ä¸­ç­‰å¤§å°é™æ€åœºæ™¯çš„ä¿¡æ¯ï¼Œä»¥å¢å¼ºè§†è§‰æ„ŸçŸ¥ã€‚M3ç»“åˆäº†3Dé«˜æ–¯ç‚¹äº‘æŠ€æœ¯å’ŒåŸºç¡€æ¨¡å‹ï¼Œæ„å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿè·¨ç²’åº¦æ¸²æŸ“ç‰¹å¾è¡¨ç¤ºçš„å¤šæ¨¡æ€è®°å¿†ç³»ç»Ÿï¼Œæ¶µç›–å¹¿æ³›çš„çŸ¥è¯†ã€‚æˆ‘ä»¬è¯†åˆ«äº†ä»¥å¾€ç‰¹å¾ç‚¹äº‘å·¥ä½œä¸­çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šå­˜å‚¨é«˜ç»´ç‰¹å¾çš„è®¡ç®—é™åˆ¶ï¼Œä»¥åŠæå–ç‰¹å¾ä¸åŸºç¡€æ¨¡å‹ç‰¹å¾ä¹‹é—´çš„é”™ä½æˆ–ä¿¡æ¯ä¸¢å¤±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†M3ï¼Œé‡‡ç”¨äº†ä¸»è¦åœºæ™¯ç»„ä»¶å’Œé«˜æ–¯è®°å¿†æ³¨æ„åŠ›çš„å…³é”®ç»„ä»¶ï¼Œå®ç°äº†é«˜æ•ˆçš„è®­ç»ƒå’Œæ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16422",
            "title": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering",
            "url": "https://huggingface.co/papers/2503.16422",
            "abstract": "4D Gaussian Splatting (4DGS) has recently gained considerable attention as a method for reconstructing dynamic scenes. Despite achieving superior quality, 4DGS typically requires substantial storage and suffers from slow rendering speed. In this work, we delve into these issues and identify two key sources of temporal redundancy. (Q1) Short-Lifespan Gaussians: 4DGS uses a large portion of Gaussians with short temporal span to represent scene dynamics, leading to an excessive number of Gaussians. (Q2) Inactive Gaussians: When rendering, only a small subset of Gaussians contributes to each frame. Despite this, all Gaussians are processed during rasterization, resulting in redundant computation overhead. To address these redundancies, we present 4DGS-1K, which runs at over 1000 FPS on modern GPUs. For Q1, we introduce the Spatial-Temporal Variation Score, a new pruning criterion that effectively removes short-lifespan Gaussians while encouraging 4DGS to capture scene dynamics using Gaussians with longer temporal spans. For Q2, we store a mask for active Gaussians across consecutive frames, significantly reducing redundant computations in rendering. Compared to vanilla 4DGS, our method achieves a 41times reduction in storage and 9times faster rasterization speed on complex dynamic scenes, while maintaining comparable visual quality. Please see our project page at https://4DGS-1K.github.io.",
            "score": 12,
            "issue_id": 2823,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "a00a6529a3f82f89",
            "authors": [
                "Yuheng Yuan",
                "Qiuhong Shen",
                "Xingyi Yang",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16422.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "4DGS-1K: Ğ¡Ğ²ĞµÑ€Ñ…Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ 4DGS-1K Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ 4D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, Ğ²Ñ‹Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾Ğ¶Ğ¸Ğ²ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ½ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¹ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ². Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ 41-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ 9-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ 4DGS Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "Streamlining Dynamic Scene Reconstruction with 4DGS-1K",
                    "desc": "This paper introduces 4DGS-1K, an improved version of 4D Gaussian Splatting that addresses issues of high storage requirements and slow rendering speeds in dynamic scene reconstruction. The authors identify two main sources of inefficiency: the use of short-lifespan Gaussians that clutter the representation and the processing of inactive Gaussians during rendering. To optimize performance, they propose a new pruning criterion called the Spatial-Temporal Variation Score, which helps retain only the most relevant Gaussians. As a result, 4DGS-1K achieves a significant reduction in storage and rendering time while preserving visual quality, making it a more efficient solution for real-time applications."
                },
                "zh": {
                    "title": "æå‡åŠ¨æ€åœºæ™¯é‡å»ºæ•ˆç‡çš„4DGS-1K",
                    "desc": "4Dé«˜æ–¯ç‚¹äº‘ï¼ˆ4DGSï¼‰æ˜¯ä¸€ç§ç”¨äºé‡å»ºåŠ¨æ€åœºæ™¯çš„æ–¹æ³•ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡å­˜å‚¨å¹¶ä¸”æ¸²æŸ“é€Ÿåº¦è¾ƒæ…¢ã€‚æœ¬æ–‡æ¢è®¨äº†å¯¼è‡´è¿™äº›é—®é¢˜çš„ä¸¤ä¸ªä¸»è¦æ¥æºï¼šçŸ­ç”Ÿå‘½å‘¨æœŸé«˜æ–¯å’Œéæ´»åŠ¨é«˜æ–¯ã€‚æˆ‘ä»¬æå‡ºäº†4DGS-1Kï¼Œé€šè¿‡å¼•å…¥ç©ºé—´-æ—¶é—´å˜åŒ–è¯„åˆ†æ¥æœ‰æ•ˆå»é™¤çŸ­ç”Ÿå‘½å‘¨æœŸé«˜æ–¯ï¼Œå¹¶å­˜å‚¨æ´»è·ƒé«˜æ–¯çš„æ©ç ï¼Œä»è€Œæ˜¾è‘—å‡å°‘å†—ä½™è®¡ç®—ã€‚ä¸ä¼ ç»Ÿ4DGSç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤æ‚åŠ¨æ€åœºæ™¯ä¸­å®ç°äº†41å€çš„å­˜å‚¨å‡å°‘å’Œ9å€çš„æ¸²æŸ“é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸ä¼¼çš„è§†è§‰è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16356",
            "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners",
            "url": "https://huggingface.co/papers/2503.16356",
            "abstract": "Knowledge Editing (KE) enables the modification of outdated or incorrect information in large language models (LLMs). While existing KE methods can update isolated facts, they struggle to generalize these updates to multi-hop reasoning tasks that depend on the modified knowledge. Through an analysis of reasoning circuits -- the neural pathways LLMs use for knowledge-based inference, we observe that current layer-localized KE approaches, such as MEMIT and WISE, which edit only single or a few model layers, struggle to effectively incorporate updated information into these reasoning pathways. To address this limitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method that enables more effective integration of updated knowledge in LLMs. CaKE leverages strategically curated data, guided by our circuits-based analysis, that enforces the model to utilize the modified knowledge, stimulating the model to develop appropriate reasoning circuits for newly integrated knowledge. Experimental results show that CaKE enables more accurate and consistent use of updated knowledge across related reasoning tasks, leading to an average of 20% improvement in multi-hop reasoning accuracy on MQuAKE dataset compared to existing KE methods. We release the code and data in https://github.com/zjunlp/CaKE.",
            "score": 11,
            "issue_id": 2823,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "8b74fed43c99c37d",
            "authors": [
                "Yunzhi Yao",
                "Jizhan Fang",
                "Jia-Chen Gu",
                "Ningyu Zhang",
                "Shumin Deng",
                "Huajun Chen",
                "Nanyun Peng"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of California, Los Angeles",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16356.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#reasoning",
                    "#dataset",
                    "#data"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CaKE: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ CaKE. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. CaKE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ»Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CaKE Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 20% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Knowledge Integration in Language Models with CaKE",
                    "desc": "This paper introduces CaKE (Circuit-aware Knowledge Editing), a new method for updating knowledge in large language models (LLMs). Current methods struggle with multi-hop reasoning tasks that require the integration of modified information. CaKE improves upon these methods by analyzing the neural pathways used for reasoning and ensuring that updated knowledge is effectively incorporated. The results show a significant increase in accuracy for reasoning tasks, demonstrating the effectiveness of this approach."
                },
                "zh": {
                    "title": "ç”µè·¯æ„ŸçŸ¥çŸ¥è¯†ç¼–è¾‘ï¼šæå‡å¤šè·³æ¨ç†çš„å‡†ç¡®æ€§",
                    "desc": "çŸ¥è¯†ç¼–è¾‘ï¼ˆKEï¼‰ä½¿å¾—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿä¿®æ”¹è¿‡æ—¶æˆ–ä¸æ­£ç¡®çš„ä¿¡æ¯ã€‚ç°æœ‰çš„KEæ–¹æ³•è™½ç„¶å¯ä»¥æ›´æ–°å­¤ç«‹çš„äº‹å®ï¼Œä½†åœ¨å¤šè·³æ¨ç†ä»»åŠ¡ä¸­å´éš¾ä»¥æœ‰æ•ˆæ¨å¹¿è¿™äº›æ›´æ–°ã€‚é€šè¿‡å¯¹æ¨ç†ç”µè·¯çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°å½“å‰çš„å±‚å±€éƒ¨KEæ–¹æ³•åœ¨æœ‰æ•ˆæ•´åˆæ›´æ–°ä¿¡æ¯æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CaKEï¼ˆç”µè·¯æ„ŸçŸ¥çŸ¥è¯†ç¼–è¾‘ï¼‰ï¼Œå®ƒé€šè¿‡ç²¾å¿ƒç­–åˆ’çš„æ•°æ®ï¼Œä¿ƒè¿›æ¨¡å‹åˆ©ç”¨ä¿®æ”¹åçš„çŸ¥è¯†ï¼Œä»è€Œæé«˜å¤šè·³æ¨ç†çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16322",
            "title": "Ultra-Resolution Adaptation with Ease",
            "url": "https://huggingface.co/papers/2503.16322",
            "abstract": "Text-to-image diffusion models have achieved remarkable progress in recent years. However, training models for high-resolution image generation remains challenging, particularly when training data and computational resources are limited. In this paper, we explore this practical problem from two key perspectives: data and parameter efficiency, and propose a set of key guidelines for ultra-resolution adaptation termed URAE. For data efficiency, we theoretically and empirically demonstrate that synthetic data generated by some teacher models can significantly promote training convergence. For parameter efficiency, we find that tuning minor components of the weight matrices outperforms widely-used low-rank adapters when synthetic data are unavailable, offering substantial performance gains while maintaining efficiency. Additionally, for models leveraging guidance distillation, such as FLUX, we show that disabling classifier-free guidance, i.e., setting the guidance scale to 1 during adaptation, is crucial for satisfactory performance. Extensive experiments validate that URAE achieves comparable 2K-generation performance to state-of-the-art closed-source models like FLUX1.1 [Pro] Ultra with only 3K samples and 2K iterations, while setting new benchmarks for 4K-resolution generation. Codes are available https://github.com/Huage001/URAE{here}.",
            "score": 11,
            "issue_id": 2824,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "48ce70d2a4cf0cf7",
            "authors": [
                "Ruonan Yu",
                "Songhua Liu",
                "Zhenxiong Tan",
                "Xinchao Wang"
            ],
            "affiliations": [
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16322.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#data",
                    "#diffusion",
                    "#cv",
                    "#synthetic",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ URAE Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²ĞµÑĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ URAE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 2K-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²ÑĞµĞ³Ğ¾ 3000 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸ 2000 Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 4K."
                },
                "en": {
                    "title": "Efficient High-Resolution Image Generation with URAE",
                    "desc": "This paper addresses the challenges of training text-to-image diffusion models for high-resolution image generation, especially when data and computational resources are limited. It introduces URAE, a set of guidelines focused on improving data and parameter efficiency. The authors demonstrate that synthetic data from teacher models can enhance training convergence, and that tuning specific components of weight matrices can yield better results than traditional low-rank adapters. Their experiments show that URAE can achieve high-quality 2K and 4K image generation with significantly fewer training samples and iterations compared to existing models."
                },
                "zh": {
                    "title": "é«˜æ•ˆç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä¸­çš„è®­ç»ƒæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®å’Œè®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€å¥—åä¸ºURAEçš„è¶…åˆ†è¾¨ç‡é€‚åº”æŒ‡å—ï¼Œæ—¨åœ¨æé«˜æ•°æ®å’Œå‚æ•°æ•ˆç‡ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒæŸäº›æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®å¯ä»¥æ˜¾è‘—ä¿ƒè¿›è®­ç»ƒæ”¶æ•›ï¼Œè€Œå¾®è°ƒæƒé‡çŸ©é˜µçš„å°‘é‡ç»„ä»¶åœ¨ç¼ºä¹åˆæˆæ•°æ®æ—¶è¡¨ç°ä¼˜äºå¸¸ç”¨çš„ä½ç§©é€‚é…å™¨ã€‚é€šè¿‡å¤§é‡å®éªŒéªŒè¯ï¼ŒURAEåœ¨ä»…ä½¿ç”¨3000ä¸ªæ ·æœ¬å’Œ2000æ¬¡è¿­ä»£çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†ä¸FLUX1.1ç­‰æœ€å…ˆè¿›é—­æºæ¨¡å‹ç›¸å½“çš„2Kç”Ÿæˆæ€§èƒ½ï¼Œå¹¶ä¸º4Kåˆ†è¾¨ç‡ç”Ÿæˆè®¾å®šäº†æ–°åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16057",
            "title": "Expert Race: A Flexible Routing Strategy for Scaling Diffusion\n  Transformer with Mixture of Experts",
            "url": "https://huggingface.co/papers/2503.16057",
            "abstract": "Diffusion models have emerged as mainstream framework in visual generation. Building upon this success, the integration of Mixture of Experts (MoE) methods has shown promise in enhancing model scalability and performance. In this paper, we introduce Race-DiT, a novel MoE model for diffusion transformers with a flexible routing strategy, Expert Race. By allowing tokens and experts to compete together and select the top candidates, the model learns to dynamically assign experts to critical tokens. Additionally, we propose per-layer regularization to address challenges in shallow layer learning, and router similarity loss to prevent mode collapse, ensuring better expert utilization. Extensive experiments on ImageNet validate the effectiveness of our approach, showcasing significant performance gains while promising scaling properties.",
            "score": 11,
            "issue_id": 2827,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "bb5d00b3068e881c",
            "authors": [
                "Yike Yuan",
                "Ziyu Wang",
                "Zihao Huang",
                "Defa Zhu",
                "Xun Zhou",
                "Jingyi Yu",
                "Qiyang Min"
            ],
            "affiliations": [
                "ByteDance Seed",
                "ShanghaiTech University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16057.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#diffusion",
                    "#cv",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸï¸",
                "ru": {
                    "title": "Race-DiT: Ğ“Ğ¾Ğ½ĞºĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Race-DiT - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Mixture of Experts (MoE) Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Expert Race. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ², Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ¼Ğ¾Ğ´. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ImageNet Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Race-DiT: Enhancing Diffusion Transformers with Expert Competition",
                    "desc": "This paper presents Race-DiT, a new model that combines diffusion transformers with Mixture of Experts (MoE) techniques to improve visual generation. The model features a flexible routing strategy called Expert Race, which allows tokens to compete and select the best experts for processing. To enhance learning in shallow layers, the authors introduce per-layer regularization and a router similarity loss to avoid mode collapse. Experimental results on ImageNet demonstrate that Race-DiT achieves significant performance improvements and scalability compared to previous models."
                },
                "zh": {
                    "title": "Race-DiTï¼šåŠ¨æ€åˆ†é…ä¸“å®¶çš„æ‰©æ•£æ¨¡å‹",
                    "desc": "æ‰©æ•£æ¨¡å‹å·²æˆä¸ºè§†è§‰ç”Ÿæˆçš„ä¸»æµæ¡†æ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ··åˆä¸“å®¶æ¨¡å‹Race-DiTï¼Œé‡‡ç”¨çµæ´»çš„è·¯ç”±ç­–ç•¥Expert Raceï¼Œä»¥æé«˜æ¨¡å‹çš„å¯æ‰©å±•æ€§å’Œæ€§èƒ½ã€‚è¯¥æ¨¡å‹é€šè¿‡è®©ä»¤ç‰Œå’Œä¸“å®¶ç«äº‰å¹¶é€‰æ‹©æœ€ä½³å€™é€‰è€…ï¼ŒåŠ¨æ€åˆ†é…ä¸“å®¶ç»™å…³é”®ä»¤ç‰Œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†æ¯å±‚æ­£åˆ™åŒ–å’Œè·¯ç”±å™¨ç›¸ä¼¼æ€§æŸå¤±ï¼Œä»¥è§£å†³æµ…å±‚å­¦ä¹ ä¸­çš„æŒ‘æˆ˜ï¼Œç¡®ä¿æ›´å¥½çš„ä¸“å®¶åˆ©ç”¨ç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16428",
            "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
            "url": "https://huggingface.co/papers/2503.16428",
            "abstract": "Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements. In this paper, we introduce XAttention, a plug-and-play framework that dramatically accelerates long-context inference in Transformers models using sparse attention. XAttention's key innovation is the insight that the sum of antidiagonal values (i.e., from the lower-left to upper-right) in the attention matrix provides a powerful proxy for block importance. This allows for precise identification and pruning of non-essential blocks, resulting in high sparsity and dramatically accelerated inference. Across comprehensive evaluations on demanding long-context benchmarks-including RULER and LongBench for language, VideoMME for video understanding, and VBench for video generation. XAttention achieves accuracy comparable to full attention while delivering substantial computational gains. We demonstrate up to 13.5x acceleration in attention computation. These results underscore XAttention's ability to unlock the practical potential of block sparse attention, paving the way for scalable and efficient deployment of LCTMs in real-world applications. Code is available at https://github.com/mit-han-lab/x-attention.",
            "score": 10,
            "issue_id": 2823,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "befbf726163c510a",
            "authors": [
                "Ruyi Xu",
                "Guangxuan Xiao",
                "Haofeng Huang",
                "Junxian Guo",
                "Song Han"
            ],
            "affiliations": [
                "Massachusetts Institute of Technology",
                "NVIDIA",
                "SJTU",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16428.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#architecture",
                    "#long_context",
                    "#optimization",
                    "#video",
                    "#benchmark"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "XAttention: Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "XAttention - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑƒĞ¼Ğ¼Ñ‹ Ğ°Ğ½Ñ‚Ğ¸Ğ´Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ñ‚ÑĞµĞºĞ°Ñ‚ÑŒ Ğ½ĞµÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. XAttention Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ¿Ñ€Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Accelerating Long-Context Transformers with XAttention",
                    "desc": "This paper presents XAttention, a new framework designed to improve the efficiency of long-context Transformer models by utilizing block-sparse attention. Traditional attention mechanisms are computationally expensive, but XAttention reduces this cost by focusing on important regions of the attention matrix. The key innovation is using the sum of antidiagonal values to identify and prune less important blocks, which maintains accuracy while enhancing speed. Evaluations show that XAttention can achieve up to 13.5 times faster attention computation without sacrificing performance, making it a significant advancement for deploying LCTMs in practical applications."
                },
                "zh": {
                    "title": "XAttentionï¼šåŠ é€Ÿé•¿ä¸Šä¸‹æ–‡æ¨ç†çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "é•¿ä¸Šä¸‹æ–‡å˜æ¢å™¨æ¨¡å‹ï¼ˆLCTMsï¼‰åœ¨å®é™…åº”ç”¨ä¸­éå¸¸é‡è¦ï¼Œä½†ç”±äºæ³¨æ„åŠ›æœºåˆ¶çš„å¹³æ–¹å¤æ‚åº¦ï¼Œè®¡ç®—æˆæœ¬å¾ˆé«˜ã€‚å—ç¨€ç–æ³¨æ„åŠ›é€šè¿‡é›†ä¸­è®¡ç®—åœ¨å…³é”®åŒºåŸŸæ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å¹³è¡¡å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†XAttentionï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨çš„æ¡†æ¶ï¼Œé€šè¿‡ç¨€ç–æ³¨æ„åŠ›æ˜¾è‘—åŠ é€Ÿå˜æ¢å™¨æ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡æ¨ç†ã€‚XAttentionçš„å…³é”®åˆ›æ–°åœ¨äºåˆ©ç”¨æ³¨æ„åŠ›çŸ©é˜µä¸­åå¯¹è§’çº¿å€¼çš„æ€»å’Œä½œä¸ºå—é‡è¦æ€§çš„å¼ºå¤§ä»£ç†ï¼Œä»è€Œå®ç°é«˜ç¨€ç–æ€§å’Œæ˜¾è‘—åŠ é€Ÿæ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16425",
            "title": "Tokenize Image as a Set",
            "url": "https://huggingface.co/papers/2503.16425",
            "abstract": "This paper proposes a fundamentally new paradigm for image generation through set-based tokenization and distribution modeling. Unlike conventional methods that serialize images into fixed-position latent codes with a uniform compression ratio, we introduce an unordered token set representation to dynamically allocate coding capacity based on regional semantic complexity. This TokenSet enhances global context aggregation and improves robustness against local perturbations. To address the critical challenge of modeling discrete sets, we devise a dual transformation mechanism that bijectively converts sets into fixed-length integer sequences with summation constraints. Further, we propose Fixed-Sum Discrete Diffusion--the first framework to simultaneously handle discrete values, fixed sequence length, and summation invariance--enabling effective set distribution modeling. Experiments demonstrate our method's superiority in semantic-aware representation and generation quality. Our innovations, spanning novel representation and modeling strategies, advance visual generation beyond traditional sequential token paradigms. Our code and models are publicly available at https://github.com/Gengzigang/TokenSet.",
            "score": 10,
            "issue_id": 2831,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "c960fe86078f8a7b",
            "authors": [
                "Zigang Geng",
                "Mengde Xu",
                "Han Hu",
                "Shuyang Gu"
            ],
            "affiliations": [
                "Tencent Hunyuan Research",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16425.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#open_source",
                    "#diffusion",
                    "#video"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¾Ñ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ°Ğ¼",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ½ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ ĞµĞ¼ĞºĞ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ±Ğ¸ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ² Ñ†ĞµĞ»Ğ¾Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° ÑÑƒĞ¼Ğ¼Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Image Generation with Dynamic Token Sets",
                    "desc": "This paper introduces a new way to generate images using a method called set-based tokenization and distribution modeling. Instead of using fixed-position codes, it uses an unordered set of tokens that can adapt based on the complexity of different image areas. This approach improves how the model understands the overall context of the image and makes it more resilient to small changes. The authors also present a new framework called Fixed-Sum Discrete Diffusion, which effectively manages discrete values and maintains certain constraints, leading to better image generation results."
                },
                "zh": {
                    "title": "å›¾åƒç”Ÿæˆçš„æ–°èŒƒå¼ï¼šé›†åˆæ ‡è®°åŒ–ä¸åˆ†å¸ƒå»ºæ¨¡",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ–°çš„å›¾åƒç”ŸæˆèŒƒå¼ï¼Œé€šè¿‡åŸºäºé›†åˆçš„æ ‡è®°åŒ–å’Œåˆ†å¸ƒå»ºæ¨¡æ¥å®ç°ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•å°†å›¾åƒåºåˆ—åŒ–ä¸ºå›ºå®šä½ç½®çš„æ½œåœ¨ç¼–ç ä¸åŒï¼Œæˆ‘ä»¬å¼•å…¥äº†æ— åºçš„æ ‡è®°é›†åˆè¡¨ç¤ºï¼Œèƒ½å¤Ÿæ ¹æ®åŒºåŸŸè¯­ä¹‰å¤æ‚æ€§åŠ¨æ€åˆ†é…ç¼–ç èƒ½åŠ›ã€‚è¯¥TokenSetå¢å¼ºäº†å…¨å±€ä¸Šä¸‹æ–‡èšåˆèƒ½åŠ›ï¼Œå¹¶æé«˜äº†å¯¹å±€éƒ¨æ‰°åŠ¨çš„é²æ£’æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†å›ºå®šå’Œç¦»æ•£æ‰©æ•£æ¡†æ¶ï¼Œé¦–æ¬¡åŒæ—¶å¤„ç†ç¦»æ•£å€¼ã€å›ºå®šåºåˆ—é•¿åº¦å’Œå’Œä¸å˜æ€§ï¼Œä»è€Œæœ‰æ•ˆåœ°å»ºæ¨¡é›†åˆåˆ†å¸ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15851",
            "title": "Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video\n  Diffusion",
            "url": "https://huggingface.co/papers/2503.15851",
            "abstract": "Animatable head avatar generation typically requires extensive data for training. To reduce the data requirements, a natural solution is to leverage existing data-free static avatar generation methods, such as pre-trained diffusion models with score distillation sampling (SDS), which align avatars with pseudo ground-truth outputs from the diffusion model. However, directly distilling 4D avatars from video diffusion often leads to over-smooth results due to spatial and temporal inconsistencies in the generated video. To address this issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial and temporal consistency dataset for 4D avatar reconstruction using the video diffusion model. Specifically, Zero-1-to-A iteratively constructs video datasets and optimizes animatable avatars in a progressive manner, ensuring that avatar quality increases smoothly and consistently throughout the learning process. This progressive learning involves two stages: (1) Spatial Consistency Learning fixes expressions and learns from front-to-side views, and (2) Temporal Consistency Learning fixes views and learns from relaxed to exaggerated expressions, generating 4D avatars in a simple-to-complex manner. Extensive experiments demonstrate that Zero-1-to-A improves fidelity, animation quality, and rendering speed compared to existing diffusion-based methods, providing a solution for lifelike avatar creation. Code is publicly available at: https://github.com/ZhenglinZhou/Zero-1-to-A.",
            "score": 9,
            "issue_id": 2826,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "470193a07d663d04",
            "authors": [
                "Zhou Zhenglin",
                "Ma Fan",
                "Fan Hehe",
                "Chua Tat-Seng"
            ],
            "affiliations": [
                "National University of Singapore",
                "ReLER, CCAI, Zhejiang University",
                "State Key Laboratory of Brain-machine Intelligence, Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15851.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#diffusion",
                    "#dataset",
                    "#synthetic",
                    "#video",
                    "#open_source"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Zero-1-to-A: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Zero-1-to-A Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ¾ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ (Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€Ğ°ĞºÑƒÑ€ÑÑ‹) Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ (Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°ĞºÑƒÑ€ÑÑ‹, Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Zero-1-to-A ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing 4D Avatar Generation with Zero-1-to-A",
                    "desc": "This paper presents Zero-1-to-A, a novel approach for generating animatable 4D avatars using video diffusion models while minimizing data requirements. The method addresses the challenge of over-smoothing in avatar generation by creating a dataset that ensures both spatial and temporal consistency. It employs a two-stage progressive learning process: first, it focuses on spatial consistency by learning from different views, and then it enhances temporal consistency by varying expressions. The results show that Zero-1-to-A significantly improves the quality and speed of avatar rendering compared to traditional methods."
                },
                "zh": {
                    "title": "é«˜æ•ˆç”ŸæˆçœŸå®æ„Ÿ4Då¤´åƒçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºZero-1-to-Açš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå¯åŠ¨ç”»çš„4Då¤´åƒï¼Œæ—¨åœ¨å‡å°‘å¯¹å¤§é‡è®­ç»ƒæ•°æ®çš„éœ€æ±‚ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡æ„å»ºç©ºé—´å’Œæ—¶é—´ä¸€è‡´æ€§çš„æ•°æ®é›†ï¼Œé€æ­¥ä¼˜åŒ–å¤´åƒçš„è´¨é‡ã€‚Zero-1-to-Açš„å­¦ä¹ è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç©ºé—´ä¸€è‡´æ€§å­¦ä¹ å’Œæ—¶é—´ä¸€è‡´æ€§å­¦ä¹ ï¼Œç¡®ä¿å¤´åƒåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è´¨é‡å¹³æ»‘æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤´åƒçš„çœŸå®æ„Ÿã€åŠ¨ç”»è´¨é‡å’Œæ¸²æŸ“é€Ÿåº¦ä¸Šä¼˜äºç°æœ‰çš„æ‰©æ•£æ¨¡å‹æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16421",
            "title": "MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance",
            "url": "https://huggingface.co/papers/2503.16421",
            "abstract": "Recent advances in video generation have led to remarkable improvements in visual quality and temporal coherence. Upon this, trajectory-controllable video generation has emerged to enable precise object motion control through explicitly defined spatial paths. However, existing methods struggle with complex object movements and multi-object motion control, resulting in imprecise trajectory adherence, poor object consistency, and compromised visual quality. Furthermore, these methods only support trajectory control in a single format, limiting their applicability in diverse scenarios. Additionally, there is no publicly available dataset or benchmark specifically tailored for trajectory-controllable video generation, hindering robust training and systematic evaluation. To address these challenges, we introduce MagicMotion, a novel image-to-video generation framework that enables trajectory control through three levels of conditions from dense to sparse: masks, bounding boxes, and sparse boxes. Given an input image and trajectories, MagicMotion seamlessly animates objects along defined trajectories while maintaining object consistency and visual quality. Furthermore, we present MagicData, a large-scale trajectory-controlled video dataset, along with an automated pipeline for annotation and filtering. We also introduce MagicBench, a comprehensive benchmark that assesses both video quality and trajectory control accuracy across different numbers of objects. Extensive experiments demonstrate that MagicMotion outperforms previous methods across various metrics. Our project page are publicly available at https://quanhaol.github.io/magicmotion-site.",
            "score": 8,
            "issue_id": 2823,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "ae5c7f7abb796973",
            "authors": [
                "Quanhao Li",
                "Zhen Xing",
                "Rui Wang",
                "Hui Zhang",
                "Qi Dai",
                "Zuxuan Wu"
            ],
            "affiliations": [
                "Fudan University",
                "Microsoft Research Asia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16421.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#video",
                    "#games",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "MagicMotion: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "MagicMotion - Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹: Ğ¼Ğ°ÑĞºĞ¸, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ¼ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ MagicData - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ MagicBench - Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MagicMotion Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼."
                },
                "en": {
                    "title": "MagicMotion: Mastering Object Motion in Video Generation",
                    "desc": "This paper presents MagicMotion, a new framework for generating videos from images while allowing precise control over object movements along defined paths. It addresses the limitations of existing methods that struggle with complex and multi-object trajectories, which often lead to poor visual quality and object consistency. MagicMotion introduces a tiered approach to trajectory control using masks, bounding boxes, and sparse boxes, enhancing flexibility and applicability. Additionally, the authors provide a new dataset, MagicData, and a benchmark, MagicBench, to facilitate better training and evaluation of trajectory-controllable video generation."
                },
                "zh": {
                    "title": "MagicMotionï¼šç²¾å‡†è½¨è¿¹æ§åˆ¶çš„è§†é¢‘ç”Ÿæˆæ–°æ¡†æ¶",
                    "desc": "æœ€è¿‘è§†é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæå‡äº†è§†è§‰è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚é’ˆå¯¹å¤æ‚ç‰©ä½“è¿åŠ¨å’Œå¤šç‰©ä½“è¿åŠ¨æ§åˆ¶çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†MagicMotionæ¡†æ¶ï¼Œæ”¯æŒé€šè¿‡ä¸åŒå±‚æ¬¡çš„æ¡ä»¶ï¼ˆå¦‚æ©ç ã€è¾¹ç•Œæ¡†å’Œç¨€ç–æ¡†ï¼‰è¿›è¡Œè½¨è¿¹æ§åˆ¶ã€‚MagicMotionèƒ½å¤Ÿåœ¨ä¿æŒç‰©ä½“ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡çš„åŒæ—¶ï¼Œæ²¿ç€å®šä¹‰çš„è½¨è¿¹æ— ç¼åŠ¨ç”»åŒ–ç‰©ä½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†MagicDataæ•°æ®é›†å’ŒMagicBenchåŸºå‡†ï¼Œä¿ƒè¿›äº†è½¨è¿¹æ§åˆ¶è§†é¢‘ç”Ÿæˆçš„ç ”ç©¶å’Œè¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16375",
            "title": "NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes",
            "url": "https://huggingface.co/papers/2503.16375",
            "abstract": "In this paper, we explore the task of generating expansive outdoor scenes, ranging from castles to high-rises. Unlike indoor scene generation, which has been a primary focus of prior work, outdoor scene generation presents unique challenges, including wide variations in scene heights and the need for a method capable of rapidly producing large landscapes. To address this, we propose an efficient approach that encodes scene chunks as uniform vector sets, offering better compression and performance than the spatially structured latents used in prior methods. Furthermore, we train an explicit outpainting model for unbounded generation, which improves coherence compared to prior resampling-based inpainting schemes while also speeding up generation by eliminating extra diffusion steps. To facilitate this task, we curate NuiScene43, a small but high-quality set of scenes, preprocessed for joint training. Notably, when trained on scenes of varying styles, our model can blend different environments, such as rural houses and city skyscrapers, within the same scene, highlighting the potential of our curation process to leverage heterogeneous scenes for joint training.",
            "score": 8,
            "issue_id": 2829,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "bd1b53cbfef36cbc",
            "authors": [
                "Han-Hung Lee",
                "Qinghong Han",
                "Angel X. Chang"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair, Amii",
                "Simon Fraser University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16375.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#diffusion",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ†ĞµĞ½: Ğ¾Ñ‚ Ğ·Ğ°Ğ¼ĞºĞ¾Ğ² Ğ´Ğ¾ Ğ½ĞµĞ±Ğ¾ÑĞºÑ€ĞµĞ±Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑÑ†ĞµĞ½, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ¼ĞºĞ¸ Ğ¸ Ğ½ĞµĞ±Ğ¾ÑĞºÑ€ĞµĞ±Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ñ‹ ÑÑ†ĞµĞ½Ñ‹ ĞºĞ°Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ½ĞµĞ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‰Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… NuiScene43, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ¸Ğ»Ğ¸ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ†ĞµĞ½Ğµ."
                },
                "en": {
                    "title": "Efficient Outdoor Scene Generation with Unified Vector Encoding",
                    "desc": "This paper presents a novel approach to generating expansive outdoor scenes, addressing the unique challenges posed by varying heights and large landscapes. The authors introduce a method that encodes scene chunks as uniform vector sets, which enhances compression and performance compared to previous spatially structured latents. They also develop an explicit outpainting model that allows for unbounded scene generation, improving coherence and speeding up the process by reducing diffusion steps. Additionally, the creation of the NuiScene43 dataset enables the model to effectively blend different environmental styles, showcasing the benefits of joint training on diverse scenes."
                },
                "zh": {
                    "title": "é«˜æ•ˆç”Ÿæˆå¹¿é˜”æˆ·å¤–åœºæ™¯çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†ç”Ÿæˆå¹¿é˜”æˆ·å¤–åœºæ™¯çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬åŸå ¡å’Œé«˜æ¥¼ç­‰ã€‚ä¸ä»¥å¾€ä¸»è¦å…³æ³¨çš„å®¤å†…åœºæ™¯ç”Ÿæˆä¸åŒï¼Œæˆ·å¤–åœºæ™¯ç”Ÿæˆé¢ä¸´ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå¦‚åœºæ™¯é«˜åº¦çš„å¹¿æ³›å˜åŒ–å’Œå¿«é€Ÿç”Ÿæˆå¤§è§„æ¨¡æ™¯è§‚çš„æ–¹æ³•éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ–¹æ³•ï¼Œå°†åœºæ™¯å—ç¼–ç ä¸ºç»Ÿä¸€çš„å‘é‡é›†ï¼Œè¿™æ¯”ä»¥å¾€æ–¹æ³•ä¸­ä½¿ç”¨çš„ç©ºé—´ç»“æ„æ½œå˜é‡æä¾›äº†æ›´å¥½çš„å‹ç¼©å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæ˜¾å¼çš„å¤–æ‰©æ¨¡å‹ï¼Œä»¥å®ç°æ— ç•Œç”Ÿæˆï¼Œæ”¹å–„äº†ä¸ä»¥å¾€åŸºäºé‡é‡‡æ ·çš„ä¿®è¡¥æ–¹æ¡ˆç›¸æ¯”çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶é€šè¿‡æ¶ˆé™¤é¢å¤–çš„æ‰©æ•£æ­¥éª¤åŠ å¿«äº†ç”Ÿæˆé€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16055",
            "title": "SALT: Singular Value Adaptation with Low-Rank Transformation",
            "url": "https://huggingface.co/papers/2503.16055",
            "abstract": "The complex nature of medical image segmentation calls for models that are specifically designed to capture detailed, domain-specific features. Large foundation models offer considerable flexibility, yet the cost of fine-tuning these models remains a significant barrier. Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model weights with low-rank matrices but may suffer from underfitting when the chosen rank is insufficient to capture domain-specific nuances. Conversely, full-rank Singular Value Decomposition (SVD) based methods provide comprehensive updates by modifying all singular values, yet they often lack flexibility and exhibit variable performance across datasets. We propose SALT (Singular Value Adaptation with Low-Rank Transformation), a method that selectively adapts the most influential singular values using trainable scale and shift parameters while complementing this with a low-rank update for the remaining subspace. This hybrid approach harnesses the advantages of both LoRA and SVD, enabling effective adaptation without relying on increasing model size or depth. Evaluated on 5 challenging medical datasets, ranging from as few as 20 samples to 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in Dice with only 3.9% trainable parameters, demonstrating robust adaptation even in low-resource settings. The code for SALT is available at: https://github.com/BioMedIA-MBZUAI/SALT",
            "score": 8,
            "issue_id": 2827,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "1b4a367b51eed08e",
            "authors": [
                "Abdelrahman Elsayed",
                "Sarim Hashmi",
                "Mohammed Elseiagy",
                "Hu Wang",
                "Mohammad Yaqub",
                "Ibrahim Almakky"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16055.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#low_resource",
                    "#healthcare",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "SALT: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SALT Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. SALT ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² LoRA Ğ¸ SVD, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ğ»Ğ¸ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞµĞ³Ğ¾ÑÑ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸-ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ½Ğ° 5 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. SALT Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ Dice Ğ½Ğ° 2-5% Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 3.9% Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "SALT: Smart Adaptation for Medical Image Segmentation",
                    "desc": "This paper introduces SALT, a novel method for fine-tuning large foundation models specifically for medical image segmentation. SALT combines the strengths of Low-Rank Adaptation (LoRA) and full-rank Singular Value Decomposition (SVD) by selectively adapting the most important singular values while applying low-rank updates to the rest. This approach allows for effective model adaptation without significantly increasing the number of trainable parameters. The results show that SALT outperforms existing Parameter-Efficient Fine-Tuning methods on various medical datasets, demonstrating its effectiveness even with limited training data."
                },
                "zh": {
                    "title": "SALTï¼šé«˜æ•ˆåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°æ–¹æ³•",
                    "desc": "åŒ»å­¦å›¾åƒåˆ†å‰²çš„å¤æ‚æ€§éœ€è¦ä¸“é—¨è®¾è®¡çš„æ¨¡å‹æ¥æ•æ‰è¯¦ç»†çš„é¢†åŸŸç‰¹å¾ã€‚è™½ç„¶å¤§å‹åŸºç¡€æ¨¡å‹æä¾›äº†çµæ´»æ€§ï¼Œä½†å¾®è°ƒè¿™äº›æ¨¡å‹çš„æˆæœ¬ä»ç„¶æ˜¯ä¸€ä¸ªé‡è¦éšœç¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSALTçš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡å¯è®­ç»ƒçš„ç¼©æ”¾å’Œåç§»å‚æ•°é€‰æ‹©æ€§åœ°é€‚åº”æœ€å…·å½±å“åŠ›çš„å¥‡å¼‚å€¼ï¼ŒåŒæ—¶å¯¹å…¶ä½™å­ç©ºé—´è¿›è¡Œä½ç§©æ›´æ–°ã€‚è¿™ç§æ··åˆæ–¹æ³•ç»“åˆäº†LoRAå’ŒSVDçš„ä¼˜ç‚¹ï¼Œåœ¨ä½èµ„æºç¯å¢ƒä¸­ä¹Ÿèƒ½å®ç°æœ‰æ•ˆçš„é€‚åº”ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16429",
            "title": "Sonata: Self-Supervised Learning of Reliable Point Representations",
            "url": "https://huggingface.co/papers/2503.16429",
            "abstract": "In this paper, we question whether we have a reliable self-supervised point cloud model that can be used for diverse 3D tasks via simple linear probing, even with limited data and minimal computation. We find that existing 3D self-supervised learning approaches fall short when evaluated on representation quality through linear probing. We hypothesize that this is due to what we term the \"geometric shortcut\", which causes representations to collapse to low-level spatial features. This challenge is unique to 3D and arises from the sparse nature of point cloud data. We address it through two key strategies: obscuring spatial information and enhancing the reliance on input features, ultimately composing a Sonata of 140k point clouds through self-distillation. Sonata is simple and intuitive, yet its learned representations are strong and reliable: zero-shot visualizations demonstrate semantic grouping, alongside strong spatial reasoning through nearest-neighbor relationships. Sonata demonstrates exceptional parameter and data efficiency, tripling linear probing accuracy (from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1% of the data compared to previous approaches. Full fine-tuning further advances SOTA across both 3D indoor and outdoor perception tasks.",
            "score": 7,
            "issue_id": 2833,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "cab3efeaf1688094",
            "authors": [
                "Xiaoyang Wu",
                "Daniel DeTone",
                "Duncan Frost",
                "Tianwei Shen",
                "Chris Xie",
                "Nan Yang",
                "Jakob Engel",
                "Richard Newcombe",
                "Hengshuang Zhao",
                "Julian Straub"
            ],
            "affiliations": [
                "Meta Reality Labs Research",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16429.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#optimization",
                    "#transfer_learning"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Sonata: Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ€Ğ° ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ 3D Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ°Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² 3D, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² 3D Ğ½Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ·-Ğ·Ğ° Ñ‚Ğ°Ğº Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ \"Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ\". Ğ­Ñ‚Ğ¾ ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ²ÑĞ·Ğ°Ğ½Ğ¾ Ñ Ñ‚ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ´ÑÑ‚ÑÑ Ğº Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼ Ğ¸Ğ·-Ğ·Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Sonata, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking 3D Potential: Sonata's Self-Supervised Breakthrough",
                    "desc": "This paper investigates the effectiveness of self-supervised learning models for point clouds in 3D tasks, particularly focusing on their performance with limited data. The authors identify a problem called the 'geometric shortcut', which leads to poor representation quality due to reliance on low-level spatial features. To overcome this, they propose a method called Sonata, which obscures spatial information and emphasizes input features, resulting in improved representation learning. Sonata achieves significant gains in linear probing accuracy and demonstrates strong performance in both zero-shot visualizations and full fine-tuning across various 3D perception tasks."
                },
                "zh": {
                    "title": "Sonataï¼šé«˜æ•ˆçš„è‡ªç›‘ç£3Dç‚¹äº‘å­¦ä¹ ",
                    "desc": "æœ¬æ–‡è´¨ç–‘ç°æœ‰çš„è‡ªç›‘ç£ç‚¹äº‘æ¨¡å‹åœ¨æœ‰é™æ•°æ®å’Œæœ€å°è®¡ç®—ä¸‹æ˜¯å¦å¯é ï¼Œèƒ½å¦ç”¨äºå¤šæ ·çš„3Dä»»åŠ¡ã€‚æˆ‘ä»¬å‘ç°ç°æœ‰çš„3Dè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨é€šè¿‡çº¿æ€§æ¢æµ‹è¯„ä¼°è¡¨ç¤ºè´¨é‡æ—¶è¡¨ç°ä¸ä½³ï¼ŒåŸå› åœ¨äºæ‰€è°“çš„â€œå‡ ä½•æ·å¾„â€ï¼Œå¯¼è‡´è¡¨ç¤ºé€€åŒ–ä¸ºä½çº§ç©ºé—´ç‰¹å¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§å…³é”®ç­–ç•¥æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼šé®è”½ç©ºé—´ä¿¡æ¯å’Œå¢å¼ºå¯¹è¾“å…¥ç‰¹å¾çš„ä¾èµ–ï¼Œæœ€ç»ˆé€šè¿‡è‡ªè’¸é¦æ„å»ºäº†ä¸€ä¸ªåŒ…å«14ä¸‡ä¸ªç‚¹äº‘çš„Sonataæ¨¡å‹ã€‚Sonataåœ¨å‚æ•°å’Œæ•°æ®æ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œåœ¨çº¿æ€§æ¢æµ‹å‡†ç¡®ç‡ä¸Šæå‡äº†ä¸‰å€ï¼Œå¹¶åœ¨å…¨å¾®è°ƒä¸­è¿›ä¸€æ­¥æ¨åŠ¨äº†3Dæ„ŸçŸ¥ä»»åŠ¡çš„æœ€æ–°è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15451",
            "title": "MotionStreamer: Streaming Motion Generation via Diffusion-based\n  Autoregressive Model in Causal Latent Space",
            "url": "https://huggingface.co/papers/2503.15451",
            "abstract": "This paper addresses the challenge of text-conditioned streaming motion generation, which requires us to predict the next-step human pose based on variable-length historical motions and incoming texts. Existing methods struggle to achieve streaming motion generation, e.g., diffusion models are constrained by pre-defined motion lengths, while GPT-based methods suffer from delayed response and error accumulation problem due to discretized non-causal tokenization. To solve these problems, we propose MotionStreamer, a novel framework that incorporates a continuous causal latent space into a probabilistic autoregressive model. The continuous latents mitigate information loss caused by discretization and effectively reduce error accumulation during long-term autoregressive generation. In addition, by establishing temporal causal dependencies between current and historical motion latents, our model fully utilizes the available information to achieve accurate online motion decoding. Experiments show that our method outperforms existing approaches while offering more applications, including multi-round generation, long-term generation, and dynamic motion composition. Project Page: https://zju3dv.github.io/MotionStreamer/",
            "score": 7,
            "issue_id": 2827,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 19",
                "zh": "3æœˆ19æ—¥"
            },
            "hash": "014f7568fd194ca1",
            "authors": [
                "Lixing Xiao",
                "Shunlin Lu",
                "Huaijin Pi",
                "Ke Fan",
                "Liang Pan",
                "Yueer Zhou",
                "Ziyong Feng",
                "Xiaowei Zhou",
                "Sida Peng",
                "Jingbo Wang"
            ],
            "affiliations": [
                "DeepGlint",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong, Shenzhen",
                "The University of Hong Kong",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15451.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#architecture",
                    "#games",
                    "#video",
                    "#long_context"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞŸĞ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MotionStreamer - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², MotionStreamer Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ² Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Real-Time Motion Generation with Text Input",
                    "desc": "This paper introduces MotionStreamer, a new framework for generating human motion in real-time based on text inputs. It addresses limitations of previous methods, such as diffusion models and GPT-based approaches, which struggle with fixed motion lengths and error accumulation. By using a continuous causal latent space, MotionStreamer reduces information loss and improves the accuracy of long-term motion generation. The framework also leverages temporal dependencies to enhance online motion decoding, demonstrating superior performance in various applications."
                },
                "zh": {
                    "title": "æµå¼è¿åŠ¨ç”Ÿæˆçš„æ–°çªç ´ï¼šMotionStreamer",
                    "desc": "æœ¬æ–‡è§£å†³äº†åŸºäºæ–‡æœ¬çš„æµå¼è¿åŠ¨ç”Ÿæˆé—®é¢˜ï¼Œæ—¨åœ¨æ ¹æ®å¯å˜é•¿åº¦çš„å†å²åŠ¨ä½œå’Œè¾“å…¥æ–‡æœ¬é¢„æµ‹ä¸‹ä¸€æ­¥çš„äººä½“å§¿æ€ã€‚ç°æœ‰æ–¹æ³•åœ¨æµå¼è¿åŠ¨ç”Ÿæˆæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä¾‹å¦‚æ‰©æ•£æ¨¡å‹å—åˆ°é¢„å®šä¹‰è¿åŠ¨é•¿åº¦çš„é™åˆ¶ï¼Œè€ŒåŸºäºGPTçš„æ–¹æ³•ç”±äºç¦»æ•£åŒ–çš„éå› æœæ ‡è®°åŒ–è€Œé¢ä¸´å“åº”å»¶è¿Ÿå’Œé”™è¯¯ç´¯ç§¯çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MotionStreamerï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå°†è¿ç»­å› æœæ½œåœ¨ç©ºé—´èå…¥æ¦‚ç‡è‡ªå›å½’æ¨¡å‹ä¸­ã€‚é€šè¿‡å»ºç«‹å½“å‰å’Œå†å²è¿åŠ¨æ½œåœ¨ä¹‹é—´çš„æ—¶é—´å› æœä¾èµ–å…³ç³»ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å……åˆ†åˆ©ç”¨å¯ç”¨ä¿¡æ¯ï¼Œå®ç°å‡†ç¡®çš„åœ¨çº¿è¿åŠ¨è§£ç ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15242",
            "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?",
            "url": "https://huggingface.co/papers/2503.15242",
            "abstract": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the capabilities of generative language models in understanding and generating code with specified time and space complexities. This benchmark addresses the gap in current evaluations that often overlook the ability of models to comprehend and produce code constrained by computational complexity. BigO(Bench) includes tooling to infer the algorithmic complexity of any Python function from profiling measurements, including human- or LLM-generated solutions. BigO(Bench) also includes of set of 3,105 coding problems and 1,190,250 solutions from Code Contests annotated with inferred (synthetic) time and space complexity labels from the complexity framework, as well as corresponding runtime and memory footprint values for a large set of input sizes. We present results from evaluating multiple state-of-the-art language models on this benchmark, highlighting their strengths and weaknesses in handling complexity requirements. In particular, token-space reasoning models are unrivaled in code generation but not in complexity understanding, hinting that they may not generalize well to tasks for which no reward was given at training time.",
            "score": 7,
            "issue_id": 2831,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 19",
                "zh": "3æœˆ19æ—¥"
            },
            "hash": "bac5c66c1f05b167",
            "authors": [
                "Pierre Chambon",
                "Baptiste Roziere",
                "Benoit Sagot",
                "Gabriel Synnaeve"
            ],
            "affiliations": [
                "FAIR at Meta",
                "Inria",
                "Mistral AI"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15242.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#benchmark",
                    "#dataset",
                    "#plp",
                    "#reasoning"
                ],
                "emoji": "â±ï¸",
                "ru": {
                    "title": "BigO(Bench): ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸",
                    "desc": "BigO(Bench) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ benchmark Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Python-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 3,105 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ 1,190,250 Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ğµ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒÑÑ‚ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ½Ğ¾ Ğ½Ğµ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Evaluating Code Complexity with BigO(Bench)",
                    "desc": "BigO(Bench) is a new benchmark created to test how well generative language models can understand and generate code while considering time and space complexities. It fills a gap in existing evaluations by focusing on the models' ability to handle computational constraints. The benchmark includes tools to analyze the algorithmic complexity of Python functions and features a large dataset of coding problems and solutions with annotated complexity labels. Results show that while some models excel at generating code, they struggle with understanding complexity, indicating potential limitations in their generalization capabilities."
                },
                "zh": {
                    "title": "è¯„ä¼°ç”Ÿæˆæ¨¡å‹çš„å¤æ‚åº¦ç†è§£èƒ½åŠ›",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†BigO(Bench)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„ç¼–ç åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°ç”Ÿæˆè¯­è¨€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆå…·æœ‰ç‰¹å®šæ—¶é—´å’Œç©ºé—´å¤æ‚åº¦çš„ä»£ç æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†å¡«è¡¥äº†å½“å‰è¯„ä¼°ä¸­å¸¸å¸¸å¿½è§†æ¨¡å‹åœ¨è®¡ç®—å¤æ‚åº¦é™åˆ¶ä¸‹ç†è§£å’Œç”Ÿæˆä»£ç çš„èƒ½åŠ›çš„ç©ºç™½ã€‚BigO(Bench)åŒ…æ‹¬å·¥å…·ï¼Œå¯ä»¥ä»æ€§èƒ½æµ‹é‡ä¸­æ¨æ–­ä»»ä½•Pythonå‡½æ•°çš„ç®—æ³•å¤æ‚åº¦ï¼Œå¹¶åŒ…å«æ¥è‡ªä»£ç ç«èµ›çš„3,105ä¸ªç¼–ç é—®é¢˜å’Œ1,190,250ä¸ªè§£å†³æ–¹æ¡ˆï¼Œæ ‡æ³¨äº†æ¨æ–­çš„æ—¶é—´å’Œç©ºé—´å¤æ‚åº¦æ ‡ç­¾ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¯¹å¤šä¸ªæœ€å…ˆè¿›è¯­è¨€æ¨¡å‹åœ¨è¯¥åŸºå‡†ä¸Šçš„è¯„ä¼°ç»“æœï¼Œçªå‡ºäº†å®ƒä»¬åœ¨å¤„ç†å¤æ‚æ€§è¦æ±‚æ–¹é¢çš„ä¼˜ç¼ºç‚¹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13356",
            "title": "Agents Play Thousands of 3D Video Games",
            "url": "https://huggingface.co/papers/2503.13356",
            "abstract": "We present PORTAL, a novel framework for developing artificial intelligence agents capable of playing thousands of 3D video games through language-guided policy generation. By transforming decision-making problems into language modeling tasks, our approach leverages large language models (LLMs) to generate behavior trees represented in domain-specific language (DSL). This method eliminates the computational burden associated with traditional reinforcement learning approaches while preserving strategic depth and rapid adaptability. Our framework introduces a hybrid policy structure that combines rule-based nodes with neural network components, enabling both high-level strategic reasoning and precise low-level control. A dual-feedback mechanism incorporating quantitative game metrics and vision-language model analysis facilitates iterative policy improvement at both tactical and strategic levels. The resulting policies are instantaneously deployable, human-interpretable, and capable of generalizing across diverse gaming environments. Experimental results demonstrate PORTAL's effectiveness across thousands of first-person shooter (FPS) games, showcasing significant improvements in development efficiency, policy generalization, and behavior diversity compared to traditional approaches. PORTAL represents a significant advancement in game AI development, offering a practical solution for creating sophisticated agents that can operate across thousands of commercial video games with minimal development overhead. Experiment results on the 3D video games are best viewed on https://zhongwen.one/projects/portal .",
            "score": 7,
            "issue_id": 2835,
            "pub_date": "2025-03-17",
            "pub_date_card": {
                "ru": "17 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 17",
                "zh": "3æœˆ17æ—¥"
            },
            "hash": "950044773aa39963",
            "authors": [
                "Zhongwen Xu",
                "Xianliang Wang",
                "Siyi Li",
                "Tao Yu",
                "Liang Wang",
                "Qiang Fu",
                "Wei Yang"
            ],
            "affiliations": [
                "Tencent"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13356.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#interpretability",
                    "#video",
                    "#3d",
                    "#games",
                    "#reasoning",
                    "#agi",
                    "#agents"
                ],
                "emoji": "ğŸ®",
                "ru": {
                    "title": "Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ¾Ñ€ÑÑÑ‚ Ğ¼Ğ¸Ñ€ 3D-Ğ¸Ğ³Ñ€",
                    "desc": "PORTAL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ‚ÑŒ Ğ² Ñ‚Ñ‹ÑÑÑ‡Ğ¸ 3D-Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ² Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ ÑƒĞ·Ğ»Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. PORTAL Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "PORTAL: Revolutionizing Game AI with Language-Guided Policies",
                    "desc": "PORTAL is a new framework that enables AI agents to play many 3D video games by using language to guide their decision-making. It turns complex decision problems into tasks that large language models can handle, generating behavior trees in a specific language. This approach reduces the heavy computational needs of traditional reinforcement learning while maintaining strategic depth and adaptability. By combining rule-based and neural network elements, PORTAL allows for both high-level strategy and detailed control, making it easier to create versatile game-playing agents."
                },
                "zh": {
                    "title": "PORTALï¼šæ¸¸æˆAIçš„æ–°çºªå…ƒ",
                    "desc": "PORTALæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¼€å‘èƒ½å¤Ÿé€šè¿‡è¯­è¨€å¼•å¯¼ç­–ç•¥ç”Ÿæˆæ¥ç©æ•°åƒæ¬¾3Dè§†é¢‘æ¸¸æˆçš„äººå·¥æ™ºèƒ½ä»£ç†ã€‚è¯¥æ–¹æ³•å°†å†³ç­–é—®é¢˜è½¬åŒ–ä¸ºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆä»¥é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ˆDSLï¼‰è¡¨ç¤ºçš„è¡Œä¸ºæ ‘ã€‚é€šè¿‡ç»“åˆåŸºäºè§„åˆ™çš„èŠ‚ç‚¹å’Œç¥ç»ç½‘ç»œç»„ä»¶ï¼ŒPORTALå®ç°äº†é«˜å±‚æ¬¡çš„æˆ˜ç•¥æ¨ç†å’Œç²¾ç¡®çš„ä½å±‚æ¬¡æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPORTALåœ¨æ•°åƒæ¬¾ç¬¬ä¸€äººç§°å°„å‡»æ¸¸æˆä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„å¼€å‘æ•ˆç‡æå‡ã€ç­–ç•¥æ³›åŒ–å’Œè¡Œä¸ºå¤šæ ·æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16188",
            "title": "CLS-RL: Image Classification with Rule-Based Reinforcement Learning",
            "url": "https://huggingface.co/papers/2503.16188",
            "abstract": "Classification is a core task in machine learning. Recent research has shown that although Multimodal Large Language Models (MLLMs) are initially poor at image classification, fine-tuning them with an adequate amount of data can significantly enhance their performance, making them comparable to SOTA classification models. However, acquiring large-scale labeled data is expensive. In this paper, we explore few-shot MLLM classification fine-tuning. We found that SFT can cause severe overfitting issues and may even degrade performance over the zero-shot approach. To address this challenge, inspired by the recent successes in rule-based reinforcement learning, we propose CLS-RL, which uses verifiable signals as reward to fine-tune MLLMs. We discovered that CLS-RL outperforms SFT in most datasets and has a much higher average accuracy on both base-to-new and few-shot learning setting. Moreover, we observed a free-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular dataset, their performance on other distinct datasets may also improve over zero-shot models, even if those datasets differ in distribution and class names. This suggests that RL-based methods effectively teach models the fundamentals of classification. Lastly, inspired by recent works in inference time thinking, we re-examine the `thinking process' during fine-tuning, a critical aspect of RL-based methods, in the context of visual classification. We question whether such tasks require extensive thinking process during fine-tuning, proposing that this may actually detract from performance. Based on this premise, we introduce the No-Thinking-CLS-RL method, which minimizes thinking processes during training by setting an equality accuracy reward. Our findings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL method achieves superior in-domain performance and generalization capabilities than CLS-RL.",
            "score": 6,
            "issue_id": 2826,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "904362034cdf6d16",
            "authors": [
                "Ming Li",
                "Shitian Zhao",
                "Jike Zhong",
                "Yuxiang Lai",
                "Kaipeng Zhang"
            ],
            "affiliations": [
                "Emory University",
                "Shanghai AI Laboratory",
                "University of Southern California"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16188.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#transfer_learning",
                    "#optimization",
                    "#cv",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (MLLM) Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¼Ğ°Ğ»Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ CLS-RL, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ MLLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CLS-RL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ No-Thinking-CLS-RL, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ 'Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ' Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Reinforcement Learning Revolutionizes Few-Shot Image Classification",
                    "desc": "This paper investigates the fine-tuning of Multimodal Large Language Models (MLLMs) for image classification, particularly in few-shot scenarios. It highlights the limitations of standard supervised fine-tuning (SFT), which can lead to overfitting and reduced performance compared to zero-shot methods. To overcome these challenges, the authors propose a novel approach called CLS-RL, which utilizes reinforcement learning with verifiable signals as rewards, resulting in improved accuracy across various datasets. Additionally, they introduce the No-Thinking-CLS-RL method, which minimizes unnecessary cognitive processes during training, further enhancing performance and generalization with less fine-tuning time."
                },
                "zh": {
                    "title": "å°‘æ ·æœ¬å¾®è°ƒï¼Œæå‡åˆ†ç±»æ€§èƒ½çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨åˆå§‹é˜¶æ®µçš„åˆ†ç±»æ€§èƒ½è¾ƒå·®ï¼Œä½†é€šè¿‡å°‘é‡æ ·æœ¬çš„å¾®è°ƒï¼Œå¯ä»¥æ˜¾è‘—æå‡å…¶è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•CLS-RLï¼Œåˆ©ç”¨å¯éªŒè¯ä¿¡å·ä½œä¸ºå¥–åŠ±æ¥å¾®è°ƒMLLMsï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šä¼˜äºä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†No-Thinking-CLS-RLæ–¹æ³•ï¼Œé€šè¿‡å‡å°‘å¾®è°ƒè¿‡ç¨‹ä¸­çš„æ€è€ƒæ—¶é—´ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15567",
            "title": "Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling",
            "url": "https://huggingface.co/papers/2503.15567",
            "abstract": "3D molecule generation is crucial for drug discovery and material science, requiring models to process complex multi-modalities, including atom types, chemical bonds, and 3D coordinates. A key challenge is integrating these modalities of different shapes while maintaining SE(3) equivariance for 3D coordinates. To achieve this, existing approaches typically maintain separate latent spaces for invariant and equivariant modalities, reducing efficiency in both training and sampling. In this work, we propose Unified Variational Auto-Encoder for 3D Molecular Latent Diffusion Modeling (UAE-3D), a multi-modal VAE that compresses 3D molecules into latent sequences from a unified latent space, while maintaining near-zero reconstruction error. This unified latent space eliminates the complexities of handling multi-modality and equivariance when performing latent diffusion modeling. We demonstrate this by employing the Diffusion Transformer--a general-purpose diffusion model without any molecular inductive bias--for latent generation. Extensive experiments on GEOM-Drugs and QM9 datasets demonstrate that our method significantly establishes new benchmarks in both de novo and conditional 3D molecule generation, achieving leading efficiency and quality.",
            "score": 6,
            "issue_id": 2822,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 19",
                "zh": "3æœˆ19æ—¥"
            },
            "hash": "d1787a1a75f723a2",
            "authors": [
                "Yanchen Luo",
                "Zhiyuan Liu",
                "Yi Zhao",
                "Sihang Li",
                "Kenji Kawaguchi",
                "Tat-Seng Chua",
                "Xiang Wang"
            ],
            "affiliations": [
                "National University of Singapore",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15567.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#benchmark",
                    "#3d",
                    "#optimization",
                    "#dataset"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ² Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ (UAE-3D) Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ 3D Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GEOM-Drugs Ğ¸ QM9 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… de novo Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»."
                },
                "en": {
                    "title": "Unified Latent Space for Efficient 3D Molecule Generation",
                    "desc": "This paper presents a new approach called Unified Variational Auto-Encoder for 3D Molecular Latent Diffusion Modeling (UAE-3D) aimed at generating 3D molecules for drug discovery and material science. The method addresses the challenge of integrating different types of data, such as atom types and chemical bonds, while ensuring that the 3D coordinates maintain SE(3) equivariance. By using a unified latent space, UAE-3D simplifies the process of handling multi-modalities and improves the efficiency of training and sampling. The results show that this approach outperforms existing methods in generating high-quality 3D molecules, setting new benchmarks in the field."
                },
                "zh": {
                    "title": "ç»Ÿä¸€æ½œåœ¨ç©ºé—´ï¼Œæå‡3Dåˆ†å­ç”Ÿæˆæ•ˆç‡",
                    "desc": "3Dåˆ†å­ç”Ÿæˆå¯¹è¯ç‰©å‘ç°å’Œææ–™ç§‘å­¦è‡³å…³é‡è¦ï¼Œéœ€è¦æ¨¡å‹å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬åŸå­ç±»å‹ã€åŒ–å­¦é”®å’Œ3Dåæ ‡ã€‚ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯å¦‚ä½•åœ¨ä¿æŒSE(3)ç­‰å˜æ€§çš„åŒæ—¶ï¼Œæ•´åˆä¸åŒå½¢çŠ¶çš„æ¨¡æ€ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆUAE-3Dï¼‰ï¼Œå®ƒå°†3Dåˆ†å­å‹ç¼©ä¸ºæ¥è‡ªç»Ÿä¸€æ½œåœ¨ç©ºé—´çš„æ½œåœ¨åºåˆ—ï¼ŒåŒæ—¶ä¿æŒè¿‘ä¹é›¶çš„é‡å»ºè¯¯å·®ã€‚é€šè¿‡ä½¿ç”¨Diffusion Transformerï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨GEOM-Drugså’ŒQM9æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ–°åˆ†å­ç”Ÿæˆçš„æ•ˆç‡å’Œè´¨é‡ï¼Œå»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16278",
            "title": "Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on\n  Compressed Spatial Tokens",
            "url": "https://huggingface.co/papers/2503.16278",
            "abstract": "Recent advancements in large language models and their multi-modal extensions have demonstrated the effectiveness of unifying generation and understanding through autoregressive next-token prediction. However, despite the critical role of 3D structural generation and understanding ({3D GU}) in AI for science, these tasks have largely evolved independently, with autoregressive methods remaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified framework that seamlessly integrates {3D GU} tasks via autoregressive prediction. At its core, Uni-3DAR employs a novel hierarchical tokenization that compresses 3D space using an octree, leveraging the inherent sparsity of 3D structures. It then applies an additional tokenization for fine-grained structural details, capturing key attributes such as atom types and precise spatial coordinates in microscopic 3D structures. We further propose two optimizations to enhance efficiency and effectiveness. The first is a two-level subtree compression strategy, which reduces the octree token sequence by up to 8x. The second is a masked next-token prediction mechanism tailored for dynamically varying token positions, significantly boosting model performance. By combining these strategies, Uni-3DAR successfully unifies diverse {3D GU} tasks within a single autoregressive framework. Extensive experiments across multiple microscopic {3D GU} tasks, including molecules, proteins, polymers, and crystals, validate its effectiveness and versatility. Notably, Uni-3DAR surpasses previous state-of-the-art diffusion models by a substantial margin, achieving up to 256\\% relative improvement while delivering inference speeds up to 21.8x faster. The code is publicly available at https://github.com/dptech-corp/Uni-3DAR.",
            "score": 5,
            "issue_id": 2822,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "7012b17f03aeb180",
            "authors": [
                "Shuqi Lu",
                "Haowei Lin",
                "Lin Yao",
                "Zhifeng Gao",
                "Xiaohong Ji",
                "Weinan E",
                "Linfeng Zhang",
                "Guolin Ke"
            ],
            "affiliations": [
                "AI for Science Institute, Beijing 100080, China",
                "Center for Machine Learning Research, Peking University, Beijing 100084, China",
                "DP Technology, Beijing, 100080, China",
                "Institute for Artificial Intelligence, Peking University, Beijing 100871, China",
                "School of Mathematical Sciences, Peking University, Beijing, 100871, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16278.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#science",
                    "#training",
                    "#architecture",
                    "#3d",
                    "#optimization",
                    "#inference"
                ],
                "emoji": "ğŸ§¬",
                "ru": {
                    "title": "Uni-3DAR: Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ 3D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Uni-3DAR - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ 3D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾ĞºÑ‚Ğ¾Ğ´ĞµÑ€ĞµĞ²Ğ° Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸: Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ² Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Uni-3DAR Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… 3D Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Unifying 3D Generation and Understanding with Uni-3DAR",
                    "desc": "This paper presents Uni-3DAR, a novel framework that integrates 3D structural generation and understanding tasks using autoregressive next-token prediction. It introduces a hierarchical tokenization method that efficiently compresses 3D space with an octree, capturing both the overall structure and fine details like atom types and spatial coordinates. The framework includes optimizations such as a two-level subtree compression strategy and a masked next-token prediction mechanism, enhancing both efficiency and model performance. Experimental results show that Uni-3DAR significantly outperforms existing models, achieving faster inference speeds and improved accuracy across various microscopic 3D tasks."
                },
                "zh": {
                    "title": "ç»Ÿä¸€3Dç»“æ„ç”Ÿæˆä¸ç†è§£çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºUni-3DARçš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªå›å½’é¢„æµ‹æ•´åˆ3Dç»“æ„ç”Ÿæˆä¸ç†è§£ï¼ˆ3D GUï¼‰ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ–°é¢–çš„åˆ†å±‚æ ‡è®°åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨å…«å‰æ ‘å‹ç¼©3Dç©ºé—´ï¼Œå¹¶ä¸ºå¾®è§‚3Dç»“æ„æ•æ‰å…³é”®å±æ€§å¦‚åŸå­ç±»å‹å’Œç²¾ç¡®åæ ‡ã€‚Uni-3DARè¿˜æå‡ºäº†ä¸¤é¡¹ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥æé«˜æ•ˆç‡å’Œæ•ˆæœï¼ŒåŒ…æ‹¬ä¸¤çº§å­æ ‘å‹ç¼©å’ŒåŠ¨æ€å˜åŒ–æ ‡è®°ä½ç½®çš„æ©ç ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æœºåˆ¶ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒUni-3DARåœ¨å¤šç§å¾®è§‚3D GUä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æ‰©æ•£æ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14237",
            "title": "Make Your Training Flexible: Towards Deployment-Efficient Video Models",
            "url": "https://huggingface.co/papers/2503.14237",
            "abstract": "Popular video training methods mainly operate on a fixed number of tokens sampled from a predetermined spatiotemporal grid, resulting in sub-optimal accuracy-computation trade-offs due to inherent video redundancy. They also lack adaptability to varying computational budgets for downstream tasks, hindering applications of the most competitive model in real-world scenes. We thus propose a new test setting, Token Optimization, for maximized input information across budgets, which optimizes the size-limited set of input tokens through token selection from more suitably sampled videos. To this end, we propose a novel augmentation tool termed Flux. By making the sampling grid flexible and leveraging token selection, it is easily adopted in most popular video training frameworks, boosting model robustness with nearly no additional cost. We integrate Flux in large-scale video pre-training, and the resulting FluxViT establishes new state-of-the-art results across extensive tasks at standard costs. Notably, with 1/4 tokens only, it can still match the performance of previous state-of-the-art models with Token Optimization, yielding nearly 90\\% savings. All models and data are available at https://github.com/OpenGVLab/FluxViT.",
            "score": 5,
            "issue_id": 2829,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "bae6f023e68eb914",
            "authors": [
                "Chenting Wang",
                "Kunchang Li",
                "Tianxiang Jiang",
                "Xiangyu Zeng",
                "Yi Wang",
                "Limin Wang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "State Key Laboratory for Novel Software Technology, Nanjing University",
                "University of Chinese Academy of Sciences",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14237.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#video",
                    "#open_source",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Flux: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Flux. ĞĞ½ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ´ĞµĞ»Ğ°Ñ ĞµĞµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹. Flux Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ FluxViT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 1/4 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Optimizing Video Training with Token Selection for Efficiency",
                    "desc": "This paper introduces a new approach to video training methods that addresses the inefficiencies of fixed token sampling from a spatiotemporal grid. The authors propose a test setting called Token Optimization, which allows for better input information management by selecting tokens from more appropriately sampled videos. They introduce a tool named Flux that enhances the flexibility of the sampling grid and can be integrated into existing video training frameworks with minimal cost. The results show that their method, FluxViT, achieves state-of-the-art performance while significantly reducing the number of tokens needed, demonstrating substantial computational savings."
                },
                "zh": {
                    "title": "ä¼˜åŒ–è§†é¢‘è®­ç»ƒï¼Œæå‡æ¨¡å‹æ•ˆç‡",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–è¾“å…¥ä¿¡æ¯çš„ä½¿ç”¨ã€‚é€šè¿‡å¼•å…¥ä¸€ç§åä¸ºFluxçš„å¢å¼ºå·¥å…·ï¼Œç ”ç©¶è€…ä»¬å®ç°äº†çµæ´»çš„é‡‡æ ·ç½‘æ ¼å’Œæœ‰æ•ˆçš„tokené€‰æ‹©ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§ã€‚è¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡è§†é¢‘é¢„è®­ç»ƒä¸­åº”ç”¨ï¼Œå–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œä¸”åœ¨ä½¿ç”¨ä»…1/4çš„tokenæ—¶ï¼Œä»èƒ½ä¸ä¹‹å‰çš„æœ€ä½³æ¨¡å‹ç›¸åª²ç¾ã€‚æ­¤ç ”ç©¶ä¸ºè§†é¢‘å¤„ç†ä»»åŠ¡æä¾›äº†æ›´é«˜æ•ˆçš„è®¡ç®—é¢„ç®—é€‚åº”æ€§ï¼Œæ˜¾è‘—é™ä½äº†èµ„æºæ¶ˆè€—ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13834",
            "title": "See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language\n  Balance to Mitigate Dominant Modality Bias",
            "url": "https://huggingface.co/papers/2503.13834",
            "abstract": "Vision-language (VL) models have demonstrated strong performance across various tasks. However, these models often rely on a specific modality for predictions, leading to \"dominant modality bias.'' This bias significantly hurts performance, especially when one modality is impaired. In this study, we analyze model behavior under dominant modality bias and theoretically show that unaligned gradients or differences in gradient magnitudes prevent balanced convergence of the loss. Based on these findings, we propose a novel framework, BalGrad to mitigate dominant modality bias. Our approach includes inter-modality gradient reweighting, adjusting the gradient of KL divergence based on each modality's contribution, and inter-task gradient projection to align task directions in a non-conflicting manner. Experiments on UPMC Food-101, Hateful Memes, and MM-IMDb datasets confirm that BalGrad effectively alleviates over-reliance on specific modalities when making predictions.",
            "score": 4,
            "issue_id": 2826,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "d17ffa2ed0a9ff25",
            "authors": [
                "JuneHyoung Kwon",
                "MiHyeon Kim",
                "Eunju Lee",
                "Juhwan Choi",
                "YoungBin Kim"
            ],
            "affiliations": [
                "Department of Artificial Intelligence, Chung-Ang University",
                "Graduate School of Advanced Imaging Sciences, Multimedia and Film, Chung-Ang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13834.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#multimodal",
                    "#optimization",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº BalGrad, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ BalGrad Ğ² ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Balancing Modalities for Better Predictions",
                    "desc": "This paper addresses the issue of dominant modality bias in vision-language (VL) models, which can negatively impact their performance when one modality is compromised. The authors analyze how unaligned gradients and varying gradient magnitudes hinder the model's ability to converge effectively. To counter this bias, they introduce a new framework called BalGrad, which employs techniques like inter-modality gradient reweighting and inter-task gradient projection. Experimental results on multiple datasets demonstrate that BalGrad successfully reduces the dependency on any single modality, leading to improved prediction accuracy."
                },
                "zh": {
                    "title": "å¹³è¡¡æ¨¡æ€åå·®ï¼Œæå‡æ¨¡å‹æ€§èƒ½",
                    "desc": "è§†è§‰è¯­è¨€ï¼ˆVLï¼‰æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç‰¹å®šçš„æ¨¡æ€è¿›è¡Œé¢„æµ‹ï¼Œè¿™å¯¼è‡´äº†â€œä¸»å¯¼æ¨¡æ€åå·®â€ã€‚è¿™ç§åå·®ä¼šæ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æŸä¸€æ¨¡æ€å—æŸæ—¶ã€‚æˆ‘ä»¬åˆ†æäº†ä¸»å¯¼æ¨¡æ€åå·®ä¸‹æ¨¡å‹çš„è¡Œä¸ºï¼Œå¹¶ç†è®ºä¸Šè¯æ˜äº†æœªå¯¹é½çš„æ¢¯åº¦æˆ–æ¢¯åº¦å¹…åº¦å·®å¼‚ä¼šé˜»ç¢æŸå¤±çš„å¹³è¡¡æ”¶æ•›ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶BalGradï¼Œé€šè¿‡æ¨¡æ€é—´æ¢¯åº¦é‡åŠ æƒå’Œä»»åŠ¡é—´æ¢¯åº¦æŠ•å½±æ¥å‡è½»ä¸»å¯¼æ¨¡æ€åå·®ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.12689",
            "title": "MagicID: Hybrid Preference Optimization for ID-Consistent and\n  Dynamic-Preserved Video Customization",
            "url": "https://huggingface.co/papers/2503.12689",
            "abstract": "Video identity customization seeks to produce high-fidelity videos that maintain consistent identity and exhibit significant dynamics based on users' reference images. However, existing approaches face two key challenges: identity degradation over extended video length and reduced dynamics during training, primarily due to their reliance on traditional self-reconstruction training with static images. To address these issues, we introduce MagicID, a novel framework designed to directly promote the generation of identity-consistent and dynamically rich videos tailored to user preferences. Specifically, we propose constructing pairwise preference video data with explicit identity and dynamic rewards for preference learning, instead of sticking to the traditional self-reconstruction. To address the constraints of customized preference data, we introduce a hybrid sampling strategy. This approach first prioritizes identity preservation by leveraging static videos derived from reference images, then enhances dynamic motion quality in the generated videos using a Frontier-based sampling method. By utilizing these hybrid preference pairs, we optimize the model to align with the reward differences between pairs of customized preferences. Extensive experiments show that MagicID successfully achieves consistent identity and natural dynamics, surpassing existing methods across various metrics.",
            "score": 4,
            "issue_id": 2824,
            "pub_date": "2025-03-16",
            "pub_date_card": {
                "ru": "16 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 16",
                "zh": "3æœˆ16æ—¥"
            },
            "hash": "2285de85beb9dbfb",
            "authors": [
                "Hengjia Li",
                "Lifan Jiang",
                "Xi Xiao",
                "Tianyang Wang",
                "Hongwei Yi",
                "Boxi Wu",
                "Deng Cai"
            ],
            "affiliations": [
                "Hedra AI",
                "University of Alabama at Birmingham",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.12689.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "MagicID: ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¾Ğ¹",
                    "desc": "MagicID - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². MagicID Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "MagicID: Dynamic Video Identity Customization Made Easy",
                    "desc": "This paper presents MagicID, a new framework for generating high-quality videos that maintain a consistent identity while exhibiting dynamic movements based on user preferences. The authors identify two main challenges in existing methods: the loss of identity over longer videos and the lack of dynamics during training due to reliance on static images. To overcome these issues, MagicID uses pairwise preference video data that incorporates explicit rewards for identity and dynamics, moving away from traditional self-reconstruction techniques. The framework employs a hybrid sampling strategy to prioritize identity preservation and enhance motion quality, leading to improved performance in generating videos that meet user expectations."
                },
                "zh": {
                    "title": "é­”æ³•èº«ä»½ï¼šå®šåˆ¶åŠ¨æ€è§†é¢‘çš„æ–°æ–¹æ³•",
                    "desc": "è§†é¢‘èº«ä»½å®šåˆ¶æ—¨åœ¨ç”Ÿæˆé«˜ä¿çœŸåº¦çš„è§†é¢‘ï¼Œè¿™äº›è§†é¢‘èƒ½å¤Ÿä¿æŒä¸€è‡´çš„èº«ä»½å¹¶æ ¹æ®ç”¨æˆ·çš„å‚è€ƒå›¾åƒå±•ç°æ˜¾è‘—çš„åŠ¨æ€æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šåœ¨è¾ƒé•¿è§†é¢‘é•¿åº¦ä¸‹èº«ä»½çš„é€€åŒ–å’Œè®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€æ€§çš„å‡å°‘ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå®ƒä»¬ä¾èµ–äºä¼ ç»Ÿçš„è‡ªæˆ‘é‡å»ºè®­ç»ƒä¸é™æ€å›¾åƒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MagicIDï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨ç›´æ¥ä¿ƒè¿›ç”Ÿæˆç¬¦åˆç”¨æˆ·åå¥½çš„èº«ä»½ä¸€è‡´ä¸”åŠ¨æ€ä¸°å¯Œçš„è§†é¢‘ã€‚æˆ‘ä»¬é€šè¿‡æ„å»ºå…·æœ‰æ˜ç¡®èº«ä»½å’ŒåŠ¨æ€å¥–åŠ±çš„æˆå¯¹åå¥½è§†é¢‘æ•°æ®æ¥è¿›è¡Œåå¥½å­¦ä¹ ï¼Œè€Œä¸æ˜¯åšæŒä¼ ç»Ÿçš„è‡ªæˆ‘é‡å»ºæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.09949",
            "title": "UVE: Are MLLMs Unified Evaluators for AI-Generated Videos?",
            "url": "https://huggingface.co/papers/2503.09949",
            "abstract": "With the rapid growth of video generative models (VGMs), it is essential to develop reliable and comprehensive automatic metrics for AI-generated videos (AIGVs). Existing methods either use off-the-shelf models optimized for other tasks or rely on human assessment data to train specialized evaluators. These approaches are constrained to specific evaluation aspects and are difficult to scale with the increasing demands for finer-grained and more comprehensive evaluations. To address this issue, this work investigates the feasibility of using multimodal large language models (MLLMs) as a unified evaluator for AIGVs, leveraging their strong visual perception and language understanding capabilities. To evaluate the performance of automatic metrics in unified AIGV evaluation, we introduce a benchmark called UVE-Bench. UVE-Bench collects videos generated by state-of-the-art VGMs and provides pairwise human preference annotations across 15 evaluation aspects. Using UVE-Bench, we extensively evaluate 16 MLLMs. Our empirical results suggest that while advanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human evaluators, they demonstrate promising ability in unified AIGV evaluation, significantly surpassing existing specialized evaluation methods. Additionally, we conduct an in-depth analysis of key design choices that impact the performance of MLLM-driven evaluators, offering valuable insights for future research on AIGV evaluation. The code is available at https://github.com/bytedance/UVE.",
            "score": 4,
            "issue_id": 2835,
            "pub_date": "2025-03-13",
            "pub_date_card": {
                "ru": "13 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 13",
                "zh": "3æœˆ13æ—¥"
            },
            "hash": "9533dd54603f19de",
            "authors": [
                "Yuanxin Liu",
                "Rui Zhu",
                "Shuhuai Ren",
                "Jiacong Wang",
                "Haoyuan Guo",
                "Xu Sun",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "Peking University",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.09949.jpg",
            "data": {
                "categories": [
                    "#interpretability",
                    "#video",
                    "#games",
                    "#optimization",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "MLLM ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº AI-Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼ (AIGV), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº UVE-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ² ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ AIGV. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ MLLM, Ñ…Ğ¾Ñ‚Ñ Ğ¸ ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ°Ğ¼, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ AIGV, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MLLM."
                },
                "en": {
                    "title": "Harnessing MLLMs for Comprehensive AI-Generated Video Evaluation",
                    "desc": "This paper addresses the need for effective evaluation metrics for AI-generated videos (AIGVs) as video generative models (VGMs) become more prevalent. It critiques existing methods that either depend on models designed for different tasks or require human assessments, which limits their scalability and comprehensiveness. The authors propose using multimodal large language models (MLLMs) as a unified evaluator, capitalizing on their capabilities in visual perception and language understanding. They introduce UVE-Bench, a benchmark for evaluating AIGVs, and find that while MLLMs do not yet match human evaluators, they outperform traditional evaluation methods, providing insights for future improvements in AIGV assessment."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æå‡AIç”Ÿæˆè§†é¢‘è¯„ä¼°çš„ç»Ÿä¸€æ€§",
                    "desc": "éšç€è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆVGMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¼€å‘å¯é çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å¯¹äºAIç”Ÿæˆè§†é¢‘ï¼ˆAIGVï¼‰å˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºä¸ºå…¶ä»–ä»»åŠ¡ä¼˜åŒ–çš„ç°æˆæ¨¡å‹æˆ–äººç±»è¯„ä¼°æ•°æ®æ¥è®­ç»ƒä¸“é—¨çš„è¯„ä¼°å™¨ï¼Œè¿™é™åˆ¶äº†è¯„ä¼°çš„å…¨é¢æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºAIGVçš„ç»Ÿä¸€è¯„ä¼°å™¨çš„å¯è¡Œæ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªåä¸ºUVE-Benchçš„åŸºå‡†ï¼Œæ”¶é›†äº†ç”±æœ€å…ˆè¿›çš„VGMç”Ÿæˆçš„è§†é¢‘åŠå…¶äººç±»åå¥½æ³¨é‡Šã€‚æˆ‘ä»¬çš„å®è¯ç»“æœè¡¨æ˜ï¼Œå°½ç®¡å…ˆè¿›çš„MLLMä»ç„¶è½åäºäººç±»è¯„ä¼°è€…ï¼Œä½†åœ¨ç»Ÿä¸€AIGVè¯„ä¼°ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„ä¸“é—¨è¯„ä¼°æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16194",
            "title": "Improving Autoregressive Image Generation through Coarse-to-Fine Token\n  Prediction",
            "url": "https://huggingface.co/papers/2503.16194",
            "abstract": "Autoregressive models have shown remarkable success in image generation by adapting sequential prediction techniques from language modeling. However, applying these approaches to images requires discretizing continuous pixel data through vector quantization methods like VQ-VAE. To alleviate the quantization errors that existed in VQ-VAE, recent works tend to use larger codebooks. However, this will accordingly expand vocabulary size, complicating the autoregressive modeling task. This paper aims to find a way to enjoy the benefits of large codebooks without making autoregressive modeling more difficult. Through empirical investigation, we discover that tokens with similar codeword representations produce similar effects on the final generated image, revealing significant redundancy in large codebooks. Based on this insight, we propose to predict tokens from coarse to fine (CTF), realized by assigning the same coarse label for similar tokens. Our framework consists of two stages: (1) an autoregressive model that sequentially predicts coarse labels for each token in the sequence, and (2) an auxiliary model that simultaneously predicts fine-grained labels for all tokens conditioned on their coarse labels. Experiments on ImageNet demonstrate our method's superior performance, achieving an average improvement of 59 points in Inception Score compared to baselines. Notably, despite adding an inference step, our approach achieves faster sampling speeds.",
            "score": 3,
            "issue_id": 2826,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "0e990c414e8ba81c",
            "authors": [
                "Ziyao Guo",
                "Kaipeng Zhang",
                "Michael Qizhe Shieh"
            ],
            "affiliations": [
                "National University of Singapore",
                "Shanghai AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16194.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞÑ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¾Ñ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ (CTF), Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… ĞºĞ½Ğ¸Ğ³ Ğ±ĞµĞ· ÑƒÑĞ»Ğ¾Ğ¶Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€ÑƒĞ±Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¸ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ImageNet Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Inception Score Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Enhancing Image Generation with Coarse-to-Fine Token Prediction",
                    "desc": "This paper explores how autoregressive models can be improved for image generation by addressing the challenges of using large codebooks in vector quantization. It identifies that many tokens in these large codebooks are redundant, meaning they have similar effects on the generated images. To tackle this, the authors propose a coarse-to-fine (CTF) prediction method that first predicts broad categories for tokens and then refines these predictions with more detailed labels. Their experiments show that this approach not only enhances image quality, as indicated by a significant increase in Inception Score, but also speeds up the sampling process despite the added complexity."
                },
                "zh": {
                    "title": "ä»ç²—åˆ°ç»†ï¼šä¼˜åŒ–è‡ªå›å½’å›¾åƒç”Ÿæˆ",
                    "desc": "è‡ªå›å½’æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå€Ÿé‰´äº†è¯­è¨€å»ºæ¨¡ä¸­çš„åºåˆ—é¢„æµ‹æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ–¹æ³•åº”ç”¨äºå›¾åƒæ—¶ï¼Œéœ€è¦é€šè¿‡å‘é‡é‡åŒ–æ–¹æ³•ï¼ˆå¦‚VQ-VAEï¼‰å°†è¿ç»­åƒç´ æ•°æ®ç¦»æ•£åŒ–ã€‚ä¸ºäº†å‡å°‘VQ-VAEä¸­å­˜åœ¨çš„é‡åŒ–è¯¯å·®ï¼Œæœ€è¿‘çš„ç ”ç©¶å€¾å‘äºä½¿ç”¨æ›´å¤§çš„ä»£ç æœ¬ï¼Œä½†è¿™ä¼šå¢åŠ è¯æ±‡é‡ï¼Œå¤æ‚åŒ–è‡ªå›å½’å»ºæ¨¡ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»ç²—åˆ°ç»†ï¼ˆCTFï¼‰é¢„æµ‹æ ‡è®°çš„æ–¹æ³•ï¼Œåˆ©ç”¨ç›¸ä¼¼ä»£ç è¯è¡¨ç¤ºçš„æ ‡è®°å¯¹æœ€ç»ˆç”Ÿæˆå›¾åƒçš„ç›¸ä¼¼å½±å“ï¼Œä»è€Œåœ¨ä¸å¢åŠ å»ºæ¨¡éš¾åº¦çš„æƒ…å†µä¸‹äº«å—å¤§ä»£ç æœ¬çš„ä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16031",
            "title": "Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging\n  Fabricated Claims with Humorous Content",
            "url": "https://huggingface.co/papers/2503.16031",
            "abstract": "This paper presents the Deceptive Humor Dataset (DHD), a novel resource for studying humor derived from fabricated claims and misinformation. In an era of rampant misinformation, understanding how humor intertwines with deception is essential. DHD consists of humor-infused comments generated from false narratives, incorporating fabricated claims and manipulated information using the ChatGPT-4o model. Each instance is labeled with a Satire Level, ranging from 1 for subtle satire to 3 for high-level satire and classified into five distinct Humor Categories: Dark Humor, Irony, Social Commentary, Wordplay, and Absurdity. The dataset spans multiple languages including English, Telugu, Hindi, Kannada, Tamil, and their code-mixed variants (Te-En, Hi-En, Ka-En, Ta-En), making it a valuable multilingual benchmark. By introducing DHD, we establish a structured foundation for analyzing humor in deceptive contexts, paving the way for a new research direction that explores how humor not only interacts with misinformation but also influences its perception and spread. We establish strong baselines for the proposed dataset, providing a foundation for future research to benchmark and advance deceptive humor detection models.",
            "score": 3,
            "issue_id": 2822,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "80726382feca8f20",
            "authors": [
                "Sai Kartheek Reddy Kasu",
                "Shankar Biradar",
                "Sunil Saumya"
            ],
            "affiliations": [
                "IIIT Dharwad",
                "MIT Manipal"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16031.jpg",
            "data": {
                "categories": [
                    "#ethics",
                    "#low_resource",
                    "#benchmark",
                    "#dataset",
                    "#multilingual"
                ],
                "emoji": "ğŸ¤¡",
                "ru": {
                    "title": "Ğ¡Ğ¼ĞµÑ… ÑĞºĞ²Ğ¾Ğ·ÑŒ Ğ»Ğ¾Ğ¶ÑŒ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¾Ğ±Ğ¼Ğ°Ğ½Ñ‡Ğ¸Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ğ¾Ñ€Ğ°",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DHD (Deceptive Humor Dataset) Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ€Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ğ²Ñ‹Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. DHD ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· ÑĞ¼Ğ¾Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ĞµĞ², ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ChatGPT-4o. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€ Ğ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ ÑĞ°Ñ‚Ğ¸Ñ€Ñ‹ Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ¿Ğ¾ Ğ¿ÑÑ‚Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ ÑĞ¼Ğ¾Ñ€Ğ°. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ñ†ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Unraveling Humor in the Age of Misinformation",
                    "desc": "The Deceptive Humor Dataset (DHD) is a new resource designed to explore the relationship between humor and misinformation. It includes humor-filled comments based on false narratives, generated using the ChatGPT-4o model, and is labeled with a Satire Level and categorized into five types of humor. This dataset supports multiple languages, making it a useful tool for multilingual research in humor and deception. By providing a structured dataset, DHD aims to enhance the understanding of how humor can affect the perception and dissemination of misinformation."
                },
                "zh": {
                    "title": "æ­ç¤ºå¹½é»˜ä¸æ¬ºéª—çš„äº¤ç»‡",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„èµ„æºâ€”â€”æ¬ºéª—å¹½é»˜æ•°æ®é›†ï¼ˆDHDï¼‰ï¼Œç”¨äºç ”ç©¶æºè‡ªè™šå‡å£°æ˜å’Œé”™è¯¯ä¿¡æ¯çš„å¹½é»˜ã€‚åœ¨ä¿¡æ¯æ³›æ»¥çš„æ—¶ä»£ï¼Œç†è§£å¹½é»˜ä¸æ¬ºéª—ä¹‹é—´çš„å…³ç³»è‡³å…³é‡è¦ã€‚DHDåŒ…å«ä»è™šå‡å™è¿°ä¸­ç”Ÿæˆçš„å¹½é»˜è¯„è®ºï¼Œå¹¶ä½¿ç”¨ChatGPT-4oæ¨¡å‹ç”Ÿæˆè™šå‡å£°æ˜å’Œæ“æ§ä¿¡æ¯ã€‚æ¯ä¸ªå®ä¾‹éƒ½æ ‡æ³¨äº†è®½åˆºæ°´å¹³ï¼Œå¹¶åˆ†ä¸ºäº”ç§å¹½é»˜ç±»åˆ«ï¼Œä¸ºåˆ†ææ¬ºéª—èƒŒæ™¯ä¸‹çš„å¹½é»˜æä¾›äº†ç»“æ„åŒ–åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15855",
            "title": "VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting\n  Generation with Flexible Pose and Multi-View Joint Modeling",
            "url": "https://huggingface.co/papers/2503.15855",
            "abstract": "We propose VideoRFSplat, a direct text-to-3D model leveraging a video generation model to generate realistic 3D Gaussian Splatting (3DGS) for unbounded real-world scenes. To generate diverse camera poses and unbounded spatial extent of real-world scenes, while ensuring generalization to arbitrary text prompts, previous methods fine-tune 2D generative models to jointly model camera poses and multi-view images. However, these methods suffer from instability when extending 2D generative models to joint modeling due to the modality gap, which necessitates additional models to stabilize training and inference. In this work, we propose an architecture and a sampling strategy to jointly model multi-view images and camera poses when fine-tuning a video generation model. Our core idea is a dual-stream architecture that attaches a dedicated pose generation model alongside a pre-trained video generation model via communication blocks, generating multi-view images and camera poses through separate streams. This design reduces interference between the pose and image modalities. Additionally, we propose an asynchronous sampling strategy that denoises camera poses faster than multi-view images, allowing rapidly denoised poses to condition multi-view generation, reducing mutual ambiguity and enhancing cross-modal consistency. Trained on multiple large-scale real-world datasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms existing text-to-3D direct generation methods that heavily depend on post-hoc refinement via score distillation sampling, achieving superior results without such refinement.",
            "score": 3,
            "issue_id": 2829,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "66858187cf22b842",
            "authors": [
                "Hyojun Go",
                "Byeongjun Park",
                "Hyelin Nam",
                "Byung-Hoon Kim",
                "Hyungjin Chung",
                "Changick Kim"
            ],
            "affiliations": [
                "EverEx",
                "KAIST",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15855.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#video",
                    "#3d"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D-ÑÑ†ĞµĞ½Ñ‹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "VideoRFSplat - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ³Ğ´Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ñ‹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Revolutionizing 3D Scene Generation with VideoRFSplat",
                    "desc": "VideoRFSplat is a novel text-to-3D model that utilizes a video generation framework to create realistic 3D Gaussian Splatting for expansive real-world scenes. Unlike previous methods that struggle with stability when combining 2D generative models for camera poses and images, our approach employs a dual-stream architecture that separates the generation of poses and images. This design minimizes interference between the two modalities and incorporates an asynchronous sampling strategy that accelerates the denoising of camera poses, enhancing the overall consistency of the generated outputs. Trained on extensive datasets, VideoRFSplat demonstrates superior performance compared to existing methods that rely on additional refinement techniques."
                },
                "zh": {
                    "title": "VideoRFSplatï¼šæ–‡æœ¬åˆ°3Dçš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æˆ‘ä»¬æå‡ºäº†VideoRFSplatï¼Œè¿™æ˜¯ä¸€ç§ç›´æ¥çš„æ–‡æœ¬åˆ°3Dæ¨¡å‹ï¼Œåˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹ç”Ÿæˆé€¼çœŸçš„3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰ï¼Œé€‚ç”¨äºæ— é™çš„çœŸå®åœºæ™¯ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡åŒæµæ¶æ„åŒæ—¶å»ºæ¨¡å¤šè§†å›¾å›¾åƒå’Œç›¸æœºå§¿æ€ï¼Œå‡å°‘äº†å§¿æ€å’Œå›¾åƒæ¨¡æ€ä¹‹é—´çš„å¹²æ‰°ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¼‚æ­¥é‡‡æ ·ç­–ç•¥ï¼Œä½¿å¾—ç›¸æœºå§¿æ€çš„å»å™ªé€Ÿåº¦å¿«äºå¤šè§†å›¾å›¾åƒï¼Œä»è€Œæé«˜äº†è·¨æ¨¡æ€çš„ä¸€è‡´æ€§ã€‚ç»è¿‡åœ¨å¤šä¸ªå¤§è§„æ¨¡çœŸå®æ•°æ®é›†ä¸Šçš„è®­ç»ƒï¼ŒVideoRFSplatåœ¨ä¸ä¾èµ–åæœŸç²¾ç‚¼çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ–‡æœ¬åˆ°3Dç›´æ¥ç”Ÿæˆæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.13891",
            "title": "Where do Large Vision-Language Models Look at when Answering Questions?",
            "url": "https://huggingface.co/papers/2503.13891",
            "abstract": "Large Vision-Language Models (LVLMs) have shown promising performance in vision-language understanding and reasoning tasks. However, their visual understanding behaviors remain underexplored. A fundamental question arises: to what extent do LVLMs rely on visual input, and which image regions contribute to their responses? It is non-trivial to interpret the free-form generation of LVLMs due to their complicated visual architecture (e.g., multiple encoders and multi-resolution) and variable-length outputs. In this paper, we extend existing heatmap visualization methods (e.g., iGOS++) to support LVLMs for open-ended visual question answering. We propose a method to select visually relevant tokens that reflect the relevance between generated answers and input image. Furthermore, we conduct a comprehensive analysis of state-of-the-art LVLMs on benchmarks designed to require visual information to answer. Our findings offer several insights into LVLM behavior, including the relationship between focus region and answer correctness, differences in visual attention across architectures, and the impact of LLM scale on visual understanding. The code and data are available at https://github.com/bytedance/LVLM_Interpretation.",
            "score": 2,
            "issue_id": 2841,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "561c77b5d6495701",
            "authors": [
                "Xiaoying Xing",
                "Chia-Wen Kuo",
                "Li Fuxin",
                "Yulei Niu",
                "Fan Chen",
                "Ming Li",
                "Ying Wu",
                "Longyin Wen",
                "Sijie Zhu"
            ],
            "affiliations": [
                "Bytedance Intelligent Creation",
                "Northwestern University",
                "Oregon State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.13891.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#benchmark",
                    "#multimodal",
                    "#interpretability",
                    "#games"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ—Ğ°Ğ³Ğ»ÑĞ´Ñ‹Ğ²Ğ°Ñ Ğ² Ğ³Ğ»Ğ°Ğ·Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ LVLM",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞ¿Ğ»Ğ¾Ğ²Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ LVLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LVLM Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ¾ĞºÑƒÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸ÑÑ… Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Unlocking the Visual Understanding of LVLMs",
                    "desc": "This paper investigates how Large Vision-Language Models (LVLMs) understand and utilize visual information when answering questions about images. It addresses the challenge of interpreting the complex visual architectures of LVLMs, which include multiple encoders and produce variable-length outputs. The authors enhance existing heatmap visualization techniques to identify which parts of an image are most relevant to the answers generated by the models. Their analysis reveals important insights into how different LVLM architectures focus on visual regions and how the scale of the language model affects its visual comprehension capabilities."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„è§†è§‰ç†è§£",
                    "desc": "å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰è¯­è¨€ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬çš„è§†è§‰ç†è§£è¡Œä¸ºä»ç„¶ä¸å¤Ÿæ˜ç¡®ã€‚æœ¬æ–‡æ¢è®¨äº†LVLMså¯¹è§†è§‰è¾“å…¥çš„ä¾èµ–ç¨‹åº¦ä»¥åŠå“ªäº›å›¾åƒåŒºåŸŸå¯¹å…¶å“åº”æœ‰è´¡çŒ®ã€‚æˆ‘ä»¬æ‰©å±•äº†ç°æœ‰çš„çƒ­å›¾å¯è§†åŒ–æ–¹æ³•ï¼Œä»¥æ”¯æŒLVLMsåœ¨å¼€æ”¾å¼è§†è§‰é—®ç­”ä¸­çš„åº”ç”¨ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€‰æ‹©ä¸ç”Ÿæˆç­”æ¡ˆç›¸å…³çš„è§†è§‰æ ‡è®°çš„æ–¹æ³•ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„LVLMsè¿›è¡Œå…¨é¢åˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†ç„¦ç‚¹åŒºåŸŸä¸ç­”æ¡ˆæ­£ç¡®æ€§ä¹‹é—´çš„å…³ç³»ï¼Œä»¥åŠä¸åŒæ¶æ„åœ¨è§†è§‰æ³¨æ„åŠ›ä¸Šçš„å·®å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.07906",
            "title": "Painting with Words: Elevating Detailed Image Captioning with Benchmark\n  and Alignment Learning",
            "url": "https://huggingface.co/papers/2503.07906",
            "abstract": "Image captioning has long been a pivotal task in visual understanding, with recent advancements in vision-language models (VLMs) significantly enhancing the ability to generate detailed image captions. However, the evaluation of detailed image captioning remains underexplored due to outdated evaluation metrics and coarse annotations. In this paper, we introduce DeCapBench along with a novel metric, DCScore, specifically designed for detailed captioning tasks. DCScore evaluates hallucinations and fine-grained comprehensiveness by deconstructing responses into the smallest self-sufficient units, termed primitive information units, and assessing them individually. Our evaluation shows that DCScore aligns more closely with human judgment than other rule-based or model-based metrics. Concurrently, DeCapBench exhibits a high correlation with VLM arena results on descriptive tasks, surpassing existing benchmarks for vision-language models. Additionally, we present an automatic fine-grained feedback collection method, FeedQuill, for preference optimization based on our advanced metric, showing robust generalization capabilities across auto-generated preference data. Extensive experiments on multiple VLMs demonstrate that our method not only significantly reduces hallucinations but also enhances performance across various benchmarks, achieving superior detail captioning performance while surpassing GPT-4o.",
            "score": 2,
            "issue_id": 2843,
            "pub_date": "2025-03-10",
            "pub_date_card": {
                "ru": "10 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 10",
                "zh": "3æœˆ10æ—¥"
            },
            "hash": "df60b4319f196868",
            "authors": [
                "Qinghao Ye",
                "Xianhan Zeng",
                "Fu Li",
                "Chunyuan Li",
                "Haoqi Fan"
            ],
            "affiliations": [
                "ByteDance Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.07906.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#hallucinations",
                    "#cv",
                    "#benchmark",
                    "#multimodal",
                    "#training"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ DCScore. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ‹ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DeCapBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ FeedQuill Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ±Ğ¾Ñ€Ğ° Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Revolutionizing Image Caption Evaluation with DeCapBench and DCScore",
                    "desc": "This paper addresses the challenge of evaluating detailed image captioning, which has been hindered by outdated metrics and coarse annotations. The authors introduce DeCapBench and a new metric called DCScore, which focuses on assessing image captions by breaking them down into primitive information units. This approach allows for a more accurate evaluation of hallucinations and the comprehensiveness of captions, aligning better with human judgment. Additionally, the paper presents FeedQuill, a method for collecting fine-grained feedback to optimize preferences, demonstrating improved performance in detailed captioning tasks across various vision-language models."
                },
                "zh": {
                    "title": "æå‡å›¾åƒæè¿°çš„è¯„ä¼°ä¸ç”Ÿæˆèƒ½åŠ›",
                    "desc": "å›¾åƒæè¿°ä¸€ç›´æ˜¯è§†è§‰ç†è§£ä¸­çš„ä¸€ä¸ªé‡è¦ä»»åŠ¡ï¼Œæœ€è¿‘è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›æ­¥æ˜¾è‘—æå‡äº†ç”Ÿæˆè¯¦ç»†å›¾åƒæè¿°çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¯¦ç»†å›¾åƒæè¿°çš„è¯„ä¼°ä»ç„¶ä¸å¤Ÿæ·±å…¥ï¼Œä¸»è¦æ˜¯å› ä¸ºè¯„ä¼°æŒ‡æ ‡è¿‡æ—¶å’Œæ³¨é‡Šç²—ç³™ã€‚æœ¬æ–‡ä»‹ç»äº†DeCapBenchå’Œä¸€ç§æ–°é¢–çš„è¯„ä¼°æŒ‡æ ‡DCScoreï¼Œä¸“é—¨ç”¨äºè¯¦ç»†æè¿°ä»»åŠ¡ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼°æè¿°çš„å‡†ç¡®æ€§å’Œç»†è‡´ç¨‹åº¦ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒDCScoreä¸äººç±»åˆ¤æ–­çš„å»åˆåº¦é«˜äºå…¶ä»–åŸºäºè§„åˆ™æˆ–æ¨¡å‹çš„æŒ‡æ ‡ï¼ŒåŒæ—¶DeCapBenchåœ¨æè¿°æ€§ä»»åŠ¡ä¸Šä¸VLMé¢†åŸŸçš„ç»“æœé«˜åº¦ç›¸å…³ï¼Œè¶…è¶Šäº†ç°æœ‰çš„åŸºå‡†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.16091",
            "title": "AIMI: Leveraging Future Knowledge and Personalization in Sparse Event\n  Forecasting for Treatment Adherence",
            "url": "https://huggingface.co/papers/2503.16091",
            "abstract": "Adherence to prescribed treatments is crucial for individuals with chronic conditions to avoid costly or adverse health outcomes. For certain patient groups, intensive lifestyle interventions are vital for enhancing medication adherence. Accurate forecasting of treatment adherence can open pathways to developing an on-demand intervention tool, enabling timely and personalized support. With the increasing popularity of smartphones and wearables, it is now easier than ever to develop and deploy smart activity monitoring systems. However, effective forecasting systems for treatment adherence based on wearable sensors are still not widely available. We close this gap by proposing Adherence Forecasting and Intervention with Machine Intelligence (AIMI). AIMI is a knowledge-guided adherence forecasting system that leverages smartphone sensors and previous medication history to estimate the likelihood of forgetting to take a prescribed medication. A user study was conducted with 27 participants who took daily medications to manage their cardiovascular diseases. We designed and developed CNN and LSTM-based forecasting models with various combinations of input features and found that LSTM models can forecast medication adherence with an accuracy of 0.932 and an F-1 score of 0.936. Moreover, through a series of ablation studies involving convolutional and recurrent neural network architectures, we demonstrate that leveraging known knowledge about future and personalized training enhances the accuracy of medication adherence forecasting. Code available: https://github.com/ab9mamun/AIMI.",
            "score": 1,
            "issue_id": 2832,
            "pub_date": "2025-03-20",
            "pub_date_card": {
                "ru": "20 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 20",
                "zh": "3æœˆ20æ—¥"
            },
            "hash": "905c53a7c6bb7214",
            "authors": [
                "Abdullah Mamun",
                "Diane J. Cook",
                "Hassan Ghasemzadeh"
            ],
            "affiliations": [
                "College of Health Solutions, Arizona State University, Phoenix, AZ 85054, USA",
                "School of Computing and Augmented Intelligence, Arizona State University, Phoenix, AZ 85054, USA",
                "School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA 99164, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.16091.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#architecture",
                    "#healthcare",
                    "#optimization",
                    "#open_source",
                    "#training"
                ],
                "emoji": "ğŸ’Š",
                "ru": {
                    "title": "Ğ˜Ğ˜ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ²Ğ°ÑˆĞµĞ³Ğ¾ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ° Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ²",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ AIMI (Adherence Forecasting and Intervention with Machine Intelligence) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²ĞµÑ€Ğ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ½Ğ¾ÑĞ¸Ğ¼Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¸ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ (CNN) Ğ¸ LSTM Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ° Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ° Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ² Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ ÑĞµÑ€Ğ´ĞµÑ‡Ğ½Ğ¾-ÑĞ¾ÑÑƒĞ´Ğ¸ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LSTM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 0,932 Ğ¸ F1-Ğ¼ĞµÑ€Ñ‹ 0,936 Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼ Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¾Ğ²."
                },
                "en": {
                    "title": "Smart Forecasting for Better Medication Adherence",
                    "desc": "This paper presents AIMI, a machine learning system designed to forecast medication adherence using smartphone sensors and patient medication history. By employing advanced models like CNN and LSTM, AIMI achieves high accuracy in predicting when patients might forget to take their medications. The study involved 27 participants with cardiovascular diseases, demonstrating that personalized training and knowledge integration significantly improve forecasting performance. The findings suggest that AIMI can serve as a valuable tool for timely interventions, ultimately enhancing treatment adherence for chronic condition management."
                },
                "zh": {
                    "title": "æ™ºèƒ½é¢„æµ‹ï¼ŒåŠ©åŠ›è¯ç‰©ä¾ä»æ€§",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºAIMIçš„è¯ç‰©ä¾ä»æ€§é¢„æµ‹ç³»ç»Ÿï¼Œåˆ©ç”¨æ™ºèƒ½æ‰‹æœºä¼ æ„Ÿå™¨å’Œæ‚£è€…çš„ç”¨è¯å†å²æ¥é¢„æµ‹æ‚£è€…å¿˜è®°æœè¯çš„å¯èƒ½æ€§ã€‚é€šè¿‡å¯¹27åå¿ƒè¡€ç®¡ç–¾ç—…æ‚£è€…çš„ç”¨æˆ·ç ”ç©¶ï¼Œä½¿ç”¨åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLSTMæ¨¡å‹åœ¨è¯ç‰©ä¾ä»æ€§é¢„æµ‹ä¸­è¾¾åˆ°äº†93.2%çš„å‡†ç¡®ç‡å’Œ93.6çš„F-1åˆ†æ•°ã€‚é€šè¿‡ä¸€ç³»åˆ—æ¶ˆèå®éªŒï¼Œè¯æ˜äº†åˆ©ç”¨å·²çŸ¥çŸ¥è¯†å’Œä¸ªæ€§åŒ–è®­ç»ƒå¯ä»¥æé«˜è¯ç‰©ä¾ä»æ€§é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.11509",
            "title": "TikZero: Zero-Shot Text-Guided Graphics Program Synthesis",
            "url": "https://huggingface.co/papers/2503.11509",
            "abstract": "With the rise of generative AI, synthesizing figures from text captions becomes a compelling application. However, achieving high geometric precision and editability requires representing figures as graphics programs in languages like TikZ, and aligned training data (i.e., graphics programs with captions) remains scarce. Meanwhile, large amounts of unaligned graphics programs and captioned raster images are more readily available. We reconcile these disparate data sources by presenting TikZero, which decouples graphics program generation from text understanding by using image representations as an intermediary bridge. It enables independent training on graphics programs and captioned images and allows for zero-shot text-guided graphics program synthesis during inference. We show that our method substantially outperforms baselines that can only operate with caption-aligned graphics programs. Furthermore, when leveraging caption-aligned graphics programs as a complementary training signal, TikZero matches or exceeds the performance of much larger models, including commercial systems like GPT-4o. Our code, datasets, and select models are publicly available.",
            "score": 1,
            "issue_id": 2837,
            "pub_date": "2025-03-14",
            "pub_date_card": {
                "ru": "14 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 14",
                "zh": "3æœˆ14æ—¥"
            },
            "hash": "2335a2440c406122",
            "authors": [
                "Jonas Belouadi",
                "Eddy Ilg",
                "Margret Keuper",
                "Hideki Tanaka",
                "Masao Utiyama",
                "Raj Dabre",
                "Steffen Eger",
                "Simone Paolo Ponzetto"
            ],
            "affiliations": [
                "National Institute of Information and Communications Technology, Japan",
                "University of Mannheim, Germany",
                "University of Technology Nuremberg, Germany"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.11509.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#synthetic",
                    "#open_source",
                    "#multimodal",
                    "#data",
                    "#dataset",
                    "#inference"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "TikZero: Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ",
                    "desc": "TikZero - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ²ĞµĞ½Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. TikZero Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸."
                },
                "en": {
                    "title": "Bridging Text and Graphics with TikZero",
                    "desc": "This paper introduces TikZero, a novel approach for generating graphics programs from text captions using generative AI. It addresses the challenge of limited aligned training data by utilizing unaligned graphics programs and captioned images as separate training sources. TikZero employs image representations as a bridge, allowing for independent training and enabling zero-shot synthesis of graphics programs guided by text. The results demonstrate that TikZero significantly outperforms existing methods that rely solely on aligned data, achieving competitive performance with larger models when combined with aligned graphics programs."
                },
                "zh": {
                    "title": "TikZeroï¼šè§£è€¦æ–‡æœ¬ä¸å›¾å½¢ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "éšç€ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½çš„å…´èµ·ï¼Œä»æ–‡æœ¬æè¿°åˆæˆå›¾å½¢æˆä¸ºä¸€ä¸ªå¼•äººæ³¨ç›®çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œå®ç°é«˜å‡ ä½•ç²¾åº¦å’Œå¯ç¼–è¾‘æ€§éœ€è¦å°†å›¾å½¢è¡¨ç¤ºä¸ºåƒTikZè¿™æ ·çš„å›¾å½¢ç¨‹åºï¼Œè€Œå¯¹é½çš„è®­ç»ƒæ•°æ®ï¼ˆå³å¸¦æœ‰æè¿°çš„å›¾å½¢ç¨‹åºï¼‰ä»ç„¶ç¨€ç¼ºã€‚æˆ‘ä»¬æå‡ºäº†TikZeroï¼Œé€šè¿‡ä½¿ç”¨å›¾åƒè¡¨ç¤ºä½œä¸ºä¸­ä»‹æ¡¥æ¢ï¼Œå°†å›¾å½¢ç¨‹åºç”Ÿæˆä¸æ–‡æœ¬ç†è§£è§£è€¦ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿç‹¬ç«‹è®­ç»ƒå›¾å½¢ç¨‹åºå’Œå¸¦æè¿°çš„å›¾åƒï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å®ç°é›¶-shotæ–‡æœ¬å¼•å¯¼çš„å›¾å½¢ç¨‹åºåˆæˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.15672",
            "title": "GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for\n  Autonomous Driving",
            "url": "https://huggingface.co/papers/2503.15672",
            "abstract": "Self-supervised pre-training based on next-token prediction has enabled large language models to capture the underlying structure of text, and has led to unprecedented performance on a large array of tasks when applied at scale. Similarly, autonomous driving generates vast amounts of spatiotemporal data, alluding to the possibility of harnessing scale to learn the underlying geometric and semantic structure of the environment and its evolution over time. In this direction, we propose a geometric and semantic self-supervised pre-training method, GASP, that learns a unified representation by predicting, at any queried future point in spacetime, (1) general occupancy, capturing the evolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle path through the environment; and (3) distilled high-level features from a vision foundation model. By modeling geometric and semantic 4D occupancy fields instead of raw sensor measurements, the model learns a structured, generalizable representation of the environment and its evolution through time. We validate GASP on multiple autonomous driving benchmarks, demonstrating significant improvements in semantic occupancy forecasting, online mapping, and ego trajectory prediction. Our results demonstrate that continuous 4D geometric and semantic occupancy prediction provides a scalable and effective pre-training paradigm for autonomous driving. For code and additional visualizations, see \\href{https://research.zenseact.com/publications/gasp/.",
            "score": 0,
            "issue_id": 2838,
            "pub_date": "2025-03-19",
            "pub_date_card": {
                "ru": "19 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 19",
                "zh": "3æœˆ19æ—¥"
            },
            "hash": "94ae46d5f2ffc089",
            "authors": [
                "William Ljungbergh",
                "Adam Lilja",
                "Adam Tonderski. Arvid Laveno Ling",
                "Carl LindstrÃ¶m",
                "Willem Verbeke",
                "Junsheng Fu",
                "Christoffer Petersson",
                "Lars Hammarstrand",
                "Michael Felsberg"
            ],
            "affiliations": [
                "Chalmers University of Technology",
                "Linkoping University",
                "Lund University",
                "Zenseact"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.15672.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#dataset",
                    "#benchmark",
                    "#3d",
                    "#architecture",
                    "#survey",
                    "#multimodal"
                ],
                "emoji": "ğŸš—",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ 4D-Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ GASP - Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. GASP Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ±ÑƒĞ´ÑƒÑ‰ÑƒÑ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¸ ÑĞ³Ğ¾-Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚ÑŒ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ ĞµĞµ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ 4D-Ğ¿Ğ¾Ğ»Ñ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ĞºĞ°Ñ€Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "GASP: Revolutionizing Autonomous Driving with 4D Occupancy Prediction",
                    "desc": "This paper introduces GASP, a self-supervised pre-training method designed for autonomous driving applications. GASP predicts future occupancy in a 4D space, focusing on general occupancy, ego occupancy, and high-level features from a vision model. By utilizing geometric and semantic representations instead of raw sensor data, GASP creates a more structured understanding of the environment. The method shows significant improvements in tasks like semantic occupancy forecasting and ego trajectory prediction across various benchmarks."
                },
                "zh": {
                    "title": "GASPï¼šè‡ªåŠ¨é©¾é©¶çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§å‡ ä½•å’Œè¯­ä¹‰è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•GASPï¼Œæ—¨åœ¨é€šè¿‡é¢„æµ‹æœªæ¥æ—¶ç©ºç‚¹çš„å ç”¨æƒ…å†µæ¥å­¦ä¹ ç¯å¢ƒçš„ç»Ÿä¸€è¡¨ç¤ºã€‚è¯¥æ–¹æ³•åŒ…æ‹¬å¯¹3Dåœºæ™¯çš„åŠ¨æ€ç»“æ„ã€è½¦è¾†åœ¨ç¯å¢ƒä¸­çš„è·¯å¾„ä»¥åŠä»è§†è§‰åŸºç¡€æ¨¡å‹æå–çš„é«˜çº§ç‰¹å¾è¿›è¡Œå»ºæ¨¡ã€‚é€šè¿‡å»ºæ¨¡å‡ ä½•å’Œè¯­ä¹‰çš„4Då ç”¨åœºï¼Œè€Œä¸æ˜¯åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°ç»“æ„åŒ–ä¸”å…·æœ‰å¯æ¨å¹¿æ€§çš„ç¯å¢ƒè¡¨ç¤ºã€‚æˆ‘ä»¬åœ¨å¤šä¸ªè‡ªåŠ¨é©¾é©¶åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†GASPï¼Œæ˜¾ç¤ºå‡ºåœ¨è¯­ä¹‰å ç”¨é¢„æµ‹ã€åœ¨çº¿åœ°å›¾æ„å»ºå’Œè‡ªæˆ‘è½¨è¿¹é¢„æµ‹æ–¹é¢çš„æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2503.14201",
            "title": "Why Personalizing Deep Learning-Based Code Completion Tools Matters",
            "url": "https://huggingface.co/papers/2503.14201",
            "abstract": "Deep learning (DL)-based code completion tools have transformed software development by enabling advanced code generation. These tools leverage models trained on vast amounts of code from numerous repositories, capturing general coding patterns. However, the impact of fine-tuning these models for specific organizations or developers to boost their performance on such subjects remains unexplored. In this work, we fill this gap by presenting solid empirical evidence answering this question. More specifically, we consider 136 developers from two organizations (Apache and Spring), two model architectures (T5 and Code Llama), and three model sizes (60M, 750M, and 7B trainable parameters). T5 models (60M, 750M) were pre-trained and fine-tuned on over 2,000 open-source projects, excluding the subject organizations' data, and compared against versions fine-tuned on organization- and developer-specific datasets. For the Code Llama model (7B), we compared the performance of the already pre-trained model publicly available online with the same model fine-tuned via parameter-efficient fine-tuning on organization- and developer-specific datasets. Our results show that there is a boost in prediction capabilities provided by both an organization-specific and a developer-specific additional fine-tuning, with the former being particularly performant. Such a finding generalizes across (i) the two subject organizations (i.e., Apache and Spring) and (ii) models of completely different magnitude (from 60M to 7B trainable parameters). Finally, we show that DL models fine-tuned on an organization-specific dataset achieve the same completion performance of pre-trained code models used out of the box and being sim10times larger, with consequent savings in terms of deployment and inference cost (e.g., smaller GPUs needed).",
            "score": 0,
            "issue_id": 2840,
            "pub_date": "2025-03-18",
            "pub_date_card": {
                "ru": "18 Ğ¼Ğ°Ñ€Ñ‚Ğ°",
                "en": "March 18",
                "zh": "3æœˆ18æ—¥"
            },
            "hash": "b244dc3db7a6fb65",
            "authors": [
                "Alessandro Giagnorio",
                "Alberto Martin-Lopez",
                "Gabriele Bavota"
            ],
            "affiliations": [
                "UniversitÃ  della Svizzera italiana, Switzerland"
            ],
            "pdf_title_img": "assets/pdf/title_img/2503.14201.jpg",
            "data": {
                "categories": [
                    "#transfer_learning",
                    "#dataset",
                    "#open_source",
                    "#small_models",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ´Ğ°: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ²Ñ‹ÑˆĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ T5 Ğ¸ Code Llama Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ² Apache Ğ¸ Spring. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑƒÑ Ğ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, ĞºĞ°Ğº Ğ¸ Ğ² 10 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‚ÑŒ Ñ€ĞµÑÑƒÑ€ÑÑ‹."
                },
                "en": {
                    "title": "Boosting Code Completion with Tailored Fine-Tuning",
                    "desc": "This paper investigates the effectiveness of fine-tuning deep learning models for code completion tailored to specific organizations and developers. It analyzes the performance of two model architectures, T5 and Code Llama, across different sizes and datasets. The study finds that fine-tuning on organization-specific and developer-specific datasets significantly enhances prediction capabilities, with organization-specific fine-tuning yielding the best results. Notably, models fine-tuned on smaller datasets can match the performance of larger pre-trained models, leading to cost savings in deployment and inference."
                },
                "zh": {
                    "title": "å¾®è°ƒæ¨¡å‹ï¼Œæå‡ä»£ç è¡¥å…¨æ€§èƒ½ï¼",
                    "desc": "åŸºäºæ·±åº¦å­¦ä¹ çš„ä»£ç è¡¥å…¨å·¥å…·é€šè¿‡å…ˆè¿›çš„ä»£ç ç”ŸæˆæŠ€æœ¯æ”¹å˜äº†è½¯ä»¶å¼€å‘ã€‚è¿™äº›å·¥å…·åˆ©ç”¨ä»å¤§é‡ä»£ç åº“ä¸­è®­ç»ƒçš„æ¨¡å‹ï¼Œæ•æ‰ä¸€èˆ¬çš„ç¼–ç æ¨¡å¼ã€‚ç„¶è€Œï¼Œé’ˆå¯¹ç‰¹å®šç»„ç»‡æˆ–å¼€å‘è€…å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥æé«˜æ€§èƒ½çš„å½±å“å°šæœªè¢«æ·±å…¥ç ”ç©¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé’ˆå¯¹ç»„ç»‡å’Œå¼€å‘è€…çš„å¾®è°ƒå¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯ç»„ç»‡ç‰¹å®šçš„å¾®è°ƒæ•ˆæœæ›´ä¸ºæ˜¾è‘—ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-03-21.html",
    "link_next": "2025-03-25.html",
    "link_month": "2025-03.html",
    "short_date_prev": {
        "ru": "21.03",
        "en": "03/21",
        "zh": "3æœˆ21æ—¥"
    },
    "short_date_next": {
        "ru": "25.03",
        "en": "03/25",
        "zh": "3æœˆ25æ—¥"
    },
    "categories": {
        "#dataset": 15,
        "#data": 6,
        "#benchmark": 18,
        "#agents": 5,
        "#cv": 12,
        "#rl": 6,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 1,
        "#inference": 7,
        "#3d": 12,
        "#audio": 0,
        "#video": 11,
        "#multimodal": 15,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 14,
        "#healthcare": 3,
        "#training": 22,
        "#robotics": 1,
        "#agi": 2,
        "#games": 8,
        "#interpretability": 5,
        "#reasoning": 9,
        "#transfer_learning": 3,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 0,
        "#optimization": 24,
        "#survey": 3,
        "#diffusion": 11,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 3,
        "#long_context": 3,
        "#synthetic": 9,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 15,
        "#small_models": 3,
        "#science": 2,
        "#low_resource": 2
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ‰©æ•£æ¨¡å‹çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚è™½ç„¶æœ‰äº›æ–¹æ³•å¯ä»¥åŠ é€ŸSRæ¨¡å‹ï¼Œä½†å®ƒä»¬è¦ä¹ˆå¤±å»ç»†èŠ‚ï¼Œè¦ä¹ˆäº§ç”Ÿä¸å­˜åœ¨çš„ç»“æ„ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†RSDï¼Œä¸€ç§æ–°çš„è’¸é¦æ–¹æ³•ã€‚RSDåœ¨å•æ­¥æ¢å¤ä¸­è¶…è¶Šäº†æ•™å¸ˆæ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå±•ç¤ºäº†ä¼˜å¼‚çš„è¡¨ç°ã€‚ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒRSDå…·æœ‰æ›´å¥½çš„æ„ŸçŸ¥è´¨é‡å’Œæ›´å°‘çš„å‚æ•°éœ€æ±‚ã€‚",
        "title": "One-Step Residual Shifting Diffusion for Image Super-Resolution via\n  Distillation",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ‰©æ•£æ¨¡å‹çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚\nZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le chÄofÄ“n bÇlÇœ (SR) kuÃ²sÃ n mÃ³xÃ­ng de jÃ¬suÃ n chÃ©ngbÄ›n wÃ¨ntÃ­.\n\nè™½ç„¶æœ‰äº›æ–¹æ³•å¯ä»¥åŠ é€ŸSRæ¨¡å‹ï¼Œä½†å®ƒä»¬è¦ä¹ˆå¤±å»ç»†èŠ‚ï¼Œè¦ä¹ˆäº§ç”Ÿä¸å­˜åœ¨çš„ç»“æ„ã€‚\nSuÄ«rÃ¡n yÇ’uxiÄ“ fÄngfÇ kÄ›yÇ jiÄsÃ¹ SR mÃ³xÃ­ng, dÃ n tÄmen yÃ omÃ©i shÄ«qÃ¹ xÃ¬jiÃ©, yÃ omÃ©i chÇnshÄ“ng bÃ¹ cÃºnzÃ i de jiÃ©gÃ²u.\n\nä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†RSDï¼Œä¸€ç§æ–°çš„è’¸é¦æ–¹æ³•ã€‚\nWÃ¨i jiÄ›juÃ© zhÃ¨xiÄ“ wÃ¨ntÃ­, zuÃ²zhÄ› tÃ­chÅ« le RSD, yÄ«zhÇ’ng xÄ«n de zhÄ“ngliÃº fÄngfÇ.\n\nRSDåœ¨å•æ­¥æ¢å¤ä¸­è¶…è¶Šäº†æ•™å¸ˆæ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå±•ç¤ºäº†ä¼˜å¼‚çš„è¡¨ç°ã€‚\nRSD zÃ i dÄnbÃ¹ huÄ«fÃ¹ zhÅng chÄoyuÃ¨ le jiÃ oshÄ« mÃ³xÃ­ng, bÃ¬ng zÃ i duÅgÃ¨ shÃ¹jÃ¹jÃ­ shÃ ng zhÇnshÃ¬ le yÅuyÃ¬ de biÇoxiÃ n.\n\nä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒRSDå…·æœ‰æ›´å¥½çš„æ„ŸçŸ¥è´¨é‡å’Œæ›´å°‘çš„å‚æ•°éœ€æ±‚ã€‚\nYÇ” qÃ­tÄ fÄngfÇ xiÄngbÇ, RSD jÃ¹yÇ’u gÃ¨ng hÇo de gÇnjuÃ© zhÃ¬liÃ ng hÃ© gÃ¨ng shÇo de cÄnshÃ¹ xÅ«qiÃº.",
        "vocab": "[{'word': 'è¶…åˆ†è¾¨ç‡', 'pinyin': 'chÄo fÄ“n biÃ o lÇœ', 'trans': 'super-resolution'},\n{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ² sÃ n', 'trans': 'diffusion'},\n{'word': 'è®¡ç®—æˆæœ¬', 'pinyin': 'jÃ¬ suÃ n chÃ©ng bÄ›n', 'trans': 'computational cost'},\n{'word': 'è®¨è®º', 'pinyin': 'tÇo lÃ¹n', 'trans': 'discuss'},\n{'word': 'è’¸é¦', 'pinyin': 'zhÄ“ng liÃº', 'trans': 'distillation'},\n{'word': 'æ¢å¤', 'pinyin': 'huÄ« fÃ¹', 'trans': 'recovery'},\n{'word': 'æ•™å¸ˆæ¨¡å‹', 'pinyin': 'jiÃ o shÄ« mÃ³ xÃ­ng', 'trans': 'teacher model'},\n{'word': 'æ•°æ®é›†', 'pinyin': 'shÃ¹ jÃ¹ jÃ­', 'trans': 'dataset'},\n{'word': 'å±•ç¤º', 'pinyin': 'zhÇn shÃ¬', 'trans': 'demonstrate'},\n{'word': 'ä¼˜å¼‚', 'pinyin': 'yÅu yÃ¬', 'trans': 'excellent'},\n{'word': 'æ„ŸçŸ¥', 'pinyin': 'gÇn zhÄ«', 'trans': 'perception'},\n{'word': 'å‚æ•°', 'pinyin': 'cÄn shÃ¹', 'trans': 'parameter'},\n{'word': 'éœ€æ±‚', 'pinyin': 'xÅ« qiÃº', 'trans': 'requirement'}]",
        "trans": "This article discusses the computational cost issues of super-resolution (SR) diffusion models. Although some methods can accelerate SR models, they either lose detail or generate non-existent structures. To address these problems, the authors propose RSD, a new distillation method. RSD surpasses the teacher model in single-step recovery and demonstrates excellent performance across multiple datasets. Compared to other methods, RSD offers better perceptual quality and requires fewer parameters.",
        "update_ts": "2025-03-23 12:41"
    }
}