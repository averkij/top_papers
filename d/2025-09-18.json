{
    "date": {
        "ru": "18 сентября",
        "en": "September 18",
        "zh": "9月18日"
    },
    "time_utc": "2025-09-18 09:12",
    "weekday": 3,
    "issue_id": 5959,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.14033",
            "title": "SAIL-VL2 Technical Report",
            "url": "https://huggingface.co/papers/2509.14033",
            "abstract": "SAIL-VL2, a vision-language foundation model, achieves state-of-the-art performance across diverse benchmarks through data curation, progressive training, and sparse MoE architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM) for comprehensive multimodal understanding and reasoning. As the successor to SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B parameter scales across diverse image and video benchmarks, demonstrating strong capabilities from fine-grained perception to complex reasoning. Three core innovations drive its effectiveness. First, a large-scale data curation pipeline with scoring and filtering strategies enhances both quality and distribution across captioning, OCR, QA, and video data, improving training efficiency. Second, a progressive training framework begins with a powerful pre-trained vision encoder (SAIL-ViT), advances through multimodal pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that systematically strengthens model capabilities. Third, architectural advances extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs. With these contributions, SAIL-VL2 demonstrates competitive performance across 106 datasets and achieves state-of-the-art results on challenging reasoning benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass leaderboard, SAIL-VL2-2B ranks first among officially released open-source models under the 4B parameter scale, while serving as an efficient and extensible foundation for the open-source multimodal community.",
            "score": 22,
            "issue_id": 5952,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "e11196999afc4056",
            "authors": [
                "Weijie Yin",
                "Yongjie Ye",
                "Fangxun Shu",
                "Yue Liao",
                "Zijian Kang",
                "Hongyuan Dong",
                "Haiyang Yu",
                "Dingkang Yang",
                "Jiacong Wang",
                "Han Wang",
                "Wenzhuo Liu",
                "Xiao Liang",
                "Shuicheng Yan",
                "Chao Feng"
            ],
            "affiliations": [
                "Douyin SAIL Team, LV-NUS Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14033.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#data",
                    "#agi",
                    "#dataset",
                    "#multimodal",
                    "#reasoning",
                    "#open_source",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "SAIL-VL2: Передовая мультимодальная модель для понимания зрения и языка",
                    "desc": "SAIL-VL2 - это мультимодальная языковая модель для понимания и рассуждений на основе зрения и языка. Модель достигает высоких результатов на различных бенчмарках благодаря трем ключевым инновациям: улучшенному процессу подготовки данных, прогрессивному обучению и архитектуре разреженной смеси экспертов (Mixture-of-Experts). SAIL-VL2 демонстрирует конкурентоспособные результаты на 106 наборах данных и достигает лучших показателей на сложных тестах рассуждений. Модель является эффективной и расширяемой основой для открытого мультимодального сообщества."
                },
                "en": {
                    "title": "SAIL-VL2: Pioneering Multimodal Understanding with Efficiency and Precision",
                    "desc": "SAIL-VL2 is a cutting-edge vision-language foundation model designed for advanced multimodal understanding and reasoning. It utilizes a large-scale data curation process to enhance the quality of training data, which improves the model's performance across various tasks like captioning and question answering. The model employs a progressive training approach that starts with a pre-trained vision encoder and evolves through multimodal training to a hybrid fine-tuning method, enhancing its reasoning capabilities. Additionally, SAIL-VL2 incorporates a sparse Mixture-of-Experts architecture, allowing it to achieve state-of-the-art results on numerous benchmarks while remaining efficient and scalable for the open-source community."
                },
                "zh": {
                    "title": "SAIL-VL2：视觉与语言的完美结合",
                    "desc": "SAIL-VL2是一种视觉-语言基础模型，能够在多种基准测试中实现最先进的性能。它通过大规模数据整理、渐进式训练和稀疏专家混合架构等创新方法，提升了模型的理解和推理能力。该模型在图像和视频处理方面表现出色，能够进行细致的感知和复杂的推理。SAIL-VL2在106个数据集上展现了竞争力，并在一些具有挑战性的推理基准上取得了优异的成绩。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14008",
            "title": "Hala Technical Report: Building Arabic-Centric Instruction & Translation\n  Models at Scale",
            "url": "https://huggingface.co/papers/2509.14008",
            "abstract": "Hala, a family of Arabic-centric instruction and translation models, achieves state-of-the-art results using a translate-and-tune pipeline, slerp merging, and fine-tuning on high-quality bilingual supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Hala, a family of Arabic-centric instruction and translation models built with our translate-and-tune pipeline. We first compress a strong ARleftrightarrowEN teacher to FP8 (yielding sim2times higher throughput with no quality loss) and use it to create high-fidelity bilingual supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this data and used to translate high-quality English instruction sets into Arabic, producing a million-scale corpus tailored to instruction following. We train Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to balance Arabic specialization with base-model strengths. On Arabic-centric benchmarks, Hala achieves state-of-the-art results within both the \"nano\" (leq2B) and \"small\" (7-9B) categories, outperforming their bases. We release models, data, evaluation, and recipes to accelerate research in Arabic NLP.",
            "score": 16,
            "issue_id": 5958,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "a5e0eac7de173e25",
            "pdf_title_img": "assets/pdf/title_img/2509.14008.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#data",
                    "#open_source",
                    "#low_resource",
                    "#training",
                    "#multilingual",
                    "#machine_translation",
                    "#dataset"
                ],
                "emoji": "🕌",
                "ru": {
                    "title": "Прорыв в арабоязычном ИИ: модели Hala устанавливают новый стандарт",
                    "desc": "Hala - это семейство моделей машинного обучения, специализирующихся на арабском языке для задач следования инструкциям и перевода. Модели Hala используют инновационный подход 'translate-and-tune', который включает сжатие учительской модели, создание двуязычных данных и тонкую настройку языковой модели. Применение метода слияния slerp позволяет сбалансировать специализацию на арабском языке с сильными сторонами базовой модели. Hala достигает передовых результатов в арабоязычных бенчмарках, превосходя базовые модели в категориях 'нано' и 'малых' моделей."
                },
                "en": {
                    "title": "Hala: Advancing Arabic NLP with State-of-the-Art Models",
                    "desc": "Hala is a series of advanced models designed specifically for Arabic instruction and translation tasks. It utilizes a unique translate-and-tune pipeline that enhances performance by merging models and fine-tuning them with high-quality bilingual data. The models are trained in various sizes, from 350M to 9B parameters, and achieve top results on Arabic benchmarks, demonstrating their effectiveness in both small and large categories. By releasing these models and resources, Hala aims to foster further research in Arabic natural language processing (NLP)."
                },
                "zh": {
                    "title": "Hala：阿拉伯语指令与翻译的突破性模型",
                    "desc": "Hala是一系列以阿拉伯语为中心的指令和翻译模型，采用翻译与调优的流程。我们首先将强大的ARleftrightarrowEN教师模型压缩到FP8格式，从而在不损失质量的情况下实现两倍的吞吐量，并利用其创建高保真的双语监督数据。接着，我们对轻量级语言模型LFM2-1.2B进行微调，使其能够将高质量的英语指令集翻译成阿拉伯语，生成一个百万规模的专门用于指令跟随的语料库。Hala模型在阿拉伯语基准测试中表现出色，超越了基础模型，推动了阿拉伯自然语言处理的研究进展。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14232",
            "title": "GenExam: A Multidisciplinary Text-to-Image Exam",
            "url": "https://huggingface.co/papers/2509.14232",
            "abstract": "GenExam is a benchmark for evaluating text-to-image generation in exam-style settings across multiple disciplines, highlighting the challenges in integrating knowledge, reasoning, and generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Exams are a fundamental test of expert-level intelligence and require integrated understanding, reasoning, and generation. Existing exam-style benchmarks mainly focus on understanding and reasoning tasks, and current generation benchmarks emphasize the illustration of world knowledge and visual concepts, neglecting the evaluation of rigorous drawing exams. We introduce GenExam, the first benchmark for multidisciplinary text-to-image exams, featuring 1,000 samples across 10 subjects with exam-style prompts organized under a four-level taxonomy. Each problem is equipped with ground-truth images and fine-grained scoring points to enable a precise evaluation of semantic correctness and visual plausibility. Experiments show that even state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve less than 15% strict scores, and most models yield almost 0%, suggesting the great challenge of our benchmark. By framing image generation as an exam, GenExam offers a rigorous assessment of models' ability to integrate knowledge, reasoning, and generation, providing insights on the path to general AGI.",
            "score": 12,
            "issue_id": 5952,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "40b921aaa9b5c031",
            "authors": [
                "Zhaokai Wang",
                "Penghao Yin",
                "Xiangyu Zhao",
                "Changyao Tian",
                "Yu Qiao",
                "Wenhai Wang",
                "Jifeng Dai",
                "Gen Luo"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14232.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#agi",
                    "#reasoning",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "GenExam: Экзамен для ИИ в генерации изображений",
                    "desc": "GenExam - это новый эталонный тест для оценки генерации изображений по тексту в экзаменационном формате по различным дисциплинам. Он включает 1000 образцов по 10 предметам с экзаменационными заданиями, организованными по четырехуровневой таксономии. Каждая задача снабжена эталонными изображениями и детальными критериями оценки для точного анализа семантической корректности и визуального правдоподобия. Эксперименты показали, что даже современные модели машинного обучения, такие как GPT-Image-1 и Gemini-2.5-Flash-Image, достигают менее 15% строгих оценок, что указывает на сложность этого теста."
                },
                "en": {
                    "title": "GenExam: A New Standard for Text-to-Image Generation Challenges",
                    "desc": "GenExam is a new benchmark designed to evaluate how well AI can generate images based on text prompts in an exam-like format across various subjects. It addresses the gap in existing benchmarks that focus mainly on understanding and reasoning, by including rigorous drawing tasks that require both knowledge and creativity. The benchmark consists of 1,000 samples organized into a four-level taxonomy, each with ground-truth images and detailed scoring criteria for accurate assessment. Results show that even advanced models struggle significantly, achieving less than 15% accuracy, highlighting the complexity of integrating knowledge, reasoning, and image generation in AI."
                },
                "zh": {
                    "title": "GenExam：多学科文本到图像生成的考试基准",
                    "desc": "GenExam是一个用于评估文本到图像生成的基准，特别是在考试风格的设置中，涵盖多个学科。该基准强调了在知识整合、推理和生成方面的挑战。GenExam包含1000个样本，分为10个学科，采用四级分类法组织考试提示，并配有真实图像和细致的评分标准，以便精确评估语义正确性和视觉可信度。实验结果表明，即使是最先进的模型也难以在该基准上取得高分，显示出这一领域的巨大挑战。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13755",
            "title": "Scrub It Out! Erasing Sensitive Memorization in Code Language Models via\n  Machine Unlearning",
            "url": "https://huggingface.co/papers/2509.13755",
            "abstract": "CodeEraser effectively and efficiently removes sensitive memorized information from Code Language Models using machine unlearning techniques without full retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t While Code Language Models (CLMs) have demonstrated superior performance in software engineering tasks such as code generation and summarization, recent empirical studies reveal a critical privacy vulnerability: these models exhibit unintended memorization of sensitive training data, enabling verbatim reproduction of confidential information when specifically prompted. To address this issue, several approaches, including training data de-duplication and differential privacy augmentation, have been proposed. However, these methods require full-model retraining for deployed CLMs, which incurs substantial computational costs. In this paper, we aim to answer the following research question: Can sensitive information memorized by CLMs be erased effectively and efficiently?   We conduct a pioneering investigation into erasing sensitive memorization in CLMs through machine unlearning - a post-hoc modification method that removes specific information from trained models without requiring full retraining. Specifically, we first quantify the memorization risks of sensitive data within CLM training datasets and curate a high-risk dataset of 50,000 sensitive memorized samples as unlearning targets. We study two widely used gradient ascent-based unlearning approaches: the vanilla and constraint-based methods, and introduce CodeEraser, an advanced variant that selectively unlearns sensitive memorized segments in code while preserving the structural integrity and functional correctness of the surrounding code. Extensive experiments on three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder, validate the effectiveness and efficiency of CodeEraser in erasing targeted sensitive memorization while maintaining model utility.",
            "score": 6,
            "issue_id": 5958,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "c349dd90390abf85",
            "authors": [
                "Zhaoyang Chu",
                "Yao Wan",
                "Zhikun Zhang",
                "Di Wang",
                "Zhou Yang",
                "Hongyu Zhang",
                "Pan Zhou",
                "Xuanhua Shi",
                "Hai Jin",
                "David Lo"
            ],
            "affiliations": [
                "Chongqing University, Chongqing, China",
                "Huazhong University of Science and Technology, Wuhan, China",
                "King Abdullah University of Science and Technology, Thuwal, Saudi Arabia",
                "Singapore Management University, Singapore, Singapore",
                "University of Alberta, Edmonton, Canada",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13755.jpg",
            "data": {
                "categories": [
                    "#leakage",
                    "#inference",
                    "#data",
                    "#training",
                    "#security"
                ],
                "emoji": "🔐",
                "ru": {
                    "title": "Эффективное стирание памяти ИИ без потери функциональности",
                    "desc": "Статья представляет CodeEraser - метод для удаления конфиденциальной информации из моделей кодирования (CLM) без полной переподготовки. Исследователи выявили проблему непреднамеренного запоминания CLM чувствительных данных из обучающей выборки. CodeEraser использует технику машинного разобучения для эффективного удаления такой информации. Эксперименты на нескольких семействах CLM подтвердили эффективность метода в стирании целевой конфиденциальной информации при сохранении полезности модели."
                },
                "en": {
                    "title": "Erase Sensitive Data Without Full Retraining!",
                    "desc": "This paper introduces CodeEraser, a method designed to remove sensitive information that Code Language Models (CLMs) have memorized without needing to retrain the entire model. It highlights the privacy risks associated with CLMs, which can inadvertently reproduce confidential data. The authors explore machine unlearning techniques to selectively erase this memorized information while keeping the model's performance intact. Through experiments on various CLMs, they demonstrate that CodeEraser effectively mitigates memorization risks while preserving the model's functionality."
                },
                "zh": {
                    "title": "CodeEraser：高效去除代码模型中的敏感记忆",
                    "desc": "CodeEraser 是一种有效且高效的技术，旨在通过机器遗忘方法从代码语言模型中删除敏感的记忆信息，而无需进行全面的重训练。研究表明，代码语言模型在软件工程任务中表现优异，但它们可能会无意中记住敏感的训练数据，导致在特定提示下重现机密信息。为了解决这个问题，本文提出了一种创新的方法，通过后期修改来去除训练模型中的特定信息，避免了高昂的计算成本。通过对三种代码语言模型的广泛实验，验证了 CodeEraser 在去除敏感记忆的同时保持模型效用的有效性和效率。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13761",
            "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical\n  Reasoning",
            "url": "https://huggingface.co/papers/2509.13761",
            "abstract": "THOR, a tool-integrated hierarchical optimization framework using RL, enhances mathematical reasoning and code generation by constructing high-quality datasets, optimizing reasoning paths, and correcting errors during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have made remarkable progress in mathematical reasoning, but still continue to struggle with high-precision tasks like numerical computation and formal symbolic manipulation. Integrating external tools has emerged as a promising approach to bridge this gap. Despite recent advances, existing methods struggle with three key challenges: constructing tool-integrated reasoning data, performing fine-grained optimization, and enhancing inference. To overcome these limitations, we propose THOR (Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen, a multi-agent actor-critic-based pipeline for constructing high-quality datasets of tool-integrated reasoning paths, aligning with the policy and generalizing well across diverse models. Second, to perform fine-grained hierarchical optimization, we introduce an RL strategy that jointly optimizes for both trajectory-level problem solving and step-level code generation. This is motivated by our key insight that the success of an intermediate tool call is a strong predictor of the final answer's correctness. Finally, THOR incorporates a self-correction mechanism that leverages immediate tool feedback to dynamically revise erroneous reasoning paths during inference. Our approach demonstrates strong generalization across diverse models, performing effectively in both reasoning and non-reasoning models. It further achieves state-of-the-art performance for models of a similar scale on multiple mathematical benchmarks, while also delivering consistent improvements on code benchmarks. Our code will be publicly available at https://github.com/JingMog/THOR.",
            "score": 5,
            "issue_id": 5952,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "d7bf12df5bb3d467",
            "authors": [
                "Qikai Chang",
                "Zhenrong Zhang",
                "Pengfei Hu",
                "Jiefeng Ma",
                "Yicheng Pan",
                "Jianshu Zhang",
                "Jun Du",
                "Quan Liu",
                "Jianqing Gao"
            ],
            "affiliations": [
                "University of Science and Technology of China",
                "iFLYTEK Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13761.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#math",
                    "#rl",
                    "#dataset",
                    "#reasoning",
                    "#training",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "THOR: Интеллектуальная оптимизация математических рассуждений и кода с помощью ИИ",
                    "desc": "THOR - это фреймворк для оптимизации математических рассуждений и генерации кода с использованием обучения с подкреплением. Он решает три ключевые проблемы: создание качественных наборов данных с интегрированными инструментами, оптимизацию путей рассуждений и исправление ошибок во время вывода. THOR использует многоагентный подход для генерации данных, иерархическую оптимизацию для улучшения решения задач и генерации кода, а также механизм самокоррекции на основе обратной связи от инструментов. Фреймворк демонстрирует сильную обобщающую способность и достигает высоких результатов на различных математических и кодовых тестах."
                },
                "en": {
                    "title": "THOR: Optimizing Reasoning with Reinforcement Learning and Tool Integration",
                    "desc": "THOR is a novel framework that enhances mathematical reasoning and code generation by integrating reinforcement learning (RL) with external tools. It addresses challenges in constructing high-quality datasets and optimizing reasoning paths through a multi-agent actor-critic approach called TIRGen. Additionally, THOR employs a hierarchical optimization strategy that focuses on both problem-solving and code generation, leveraging the success of tool calls to predict the correctness of answers. The framework also features a self-correction mechanism that allows it to dynamically adjust reasoning paths based on immediate feedback, leading to improved performance across various mathematical and coding tasks."
                },
                "zh": {
                    "title": "THOR：提升数学推理与代码生成的智能工具",
                    "desc": "THOR是一个基于强化学习的工具集成层次优化框架，旨在提升数学推理和代码生成的能力。它通过构建高质量的数据集、优化推理路径和在推理过程中纠正错误来实现这一目标。THOR引入了多智能体的演员-评论家管道TIRGen，以生成工具集成推理路径的数据集，并采用强化学习策略进行细粒度的层次优化。该框架还结合了自我纠正机制，利用即时工具反馈动态修正推理过程中的错误，展现出在多种模型上的强泛化能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14055",
            "title": "Wan-Animate: Unified Character Animation and Replacement with Holistic\n  Replication",
            "url": "https://huggingface.co/papers/2509.14055",
            "abstract": "Wan-Animate is a unified framework for character animation and replacement, using spatially-aligned skeleton signals and implicit facial features to generate high-fidelity character videos with seamless environmental integration.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Wan-Animate, a unified framework for character animation and replacement. Given a character image and a reference video, Wan-Animate can animate the character by precisely replicating the expressions and movements of the character in the video to generate high-fidelity character videos. Alternatively, it can integrate the animated character into the reference video to replace the original character, replicating the scene's lighting and color tone to achieve seamless environmental integration. Wan-Animate is built upon the Wan model. To adapt it for character animation tasks, we employ a modified input paradigm to differentiate between reference conditions and regions for generation. This design unifies multiple tasks into a common symbolic representation. We use spatially-aligned skeleton signals to replicate body motion and implicit facial features extracted from source images to reenact expressions, enabling the generation of character videos with high controllability and expressiveness. Furthermore, to enhance environmental integration during character replacement, we develop an auxiliary Relighting LoRA. This module preserves the character's appearance consistency while applying the appropriate environmental lighting and color tone. Experimental results demonstrate that Wan-Animate achieves state-of-the-art performance. We are committed to open-sourcing the model weights and its source code.",
            "score": 4,
            "issue_id": 5952,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "da77342ad4a41fa8",
            "authors": [
                "Gang Cheng",
                "Xin Gao",
                "Li Hu",
                "Siqi Hu",
                "Mingyang Huang",
                "Chaonan Ji",
                "Ju Li",
                "Dechao Meng",
                "Jinwei Qi",
                "Penchong Qiao",
                "Zhen Shen",
                "Yafei Song",
                "Ke Sun",
                "Linrui Tian",
                "Feng Wang",
                "Guangyuan Wang",
                "Qi Wang",
                "Zhongjian Wang",
                "Jiayu Xiao",
                "Sheng Xu",
                "Bang Zhang",
                "Peng Zhang",
                "Xindi Zhang",
                "Zhe Zhang",
                "Jingren Zhou",
                "Lian Zhuo"
            ],
            "affiliations": [
                "HumanAIGC Team Tongyi Lab, Alibaba"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14055.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#video",
                    "#multimodal",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Революция в анимации персонажей: от скелета до реалистичного видео",
                    "desc": "Wan-Animate - это унифицированная система для анимации и замены персонажей в видео. Она использует пространственно-выровненные сигналы скелета и неявные лицевые признаки для создания высококачественных видео с персонажами. Система может точно воспроизводить выражения лица и движения персонажа из эталонного видео, а также интегрировать анимированного персонажа в исходное видео, сохраняя освещение и цветовой тон сцены. Wan-Animate основан на модели Wan и использует модифицированную парадигму ввода для различения условий ссылки и областей генерации."
                },
                "en": {
                    "title": "Seamless Character Animation and Replacement with Wan-Animate",
                    "desc": "Wan-Animate is a comprehensive framework designed for character animation and replacement, utilizing spatially-aligned skeleton signals and implicit facial features. It allows users to animate a character by mimicking the expressions and movements from a reference video, resulting in high-quality character videos. Additionally, it can seamlessly integrate animated characters into existing scenes by matching the lighting and color tones of the environment. The framework employs a modified input approach to unify various tasks into a single symbolic representation, enhancing both controllability and expressiveness in character animation."
                },
                "zh": {
                    "title": "Wan-Animate：无缝角色动画与替换的统一框架",
                    "desc": "Wan-Animate是一个统一的角色动画和替换框架，利用空间对齐的骨骼信号和隐式面部特征生成高保真的角色视频。该框架可以根据给定的角色图像和参考视频，精确复制角色的表情和动作，实现角色动画。它还可以将动画角色无缝地集成到参考视频中，保持场景的光照和色调一致。Wan-Animate通过修改输入范式，统一多个任务为一个共同的符号表示，展示了在角色动画任务中的高可控性和表现力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.12989",
            "title": "PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era",
            "url": "https://huggingface.co/papers/2509.12989",
            "abstract": "Recent advancements in omnidirectional vision, driven by industrial and academic interest, have led to breakthroughs in generation, perception, and understanding, with the proposal of a new system architecture called PANORAMA.  \t\t\t\t\tAI-generated summary \t\t\t\t Omnidirectional vision, using 360-degree vision to understand the environment, has become increasingly critical across domains like robotics, industrial inspection, and environmental monitoring. Compared to traditional pinhole vision, omnidirectional vision provides holistic environmental awareness, significantly enhancing the completeness of scene perception and the reliability of decision-making. However, foundational research in this area has historically lagged behind traditional pinhole vision. This talk presents an emerging trend in the embodied AI era: the rapid development of omnidirectional vision, driven by growing industrial demand and academic interest. We highlight recent breakthroughs in omnidirectional generation, omnidirectional perception, omnidirectional understanding, and related datasets. Drawing on insights from both academia and industry, we propose an ideal panoramic system architecture in the embodied AI era, PANORAMA, which consists of four key subsystems. Moreover, we offer in-depth opinions related to emerging trends and cross-community impacts at the intersection of panoramic vision and embodied AI, along with the future roadmap and open challenges. This overview synthesizes state-of-the-art advancements and outlines challenges and opportunities for future research in building robust, general-purpose omnidirectional AI systems in the embodied AI era.",
            "score": 3,
            "issue_id": 5959,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 сентября",
                "en": "September 16",
                "zh": "9月16日"
            },
            "hash": "ff414da8b6853ab9",
            "authors": [
                "Xu Zheng",
                "Chenfei Liao",
                "Ziqiao Weng",
                "Kaiyu Lei",
                "Zihao Dongfang",
                "Haocong He",
                "Yuanhuiyi Lyu",
                "Lutao Jiang",
                "Lu Qi",
                "Li Chen",
                "Danda Pani Paudel",
                "Kailun Yang",
                "Linfeng Zhang",
                "Luc Van Gool",
                "Xuming Hu"
            ],
            "affiliations": [
                "CSE, HKUST",
                "HKUST(GZ)",
                "Hunan University",
                "INSAIT, Sofia University St. Kliment Ohridski",
                "Insta360 Presenter",
                "Shanghai Jiao Tong University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.12989.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#robotics",
                    "#architecture"
                ],
                "emoji": "🌐",
                "ru": {
                    "title": "PANORAMA: новый взгляд на всенаправленное зрение в эпоху воплощенного ИИ",
                    "desc": "Статья описывает развитие всенаправленного зрения в области искусственного интеллекта. Авторы представляют новую системную архитектуру PANORAMA, состоящую из четырех подсистем для всенаправленной генерации, восприятия и понимания. Рассматриваются прорывы в этой области, связанные с растущим промышленным и академическим интересом. Обсуждаются перспективы и открытые проблемы в создании надежных всенаправленных ИИ-систем для воплощенного искусственного интеллекта."
                },
                "en": {
                    "title": "Revolutionizing Vision: The PANORAMA Architecture for 360-Degree Understanding",
                    "desc": "This paper discusses the advancements in omnidirectional vision, which allows machines to see and understand their surroundings in 360 degrees. It introduces a new system architecture called PANORAMA, designed to enhance the capabilities of AI in various applications like robotics and environmental monitoring. The paper highlights recent breakthroughs in omnidirectional generation, perception, and understanding, emphasizing the importance of these developments in improving decision-making processes. Additionally, it outlines future challenges and opportunities for research in creating effective omnidirectional AI systems."
                },
                "zh": {
                    "title": "全向视觉：未来人工智能的关键",
                    "desc": "全向视觉是指利用360度视野来理解环境，近年来在机器人、工业检测和环境监测等领域变得越来越重要。与传统的针孔视觉相比，全向视觉提供了更全面的环境感知，显著提高了场景感知的完整性和决策的可靠性。尽管这一领域的基础研究相对滞后，但随着工业需求和学术兴趣的增长，全向视觉的快速发展正在成为一种新趋势。本文提出了一种理想的全景系统架构PANORAMA，包含四个关键子系统，并探讨了全向视觉与具身人工智能交叉领域的未来挑战和机遇。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13450",
            "title": "SteeringControl: Holistic Evaluation of Alignment Steering in LLMs",
            "url": "https://huggingface.co/papers/2509.13450",
            "abstract": "SteeringControl evaluates representation steering methods across bias, harmful generation, and hallucination, revealing tradeoffs and entanglement effects on secondary behaviors like sycophancy and commonsense morality.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SteeringControl, a benchmark for evaluating representation steering methods across core alignment objectives--bias, harmful generation, and hallucination--and their effects on secondary behaviors such as sycophancy and commonsense morality. While prior alignment work often highlights truthfulness or reasoning ability to demonstrate the side effects of representation steering, we find there are many unexplored tradeoffs not yet understood in a systematic way. We collect a dataset of safety-relevant primary and secondary behaviors to evaluate steering effectiveness and behavioral entanglement centered around five popular steering methods. To enable this, we craft a modular steering framework based on unique components that serve as the building blocks of many existing methods. Our results on Qwen-2.5-7B and Llama-3.1-8B find that strong steering performance is dependent on the specific combination of steering method, model, and targeted behavior, and that severe concept entanglement can result from poor combinations of these three as well. We release our code here: https://github.com/wang-research-lab/SteeringControl.git.",
            "score": 2,
            "issue_id": 5956,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 сентября",
                "en": "September 16",
                "zh": "9月16日"
            },
            "hash": "b8fbcce2f7976676",
            "authors": [
                "Vincent Siu",
                "Nicholas Crispino",
                "David Park",
                "Nathan W. Henry",
                "Zhun Wang",
                "Yang Liu",
                "Dawn Song",
                "Chenguang Wang"
            ],
            "affiliations": [
                "University of California, Berkeley",
                "University of California, Santa Cruz",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13450.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#alignment",
                    "#ethics",
                    "#hallucinations",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "🎛️",
                "ru": {
                    "title": "SteeringControl: Комплексная оценка методов управления ИИ",
                    "desc": "SteeringControl - это новый бенчмарк для оценки методов управления представлениями в контексте ключевых целей выравнивания ИИ, таких как предвзятость, вредоносная генерация и галлюцинации. Исследование выявляет неожиданные компромиссы и эффекты запутывания во вторичных поведениях, например, подхалимстве и здравом смысле. Авторы создали модульную структуру управления и собрали набор данных для оценки эффективности управления и поведенческой запутанности. Результаты показывают, что производительность сильно зависит от конкретной комбинации метода управления, модели и целевого поведения."
                },
                "en": {
                    "title": "Navigating the Tradeoffs of AI Steering Methods",
                    "desc": "SteeringControl is a benchmark designed to assess various representation steering methods in machine learning, focusing on key alignment goals such as bias, harmful generation, and hallucination. The study uncovers the complex tradeoffs and entanglement effects these methods have on secondary behaviors, including sycophancy and commonsense morality. By creating a dataset that captures both primary and secondary safety-relevant behaviors, the research evaluates the effectiveness of different steering techniques. The findings indicate that the success of steering methods is highly dependent on the specific combinations of the method, model, and targeted behavior, highlighting the risks of concept entanglement in poor configurations."
                },
                "zh": {
                    "title": "评估表示引导方法的权衡与影响",
                    "desc": "SteeringControl是一个基准，用于评估表示引导方法在偏见、有害生成和幻觉等核心对齐目标上的表现，以及它们对次要行为（如谄媚和常识道德）的影响。我们发现，许多未被系统理解的权衡关系影响着表示引导的效果。通过收集与安全相关的主要和次要行为数据集，我们评估了五种流行引导方法的有效性和行为纠缠。我们的研究表明，引导性能强弱依赖于引导方法、模型和目标行为的特定组合，且不良组合可能导致严重的概念纠缠。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14142",
            "title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,\n  Results, Discussion, and Outlook",
            "url": "https://huggingface.co/papers/2509.14142",
            "abstract": "The MARS2 2025 Challenge focuses on multimodal reasoning with large language models through real-world and specialized scenarios, evaluating 40+ models across three competition tracks using tailored datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim to bring together different approaches in multimodal machine learning and LLMs via a large benchmark. We hope it better allows researchers to follow the state-of-the-art in this very dynamic area. Meanwhile, a growing number of testbeds have boosted the evolution of general-purpose large language models. Thus, this year's MARS2 focuses on real-world and specialized scenarios to broaden the multimodal reasoning applications of MLLMs. Our organizing team released two tailored datasets Lens and AdsQA as test sets, which support general reasoning in 12 daily scenarios and domain-specific reasoning in advertisement videos, respectively. We evaluated 40+ baselines that include both generalist MLLMs and task-specific models, and opened up three competition tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and industrial institutions have registered and 40+ valid submissions (out of 1200+) have been included in our ranking lists. Our datasets, code sets (40+ baselines and 15+ participants' methods), and rankings are publicly available on the MARS2 workshop website and our GitHub organization page https://github.com/mars2workshop/, where our updates and announcements of upcoming events will be continuously provided.",
            "score": 0,
            "issue_id": 5952,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 сентября",
                "en": "September 17",
                "zh": "9月17日"
            },
            "hash": "00749969a87efa2f",
            "authors": [
                "Peng Xu",
                "Shengwu Xiong",
                "Jiajun Zhang",
                "Yaxiong Chen",
                "Bowen Zhou",
                "Chen Change Loy",
                "David A. Clifton",
                "Kyoung Mu Lee",
                "Luc Van Gool",
                "Ruiming He",
                "Ruilin Yao",
                "Xinwei Long",
                "Jirui Huang",
                "Kai Tian",
                "Sa Yang",
                "Yihua Shao",
                "Jin Feng",
                "Yue Zhong",
                "Jiakai Zhou",
                "Cheng Tang",
                "Tianyu Zou",
                "Yifang Zhang",
                "Junming Liang",
                "Guoyou Li",
                "Zhaoxiang Wang",
                "Qiang Zhou",
                "Yichen Zhao",
                "Shili Xiong",
                "Hyeongjin Nam",
                "Jaerin Lee",
                "Jaeyoung Chung",
                "JoonKyu Park",
                "Junghun Oh",
                "Kanggeon Lee",
                "Wooseok Lee",
                "Juneyoung Ro",
                "Turghun Osman",
                "Can Hu",
                "Chaoyang Liao",
                "Cheng Chen",
                "Chengcheng Han",
                "Chenhao Qiu",
                "Chong Peng",
                "Cong Xu",
                "Dailin Li",
                "Feiyu Wang",
                "Feng Gao",
                "Guibo Zhu",
                "Guopeng Tang",
                "Haibo Lu",
                "Han Fang",
                "Han Qi",
                "Hanxiao Wu",
                "Haobo Cheng",
                "Hongbo Sun",
                "Hongyao Chen",
                "Huayong Hu",
                "Hui Li",
                "Jiaheng Ma",
                "Jiang Yu",
                "Jianing Wang",
                "Jie Yang",
                "Jing He",
                "Jinglin Zhou",
                "Jingxuan Li",
                "Josef Kittler",
                "Lihao Zheng",
                "Linnan Zhao",
                "Mengxi Jia",
                "Muyang Yan",
                "Nguyen Thanh Thien",
                "Pu Luo",
                "Qi Li",
                "Shien Song",
                "Shijie Dong",
                "Shuai Shao",
                "Shutao Li",
                "Taofeng Xue",
                "Tianyang Xu",
                "Tianyi Gao",
                "Tingting Li",
                "Wei Zhang",
                "Weiyang Su",
                "Xiaodong Dong",
                "Xiao-Jun Wu",
                "Xiaopeng Zhou",
                "Xin Chen",
                "Xin Wei",
                "Xinyi You",
                "Xudong Kang",
                "Xujie Zhou",
                "Xusheng Liu",
                "Yanan Wang",
                "Yanbin Huang",
                "Yang Liu",
                "Yang Yang",
                "Yanglin Deng",
                "Yashu Kang",
                "Ye Yuan",
                "Yi Wen",
                "Yicen Tian",
                "Yilin Tao",
                "Yin Tang",
                "Yipeng Lin",
                "Yiqing Wang",
                "Yiting Xi",
                "Yongkang Yu",
                "Yumei Li",
                "Yuxin Qin",
                "Yuying Chen",
                "Yuzhe Cen",
                "Zhaofan Zou",
                "Zhaohong Liu",
                "Zhehao Shen",
                "Zhenglin Du",
                "Zhengyang Li",
                "Zhenni Huang",
                "Zhenwei Shao",
                "Zhilong Song",
                "Zhiyong Feng",
                "Zhiyu Wang",
                "Zhou Yu",
                "Ziang Li",
                "Zihan Zhai",
                "Zijian Zhang",
                "Ziyang Peng",
                "Ziyun Xiao",
                "Zongshu Li"
            ],
            "affiliations": [
                "ByteDance",
                "Meituan",
                "NVIDIA",
                "Samsung"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14142.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#reasoning",
                    "#open_source",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Новый рубеж в мультимодальном ИИ: соревнование MARS2 2025",
                    "desc": "Статья описывает соревнование MARS2 2025 по мультимодальному рассуждению с использованием больших языковых моделей. Организаторы создали два специализированных набора данных для оценки моделей в реальных и специфических сценариях. В соревновании участвовало более 40 базовых моделей и 76 команд, которые соревновались в трех направлениях: визуальная привязка в реальных сценариях, визуальные вопросы-ответы с пространственным пониманием и визуальное рассуждение в рекламных видео. Все данные, код и результаты доступны публично для дальнейших исследований в этой динамично развивающейся области."
                },
                "en": {
                    "title": "Advancing Multimodal Reasoning with MARS2 2025 Challenge",
                    "desc": "The MARS2 2025 Challenge is a competition that evaluates multimodal reasoning capabilities of large language models (LLMs) using real-world and specialized scenarios. It features tailored datasets, Lens and AdsQA, designed for general reasoning and domain-specific tasks in advertisement videos. Over 40 models were assessed across three tracks: Visual Grounding in Real-world Scenarios, Visual Question Answering with Spatial Awareness, and Visual Reasoning in Creative Advertisement Videos. The challenge aims to advance the field of multimodal machine learning by providing a comprehensive benchmark and fostering collaboration among researchers."
                },
                "zh": {
                    "title": "多模态推理的未来挑战",
                    "desc": "MARS2 2025挑战专注于多模态推理，利用大型语言模型在真实世界和专业场景中进行评估。我们通过定制的数据集，评估了40多个模型，涵盖了视觉定位、视觉问答和创意广告视频等多个竞赛方向。此次挑战旨在促进多模态机器学习和大型语言模型的不同方法的结合，推动该领域的研究进展。我们提供的Lens和AdsQA数据集支持日常场景和广告视频中的推理，促进了多模态推理应用的扩展。"
                }
            }
        }
    ],
    "link_prev": "2025-09-17.html",
    "link_next": "2025-09-19.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "17.09",
        "en": "09/17",
        "zh": "9月17日"
    },
    "short_date_next": {
        "ru": "19.09",
        "en": "09/19",
        "zh": "9月19日"
    },
    "categories": {
        "#dataset": 5,
        "#data": 4,
        "#benchmark": 5,
        "#agents": 0,
        "#cv": 2,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 3,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 1,
        "#agi": 2,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 4,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 1,
        "#leakage": 1,
        "#open_source": 4,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    }
}