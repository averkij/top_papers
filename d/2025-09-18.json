{
    "date": {
        "ru": "18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 18",
        "zh": "9æœˆ18æ—¥"
    },
    "time_utc": "2025-09-18 19:08",
    "weekday": 3,
    "issue_id": 5969,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2509.14008",
            "title": "Hala Technical Report: Building Arabic-Centric Instruction & Translation\n  Models at Scale",
            "url": "https://huggingface.co/papers/2509.14008",
            "abstract": "Hala, a family of Arabic-centric instruction and translation models, achieves state-of-the-art results using a translate-and-tune pipeline, slerp merging, and fine-tuning on high-quality bilingual supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Hala, a family of Arabic-centric instruction and translation models built with our translate-and-tune pipeline. We first compress a strong ARleftrightarrowEN teacher to FP8 (yielding sim2times higher throughput with no quality loss) and use it to create high-fidelity bilingual supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this data and used to translate high-quality English instruction sets into Arabic, producing a million-scale corpus tailored to instruction following. We train Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to balance Arabic specialization with base-model strengths. On Arabic-centric benchmarks, Hala achieves state-of-the-art results within both the \"nano\" (leq2B) and \"small\" (7-9B) categories, outperforming their bases. We release models, data, evaluation, and recipes to accelerate research in Arabic NLP.",
            "score": 59,
            "issue_id": 5958,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "a5e0eac7de173e25",
            "authors": [
                "Hasan Abed Al Kader Hammoud",
                "Mohammad Zbeeb",
                "Bernard Ghanem"
            ],
            "affiliations": [
                "KAUST"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14008.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#data",
                    "#open_source",
                    "#low_resource",
                    "#training",
                    "#multilingual",
                    "#machine_translation",
                    "#dataset"
                ],
                "emoji": "ğŸ•Œ",
                "ru": {
                    "title": "ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ°Ñ€Ğ°Ğ±Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ğ˜Ğ˜: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Hala ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚",
                    "desc": "Hala - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ° Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Hala Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ 'translate-and-tune', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ slerp Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Hala Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ°Ñ€Ğ°Ğ±Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑÑ… 'Ğ½Ğ°Ğ½Ğ¾' Ğ¸ 'Ğ¼Ğ°Ğ»Ñ‹Ñ…' Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Hala: Advancing Arabic NLP with State-of-the-Art Models",
                    "desc": "Hala is a series of advanced models designed specifically for Arabic instruction and translation tasks. It utilizes a unique translate-and-tune pipeline that enhances performance by merging models and fine-tuning them with high-quality bilingual data. The models are trained in various sizes, from 350M to 9B parameters, and achieve top results on Arabic benchmarks, demonstrating their effectiveness in both small and large categories. By releasing these models and resources, Hala aims to foster further research in Arabic natural language processing (NLP)."
                },
                "zh": {
                    "title": "Halaï¼šé˜¿æ‹‰ä¼¯è¯­æŒ‡ä»¤ä¸ç¿»è¯‘çš„çªç ´æ€§æ¨¡å‹",
                    "desc": "Halaæ˜¯ä¸€ç³»åˆ—ä»¥é˜¿æ‹‰ä¼¯è¯­ä¸ºä¸­å¿ƒçš„æŒ‡ä»¤å’Œç¿»è¯‘æ¨¡å‹ï¼Œé‡‡ç”¨ç¿»è¯‘ä¸è°ƒä¼˜çš„æµç¨‹ã€‚æˆ‘ä»¬é¦–å…ˆå°†å¼ºå¤§çš„ARleftrightarrowENæ•™å¸ˆæ¨¡å‹å‹ç¼©åˆ°FP8æ ¼å¼ï¼Œä»è€Œåœ¨ä¸æŸå¤±è´¨é‡çš„æƒ…å†µä¸‹å®ç°ä¸¤å€çš„ååé‡ï¼Œå¹¶åˆ©ç”¨å…¶åˆ›å»ºé«˜ä¿çœŸçš„åŒè¯­ç›‘ç£æ•°æ®ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å¯¹è½»é‡çº§è¯­è¨€æ¨¡å‹LFM2-1.2Bè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿå°†é«˜è´¨é‡çš„è‹±è¯­æŒ‡ä»¤é›†ç¿»è¯‘æˆé˜¿æ‹‰ä¼¯è¯­ï¼Œç”Ÿæˆä¸€ä¸ªç™¾ä¸‡è§„æ¨¡çš„ä¸“é—¨ç”¨äºæŒ‡ä»¤è·Ÿéšçš„è¯­æ–™åº“ã€‚Halaæ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†åŸºç¡€æ¨¡å‹ï¼Œæ¨åŠ¨äº†é˜¿æ‹‰ä¼¯è‡ªç„¶è¯­è¨€å¤„ç†çš„ç ”ç©¶è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14033",
            "title": "SAIL-VL2 Technical Report",
            "url": "https://huggingface.co/papers/2509.14033",
            "abstract": "SAIL-VL2, a vision-language foundation model, achieves state-of-the-art performance across diverse benchmarks through data curation, progressive training, and sparse MoE architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM) for comprehensive multimodal understanding and reasoning. As the successor to SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B parameter scales across diverse image and video benchmarks, demonstrating strong capabilities from fine-grained perception to complex reasoning. Three core innovations drive its effectiveness. First, a large-scale data curation pipeline with scoring and filtering strategies enhances both quality and distribution across captioning, OCR, QA, and video data, improving training efficiency. Second, a progressive training framework begins with a powerful pre-trained vision encoder (SAIL-ViT), advances through multimodal pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that systematically strengthens model capabilities. Third, architectural advances extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs. With these contributions, SAIL-VL2 demonstrates competitive performance across 106 datasets and achieves state-of-the-art results on challenging reasoning benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass leaderboard, SAIL-VL2-2B ranks first among officially released open-source models under the 4B parameter scale, while serving as an efficient and extensible foundation for the open-source multimodal community.",
            "score": 27,
            "issue_id": 5952,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "e11196999afc4056",
            "authors": [
                "Weijie Yin",
                "Yongjie Ye",
                "Fangxun Shu",
                "Yue Liao",
                "Zijian Kang",
                "Hongyuan Dong",
                "Haiyang Yu",
                "Dingkang Yang",
                "Jiacong Wang",
                "Han Wang",
                "Wenzhuo Liu",
                "Xiao Liang",
                "Shuicheng Yan",
                "Chao Feng"
            ],
            "affiliations": [
                "Douyin SAIL Team, LV-NUS Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14033.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#data",
                    "#agi",
                    "#dataset",
                    "#multimodal",
                    "#reasoning",
                    "#open_source",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "SAIL-VL2: ĞŸĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°",
                    "desc": "SAIL-VL2 - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ‚Ñ€ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸ÑĞ¼: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑƒ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (Mixture-of-Experts). SAIL-VL2 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 106 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ğ¾Ğ¹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°."
                },
                "en": {
                    "title": "SAIL-VL2: Pioneering Multimodal Understanding with Efficiency and Precision",
                    "desc": "SAIL-VL2 is a cutting-edge vision-language foundation model designed for advanced multimodal understanding and reasoning. It utilizes a large-scale data curation process to enhance the quality of training data, which improves the model's performance across various tasks like captioning and question answering. The model employs a progressive training approach that starts with a pre-trained vision encoder and evolves through multimodal training to a hybrid fine-tuning method, enhancing its reasoning capabilities. Additionally, SAIL-VL2 incorporates a sparse Mixture-of-Experts architecture, allowing it to achieve state-of-the-art results on numerous benchmarks while remaining efficient and scalable for the open-source community."
                },
                "zh": {
                    "title": "SAIL-VL2ï¼šè§†è§‰ä¸è¯­è¨€çš„å®Œç¾ç»“åˆ",
                    "desc": "SAIL-VL2æ˜¯ä¸€ç§è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒé€šè¿‡å¤§è§„æ¨¡æ•°æ®æ•´ç†ã€æ¸è¿›å¼è®­ç»ƒå’Œç¨€ç–ä¸“å®¶æ··åˆæ¶æ„ç­‰åˆ›æ–°æ–¹æ³•ï¼Œæå‡äº†æ¨¡å‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿè¿›è¡Œç»†è‡´çš„æ„ŸçŸ¥å’Œå¤æ‚çš„æ¨ç†ã€‚SAIL-VL2åœ¨106ä¸ªæ•°æ®é›†ä¸Šå±•ç°äº†ç«äº‰åŠ›ï¼Œå¹¶åœ¨ä¸€äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åŸºå‡†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æˆç»©ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.12989",
            "title": "PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era",
            "url": "https://huggingface.co/papers/2509.12989",
            "abstract": "Recent advancements in omnidirectional vision, driven by industrial and academic interest, have led to breakthroughs in generation, perception, and understanding, with the proposal of a new system architecture called PANORAMA.  \t\t\t\t\tAI-generated summary \t\t\t\t Omnidirectional vision, using 360-degree vision to understand the environment, has become increasingly critical across domains like robotics, industrial inspection, and environmental monitoring. Compared to traditional pinhole vision, omnidirectional vision provides holistic environmental awareness, significantly enhancing the completeness of scene perception and the reliability of decision-making. However, foundational research in this area has historically lagged behind traditional pinhole vision. This talk presents an emerging trend in the embodied AI era: the rapid development of omnidirectional vision, driven by growing industrial demand and academic interest. We highlight recent breakthroughs in omnidirectional generation, omnidirectional perception, omnidirectional understanding, and related datasets. Drawing on insights from both academia and industry, we propose an ideal panoramic system architecture in the embodied AI era, PANORAMA, which consists of four key subsystems. Moreover, we offer in-depth opinions related to emerging trends and cross-community impacts at the intersection of panoramic vision and embodied AI, along with the future roadmap and open challenges. This overview synthesizes state-of-the-art advancements and outlines challenges and opportunities for future research in building robust, general-purpose omnidirectional AI systems in the embodied AI era.",
            "score": 20,
            "issue_id": 5959,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 16",
                "zh": "9æœˆ16æ—¥"
            },
            "hash": "ff414da8b6853ab9",
            "authors": [
                "Xu Zheng",
                "Chenfei Liao",
                "Ziqiao Weng",
                "Kaiyu Lei",
                "Zihao Dongfang",
                "Haocong He",
                "Yuanhuiyi Lyu",
                "Lutao Jiang",
                "Lu Qi",
                "Li Chen",
                "Danda Pani Paudel",
                "Kailun Yang",
                "Linfeng Zhang",
                "Luc Van Gool",
                "Xuming Hu"
            ],
            "affiliations": [
                "CSE, HKUST",
                "HKUST(GZ)",
                "Hunan University",
                "INSAIT, Sofia University St. Kliment Ohridski",
                "Insta360 Presenter",
                "Shanghai Jiao Tong University",
                "Wuhan University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.12989.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#robotics",
                    "#architecture"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "PANORAMA: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ²ÑĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ²ÑĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ PANORAMA, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰ÑƒÑ Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ¿Ğ¾Ğ´ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´Ğ»Ñ Ğ²ÑĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ñ€Ğ°ÑÑ‚ÑƒÑ‰Ğ¸Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ¾Ğ¼. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ²ÑĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Vision: The PANORAMA Architecture for 360-Degree Understanding",
                    "desc": "This paper discusses the advancements in omnidirectional vision, which allows machines to see and understand their surroundings in 360 degrees. It introduces a new system architecture called PANORAMA, designed to enhance the capabilities of AI in various applications like robotics and environmental monitoring. The paper highlights recent breakthroughs in omnidirectional generation, perception, and understanding, emphasizing the importance of these developments in improving decision-making processes. Additionally, it outlines future challenges and opportunities for research in creating effective omnidirectional AI systems."
                },
                "zh": {
                    "title": "å…¨å‘è§†è§‰ï¼šæœªæ¥äººå·¥æ™ºèƒ½çš„å…³é”®",
                    "desc": "å…¨å‘è§†è§‰æ˜¯æŒ‡åˆ©ç”¨360åº¦è§†é‡æ¥ç†è§£ç¯å¢ƒï¼Œè¿‘å¹´æ¥åœ¨æœºå™¨äººã€å·¥ä¸šæ£€æµ‹å’Œç¯å¢ƒç›‘æµ‹ç­‰é¢†åŸŸå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ä¸ä¼ ç»Ÿçš„é’ˆå­”è§†è§‰ç›¸æ¯”ï¼Œå…¨å‘è§†è§‰æä¾›äº†æ›´å…¨é¢çš„ç¯å¢ƒæ„ŸçŸ¥ï¼Œæ˜¾è‘—æé«˜äº†åœºæ™¯æ„ŸçŸ¥çš„å®Œæ•´æ€§å’Œå†³ç­–çš„å¯é æ€§ã€‚å°½ç®¡è¿™ä¸€é¢†åŸŸçš„åŸºç¡€ç ”ç©¶ç›¸å¯¹æ»åï¼Œä½†éšç€å·¥ä¸šéœ€æ±‚å’Œå­¦æœ¯å…´è¶£çš„å¢é•¿ï¼Œå…¨å‘è§†è§‰çš„å¿«é€Ÿå‘å±•æ­£åœ¨æˆä¸ºä¸€ç§æ–°è¶‹åŠ¿ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç†æƒ³çš„å…¨æ™¯ç³»ç»Ÿæ¶æ„PANORAMAï¼ŒåŒ…å«å››ä¸ªå…³é”®å­ç³»ç»Ÿï¼Œå¹¶æ¢è®¨äº†å…¨å‘è§†è§‰ä¸å…·èº«äººå·¥æ™ºèƒ½äº¤å‰é¢†åŸŸçš„æœªæ¥æŒ‘æˆ˜å’Œæœºé‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14232",
            "title": "GenExam: A Multidisciplinary Text-to-Image Exam",
            "url": "https://huggingface.co/papers/2509.14232",
            "abstract": "GenExam is a benchmark for evaluating text-to-image generation in exam-style settings across multiple disciplines, highlighting the challenges in integrating knowledge, reasoning, and generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Exams are a fundamental test of expert-level intelligence and require integrated understanding, reasoning, and generation. Existing exam-style benchmarks mainly focus on understanding and reasoning tasks, and current generation benchmarks emphasize the illustration of world knowledge and visual concepts, neglecting the evaluation of rigorous drawing exams. We introduce GenExam, the first benchmark for multidisciplinary text-to-image exams, featuring 1,000 samples across 10 subjects with exam-style prompts organized under a four-level taxonomy. Each problem is equipped with ground-truth images and fine-grained scoring points to enable a precise evaluation of semantic correctness and visual plausibility. Experiments show that even state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve less than 15% strict scores, and most models yield almost 0%, suggesting the great challenge of our benchmark. By framing image generation as an exam, GenExam offers a rigorous assessment of models' ability to integrate knowledge, reasoning, and generation, providing insights on the path to general AGI.",
            "score": 16,
            "issue_id": 5952,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "40b921aaa9b5c031",
            "authors": [
                "Zhaokai Wang",
                "Penghao Yin",
                "Xiangyu Zhao",
                "Changyao Tian",
                "Yu Qiao",
                "Wenhai Wang",
                "Jifeng Dai",
                "Gen Luo"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "The Chinese University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14232.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#agi",
                    "#reasoning",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "GenExam: Ğ­ĞºĞ·Ğ°Ğ¼ĞµĞ½ Ğ´Ğ»Ñ Ğ˜Ğ˜ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "GenExam - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ² ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ğ¼. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1000 Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¿Ğ¾ 10 Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ°Ğ¼ Ñ ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸. ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° ÑĞ½Ğ°Ğ±Ğ¶ĞµĞ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-Image-1 Ğ¸ Gemini-2.5-Flash-Image, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 15% ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ°."
                },
                "en": {
                    "title": "GenExam: A New Standard for Text-to-Image Generation Challenges",
                    "desc": "GenExam is a new benchmark designed to evaluate how well AI can generate images based on text prompts in an exam-like format across various subjects. It addresses the gap in existing benchmarks that focus mainly on understanding and reasoning, by including rigorous drawing tasks that require both knowledge and creativity. The benchmark consists of 1,000 samples organized into a four-level taxonomy, each with ground-truth images and detailed scoring criteria for accurate assessment. Results show that even advanced models struggle significantly, achieving less than 15% accuracy, highlighting the complexity of integrating knowledge, reasoning, and image generation in AI."
                },
                "zh": {
                    "title": "GenExamï¼šå¤šå­¦ç§‘æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„è€ƒè¯•åŸºå‡†",
                    "desc": "GenExamæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„åŸºå‡†ï¼Œç‰¹åˆ«æ˜¯åœ¨è€ƒè¯•é£æ ¼çš„è®¾ç½®ä¸­ï¼Œæ¶µç›–å¤šä¸ªå­¦ç§‘ã€‚è¯¥åŸºå‡†å¼ºè°ƒäº†åœ¨çŸ¥è¯†æ•´åˆã€æ¨ç†å’Œç”Ÿæˆæ–¹é¢çš„æŒ‘æˆ˜ã€‚GenExamåŒ…å«1000ä¸ªæ ·æœ¬ï¼Œåˆ†ä¸º10ä¸ªå­¦ç§‘ï¼Œé‡‡ç”¨å››çº§åˆ†ç±»æ³•ç»„ç»‡è€ƒè¯•æç¤ºï¼Œå¹¶é…æœ‰çœŸå®å›¾åƒå’Œç»†è‡´çš„è¯„åˆ†æ ‡å‡†ï¼Œä»¥ä¾¿ç²¾ç¡®è¯„ä¼°è¯­ä¹‰æ­£ç¡®æ€§å’Œè§†è§‰å¯ä¿¡åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿéš¾ä»¥åœ¨è¯¥åŸºå‡†ä¸Šå–å¾—é«˜åˆ†ï¼Œæ˜¾ç¤ºå‡ºè¿™ä¸€é¢†åŸŸçš„å·¨å¤§æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13755",
            "title": "Scrub It Out! Erasing Sensitive Memorization in Code Language Models via\n  Machine Unlearning",
            "url": "https://huggingface.co/papers/2509.13755",
            "abstract": "CodeEraser effectively and efficiently removes sensitive memorized information from Code Language Models using machine unlearning techniques without full retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t While Code Language Models (CLMs) have demonstrated superior performance in software engineering tasks such as code generation and summarization, recent empirical studies reveal a critical privacy vulnerability: these models exhibit unintended memorization of sensitive training data, enabling verbatim reproduction of confidential information when specifically prompted. To address this issue, several approaches, including training data de-duplication and differential privacy augmentation, have been proposed. However, these methods require full-model retraining for deployed CLMs, which incurs substantial computational costs. In this paper, we aim to answer the following research question: Can sensitive information memorized by CLMs be erased effectively and efficiently?   We conduct a pioneering investigation into erasing sensitive memorization in CLMs through machine unlearning - a post-hoc modification method that removes specific information from trained models without requiring full retraining. Specifically, we first quantify the memorization risks of sensitive data within CLM training datasets and curate a high-risk dataset of 50,000 sensitive memorized samples as unlearning targets. We study two widely used gradient ascent-based unlearning approaches: the vanilla and constraint-based methods, and introduce CodeEraser, an advanced variant that selectively unlearns sensitive memorized segments in code while preserving the structural integrity and functional correctness of the surrounding code. Extensive experiments on three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder, validate the effectiveness and efficiency of CodeEraser in erasing targeted sensitive memorization while maintaining model utility.",
            "score": 12,
            "issue_id": 5958,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "c349dd90390abf85",
            "authors": [
                "Zhaoyang Chu",
                "Yao Wan",
                "Zhikun Zhang",
                "Di Wang",
                "Zhou Yang",
                "Hongyu Zhang",
                "Pan Zhou",
                "Xuanhua Shi",
                "Hai Jin",
                "David Lo"
            ],
            "affiliations": [
                "Chongqing University, Chongqing, China",
                "Huazhong University of Science and Technology, Wuhan, China",
                "King Abdullah University of Science and Technology, Thuwal, Saudi Arabia",
                "Singapore Management University, Singapore, Singapore",
                "University of Alberta, Edmonton, Canada",
                "Zhejiang University, Hangzhou, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13755.jpg",
            "data": {
                "categories": [
                    "#leakage",
                    "#inference",
                    "#data",
                    "#training",
                    "#security"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑÑ‚Ğ¸Ñ€Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CodeEraser - Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (CLM) Ğ±ĞµĞ· Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ CLM Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸. CodeEraser Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ°Ñ… CLM Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² ÑÑ‚Ğ¸Ñ€Ğ°Ğ½Ğ¸Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ´ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Erase Sensitive Data Without Full Retraining!",
                    "desc": "This paper introduces CodeEraser, a method designed to remove sensitive information that Code Language Models (CLMs) have memorized without needing to retrain the entire model. It highlights the privacy risks associated with CLMs, which can inadvertently reproduce confidential data. The authors explore machine unlearning techniques to selectively erase this memorized information while keeping the model's performance intact. Through experiments on various CLMs, they demonstrate that CodeEraser effectively mitigates memorization risks while preserving the model's functionality."
                },
                "zh": {
                    "title": "CodeEraserï¼šé«˜æ•ˆå»é™¤ä»£ç æ¨¡å‹ä¸­çš„æ•æ„Ÿè®°å¿†",
                    "desc": "CodeEraser æ˜¯ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡æœºå™¨é—å¿˜æ–¹æ³•ä»ä»£ç è¯­è¨€æ¨¡å‹ä¸­åˆ é™¤æ•æ„Ÿçš„è®°å¿†ä¿¡æ¯ï¼Œè€Œæ— éœ€è¿›è¡Œå…¨é¢çš„é‡è®­ç»ƒã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»£ç è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†å®ƒä»¬å¯èƒ½ä¼šæ— æ„ä¸­è®°ä½æ•æ„Ÿçš„è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´åœ¨ç‰¹å®šæç¤ºä¸‹é‡ç°æœºå¯†ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡åæœŸä¿®æ”¹æ¥å»é™¤è®­ç»ƒæ¨¡å‹ä¸­çš„ç‰¹å®šä¿¡æ¯ï¼Œé¿å…äº†é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€‚é€šè¿‡å¯¹ä¸‰ç§ä»£ç è¯­è¨€æ¨¡å‹çš„å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº† CodeEraser åœ¨å»é™¤æ•æ„Ÿè®°å¿†çš„åŒæ—¶ä¿æŒæ¨¡å‹æ•ˆç”¨çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2508.14880",
            "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A\n  Knowledge-Informed Trajectory Synthesis Framework",
            "url": "https://huggingface.co/papers/2508.14880",
            "abstract": "A medical deep research agent using medical knowledge graphs and a custom retrieval engine achieves state-of-the-art performance on medical benchmarks while maintaining competitiveness in general research tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent developments in Large Language Model (LLM)-based agents have shown impressive capabilities spanning multiple domains, exemplified by deep research systems that demonstrate superior performance on complex information-seeking and synthesis tasks. While general-purpose deep research agents have shown impressive capabilities, they struggle significantly with medical domain challenges, as evidenced by leading proprietary systems achieving limited accuracy on complex medical benchmarks. The key limitations are: (1) the model lacks sufficient dense medical knowledge for clinical reasoning, and (2) the framework is constrained by the absence of specialized retrieval tools tailored for medical contexts.We present a medical deep research agent that addresses these challenges through two core innovations. First, we develop a novel data synthesis framework using medical knowledge graphs, extracting the longest chains from subgraphs around rare medical entities to generate complex multi-hop question-answer pairs. Second, we integrate a custom-built private medical retrieval engine alongside general-purpose tools, enabling accurate medical information synthesis. Our approach generates 2100+ diverse trajectories across 12 medical specialties, each averaging 4.2 tool interactions.Through a two-stage training paradigm combining supervised fine-tuning and online reinforcement learning with composite rewards, our MedResearcher-R1-32B model demonstrates exceptional performance, establishing new state-of-the-art results on medical benchmarks while maintaining competitive performance on general deep research tasks. Our work demonstrates that strategic domain-specific innovations in architecture, tool design, and training data construction can enable smaller open-source models to outperform much larger proprietary systems in specialized domains.",
            "score": 9,
            "issue_id": 5964,
            "pub_date": "2025-08-20",
            "pub_date_card": {
                "ru": "20 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
                "en": "August 20",
                "zh": "8æœˆ20æ—¥"
            },
            "hash": "b45591e23fcc0d2a",
            "authors": [
                "Ailing Yu",
                "Lan Yao",
                "Jingnan Liu",
                "Zhe Chen",
                "Jiajun Yin",
                "Yuan Wang",
                "Xinhao Liao",
                "Zhiling Ye",
                "Ji Li",
                "Yun Yue",
                "Hansong Xiao",
                "Hualei Zhou",
                "Chunxiao Guo",
                "Peng Wei",
                "Junwei Liu",
                "Jinjie Gu"
            ],
            "affiliations": [
                "Ant Group",
                "Harbin Institute of Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2508.14880.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#agents",
                    "#optimization",
                    "#open_source",
                    "#science",
                    "#dataset",
                    "#training",
                    "#architecture",
                    "#graphs",
                    "#healthcare"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¹ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² AI-Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ³ĞµĞ½Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ°. Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Revolutionizing Medical Research with Specialized AI Agents",
                    "desc": "This paper introduces a medical deep research agent that utilizes medical knowledge graphs and a specialized retrieval engine to enhance performance in medical tasks. The agent addresses limitations of existing models by employing a novel data synthesis framework that generates complex question-answer pairs from medical subgraphs. Additionally, it integrates a custom retrieval engine tailored for medical contexts, allowing for accurate information synthesis. The resulting model, MedResearcher-R1-32B, achieves state-of-the-art results on medical benchmarks while remaining competitive in general research tasks, showcasing the effectiveness of domain-specific innovations."
                },
                "zh": {
                    "title": "åŒ»ç–—æ·±åº¦ç ”ç©¶ä»£ç†ï¼šè¶…è¶Šä¼ ç»Ÿçš„åˆ›æ–°ä¹‹è·¯",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŒ»ç–—æ·±åº¦ç ”ç©¶ä»£ç†ï¼Œåˆ©ç”¨åŒ»ç–—çŸ¥è¯†å›¾è°±å’Œå®šåˆ¶çš„æ£€ç´¢å¼•æ“ï¼Œè§£å†³äº†åŒ»ç–—é¢†åŸŸä¸­çš„ä¿¡æ¯æ£€ç´¢å’Œç»¼åˆé—®é¢˜ã€‚è¯¥ä»£ç†é€šè¿‡æå–ç¨€æœ‰åŒ»ç–—å®ä½“å‘¨å›´çš„å­å›¾ï¼Œç”Ÿæˆå¤æ‚çš„å¤šè·³é—®ç­”å¯¹ï¼Œä»è€Œå¢å¼ºäº†ä¸´åºŠæ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ç»“åˆç›‘ç£å¾®è°ƒå’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œæ¨¡å‹åœ¨åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„æœ€ä½³æˆç»©ï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬ç ”ç©¶ä»»åŠ¡ä¸­ä¹Ÿä¿æŒäº†ç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œé’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„åˆ›æ–°å¯ä»¥ä½¿è¾ƒå°çš„å¼€æºæ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸè¶…è¶Šæ›´å¤§çš„ä¸“æœ‰ç³»ç»Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14142",
            "title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,\n  Results, Discussion, and Outlook",
            "url": "https://huggingface.co/papers/2509.14142",
            "abstract": "The MARS2 2025 Challenge focuses on multimodal reasoning with large language models through real-world and specialized scenarios, evaluating 40+ models across three competition tracks using tailored datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim to bring together different approaches in multimodal machine learning and LLMs via a large benchmark. We hope it better allows researchers to follow the state-of-the-art in this very dynamic area. Meanwhile, a growing number of testbeds have boosted the evolution of general-purpose large language models. Thus, this year's MARS2 focuses on real-world and specialized scenarios to broaden the multimodal reasoning applications of MLLMs. Our organizing team released two tailored datasets Lens and AdsQA as test sets, which support general reasoning in 12 daily scenarios and domain-specific reasoning in advertisement videos, respectively. We evaluated 40+ baselines that include both generalist MLLMs and task-specific models, and opened up three competition tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and industrial institutions have registered and 40+ valid submissions (out of 1200+) have been included in our ranking lists. Our datasets, code sets (40+ baselines and 15+ participants' methods), and rankings are publicly available on the MARS2 workshop website and our GitHub organization page https://github.com/mars2workshop/, where our updates and announcements of upcoming events will be continuously provided.",
            "score": 6,
            "issue_id": 5952,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "00749969a87efa2f",
            "authors": [
                "Peng Xu",
                "Shengwu Xiong",
                "Jiajun Zhang",
                "Yaxiong Chen",
                "Bowen Zhou",
                "Chen Change Loy",
                "David A. Clifton",
                "Kyoung Mu Lee",
                "Luc Van Gool",
                "Ruiming He",
                "Ruilin Yao",
                "Xinwei Long",
                "Jirui Huang",
                "Kai Tian",
                "Sa Yang",
                "Yihua Shao",
                "Jin Feng",
                "Yue Zhong",
                "Jiakai Zhou",
                "Cheng Tang",
                "Tianyu Zou",
                "Yifang Zhang",
                "Junming Liang",
                "Guoyou Li",
                "Zhaoxiang Wang",
                "Qiang Zhou",
                "Yichen Zhao",
                "Shili Xiong",
                "Hyeongjin Nam",
                "Jaerin Lee",
                "Jaeyoung Chung",
                "JoonKyu Park",
                "Junghun Oh",
                "Kanggeon Lee",
                "Wooseok Lee",
                "Juneyoung Ro",
                "Turghun Osman",
                "Can Hu",
                "Chaoyang Liao",
                "Cheng Chen",
                "Chengcheng Han",
                "Chenhao Qiu",
                "Chong Peng",
                "Cong Xu",
                "Dailin Li",
                "Feiyu Wang",
                "Feng Gao",
                "Guibo Zhu",
                "Guopeng Tang",
                "Haibo Lu",
                "Han Fang",
                "Han Qi",
                "Hanxiao Wu",
                "Haobo Cheng",
                "Hongbo Sun",
                "Hongyao Chen",
                "Huayong Hu",
                "Hui Li",
                "Jiaheng Ma",
                "Jiang Yu",
                "Jianing Wang",
                "Jie Yang",
                "Jing He",
                "Jinglin Zhou",
                "Jingxuan Li",
                "Josef Kittler",
                "Lihao Zheng",
                "Linnan Zhao",
                "Mengxi Jia",
                "Muyang Yan",
                "Nguyen Thanh Thien",
                "Pu Luo",
                "Qi Li",
                "Shien Song",
                "Shijie Dong",
                "Shuai Shao",
                "Shutao Li",
                "Taofeng Xue",
                "Tianyang Xu",
                "Tianyi Gao",
                "Tingting Li",
                "Wei Zhang",
                "Weiyang Su",
                "Xiaodong Dong",
                "Xiao-Jun Wu",
                "Xiaopeng Zhou",
                "Xin Chen",
                "Xin Wei",
                "Xinyi You",
                "Xudong Kang",
                "Xujie Zhou",
                "Xusheng Liu",
                "Yanan Wang",
                "Yanbin Huang",
                "Yang Liu",
                "Yang Yang",
                "Yanglin Deng",
                "Yashu Kang",
                "Ye Yuan",
                "Yi Wen",
                "Yicen Tian",
                "Yilin Tao",
                "Yin Tang",
                "Yipeng Lin",
                "Yiqing Wang",
                "Yiting Xi",
                "Yongkang Yu",
                "Yumei Li",
                "Yuxin Qin",
                "Yuying Chen",
                "Yuzhe Cen",
                "Zhaofan Zou",
                "Zhaohong Liu",
                "Zhehao Shen",
                "Zhenglin Du",
                "Zhengyang Li",
                "Zhenni Huang",
                "Zhenwei Shao",
                "Zhilong Song",
                "Zhiyong Feng",
                "Zhiyu Wang",
                "Zhou Yu",
                "Ziang Li",
                "Zihan Zhai",
                "Zijian Zhang",
                "Ziyang Peng",
                "Ziyun Xiao",
                "Zongshu Li"
            ],
            "affiliations": [
                "ByteDance",
                "Meituan",
                "NVIDIA",
                "Samsung"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14142.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#multimodal",
                    "#reasoning",
                    "#open_source",
                    "#benchmark",
                    "#survey"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ˜Ğ˜: ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ MARS2 2025",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ MARS2 2025 Ğ¿Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ’ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑƒÑ‡Ğ°ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ»Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ 40 Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ 76 ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ² Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑÑ…: Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹-Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ñ€ĞµĞºĞ»Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’ÑĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ĞºĞ¾Ğ´ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Advancing Multimodal Reasoning with MARS2 2025 Challenge",
                    "desc": "The MARS2 2025 Challenge is a competition that evaluates multimodal reasoning capabilities of large language models (LLMs) using real-world and specialized scenarios. It features tailored datasets, Lens and AdsQA, designed for general reasoning and domain-specific tasks in advertisement videos. Over 40 models were assessed across three tracks: Visual Grounding in Real-world Scenarios, Visual Question Answering with Spatial Awareness, and Visual Reasoning in Creative Advertisement Videos. The challenge aims to advance the field of multimodal machine learning by providing a comprehensive benchmark and fostering collaboration among researchers."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ¨ç†çš„æœªæ¥æŒ‘æˆ˜",
                    "desc": "MARS2 2025æŒ‘æˆ˜ä¸“æ³¨äºå¤šæ¨¡æ€æ¨ç†ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå’Œä¸“ä¸šåœºæ™¯ä¸­è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬é€šè¿‡å®šåˆ¶çš„æ•°æ®é›†ï¼Œè¯„ä¼°äº†40å¤šä¸ªæ¨¡å‹ï¼Œæ¶µç›–äº†è§†è§‰å®šä½ã€è§†è§‰é—®ç­”å’Œåˆ›æ„å¹¿å‘Šè§†é¢‘ç­‰å¤šä¸ªç«èµ›æ–¹å‘ã€‚æ­¤æ¬¡æŒ‘æˆ˜æ—¨åœ¨ä¿ƒè¿›å¤šæ¨¡æ€æœºå™¨å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸åŒæ–¹æ³•çš„ç»“åˆï¼Œæ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚æˆ‘ä»¬æä¾›çš„Lenså’ŒAdsQAæ•°æ®é›†æ”¯æŒæ—¥å¸¸åœºæ™¯å’Œå¹¿å‘Šè§†é¢‘ä¸­çš„æ¨ç†ï¼Œä¿ƒè¿›äº†å¤šæ¨¡æ€æ¨ç†åº”ç”¨çš„æ‰©å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14055",
            "title": "Wan-Animate: Unified Character Animation and Replacement with Holistic\n  Replication",
            "url": "https://huggingface.co/papers/2509.14055",
            "abstract": "Wan-Animate is a unified framework for character animation and replacement, using spatially-aligned skeleton signals and implicit facial features to generate high-fidelity character videos with seamless environmental integration.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Wan-Animate, a unified framework for character animation and replacement. Given a character image and a reference video, Wan-Animate can animate the character by precisely replicating the expressions and movements of the character in the video to generate high-fidelity character videos. Alternatively, it can integrate the animated character into the reference video to replace the original character, replicating the scene's lighting and color tone to achieve seamless environmental integration. Wan-Animate is built upon the Wan model. To adapt it for character animation tasks, we employ a modified input paradigm to differentiate between reference conditions and regions for generation. This design unifies multiple tasks into a common symbolic representation. We use spatially-aligned skeleton signals to replicate body motion and implicit facial features extracted from source images to reenact expressions, enabling the generation of character videos with high controllability and expressiveness. Furthermore, to enhance environmental integration during character replacement, we develop an auxiliary Relighting LoRA. This module preserves the character's appearance consistency while applying the appropriate environmental lighting and color tone. Experimental results demonstrate that Wan-Animate achieves state-of-the-art performance. We are committed to open-sourcing the model weights and its source code.",
            "score": 6,
            "issue_id": 5952,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "da77342ad4a41fa8",
            "authors": [
                "Gang Cheng",
                "Xin Gao",
                "Li Hu",
                "Siqi Hu",
                "Mingyang Huang",
                "Chaonan Ji",
                "Ju Li",
                "Dechao Meng",
                "Jinwei Qi",
                "Penchong Qiao",
                "Zhen Shen",
                "Yafei Song",
                "Ke Sun",
                "Linrui Tian",
                "Feng Wang",
                "Guangyuan Wang",
                "Qi Wang",
                "Zhongjian Wang",
                "Jiayu Xiao",
                "Sheng Xu",
                "Bang Zhang",
                "Peng Zhang",
                "Xindi Zhang",
                "Zhe Zhang",
                "Jingren Zhou",
                "Lian Zhuo"
            ],
            "affiliations": [
                "HumanAIGC Team Tongyi Lab, Alibaba"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14055.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#video",
                    "#multimodal",
                    "#open_source",
                    "#cv"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹: Ğ¾Ñ‚ ÑĞºĞµĞ»ĞµÑ‚Ğ° Ğ´Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Wan-Animate - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ÑĞºĞµĞ»ĞµÑ‚Ğ° Ğ¸ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ†Ğ° Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ° Ğ¸Ğ· ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ° Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ†Ğ²ĞµÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ ÑÑ†ĞµĞ½Ñ‹. Wan-Animate Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Wan Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Seamless Character Animation and Replacement with Wan-Animate",
                    "desc": "Wan-Animate is a comprehensive framework designed for character animation and replacement, utilizing spatially-aligned skeleton signals and implicit facial features. It allows users to animate a character by mimicking the expressions and movements from a reference video, resulting in high-quality character videos. Additionally, it can seamlessly integrate animated characters into existing scenes by matching the lighting and color tones of the environment. The framework employs a modified input approach to unify various tasks into a single symbolic representation, enhancing both controllability and expressiveness in character animation."
                },
                "zh": {
                    "title": "Wan-Animateï¼šæ— ç¼è§’è‰²åŠ¨ç”»ä¸æ›¿æ¢çš„ç»Ÿä¸€æ¡†æ¶",
                    "desc": "Wan-Animateæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§’è‰²åŠ¨ç”»å’Œæ›¿æ¢æ¡†æ¶ï¼Œåˆ©ç”¨ç©ºé—´å¯¹é½çš„éª¨éª¼ä¿¡å·å’Œéšå¼é¢éƒ¨ç‰¹å¾ç”Ÿæˆé«˜ä¿çœŸçš„è§’è‰²è§†é¢‘ã€‚è¯¥æ¡†æ¶å¯ä»¥æ ¹æ®ç»™å®šçš„è§’è‰²å›¾åƒå’Œå‚è€ƒè§†é¢‘ï¼Œç²¾ç¡®å¤åˆ¶è§’è‰²çš„è¡¨æƒ…å’ŒåŠ¨ä½œï¼Œå®ç°è§’è‰²åŠ¨ç”»ã€‚å®ƒè¿˜å¯ä»¥å°†åŠ¨ç”»è§’è‰²æ— ç¼åœ°é›†æˆåˆ°å‚è€ƒè§†é¢‘ä¸­ï¼Œä¿æŒåœºæ™¯çš„å…‰ç…§å’Œè‰²è°ƒä¸€è‡´ã€‚Wan-Animateé€šè¿‡ä¿®æ”¹è¾“å…¥èŒƒå¼ï¼Œç»Ÿä¸€å¤šä¸ªä»»åŠ¡ä¸ºä¸€ä¸ªå…±åŒçš„ç¬¦å·è¡¨ç¤ºï¼Œå±•ç¤ºäº†åœ¨è§’è‰²åŠ¨ç”»ä»»åŠ¡ä¸­çš„é«˜å¯æ§æ€§å’Œè¡¨ç°åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13761",
            "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical\n  Reasoning",
            "url": "https://huggingface.co/papers/2509.13761",
            "abstract": "THOR, a tool-integrated hierarchical optimization framework using RL, enhances mathematical reasoning and code generation by constructing high-quality datasets, optimizing reasoning paths, and correcting errors during inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have made remarkable progress in mathematical reasoning, but still continue to struggle with high-precision tasks like numerical computation and formal symbolic manipulation. Integrating external tools has emerged as a promising approach to bridge this gap. Despite recent advances, existing methods struggle with three key challenges: constructing tool-integrated reasoning data, performing fine-grained optimization, and enhancing inference. To overcome these limitations, we propose THOR (Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen, a multi-agent actor-critic-based pipeline for constructing high-quality datasets of tool-integrated reasoning paths, aligning with the policy and generalizing well across diverse models. Second, to perform fine-grained hierarchical optimization, we introduce an RL strategy that jointly optimizes for both trajectory-level problem solving and step-level code generation. This is motivated by our key insight that the success of an intermediate tool call is a strong predictor of the final answer's correctness. Finally, THOR incorporates a self-correction mechanism that leverages immediate tool feedback to dynamically revise erroneous reasoning paths during inference. Our approach demonstrates strong generalization across diverse models, performing effectively in both reasoning and non-reasoning models. It further achieves state-of-the-art performance for models of a similar scale on multiple mathematical benchmarks, while also delivering consistent improvements on code benchmarks. Our code will be publicly available at https://github.com/JingMog/THOR.",
            "score": 6,
            "issue_id": 5952,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "d7bf12df5bb3d467",
            "authors": [
                "Qikai Chang",
                "Zhenrong Zhang",
                "Pengfei Hu",
                "Jiefeng Ma",
                "Yicheng Pan",
                "Jianshu Zhang",
                "Jun Du",
                "Quan Liu",
                "Jianqing Gao"
            ],
            "affiliations": [
                "University of Science and Technology of China",
                "iFLYTEK Research"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13761.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#math",
                    "#rl",
                    "#dataset",
                    "#reasoning",
                    "#training",
                    "#benchmark",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "THOR: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "THOR - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. THOR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ĞºĞ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "THOR: Optimizing Reasoning with Reinforcement Learning and Tool Integration",
                    "desc": "THOR is a novel framework that enhances mathematical reasoning and code generation by integrating reinforcement learning (RL) with external tools. It addresses challenges in constructing high-quality datasets and optimizing reasoning paths through a multi-agent actor-critic approach called TIRGen. Additionally, THOR employs a hierarchical optimization strategy that focuses on both problem-solving and code generation, leveraging the success of tool calls to predict the correctness of answers. The framework also features a self-correction mechanism that allows it to dynamically adjust reasoning paths based on immediate feedback, leading to improved performance across various mathematical and coding tasks."
                },
                "zh": {
                    "title": "THORï¼šæå‡æ•°å­¦æ¨ç†ä¸ä»£ç ç”Ÿæˆçš„æ™ºèƒ½å·¥å…·",
                    "desc": "THORæ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„å·¥å…·é›†æˆå±‚æ¬¡ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆçš„èƒ½åŠ›ã€‚å®ƒé€šè¿‡æ„å»ºé«˜è´¨é‡çš„æ•°æ®é›†ã€ä¼˜åŒ–æ¨ç†è·¯å¾„å’Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­çº æ­£é”™è¯¯æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚THORå¼•å…¥äº†å¤šæ™ºèƒ½ä½“çš„æ¼”å‘˜-è¯„è®ºå®¶ç®¡é“TIRGenï¼Œä»¥ç”Ÿæˆå·¥å…·é›†æˆæ¨ç†è·¯å¾„çš„æ•°æ®é›†ï¼Œå¹¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥è¿›è¡Œç»†ç²’åº¦çš„å±‚æ¬¡ä¼˜åŒ–ã€‚è¯¥æ¡†æ¶è¿˜ç»“åˆäº†è‡ªæˆ‘çº æ­£æœºåˆ¶ï¼Œåˆ©ç”¨å³æ—¶å·¥å…·åé¦ˆåŠ¨æ€ä¿®æ­£æ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œå±•ç°å‡ºåœ¨å¤šç§æ¨¡å‹ä¸Šçš„å¼ºæ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13523",
            "title": "AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions",
            "url": "https://huggingface.co/papers/2509.13523",
            "abstract": "AERIS, a large-scale pixel-level Swin diffusion transformer, addresses scaling issues in high-resolution weather forecasting using SWiPe parallelism, achieving high performance and stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative machine learning offers new opportunities to better understand complex Earth system dynamics. Recent diffusion-based methods address spectral biases and improve ensemble calibration in weather forecasting compared to deterministic methods, yet have so far proven difficult to scale stably at high resolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin diffusion transformer to address this gap, and SWiPe, a generalizable technique that composes window parallelism with sequence and pipeline parallelism to shard window-based transformers without added communication cost or increased global batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS (mixed precision) and a peak performance of 11.21 ExaFLOPS with 1 times 1 patch size on the 0.25{\\deg} ERA5 dataset, achieving 95.5% weak scaling efficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS and remains stable on seasonal scales to 90 days, highlighting the potential of billion-parameter diffusion models for weather and climate prediction.",
            "score": 6,
            "issue_id": 5963,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 16",
                "zh": "9æœˆ16æ—¥"
            },
            "hash": "82d40a08f83e0e85",
            "authors": [
                "VÃ¤inÃ¶ HatanpÃ¤Ã¤",
                "Eugene Ku",
                "Jason Stock",
                "Murali Emani",
                "Sam Foreman",
                "Chunyong Jung",
                "Sandeep Madireddy",
                "Tung Nguyen",
                "Varuni Sastry",
                "Ray A. O. Sinurat",
                "Sam Wheeler",
                "Huihuo Zheng",
                "Troy Arcomano",
                "Venkatram Vishwanath",
                "Rao Kotamarthi"
            ],
            "affiliations": [
                "Allen Institute for AI, Seattle, Washington, USA",
                "Argonne National Laboratory, Lemont, Illinois, USA",
                "University of California, Los Angeles, California, USA",
                "University of Chicago, Chicago, Illinois, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13523.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#diffusion",
                    "#training",
                    "#dataset",
                    "#optimization",
                    "#architecture"
                ],
                "emoji": "ğŸŒ¦ï¸",
                "ru": {
                    "title": "AERIS: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "AERIS - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Swin Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ SWiPe Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. AERIS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑÑƒĞ¿ĞµÑ€ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğµ Aurora Ğ¸ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¾ 90 Ğ´Ğ½ĞµĞ¹."
                },
                "en": {
                    "title": "AERIS: Revolutionizing Weather Forecasting with Scalable Diffusion Models",
                    "desc": "The paper introduces AERIS, a powerful pixel-level Swin diffusion transformer designed for high-resolution weather forecasting. It tackles the challenges of scaling diffusion models by employing SWiPe, a novel technique that combines different types of parallelism to enhance performance without increasing communication costs. AERIS demonstrates impressive computational capabilities, achieving over 10 ExaFLOPS on a large-scale computing system while maintaining high efficiency in both weak and strong scaling scenarios. This approach not only improves the accuracy of weather predictions but also showcases the potential of large-scale generative models in understanding complex Earth system dynamics."
                },
                "zh": {
                    "title": "äº¿å‚æ•°æ‰©æ•£æ¨¡å‹åŠ©åŠ›å¤©æ°”é¢„æŠ¥æ–°çªç ´",
                    "desc": "AERISæ˜¯ä¸€ç§å¤§è§„æ¨¡çš„åƒç´ çº§Swinæ‰©æ•£å˜æ¢å™¨ï¼Œæ—¨åœ¨è§£å†³é«˜åˆ†è¾¨ç‡å¤©æ°”é¢„æŠ¥ä¸­çš„æ‰©å±•é—®é¢˜ã€‚å®ƒé‡‡ç”¨SWiPeå¹¶è¡ŒæŠ€æœ¯ï¼Œç»“åˆçª—å£å¹¶è¡Œæ€§ã€åºåˆ—å¹¶è¡Œæ€§å’Œç®¡é“å¹¶è¡Œæ€§ï¼Œæœ‰æ•ˆåœ°æé«˜äº†æ€§èƒ½å’Œç¨³å®šæ€§ã€‚é€šè¿‡åœ¨Auroraä¸Šè¿è¡Œï¼ŒAERISå®ç°äº†é«˜è¾¾10.21 ExaFLOPSçš„è®¡ç®—èƒ½åŠ›ï¼Œå¹¶åœ¨å­£èŠ‚å°ºåº¦ä¸Šä¿æŒç¨³å®šï¼Œå±•ç¤ºäº†äº¿å‚æ•°æ‰©æ•£æ¨¡å‹åœ¨å¤©æ°”å’Œæ°”å€™é¢„æµ‹ä¸­çš„æ½œåŠ›ã€‚è¯¥æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„åœ°çƒç³»ç»ŸåŠ¨æ€æ–¹é¢æä¾›äº†æ–°çš„æœºé‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13683",
            "title": "Improving Context Fidelity via Native Retrieval-Augmented Reasoning",
            "url": "https://huggingface.co/papers/2509.13683",
            "abstract": "CARE, a retrieval-augmented reasoning framework, enhances LLMs by integrating in-context evidence, improving retrieval accuracy and answer generation performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) often struggle with context fidelity, producing inconsistent answers when responding to questions based on provided information. Existing approaches either rely on expensive supervised fine-tuning to generate evidence post-answer or train models to perform web searches without necessarily improving utilization of the given context. We propose CARE, a novel native retrieval-augmented reasoning framework that teaches LLMs to explicitly integrate in-context evidence within their reasoning process with the model's own retrieval capabilities. Our method requires limited labeled evidence data while significantly enhancing both retrieval accuracy and answer generation performance through strategically retrieved in-context tokens in the reasoning chain. Extensive experiments on multiple real-world and counterfactual QA benchmarks demonstrate that our approach substantially outperforms supervised fine-tuning, traditional retrieval-augmented generation methods, and external retrieval solutions. This work represents a fundamental advancement in making LLMs more accurate, reliable, and efficient for knowledge-intensive tasks.",
            "score": 5,
            "issue_id": 5962,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "1aeb81ad2adfec22",
            "authors": [
                "Suyuchen Wang",
                "Jinlin Wang",
                "Xinyu Wang",
                "Shiqi Li",
                "Xiangru Tang",
                "Sirui Hong",
                "Xiao-Wen Chang",
                "Chenglin Wu",
                "Bang Liu"
            ],
            "affiliations": [
                "Canada CIFAR AI Chair",
                "DIRO, UniversitÃ© de MontrÃ©al",
                "McGill University",
                "MetaGPT",
                "Mila - Quebec AI Institute",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13683.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#rag",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "CARE: Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼",
                    "desc": "CARE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ LLM ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµĞµ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². CARE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ²Ğ¿ĞµÑ€ĞµĞ´ Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "CARE: Enhancing LLMs with In-Context Evidence for Better Answers",
                    "desc": "The paper introduces CARE, a framework that improves large language models (LLMs) by integrating in-context evidence during their reasoning process. Unlike traditional methods that require extensive fine-tuning or external searches, CARE allows LLMs to utilize their own retrieval capabilities to enhance answer generation. This approach significantly boosts retrieval accuracy and performance while needing minimal labeled data. The results from various QA benchmarks show that CARE outperforms existing methods, making LLMs more reliable for tasks that require deep knowledge."
                },
                "zh": {
                    "title": "CAREï¼šæå‡LLMçš„æ£€ç´¢ä¸æ¨ç†èƒ½åŠ›",
                    "desc": "CAREæ˜¯ä¸€ä¸ªå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢å¢å¼ºæ¨ç†æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆä¸Šä¸‹æ–‡è¯æ®æ¥æé«˜æ£€ç´¢å‡†ç¡®æ€§å’Œç­”æ¡ˆç”Ÿæˆæ€§èƒ½ã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€ä¾èµ–æ˜‚è´µçš„ç›‘ç£å¾®è°ƒæˆ–è®­ç»ƒæ¨¡å‹è¿›è¡Œç½‘ç»œæœç´¢ï¼Œä½†æœªèƒ½æœ‰æ•ˆåˆ©ç”¨ç»™å®šçš„ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬æå‡ºçš„CAREæ¡†æ¶æ•™ä¼šLLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ˜ç¡®æ•´åˆä¸Šä¸‹æ–‡è¯æ®ï¼Œåˆ©ç”¨æ¨¡å‹è‡ªèº«çš„æ£€ç´¢èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCAREåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œå’Œåäº‹å®é—®ç­”åŸºå‡†ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä»£è¡¨äº†LLMåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„é‡è¦è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13450",
            "title": "SteeringControl: Holistic Evaluation of Alignment Steering in LLMs",
            "url": "https://huggingface.co/papers/2509.13450",
            "abstract": "SteeringControl evaluates representation steering methods across bias, harmful generation, and hallucination, revealing tradeoffs and entanglement effects on secondary behaviors like sycophancy and commonsense morality.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SteeringControl, a benchmark for evaluating representation steering methods across core alignment objectives--bias, harmful generation, and hallucination--and their effects on secondary behaviors such as sycophancy and commonsense morality. While prior alignment work often highlights truthfulness or reasoning ability to demonstrate the side effects of representation steering, we find there are many unexplored tradeoffs not yet understood in a systematic way. We collect a dataset of safety-relevant primary and secondary behaviors to evaluate steering effectiveness and behavioral entanglement centered around five popular steering methods. To enable this, we craft a modular steering framework based on unique components that serve as the building blocks of many existing methods. Our results on Qwen-2.5-7B and Llama-3.1-8B find that strong steering performance is dependent on the specific combination of steering method, model, and targeted behavior, and that severe concept entanglement can result from poor combinations of these three as well. We release our code here: https://github.com/wang-research-lab/SteeringControl.git.",
            "score": 3,
            "issue_id": 5956,
            "pub_date": "2025-09-16",
            "pub_date_card": {
                "ru": "16 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 16",
                "zh": "9æœˆ16æ—¥"
            },
            "hash": "b8fbcce2f7976676",
            "authors": [
                "Vincent Siu",
                "Nicholas Crispino",
                "David Park",
                "Nathan W. Henry",
                "Zhun Wang",
                "Yang Liu",
                "Dawn Song",
                "Chenguang Wang"
            ],
            "affiliations": [
                "University of California, Berkeley",
                "University of California, Santa Cruz",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13450.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#alignment",
                    "#ethics",
                    "#hallucinations",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ›ï¸",
                "ru": {
                    "title": "SteeringControl: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜",
                    "desc": "SteeringControl - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒ, Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑÑ‹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ·Ğ°Ğ¿ÑƒÑ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ÑÑ…, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¿Ğ¾Ğ´Ñ…Ğ°Ğ»Ğ¸Ğ¼ÑÑ‚Ğ²Ğµ Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¼ ÑĞ¼Ñ‹ÑĞ»Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑ‚Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Navigating the Tradeoffs of AI Steering Methods",
                    "desc": "SteeringControl is a benchmark designed to assess various representation steering methods in machine learning, focusing on key alignment goals such as bias, harmful generation, and hallucination. The study uncovers the complex tradeoffs and entanglement effects these methods have on secondary behaviors, including sycophancy and commonsense morality. By creating a dataset that captures both primary and secondary safety-relevant behaviors, the research evaluates the effectiveness of different steering techniques. The findings indicate that the success of steering methods is highly dependent on the specific combinations of the method, model, and targeted behavior, highlighting the risks of concept entanglement in poor configurations."
                },
                "zh": {
                    "title": "è¯„ä¼°è¡¨ç¤ºå¼•å¯¼æ–¹æ³•çš„æƒè¡¡ä¸å½±å“",
                    "desc": "SteeringControlæ˜¯ä¸€ä¸ªåŸºå‡†ï¼Œç”¨äºè¯„ä¼°è¡¨ç¤ºå¼•å¯¼æ–¹æ³•åœ¨åè§ã€æœ‰å®³ç”Ÿæˆå’Œå¹»è§‰ç­‰æ ¸å¿ƒå¯¹é½ç›®æ ‡ä¸Šçš„è¡¨ç°ï¼Œä»¥åŠå®ƒä»¬å¯¹æ¬¡è¦è¡Œä¸ºï¼ˆå¦‚è°„åªšå’Œå¸¸è¯†é“å¾·ï¼‰çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œè®¸å¤šæœªè¢«ç³»ç»Ÿç†è§£çš„æƒè¡¡å…³ç³»å½±å“ç€è¡¨ç¤ºå¼•å¯¼çš„æ•ˆæœã€‚é€šè¿‡æ”¶é›†ä¸å®‰å…¨ç›¸å…³çš„ä¸»è¦å’Œæ¬¡è¦è¡Œä¸ºæ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†äº”ç§æµè¡Œå¼•å¯¼æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œè¡Œä¸ºçº ç¼ ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå¼•å¯¼æ€§èƒ½å¼ºå¼±ä¾èµ–äºå¼•å¯¼æ–¹æ³•ã€æ¨¡å‹å’Œç›®æ ‡è¡Œä¸ºçš„ç‰¹å®šç»„åˆï¼Œä¸”ä¸è‰¯ç»„åˆå¯èƒ½å¯¼è‡´ä¸¥é‡çš„æ¦‚å¿µçº ç¼ ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14026",
            "title": "Quantum Variational Activation Functions Empower Kolmogorov-Arnold\n  Networks",
            "url": "https://huggingface.co/papers/2509.14026",
            "abstract": "Quantum variational activation functions (QVAFs) and quantum-inspired Kolmogorov-Arnold networks (QKANs) enhance parameter efficiency and expressivity in quantum machine learning, offering scalability and improved performance in various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Variational quantum circuits (VQCs) are central to quantum machine learning, while recent progress in Kolmogorov-Arnold networks (KANs) highlights the power of learnable activation functions. We unify these directions by introducing quantum variational activation functions (QVAFs), realized through single-qubit data re-uploading circuits called DatA Re-Uploading ActivatioNs (DARUANs). We show that DARUAN with trainable weights in data pre-processing possesses an exponentially growing frequency spectrum with data repetitions, enabling an exponential reduction in parameter size compared with Fourier-based activations without loss of expressivity. Embedding DARUAN into KANs yields quantum-inspired KANs (QKANs), which retain the interpretability of KANs while improving their parameter efficiency, expressivity, and generalization. We further introduce two novel techniques to enhance scalability, feasibility and computational efficiency, such as layer extension and hybrid QKANs (HQKANs) as drop-in replacements of multi-layer perceptrons (MLPs) for feed-forward networks in large-scale models. We provide theoretical analysis and extensive experiments on function regression, image classification, and autoregressive generative language modeling, demonstrating the efficiency and scalability of QKANs. DARUANs and QKANs offer a promising direction for advancing quantum machine learning on both noisy intermediate-scale quantum (NISQ) hardware and classical quantum simulators.",
            "score": 2,
            "issue_id": 5964,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "7a7e15daaa1b60ea",
            "authors": [
                "Jiun-Cheng Jiang",
                "Morris Yu-Chao Huang",
                "Tianlong Chen",
                "Hsi-Sheng Goan"
            ],
            "affiliations": [
                "Center for Quantum Science and Engineering, National Taiwan University",
                "Department of Physics and Center for Theoretical Physics, National Taiwan University, Taipei 106319, Taiwan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.14026.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#training",
                    "#data"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ² ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ - ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ (QVAF) Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾-Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ ĞšĞ¾Ğ»Ğ¼Ğ¾Ğ³Ğ¾Ñ€Ğ¾Ğ²Ğ°-ĞÑ€Ğ½Ğ¾Ğ»ÑŒĞ´Ğ° (QKAN). Ğ­Ñ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. QVAF Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ´Ğ½Ğ¾ĞºÑƒĞ±Ğ¸Ñ‚Ğ½Ñ‹Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ DARUAN. QKAN, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ DARUAN, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞµÑ‚ĞµĞ¹ ĞšĞ¾Ğ»Ğ¼Ğ¾Ğ³Ğ¾Ñ€Ğ¾Ğ²Ğ°-ĞÑ€Ğ½Ğ¾Ğ»ÑŒĞ´Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Quantum Machine Learning with QVAFs and QKANs",
                    "desc": "This paper introduces Quantum Variational Activation Functions (QVAFs) and Quantum-Inspired Kolmogorov-Arnold Networks (QKANs) to improve efficiency and performance in quantum machine learning. The authors present a novel approach using single-qubit data re-uploading circuits, called DatA Re-Uploading ActivatioNs (DARUANs), which allow for a significant reduction in parameter size while maintaining expressivity. By integrating DARUAN into KANs, they create QKANs that enhance interpretability and scalability, making them suitable for large-scale models. The paper includes theoretical analysis and experiments demonstrating the effectiveness of QKANs in various tasks like function regression and image classification."
                },
                "zh": {
                    "title": "é‡å­æœºå™¨å­¦ä¹ çš„æ–°æ–¹å‘ï¼šé«˜æ•ˆä¸å¯æ‰©å±•æ€§",
                    "desc": "é‡å­å˜åˆ†æ¿€æ´»å‡½æ•°ï¼ˆQVAFsï¼‰å’Œé‡å­å¯å‘çš„Kolmogorov-Arnoldç½‘ç»œï¼ˆQKANsï¼‰åœ¨é‡å­æœºå™¨å­¦ä¹ ä¸­æé«˜äº†å‚æ•°æ•ˆç‡å’Œè¡¨è¾¾èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å•é‡å­æ¯”ç‰¹æ•°æ®é‡ä¸Šä¼ ç”µè·¯ï¼ˆDARUANsï¼‰å®ç°QVAFsï¼Œå±•ç¤ºäº†åœ¨æ•°æ®é¢„å¤„ç†ä¸­çš„å¯è®­ç»ƒæƒé‡å¯ä»¥æ˜¾è‘—å‡å°‘å‚æ•°è§„æ¨¡ã€‚å°†DARUANåµŒå…¥KANsä¸­å½¢æˆQKANsï¼Œä¿ç•™äº†KANsçš„å¯è§£é‡Šæ€§ï¼ŒåŒæ—¶æå‡äº†å‚æ•°æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸¤ç§æ–°æŠ€æœ¯ä»¥å¢å¼ºå¯æ‰©å±•æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œå±•ç¤ºäº†QKANsåœ¨å¤šç§ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.14180",
            "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation\n  Framework for Personal Finance LLMs",
            "url": "https://huggingface.co/papers/2509.14180",
            "abstract": "A novel framework integrates financial context and behavioral finance to fine-tune a Qwen-3-8B model for personalized financial advice, achieving performance comparable to larger models with lower costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Personalized financial advice requires consideration of user goals, constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on support systems for investors and financial planners. Simultaneously, numerous recent studies examine broader personal finance tasks, including budgeting, debt management, retirement, and estate planning, through agentic pipelines that incur high maintenance costs, yielding less than 25% of their expected financial returns. In this study, we introduce a novel and reproducible framework that integrates relevant financial context with behavioral finance studies to construct supervision data for end-to-end advisors. Using this framework, we create a 19k sample reasoning dataset and conduct a comprehensive fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test split and a blind LLM-jury study, we demonstrate that through careful data curation and behavioral integration, our 8B model achieves performance comparable to significantly larger baselines (14-32B parameters) across factual accuracy, fluency, and personalization metrics while incurring 80% lower costs than the larger counterparts.",
            "score": 1,
            "issue_id": 5966,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "67b0e10ac8d79758",
            "authors": [
                "Akhil Theerthala"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2509.14180.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#dataset",
                    "#alignment",
                    "#data",
                    "#optimization",
                    "#training",
                    "#science",
                    "#reasoning"
                ],
                "emoji": "ğŸ’°",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğµ ÑĞ¾Ğ²ĞµÑ‚Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen-3-8B. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 19 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ 8-Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Personalized Financial Advice Made Affordable with Qwen-3-8B",
                    "desc": "This paper presents a new framework that combines financial context with behavioral finance to enhance the Qwen-3-8B model for delivering personalized financial advice. The approach focuses on understanding user-specific factors such as goals, risk tolerance, and constraints, which are crucial for effective financial guidance. By creating a dataset of 19,000 samples and fine-tuning the model, the authors show that their smaller model can perform as well as much larger models while being significantly more cost-effective. The results indicate that careful data curation and the integration of behavioral insights can lead to high-quality financial advice without the high costs associated with larger models."
                },
                "zh": {
                    "title": "ä¸ªæ€§åŒ–è´¢åŠ¡å»ºè®®çš„æ–°æ¡†æ¶",
                    "desc": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå°†é‡‘èèƒŒæ™¯ä¸è¡Œä¸ºé‡‘èå­¦ç›¸ç»“åˆï¼Œä»¥ä¼˜åŒ–Qwen-3-8Bæ¨¡å‹ï¼Œä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–çš„è´¢åŠ¡å»ºè®®ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºç›‘ç£æ•°æ®ï¼Œåˆ›å»ºäº†ä¸€ä¸ªåŒ…å«19,000ä¸ªæ ·æœ¬çš„æ¨ç†æ•°æ®é›†ï¼Œå¹¶å¯¹æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„å¾®è°ƒã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡ç²¾å¿ƒçš„æ•°æ®ç­–åˆ’å’Œè¡Œä¸ºæ•´åˆï¼Œæˆ‘ä»¬çš„8Bæ¨¡å‹åœ¨äº‹å®å‡†ç¡®æ€§ã€æµç•…æ€§å’Œä¸ªæ€§åŒ–æŒ‡æ ‡ä¸Šï¼Œè¡¨ç°å‡ºä¸æ›´å¤§æ¨¡å‹ï¼ˆ14-32Bå‚æ•°ï¼‰ç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶æˆæœ¬é™ä½äº†80%ã€‚è¿™ä¸€æˆæœä¸ºä¸ªæ€§åŒ–è´¢åŠ¡é¡¾é—®ç³»ç»Ÿçš„å¼€å‘æä¾›äº†æ–°çš„æ€è·¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13642",
            "title": "LLM-I: LLMs are Naturally Interleaved Multimodal Creators",
            "url": "https://huggingface.co/papers/2509.13642",
            "abstract": "LLM-Interleaved (LLM-I) is a flexible framework that uses a central LLM to orchestrate a toolkit of specialized visual tools, achieving state-of-the-art performance in image-text generation through reinforcement learning and a novel scaling strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that reframes interleaved image-text generation as a tool-use problem. LLM-I is designed to overcome the \"one-tool\" bottleneck of current unified models, which are limited to synthetic imagery and struggle with tasks requiring factual grounding or programmatic precision. Our framework empowers a central LLM or MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual tools, including online image search, diffusion-based generation, code execution, and image editing. The agent is trained to select and apply these tools proficiently via a Reinforcement Learning (RL) framework that features a hybrid reward system combining rule-based logic with judgments from LLM and MLLM evaluators. Trained on a diverse new dataset using four different model backbones, LLM-I demonstrates state-of-the-art performance, outperforming existing methods by a large margin across four benchmarks. We also introduce a novel test-time scaling strategy that provides further performance gains. Project Page: https://github.com/ByteDance-BandAI/LLM-I.",
            "score": 1,
            "issue_id": 5965,
            "pub_date": "2025-09-17",
            "pub_date_card": {
                "ru": "17 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 17",
                "zh": "9æœˆ17æ—¥"
            },
            "hash": "2f0d1f84410e6835",
            "authors": [
                "Zirun Guo",
                "Feng Zhang",
                "Kai Jia",
                "Tao Jin"
            ],
            "affiliations": [
                "BandAI",
                "ByteDance",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13642.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#games",
                    "#multimodal",
                    "#agents",
                    "#optimization",
                    "#benchmark",
                    "#diffusion",
                    "#dataset",
                    "#rag"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LLM-I: ĞÑ€ĞºĞµÑÑ‚Ñ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ˜Ğ˜",
                    "desc": "LLM-Interleaved (LLM-I) - ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±ĞºĞ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ¾Ñ‚ Ğ¯Ğœ Ğ¸ ĞœĞ¯ĞœĞœ. LLM-I Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Empowering Image-Text Generation with LLM-I: A Tool-Use Revolution",
                    "desc": "LLM-Interleaved (LLM-I) is a new framework that enhances image-text generation by using a central large language model (LLM) to manage various specialized visual tools. This approach addresses the limitations of existing models that can only use one tool at a time, which often leads to poor performance in tasks needing accurate information or detailed execution. By employing reinforcement learning, LLM-I trains the LLM to effectively choose and utilize these tools, such as image search and editing, based on a unique reward system. The framework has shown significant improvements in performance across multiple benchmarks, thanks to its innovative scaling strategy and diverse training dataset."
                },
                "zh": {
                    "title": "çµæ´»çš„å›¾åƒ-æ–‡æœ¬ç”Ÿæˆæ¡†æ¶",
                    "desc": "LLM-Interleaved (LLM-I) æ˜¯ä¸€ä¸ªçµæ´»çš„æ¡†æ¶ï¼Œåˆ©ç”¨ä¸­å¤®å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥åè°ƒä¸€å¥—ä¸“é—¨çš„è§†è§‰å·¥å…·ï¼Œä»è€Œåœ¨å›¾åƒ-æ–‡æœ¬ç”Ÿæˆæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œæ–°é¢–çš„æ‰©å±•ç­–ç•¥ï¼Œè§£å†³äº†å½“å‰ç»Ÿä¸€æ¨¡å‹åœ¨åˆæˆå›¾åƒæ–¹é¢çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦äº‹å®åŸºç¡€æˆ–ç¨‹åºç²¾ç¡®åº¦çš„ä»»åŠ¡ä¸­ã€‚LLM-I ä½¿å¾—ä¸­å¤® LLM æˆ–å¤šè¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä»£ç†èƒ½å¤Ÿæ™ºèƒ½åœ°é€‰æ‹©å’Œåº”ç”¨å¤šç§è§†è§‰å·¥å…·ï¼ŒåŒ…æ‹¬åœ¨çº¿å›¾åƒæœç´¢ã€åŸºäºæ‰©æ•£çš„ç”Ÿæˆã€ä»£ç æ‰§è¡Œå’Œå›¾åƒç¼–è¾‘ã€‚ç»è¿‡åœ¨å¤šæ ·åŒ–çš„æ–°æ•°æ®é›†ä¸Šè®­ç»ƒï¼ŒLLM-I åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.12474",
            "title": "Image Tokenizer Needs Post-Training",
            "url": "https://huggingface.co/papers/2509.12474",
            "abstract": "A novel tokenizer training scheme, including main and post-training phases, improves latent space construction and decoding, enhancing image generation quality and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent image generative models typically capture the image distribution in a pre-constructed latent space, relying on a frozen image tokenizer. However, there exists a significant discrepancy between the reconstruction and generation distribution, where current tokenizers only prioritize the reconstruction task that happens before generative training without considering the generation errors during sampling. In this paper, we comprehensively analyze the reason for this discrepancy in a discrete latent space, and, from which, we propose a novel tokenizer training scheme including both main-training and post-training, focusing on improving latent space construction and decoding respectively. During the main training, a latent perturbation strategy is proposed to simulate sampling noises, \\ie, the unexpected tokens generated in generative inference. Specifically, we propose a plug-and-play tokenizer training scheme, which significantly enhances the robustness of tokenizer, thus boosting the generation quality and convergence speed, and a novel tokenizer evaluation metric, \\ie, pFID, which successfully correlates the tokenizer performance to generation quality. During post-training, we further optimize the tokenizer decoder regarding a well-trained generative model to mitigate the distribution difference between generated and reconstructed tokens. With a sim400M generator, a discrete tokenizer trained with our proposed main training achieves a notable 1.60 gFID and further obtains 1.36 gFID with the additional post-training. Further experiments are conducted to broadly validate the effectiveness of our post-training strategy on off-the-shelf discrete and continuous tokenizers, coupled with autoregressive and diffusion-based generators.",
            "score": 0,
            "issue_id": 5969,
            "pub_date": "2025-09-15",
            "pub_date_card": {
                "ru": "15 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 15",
                "zh": "9æœˆ15æ—¥"
            },
            "hash": "838407f1a104d2ca",
            "authors": [
                "Kai Qiu",
                "Xiang Li",
                "Hao Chen",
                "Jason Kuen",
                "Xiaohao Xu",
                "Jiuxiang Gu",
                "Yinyi Luo",
                "Bhiksha Raj",
                "Zhe Lin",
                "Marios Savvides"
            ],
            "affiliations": [
                "Adobe Research",
                "Carnegie Mellon University",
                "University of Michigan"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.12474.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#diffusion",
                    "#training",
                    "#optimization",
                    "#data",
                    "#architecture"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ´Ğ²ÑƒÑ…Ñ„Ğ°Ğ·Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°. Ğ­Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° pFID, ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Enhancing Image Generation with Advanced Tokenizer Training",
                    "desc": "This paper introduces a new training method for tokenizers that enhances the quality of image generation in machine learning models. It identifies a gap between how images are reconstructed and how they are generated, which is often overlooked by existing tokenizers. The proposed method includes a main training phase that simulates sampling errors and a post-training phase that refines the tokenizer's decoding process. By implementing this dual-phase approach, the authors demonstrate improved robustness and faster convergence in image generation tasks, validated through new evaluation metrics and experiments with various models."
                },
                "zh": {
                    "title": "æå‡å›¾åƒç”Ÿæˆè´¨é‡çš„æ–°åˆ†è¯å™¨è®­ç»ƒæ–¹æ¡ˆ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åˆ†è¯å™¨è®­ç»ƒæ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä¸»è¦è®­ç»ƒå’ŒåæœŸè®­ç»ƒé˜¶æ®µï¼Œæ—¨åœ¨æ”¹å–„æ½œåœ¨ç©ºé—´çš„æ„å»ºå’Œè§£ç ï¼Œä»è€Œæé«˜å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œé²æ£’æ€§ã€‚å½“å‰çš„å›¾åƒç”Ÿæˆæ¨¡å‹é€šå¸¸ä¾èµ–äºå›ºå®šçš„å›¾åƒåˆ†è¯å™¨ï¼Œä½†åœ¨é‡å»ºå’Œç”Ÿæˆåˆ†å¸ƒä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬åˆ†æäº†è¿™ç§å·®å¼‚çš„åŸå› ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„åˆ†è¯å™¨è®­ç»ƒæ–¹æ¡ˆï¼Œé‡ç‚¹æ”¹å–„æ½œåœ¨ç©ºé—´çš„æ„å»ºå’Œè§£ç ã€‚é€šè¿‡å¼•å…¥æ½œåœ¨æ‰°åŠ¨ç­–ç•¥å’Œæ–°çš„è¯„ä¼°æŒ‡æ ‡pFIDï¼Œæˆ‘ä»¬æ˜¾è‘—æå‡äº†åˆ†è¯å™¨çš„é²æ£’æ€§å’Œç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2509.13353",
            "title": "Hybrid Quantum-Classical Model for Image Classification",
            "url": "https://huggingface.co/papers/2509.13353",
            "abstract": "Hybrid quantum-classical neural networks outperform classical models in accuracy, training efficiency, and parameter scalability across various datasets, especially for complex vision tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This study presents a systematic comparison between hybrid quantum-classical neural networks and purely classical models across three benchmark datasets (MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and robustness. The hybrid models integrate parameterized quantum circuits with classical deep learning architectures, while the classical counterparts use conventional convolutional neural networks (CNNs). Experiments were conducted over 50 training epochs for each dataset, with evaluations on validation accuracy, test accuracy, training time, computational resource usage, and adversarial robustness (tested with epsilon=0.1 perturbations).Key findings demonstrate that hybrid models consistently outperform classical models in final accuracy, achieving {99.38\\% (MNIST), 41.69\\% (CIFAR100), and 74.05\\% (STL10) validation accuracy, compared to classical benchmarks of 98.21\\%, 32.25\\%, and 63.76\\%, respectively. Notably, the hybrid advantage scales with dataset complexity, showing the most significant gains on CIFAR100 (+9.44\\%) and STL10 (+10.29\\%). Hybrid models also train 5--12times faster (e.g., 21.23s vs. 108.44s per epoch on MNIST) and use 6--32\\% fewer parameters} while maintaining superior generalization to unseen test data.Adversarial robustness tests reveal that hybrid models are significantly more resilient on simpler datasets (e.g., 45.27\\% robust accuracy on MNIST vs. 10.80\\% for classical) but show comparable fragility on complex datasets like CIFAR100 (sim1\\% robustness for both). Resource efficiency analyses indicate that hybrid models consume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization (9.5\\% vs. 23.2\\% on average).These results suggest that hybrid quantum-classical architectures offer compelling advantages in accuracy, training efficiency, and parameter scalability, particularly for complex vision tasks.",
            "score": 0,
            "issue_id": 5964,
            "pub_date": "2025-09-14",
            "pub_date_card": {
                "ru": "14 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 14",
                "zh": "9æœˆ14æ—¥"
            },
            "hash": "4ec1741e6a502b77",
            "authors": [
                "Muhammad Adnan Shahzad"
            ],
            "affiliations": [
                "Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada"
            ],
            "pdf_title_img": "assets/pdf/title_img/2509.13353.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#architecture",
                    "#security",
                    "#benchmark"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾-ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾-ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ñ Ñ‡Ğ¸ÑÑ‚Ğ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Hybrid Quantum-Classical Models: A Leap in Accuracy and Efficiency!",
                    "desc": "This paper investigates the performance of hybrid quantum-classical neural networks compared to traditional classical models. The study shows that hybrid models, which combine quantum circuits with classical deep learning techniques, achieve higher accuracy and faster training times across various datasets. Specifically, they outperform classical convolutional neural networks in tasks like image classification, especially as dataset complexity increases. Additionally, hybrid models demonstrate better resource efficiency and robustness against adversarial attacks on simpler datasets."
                },
                "zh": {
                    "title": "æ··åˆé‡å­-ç»å…¸ç¥ç»ç½‘ç»œçš„ä¼˜åŠ¿",
                    "desc": "è¿™é¡¹ç ”ç©¶æ¯”è¾ƒäº†æ··åˆé‡å­-ç»å…¸ç¥ç»ç½‘ç»œä¸çº¯ç»å…¸æ¨¡å‹åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆMNISTã€CIFAR100å’ŒSTL10ï¼‰ä¸Šçš„è¡¨ç°ã€‚æ··åˆæ¨¡å‹ç»“åˆäº†å‚æ•°åŒ–çš„é‡å­ç”µè·¯å’Œç»å…¸æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œè€Œç»å…¸æ¨¡å‹åˆ™ä½¿ç”¨ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ··åˆæ¨¡å‹åœ¨æœ€ç»ˆå‡†ç¡®ç‡ã€è®­ç»ƒæ•ˆç‡å’Œå‚æ•°å¯æ‰©å±•æ€§æ–¹é¢å‡ä¼˜äºç»å…¸æ¨¡å‹ï¼Œå°¤å…¶åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°æ›´ä¸ºçªå‡ºã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ··åˆæ¨¡å‹åœ¨è®­ç»ƒé€Ÿåº¦ä¸Šå¿«5åˆ°12å€ï¼Œå¹¶ä¸”åœ¨å†…å­˜å’ŒCPUåˆ©ç”¨ç‡ä¸Šä¹Ÿæ›´ä¸ºé«˜æ•ˆã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-09-17.html",
    "link_next": "2025-09-19.html",
    "link_month": "2025-09.html",
    "short_date_prev": {
        "ru": "17.09",
        "en": "09/17",
        "zh": "9æœˆ17æ—¥"
    },
    "short_date_next": {
        "ru": "19.09",
        "en": "09/19",
        "zh": "9æœˆ19æ—¥"
    },
    "categories": {
        "#dataset": 11,
        "#data": 7,
        "#benchmark": 8,
        "#agents": 3,
        "#cv": 3,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 2,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 4,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 8,
        "#healthcare": 1,
        "#training": 10,
        "#robotics": 1,
        "#agi": 2,
        "#games": 3,
        "#interpretability": 0,
        "#reasoning": 6,
        "#transfer_learning": 0,
        "#graphs": 1,
        "#ethics": 1,
        "#security": 2,
        "#optimization": 8,
        "#survey": 1,
        "#diffusion": 3,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 0,
        "#machine_translation": 1,
        "#leakage": 1,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 3,
        "#low_resource": 1
    }
}