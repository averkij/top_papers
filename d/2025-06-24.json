{
    "date": {
        "ru": "24 июня",
        "en": "June 24",
        "zh": "6月24日"
    },
    "time_utc": "2025-06-24 02:44",
    "weekday": 1,
    "issue_id": 4447,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.18871",
            "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
            "url": "https://huggingface.co/papers/2506.18871",
            "abstract": "OmniGen2, a versatile generative model, introduces dual decoding pathways for text and images, preserves original text generation, and achieves competitive results with a new subject-driven benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen2",
            "score": 9,
            "issue_id": 4447,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 июня",
                "en": "June 23",
                "zh": "6月23日"
            },
            "hash": "18382718ba53ccf7",
            "authors": [
                "Chenyuan Wu",
                "Pengfei Zheng",
                "Ruiran Yan",
                "Shitao Xiao",
                "Xin Luo",
                "Yueze Wang",
                "Wanli Li",
                "Xiyan Jiang",
                "Yexin Liu",
                "Junjie Zhou",
                "Ze Liu",
                "Ziyi Xia",
                "Chaofan Li",
                "Haoge Deng",
                "Jiahao Wang",
                "Kun Luo",
                "Bo Zhang",
                "Defu Lian",
                "Xinlong Wang",
                "Zhongyuan Wang",
                "Tiejun Huang",
                "Zheng Liu"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18871.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#data",
                    "#dataset",
                    "#multimodal",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "OmniGen2: Универсальная модель для многозадачной генерации текста и изображений",
                    "desc": "OmniGen2 - это универсальная генеративная модель с открытым исходным кодом, способная решать различные задачи генерации, включая преобразование текста в изображение, редактирование изображений и генерацию в контексте. Модель имеет два отдельных пути декодирования для текста и изображений, что позволяет сохранить исходные возможности генерации текста. Несмотря на относительно небольшой размер параметров, OmniGen2 достигает конкурентоспособных результатов на нескольких эталонных тестах. Авторы также представили новый бенчмарк OmniContext для оценки генерации в контексте."
                },
                "en": {
                    "title": "OmniGen2: Unifying Text and Image Generation with Dual Pathways",
                    "desc": "OmniGen2 is a generative model that enhances the creation of text and images through dual decoding pathways, allowing for specialized processing of each modality. It maintains the original text generation capabilities while introducing a new image tokenizer and reflection mechanism for improved image tasks. The model is trained using comprehensive data pipelines that support various generation tasks, including image editing and in-context generation. Despite its smaller size, OmniGen2 achieves competitive performance on benchmarks, particularly in subject-driven tasks, and aims to advance research in multimodal generation."
                },
                "zh": {
                    "title": "OmniGen2：多模态生成的统一解决方案",
                    "desc": "OmniGen2是一种多功能的生成模型，旨在为文本和图像生成任务提供统一的解决方案。与OmniGen v1不同，OmniGen2采用了两个独立的解码路径，分别处理文本和图像，使用了不共享的参数和解耦的图像标记器。这种设计使得OmniGen2能够在不重新适配VAE输入的情况下，保留原有的文本生成能力。尽管参数量相对较小，OmniGen2在多个任务基准上取得了竞争力的结果，特别是在文本到图像生成和图像编辑方面。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18841",
            "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2506.18841",
            "abstract": "An incentivization-based reinforcement learning approach is used to develop a large language model capable of generating ultra-long, high-quality text without the need for synthetic data or supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under https://huggingface.co/THU-KEG/LongWriter-Zero-32B",
            "score": 6,
            "issue_id": 4447,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 июня",
                "en": "June 23",
                "zh": "6月23日"
            },
            "hash": "8589c05aae4b8258",
            "authors": [
                "Yuhao Wu",
                "Yushi Bai",
                "Zhiqiang Hu",
                "Roy Ka-Wei Lee",
                "Juanzi Li"
            ],
            "affiliations": [
                "Singapore University of Technology and Design, Singapore",
                "Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18841.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#rl",
                    "#open_source",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "📝",
                "ru": {
                    "title": "Революция в генерации длинных текстов: RL вместо синтетических данных",
                    "desc": "В этой статье представлен новый подход к обучению больших языковых моделей (LLM) для генерации сверхдлинных и качественных текстов без использования синтетических данных или контролируемой дообучения. Метод основан на обучении с подкреплением (RL) и стимулировании модели к планированию и улучшению процесса написания. Авторы разработали специальные модели вознаграждения для улучшения контроля длины, качества письма и структурного форматирования. Эксперименты показали, что их модель LongWriter-Zero превосходит традиционные методы на задачах написания длинных текстов, достигая лучших результатов на бенчмарках WritingBench и Arena-Write."
                },
                "en": {
                    "title": "Reinforcement Learning for Ultra-Long Text Generation Without Synthetic Data",
                    "desc": "This paper presents a novel approach to generating ultra-long, high-quality text using a large language model (LLM) without relying on synthetic data or supervised fine-tuning. The authors introduce an incentivization-based reinforcement learning (RL) method that allows the model to learn from scratch, enhancing its ability to produce coherent and structured long-form content. By employing specialized reward models, the LLM is guided to improve its writing quality, length control, and formatting during the generation process. Experimental results demonstrate that the proposed LongWriter-Zero model outperforms traditional methods, achieving state-of-the-art performance on long-form writing benchmarks."
                },
                "zh": {
                    "title": "激励强化学习，超长文本生成新突破",
                    "desc": "本论文提出了一种基于激励的强化学习方法，旨在开发一种大型语言模型，能够生成超长且高质量的文本，而无需合成数据或监督微调。以往的方法通常依赖于监督微调（SFT），这需要构建合成的长文本输出，成本高且难以实现。我们的方法从零开始，通过强化学习（RL）训练模型，促进超长高质量文本生成能力的出现。实验结果表明，我们的LongWriter-Zero模型在长文本写作任务中表现优于传统的SFT方法，达到了最新的技术水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18896",
            "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs",
            "url": "https://huggingface.co/papers/2506.18896",
            "abstract": "ReasonFlux-PRM, a novel trajectory-aware Process Reward Model, evaluates reasoning traces with step-level and trajectory-level supervision, enhancing performance in model distillation, reinforcement learning, and test-time scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: https://github.com/Gen-Verse/ReasonFlux",
            "score": 5,
            "issue_id": 4447,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 июня",
                "en": "June 23",
                "zh": "6月23日"
            },
            "hash": "6a30d79f40f7d98d",
            "authors": [
                "Jiaru Zou",
                "Ling Yang",
                "Jingwen Gu",
                "Jiahao Qiu",
                "Ke Shen",
                "Jingrui He",
                "Mengdi Wang"
            ],
            "affiliations": [
                "ByteDance",
                "Cornell University",
                "Princeton University",
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18896.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#small_models",
                    "#optimization",
                    "#rl",
                    "#dataset",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение рассуждений ИИ через оценку траекторий мышления",
                    "desc": "ReasonFlux-PRM - это новая модель оценки процесса рассуждений, которая улучшает работу больших языковых моделей (LLM). Она оценивает промежуточные шаги рассуждений и целые траектории мышления, что позволяет более точно назначать награды в процессе обучения. ReasonFlux-PRM применяется для отбора качественных данных при дистилляции моделей, оптимизации политики в обучении с подкреплением и масштабировании во время тестирования. Эксперименты показали, что ReasonFlux-PRM превосходит существующие модели оценки процесса рассуждений на сложных задачах."
                },
                "en": {
                    "title": "Enhancing Reasoning Evaluation with ReasonFlux-PRM",
                    "desc": "ReasonFlux-PRM is a new model that improves how we evaluate reasoning processes in large language models by focusing on both individual steps and overall trajectories. It addresses the limitations of previous Process Reward Models that mainly assessed final outputs, which often missed the nuances of intermediate reasoning. By using both step-level and trajectory-level supervision, it provides more accurate rewards that align with structured reasoning data. The model has shown significant performance improvements in tasks like model distillation, reinforcement learning, and test-time scaling, outperforming existing models and human-curated benchmarks."
                },
                "zh": {
                    "title": "推理轨迹的智能评估",
                    "desc": "ReasonFlux-PRM是一种新颖的过程奖励模型，专注于评估推理轨迹，结合了逐步和轨迹级的监督。这种模型能够在模型蒸馏、强化学习和测试时扩展中提升性能。通过对推理过程的细致奖励分配，ReasonFlux-PRM能够更好地处理复杂的推理任务。实验证明，该模型在多个基准测试中表现优于传统的过程奖励模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18851",
            "title": "Phantom-Data : Towards a General Subject-Consistent Video Generation\n  Dataset",
            "url": "https://huggingface.co/papers/2506.18851",
            "abstract": "A cross-pair dataset called Phantom-Data improves subject-to-video generation by enhancing prompt alignment and visual quality while maintaining identity consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Subject-to-video generation has witnessed substantial progress in recent years. However, existing models still face significant challenges in faithfully following textual instructions. This limitation, commonly known as the copy-paste problem, arises from the widely used in-pair training paradigm. This approach inherently entangles subject identity with background and contextual attributes by sampling reference images from the same scene as the target video. To address this issue, we introduce Phantom-Data, the first general-purpose cross-pair subject-to-video consistency dataset, containing approximately one million identity-consistent pairs across diverse categories. Our dataset is constructed via a three-stage pipeline: (1) a general and input-aligned subject detection module, (2) large-scale cross-context subject retrieval from more than 53 million videos and 3 billion images, and (3) prior-guided identity verification to ensure visual consistency under contextual variation. Comprehensive experiments show that training with Phantom-Data significantly improves prompt alignment and visual quality while preserving identity consistency on par with in-pair baselines.",
            "score": 1,
            "issue_id": 4447,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 июня",
                "en": "June 23",
                "zh": "6月23日"
            },
            "hash": "525a4c676b83f9a6",
            "authors": [
                "Zhuowei Chen",
                "Bingchuan Li",
                "Tianxiang Ma",
                "Lijie Liu",
                "Mingcong Liu",
                "Yi Zhang",
                "Gen Li",
                "Xinghui Li",
                "Siyu Zhou",
                "Qian He",
                "Xinglong Wu"
            ],
            "affiliations": [
                "Intelligent Creation Lab, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18851.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Phantom-Data: преодоление ограничений генерации видео по субъекту",
                    "desc": "Представлен новый набор данных Phantom-Data для улучшения генерации видео по субъекту. Он содержит около миллиона пар изображений с согласованной идентичностью субъекта в разных контекстах. Набор данных создан с помощью трехэтапного процесса, включающего обнаружение субъекта, поиск по большой базе видео и изображений, и проверку идентичности. Эксперименты показывают, что обучение на Phantom-Data значительно улучшает соответствие промпту и визуальное качество при сохранении согласованности идентичности."
                },
                "en": {
                    "title": "Phantom-Data: Enhancing Video Generation with Identity Consistency",
                    "desc": "The paper introduces Phantom-Data, a new dataset designed to enhance subject-to-video generation in machine learning. This dataset addresses the 'copy-paste problem' by providing identity-consistent pairs that are not tied to specific backgrounds or contexts. It is created through a three-stage process that includes subject detection, cross-context retrieval, and identity verification. Experiments demonstrate that using Phantom-Data leads to better alignment with prompts and improved visual quality while maintaining consistent subject identity."
                },
                "zh": {
                    "title": "Phantom-Data：提升视频生成的身份一致性与视觉质量",
                    "desc": "本文介绍了一种名为Phantom-Data的跨对数据集，旨在改善基于文本生成视频的效果。该数据集通过增强提示对齐和视觉质量，同时保持身份一致性，解决了现有模型在遵循文本指令时面临的挑战。Phantom-Data包含约一百万个身份一致的配对，涵盖多种类别，采用三阶段流程构建。实验结果表明，使用Phantom-Data进行训练显著提高了生成视频的质量和一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18898",
            "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations",
            "url": "https://huggingface.co/papers/2506.18898",
            "abstract": "A multimodal framework uses a Text-Aligned Tokenizer (TA-Tok) to integrate vision and text into a unified space, employing a generative de-tokenizer with autoregressive and diffusion-based models for efficient and high-fidelity visual outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at https://tar.csuhan.com",
            "score": 0,
            "issue_id": 4447,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 июня",
                "en": "June 23",
                "zh": "6月23日"
            },
            "hash": "988f76bd08498ba9",
            "authors": [
                "Jiaming Han",
                "Hao Chen",
                "Yang Zhao",
                "Hanyu Wang",
                "Qi Zhao",
                "Ziyan Yang",
                "Hao He",
                "Xiangyu Yue",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "CUHK MMLab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18898.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#multimodal",
                    "#games",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Единое пространство для текста и изображений в мультимодальных языковых моделях",
                    "desc": "Эта статья представляет мультимодальную систему, объединяющую понимание и генерацию визуальной информации в едином дискретном семантическом представлении. В основе системы лежит Text-Aligned Tokenizer (TA-Tok), который преобразует изображения в дискретные токены, используя кодовую книгу, спроектированную из словаря большой языковой модели. Система использует масштабируемое кодирование и декодирование для баланса эффективности и визуальной детализации, а также генеративный детокенизатор для создания высококачественных визуальных выходов. Эксперименты показывают, что предложенная модель Tar соответствует или превосходит существующие мультимодальные методы на основе больших языковых моделей, достигая более быстрой сходимости и большей эффективности обучения."
                },
                "en": {
                    "title": "Unifying Vision and Text with TA-Tok for Enhanced Multimodal Learning",
                    "desc": "This paper introduces a multimodal framework that combines visual and textual data into a single representation using a Text-Aligned Tokenizer (TA-Tok). The TA-Tok transforms images into discrete tokens aligned with a large language model's vocabulary, allowing for seamless interaction between text and images. The framework employs a generative de-tokenizer that includes both autoregressive and diffusion-based models to generate high-quality visual outputs efficiently. Experimental results indicate that this approach not only enhances visual understanding and generation but also outperforms existing multimodal models in terms of training speed and efficiency."
                },
                "zh": {
                    "title": "统一视觉与文本的多模态框架",
                    "desc": "这篇论文提出了一种多模态框架，旨在将视觉理解和生成统一到一个共享的离散语义表示中。核心是文本对齐的标记器（TA-Tok），它使用大型语言模型的词汇将图像转换为离散标记。通过扩展词汇，我们的多模态大语言模型Tar实现了跨模态输入和输出，避免了特定模态设计的需求。此外，我们还提出了规模自适应的编码和解码方法，以平衡效率和视觉细节，并使用生成性去标记器生成高保真视觉输出。"
                }
            }
        }
    ],
    "link_prev": "2025-06-23.html",
    "link_next": "2025-06-25.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "23.06",
        "en": "06/23",
        "zh": "6月23日"
    },
    "short_date_next": {
        "ru": "25.06",
        "en": "06/25",
        "zh": "6月25日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 2,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 1,
        "#rl": 2,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 0,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    }
}