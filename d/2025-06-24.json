{
    "date": {
        "ru": "24 июня",
        "en": "June 24",
        "zh": "6月24日"
    },
    "time_utc": "2025-06-24 03:44",
    "weekday": 1,
    "issue_id": 4448,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2506.18871",
            "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
            "url": "https://huggingface.co/papers/2506.18871",
            "abstract": "OmniGen2, a versatile generative model, introduces dual decoding pathways for text and images, preserves original text generation, and achieves competitive results with a new subject-driven benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link: https://github.com/VectorSpaceLab/OmniGen2",
            "score": 21,
            "issue_id": 4447,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 июня",
                "en": "June 23",
                "zh": "6月23日"
            },
            "hash": "18382718ba53ccf7",
            "authors": [
                "Chenyuan Wu",
                "Pengfei Zheng",
                "Ruiran Yan",
                "Shitao Xiao",
                "Xin Luo",
                "Yueze Wang",
                "Wanli Li",
                "Xiyan Jiang",
                "Yexin Liu",
                "Junjie Zhou",
                "Ze Liu",
                "Ziyi Xia",
                "Chaofan Li",
                "Haoge Deng",
                "Jiahao Wang",
                "Kun Luo",
                "Bo Zhang",
                "Defu Lian",
                "Xinlong Wang",
                "Zhongyuan Wang",
                "Tiejun Huang",
                "Zheng Liu"
            ],
            "affiliations": [
                "Beijing Academy of Artificial Intelligence"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18871.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#data",
                    "#dataset",
                    "#multimodal",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "🎨",
                "ru": {
                    "title": "OmniGen2: Универсальная модель для многозадачной генерации текста и изображений",
                    "desc": "OmniGen2 - это универсальная генеративная модель с открытым исходным кодом, способная решать различные задачи генерации, включая преобразование текста в изображение, редактирование изображений и генерацию в контексте. Модель имеет два отдельных пути декодирования для текста и изображений, что позволяет сохранить исходные возможности генерации текста. Несмотря на относительно небольшой размер параметров, OmniGen2 достигает конкурентоспособных результатов на нескольких эталонных тестах. Авторы также представили новый бенчмарк OmniContext для оценки генерации в контексте."
                },
                "en": {
                    "title": "OmniGen2: Unifying Text and Image Generation with Dual Pathways",
                    "desc": "OmniGen2 is a generative model that enhances the creation of text and images through dual decoding pathways, allowing for specialized processing of each modality. It maintains the original text generation capabilities while introducing a new image tokenizer and reflection mechanism for improved image tasks. The model is trained using comprehensive data pipelines that support various generation tasks, including image editing and in-context generation. Despite its smaller size, OmniGen2 achieves competitive performance on benchmarks, particularly in subject-driven tasks, and aims to advance research in multimodal generation."
                },
                "zh": {
                    "title": "OmniGen2：多模态生成的统一解决方案",
                    "desc": "OmniGen2是一种多功能的生成模型，旨在为文本和图像生成任务提供统一的解决方案。与OmniGen v1不同，OmniGen2采用了两个独立的解码路径，分别处理文本和图像，使用了不共享的参数和解耦的图像标记器。这种设计使得OmniGen2能够在不重新适配VAE输入的情况下，保留原有的文本生成能力。尽管参数量相对较小，OmniGen2在多个任务基准上取得了竞争力的结果，特别是在文本到图像生成和图像编辑方面。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18841",
            "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning",
            "url": "https://huggingface.co/papers/2506.18841",
            "abstract": "An incentivization-based reinforcement learning approach is used to develop a large language model capable of generating ultra-long, high-quality text without the need for synthetic data or supervised fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under https://huggingface.co/THU-KEG/LongWriter-Zero-32B",
            "score": 21,
            "issue_id": 4447,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 июня",
                "en": "June 23",
                "zh": "6月23日"
            },
            "hash": "8589c05aae4b8258",
            "authors": [
                "Yuhao Wu",
                "Yushi Bai",
                "Zhiqiang Hu",
                "Roy Ka-Wei Lee",
                "Juanzi Li"
            ],
            "affiliations": [
                "Singapore University of Technology and Design, Singapore",
                "Tsinghua University, Beijing, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18841.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#long_context",
                    "#rl",
                    "#open_source",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "📝",
                "ru": {
                    "title": "Революция в генерации длинных текстов: RL вместо синтетических данных",
                    "desc": "В этой статье представлен новый подход к обучению больших языковых моделей (LLM) для генерации сверхдлинных и качественных текстов без использования синтетических данных или контролируемой дообучения. Метод основан на обучении с подкреплением (RL) и стимулировании модели к планированию и улучшению процесса написания. Авторы разработали специальные модели вознаграждения для улучшения контроля длины, качества письма и структурного форматирования. Эксперименты показали, что их модель LongWriter-Zero превосходит традиционные методы на задачах написания длинных текстов, достигая лучших результатов на бенчмарках WritingBench и Arena-Write."
                },
                "en": {
                    "title": "Reinforcement Learning for Ultra-Long Text Generation Without Synthetic Data",
                    "desc": "This paper presents a novel approach to generating ultra-long, high-quality text using a large language model (LLM) without relying on synthetic data or supervised fine-tuning. The authors introduce an incentivization-based reinforcement learning (RL) method that allows the model to learn from scratch, enhancing its ability to produce coherent and structured long-form content. By employing specialized reward models, the LLM is guided to improve its writing quality, length control, and formatting during the generation process. Experimental results demonstrate that the proposed LongWriter-Zero model outperforms traditional methods, achieving state-of-the-art performance on long-form writing benchmarks."
                },
                "zh": {
                    "title": "激励强化学习，超长文本生成新突破",
                    "desc": "本论文提出了一种基于激励的强化学习方法，旨在开发一种大型语言模型，能够生成超长且高质量的文本，而无需合成数据或监督微调。以往的方法通常依赖于监督微调（SFT），这需要构建合成的长文本输出，成本高且难以实现。我们的方法从零开始，通过强化学习（RL）训练模型，促进超长高质量文本生成能力的出现。实验结果表明，我们的LongWriter-Zero模型在长文本写作任务中表现优于传统的SFT方法，达到了最新的技术水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18851",
            "title": "Phantom-Data : Towards a General Subject-Consistent Video Generation\n  Dataset",
            "url": "https://huggingface.co/papers/2506.18851",
            "abstract": "A cross-pair dataset called Phantom-Data improves subject-to-video generation by enhancing prompt alignment and visual quality while maintaining identity consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Subject-to-video generation has witnessed substantial progress in recent years. However, existing models still face significant challenges in faithfully following textual instructions. This limitation, commonly known as the copy-paste problem, arises from the widely used in-pair training paradigm. This approach inherently entangles subject identity with background and contextual attributes by sampling reference images from the same scene as the target video. To address this issue, we introduce Phantom-Data, the first general-purpose cross-pair subject-to-video consistency dataset, containing approximately one million identity-consistent pairs across diverse categories. Our dataset is constructed via a three-stage pipeline: (1) a general and input-aligned subject detection module, (2) large-scale cross-context subject retrieval from more than 53 million videos and 3 billion images, and (3) prior-guided identity verification to ensure visual consistency under contextual variation. Comprehensive experiments show that training with Phantom-Data significantly improves prompt alignment and visual quality while preserving identity consistency on par with in-pair baselines.",
            "score": 19,
            "issue_id": 4447,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 июня",
                "en": "June 23",
                "zh": "6月23日"
            },
            "hash": "525a4c676b83f9a6",
            "authors": [
                "Zhuowei Chen",
                "Bingchuan Li",
                "Tianxiang Ma",
                "Lijie Liu",
                "Mingcong Liu",
                "Yi Zhang",
                "Gen Li",
                "Xinghui Li",
                "Siyu Zhou",
                "Qian He",
                "Xinglong Wu"
            ],
            "affiliations": [
                "Intelligent Creation Lab, ByteDance"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18851.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#dataset",
                    "#synthetic"
                ],
                "emoji": "🎭",
                "ru": {
                    "title": "Phantom-Data: преодоление ограничений генерации видео по субъекту",
                    "desc": "Представлен новый набор данных Phantom-Data для улучшения генерации видео по субъекту. Он содержит около миллиона пар изображений с согласованной идентичностью субъекта в разных контекстах. Набор данных создан с помощью трехэтапного процесса, включающего обнаружение субъекта, поиск по большой базе видео и изображений, и проверку идентичности. Эксперименты показывают, что обучение на Phantom-Data значительно улучшает соответствие промпту и визуальное качество при сохранении согласованности идентичности."
                },
                "en": {
                    "title": "Phantom-Data: Enhancing Video Generation with Identity Consistency",
                    "desc": "The paper introduces Phantom-Data, a new dataset designed to enhance subject-to-video generation in machine learning. This dataset addresses the 'copy-paste problem' by providing identity-consistent pairs that are not tied to specific backgrounds or contexts. It is created through a three-stage process that includes subject detection, cross-context retrieval, and identity verification. Experiments demonstrate that using Phantom-Data leads to better alignment with prompts and improved visual quality while maintaining consistent subject identity."
                },
                "zh": {
                    "title": "Phantom-Data：提升视频生成的身份一致性与视觉质量",
                    "desc": "本文介绍了一种名为Phantom-Data的跨对数据集，旨在改善基于文本生成视频的效果。该数据集通过增强提示对齐和视觉质量，同时保持身份一致性，解决了现有模型在遵循文本指令时面临的挑战。Phantom-Data包含约一百万个身份一致的配对，涵盖多种类别，采用三阶段流程构建。实验结果表明，使用Phantom-Data进行训练显著提高了生成视频的质量和一致性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18898",
            "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations",
            "url": "https://huggingface.co/papers/2506.18898",
            "abstract": "A multimodal framework uses a Text-Aligned Tokenizer (TA-Tok) to integrate vision and text into a unified space, employing a generative de-tokenizer with autoregressive and diffusion-based models for efficient and high-fidelity visual outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at https://tar.csuhan.com",
            "score": 7,
            "issue_id": 4447,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 июня",
                "en": "June 23",
                "zh": "6月23日"
            },
            "hash": "988f76bd08498ba9",
            "authors": [
                "Jiaming Han",
                "Hao Chen",
                "Yang Zhao",
                "Hanyu Wang",
                "Qi Zhao",
                "Ziyan Yang",
                "Hao He",
                "Xiangyu Yue",
                "Lu Jiang"
            ],
            "affiliations": [
                "ByteDance Seed",
                "CUHK MMLab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18898.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#multimodal",
                    "#games",
                    "#benchmark",
                    "#diffusion"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Единое пространство для текста и изображений в мультимодальных языковых моделях",
                    "desc": "Эта статья представляет мультимодальную систему, объединяющую понимание и генерацию визуальной информации в едином дискретном семантическом представлении. В основе системы лежит Text-Aligned Tokenizer (TA-Tok), который преобразует изображения в дискретные токены, используя кодовую книгу, спроектированную из словаря большой языковой модели. Система использует масштабируемое кодирование и декодирование для баланса эффективности и визуальной детализации, а также генеративный детокенизатор для создания высококачественных визуальных выходов. Эксперименты показывают, что предложенная модель Tar соответствует или превосходит существующие мультимодальные методы на основе больших языковых моделей, достигая более быстрой сходимости и большей эффективности обучения."
                },
                "en": {
                    "title": "Unifying Vision and Text with TA-Tok for Enhanced Multimodal Learning",
                    "desc": "This paper introduces a multimodal framework that combines visual and textual data into a single representation using a Text-Aligned Tokenizer (TA-Tok). The TA-Tok transforms images into discrete tokens aligned with a large language model's vocabulary, allowing for seamless interaction between text and images. The framework employs a generative de-tokenizer that includes both autoregressive and diffusion-based models to generate high-quality visual outputs efficiently. Experimental results indicate that this approach not only enhances visual understanding and generation but also outperforms existing multimodal models in terms of training speed and efficiency."
                },
                "zh": {
                    "title": "统一视觉与文本的多模态框架",
                    "desc": "这篇论文提出了一种多模态框架，旨在将视觉理解和生成统一到一个共享的离散语义表示中。核心是文本对齐的标记器（TA-Tok），它使用大型语言模型的词汇将图像转换为离散标记。通过扩展词汇，我们的多模态大语言模型Tar实现了跨模态输入和输出，避免了特定模态设计的需求。此外，我们还提出了规模自适应的编码和解码方法，以平衡效率和视觉细节，并使用生成性去标记器生成高保真视觉输出。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18896",
            "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs",
            "url": "https://huggingface.co/papers/2506.18896",
            "abstract": "ReasonFlux-PRM, a novel trajectory-aware Process Reward Model, evaluates reasoning traces with step-level and trajectory-level supervision, enhancing performance in model distillation, reinforcement learning, and test-time scaling.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: https://github.com/Gen-Verse/ReasonFlux",
            "score": 7,
            "issue_id": 4447,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 июня",
                "en": "June 23",
                "zh": "6月23日"
            },
            "hash": "6a30d79f40f7d98d",
            "authors": [
                "Jiaru Zou",
                "Ling Yang",
                "Jingwen Gu",
                "Jiahao Qiu",
                "Ke Shen",
                "Jingrui He",
                "Mengdi Wang"
            ],
            "affiliations": [
                "ByteDance",
                "Cornell University",
                "Princeton University",
                "UIUC"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18896.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#small_models",
                    "#optimization",
                    "#rl",
                    "#dataset",
                    "#benchmark",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Улучшение рассуждений ИИ через оценку траекторий мышления",
                    "desc": "ReasonFlux-PRM - это новая модель оценки процесса рассуждений, которая улучшает работу больших языковых моделей (LLM). Она оценивает промежуточные шаги рассуждений и целые траектории мышления, что позволяет более точно назначать награды в процессе обучения. ReasonFlux-PRM применяется для отбора качественных данных при дистилляции моделей, оптимизации политики в обучении с подкреплением и масштабировании во время тестирования. Эксперименты показали, что ReasonFlux-PRM превосходит существующие модели оценки процесса рассуждений на сложных задачах."
                },
                "en": {
                    "title": "Enhancing Reasoning Evaluation with ReasonFlux-PRM",
                    "desc": "ReasonFlux-PRM is a new model that improves how we evaluate reasoning processes in large language models by focusing on both individual steps and overall trajectories. It addresses the limitations of previous Process Reward Models that mainly assessed final outputs, which often missed the nuances of intermediate reasoning. By using both step-level and trajectory-level supervision, it provides more accurate rewards that align with structured reasoning data. The model has shown significant performance improvements in tasks like model distillation, reinforcement learning, and test-time scaling, outperforming existing models and human-curated benchmarks."
                },
                "zh": {
                    "title": "推理轨迹的智能评估",
                    "desc": "ReasonFlux-PRM是一种新颖的过程奖励模型，专注于评估推理轨迹，结合了逐步和轨迹级的监督。这种模型能够在模型蒸馏、强化学习和测试时扩展中提升性能。通过对推理过程的细致奖励分配，ReasonFlux-PRM能够更好地处理复杂的推理任务。实验证明，该模型在多个基准测试中表现优于传统的过程奖励模型。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18879",
            "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
            "url": "https://huggingface.co/papers/2506.18879",
            "abstract": "Commutative Vector Quantization (CommVQ) reduces memory usage in long-context LLM inference by compressing the KV cache with additive quantization and integration of Rotary Position Embedding (RoPE).  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ.",
            "score": 1,
            "issue_id": 4448,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 июня",
                "en": "June 23",
                "zh": "6月23日"
            },
            "hash": "1d442a58c0e72d5c",
            "authors": [
                "Junyan Li",
                "Yang Zhang",
                "Muhammad Yusuf Hassan",
                "Talha Chafekar",
                "Tianle Cai",
                "Zhile Ren",
                "Pengsheng Guo",
                "Foroozan Karimzadeh",
                "Colorado Reed",
                "Chong Wang",
                "Chuang Gan"
            ],
            "affiliations": [
                "Apple Inc.",
                "Massachusetts Institute of Technology",
                "Princeton University",
                "University of Massachusetts Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18879.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#training",
                    "#open_source",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Эффективное сжатие памяти для длинноконтекстных языковых моделей",
                    "desc": "Статья представляет метод Commutative Vector Quantization (CommVQ) для сжатия кэша ключ-значение в больших языковых моделях. CommVQ использует аддитивное квантование и интеграцию с Rotary Position Embedding для значительного уменьшения использования памяти. Этот подход позволяет сократить размер кэша на 87.5% при 2-битном квантовании, сохраняя высокую точность. Метод также делает возможным 1-битное квантование с минимальной потерей точности, что позволяет запускать модель LLaMA-3.1 8B с контекстом 128K на одном GPU RTX 4090."
                },
                "en": {
                    "title": "Efficient Memory Management for Long-Context LLMs with CommVQ",
                    "desc": "This paper introduces Commutative Vector Quantization (CommVQ), a method designed to reduce memory usage in long-context inference for Large Language Models (LLMs). By employing additive quantization and a commutative codebook integrated with Rotary Position Embedding (RoPE), the method compresses the key-value (KV) cache effectively. The approach allows for efficient decoding through simple matrix multiplication, significantly lowering computational costs. Experiments demonstrate that CommVQ can reduce the KV cache size by 87.5% while maintaining high accuracy, enabling LLMs to handle longer contexts on standard GPUs."
                },
                "zh": {
                    "title": "可交换向量量化：优化长上下文推理的内存使用",
                    "desc": "本文提出了一种称为可交换向量量化（CommVQ）的方法，旨在减少长上下文大语言模型（LLM）推理中的内存使用。通过引入加法量化和轻量级编码器，CommVQ能够有效压缩键值（KV）缓存，并通过简单的矩阵乘法进行解码。我们还设计了与旋转位置嵌入（RoPE）兼容的代码本，并使用期望最大化（EM）算法进行训练，从而在自注意力机制中实现高效解码。实验结果表明，该方法在保持高准确率的同时，能够将FP16 KV缓存大小减少87.5%，并在1位量化下实现最小的准确性损失。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18903",
            "title": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed\n  View Memory",
            "url": "https://huggingface.co/papers/2506.18903",
            "abstract": "A novel memory mechanism called Surfel-Indexed View Memory enhances video generation by efficiently remembering and retrieving relevant past views, improving long-term scene coherence and reducing computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a novel memory mechanism to build video generators that can explore environments interactively. Similar results have previously been achieved by out-painting 2D views of the scene while incrementally reconstructing its 3D geometry, which quickly accumulates errors, or by video generators with a short context window, which struggle to maintain scene coherence over the long term. To address these limitations, we introduce Surfel-Indexed View Memory (VMem), a mechanism that remembers past views by indexing them geometrically based on the 3D surface elements (surfels) they have observed. VMem enables the efficient retrieval of the most relevant past views when generating new ones. By focusing only on these relevant views, our method produces consistent explorations of imagined environments at a fraction of the computational cost of using all past views as context. We evaluate our approach on challenging long-term scene synthesis benchmarks and demonstrate superior performance compared to existing methods in maintaining scene coherence and camera control.",
            "score": 0,
            "issue_id": 4448,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 июня",
                "en": "June 23",
                "zh": "6月23日"
            },
            "hash": "fe5d31c2b125d778",
            "authors": [
                "Runjia Li",
                "Philip Torr",
                "Andrea Vedaldi",
                "Tomas Jakab"
            ],
            "affiliations": [
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18903.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#video",
                    "#benchmark",
                    "#optimization",
                    "#3d"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Эффективная память для согласованной генерации видео",
                    "desc": "Статья представляет новый механизм памяти под названием Surfel-Indexed View Memory (VMem) для улучшения генерации видео. VMem позволяет эффективно запоминать и извлекать релевантные прошлые виды сцены, используя индексацию на основе 3D поверхностных элементов (сурфелей). Этот подход улучшает долгосрочную согласованность сцены и снижает вычислительные затраты по сравнению с использованием всех прошлых видов в качестве контекста. Метод демонстрирует превосходную производительность на сложных задачах долгосрочного синтеза сцен по сравнению с существующими методами."
                },
                "en": {
                    "title": "Enhancing Video Generation with Efficient Memory Retrieval",
                    "desc": "This paper introduces a new memory mechanism called Surfel-Indexed View Memory (VMem) that improves video generation by efficiently recalling relevant past views. Unlike traditional methods that either accumulate errors or have limited context, VMem uses geometric indexing based on 3D surface elements to enhance long-term scene coherence. By focusing on the most pertinent past views, it reduces computational costs while generating consistent and coherent video outputs. The approach is evaluated against challenging benchmarks, showing better performance in maintaining scene integrity and camera control compared to existing techniques."
                },
                "zh": {
                    "title": "高效记忆，提升视频生成的一致性",
                    "desc": "本文提出了一种新颖的记忆机制，称为表面索引视图记忆（Surfel-Indexed View Memory），旨在提高视频生成的效果。该机制通过几何索引过去的视图，基于观察到的三维表面元素（surfels）来有效地记忆和检索相关的历史视图。与传统方法相比，VMem能够在生成新视图时高效地提取最相关的过去视图，从而在降低计算成本的同时保持长期场景的一致性。我们在长期场景合成基准测试中评估了该方法，结果显示其在场景一致性和相机控制方面的表现优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.18631",
            "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
            "url": "https://huggingface.co/papers/2506.18631",
            "abstract": "ReDit, a reward dithering method, addresses issues in discrete reward systems by introducing noise, leading to smoother optimization and faster convergence compared to standard methods.  \t\t\t\t\tAI-generated summary \t\t\t\t DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages.",
            "score": 0,
            "issue_id": 4448,
            "pub_date": "2025-06-23",
            "pub_date_card": {
                "ru": "23 июня",
                "en": "June 23",
                "zh": "6月23日"
            },
            "hash": "cc6b6162c9368cf4",
            "authors": [
                "Chenxing Wei",
                "Jiarui Yu",
                "Ying Tiffany He",
                "Hande Dong",
                "Yao Shu",
                "Fei Yu"
            ],
            "affiliations": [
                "College of Computer Science and Software Engineering, Shenzhen University, China",
                "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), China",
                "Hong Kong University of Science and Technology (Guangzhou), China",
                "Tencent, Shenzhen, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.18631.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#rl",
                    "#reasoning",
                    "#optimization"
                ],
                "emoji": "🎲",
                "ru": {
                    "title": "Шум во благо: ReDit улучшает обучение языковых моделей",
                    "desc": "ReDit - это метод добавления шума в дискретные системы вознаграждения для улучшения обучения языковых моделей. Он решает проблемы аномалий градиента и медленной сходимости, характерные для дискретных наград. ReDit обеспечивает более плавную оптимизацию и ускоряет сходимость по сравнению со стандартными методами. Эксперименты показали, что ReDit достигает сопоставимой производительности примерно за 10% шагов обучения по сравнению с обычным GRPO."
                },
                "en": {
                    "title": "ReDit: Smoother Rewards for Faster Learning",
                    "desc": "ReDit is a novel method designed to improve optimization in systems that use discrete rewards by adding random noise to the reward signal. This noise helps to create smoother gradient updates, which leads to faster convergence during training. By introducing stochasticity, ReDit encourages exploration of new policies, helping models avoid getting stuck in local optima. Experimental results show that ReDit not only reduces training time significantly but also enhances performance compared to traditional methods."
                },
                "zh": {
                    "title": "ReDit：提升离散奖励系统的优化效率",
                    "desc": "ReDit是一种奖励抖动方法，旨在解决离散奖励系统中的问题。通过引入噪声，ReDit使得优化过程更加平滑，并且收敛速度比标准方法更快。实验表明，离散奖励可能导致梯度异常和不稳定的优化，而ReDit通过添加随机噪声来改善这一点。最终，ReDit在多个任务中表现出色，训练步骤仅为传统方法的10%，同时在相似训练时间内性能提升了4%。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2506.10597",
            "title": "SoK: Evaluating Jailbreak Guardrails for Large Language Models",
            "url": "https://huggingface.co/papers/2506.10597",
            "abstract": "A systematic analysis and evaluation framework for jailbreak guardrails in Large Language Models is presented, categorizing and assessing their effectiveness and optimization potential.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable progress, but their deployment has exposed critical vulnerabilities, particularly to jailbreak attacks that circumvent safety mechanisms. Guardrails--external defense mechanisms that monitor and control LLM interaction--have emerged as a promising solution. However, the current landscape of LLM guardrails is fragmented, lacking a unified taxonomy and comprehensive evaluation framework. In this Systematization of Knowledge (SoK) paper, we present the first holistic analysis of jailbreak guardrails for LLMs. We propose a novel, multi-dimensional taxonomy that categorizes guardrails along six key dimensions, and introduce a Security-Efficiency-Utility evaluation framework to assess their practical effectiveness. Through extensive analysis and experiments, we identify the strengths and limitations of existing guardrail approaches, explore their universality across attack types, and provide insights into optimizing defense combinations. Our work offers a structured foundation for future research and development, aiming to guide the principled advancement and deployment of robust LLM guardrails. The code is available at https://github.com/xunguangwang/SoK4JailbreakGuardrails.",
            "score": 0,
            "issue_id": 4448,
            "pub_date": "2025-06-12",
            "pub_date_card": {
                "ru": "12 июня",
                "en": "June 12",
                "zh": "6月12日"
            },
            "hash": "4122cc84dd4333e8",
            "authors": [
                "Xunguang Wang",
                "Zhenlan Ji",
                "Wenxuan Wang",
                "Zongjie Li",
                "Daoyuan Wu",
                "Shuai Wang"
            ],
            "affiliations": [
                "Renmin University of China",
                "The Hong Kong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2506.10597.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#benchmark",
                    "#security",
                    "#optimization"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Комплексный анализ защиты LLM от взлома: создание надежных барьеров",
                    "desc": "В статье представлен систематический анализ и оценка защитных механизмов (гардрейлов) для больших языковых моделей (LLM) от атак типа jailbreak. Авторы предлагают многомерную таксономию, классифицирующую гардрейлы по шести ключевым параметрам. Вводится система оценки Security-Efficiency-Utility для определения практической эффективности защитных механизмов. Исследование выявляет сильные и слабые стороны существующих подходов к защите LLM, а также предлагает рекомендации по оптимизации комбинаций защитных мер."
                },
                "en": {
                    "title": "Strengthening LLMs: A New Framework for Jailbreak Guardrails",
                    "desc": "This paper presents a comprehensive framework for analyzing and evaluating guardrails designed to protect Large Language Models (LLMs) from jailbreak attacks. It introduces a multi-dimensional taxonomy that categorizes these guardrails based on six important aspects, helping to clarify their roles and effectiveness. Additionally, the authors propose a new evaluation framework that balances security, efficiency, and utility, allowing for a thorough assessment of guardrail performance. By identifying the strengths and weaknesses of current approaches, this work aims to enhance the development of more effective defenses for LLMs against potential vulnerabilities."
                },
                "zh": {
                    "title": "系统化评估大型语言模型的越狱防护机制",
                    "desc": "本文提出了一种系统化的分析和评估框架，用于大型语言模型（LLMs）中的越狱防护机制。研究表明，尽管LLMs取得了显著进展，但在实际应用中暴露了关键的安全漏洞，尤其是越狱攻击。我们提出了一种新的多维分类法，将防护机制分为六个关键维度，并引入了安全性、效率和实用性评估框架，以评估其实际效果。通过广泛的分析和实验，我们识别了现有防护方法的优缺点，并为未来的研究和开发提供了结构化的基础。"
                }
            }
        }
    ],
    "link_prev": "2025-06-23.html",
    "link_next": "2025-06-25.html",
    "link_month": "2025-06.html",
    "short_date_prev": {
        "ru": "23.06",
        "en": "06/23",
        "zh": "6月23日"
    },
    "short_date_next": {
        "ru": "25.06",
        "en": "06/25",
        "zh": "6月25日"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 6,
        "#agents": 0,
        "#cv": 1,
        "#rl": 3,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 1,
        "#3d": 1,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 0,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 1,
        "#optimization": 5,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 3,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 3,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 0
    }
}