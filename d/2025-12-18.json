{
    "date": {
        "ru": "18 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 18",
        "zh": "12æœˆ18æ—¥"
    },
    "time_utc": "2025-12-18 17:24",
    "weekday": 3,
    "issue_id": 131,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2512.15431",
            "title": "Step-GUI Technical Report",
            "url": "https://huggingface.co/papers/2512.15431",
            "abstract": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.",
            "score": 75,
            "issue_id": 118,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "4b0a9b529bca3650",
            "authors": [
                "Haolong Yan",
                "Jia Wang",
                "Xin Huang",
                "Yeqing Shen",
                "Ziyang Meng",
                "Zhimin Fan",
                "Kaijun Tan",
                "Jin Gao",
                "Lieyu Shi",
                "Mi Yang",
                "Shiliang Yang",
                "Zhirui Wang",
                "Brian Li",
                "Kang An",
                "Chenyang Li",
                "Lei Lei",
                "Mengmeng Duan",
                "Danxun Liang",
                "Guodong Liu",
                "Hang Cheng",
                "Hao Wu",
                "Jie Dong",
                "Junhao Huang",
                "Mei Chen",
                "Renjie Yu",
                "Shunshan Li",
                "Xu Zhou",
                "Yiting Dai",
                "Yineng Deng",
                "Yingdan Liang",
                "Zelin Chen",
                "Wen Sun",
                "Chengxu Yan",
                "Chunqin Xu",
                "Dong Li",
                "Fengqiong Xiao",
                "Guanghao Fan",
                "Guopeng Li",
                "Guozhen Peng",
                "Hongbing Li",
                "Hang Li",
                "Hongming Chen",
                "Jingjing Xie",
                "Jianyong Li",
                "Jingyang Zhang",
                "Jiaju Ren",
                "Jiayu Yuan",
                "Jianpeng Yin",
                "Kai Cao",
                "Liang Zhao",
                "Liguo Tan",
                "Liying Shi",
                "Mengqiang Ren",
                "Min Xu",
                "Manjiao Liu",
                "Mao Luo",
                "Mingxin Wan",
                "Na Wang",
                "Nan Wu",
                "Ning Wang",
                "Peiyao Ma",
                "Qingzhou Zhang",
                "Qiao Wang",
                "Qinlin Zeng",
                "Qiong Gao",
                "Qiongyao Li",
                "Shangwu Zhong",
                "Shuli Gao",
                "Shaofan Liu",
                "Shisi Gao",
                "Shuang Luo",
                "Xingbin Liu",
                "Xiaojia Liu",
                "Xiaojie Hou",
                "Xin Liu",
                "Xuanti Feng",
                "Xuedan Cai",
                "Xuan Wen",
                "Xianwei Zhu",
                "Xin Liang",
                "Xin Liu",
                "Xin Zhou",
                "Yingxiu Zhao",
                "Yukang Shi",
                "Yunfang Xu",
                "Yuqing Zeng",
                "Yixun Zhang",
                "Zejia Weng",
                "Zhonghao Yan",
                "Zhiguo Huang",
                "Zhuoyu Wang",
                "Zheng Ge",
                "Jing Li",
                "Yibo Zhu",
                "Binxing Jiao",
                "Xiangyu Zhang",
                "Daxin Jiang"
            ],
            "affiliations": [
                "GELab-Team, StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15431.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#multimodal",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ÑÑ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° ÑˆĞ°Ğ³Ğ¸ (Calibrated Step Reward System), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ğ² Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞµĞ¼ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Step-GUI (4B/8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²), Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ°Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… AndroidWorld, OSWorld Ğ¸ ScreenShot-Pro. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» GUI-MCP Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ´ĞµĞ»ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ’Ğ²ĞµĞ´Ñ‘Ğ½ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AndroidDaily, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ñ… ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 3000 ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ 235 end-to-end Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Revolutionizing GUI Automation with Self-Evolving Training and Privacy Protocols",
                    "desc": "This paper presents a self-evolving training pipeline that enhances GUI automation through the Calibrated Step Reward System, which ensures high-quality training data with over 90% annotation accuracy at a significantly reduced cost. The authors introduce Step-GUI, a series of models that achieve state-of-the-art performance in GUI tasks while maintaining strong general capabilities. To address privacy concerns, they propose the GUI-MCP protocol, which allows for secure execution of tasks by keeping sensitive data on-device. Additionally, they introduce the AndroidDaily benchmark to evaluate agent performance in real-world scenarios, demonstrating the practical applicability of their advancements in GUI automation."
                },
                "zh": {
                    "title": "è‡ªæˆ‘æ¼”åŒ–çš„è®­ç»ƒç®¡é“ï¼Œæå‡GUIè‡ªåŠ¨åŒ–æ•ˆç‡ä¸éšç§ä¿æŠ¤",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªæˆ‘æ¼”åŒ–çš„è®­ç»ƒç®¡é“ï¼Œåˆ©ç”¨æ ¡å‡†æ­¥éª¤å¥–åŠ±ç³»ç»Ÿæ¥æé«˜å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰è‡ªåŠ¨åŒ–çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡è½¨è¿¹çº§æ ¡å‡†ï¼Œè¯¥ç³»ç»Ÿå°†æ¨¡å‹ç”Ÿæˆçš„è½¨è¿¹è½¬åŒ–ä¸ºå¯é çš„è®­ç»ƒä¿¡å·ï¼Œå®ç°äº†è¶…è¿‡90%çš„æ ‡æ³¨å‡†ç¡®ç‡ï¼Œå¹¶ä¸”æˆæœ¬é™ä½äº†10åˆ°100å€ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†Step-GUIæ¨¡å‹ç³»åˆ—ï¼Œå±•ç¤ºäº†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜è¶Šçš„GUIæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„é€šç”¨èƒ½åŠ›ã€‚ä¸ºäº†ä¿æŠ¤ç”¨æˆ·éšç§ï¼Œæˆ‘ä»¬æå‡ºäº†GUI-MCPåè®®ï¼Œç»“åˆä½çº§åŸå­æ“ä½œå’Œé«˜çº§ä»»åŠ¡å§”æ´¾ï¼Œå®ç°äº†é«˜éšç§æ‰§è¡Œã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15176",
            "title": "DEER: Draft with Diffusion, Verify with Autoregressive Models",
            "url": "https://huggingface.co/papers/2512.15176",
            "abstract": "DEER framework uses diffusion large language models for efficient speculative decoding, overcoming the limitations of autoregressive drafters with better speed and draft quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/",
            "score": 37,
            "issue_id": 117,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "7a3258e2f37942b7",
            "authors": [
                "Zicong Cheng",
                "Guo-Wei Yang",
                "Jia Li",
                "Zhijie Deng",
                "Meng-Hao Guo",
                "Shi-Min Hu"
            ],
            "affiliations": [
                "Proxseer Inc",
                "Shanghai Jiao Tong University",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15176.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#training",
                    "#diffusion",
                    "#architecture",
                    "#optimization",
                    "#open_source"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸: ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° DEER â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ´Ñ€Ğ°Ñ„Ñ‚ĞµÑ€Ğ¾Ğ². ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ÑÑ‚ ÑÑ‚Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ¾Ğ² Ğ·Ğ° Ñ€Ğ°Ğ·. DEER Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ñ€Ğ°Ñ„Ñ‚ĞµÑ€Ğ° Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰ĞµĞ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 5.54 Ñ€Ğ°Ğ·Ğ° Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ°Ñ… HumanEval."
                },
                "en": {
                    "title": "DEER: Speeding Up Speculative Decoding with Diffusion Models",
                    "desc": "The DEER framework introduces a novel approach to speculative decoding using diffusion large language models (dLLMs), which enhances both speed and draft quality compared to traditional autoregressive (AR) models. By employing a draft-verify scheme, DEER mitigates the issues of uncertainty accumulation and sequential decoding that plague existing AR drafters. The framework utilizes a two-stage training process to align dLLM drafters with target AR models, allowing for efficient single-step decoding of longer draft segments. Experimental results demonstrate that DEER significantly outperforms previous methods, achieving faster draft acceptance and higher quality outputs."
                },
                "zh": {
                    "title": "DEERï¼šé«˜æ•ˆæ¨æµ‹è§£ç çš„æ–°æ¡†æ¶",
                    "desc": "DEERæ¡†æ¶åˆ©ç”¨æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMï¼‰è¿›è¡Œé«˜æ•ˆçš„æ¨æµ‹è§£ç ï¼Œå…‹æœäº†è‡ªå›å½’ï¼ˆARï¼‰è‰æ‹Ÿæ¨¡å‹çš„é€Ÿåº¦å’Œè‰ç¨¿è´¨é‡é™åˆ¶ã€‚æ¨æµ‹è§£ç é€šè¿‡è‰æ‹Ÿ-éªŒè¯æ–¹æ¡ˆå‡è½»äº†å»¶è¿Ÿé—®é¢˜ï¼Œä½†ç°æœ‰æ–¹æ³•ä¾èµ–äºARè‰æ‹Ÿæ¨¡å‹ï¼Œå¯¼è‡´ä¿¡ä»»åº¦é€æ­¥ä¸‹é™å’Œè§£ç è¿‡ç¨‹çš„é¡ºåºæ€§ã€‚æœ¬æ–‡å±•ç¤ºäº†dLLMè‰æ‹Ÿæ¨¡å‹å¦‚ä½•é€šè¿‡å…¶ä¸åŒçš„æ¦‚ç‡å»ºæ¨¡å’Œé«˜æ•ˆçš„å¹¶è¡Œè§£ç ç­–ç•¥è‡ªç„¶å…‹æœè¿™äº›é—®é¢˜ã€‚DEERæ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“å’Œå•æ­¥è§£ç ç”Ÿæˆé•¿è‰ç¨¿æ®µï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨è‰ç¨¿æ¥å—é•¿åº¦å’Œé€Ÿåº¦ä¸Šå‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.14681",
            "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing",
            "url": "https://huggingface.co/papers/2512.14681",
            "abstract": "Jacobi Forcing is a progressive distillation method that enables efficient parallel decoding of transformer-based models while maintaining performance, significantly reducing inference latency.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.",
            "score": 36,
            "issue_id": 118,
            "pub_date": "2025-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "f2e27d4f14a451aa",
            "authors": [
                "Lanxiang Hu",
                "Siqi Kou",
                "Yichao Fu",
                "Samyam Rajbhandari",
                "Tajana Rosing",
                "Yuxiong He",
                "Zhijie Deng",
                "Hao Zhang"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Snowflake",
                "UC San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.14681.jpg",
            "data": {
                "categories": [],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ğ½Ğ³ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Jacobi Forcing â€” Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ñ‹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ… Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 3.8x ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚ĞµÑ€ÑÑ… ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² â€” Ğ²Ğ¿Ğ»Ğ¾Ñ‚ÑŒ Ğ´Ğ¾ 4.5x ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Accelerating Transformer Decoding with Jacobi Forcing",
                    "desc": "Jacobi Forcing is a new method that improves the efficiency of decoding in transformer models by allowing them to work in parallel without losing performance. It addresses the issue of slow inference in large language models by training them on their own generated outputs, which helps maintain the causal properties learned during pretraining. This method results in a significant speedup, achieving up to 3.8 times faster decoding on specific tasks while keeping the quality of the output high. Additionally, it introduces a technique called multi-block decoding with rejection recycling, which further enhances the speed and efficiency of token generation."
                },
                "zh": {
                    "title": "Jacobi Forcingï¼šé«˜æ•ˆå¹¶è¡Œè§£ç çš„æ–°æ–¹æ³•",
                    "desc": "Jacobi Forcingæ˜¯ä¸€ç§æ¸è¿›å¼è’¸é¦æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åŸºäºå˜æ¢å™¨æ¨¡å‹çš„å¹¶è¡Œè§£ç æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ï¼Œæ˜¾è‘—å‡å°‘æ¨ç†å»¶è¿Ÿã€‚è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒæ¨¡å‹åœ¨è‡ªèº«ç”Ÿæˆçš„å¹¶è¡Œè§£ç è½¨è¿¹ä¸Šï¼Œå¹³æ»‘åœ°å°†è‡ªå›å½’æ¨¡å‹è½¬å˜ä¸ºé«˜æ•ˆçš„å¹¶è¡Œè§£ç å™¨ï¼Œä¿ç•™äº†é¢„è®­ç»ƒçš„å› æœæ¨ç†ç‰¹æ€§ã€‚Jacobi Forcingæ¨¡å‹åœ¨ç¼–ç å’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†3.8å€çš„å¢™é’Ÿé€Ÿåº¦æå‡ï¼Œæ€§èƒ½æŸå¤±æå°ã€‚æ­¤å¤–ï¼ŒåŸºäºJacobi Forcingæ¨¡å‹çš„è½¨è¿¹ç‰¹æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šå—è§£ç å’Œæ‹’ç»å›æ”¶æœºåˆ¶ï¼Œä½¿æ¯æ¬¡è¿­ä»£çš„ä»¤ç‰Œæ¥å—æ•°é‡æé«˜äº†4.5å€ï¼Œå¢™é’Ÿé€Ÿåº¦æå‡æ¥è¿‘4.0å€ï¼Œæœ‰æ•ˆåœ°é€šè¿‡å¢åŠ è®¡ç®—é‡æ¥é™ä½æ¨ç†å»¶è¿Ÿã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.14052",
            "title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices",
            "url": "https://huggingface.co/papers/2512.14052",
            "abstract": "HyperVL, an efficient multimodal large language model for on-device inference, uses image tiling, Visual Resolution Compressor, and Dual Consistency Learning to reduce memory usage, latency, and power consumption while maintaining performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.",
            "score": 29,
            "issue_id": 122,
            "pub_date": "2025-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "f286ddbad2fda2bc",
            "authors": [
                "HyperAI Team",
                "Yuchen Liu",
                "Kaiyang Han",
                "Zhiqiang Xia",
                "Yuhang Dong",
                "Chen Song",
                "Kangyu Tang",
                "Jiaming Xu",
                "Xiushi Feng",
                "WenXuan Yu",
                "Li Peng",
                "Mingyang Wang",
                "Kai Wang",
                "Changpeng Yang",
                "Yang Li",
                "Haoyu Lu",
                "Hao Wang",
                "Bingna Xu",
                "Guangyao Liu",
                "Long Huang",
                "Kaibin Guo",
                "Jinyang Wu",
                "Dan Wu",
                "Hongzhen Wang",
                "Peng Zhou",
                "Shuai Nie",
                "Shande Wang",
                "Runyu Shi",
                "Ying Huang"
            ],
            "affiliations": [
                "Xiaomi Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.14052.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#multimodal",
                    "#architecture",
                    "#inference",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…",
                    "desc": "HyperVL â€” ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ»Ğ¸Ñ‚ĞºĞ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ²Ğ¾Ğ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Vision Transformer ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ² Ğ¿Ğ¾Ğ´ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ HyperVL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¸ ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…."
                },
                "en": {
                    "title": "HyperVL: Efficient Multimodal Inference for Mobile Devices",
                    "desc": "HyperVL is a multimodal large language model designed for efficient on-device inference, addressing the high computational demands of traditional models. It employs an image-tiling approach to manage memory usage and introduces a Visual Resolution Compressor to optimize encoding resolutions, reducing unnecessary computations. Additionally, HyperVL utilizes Dual Consistency Learning to synchronize multi-scale Vision Transformer encoders, allowing for flexible processing of visual data. The model achieves top performance in benchmarks while significantly lowering latency and power consumption, making it suitable for mobile applications."
                },
                "zh": {
                    "title": "HyperVLï¼šé«˜æ•ˆçš„è®¾å¤‡ç«¯å¤šæ¨¡æ€æ¨ç†æ¨¡å‹",
                    "desc": "HyperVLæ˜¯ä¸€ç§é«˜æ•ˆçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œä¸“ä¸ºè®¾å¤‡ç«¯æ¨ç†è®¾è®¡ã€‚å®ƒé€šè¿‡å›¾åƒåˆ‡ç‰‡ã€è§†è§‰åˆ†è¾¨ç‡å‹ç¼©å™¨å’ŒåŒä¸€è‡´æ€§å­¦ä¹ ç­‰æŠ€æœ¯ï¼Œé™ä½äº†å†…å­˜ä½¿ç”¨ã€å»¶è¿Ÿå’ŒåŠŸè€—ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹è§£å†³äº†ä¼ ç»Ÿè§†è§‰å˜æ¢å™¨åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡è¾“å…¥æ—¶çš„å»¶è¿Ÿå’Œå†…å­˜æ¶ˆè€—é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒHyperVLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œé€‚åˆåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¿›è¡Œå¤šæ¨¡æ€æ¨ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15635",
            "title": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning",
            "url": "https://huggingface.co/papers/2512.15635",
            "abstract": "IC-Effect, an instruction-guided DiT-based framework, synthesizes complex video VFX effects while preserving spatial and temporal consistency using a two-stage training strategy and spatiotemporal sparse tokenization.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose IC-Effect, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning 15 high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.",
            "score": 17,
            "issue_id": 120,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "fe21bf06affed5c0",
            "authors": [
                "Yuanhang Li",
                "Yiren Song",
                "Junzhe Bai",
                "Xinran Liang",
                "Hu Yang",
                "Libiao Jin",
                "Qi Mao"
            ],
            "affiliations": [
                "Baidu Inc., Beijing, China",
                "School of Information and Communication Engineering, Communication University of China",
                "Show Lab, National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15635.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#video",
                    "#open_source",
                    "#dataset",
                    "#training",
                    "#diffusion"
                ],
                "emoji": "âœ¨",
                "ru": {
                    "title": "Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "IC-Effect â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (DiT) Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ğ³Ğ¾Ğ½ÑŒ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ†Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· LoRA, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ². Ğ’Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°."
                },
                "en": {
                    "title": "IC-Effect: Seamless Video VFX Editing with Instruction-Guided Learning",
                    "desc": "IC-Effect is a novel framework designed for video visual effects (VFX) editing that utilizes instruction-guided learning and DiT (Denoising Image Transformer) models. It effectively synthesizes complex effects like flames and particles while ensuring that the original video background remains unchanged and consistent over time. The framework employs a two-stage training approach, which includes general editing adaptation and specific learning for different effects, enhancing its ability to follow instructions and model effects accurately. Additionally, it introduces spatiotemporal sparse tokenization to improve computational efficiency while maintaining high fidelity in the generated effects."
                },
                "zh": {
                    "title": "IC-Effectï¼šé«˜æ•ˆè§†é¢‘ç‰¹æ•ˆåˆæˆçš„æ–°æ–¹æ³•",
                    "desc": "IC-Effectæ˜¯ä¸€ç§åŸºäºæŒ‡ä»¤å¼•å¯¼çš„DiTæ¡†æ¶ï¼Œæ—¨åœ¨åˆæˆå¤æ‚çš„è§†é¢‘ç‰¹æ•ˆï¼ŒåŒæ—¶ä¿æŒç©ºé—´å’Œæ—¶é—´çš„ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œç»“åˆç¨€ç–æ—¶ç©ºæ ‡è®°åŒ–æŠ€æœ¯ï¼Œæœ‰æ•ˆåœ°ä»æœ‰é™çš„é…å¯¹æ•°æ®ä¸­å­¦ä¹ ç‰¹æ•ˆæ¨¡å¼ã€‚IC-Effectåˆ©ç”¨æºè§†é¢‘ä½œä¸ºå¹²å‡€çš„ä¸Šä¸‹æ–‡æ¡ä»¶ï¼Œç¡®ä¿èƒŒæ™¯ä¿æŒä¸å˜ï¼Œå¹¶å®ç°è‡ªç„¶çš„ç‰¹æ•ˆæ³¨å…¥ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼ŒIC-Effectå±•ç¤ºäº†é«˜è´¨é‡ã€å¯æ§ä¸”æ—¶é—´ä¸€è‡´çš„è§†é¢‘ç‰¹æ•ˆç¼–è¾‘èƒ½åŠ›ï¼Œæ¨åŠ¨äº†è§†é¢‘åˆ›ä½œçš„æ–°å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15693",
            "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning",
            "url": "https://huggingface.co/papers/2512.15693",
            "abstract": "Skyra, a specialized multimodal large language model, detects and explains visual artifacts in AI-generated videos using a novel dataset and two-stage training strategy, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.",
            "score": 16,
            "issue_id": 117,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "6d355b4b185d77c6",
            "authors": [
                "Yifei Li",
                "Wenzhao Zheng",
                "Yanran Zhang",
                "Runze Sun",
                "Yu Zheng",
                "Lei Chen",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "affiliations": [
                "Department of Automation, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15693.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#multimodal",
                    "#dataset",
                    "#benchmark",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞĞ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ² ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Skyra â€” ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑĞ¼Ğ¸, Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ Ğ¸Ñ… Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ViF-CoT-4K â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼. Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ViF-Bench Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Skyra Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Skyra: Unveiling the Truth in AI-Generated Videos",
                    "desc": "Skyra is a specialized multimodal large language model designed to detect and explain visual artifacts in AI-generated videos. It utilizes a novel dataset called ViF-CoT-4K, which is the first large-scale collection of AI-generated video artifacts with detailed human annotations. The model employs a two-stage training strategy to improve its ability to perceive spatio-temporal artifacts and provide explanations for its detections. Skyra outperforms existing detection methods and contributes to the field of explainable AI by offering insights into the detection process."
                },
                "zh": {
                    "title": "Skyraï¼šè¶…è¶Šç°æœ‰æ–¹æ³•çš„AIè§†é¢‘ä¼ªå½±æ£€æµ‹ä¸è§£é‡Š",
                    "desc": "Skyraæ˜¯ä¸€ç§ä¸“é—¨çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ£€æµ‹å’Œè§£é‡ŠAIç”Ÿæˆè§†é¢‘ä¸­çš„è§†è§‰ä¼ªå½±ã€‚å®ƒä½¿ç”¨äº†ä¸€ç§æ–°é¢–çš„æ•°æ®é›†å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æ–¹æ³•ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€ç›®æ ‡ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ViF-CoT-4Kæ•°æ®é›†ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…·æœ‰ç»†ç²’åº¦äººç±»æ³¨é‡Šçš„å¤§è§„æ¨¡AIç”Ÿæˆè§†é¢‘ä¼ªå½±æ•°æ®é›†ã€‚Skyraçš„è¯„ä¼°é€šè¿‡ViF-BenchåŸºå‡†è¿›è¡Œï¼Œæ˜¾ç¤ºå…¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸ºå¯è§£é‡Šçš„AIç”Ÿæˆè§†é¢‘æ£€æµ‹æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.14693",
            "title": "Universal Reasoning Model",
            "url": "https://huggingface.co/papers/2512.14693",
            "abstract": "The Universal Reasoning Model enhances Universal Transformers with short convolution and truncated backpropagation to improve reasoning performance on ARC-AGI tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.",
            "score": 16,
            "issue_id": 117,
            "pub_date": "2025-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "2fbca075ff0f8651",
            "authors": [
                "Zitian Gao",
                "Lynx Chen",
                "Yihao Xiao",
                "He Xing",
                "Ran Tao",
                "Haoming Luo",
                "Joey Zhou",
                "Bryan Dai"
            ],
            "affiliations": [
                "Ubiquant"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.14693.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#agi",
                    "#architecture",
                    "#open_source"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ",
                    "desc": "Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Universal Transformers Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ARC-AGI. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ¸Ğ· Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ° Ğ½Ğµ Ğ¸Ğ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Universal Reasoning Model (URM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ UT ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾Ğ¹ ÑĞ²ĞµÑ€Ñ‚ĞºĞ¾Ğ¹ Ğ¸ ÑƒÑĞµÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¿Ğ°Ğ³Ğ°Ñ†Ğ¸ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²: 53.8% pass@1 Ğ½Ğ° ARC-AGI 1 Ğ¸ 16.0% pass@1 Ğ½Ğ° ARC-AGI 2."
                },
                "en": {
                    "title": "Enhancing Reasoning with Universal Transformers",
                    "desc": "The Universal Reasoning Model (URM) builds on Universal Transformers to enhance their reasoning capabilities for tasks like ARC-AGI. It introduces short convolution and truncated backpropagation to leverage the strengths of recurrent inductive bias and nonlinear components in Transformers. The study reveals that these enhancements lead to significant performance improvements, achieving state-of-the-art results on ARC-AGI benchmarks. This work highlights the importance of understanding the underlying mechanisms of model performance rather than just focusing on complex architectures."
                },
                "zh": {
                    "title": "æå‡æ¨ç†æ€§èƒ½çš„é€šç”¨æ¨ç†æ¨¡å‹",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†é€šç”¨æ¨ç†æ¨¡å‹ï¼ˆURMï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨é€šç”¨å˜æ¢å™¨ï¼ˆUTï¼‰çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡çŸ­å·ç§¯å’Œæˆªæ–­åå‘ä¼ æ’­æ¥æå‡æ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬ç³»ç»Ÿåˆ†æäº†UTçš„ä¸åŒå˜ä½“ï¼Œå‘ç°å…¶åœ¨ARC-AGIä»»åŠ¡ä¸Šçš„æ€§èƒ½æå‡ä¸»è¦æºäºå˜æ¢å™¨çš„é€’å½’å½’çº³åç½®å’Œå¼ºéçº¿æ€§ç»„ä»¶ï¼Œè€Œéå¤æ‚çš„æ¶æ„è®¾è®¡ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒURMåœ¨ARC-AGI 1ä»»åŠ¡ä¸Šè¾¾åˆ°äº†53.8%çš„æœ€ä½³é€šè¿‡ç‡ï¼Œåœ¨ARC-AGI 2ä»»åŠ¡ä¸Šè¾¾åˆ°äº†16.0%çš„æœ€ä½³é€šè¿‡ç‡ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15603",
            "title": "Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition",
            "url": "https://huggingface.co/papers/2512.15603",
            "abstract": "Qwen-Image-Layered decomposes images into semantically disentangled RGBA layers using a diffusion model, enabling independent editing of each layer and improving decomposition quality and consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose Qwen-Image-Layered, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling inherent editability, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on https://github.com/QwenLM/Qwen-Image-Layered{https://github.com/QwenLM/Qwen-Image-Layered}",
            "score": 14,
            "issue_id": 118,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "78e6f8545e2ff5cf",
            "authors": [
                "Shengming Yin",
                "Zekai Zhang",
                "Zecheng Tang",
                "Kaiyuan Gao",
                "Xiao Xu",
                "Kun Yan",
                "Jiahao Li",
                "Yilei Chen",
                "Yuxiang Chen",
                "Heung-Yeung Shum",
                "Lionel M. Ni",
                "Jingren Zhou",
                "Junyang Lin",
                "Chenfei Wu"
            ],
            "affiliations": [
                "Alibaba",
                "HKUST",
                "HKUST(GZ)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15603.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ¡Ğ»Ğ¾Ğ¸ÑÑ‚Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "Qwen-Image-Layered â€” ÑÑ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ RGBA, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ ÑĞ»Ğ¾ÑĞ¼ Ğ² Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ VLD-MMDiT Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ»Ğ¾Ñ‘Ğ² Ğ¸ RGBA-VAE Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ RGB Ğ¸ RGBA Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Photoshop Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»ÑƒÑ‡ÑˆÑƒÑ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "Revolutionizing Image Editing with Layered Decomposition",
                    "desc": "Qwen-Image-Layered is a novel diffusion model that breaks down images into separate RGBA layers, allowing for independent editing of each layer. This approach addresses the common issue of inconsistency in image editing by providing a structured, layered representation similar to professional design tools. The model incorporates an RGBA-VAE for unified latent representation, a VLD-MMDiT architecture for variable layer decomposition, and a Multi-stage Training strategy to enhance performance. By utilizing a pipeline to extract multilayer images from Photoshop documents, the method achieves superior decomposition quality and sets a new standard for consistent image editing."
                },
                "zh": {
                    "title": "å›¾åƒåˆ†è§£ä¸ç‹¬ç«‹ç¼–è¾‘çš„æ–°æ–¹æ³•",
                    "desc": "Qwen-Image-Layered æ˜¯ä¸€ç§ä½¿ç”¨æ‰©æ•£æ¨¡å‹å°†å›¾åƒåˆ†è§£ä¸ºè¯­ä¹‰ä¸Šç‹¬ç«‹çš„ RGBA å±‚çš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å…è®¸å¯¹æ¯ä¸ªå±‚è¿›è¡Œç‹¬ç«‹ç¼–è¾‘ï¼Œä»è€Œæé«˜äº†åˆ†è§£çš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚ä¸ä¼ ç»Ÿçš„å…‰æ …å›¾åƒä¸åŒï¼ŒQwen-Image-Layered é‡‡ç”¨åˆ†å±‚è¡¨ç¤ºï¼Œä½¿å¾—åœ¨ç¼–è¾‘æ—¶å¯ä»¥ä¿æŒå†…å®¹çš„ä¸€è‡´æ€§ã€‚é€šè¿‡å¼•å…¥ RGBA-VAEã€VLD-MMDiT æ¶æ„å’Œå¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¯å˜é•¿åº¦çš„å›¾åƒåˆ†è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15182",
            "title": "Robust and Calibrated Detection of Authentic Multimedia Content",
            "url": "https://huggingface.co/papers/2512.15182",
            "abstract": "A resynthesis framework enhances deepfake detection by verifying authenticity with low false positive rates and robustness against efficient adversaries, supporting multiple modalities.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative models can synthesize highly realistic content, so-called deepfakes, that are already being misused at scale to undermine digital media authenticity. Current deepfake detection methods are unreliable for two reasons: (i) distinguishing inauthentic content post-hoc is often impossible (e.g., with memorized samples), leading to an unbounded false positive rate (FPR); and (ii) detection lacks robustness, as adversaries can adapt to known detectors with near-perfect accuracy using minimal computational resources. To address these limitations, we propose a resynthesis framework to determine if a sample is authentic or if its authenticity can be plausibly denied. We make two key contributions focusing on the high-precision, low-recall setting against efficient (i.e., compute-restricted) adversaries. First, we demonstrate that our calibrated resynthesis method is the most reliable approach for verifying authentic samples while maintaining controllable, low FPRs. Second, we show that our method achieves adversarial robustness against efficient adversaries, whereas prior methods are easily evaded under identical compute budgets. Our approach supports multiple modalities and leverages state-of-the-art inversion techniques.",
            "score": 14,
            "issue_id": 118,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "1fd93a05e5ce6729",
            "authors": [
                "Sarim Hashmi",
                "Abdelrahman Elsayed",
                "Mohammed Talha Alam",
                "Samuele Poppi",
                "Nils Lukas"
            ],
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15182.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ ĞµÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ¿Ñ„ĞµĞ¹ĞºĞ¾Ğ² Ñ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸ĞµĞ¹ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ¿Ñ„ĞµĞ¹ĞºĞ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ Ñ€ĞµÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ Ğ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑÑ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ´Ğ²Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²: Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ñ‹ Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ½Ğ¸ĞºĞ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "Enhancing Deepfake Detection with Robust Resynthesis",
                    "desc": "This paper presents a new framework for detecting deepfakes, which are realistic but fake digital content. The proposed resynthesis method improves the reliability of authenticity verification while keeping false positive rates low. It is designed to withstand attacks from adversaries who use limited computational resources to create convincing fakes. By supporting multiple modalities and utilizing advanced inversion techniques, this framework enhances the robustness of deepfake detection against evolving threats."
                },
                "zh": {
                    "title": "é‡åˆæˆæ¡†æ¶ï¼šæå‡æ·±åº¦ä¼ªé€ æ£€æµ‹çš„å¯é æ€§ä¸é²æ£’æ€§",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é‡åˆæˆæ¡†æ¶ï¼Œç”¨äºæé«˜æ·±åº¦ä¼ªé€ æ£€æµ‹çš„å‡†ç¡®æ€§ï¼Œèƒ½å¤Ÿåœ¨ä½å‡é˜³æ€§ç‡ä¸‹éªŒè¯å†…å®¹çš„çœŸå®æ€§ï¼Œå¹¶å¯¹é«˜æ•ˆå¯¹æ‰‹å…·æœ‰é²æ£’æ€§ã€‚å½“å‰çš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯åæœŸåŒºåˆ†ä¼ªé€ å†…å®¹å‡ ä¹ä¸å¯èƒ½ï¼Œå¯¼è‡´å‡é˜³æ€§ç‡æ— æ³•æ§åˆ¶ï¼›äºŒæ˜¯æ£€æµ‹æ–¹æ³•ç¼ºä¹é²æ£’æ€§ï¼Œå®¹æ˜“è¢«å¯¹æ‰‹åˆ©ç”¨æœ€å°è®¡ç®—èµ„æºè¿›è¡Œè§„é¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ ¡å‡†é‡åˆæˆæŠ€æœ¯ï¼Œèƒ½å¤Ÿå¯é åœ°éªŒè¯çœŸå®æ ·æœ¬ï¼ŒåŒæ—¶ä¿æŒå¯æ§çš„ä½å‡é˜³æ€§ç‡ã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒå¤šç§æ¨¡æ€ï¼Œå¹¶åˆ©ç”¨äº†æœ€å…ˆè¿›çš„åæ¼”æŠ€æœ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15687",
            "title": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning",
            "url": "https://huggingface.co/papers/2512.15687",
            "abstract": "G2RL, a gradient-guided reinforcement learning framework, enhances exploration in large language models by leveraging the model's own update geometry, leading to improved performance on various reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.",
            "score": 12,
            "issue_id": 117,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "f95a92d3ec882673",
            "authors": [
                "Zhenwen Liang",
                "Sidi Lu",
                "Wenhao Yu",
                "Kishan Panaganti",
                "Yujun Zhou",
                "Haitao Mi",
                "Dong Yu"
            ],
            "affiliations": [
                "Tencent AI Lab",
                "University of Notre Dame"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15687.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#small_models",
                    "#reasoning",
                    "#training",
                    "#rlhf",
                    "#optimization",
                    "#math"
                ],
                "emoji": "ğŸ§­",
                "ru": {
                    "title": "Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ²ĞµĞ´ĞºĞ°: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ°Ğ¼Ğ° ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ",
                    "desc": "G2RL â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ²ĞµĞ´ĞºÑƒ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ğº. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ½ÑƒÑĞ¾Ğ² ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚, ĞºĞ°Ğº ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ multiplicative Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ²Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ (MATH500, AMC, AIME, GPQA) G2RL Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ Ñ€Ğ°Ğ·Ğ²ĞµĞ´ĞºÑƒ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Guiding Exploration with Model's Own Geometry",
                    "desc": "G2RL is a new framework for reinforcement learning that improves how large language models explore their learning space. Instead of relying on external methods, G2RL uses the model's own update geometry to guide exploration, making it more aligned with the model's learning process. By analyzing the model's sensitivity, G2RL rewards trajectories that introduce new gradient directions while reducing emphasis on redundant updates. This approach leads to better performance on reasoning tasks, demonstrating that leveraging the model's internal structure can enhance exploration effectively."
                },
                "zh": {
                    "title": "G2RLï¼šå¼•å¯¼æ¢ç´¢çš„å¼ºåŒ–å­¦ä¹ æ–°æ¡†æ¶",
                    "desc": "G2RLæ˜¯ä¸€ç§åŸºäºæ¢¯åº¦å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ©ç”¨æ¨¡å‹è‡ªèº«çš„æ›´æ–°å‡ ä½•æ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„æ¢ç´¢æœºåˆ¶ä¸åŒï¼ŒG2RLä¸ä¾èµ–å¤–éƒ¨å¯å‘å¼æ–¹æ³•ï¼Œè€Œæ˜¯é€šè¿‡æ¨¡å‹çš„ç¬¬ä¸€é˜¶æ›´æ–°å‡ ä½•æ¥é©±åŠ¨æ¢ç´¢ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¯”è¾ƒæ ·æœ¬ç»„å†…çš„ç‰¹å¾ï¼Œå¥–åŠ±å¼•å…¥æ–°æ¢¯åº¦æ–¹å‘çš„è½¨è¿¹ï¼Œä»è€Œå®ç°è‡ªæˆ‘å‚è€ƒçš„æ¢ç´¢ä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒG2RLåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.13884",
            "title": "FiNERweb: Datasets and Artifacts for Scalable Multilingual Named Entity Recognition",
            "url": "https://huggingface.co/papers/2512.13884",
            "abstract": "Recent multilingual named entity recognition (NER) work has shown that large language models (LLMs) can provide effective synthetic supervision, yet such datasets have mostly appeared as by-products of broader experiments rather than as systematic, reusable resources. We introduce FiNERweb, a dataset-creation pipeline that scales the teacher-student paradigm to 91 languages and 25 scripts. Building on FineWeb-Edu, our approach trains regression models to identify NER-relevant passages and annotates them with multilingual LLMs, resulting in about 225k passages with 235k distinct entity labels. Our experiments show that the regression model achieves more than 84 F1, and that models trained on FiNERweb obtain comparable or improved performance in zero shot transfer settings on English, Thai, and Swahili, despite being trained on 19x less data than strong baselines. In addition, we assess annotation quality using LLM-as-a-judge and observe consistently high scores for both faithfulness (3.99 out of 5) and completeness (4.05 out of 5), indicating reliable and informative annotations. Further, we release the dataset with both English labels and translated label sets in the respective target languages because we observe that the performance of current state-of-the-art models drops by 0.02 to 0.09 F1 when evaluated using target language labels instead of English ones. We release FiNERweb together with all accompanying artifacts to the research community in order to facilitate more effective student-teacher training for multilingual named entity recognition.",
            "score": 12,
            "issue_id": 126,
            "pub_date": "2025-12-15",
            "pub_date_card": {
                "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 15",
                "zh": "12æœˆ15æ—¥"
            },
            "hash": "178425da9b8ad28c",
            "authors": [
                "Jonas Golde",
                "Patrick Haller",
                "Alan Akbik"
            ],
            "affiliations": [
                "Humboldt UniversitÃ¤t zu Berlin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.13884.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#open_source",
                    "#synthetic",
                    "#data",
                    "#transfer_learning",
                    "#multilingual",
                    "#low_resource"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ NER Ğ½Ğ° 91 ÑĞ·Ñ‹Ğº Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ FiNERweb â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° 91 ÑĞ·Ñ‹ĞºĞµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ \"ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ-ÑƒÑ‡ĞµĞ½Ğ¸Ğº\" Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ NER Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ LLM, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ² Ğ¾ĞºĞ¾Ğ»Ğ¾ 225k Ğ¾Ñ‚Ñ€Ñ‹Ğ²ĞºĞ¾Ğ² Ñ 235k ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° FiNERweb, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞµ Ğ´Ğ»Ñ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾, Ñ‚Ğ°Ğ¸Ğ»Ğ°Ğ½Ğ´ÑĞºĞ¾Ğ³Ğ¾ Ğ¸ ÑÑƒĞ°Ñ…Ğ¸Ğ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ² 19 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ¼ baseline. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ°ÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¾Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹."
                },
                "en": {
                    "title": "FiNERweb: Scalable Multilingual NER with LLMs",
                    "desc": "This paper presents FiNERweb, a novel dataset-creation pipeline designed for multilingual named entity recognition (NER) using large language models (LLMs). The approach employs a teacher-student paradigm across 91 languages and 25 scripts, generating approximately 225,000 passages annotated with 235,000 distinct entity labels. The regression model used in this pipeline achieves an impressive F1 score of over 84, demonstrating its effectiveness even with significantly less training data compared to existing baselines. Additionally, the quality of annotations is validated through LLM assessments, showing high scores for both faithfulness and completeness, making FiNERweb a valuable resource for enhancing multilingual NER tasks."
                },
                "zh": {
                    "title": "FiNERwebï¼šå¤šè¯­è¨€å‘½åå®ä½“è¯†åˆ«çš„æ–°èµ„æº",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†FiNERwebï¼Œä¸€ä¸ªç”¨äºå¤šè¯­è¨€å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ•°æ®é›†åˆ›å»ºçš„ç®¡é“ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ•™å¸ˆ-å­¦ç”ŸèŒƒå¼ï¼Œæ‰©å±•åˆ°91ç§è¯­è¨€å’Œ25ç§ä¹¦å†™ç³»ç»Ÿï¼Œç”Ÿæˆçº¦225,000ä¸ªæ®µè½å’Œ235,000ä¸ªä¸åŒçš„å®ä½“æ ‡ç­¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå›å½’æ¨¡å‹åœ¨é›¶æ ·æœ¬è¿ç§»è®¾ç½®ä¸­è¡¨ç°è‰¯å¥½ï¼Œå°½ç®¡è®­ç»ƒæ•°æ®é‡ä»…ä¸ºå¼ºåŸºçº¿çš„19å€ï¼Œä½†åœ¨è‹±è¯­ã€æ³°è¯­å’Œæ–¯ç“¦å¸Œé‡Œè¯­ä¸Šå–å¾—äº†å¯æ¯”æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æ³¨é‡Šè´¨é‡ï¼Œå‘ç°å…¶åœ¨å‡†ç¡®æ€§å’Œå®Œæ•´æ€§æ–¹é¢å‡è¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†å¯é çš„ä¿¡æ¯æ³¨é‡Šã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.10863",
            "title": "MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence",
            "url": "https://huggingface.co/papers/2512.10863",
            "abstract": "MMSI-Video-Bench is a comprehensive benchmark for video-based spatial intelligence in MLLMs, revealing significant gaps between human and AI performance and highlighting challenges in geometric reasoning, motion grounding, and cross-video correspondence.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.",
            "score": 12,
            "issue_id": 120,
            "pub_date": "2025-12-11",
            "pub_date_card": {
                "ru": "11 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 11",
                "zh": "12æœˆ11æ—¥"
            },
            "hash": "5348b8e5d1072d75",
            "authors": [
                "Jingli Lin",
                "Runsen Xu",
                "Shaohao Zhu",
                "Sihan Yang",
                "Peizhou Cao",
                "Yunlong Ran",
                "Miao Hu",
                "Chenming Zhu",
                "Yiman Xie",
                "Yilin Long",
                "Wenbo Hu",
                "Dahua Lin",
                "Tai Wang",
                "Jiangmiao Pang"
            ],
            "affiliations": [
                "Beihang University",
                "Fudan University",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University",
                "The Chinese University of Hong Kong",
                "University of California, Los Angeles",
                "University of Hong Kong",
                "Xian Jiaotong University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.10863.jpg",
            "data": {
                "categories": [
                    "#survey",
                    "#reasoning",
                    "#video",
                    "#benchmark",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ˜Ğ·Ğ¼ĞµÑ€ÑÑ Ğ¿Ñ€Ğ¾Ğ¿Ğ°ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ Ğ˜Ğ˜ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MMSI-Video-Bench â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1106 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 1278 Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° 60%, Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ²Ñ‹ÑĞ²Ğ¸Ğ» ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸, Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Bridging the Gap: Advancing AI's Spatial Intelligence in Videos",
                    "desc": "MMSI-Video-Bench is a new benchmark designed to evaluate video-based spatial intelligence in machine learning language models (MLLMs). It identifies significant performance gaps between humans and AI, particularly in areas like geometric reasoning and motion grounding. The benchmark includes a comprehensive set of 1,106 questions based on 1,278 video clips, organized into a four-level framework: Perception, Planning, Prediction, and Cross-Video Reasoning. The findings reveal that many MLLMs struggle with these tasks, highlighting the need for improved models that can better understand and reason about spatial information in videos."
                },
                "zh": {
                    "title": "è§†é¢‘åŸºç¡€ç©ºé—´æ™ºèƒ½çš„å…¨é¢è¯„ä¼°",
                    "desc": "MMSI-Video-Benchæ˜¯ä¸€ä¸ªå…¨é¢çš„è§†é¢‘åŸºç¡€ç©ºé—´æ™ºèƒ½åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æœºå™¨å­¦ä¹ è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç©ºé—´ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†æ­ç¤ºäº†äººç±»ä¸äººå·¥æ™ºèƒ½ä¹‹é—´çš„æ˜¾è‘—å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡ ä½•æ¨ç†ã€è¿åŠ¨å®šä½å’Œè·¨è§†é¢‘å¯¹åº”ç­‰æ–¹é¢ã€‚é€šè¿‡1,106ä¸ªé—®é¢˜å’Œ1,278ä¸ªè§†é¢‘ç‰‡æ®µï¼ŒMMSI-Video-Benchæä¾›äº†ä¸€ä¸ªå››çº§æ¡†æ¶ï¼Œæ¶µç›–æ„ŸçŸ¥ã€è§„åˆ’ã€é¢„æµ‹å’Œè·¨è§†é¢‘æ¨ç†ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œè®¸å¤šæ¨¡å‹çš„è¡¨ç°æ¥è¿‘éšæœºï¼Œè€Œæœ€ä½³æ¨ç†æ¨¡å‹çš„è¡¨ç°æ¯”äººç±»ä½è¿‘60%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15713",
            "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
            "url": "https://huggingface.co/papers/2512.15713",
            "abstract": "DiffusionVL, a family of diffusion vision language models derived from autoregressive models through fine-tuning, achieves performance improvements and faster inference speeds compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.",
            "score": 10,
            "issue_id": 118,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "33f3d5e9017750ad",
            "authors": [
                "Lunbin Zeng",
                "Jingfeng Yao",
                "Bencheng Liao",
                "Hongyuan Tao",
                "Wenyu Liu",
                "Xinggang Wang"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15713.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#diffusion",
                    "#training",
                    "#open_source",
                    "#architecture",
                    "#inference"
                ],
                "emoji": "âš¡",
                "ru": {
                    "title": "ĞÑ‚ Ğ°Ğ²Ñ‚Ğ°Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸: Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° DiffusionVL â€” ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ°Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ°Ğ²Ñ‚Ğ°Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ AR Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ â€” Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹, Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ KV-ĞºĞµÑˆĞ° Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ DiffusionVL Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹: Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ½Ğ° 34.4% Ğ¿Ğ¾ MMMU-Pro Ğ¸ 37.5% Ğ¿Ğ¾ MME Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµĞ½ĞµĞµ 5% Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Transforming Autoregressive Power into Diffusion Efficiency",
                    "desc": "DiffusionVL is a new family of diffusion vision language models that improves performance and speeds up inference by fine-tuning existing autoregressive models. This research explores the potential of transforming powerful autoregressive models into diffusion models, addressing the limitations of traditional diffusion language models. The study demonstrates that this paradigm shift not only enhances performance but also allows for efficient generation through a novel block-decoding design. With significantly less training data, DiffusionVL achieves notable gains in benchmark tests while doubling inference speed compared to previous methods."
                },
                "zh": {
                    "title": "æ‰©æ•£æ¨¡å‹çš„åˆ›æ–°çªç ´",
                    "desc": "DiffusionVLæ˜¯ä¸€ç§åŸºäºè‡ªå›å½’æ¨¡å‹å¾®è°ƒçš„æ‰©æ•£è§†è§‰è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œå…·æœ‰æ›´å¥½çš„æ€§èƒ½å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚è¯¥æ¨¡å‹é€šè¿‡ç®€å•çš„å¾®è°ƒï¼Œå°†å¼ºå¤§çš„è‡ªå›å½’é¢„è®­ç»ƒæ¨¡å‹è½¬åŒ–ä¸ºæ‰©æ•£æ¨¡å‹ï¼Œå±•ç¤ºäº†ä»è‡ªå›å½’åˆ°æ‰©æ•£çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffusionVLåœ¨æ•°æ®ä½¿ç”¨é‡å°‘äº5%çš„æƒ…å†µä¸‹ï¼Œä»èƒ½åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¯¥æ¨¡å‹çš„è®¾è®¡è¿˜å¼•å…¥äº†å—è§£ç ï¼Œæ”¯æŒä»»æ„é•¿åº¦çš„ç”Ÿæˆå’ŒKVç¼“å­˜é‡ç”¨ï¼Œè¿›ä¸€æ­¥åŠ å¿«äº†æ¨ç†é€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.13874",
            "title": "SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning",
            "url": "https://huggingface.co/papers/2512.13874",
            "abstract": "The paper proposes SAGE, a multi-turn reasoning system for video that mimics human behavior, using synthetic data and reinforcement learning to improve performance on long videos.  \t\t\t\t\tAI-generated summary \t\t\t\t As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes.",
            "score": 7,
            "issue_id": 118,
            "pub_date": "2025-12-15",
            "pub_date_card": {
                "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 15",
                "zh": "12æœˆ15æ—¥"
            },
            "hash": "a5f255ef0889a270",
            "authors": [
                "Jitesh Jain",
                "Jialuo Li",
                "Zixian Ma",
                "Jieyu Zhang",
                "Chris Dongjoo Kim",
                "Sangho Lee",
                "Rohun Tripathi",
                "Tanmay Gupta",
                "Christopher Clark",
                "Humphrey Shi"
            ],
            "affiliations": [
                "Allen AI",
                "SHI Labs @ Georgia Tech",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.13874.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#agents",
                    "#reasoning",
                    "#dataset",
                    "#training",
                    "#open_source",
                    "#synthetic",
                    "#long_context",
                    "#rl",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° SAGE Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Gemini 2.5, Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ° SAGE-MM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾ Ğ¸Ğ»Ğ¸ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ·. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ° 6.1% Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ½Ğ° 8.2% Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ²Ñ‹ÑˆĞµ 10 Ğ¼Ğ¸Ğ½ÑƒÑ‚. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SAGE-Bench Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ±Ğ¾Ğ»ĞµĞµ 700 ÑĞµĞºÑƒĞ½Ğ´, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰ĞµĞ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
                },
                "en": {
                    "title": "SAGE: Human-like Multi-Turn Reasoning for Video Analysis",
                    "desc": "The paper introduces SAGE, a multi-turn reasoning system designed for video analysis that emulates human-like decision-making. It utilizes synthetic data and reinforcement learning to enhance performance on lengthy videos, allowing the model to process information in a more flexible manner. Unlike traditional models that require processing all frames in one go, SAGE can handle simpler tasks in a single turn while engaging in multi-turn reasoning for complex scenarios. The authors validate their approach through SAGE-Bench, demonstrating significant improvements in video reasoning tasks, particularly for longer videos."
                },
                "zh": {
                    "title": "SAGEï¼šæ¨¡ä»¿äººç±»çš„å¤šè½®è§†é¢‘æ¨ç†ç³»ç»Ÿ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSAGEçš„å¤šè½®æ¨ç†ç³»ç»Ÿï¼Œæ—¨åœ¨æ¨¡ä»¿äººç±»åœ¨è§‚çœ‹è§†é¢‘æ—¶çš„è¡Œä¸ºã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨åˆæˆæ•°æ®å’Œå¼ºåŒ–å­¦ä¹ æ¥æå‡å¯¹é•¿è§†é¢‘çš„æ¨ç†æ€§èƒ½ã€‚ä¸ç°æœ‰æ¨¡å‹ä¸åŒï¼ŒSAGEèƒ½å¤Ÿåœ¨å¤„ç†å¤æ‚é—®é¢˜æ—¶è¿›è¡Œå¤šè½®æ¨ç†ï¼Œè€Œåœ¨ç®€å•é—®é¢˜ä¸Šåˆ™é‡‡ç”¨å•è½®æ¨ç†ã€‚é€šè¿‡å®éªŒè¯æ˜ï¼ŒSAGEåœ¨å¼€æ”¾å¼è§†é¢‘æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†è¶…è¿‡10åˆ†é’Ÿçš„è§†é¢‘æ—¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15702",
            "title": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
            "url": "https://huggingface.co/papers/2512.15702",
            "abstract": "Resampling Forcing is introduced as a teacher-free framework to train autoregressive video diffusion models with improved temporal consistency using self-resampling and history routing.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.",
            "score": 6,
            "issue_id": 118,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "0b35f4e888290db1",
            "authors": [
                "Yuwei Guo",
                "Ceyuan Yang",
                "Hao He",
                "Yang Zhao",
                "Meng Wei",
                "Zhenheng Yang",
                "Weilin Huang",
                "Dahua Lin"
            ],
            "affiliations": [
                "ByteDance",
                "ByteDance Seed",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15702.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#video",
                    "#training"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ‘ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğº ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¿ĞµÑ€ĞµÑƒÑ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ğ¿ĞµÑ€ĞµĞ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Resampling Forcing â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞ°Ğ¼Ğ¾Ğ¿ĞµÑ€ĞµĞ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ history routing, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Teacher-Free Training for Consistent Video Generation",
                    "desc": "This paper presents Resampling Forcing, a novel framework for training autoregressive video diffusion models without the need for a teacher model. The method addresses exposure bias by using self-resampling to mimic model errors during training, which helps improve temporal consistency in generated videos. Additionally, a sparse causal mask is employed to maintain temporal causality while allowing for efficient parallel training. The introduction of history routing further enhances the model's ability to generate long videos by dynamically selecting the most relevant past frames for each new frame being generated."
                },
                "zh": {
                    "title": "æ— æ•™å¸ˆæ¡†æ¶æå‡è§†é¢‘ç”Ÿæˆä¸€è‡´æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé‡é‡‡æ ·å¼ºåˆ¶ï¼ˆResampling Forcingï¼‰çš„æ— æ•™å¸ˆæ¡†æ¶ï¼Œç”¨äºè®­ç»ƒè‡ªå›å½’è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œä»¥æé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªæˆ‘é‡é‡‡æ ·å’Œå†å²è·¯ç”±æŠ€æœ¯ï¼Œè§£å†³äº†è®­ç»ƒå’Œæµ‹è¯•ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ï¼Œé¿å…äº†ä¾èµ–åŒå‘æ•™å¸ˆæ¨¡å‹æˆ–åœ¨çº¿åˆ¤åˆ«å™¨ã€‚é‡é‡‡æ ·å¼ºåˆ¶çš„æ ¸å¿ƒæ˜¯è‡ªæˆ‘é‡é‡‡æ ·æ–¹æ¡ˆï¼Œå®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡æ‹Ÿæ¨ç†æ—¶æ¨¡å‹åœ¨å†å²å¸§ä¸Šçš„é”™è¯¯ã€‚é€šè¿‡ç¨€ç–å› æœæ©ç å’Œå†å²è·¯ç”±æœºåˆ¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œé•¿æ—¶é—´åºåˆ—ç”Ÿæˆï¼Œå¹¶åœ¨é•¿è§†é¢‘ä¸Šè¡¨ç°å‡ºä¼˜è¶Šçš„æ—¶é—´ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.09299",
            "title": "VABench: A Comprehensive Benchmark for Audio-Video Generation",
            "url": "https://huggingface.co/papers/2512.09299",
            "abstract": "VABench is a benchmark framework for evaluating audio-video generation models, covering text-to-audio-video, image-to-audio-video, and stereo audio-video tasks with 15 evaluation dimensions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field.",
            "score": 6,
            "issue_id": 117,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "2d3ec74bffc0d2c1",
            "authors": [
                "Daili Hua",
                "Xizhi Wang",
                "Bohan Zeng",
                "Xinyi Huang",
                "Hao Liang",
                "Junbo Niu",
                "Xinlong Chen",
                "Quanqing Xu",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Ant Group",
                "Huazhong University of Science and Technology",
                "Institute of Automation, Chinese Academy of Sciences",
                "Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.09299.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#audio",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°",
                    "desc": "VABench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ‚ĞµÑ€ĞµĞ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 15 Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾, ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ° Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³ÑƒĞ±. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞµĞ¼ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°: Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ñ‹Ğµ, Ğ·Ğ²ÑƒĞºĞ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ¼ÑƒĞ·Ñ‹ĞºÑƒ, Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰Ğ¸Ğµ Ğ·Ğ²ÑƒĞºĞ¸, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ²ÑƒĞºĞ¸, ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¸Ñ€Ñ‹. VABench ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "VABench: Setting the Standard for Audio-Video Generation Evaluation",
                    "desc": "VABench is a new framework designed to evaluate models that generate audio and video together, focusing on their ability to synchronize these elements. It includes three main tasks: generating audio-video from text, from images, and stereo audio-video generation. The framework assesses performance across 15 different dimensions, such as how well the audio matches the video and how accurately lip movements correspond to speech. By providing a structured way to analyze these models, VABench aims to set a new standard for evaluating audio-video generation in machine learning."
                },
                "zh": {
                    "title": "éŸ³è§†é¢‘ç”Ÿæˆçš„æ–°æ ‡å‡†",
                    "desc": "VABenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°éŸ³è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„åŸºå‡†æ¡†æ¶ï¼Œæ¶µç›–äº†æ–‡æœ¬åˆ°éŸ³è§†é¢‘ã€å›¾åƒåˆ°éŸ³è§†é¢‘å’Œç«‹ä½“éŸ³è§†é¢‘ç­‰ä»»åŠ¡ï¼Œè®¾æœ‰15ä¸ªè¯„ä¼°ç»´åº¦ã€‚è¯¥æ¡†æ¶æ—¨åœ¨å¡«è¡¥ç°æœ‰è§†é¢‘ç”ŸæˆåŸºå‡†åœ¨éŸ³è§†é¢‘ç”Ÿæˆè¯„ä¼°æ–¹é¢çš„ç©ºç™½ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åŒæ­¥éŸ³è§†é¢‘è¾“å‡ºçš„æ¨¡å‹ã€‚VABenché€šè¿‡è¯„ä¼°æ–‡æœ¬ä¸è§†é¢‘ã€æ–‡æœ¬ä¸éŸ³é¢‘ã€è§†é¢‘ä¸éŸ³é¢‘ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä»¥åŠéŸ³è§†é¢‘åŒæ­¥å’Œå”‡è¯­ä¸€è‡´æ€§ç­‰æŒ‡æ ‡ï¼Œæä¾›äº†å…¨é¢çš„è¯„ä¼°æ¨¡å—ã€‚è¯¥æ¡†æ¶è¿˜æ¶µç›–äº†åŠ¨ç‰©ã€äººå£°ã€éŸ³ä¹ç­‰ä¸ƒå¤§å†…å®¹ç±»åˆ«ï¼Œæ—¨åœ¨ä¸ºéŸ³è§†é¢‘ç”Ÿæˆæ¨¡å‹å»ºç«‹æ–°çš„è¯„ä¼°æ ‡å‡†ï¼Œæ¨åŠ¨è¯¥é¢†åŸŸçš„å…¨é¢å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15649",
            "title": "VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?",
            "url": "https://huggingface.co/papers/2512.15649",
            "abstract": "A benchmark evaluates the performance of vision-language models on understanding long-context information compressed into dense visual representations, revealing significant limitations in capturing long-term dependencies.  \t\t\t\t\tAI-generated summary \t\t\t\t The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.",
            "score": 5,
            "issue_id": 117,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "ce63fddb0561a4e7",
            "authors": [
                "Hongbo Zhao",
                "Meng Wang",
                "Fei Zhu",
                "Wenzhuo Liu",
                "Bolin Ni",
                "Fanhu Zeng",
                "Gaofeng Meng",
                "Zhaoxiang Zhang"
            ],
            "affiliations": [
                "Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation, CAS",
                "Institute of Automation, Chinese Academy of Sciences",
                "School of Artificial Intelligence, University of Chinese Academy of Sciences",
                "Tencent Hunyuan Team"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15649.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multimodal",
                    "#cv"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "VLM Ğ½Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚: Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, ÑĞ¶Ğ°Ñ‚Ñ‹Ğ¹ Ğ² Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° vision-text compression. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒÑÑ‚ VLM Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ ÑĞ¾ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ñ…Ğ¾Ñ€Ğ¾ÑˆĞµĞµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ¼ĞµÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ½Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒĞ´Ğ°Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‡Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ VLM Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ ÑĞ¾ ÑĞ¶Ğ°Ñ‚Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking Long-Context Understanding in Vision-Language Models",
                    "desc": "This paper introduces a benchmark to evaluate how well vision-language models (VLMs) understand long-context information that has been compressed into dense visual formats. It highlights the limitations of these models in capturing long-term dependencies, especially when using vision-text compression (VTC) techniques. The study assesses VLM performance across three tasks: retrieving information, reasoning with minimal text overlap, and answering questions based on long-term memory. The findings reveal that while VLMs can decode text effectively, they struggle with understanding long-context relationships in VTC-compressed data, indicating a need for improved model designs."
                },
                "zh": {
                    "title": "æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›",
                    "desc": "æœ¬ç ”ç©¶è¯„ä¼°äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç†è§£å‹ç¼©ä¸ºå¯†é›†è§†è§‰è¡¨ç¤ºçš„é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢çš„è¡¨ç°ï¼Œæ­ç¤ºäº†å…¶åœ¨æ•æ‰é•¿æœŸä¾èµ–æ€§æ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ã€‚æˆ‘ä»¬æå‡ºäº†è§†è§‰æ–‡æœ¬å‹ç¼©ï¼ˆVTCï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å°†é•¿æ–‡æœ¬è½¬æ¢ä¸ºå¯†é›†çš„äºŒç»´è§†è§‰è¡¨ç¤ºï¼Œæ˜¾è‘—æé«˜äº†ä¿¡æ¯å¯†åº¦ã€‚å°½ç®¡å¤§å¤šæ•°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§£ç æ–‡æœ¬ä¿¡æ¯æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†VTCå‹ç¼©ä¿¡æ¯æ—¶ï¼Œå®ƒä»¬çš„é•¿ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›å´å‡ºä¹æ„æ–™åœ°å·®ã€‚è¯¥ç ”ç©¶ä¸ºè®¾è®¡æ›´é«˜æ•ˆã€å¯æ‰©å±•çš„è§†è§‰è¯­è¨€æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15110",
            "title": "Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets",
            "url": "https://huggingface.co/papers/2512.15110",
            "abstract": "Nano Banana Pro excels in subjective visual quality across low-level vision tasks without fine-tuning but struggles with traditional reference-based quantitative metrics due to generative model stochasticity.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro a Low-Level Vision All-Rounder? We conducted a comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals a distinct performance dichotomy: while Nano Banana Pro demonstrates superior subjective visual quality, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional reference-based quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as a capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains a significant hurdle.",
            "score": 5,
            "issue_id": 118,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "82d5083e03c43a72",
            "authors": [
                "Jialong Zuo",
                "Haoyou Deng",
                "Hanyu Zhou",
                "Jiaxin Zhu",
                "Yicheng Zhang",
                "Yiwei Zhang",
                "Yongxin Yan",
                "Kaixing Huang",
                "Weisen Chen",
                "Yongtai Deng",
                "Rui Jin",
                "Nong Sang",
                "Changxin Gao"
            ],
            "affiliations": [
                "National Key Laboratory of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15110.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#open_source",
                    "#hallucinations"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Nano Banana Pro Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ½Ğ° 14 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ 40 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸, Ğ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ĞµÑ‚ Ğ¿Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¸Ğ·-Ğ·Ğ° ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ zero-shot Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ, Ğ½Ğ¾ Ğ¿Ğ¾ĞºĞ° ÑƒÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ."
                },
                "en": {
                    "title": "Nano Banana Pro: A Promising Yet Inconsistent Low-Level Vision Contender",
                    "desc": "This paper evaluates the performance of Nano Banana Pro, a text-to-image generation model, in low-level vision tasks without any fine-tuning. The study reveals that while Nano Banana Pro excels in subjective visual quality, producing impressive high-frequency details, it does not perform as well on traditional quantitative metrics due to the stochastic nature of generative models. The authors conducted a zero-shot evaluation across 14 low-level vision tasks and found that Nano Banana Pro can compete with specialist models in some aspects but struggles with pixel-level consistency. Overall, the findings suggest that while Nano Banana Pro shows promise, it still faces challenges in achieving the precision of dedicated low-level vision models."
                },
                "zh": {
                    "title": "Nano Banana Proï¼šä½çº§è§†è§‰ä»»åŠ¡çš„æ–°é€‰æ‹©",
                    "desc": "Nano Banana Proåœ¨ä½çº§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨ä¸»è§‚è§†è§‰è´¨é‡æ–¹é¢ï¼Œæ— éœ€å¾®è°ƒå³å¯å®ç°è‰¯å¥½æ•ˆæœã€‚ç„¶è€Œï¼Œç”±äºç”Ÿæˆæ¨¡å‹çš„éšæœºæ€§ï¼Œå®ƒåœ¨ä¼ ç»Ÿçš„åŸºäºå‚è€ƒçš„å®šé‡æŒ‡æ ‡ä¸Šè¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬å¯¹14ä¸ªä¸åŒçš„ä½çº§ä»»åŠ¡è¿›è¡Œäº†å…¨é¢çš„é›¶-shotè¯„ä¼°ï¼Œå‘ç°Nano Banana Proåœ¨ä¸»è§‚è´¨é‡ä¸Šä¼˜äºä¸“ä¸šæ¨¡å‹ï¼Œä½†åœ¨ä¸¥æ ¼çš„åƒç´ çº§ä¸€è‡´æ€§æ–¹é¢ä»æœ‰ä¸è¶³ã€‚æ€»ä½“è€Œè¨€ï¼ŒNano Banana Proè¢«è®¤ä¸ºæ˜¯ä½çº§è§†è§‰ä»»åŠ¡çš„æœ‰åŠ›ç«äº‰è€…ï¼Œä½†è¦è¾¾åˆ°é¢†åŸŸä¸“å®¶çš„é«˜ä¿çœŸåº¦ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.13190",
            "title": "WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory",
            "url": "https://huggingface.co/papers/2512.13190",
            "abstract": "A novel deep learning architecture, WAY, uses nested sequence structures and spatial grids for accurate long-term vessel destination estimation from AIS data, incorporating CASP blocks and Gradient Dropout for improved performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.",
            "score": 5,
            "issue_id": 120,
            "pub_date": "2025-12-15",
            "pub_date_card": {
                "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 15",
                "zh": "12æœˆ15æ—¥"
            },
            "hash": "c9720e9dbb34a3c8",
            "authors": [
                "Jin Sob Kim",
                "Hyun Joon Park",
                "Wooseok Shin",
                "Dongil Park",
                "Sung Won Han"
            ],
            "affiliations": [
                "Korea University, Seoul 02841, Republic of Korea",
                "SeaVantage, Seoul 06119, Republic of Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.13190.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸš¢",
                "ru": {
                    "title": "ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¾Ğ² ÑÑƒĞ´Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ WAY Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒĞ´Ğ½Ğ° Ğ¿Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ AIS Ğ½Ğ° ÑÑ€Ğ¾Ğº Ğ¾Ñ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ´Ğ½ĞµĞ¹ Ğ´Ğ¾ Ğ½ĞµĞ´ĞµĞ»ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ ÑÑƒĞ´Ğ¾Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ² Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞµÑ‚ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¸ Channel-Aggregative Sequential Processing (CASP), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Gradient Dropout Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑ‰Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "WAY: Revolutionizing Vessel Destination Estimation with Deep Learning",
                    "desc": "The paper presents a new deep learning model called WAY, which is designed to improve the accuracy of predicting vessel destinations using data from the Automatic Identification System (AIS). It introduces a unique structure that organizes long maritime trajectories into nested sequences, allowing for better handling of spatio-temporal data. The model incorporates advanced techniques like Channel-Aggregative Sequential Processing (CASP) blocks and a specialized Gradient Dropout method to enhance learning and reduce bias. Experiments show that WAY outperforms traditional methods, making it a promising tool for real-world maritime applications such as estimated time of arrival (ETA) predictions."
                },
                "zh": {
                    "title": "WAYï¼šç²¾å‡†çš„èˆ¹èˆ¶ç›®çš„åœ°ä¼°è®¡æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ·±åº¦å­¦ä¹ æ¶æ„WAYï¼Œæ—¨åœ¨ä»AISæ•°æ®ä¸­å‡†ç¡®ä¼°è®¡èˆ¹èˆ¶çš„é•¿æœŸç›®çš„åœ°ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ¸¯å£é—´çš„è½¨è¿¹é‡æ„ä¸ºåµŒå¥—åºåˆ—ç»“æ„ï¼Œå¹¶ç»“åˆç©ºé—´ç½‘æ ¼ï¼Œè§£å†³äº†æ—¶ç©ºåå·®é—®é¢˜ã€‚WAYæ¶æ„åŒ…æ‹¬è½¨è¿¹è¡¨ç¤ºå±‚å’Œé€šé“èšåˆåºåˆ—å¤„ç†ï¼ˆCASPï¼‰æ¨¡å—ï¼Œåˆ©ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œä¿¡æ¯èšåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWAYåœ¨å¤„ç†AISæ•°æ®æ—¶ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå¹¶ä¸”å¼•å…¥çš„æ¢¯åº¦ä¸¢å¼ƒæŠ€æœ¯è¿›ä¸€æ­¥æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15715",
            "title": "In Pursuit of Pixel Supervision for Visual Pre-training",
            "url": "https://huggingface.co/papers/2512.15715",
            "abstract": "Pixio, an enhanced masked autoencoder, demonstrates competitive performance across various downstream tasks using pixel-space self-supervised learning, outperforming latent-space approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed \"Pixio\", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.",
            "score": 4,
            "issue_id": 118,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "b55eaf95d77d8b4a",
            "authors": [
                "Lihe Yang",
                "Shang-Wen Li",
                "Yang Li",
                "Xinjie Lei",
                "Dong Wang",
                "Abdelrahman Mohamed",
                "Hengshuang Zhao",
                "Hu Xu"
            ],
            "affiliations": [
                "FAIR, Meta",
                "HKU"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15715.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#training",
                    "#cv",
                    "#architecture",
                    "#robotics",
                    "#3d"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞÑ‚ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğº Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼: Ğ¼Ğ¾Ñ‰ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ² ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Pixio â€” ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ° Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Pixio: Revolutionizing Pixel-Space Self-Supervised Learning",
                    "desc": "Pixio is an advanced masked autoencoder that excels in self-supervised learning directly from pixel data. It outperforms traditional latent-space methods by effectively learning representations that are useful for various tasks like depth estimation and semantic segmentation. The model is trained on a massive dataset of 2 billion images with minimal human intervention, showcasing its efficiency and scalability. Overall, Pixio demonstrates that pixel-space learning remains a strong contender in the field of machine learning, providing robust performance across multiple applications."
                },
                "zh": {
                    "title": "Pixioï¼šåƒç´ ç©ºé—´è‡ªç›‘ç£å­¦ä¹ çš„æ–°é€‰æ‹©",
                    "desc": "Pixioæ˜¯ä¸€ç§å¢å¼ºå‹çš„æ©è”½è‡ªç¼–ç å™¨ï¼Œå±•ç¤ºäº†åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚å®ƒé€šè¿‡åƒç´ ç©ºé—´çš„è‡ªç›‘ç£å­¦ä¹ ï¼Œè¶…è¶Šäº†æ½œåœ¨ç©ºé—´çš„æ–¹æ³•ã€‚è¯¥æ¨¡å‹åœ¨2äº¿å¼ ç½‘ç»œçˆ¬å–çš„å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨äº†æ›´å…·æŒ‘æˆ˜æ€§çš„é¢„è®­ç»ƒä»»åŠ¡å’Œæ›´å¼ºå¤§çš„æ¶æ„ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œåƒç´ ç©ºé—´çš„è‡ªç›‘ç£å­¦ä¹ å¯ä»¥ä½œä¸ºæ½œåœ¨ç©ºé—´æ–¹æ³•çš„æœ‰åŠ›æ›¿ä»£å’Œè¡¥å……ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15374",
            "title": "SCOPE: Prompt Evolution for Enhancing Agent Effectiveness",
            "url": "https://huggingface.co/papers/2512.15374",
            "abstract": "SCOPE enhances LLM agents' context management through prompt evolution, improving task success rates in dynamic environments without human intervention.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce SCOPE (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an online optimization problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\\% to 38.64\\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.",
            "score": 3,
            "issue_id": 117,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "2b8eded0bfaa66d6",
            "authors": [
                "Zehua Pei",
                "Hui-Ling Zhen",
                "Shixiong Kai",
                "Sinno Jialin Pan",
                "Yunhe Wang",
                "Mingxuan Yuan",
                "Bei Yu"
            ],
            "affiliations": [
                "Noahs Ark Lab, Huawei, Hong Kong SAR",
                "The Chinese University of Hong Kong, Hong Kong SAR"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15374.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#open_source",
                    "#long_context"
                ],
                "emoji": "ğŸ”„",
                "ru": {
                    "title": "Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SCOPE â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ñ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ‚Ñ€Ğ°ÑÑĞ¸Ñ€Ğ¾Ğ²Ğ¾Ğº Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ğ±Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ SCOPE ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ 14,23% Ğ´Ğ¾ 38,64% Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Evolving Prompts for Smarter Context Management in LLMs",
                    "desc": "SCOPE is a method designed to improve how Large Language Model (LLM) agents handle changing contexts in their tasks. It does this by evolving the prompts used by the agents, allowing them to adapt to new information without needing human help. The approach treats context management as an optimization problem, using past execution data to refine prompts over time. By balancing immediate problem-solving with long-term strategy development, SCOPE significantly boosts the success rates of LLM agents in dynamic environments."
                },
                "zh": {
                    "title": "SCOPEï¼šæ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†çš„è‡ªæˆ‘æ¼”å˜",
                    "desc": "SCOPEæ˜¯ä¸€ç§é€šè¿‡æç¤ºæ¼”å˜æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„ä¸Šä¸‹æ–‡ç®¡ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚å®ƒå°†ä¸Šä¸‹æ–‡ç®¡ç†è§†ä¸ºä¸€ä¸ªåœ¨çº¿ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡æ‰§è¡Œè½¨è¿¹åˆæˆæŒ‡å¯¼æ–¹é’ˆï¼Œè‡ªåŠ¨æ¼”å˜ä»£ç†çš„æç¤ºã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŒæµæœºåˆ¶ï¼Œå¹³è¡¡æˆ˜æœ¯ç‰¹å¼‚æ€§å’Œæˆ˜ç•¥æ™®éæ€§ï¼Œä»¥æé«˜ä»»åŠ¡æˆåŠŸç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSCOPEåœ¨æ²¡æœ‰äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹ï¼Œä»»åŠ¡æˆåŠŸç‡ä»14.23%æé«˜åˆ°38.64%ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.14202",
            "title": "Understanding and Improving Hyperbolic Deep Reinforcement Learning",
            "url": "https://huggingface.co/papers/2512.14202",
            "abstract": "Hyper++ is a hyperbolic deep RL agent that improves stability and performance by addressing gradient issues and norm constraints in hyperbolic feature spaces.  \t\t\t\t\tAI-generated summary \t\t\t\t The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the PoincarÃ© Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .",
            "score": 3,
            "issue_id": 126,
            "pub_date": "2025-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "26fc34c5aa88efa0",
            "authors": [
                "Timo Klein",
                "Thomas Lang",
                "Andrii Shkabrii",
                "Alexander Sturm",
                "Kevin Sidak",
                "Lukas Miklautz",
                "Claudia Plant",
                "Yllka Velaj",
                "Sebastian Tschiatschek"
            ],
            "affiliations": [
                "Department of Machine Learning and Systems Biology, Max Planck Institute of Biochemistry",
                "Doctoral School Computer Science, University of Vienna",
                "Faculty of Computer Science, University of Vienna",
                "ds:UniVie, University of Vienna"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.14202.jpg",
            "data": {
                "categories": [],
                "emoji": "â›°ï¸",
                "ru": {
                    "title": "Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ³Ğ¸Ğ¿ĞµÑ€Ğ±Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ…",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Hyper++, Ğ°Ğ³ĞµĞ½Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ² Ğ³Ğ¸Ğ¿ĞµÑ€Ğ±Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ³Ğ¸Ğ¿ĞµÑ€Ğ±Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ…: Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ½Ğ¾Ñ€Ğ¼Ñ‹ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ÑĞ¿ÑƒÑĞº Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ Ğ² PPO. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ±Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ»Ğ¾Ñ‘Ğ² ÑĞµÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ProcGen Ğ¸ Atari Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 30 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Hyper++: Stability and Performance in Hyperbolic Deep RL",
                    "desc": "Hyper++ is a novel deep reinforcement learning (RL) agent that enhances stability and performance by tackling gradient issues in hyperbolic feature spaces. These spaces are effective for representing complex hierarchical structures in RL tasks, but they pose optimization challenges due to their nonstationarity. The paper identifies critical factors affecting the training of hyperbolic agents and introduces solutions such as stable critic training, feature regularization, and improved hyperbolic network formulations. Experimental results demonstrate that Hyper++ not only stabilizes learning but also significantly outperforms existing hyperbolic agents and reduces training time."
                },
                "zh": {
                    "title": "Hyper++ï¼šæå‡è¶…æ›²ç‡æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ç¨³å®šæ€§ä¸æ€§èƒ½",
                    "desc": "Hyper++æ˜¯ä¸€ç§è¶…æ›²ç‡æ·±åº¦å¼ºåŒ–å­¦ä¹ ä»£ç†ï¼Œé€šè¿‡è§£å†³è¶…æ›²ç‡ç‰¹å¾ç©ºé—´ä¸­çš„æ¢¯åº¦é—®é¢˜å’ŒèŒƒæ•°çº¦æŸï¼Œæé«˜äº†ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚è¶…æ›²ç‡ç‰¹å¾ç©ºé—´èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å¤æ‚å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­çš„å±‚æ¬¡å’Œå…³ç³»ç»“æ„ï¼Œä½†åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡åˆ†æäº†è¶…æ›²ç‡å‡ ä½•ä¸­PoincarÃ©çƒå’Œè¶…æ›²é¢æ¨¡å‹çš„æ ¸å¿ƒæ“ä½œæ¢¯åº¦ï¼Œå‘ç°å¤§èŒƒæ•°åµŒå…¥ä¼šå¯¼è‡´åŸºäºæ¢¯åº¦çš„è®­ç»ƒä¸ç¨³å®šã€‚åŸºäºè¿™äº›å‘ç°ï¼ŒHyper++å¼•å…¥äº†ç¨³å®šçš„è¯„è®ºå‘˜è®­ç»ƒã€ç‰¹å¾æ­£åˆ™åŒ–å’Œä¼˜åŒ–å‹å¥½çš„è¶…æ›²ç‡ç½‘ç»œå±‚çš„ä¸‰å¤§ç»„ä»¶ï¼Œæ˜¾è‘—æå‡äº†å­¦ä¹ çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.12072",
            "title": "VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs",
            "url": "https://huggingface.co/papers/2512.12072",
            "abstract": "Voyager is a method that uses determinantal point processes to iteratively generate diverse synthetic datasets for model evaluation and training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.",
            "score": 3,
            "issue_id": 121,
            "pub_date": "2025-12-12",
            "pub_date_card": {
                "ru": "12 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 12",
                "zh": "12æœˆ12æ—¥"
            },
            "hash": "b763fa6083309058",
            "authors": [
                "Avinash Amballa",
                "Yashas Malur Saidutta",
                "Chi-Heng Lin",
                "Vivek Kulkarni",
                "Srinivas Chappidi"
            ],
            "affiliations": [
                "Samsung Research America"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.12072.jpg",
            "data": {
                "categories": [],
                "emoji": "ğŸ—ºï¸",
                "ru": {
                    "title": "Ğ Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹",
                    "desc": "Voyager â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼ĞµÑ€Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Voyager Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ² 1.5-3 Ñ€Ğ°Ğ·Ğ°."
                },
                "en": {
                    "title": "Voyager: Enhancing Dataset Diversity with Determinantal Point Processes",
                    "desc": "Voyager is a new method that enhances the generation of synthetic datasets for training machine learning models by ensuring greater diversity. It uses determinantal point processes to iteratively create datasets that are not only varied but also optimized for model evaluation. This approach does not require any training and can be applied to models that are not open-source, making it versatile and scalable. Experimental results show that Voyager significantly improves dataset diversity compared to existing methods, achieving a 1.5-3x increase in diversity metrics."
                },
                "zh": {
                    "title": "Voyagerï¼šç”Ÿæˆå¤šæ ·åŒ–åˆæˆæ•°æ®é›†çš„æ–°æ–¹æ³•",
                    "desc": "Voyageræ˜¯ä¸€ç§åˆ©ç”¨è¡Œåˆ—å¼ç‚¹è¿‡ç¨‹è¿­ä»£ç”Ÿæˆå¤šæ ·åŒ–åˆæˆæ•°æ®é›†çš„æ–¹æ³•ï¼Œæ—¨åœ¨ç”¨äºæ¨¡å‹è¯„ä¼°å’Œè®­ç»ƒã€‚ä»¥å¾€çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ•°æ®ç¼ºä¹å¤šæ ·æ€§ï¼Œè€ŒVoyageré€šè¿‡ä¼˜åŒ–æ•°å­¦é‡æ¥ç›´æ¥æå‡æ•°æ®é›†çš„å¤šæ ·æ€§ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦è®­ç»ƒï¼Œé€‚ç”¨äºå°é—­æºæ¨¡å‹ï¼Œå¹¶ä¸”å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVoyageråœ¨å¤šæ ·æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºæµè¡Œçš„åŸºçº¿æ–¹æ³•ï¼Œæå‡å¹…åº¦è¾¾åˆ°1.5åˆ°3å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.14944",
            "title": "Puzzle Curriculum GRPO for Vision-Centric Reasoning",
            "url": "https://huggingface.co/papers/2512.14944",
            "abstract": "Puzzle Curriculum GRPO enhances visual reasoning in Vision Language Models through self-supervised environments and a difficulty-aware curriculum, improving consistency and accuracy without external annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.",
            "score": 2,
            "issue_id": 130,
            "pub_date": "2025-12-16",
            "pub_date_card": {
                "ru": "16 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 16",
                "zh": "12æœˆ16æ—¥"
            },
            "hash": "b8c5af0ad3a1b965",
            "authors": [
                "Ahmadreza Jeddi",
                "Hakki Can Karaimer",
                "Hue Nguyen",
                "Zhongling Wang",
                "Ke Zhao",
                "Javad Rajabi",
                "Ran Zhang",
                "Raghav Goyal",
                "Babak Taati",
                "Radek Grzeszczuk"
            ],
            "affiliations": [
                "AI Center-Toronto, Samsung Electronics",
                "University of Toronto",
                "Vector Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.14944.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#cv",
                    "#training",
                    "#reasoning",
                    "#rlhf",
                    "#rl",
                    "#interpretability"
                ],
                "emoji": "ğŸ§©",
                "ru": {
                    "title": "Ğ¡Ğ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Puzzle Curriculum GRPO (PC-GRPO) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Vision Language Models Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğµ Ñ€ÑƒÑ‡Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ½Ğ° Ñ‚Ñ€Ğ¸ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼ĞºĞ¸-Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ (PatchFit, Rotation, Jigsaw) Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ…ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´ â€” Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ curriculum, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸."
                },
                "en": {
                    "title": "Enhancing Visual Reasoning with Self-Supervised Learning",
                    "desc": "The paper introduces Puzzle Curriculum GRPO (PC-GRPO), a novel approach to enhance visual reasoning in Vision Language Models (VLMs) using self-supervised learning. It addresses the limitations of traditional reinforcement learning methods, such as reliance on expensive annotations and sparse reward structures. By implementing three self-supervised puzzle environments, PC-GRPO provides a more effective training framework that dynamically adjusts difficulty to improve learning outcomes. The results show that this method not only boosts reasoning quality and training stability but also correlates with higher accuracy in downstream tasks."
                },
                "zh": {
                    "title": "æ— ç›‘ç£çš„è§†è§‰æ¨ç†æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPuzzle Curriculum GRPOï¼ˆPC-GRPOï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç›‘ç£ç¯å¢ƒå’Œéš¾åº¦æ„ŸçŸ¥è¯¾ç¨‹æ¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚PC-GRPOä¸ä¾èµ–äºå¤–éƒ¨æ³¨é‡Šæˆ–éªŒè¯è€…ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸‰ç§è‡ªç›‘ç£çš„æ‹¼å›¾ç¯å¢ƒæ¥æ›¿ä»£æ ‡ç­¾ï¼Œä»è€Œæé«˜å¥–åŠ±çš„ç¨€ç–æ€§å’Œä¸€è‡´æ€§ã€‚é€šè¿‡åŠ¨æ€åŠ æƒæ ·æœ¬çš„éš¾åº¦ï¼ŒPC-GRPOæœ‰æ•ˆåœ°è§£å†³äº†å¹³å¦å¥–åŠ±çš„é—®é¢˜ï¼Œå¹¶å»¶è¿Ÿäº†æ¨ç†ä¸ç­”æ¡ˆä¸€è‡´æ€§çš„ä¸‹é™ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPC-GRPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨ç†è´¨é‡å’Œä»»åŠ¡å‡†ç¡®æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.14719",
            "title": "Hybrid Attribution Priors for Explainable and Robust Model Training",
            "url": "https://huggingface.co/papers/2512.14719",
            "abstract": "A novel framework, Class-Aware Attribution Prior (CAP), enhances language model interpretability and robustness by guiding the model to capture fine-grained class distinctions and combining with existing attribution methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.",
            "score": 2,
            "issue_id": 120,
            "pub_date": "2025-12-09",
            "pub_date_card": {
                "ru": "9 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 9",
                "zh": "12æœˆ9æ—¥"
            },
            "hash": "d77b8152219fd212",
            "authors": [
                "Zhuoran Zhang",
                "Feng Zhang",
                "Shangyuan Li",
                "Yang Shi",
                "Yuanxing Zhang",
                "Wei Chen",
                "Tengjiao Wang",
                "Kam-Fai Wong"
            ],
            "affiliations": [
                "Department of Systems Engineering and Engineering Management, CUHK",
                "Key Lab of High Confidence Software Technologies, Peking University",
                "Kling Team",
                "School of Computer Science, Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.14719.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#small_models"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞšĞ»Ğ°ÑÑ-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¾Ğ²",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Framework Class-Aware Attribution Prior (CAP), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ»Ğ°ÑÑĞ°Ğ¼Ğ¸. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ°Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²Ğ°Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… ĞºĞ»Ğ°ÑÑĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ½Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. CAP Hybrid ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ¸Ğ· CAP Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¸Ğ· Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ°Ğº Ğ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²ÑĞµÑ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Language Model Interpretability with Class-Aware Attribution",
                    "desc": "The paper introduces a new framework called Class-Aware Attribution Prior (CAP) that improves how language models understand and explain their decisions. It addresses the challenge of existing attribution methods that often highlight common keywords, which can confuse similar classes. By focusing on fine-grained class distinctions, CAP helps models generate clearer and more useful explanations. Additionally, the CAP Hybrid approach combines these new priors with traditional methods to enhance the model's learning of important features, leading to better performance in various scenarios."
                },
                "zh": {
                    "title": "ç±»æ„ŸçŸ¥å½’å› å…ˆéªŒï¼šæå‡æ¨¡å‹å¯è§£é‡Šæ€§ä¸é²æ£’æ€§",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œç§°ä¸ºç±»æ„ŸçŸ¥å½’å› å…ˆéªŒï¼ˆCAPï¼‰ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œé²æ£’æ€§ã€‚CAPé€šè¿‡å¼•å¯¼æ¨¡å‹æ•æ‰ç»†ç²’åº¦çš„ç±»åˆ«åŒºåˆ†ï¼Œå¹¶ä¸ç°æœ‰çš„å½’å› æ–¹æ³•ç›¸ç»“åˆï¼Œæ¥æ”¹å–„æ¨¡å‹çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„å½’å› æ–¹æ³•å¾€å¾€åªå…³æ³¨è¯­ä¹‰ç›¸ä¼¼ç±»åˆ«ä¹‹é—´çš„å…±åŒå…³é”®è¯ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥åŒºåˆ†è¿™äº›ç±»åˆ«ã€‚é€šè¿‡å¼•å…¥CAP Hybridï¼Œç»“åˆCAPå’Œç°æœ‰å½’å› æŠ€æœ¯çš„å…ˆéªŒä¿¡æ¯ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æœ‰æ•ˆæå‡äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œé²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.09851",
            "title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation",
            "url": "https://huggingface.co/papers/2512.09851",
            "abstract": "TacThru-UMI, a system combining a TacThru sensor with a Transformer-based Diffusion Policy, achieves superior performance in robotic manipulation tasks by integrating simultaneous multimodal perception.  \t\t\t\t\tAI-generated summary \t\t\t\t Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.",
            "score": 1,
            "issue_id": 117,
            "pub_date": "2025-12-10",
            "pub_date_card": {
                "ru": "10 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 10",
                "zh": "12æœˆ10æ—¥"
            },
            "hash": "7e1108830f59e03e",
            "authors": [
                "Yuyang Li",
                "Yinghan Chen",
                "Zihang Zhao",
                "Puhao Li",
                "Tengyu Liu",
                "Siyuan Huang",
                "Yixin Zhu"
            ],
            "affiliations": [
                "National Comprehensive Experimental Base for Governance of Intelligent Society, Wuhan East Lake High-Tech Development Zone",
                "PKU-BingJi Joint Laboratory for Artificial Intelligence",
                "Peking University",
                "State Key Lab of General AI at Peking University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.09851.jpg",
            "data": {
                "categories": [
                    "#robotics",
                    "#training",
                    "#multimodal",
                    "#diffusion"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸",
                    "desc": "Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° TacThru-UMI, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ°Ñ‚Ñ‡Ğ¸Ğº TacThru Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Transformer Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ”Ğ°Ñ‚Ñ‡Ğ¸Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğµ Ñ‚Ğ°ĞºÑ‚Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğ¼Ñƒ ÑĞ»Ğ°ÑÑ‚Ğ¾Ğ¼ĞµÑ€Ñƒ Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ°Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ñ‚Ğ°ĞºÑ‚Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Transformer. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ TacThru-UMI Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 85.5% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸Ğ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ĞµĞµÑÑ Ñ‚Ğ°ĞºÑ‚Ğ¸Ğ»ÑŒĞ½Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ."
                },
                "en": {
                    "title": "Revolutionizing Robotic Manipulation with Multimodal Perception",
                    "desc": "TacThru-UMI is a novel system that enhances robotic manipulation by integrating a TacThru sensor with a Transformer-based Diffusion Policy. This system allows for simultaneous multimodal perception, combining tactile and visual data to improve the reliability of robotic tasks. The TacThru sensor features advanced design elements that ensure effective tracking and signal extraction, addressing limitations found in previous designs. Experiments show that TacThru-UMI significantly outperforms traditional methods, achieving an 85.5% success rate in complex manipulation tasks, highlighting the importance of multimodal integration in robotics."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€æ„ŸçŸ¥ä¸å­¦ä¹ æ¡†æ¶ç»“åˆï¼Œå®ç°ç²¾å‡†æœºå™¨äººæ“ä½œ",
                    "desc": "TacThru-UMIæ˜¯ä¸€ç§ç»“åˆTacThruä¼ æ„Ÿå™¨å’ŒåŸºäºTransformerçš„æ‰©æ•£ç­–ç•¥çš„ç³»ç»Ÿï¼Œèƒ½å¤Ÿåœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­å®ç°å“è¶Šçš„æ€§èƒ½ã€‚è¯¥ç³»ç»Ÿé€šè¿‡é›†æˆåŒæ—¶çš„å¤šæ¨¡æ€æ„ŸçŸ¥ï¼Œè§£å†³äº†ä¼ ç»Ÿè®¾è®¡ä¸­ç¼ºä¹å¯é è§¦è§‰è·Ÿè¸ªçš„é—®é¢˜ã€‚TacThruä¼ æ„Ÿå™¨å…·å¤‡é€æ˜çš„å¼¹æ€§ææ–™å’Œé«˜æ•ˆçš„è·Ÿè¸ªèƒ½åŠ›ï¼Œèƒ½å¤ŸåŒæ—¶æå–è§†è§‰å’Œè§¦è§‰ä¿¡å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTacThru-UMIåœ¨äº”ä¸ªå¤æ‚çš„ç°å®ä»»åŠ¡ä¸­å¹³å‡æˆåŠŸç‡è¾¾åˆ°85.5%ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„è§¦è§‰-è§†è§‰äº¤æ›¿å’Œä»…è§†è§‰çš„åŸºçº¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.15340",
            "title": "Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics",
            "url": "https://huggingface.co/papers/2512.15340",
            "abstract": "TIMAR, a causal framework for 3D conversational head generation, models dialogue as interleaved audio-visual contexts and predicts continuous 3D head dynamics, improving coherence and expressive variability.  \t\t\t\t\tAI-generated summary \t\t\t\t Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal full-sequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while a lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark show that TIMAR reduces FrÃ©chet Distance and MSE by 15-30% on the test set, and achieves similar gains on out-of-distribution data. The source code will be released in the GitHub repository https://github.com/CoderChen01/towards-seamleass-interaction.",
            "score": 0,
            "issue_id": 125,
            "pub_date": "2025-12-17",
            "pub_date_card": {
                "ru": "17 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 17",
                "zh": "12æœˆ17æ—¥"
            },
            "hash": "d2e36af776e63953",
            "authors": [
                "Junjie Chen",
                "Fei Wang",
                "Zhihao Huang",
                "Qing Zhou",
                "Kun Li",
                "Dan Guo",
                "Linfeng Zhang",
                "Xun Yang"
            ],
            "affiliations": [
                "Anhui Polytechnic University",
                "Hefei University of Technology",
                "IAI, Hefei Comprehensive National Science Center",
                "Northwestern Polytechnical University",
                "SJTU",
                "TeleAI, China Telecom",
                "USTC",
                "United Arab Emirates University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.15340.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#multimodal",
                    "#audio",
                    "#3d",
                    "#video",
                    "#robotics",
                    "#open_source"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸",
                    "desc": "TIMAR Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ ĞºĞ°Ğº Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸ĞµÑÑ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµĞ¶Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°. ĞĞ±Ğ»ĞµĞ³Ñ‡Ñ‘Ğ½Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹, Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ ĞºĞ°Ğº ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¤Ñ€ĞµÑˆĞµ Ğ¸ MSE Ğ½Ğ° 15-30% Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ½Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°."
                },
                "en": {
                    "title": "Enhancing Conversational Avatars with TIMAR: Realistic 3D Head Dynamics",
                    "desc": "TIMAR is a new framework designed for generating 3D head movements in conversations by integrating audio and visual cues. It treats dialogue as a series of interleaved contexts, allowing for a more natural representation of how people communicate. By using turn-level causal attention, TIMAR effectively captures the history of the conversation, leading to more coherent and expressive head dynamics. Experiments show that TIMAR significantly improves the quality of generated head movements compared to previous methods, making it a valuable tool for creating realistic avatars and interactive robots."
                },
                "zh": {
                    "title": "TIMARï¼šæå‡3Då¯¹è¯å¤´éƒ¨ç”Ÿæˆçš„å› æœæ¡†æ¶",
                    "desc": "TIMARæ˜¯ä¸€ä¸ªç”¨äº3Då¯¹è¯å¤´éƒ¨ç”Ÿæˆçš„å› æœæ¡†æ¶ï¼Œå®ƒå°†å¯¹è¯å»ºæ¨¡ä¸ºäº¤é”™çš„éŸ³é¢‘-è§†è§‰ä¸Šä¸‹æ–‡ï¼Œå¹¶é¢„æµ‹è¿ç»­çš„3Då¤´éƒ¨åŠ¨æ€ï¼Œä»è€Œæé«˜äº†è¿è´¯æ€§å’Œè¡¨ç°çš„å¤šæ ·æ€§ã€‚äººç±»å¯¹è¯ä¸­åŒ…å«äº†è¯­éŸ³å’Œéè¯­è¨€çº¿ç´¢çš„æŒç»­äº¤æ¢ï¼Œå¦‚ç‚¹å¤´ã€è§†çº¿è½¬ç§»å’Œé¢éƒ¨è¡¨æƒ…ï¼Œè¿™äº›éƒ½ä¼ è¾¾äº†æ³¨æ„åŠ›å’Œæƒ…æ„Ÿã€‚TIMARé€šè¿‡èåˆæ¯ä¸ªå›åˆçš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¹¶åº”ç”¨å›åˆçº§å› æœæ³¨æ„åŠ›æ¥ç§¯ç´¯å¯¹è¯å†å²ï¼Œå…‹æœäº†ä¼ ç»Ÿæ¡†æ¶ä¸­å¯¹è¯è¿‡ç¨‹ç‹¬ç«‹å¤„ç†çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTIMARåœ¨DualTalkåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—é™ä½äº†FrÃ©chetè·ç¦»å’Œå‡æ–¹è¯¯å·®ï¼Œå±•ç¤ºäº†å…¶åœ¨ç”Ÿæˆè‡ªç„¶å¯¹è¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2512.13077",
            "title": "LikeBench: Evaluating Subjective Likability in LLMs for Personalization",
            "url": "https://huggingface.co/papers/2512.13077",
            "abstract": "LikeBench introduces a multi-session evaluation framework to measure the likability of LLMs by their ability to adapt to user preferences across multiple dimensions, demonstrating that strong memory performance does not necessarily equate to higher likability.  \t\t\t\t\tAI-generated summary \t\t\t\t A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.",
            "score": 0,
            "issue_id": 119,
            "pub_date": "2025-12-15",
            "pub_date_card": {
                "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 15",
                "zh": "12æœˆ15æ—¥"
            },
            "hash": "1245306ac747a5f0",
            "authors": [
                "Md Awsafur Rahman",
                "Adam Gabrys",
                "Doug Kang",
                "Jingjing Sun",
                "Tian Tan",
                "Ashwin Chandramouli"
            ],
            "affiliations": [
                "Amazon",
                "UC Santa Barbara"
            ],
            "pdf_title_img": "assets/pdf/title_img/2512.13077.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞŸÑ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸",
                    "desc": "LikeBench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞµĞ°Ğ½ÑĞ¾Ğ²ÑƒÑ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¿Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ñƒ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞµĞ¼ÑŒ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ÑƒÑ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ±ĞµÑĞµĞ´Ñ‹, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ÑĞ¼Ğ¾Ñ€Ñƒ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ ÑÑÑ‹Ğ»ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ Ğ±Ğ°Ğ»Ğ» Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ."
                },
                "en": {
                    "title": "Measuring Likability: Beyond Memory in LLM Adaptation",
                    "desc": "LikeBench is a new evaluation framework designed to assess how well large language models (LLMs) can adapt to user preferences over time, focusing on likability as a key metric. It introduces a multi-session approach where LLMs interact with a simulated user, learning and adjusting based on ongoing dialogue. Unlike previous benchmarks that primarily measure memory accuracy and application, LikeBench emphasizes likability across seven dimensions, such as emotional adaptation and humor fit. The findings reveal that high memory performance does not always correlate with higher likability, highlighting the importance of user experience in LLM interactions."
                },
                "zh": {
                    "title": "å¯å–œçˆ±æ€§ï¼šè¶…è¶Šè®°å¿†çš„è¯„ä¼°æ ‡å‡†",
                    "desc": "LikeBench æ˜¯ä¸€ä¸ªå¤šä¼šè¯è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºæµ‹é‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€‚åº”ç”¨æˆ·åå¥½æ–¹é¢çš„å¯å–œçˆ±ç¨‹åº¦ã€‚è¯¥æ¡†æ¶å¼ºè°ƒå¯å–œçˆ±æ€§ä¸ä»…ä»…ä¾èµ–äºè®°å¿†æ€§èƒ½ï¼Œè¿˜æ¶‰åŠæƒ…æ„Ÿé€‚åº”ã€æ­£å¼æ€§åŒ¹é…ç­‰å¤šä¸ªç»´åº¦ã€‚é€šè¿‡ä¸æ¨¡æ‹Ÿç”¨æˆ·çš„å¯¹è¯ï¼ŒLLM èƒ½å¤Ÿå­¦ä¹ ç”¨æˆ·çš„åå¥½å¹¶è¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Œä»è€Œæä¾›æ›´ç¬¦åˆç”¨æˆ·æœŸæœ›çš„å“åº”ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¼ºå¤§çš„è®°å¿†èƒ½åŠ›å¹¶ä¸ä¸€å®šæ„å‘³ç€æ›´é«˜çš„å¯å–œçˆ±æ€§ï¼ŒæŸäº›æ¨¡å‹åœ¨å¯å–œçˆ±æ€§è¯„åˆ†ä¸Šè¡¨ç°æ›´ä½³ï¼Œå³ä½¿å®ƒä»¬çš„è®°å¿†å‡†ç¡®æ€§è¾ƒä½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-12-17.html",
    "link_next": "2025-12-19.html",
    "link_month": "2025-12.html",
    "short_date_prev": {
        "ru": "17.12",
        "en": "12/17",
        "zh": "12æœˆ17æ—¥"
    },
    "short_date_next": {
        "ru": "19.12",
        "en": "12/19",
        "zh": "12æœˆ19æ—¥"
    },
    "categories": {
        "#dataset": 7,
        "#data": 1,
        "#benchmark": 10,
        "#agents": 2,
        "#cv": 4,
        "#rl": 3,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 3,
        "#3d": 3,
        "#audio": 2,
        "#video": 7,
        "#multimodal": 11,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 0,
        "#training": 13,
        "#robotics": 3,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 1,
        "#reasoning": 5,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 5,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 2,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 9,
        "#small_models": 3,
        "#science": 0,
        "#low_resource": 1
    }
}