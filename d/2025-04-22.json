{
    "date": {
        "ru": "22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        "en": "April 22",
        "zh": "4æœˆ22æ—¥"
    },
    "time_utc": "2025-04-22 06:16",
    "weekday": 1,
    "issue_id": 3361,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.14945",
            "title": "Learning to Reason under Off-Policy Guidance",
            "url": "https://huggingface.co/papers/2504.14945",
            "abstract": "Recent advances in large reasoning models (LRMs) demonstrate that sophisticated behaviors such as multi-step reasoning and self-reflection can emerge via reinforcement learning (RL) with simple rule-based rewards. However, existing zero-RL approaches are inherently ``on-policy'', limiting learning to a model's own outputs and failing to acquire reasoning abilities beyond its initial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY guidance), a framework that augments zero-RL with off-policy reasoning traces. LUFFY dynamically balances imitation and exploration by combining off-policy demonstrations with on-policy rollouts during training. Notably, we propose policy shaping via regularized importance sampling to avoid superficial and rigid imitation during mixed-policy training. Remarkably, LUFFY achieves an over +7.0 average gain across six math benchmarks and an advantage of over +6.2 points in out-of-distribution tasks. It also substantially surpasses imitation-based supervised fine-tuning (SFT), particularly in generalization. Analysis shows LUFFY not only imitates effectively but also explores beyond demonstrations, offering a scalable path to train generalizable reasoning models with off-policy guidance.",
            "score": 22,
            "issue_id": 3358,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 21",
                "zh": "4æœˆ21æ—¥"
            },
            "hash": "a0e7730fccfb6534",
            "authors": [
                "Jianhao Yan",
                "Yafu Li",
                "Zican Hu",
                "Zhi Wang",
                "Ganqu Cui",
                "Xiaoye Qu",
                "Yu Cheng",
                "Yue Zhang"
            ],
            "affiliations": [
                "Nanjing University",
                "Shanghai AI Laboratory",
                "The Chinese University of Hong Kong",
                "Westlake University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14945.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#reasoning",
                    "#math",
                    "#optimization",
                    "#rl"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LUFFY: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¾Ğ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼",
                    "desc": "LUFFY - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², LUFFY ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğ¾ Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ·Ğ° Ğ¸Ñ… Ñ€Ğ°Ğ¼ĞºĞ¸, Ğ½Ğ°Ñ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ½Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "LUFFY: Expanding Reasoning with Off-Policy Learning",
                    "desc": "This paper presents LUFFY, a new framework that enhances zero-reinforcement learning (zero-RL) by incorporating off-policy reasoning traces. Unlike traditional zero-RL methods that are limited to on-policy learning, LUFFY allows models to learn from a broader range of experiences by balancing imitation of off-policy demonstrations with exploration of their own outputs. The framework employs policy shaping through regularized importance sampling to ensure that the model does not merely imitate but also develops deeper reasoning capabilities. LUFFY shows significant improvements in performance on math benchmarks and out-of-distribution tasks, demonstrating its effectiveness in training generalizable reasoning models."
                },
                "zh": {
                    "title": "LUFFYï¼šè¶…è¶Šåˆå§‹èƒ½åŠ›çš„æ¨ç†å­¦ä¹ ",
                    "desc": "æœ€è¿‘çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰è¿›å±•è¡¨æ˜ï¼Œé€šè¿‡ç®€å•çš„åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥äº§ç”Ÿå¤æ‚çš„è¡Œä¸ºï¼Œå¦‚å¤šæ­¥æ¨ç†å’Œè‡ªæˆ‘åæ€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é›¶å¼ºåŒ–å­¦ä¹ æ–¹æ³•æœ¬è´¨ä¸Šæ˜¯â€œåœ¨æ”¿ç­–ä¸Šâ€çš„ï¼Œè¿™é™åˆ¶äº†å­¦ä¹ ä»…é™äºæ¨¡å‹è‡ªèº«çš„è¾“å‡ºï¼Œæ— æ³•è·å¾—è¶…å‡ºåˆå§‹èƒ½åŠ›çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†LUFFYï¼ˆåœ¨æ”¿ç­–å¤–æŒ‡å¯¼ä¸‹å­¦ä¹ æ¨ç†ï¼‰ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼•å…¥æ”¿ç­–å¤–çš„æ¨ç†è½¨è¿¹æ¥å¢å¼ºé›¶å¼ºåŒ–å­¦ä¹ ã€‚LUFFYåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€å¹³è¡¡æ¨¡ä»¿å’Œæ¢ç´¢ï¼Œç»“åˆäº†æ”¿ç­–å¤–çš„ç¤ºèŒƒå’Œæ”¿ç­–å†…çš„å›æ”¾ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15257",
            "title": "FlowReasoner: Reinforcing Query-Level Meta-Agents",
            "url": "https://huggingface.co/papers/2504.15257",
            "abstract": "This paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with external execution feedback. A multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency. In this manner, FlowReasoner is enabled to generate a personalized multi-agent system for each user query via deliberative reasoning. Experiments on both engineering and competition code benchmarks demonstrate the superiority of FlowReasoner. Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks. The code is available at https://github.com/sail-sg/FlowReasoner.",
            "score": 17,
            "issue_id": 3359,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 21",
                "zh": "4æœˆ21æ—¥"
            },
            "hash": "edb79ae8d9c372b4",
            "authors": [
                "Hongcheng Gao",
                "Yue Liu",
                "Yufei He",
                "Longxu Dou",
                "Chao Du",
                "Zhijie Deng",
                "Bryan Hooi",
                "Min Lin",
                "Tianyu Pang"
            ],
            "affiliations": [
                "National University of Singapore",
                "Sea AI Lab, Singapore",
                "Shanghai Jiao Tong University",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15257.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#reasoning",
                    "#rl",
                    "#agents"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "FlowReasoner: Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ FlowReasoner - Ğ¼ĞµÑ‚Ğ°-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ°-Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. FlowReasoner Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DeepSeek R1 Ğ¸ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ FlowReasoner Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ² Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ÑÑ Ğ½Ğ° 10.52% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ o1-mini Ğ½Ğ° Ñ‚Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Automating Personalized Multi-Agent Systems with FlowReasoner",
                    "desc": "This paper introduces FlowReasoner, a meta-agent designed to automate the creation of multi-agent systems tailored to individual user queries. The approach leverages external execution feedback to enhance the reasoning capabilities of the agent, which is initially based on the DeepSeek R1 model. By employing reinforcement learning, FlowReasoner is trained with a multi-faceted reward system that optimizes for performance, complexity, and efficiency. Experimental results show that FlowReasoner outperforms existing methods, achieving a 10.52% accuracy improvement across various benchmarks."
                },
                "zh": {
                    "title": "FlowReasonerï¼šä¸ºæ¯ä¸ªæŸ¥è¯¢å®šåˆ¶æ™ºèƒ½ä»£ç†ç³»ç»Ÿ",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFlowReasonerçš„æŸ¥è¯¢çº§å…ƒä»£ç†ï¼Œç”¨äºè‡ªåŠ¨è®¾è®¡æŸ¥è¯¢çº§å¤šä»£ç†ç³»ç»Ÿï¼Œå³ä¸ºæ¯ä¸ªç”¨æˆ·æŸ¥è¯¢æ„å»ºä¸€ä¸ªç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¤–éƒ¨æ‰§è¡Œåé¦ˆæ¥æ¿€åŠ±åŸºäºæ¨ç†çš„å…ƒä»£ç†ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡æç‚¼DeepSeek R1ï¼Œæˆ‘ä»¬é¦–å…ˆèµ‹äºˆFlowReasoneråŸºæœ¬çš„å¤šä»£ç†ç³»ç»Ÿç”Ÿæˆæ¨ç†èƒ½åŠ›ã€‚ç„¶åï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¤–éƒ¨æ‰§è¡Œåé¦ˆè¿›ä¸€æ­¥å¢å¼ºå…¶èƒ½åŠ›ï¼Œè®¾è®¡äº†ä¸€ä¸ªå¤šç”¨é€”å¥–åŠ±æ¥æŒ‡å¯¼RLè®­ç»ƒï¼Œä»æ€§èƒ½ã€å¤æ‚æ€§å’Œæ•ˆç‡ç­‰æ–¹é¢è¿›è¡Œä¼˜åŒ–ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14396",
            "title": "SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video\n  Generation via Spherical Latent Representation",
            "url": "https://huggingface.co/papers/2504.14396",
            "abstract": "The increasing demand for AR/VR applications has highlighted the need for high-quality 360-degree panoramic content. However, generating high-quality 360-degree panoramic images and videos remains a challenging task due to the severe distortions introduced by equirectangular projection (ERP). Existing approaches either fine-tune pretrained diffusion models on limited ERP datasets or attempt tuning-free methods that still rely on ERP latent representations, leading to discontinuities near the poles. In this paper, we introduce SphereDiff, a novel approach for seamless 360-degree panoramic image and video generation using state-of-the-art diffusion models without additional tuning. We define a spherical latent representation that ensures uniform distribution across all perspectives, mitigating the distortions inherent in ERP. We extend MultiDiffusion to spherical latent space and propose a spherical latent sampling method to enable direct use of pretrained diffusion models. Moreover, we introduce distortion-aware weighted averaging to further improve the generation quality in the projection process. Our method outperforms existing approaches in generating 360-degree panoramic content while maintaining high fidelity, making it a robust solution for immersive AR/VR applications. The code is available here. https://github.com/pmh9960/SphereDiff",
            "score": 17,
            "issue_id": 3357,
            "pub_date": "2025-04-19",
            "pub_date_card": {
                "ru": "19 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 19",
                "zh": "4æœˆ19æ—¥"
            },
            "hash": "9688d3d72143f02c",
            "authors": [
                "Minho Park",
                "Taewoong Kang",
                "Jooyeol Yun",
                "Sungwon Hwang",
                "Jaegul Choo"
            ],
            "affiliations": [
                "Korea Advanced Institute of Science and Technology (KAIST)"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14396.jpg",
            "data": {
                "categories": [
                    "#3d",
                    "#diffusion",
                    "#multimodal",
                    "#open_source",
                    "#video"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "SphereDiff: Ğ‘ĞµÑÑˆĞ¾Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼ 360Â° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "SphereDiff - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ¼ 360 Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ÑÑƒÑ‰Ğ¸Ğµ ÑĞºĞ²Ğ¸Ğ´Ğ¸ÑÑ‚Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸. SphereDiff Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ MultiDiffusion Ğ½Ğ° ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğµ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "SphereDiff: Seamless 360-Degree Content Generation for AR/VR",
                    "desc": "This paper presents SphereDiff, a new method for generating high-quality 360-degree panoramic images and videos using diffusion models. It addresses the challenges of distortions caused by equirectangular projection (ERP) by introducing a spherical latent representation that provides a uniform perspective distribution. SphereDiff enhances the existing MultiDiffusion framework by allowing direct use of pretrained models without the need for additional tuning. The method also incorporates distortion-aware weighted averaging to improve the quality of the generated content, outperforming previous techniques in fidelity and robustness for AR/VR applications."
                },
                "zh": {
                    "title": "SphereDiffï¼šæ— ç¼ç”Ÿæˆ360åº¦å…¨æ™¯å†…å®¹çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "éšç€å¢å¼ºç°å®å’Œè™šæ‹Ÿç°å®åº”ç”¨çš„éœ€æ±‚å¢åŠ ï¼Œé«˜è´¨é‡çš„360åº¦å…¨æ™¯å†…å®¹å˜å¾—å°¤ä¸ºé‡è¦ã€‚ç„¶è€Œï¼Œç”±äºç­‰è·çŸ©å½¢æŠ•å½±ï¼ˆERPï¼‰å¼•å…¥çš„ä¸¥é‡å¤±çœŸï¼Œç”Ÿæˆé«˜è´¨é‡çš„360åº¦å…¨æ™¯å›¾åƒå’Œè§†é¢‘ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSphereDiffçš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹å®ç°æ— ç¼çš„360åº¦å…¨æ™¯å›¾åƒå’Œè§†é¢‘ç”Ÿæˆï¼Œä¸”æ— éœ€é¢å¤–è°ƒä¼˜ã€‚æˆ‘ä»¬å®šä¹‰äº†ä¸€ç§çƒå½¢æ½œåœ¨è¡¨ç¤ºï¼Œç¡®ä¿å„ä¸ªè§†è§’çš„å‡åŒ€åˆ†å¸ƒï¼Œä»è€Œå‡è½»ERPå›ºæœ‰çš„å¤±çœŸï¼Œæ˜¾è‘—æé«˜ç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13958",
            "title": "ToolRL: Reward is All Tool Learning Needs",
            "url": "https://huggingface.co/papers/2504.13958",
            "abstract": "Current Large Language Models (LLMs) often undergo supervised fine-tuning (SFT) to acquire tool use capabilities. However, SFT struggles to generalize to unfamiliar or complex tool use scenarios. Recent advancements in reinforcement learning (RL), particularly with R1-like models, have demonstrated promising reasoning and generalization abilities. Yet, reward design for tool use presents unique challenges: multiple tools may be invoked with diverse parameters, and coarse-grained reward signals, such as answer matching, fail to offer the finegrained feedback required for effective learning. In this work, we present the first comprehensive study on reward design for tool selection and application tasks within the RL paradigm. We systematically explore a wide range of reward strategies, analyzing their types, scales, granularity, and temporal dynamics. Building on these insights, we propose a principled reward design tailored for tool use tasks and apply it to train LLMs using Group Relative Policy Optimization (GRPO). Empirical evaluations across diverse benchmarks demonstrate that our approach yields robust, scalable, and stable training, achieving a 17% improvement over base models and a 15% gain over SFT models. These results highlight the critical role of thoughtful reward design in enhancing the tool use capabilities and generalization performance of LLMs. All the codes are released to facilitate future research.",
            "score": 17,
            "issue_id": 3358,
            "pub_date": "2025-04-16",
            "pub_date_card": {
                "ru": "16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 16",
                "zh": "4æœˆ16æ—¥"
            },
            "hash": "53a7757486eb7210",
            "authors": [
                "Cheng Qian",
                "Emre Can Acikgoz",
                "Qi He",
                "Hongru Wang",
                "Xiusi Chen",
                "Dilek Hakkani-TÃ¼r",
                "Gokhan Tur",
                "Heng Ji"
            ],
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13958.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#rlhf",
                    "#survey",
                    "#reasoning",
                    "#optimization",
                    "#benchmark",
                    "#rl"
                ],
                "emoji": "ğŸ› ï¸",
                "ru": {
                    "title": "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² LLM Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑÑ ÑĞ²Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Group Relative Policy Optimization (GRPO), Ğ¾Ğ½Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 17% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ½Ğ° 15% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ supervised fine-tuning (SFT). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Enhancing Tool Use in LLMs through Smart Reward Design",
                    "desc": "This paper addresses the challenges of training Large Language Models (LLMs) to effectively use tools through reinforcement learning (RL). It highlights the limitations of supervised fine-tuning (SFT) in generalizing to new tool use scenarios and proposes a novel reward design specifically for tool selection and application tasks. The authors systematically investigate various reward strategies, focusing on their effectiveness in providing fine-grained feedback necessary for learning. Their proposed method, Group Relative Policy Optimization (GRPO), shows significant improvements in training outcomes, demonstrating the importance of well-designed rewards in enhancing LLMs' tool use capabilities."
                },
                "zh": {
                    "title": "ä¼˜åŒ–å¥–åŠ±è®¾è®¡ï¼Œæå‡å·¥å…·ä½¿ç”¨èƒ½åŠ›",
                    "desc": "å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥è·å¾—å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒSFTåœ¨é¢å¯¹ä¸ç†Ÿæ‚‰æˆ–å¤æ‚çš„å·¥å…·ä½¿ç”¨åœºæ™¯æ—¶ï¼Œæ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚æœ€è¿‘ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç‰¹åˆ«æ˜¯R1ç±»æ¨¡å‹çš„è¿›å±•æ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½†å·¥å…·ä½¿ç”¨çš„å¥–åŠ±è®¾è®¡é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢ç ”ç©¶äº†åœ¨RLèŒƒå¼ä¸‹å·¥å…·é€‰æ‹©å’Œåº”ç”¨ä»»åŠ¡çš„å¥–åŠ±è®¾è®¡ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹å·¥å…·ä½¿ç”¨ä»»åŠ¡çš„åŸåˆ™æ€§å¥–åŠ±è®¾è®¡ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶åœ¨è®­ç»ƒLLMsä¸­çš„æœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13203",
            "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents",
            "url": "https://huggingface.co/papers/2504.13203",
            "abstract": "Multi-turn interactions with language models (LMs) pose critical safety risks, as harmful intent can be strategically spread across exchanges. Yet, the vast majority of prior work has focused on single-turn safety, while adaptability and diversity remain among the key challenges of multi-turn red-teaming. To address these challenges, we present X-Teaming, a scalable framework that systematically explores how seemingly harmless interactions escalate into harmful outcomes and generates corresponding attack scenarios. X-Teaming employs collaborative agents for planning, attack optimization, and verification, achieving state-of-the-art multi-turn jailbreak effectiveness and diversity with success rates up to 98.1% across representative leading open-weight and closed-source models. In particular, X-Teaming achieves a 96.2% attack success rate against the latest Claude 3.7 Sonnet model, which has been considered nearly immune to single-turn attacks. Building on X-Teaming, we introduce XGuard-Train, an open-source multi-turn safety training dataset that is 20x larger than the previous best resource, comprising 30K interactive jailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our work offers essential tools and insights for mitigating sophisticated conversational attacks, advancing the multi-turn safety of LMs.",
            "score": 13,
            "issue_id": 3358,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 15",
                "zh": "4æœˆ15æ—¥"
            },
            "hash": "43e0ba05f6baef62",
            "authors": [
                "Salman Rahman",
                "Liwei Jiang",
                "James Shiffer",
                "Genglin Liu",
                "Sheriff Issaka",
                "Md Rizwan Parvez",
                "Hamid Palangi",
                "Kai-Wei Chang",
                "Yejin Choi",
                "Saadia Gabriel"
            ],
            "affiliations": [
                "Google",
                "Qatar Computing Research Institute",
                "Stanford University",
                "University of California, Los Angeles",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13203.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#training",
                    "#open_source",
                    "#security",
                    "#agents",
                    "#dataset"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ£ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¼ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ X-Teaming - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ°Ñ‚Ğ°Ğº, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ X-Teaming ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… XGuard-Train Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¯Ğœ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Multi-Turn Safety in Language Models with X-Teaming",
                    "desc": "This paper introduces X-Teaming, a framework designed to enhance the safety of multi-turn interactions with language models (LMs). It addresses the challenge of harmful intent spreading across multiple exchanges, which has been largely overlooked in previous research focused on single-turn interactions. X-Teaming utilizes collaborative agents to plan, optimize, and verify attack scenarios, achieving high effectiveness in multi-turn jailbreaks with success rates up to 98.1%. Additionally, the authors present XGuard-Train, a large dataset for training LMs on multi-turn safety, significantly improving the ability to defend against complex conversational attacks."
                },
                "zh": {
                    "title": "æå‡å¤šè½®äº¤äº’å®‰å…¨æ€§çš„X-Teamingæ¡†æ¶",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºX-Teamingçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤šè½®äº¤äº’ä¸­è¯­è¨€æ¨¡å‹çš„å®‰å…¨é£é™©ã€‚è¯¥æ¡†æ¶é€šè¿‡ç³»ç»Ÿæ€§æ¢ç´¢æ— å®³äº’åŠ¨å¦‚ä½•æ¼”å˜ä¸ºæœ‰å®³ç»“æœï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„æ”»å‡»åœºæ™¯ã€‚X-Teamingåˆ©ç”¨åä½œä»£ç†è¿›è¡Œè§„åˆ’ã€æ”»å‡»ä¼˜åŒ–å’ŒéªŒè¯ï¼ŒæˆåŠŸç‡é«˜è¾¾98.1%ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†XGuard-Trainï¼Œä¸€ä¸ªå¼€æºçš„å¤šè½®å®‰å…¨è®­ç»ƒæ•°æ®é›†ï¼Œè§„æ¨¡æ˜¯ä¹‹å‰æœ€ä½³èµ„æºçš„20å€ï¼ŒåŒ…å«3ä¸‡ä¸ªäº’åŠ¨è¶Šç‹±æ¡ˆä¾‹ï¼Œæ—¨åœ¨å¢å¼ºè¯­è¨€æ¨¡å‹çš„å¤šè½®å®‰å…¨æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14603",
            "title": "UFO2: The Desktop AgentOS",
            "url": "https://huggingface.co/papers/2504.14603",
            "abstract": "Recent Computer-Using Agents (CUAs), powered by multimodal large language models (LLMs), offer a promising direction for automating complex desktop workflows through natural language. However, most existing CUAs remain conceptual prototypes, hindered by shallow OS integration, fragile screenshot-based interaction, and disruptive execution.   We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs into practical, system-level automation. UFO2 features a centralized HostAgent for task decomposition and coordination, alongside a collection of application-specialized AppAgent equipped with native APIs, domain-specific knowledge, and a unified GUI--API action layer. This architecture enables robust task execution while preserving modularity and extensibility. A hybrid control detection pipeline fuses Windows UI Automation (UIA) with vision-based parsing to support diverse interface styles. Runtime efficiency is further enhanced through speculative multi-action planning, reducing per-step LLM overhead. Finally, a Picture-in-Picture (PiP) interface enables automation within an isolated virtual desktop, allowing agents and users to operate concurrently without interference.   We evaluate UFO2 across over 20 real-world Windows applications, demonstrating substantial improvements in robustness and execution accuracy over prior CUAs. Our results show that deep OS integration unlocks a scalable path toward reliable, user-aligned desktop automation.",
            "score": 12,
            "issue_id": 3357,
            "pub_date": "2025-04-20",
            "pub_date_card": {
                "ru": "20 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 20",
                "zh": "4æœˆ20æ—¥"
            },
            "hash": "81eea84c9d10e4d0",
            "authors": [
                "Chaoyun Zhang",
                "He Huang",
                "Chiming Ni",
                "Jian Mu",
                "Si Qin",
                "Shilin He",
                "Lu Wang",
                "Fangkai Yang",
                "Pu Zhao",
                "Chao Du",
                "Liqun Li",
                "Yu Kang",
                "Zhao Jiang",
                "Suzhen Zheng",
                "Rujia Wang",
                "Jiaxu Qian",
                "Minghua Ma",
                "Jian-Guang Lou",
                "Qingwei Lin",
                "Saravan Rajmohan",
                "Dongmei Zhang"
            ],
            "affiliations": [
                "Microsoft",
                "Nanjing University",
                "Peking University",
                "ZJU-UIUC Institute"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14603.jpg",
            "data": {
                "categories": [
                    "#agents",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "UFO2: ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Windows Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ ĞĞ¡",
                    "desc": "UFO2 - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ ÑÑ‚Ğ¾Ğ»Ğ° Windows, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ HostAgent Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… AppAgent Ñ Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ API Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. UFO2 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Windows UI Automation Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "UFO2: Elevating Desktop Automation with Intelligent Agents",
                    "desc": "The paper introduces UFO2, a multiagent system designed to enhance the functionality of Computer-Using Agents (CUAs) on Windows desktops. It addresses limitations of existing CUAs by integrating a centralized HostAgent for better task management and specialized AppAgents that utilize native APIs for improved interaction. The system employs a hybrid control detection pipeline that combines UI Automation with vision-based techniques, allowing it to handle various interface styles effectively. Evaluation results indicate that UFO2 significantly improves the robustness and accuracy of desktop automation tasks compared to previous models, showcasing the benefits of deep OS integration."
                },
                "zh": {
                    "title": "UFO2ï¼šæå‡æ¡Œé¢è‡ªåŠ¨åŒ–çš„æ™ºèƒ½ä»£ç†ç³»ç»Ÿ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºUFO2çš„å¤šä»£ç†AgentOSï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€å®ç°Windowsæ¡Œé¢çš„å¤æ‚å·¥ä½œæµç¨‹è‡ªåŠ¨åŒ–ã€‚UFO2é‡‡ç”¨é›†ä¸­å¼çš„HostAgentè¿›è¡Œä»»åŠ¡åˆ†è§£å’Œåè°ƒï¼Œå¹¶é…å¤‡äº†åº”ç”¨ä¸“ç”¨çš„AppAgentï¼Œåˆ©ç”¨æœ¬åœ°APIå’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†æ¥å¢å¼ºç³»ç»Ÿé›†æˆã€‚è¯¥æ¶æ„æ”¯æŒå¼ºå¤§çš„ä»»åŠ¡æ‰§è¡Œï¼ŒåŒæ—¶ä¿æŒæ¨¡å—åŒ–å’Œå¯æ‰©å±•æ€§ï¼Œç»“åˆäº†Windows UIè‡ªåŠ¨åŒ–å’Œè§†è§‰è§£ææŠ€æœ¯ï¼Œä»¥é€‚åº”å¤šæ ·åŒ–çš„ç•Œé¢é£æ ¼ã€‚é€šè¿‡åœ¨20å¤šä¸ªçœŸå®Windowsåº”ç”¨ç¨‹åºä¸­çš„è¯„ä¼°ï¼ŒUFO2åœ¨é²æ£’æ€§å’Œæ‰§è¡Œå‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¹‹å‰çš„CUAï¼Œå±•ç¤ºäº†æ·±åº¦æ“ä½œç³»ç»Ÿé›†æˆçš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15281",
            "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on\n  3D Gaussians",
            "url": "https://huggingface.co/papers/2504.15281",
            "abstract": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction but struggles with stylized scenarios (e.g., cartoons, games) due to fragmented textures, semantic misalignment, and limited adaptability to abstract aesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer that integrates multi-modal style conditioning, multi-level semantic alignment, and perceptual quality enhancement. Our key insights include: (1) optimizing only RGB attributes preserves geometric integrity during stylization; (2) disentangling low-, medium-, and high-level semantics is critical for coherent style transfer; (3) scalability across isolated objects and complex scenes is essential for practical deployment. StyleMe3D introduces four novel components: Dynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent space for semantic alignment; Contrastive Style Descriptor (CSD) for localized, content-aware texture transfer; Simultaneously Optimized Scale (SOS) to decouple style details and structural coherence; and 3D Gaussian Quality Assessment (3DG-QA), a differentiable aesthetic prior trained on human-rated data to suppress artifacts and enhance visual harmony. Evaluated on NeRF synthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D outperforms state-of-the-art methods in preserving geometric details (e.g., carvings on sculptures) and ensuring stylistic consistency across scenes (e.g., coherent lighting in landscapes), while maintaining real-time rendering. This work bridges photorealistic 3D GS and artistic stylization, unlocking applications in gaming, virtual worlds, and digital art.",
            "score": 7,
            "issue_id": 3361,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 21",
                "zh": "4æœˆ21æ—¥"
            },
            "hash": "f2788379a7878393",
            "authors": [
                "Cailin Zhuang",
                "Yaoqi Hu",
                "Xuanyang Zhang",
                "Wei Cheng",
                "Jiacheng Bao",
                "Shengqi Liu",
                "Yiying Yang",
                "Xianfang Zeng",
                "Gang Yu",
                "Ming Li"
            ],
            "affiliations": [
                "AIGC Research",
                "Guangming Laboratory",
                "ShanghaiTech University",
                "StepFun"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15281.jpg",
            "data": {
                "categories": [
                    "#synthetic",
                    "#3d",
                    "#games",
                    "#multimodal"
                ],
                "emoji": "ğŸ¨",
                "ru": {
                    "title": "StyleMe3D: ĞœĞ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ 3D Ğ¸ Ñ…ÑƒĞ´Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹",
                    "desc": "StyleMe3D - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ 3D-ÑÑ†ĞµĞ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ 3D Gaussian Splatting. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚ÑƒÑ€, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºĞµ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ½Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ñ, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. StyleMe3D Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸."
                },
                "en": {
                    "title": "Bridging Realism and Artistry in 3D Style Transfer",
                    "desc": "The paper introduces StyleMe3D, a framework designed to enhance 3D Gaussian Splatting (3DGS) for style transfer in stylized scenarios like cartoons and games. It addresses challenges such as fragmented textures and semantic misalignment by optimizing RGB attributes to maintain geometric integrity and disentangling different levels of semantics for coherent style application. The framework includes innovative components like Dynamic Style Score Distillation and Contrastive Style Descriptor to improve texture transfer and aesthetic quality. Evaluations show that StyleMe3D outperforms existing methods in preserving details and ensuring stylistic consistency, making it suitable for applications in gaming and digital art."
                },
                "zh": {
                    "title": "é£æ ¼åŒ–3Dé‡å»ºçš„æ–°çªç ´",
                    "desc": "3D Gaussian Splattingï¼ˆ3DGSï¼‰åœ¨çœŸå®åœºæ™¯é‡å»ºæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é£æ ¼åŒ–åœºæ™¯ï¼ˆå¦‚å¡é€šå’Œæ¸¸æˆï¼‰ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†StyleMe3Dï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„3Dé£æ ¼è½¬ç§»æ¡†æ¶ï¼Œé›†æˆäº†å¤šæ¨¡æ€é£æ ¼æ¡ä»¶ã€å¤šå±‚æ¬¡è¯­ä¹‰å¯¹é½å’Œæ„ŸçŸ¥è´¨é‡å¢å¼ºã€‚æˆ‘ä»¬çš„å…³é”®è§è§£åŒ…æ‹¬ï¼šä¼˜åŒ–RGBå±æ€§å¯ä»¥åœ¨é£æ ¼åŒ–è¿‡ç¨‹ä¸­ä¿æŒå‡ ä½•å®Œæ•´æ€§ï¼›è§£è€¦ä½ã€ä¸­ã€é«˜å±‚æ¬¡è¯­ä¹‰å¯¹äºä¸€è‡´çš„é£æ ¼è½¬ç§»è‡³å…³é‡è¦ã€‚StyleMe3Då¼•å…¥äº†å››ä¸ªæ–°ç»„ä»¶ï¼Œæ˜¾è‘—æå‡äº†é£æ ¼è½¬ç§»çš„æ•ˆæœï¼Œèƒ½å¤Ÿåœ¨æ¸¸æˆã€è™šæ‹Ÿä¸–ç•Œå’Œæ•°å­—è‰ºæœ¯ç­‰é¢†åŸŸåº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15280",
            "title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in\n  MLLMs",
            "url": "https://huggingface.co/papers/2504.15280",
            "abstract": "Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is a fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive advances in high-level reasoning and planning, they frequently fall short when confronted with multi-view geometric consistency and cross-view correspondence. To comprehensively evaluate the challenges of MLLMs in multi-view scene reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human carefully annotated multi-view question-answer pairs across 90 diverse real-world scenes. Our six tasks (counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation) specifically test model's geometric correspondence and the capacity to align information consistently across views. Our extensive experiments, benchmark on 27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and GPT-4o against human evaluators reveals a substantial performance gap, indicating that current MLLMs remain far from human-level proficiency. Through in-depth analysis, we show that MLLMs are particularly underperforming under two aspects: (1) cross-view correspondence for partially occluded views and (2) establishing the coarse camera poses. These findings highlight the necessity of domain-specific refinements or modules that embed stronger multi-view awareness. We believe that our All-Angles Bench offers valuable insights and contribute to bridging the gap between MLLMs and human-level multi-view understanding. The project and benchmark are publicly available at https://danielchyeh.github.io/All-Angles-Bench/.",
            "score": 6,
            "issue_id": 3358,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 21",
                "zh": "4æœˆ21æ—¥"
            },
            "hash": "5f2365c2ff2ff7f8",
            "authors": [
                "Chun-Hsiao Yeh",
                "Chenyu Wang",
                "Shengbang Tong",
                "Ta-Ying Cheng",
                "Rouyu Wang",
                "Tianzhe Chu",
                "Yuexiang Zhai",
                "Yubei Chen",
                "Shenghua Gao",
                "Yi Ma"
            ],
            "affiliations": [
                "HKU",
                "NYU",
                "TranscEngram",
                "UC Berkeley",
                "UC Davis",
                "University of Oxford"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15280.jpg",
            "data": {
                "categories": [
                    "#multimodal",
                    "#3d",
                    "#survey",
                    "#reasoning",
                    "#benchmark"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº All-Angles Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 2100 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ 90 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ†ĞµĞ½Ğ°Ğ¼, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚Ğ°, Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ², Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ² ÑÑ‚Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ğ¾ÑĞ¾Ğ±Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸ÑÑ… Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹."
                },
                "en": {
                    "title": "Bridging the Gap in Multi-View Understanding for MLLMs",
                    "desc": "This paper addresses the challenge of multi-view understanding in Multi-Modal Large Language Models (MLLMs), which is crucial for tasks like navigation and scene comprehension. The authors introduce All-Angles Bench, a benchmark consisting of over 2,100 annotated question-answer pairs designed to evaluate MLLMs on their ability to handle geometric consistency and cross-view correspondence. They conduct experiments on 27 MLLMs, revealing a significant performance gap compared to human evaluators, particularly in handling occluded views and estimating camera poses. The findings suggest that enhancing MLLMs with domain-specific modules for better multi-view awareness is essential for improving their performance."
                },
                "zh": {
                    "title": "æå‡å¤šè§†è§’ç†è§£ï¼Œç¼©å°äººæœºå·®è·ï¼",
                    "desc": "å¤šè§†è§’ç†è§£æ˜¯å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢ä¸´çš„ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œæ¶‰åŠåœ¨ä¸åŒè§†è§’ä¸‹åè°ƒè§†è§‰ä¿¡æ¯ä»¥å®ç°æœ‰æ•ˆå¯¼èˆªå’Œ3Dåœºæ™¯ç†è§£ã€‚å°½ç®¡æœ€è¿‘çš„MLLMsåœ¨é«˜å±‚æ¬¡æ¨ç†å’Œè§„åˆ’æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤šè§†è§’å‡ ä½•ä¸€è‡´æ€§å’Œè§†è§’é—´å¯¹åº”å…³ç³»æ–¹é¢ä»ç„¶å­˜åœ¨ä¸è¶³ã€‚ä¸ºå…¨é¢è¯„ä¼°MLLMsåœ¨å¤šè§†è§’åœºæ™¯æ¨ç†ä¸­çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†All-Angles Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«2100å¤šä¸ªäººå·¥ç²¾å¿ƒæ ‡æ³¨çš„å¤šè§†è§’é—®ç­”å¯¹çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“å‰çš„MLLMsåœ¨ä¸äººç±»è¯„ä¼°è€…çš„æ¯”è¾ƒä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œå¼ºè°ƒäº†å¢å¼ºå¤šè§†è§’æ„è¯†çš„å¿…è¦æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13805",
            "title": "LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration\n  Benchmark",
            "url": "https://huggingface.co/papers/2504.13805",
            "abstract": "Mobile GUI agents show promise in automating tasks but face generalization challenges in diverse real-world scenarios. Traditional approaches using pre-training or fine-tuning with massive datasets struggle with the diversity of mobile applications and user-specific tasks. We propose enhancing mobile GUI agent capabilities through human demonstrations, focusing on improving performance in unseen scenarios rather than pursuing universal generalization through larger datasets. To realize this paradigm, we introduce LearnGUI, the first comprehensive dataset specifically designed for studying demonstration-based learning in mobile GUI agents, comprising 2,252 offline tasks and 101 online tasks with high-quality human demonstrations. We further develop LearnAct, a sophisticated multi-agent framework that automatically extracts knowledge from demonstrations to enhance task completion. This framework integrates three specialized agents: DemoParser for knowledge extraction, KnowSeeker for relevant knowledge retrieval, and ActExecutor for demonstration-enhanced task execution. Our experimental results show significant performance gains in both offline and online evaluations. In offline assessments, a single demonstration improves model performance, increasing Gemini-1.5-Pro's accuracy from 19.3% to 51.7%. In online evaluations, our framework enhances UI-TARS-7B-SFT's task success rate from 18.1% to 32.8%. LearnAct framework and LearnGUI benchmark establish demonstration-based learning as a promising direction for more adaptable, personalized, and deployable mobile GUI agents.",
            "score": 6,
            "issue_id": 3358,
            "pub_date": "2025-04-18",
            "pub_date_card": {
                "ru": "18 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 18",
                "zh": "4æœˆ18æ—¥"
            },
            "hash": "56c81be35c66aa14",
            "authors": [
                "Guangyi Liu",
                "Pengxiang Zhao",
                "Liang Liu",
                "Zhiming Chen",
                "Yuxiang Chai",
                "Shuai Ren",
                "Hao Wang",
                "Shibo He",
                "Wenchao Meng"
            ],
            "affiliations": [
                "Zhejiang University Hangzhou, China",
                "vivo AI Lab Hangzhou, China",
                "vivo AI Lab ShenZhen, China"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13805.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#dataset",
                    "#transfer_learning",
                    "#agents"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ğ½Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LearnGUI Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº LearnAct, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ°Ğº Ğ² Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²."
                },
                "en": {
                    "title": "Empowering Mobile GUI Agents with Human Demonstrations",
                    "desc": "This paper addresses the challenges faced by mobile GUI agents in generalizing across diverse real-world applications. It critiques traditional methods that rely on large datasets for pre-training or fine-tuning, which often fall short in user-specific tasks. The authors propose a novel approach that leverages human demonstrations to improve the performance of these agents in unseen scenarios. They introduce LearnGUI, a dataset for demonstration-based learning, and LearnAct, a multi-agent framework that enhances task execution by extracting and utilizing knowledge from these demonstrations, leading to significant performance improvements in both offline and online tasks."
                },
                "zh": {
                    "title": "åŸºäºç¤ºèŒƒå­¦ä¹ çš„ç§»åŠ¨GUIä»£ç†æ–°æ–¹å‘",
                    "desc": "ç§»åŠ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†åœ¨è‡ªåŠ¨åŒ–ä»»åŠ¡æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨å¤šæ ·åŒ–çš„ç°å®åœºæ™¯ä¸­é¢ä¸´æ³›åŒ–æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºä½¿ç”¨å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒæˆ–å¾®è°ƒï¼Œéš¾ä»¥åº”å¯¹ç§»åŠ¨åº”ç”¨å’Œç”¨æˆ·ç‰¹å®šä»»åŠ¡çš„å¤šæ ·æ€§ã€‚æˆ‘ä»¬æå‡ºé€šè¿‡äººç±»ç¤ºèŒƒæ¥å¢å¼ºç§»åŠ¨GUIä»£ç†çš„èƒ½åŠ›ï¼Œé‡ç‚¹æ”¹å–„åœ¨æœªè§åœºæ™¯ä¸­çš„è¡¨ç°ï¼Œè€Œä¸æ˜¯é€šè¿‡æ›´å¤§çš„æ•°æ®é›†è¿½æ±‚æ™®éæ³›åŒ–ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†LearnGUIï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºç ”ç©¶åŸºäºç¤ºèŒƒå­¦ä¹ çš„ç§»åŠ¨GUIä»£ç†çš„ç»¼åˆæ•°æ®é›†ï¼ŒåŒ…å«2252ä¸ªç¦»çº¿ä»»åŠ¡å’Œ101ä¸ªåœ¨çº¿ä»»åŠ¡ï¼Œé…æœ‰é«˜è´¨é‡çš„äººç±»ç¤ºèŒƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15217",
            "title": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models",
            "url": "https://huggingface.co/papers/2504.15217",
            "abstract": "We present Distributional RewArds for Generative OptimizatioN (DRAGON), a versatile framework for fine-tuning media generation models towards a desired outcome. Compared with traditional reinforcement learning with human feedback (RLHF) or pairwise preference approaches such as direct preference optimization (DPO), DRAGON is more flexible. It can optimize reward functions that evaluate either individual examples or distributions of them, making it compatible with a broad spectrum of instance-wise, instance-to-distribution, and distribution-to-distribution rewards. Leveraging this versatility, we construct novel reward functions by selecting an encoder and a set of reference examples to create an exemplar distribution. When cross-modality encoders such as CLAP are used, the reference examples may be of a different modality (e.g., text versus audio). Then, DRAGON gathers online and on-policy generations, scores them to construct a positive demonstration set and a negative set, and leverages the contrast between the two sets to maximize the reward. For evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20 different reward functions, including a custom music aesthetics model, CLAP score, Vendi diversity, and Frechet audio distance (FAD). We further compare instance-wise (per-song) and full-dataset FAD settings while ablating multiple FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an 81.45% average win rate. Moreover, reward functions based on exemplar sets indeed enhance generations and are comparable to model-based rewards. With an appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality win rate without training on human preference annotations. As such, DRAGON exhibits a new approach to designing and optimizing reward functions for improving human-perceived quality. Sound examples at https://ml-dragon.github.io/web.",
            "score": 4,
            "issue_id": 3359,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 21",
                "zh": "4æœˆ21æ—¥"
            },
            "hash": "31ab92756cb919ff",
            "authors": [
                "Yatong Bai",
                "Jonah Casebeer",
                "Somayeh Sojoudi",
                "Nicholas J. Bryan"
            ],
            "affiliations": [
                "Adobe Research",
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15217.jpg",
            "data": {
                "categories": [
                    "#audio",
                    "#rlhf",
                    "#diffusion",
                    "#optimization",
                    "#training"
                ],
                "emoji": "ğŸ‰",
                "ru": {
                    "title": "DRAGON: Ğ³Ğ¸Ğ±ĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "DRAGON - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ĞµĞ´Ğ¸Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², DRAGON Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ ĞºĞ°Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸. DRAGON Ğ±Ñ‹Ğ» Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° 20 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑÑ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "DRAGON: Revolutionizing Reward Optimization for Media Generation",
                    "desc": "The paper introduces DRAGON, a framework designed to enhance media generation models by fine-tuning them with flexible reward functions. Unlike traditional methods like reinforcement learning with human feedback, DRAGON can optimize rewards based on individual examples or distributions, allowing for a wider range of applications. It constructs novel reward functions using encoders and reference examples, which can come from different modalities, such as text and audio. The results show that DRAGON significantly improves generation quality, achieving high win rates in both human evaluations and various reward settings without relying on human preference annotations."
                },
                "zh": {
                    "title": "DRAGONï¼šçµæ´»ä¼˜åŒ–ç”Ÿæˆæ¨¡å‹çš„å¥–åŠ±æ¡†æ¶",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDRAGONçš„æ¡†æ¶ï¼Œç”¨äºå¾®è°ƒåª’ä½“ç”Ÿæˆæ¨¡å‹ä»¥å®ç°æœŸæœ›çš„ç»“æœã€‚ä¸ä¼ ç»Ÿçš„åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æˆ–æˆå¯¹åå¥½æ–¹æ³•ç›¸æ¯”ï¼ŒDRAGONæ›´åŠ çµæ´»ï¼Œèƒ½å¤Ÿä¼˜åŒ–è¯„ä¼°å•ä¸ªç¤ºä¾‹æˆ–å…¶åˆ†å¸ƒçš„å¥–åŠ±å‡½æ•°ã€‚é€šè¿‡é€‰æ‹©ç¼–ç å™¨å’Œå‚è€ƒç¤ºä¾‹é›†ï¼ŒDRAGONæ„å»ºäº†æ–°é¢–çš„å¥–åŠ±å‡½æ•°ï¼Œå¹¶åˆ©ç”¨åœ¨çº¿ç”Ÿæˆçš„å¯¹æ¯”é›†æ¥æœ€å¤§åŒ–å¥–åŠ±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRAGONåœ¨å¤šç§å¥–åŠ±å‡½æ•°ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå†…å®¹çš„è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14655",
            "title": "LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient\n  Training of Code LLMs",
            "url": "https://huggingface.co/papers/2504.14655",
            "abstract": "We introduce LeetCodeDataset, a high-quality benchmark for evaluating and training code-generation models, addressing two key challenges in LLM research: the lack of reasoning-focused coding benchmarks and self-contained training testbeds. By curating LeetCode Python problems with rich metadata, broad coverage, 100+ test cases per problem, and temporal splits (pre/post July 2024), our dataset enables contamination-free evaluation and efficient supervised fine-tuning (SFT). Experiments show reasoning models significantly outperform non-reasoning counterparts, while SFT with only 2.6K model-generated solutions achieves performance comparable to 110K-sample counterparts. The dataset and evaluation framework are available on Hugging Face and Github.",
            "score": 4,
            "issue_id": 3358,
            "pub_date": "2025-04-20",
            "pub_date_card": {
                "ru": "20 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 20",
                "zh": "4æœˆ20æ—¥"
            },
            "hash": "4ca6f62bd5518a9d",
            "authors": [
                "Yunhui Xia",
                "Wei Shen",
                "Yan Wang",
                "Jason Klein Liu",
                "Huifeng Sun",
                "Siyue Wu",
                "Jian Hu",
                "Xiaolong Xu"
            ],
            "affiliations": [],
            "pdf_title_img": "assets/pdf/title_img/2504.14655.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#reasoning",
                    "#data",
                    "#optimization",
                    "#benchmark",
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LeetCodeDataset: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜ Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "LeetCodeDataset Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…, Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ LeetCode Ğ½Ğ° Python Ñ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¼ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ¾Ğ¼ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "LeetCodeDataset: Elevating Code Generation with Reasoning!",
                    "desc": "The paper presents LeetCodeDataset, a new benchmark designed for assessing and training code-generation models in machine learning. It addresses the challenges of lacking reasoning-focused coding benchmarks and the need for self-contained training environments. The dataset includes a variety of curated Python problems with extensive metadata and numerous test cases, allowing for effective supervised fine-tuning. Results indicate that models utilizing reasoning outperform those that do not, and even a small number of model-generated solutions can yield competitive performance."
                },
                "zh": {
                    "title": "LeetCodeDatasetï¼šæ¨ç†é©±åŠ¨çš„ä»£ç ç”ŸæˆåŸºå‡†",
                    "desc": "æˆ‘ä»¬ä»‹ç»äº†LeetCodeDatasetï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„åŸºå‡†æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°å’Œè®­ç»ƒä»£ç ç”Ÿæˆæ¨¡å‹ï¼Œè§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹ç ”ç©¶ä¸­çš„ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šç¼ºä¹ä»¥æ¨ç†ä¸ºé‡ç‚¹çš„ç¼–ç åŸºå‡†å’Œè‡ªåŒ…å«çš„è®­ç»ƒæµ‹è¯•ç¯å¢ƒã€‚é€šè¿‡æ•´ç†LeetCodeçš„Pythoné—®é¢˜ï¼Œæä¾›ä¸°å¯Œçš„å…ƒæ•°æ®ã€å¹¿æ³›çš„è¦†ç›–èŒƒå›´ã€æ¯ä¸ªé—®é¢˜è¶…è¿‡100ä¸ªæµ‹è¯•ç”¨ä¾‹ä»¥åŠæ—¶é—´åˆ†å‰²ï¼ˆ2024å¹´7æœˆå‰åï¼‰ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†å®ç°äº†æ— æ±¡æŸ“è¯„ä¼°å’Œé«˜æ•ˆçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œæ¨ç†æ¨¡å‹çš„è¡¨ç°æ˜¾è‘—ä¼˜äºéæ¨ç†æ¨¡å‹ï¼Œè€Œä»…ä½¿ç”¨2.6Kä¸ªæ¨¡å‹ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆè¿›è¡ŒSFTçš„æ€§èƒ½ä¸110Kæ ·æœ¬çš„æ¨¡å‹ç›¸å½“ã€‚è¯¥æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶å·²åœ¨Hugging Faceå’ŒGithubä¸Šå‘å¸ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14239",
            "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to\n  Deliberative Reasoners",
            "url": "https://huggingface.co/papers/2504.14239",
            "abstract": "Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoning templates, which may result in reasoning that is not sufficiently robust and adaptive for complex GUI environments. Meanwhile, some existing agents continue to operate as Reactive Actors, relying primarily on implicit reasoning that may lack sufficient depth for GUI tasks demanding planning and error recovery. We argue that advancing these agents requires a shift from reactive acting towards acting based on deliberate reasoning. To facilitate this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed through our Actor2Reasoner framework, a reasoning-centric, two-stage training approach designed to progressively evolve agents from Reactive Actors to Deliberative Reasoners. The first stage, Reasoning Injection, focuses on establishing a basic reasoner. We employ Spatial Reasoning Distillation to transfer cross-modal spatial reasoning capabilities from teacher models to MLLMs through trajectories with explicit reasoning steps, enabling models to integrate GUI visual-spatial information with logical reasoning before action generation. The second stage, Deliberation Enhancement, refines the basic reasoner into a deliberative one using Reinforcement Learning. This stage introduces two approaches: Sub-goal Guidance, which rewards models for generating accurate intermediate sub-goals, and Error Recovery Scenario Construction, which creates failure-and-recovery training scenarios from identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves strong performance in GUI grounding and trajectory tasks. Resources at https://github.com/Reallm-Labs/InfiGUI-R1.",
            "score": 4,
            "issue_id": 3359,
            "pub_date": "2025-04-19",
            "pub_date_card": {
                "ru": "19 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 19",
                "zh": "4æœˆ19æ—¥"
            },
            "hash": "74a4065180d1bcb9",
            "authors": [
                "Yuhang Liu",
                "Pengxiang Li",
                "Congkai Xie",
                "Xavier Hu",
                "Xiaotian Han",
                "Shengyu Zhang",
                "Hongxia Yang",
                "Fei Wu"
            ],
            "affiliations": [
                "Dalian University of Technology",
                "Reallm Labs",
                "The Hong Kong Polytechnic University",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14239.jpg",
            "data": {
                "categories": [
                    "#games",
                    "#reasoning",
                    "#multimodal",
                    "#rl",
                    "#agents",
                    "#training"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞÑ‚ Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ñ‘Ñ€Ğ¾Ğ² Ğº Ğ¾Ğ±Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ² GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° (GUI) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Actor2Reasoner, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ´ĞµĞ»Ğ¸Ğ±ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿, 'Ğ’Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¾Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿, 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ', Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº."
                },
                "en": {
                    "title": "From Reactive to Deliberative: Advancing GUI Agents with InfiGUI-R1",
                    "desc": "This paper presents InfiGUI-R1, a Multimodal Large Language Model (MLLM) designed to enhance Graphical User Interface (GUI) agents by shifting from reactive to deliberative reasoning. The proposed Actor2Reasoner framework employs a two-stage training process: the first stage focuses on Reasoning Injection, where spatial reasoning capabilities are transferred to the MLLM, allowing it to better understand GUI visual-spatial information. The second stage, Deliberation Enhancement, uses Reinforcement Learning to refine the agent's reasoning abilities by rewarding the generation of accurate sub-goals and constructing training scenarios for error recovery. Experimental results demonstrate that InfiGUI-R1 significantly improves performance in tasks involving GUI grounding and trajectory planning."
                },
                "zh": {
                    "title": "ä»ååº”åˆ°æ·±æ€ç†Ÿè™‘ï¼šæå‡GUIä»£ç†çš„æ¨ç†èƒ½åŠ›",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æä¾›äº†åŠ¨åŠ›ï¼Œå±•ç°äº†åœ¨è®¡ç®—è®¾å¤‡ä¸Šè‡ªåŠ¨åŒ–ä»»åŠ¡çš„æ½œåŠ›ã€‚å½“å‰è®¸å¤šæ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨è®¾è®¡çš„æ¨ç†æ¨¡æ¿ï¼Œè¿™å¯èƒ½å¯¼è‡´åœ¨å¤æ‚çš„GUIç¯å¢ƒä¸­æ¨ç†ä¸å¤Ÿç¨³å¥å’Œé€‚åº”ã€‚æˆ‘ä»¬æå‡ºInfiGUI-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºMLLMçš„GUIä»£ç†ï¼Œé€šè¿‡Actor2Reasoneræ¡†æ¶å¼€å‘ï¼Œæ—¨åœ¨å°†ä»£ç†ä»ååº”å‹è¡Œä¸ºè€…è½¬å˜ä¸ºæ·±æ€ç†Ÿè™‘çš„æ¨ç†è€…ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šæ¨ç†æ³¨å…¥å’Œæ·±æ€ç†Ÿè™‘å¢å¼ºï¼Œå®éªŒç»“æœè¡¨æ˜InfiGUI-R1åœ¨GUIä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.15047",
            "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary\n  Quality-Diversity Search",
            "url": "https://huggingface.co/papers/2504.15047",
            "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities but are susceptible to adversarial prompts that exploit vulnerabilities to produce unsafe or biased outputs. Existing red-teaming methods often face scalability challenges, resource-intensive requirements, or limited diversity in attack strategies. We propose RainbowPlus, a novel red-teaming framework rooted in evolutionary computation, enhancing adversarial prompt generation through an adaptive quality-diversity (QD) search that extends classical evolutionary algorithms like MAP-Elites with innovations tailored for language models. By employing a multi-element archive to store diverse high-quality prompts and a comprehensive fitness function to evaluate multiple prompts concurrently, RainbowPlus overcomes the constraints of single-prompt archives and pairwise comparisons in prior QD methods like Rainbow Teaming. Experiments comparing RainbowPlus to QD methods across six benchmark datasets and four open-source LLMs demonstrate superior attack success rate (ASR) and diversity (Diverse-Score approx 0.84), generating up to 100 times more unique prompts (e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine state-of-the-art methods on the HarmBench dataset with twelve LLMs (ten open-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%, surpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours). Our open-source implementation fosters further advancements in LLM safety, offering a scalable tool for vulnerability assessment. Code and resources are publicly available at https://github.com/knoveleng/rainbowplus, supporting reproducibility and future research in LLM red-teaming.",
            "score": 3,
            "issue_id": 3357,
            "pub_date": "2025-04-21",
            "pub_date_card": {
                "ru": "21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 21",
                "zh": "4æœˆ21æ—¥"
            },
            "hash": "834479c504a9e5f7",
            "authors": [
                "Quy-Anh Dang",
                "Chris Ngo",
                "Truong-Son Hy"
            ],
            "affiliations": [
                "Knovel Engineering Lab, Singapore",
                "University of Alabama at Birmingham, United States",
                "VNU University of Science, Vietnam"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.15047.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#benchmark",
                    "#data",
                    "#open_source",
                    "#dataset"
                ],
                "emoji": "ğŸŒˆ",
                "ru": {
                    "title": "RainbowPlus: Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RainbowPlus - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑÑ…. RainbowPlus Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ²Ğ¾ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ RainbowPlus Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ LLM, ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒÑ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "RainbowPlus: Evolving Safer Language Models with Diverse Adversarial Prompts",
                    "desc": "This paper introduces RainbowPlus, a new framework for testing the safety of Large Language Models (LLMs) against adversarial prompts. It uses evolutionary computation techniques to generate diverse and high-quality prompts that can exploit vulnerabilities in LLMs. By employing a multi-element archive and a comprehensive fitness function, RainbowPlus significantly improves the efficiency and effectiveness of prompt generation compared to previous methods. Experiments show that it achieves a higher attack success rate and generates many more unique prompts, making it a valuable tool for enhancing LLM safety."
                },
                "zh": {
                    "title": "RainbowPlusï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨æ€§çš„åˆ›æ–°çº¢é˜Ÿæ¡†æ¶",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·æœ‰å‡ºè‰²çš„èƒ½åŠ›ï¼Œä½†å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æç¤ºçš„å½±å“ï¼Œè¿™äº›æç¤ºåˆ©ç”¨äº†æ¨¡å‹çš„è„†å¼±æ€§ï¼Œå¯¼è‡´ä¸å®‰å…¨æˆ–æœ‰åè§çš„è¾“å‡ºã€‚ç°æœ‰çš„çº¢é˜Ÿæ–¹æ³•å¸¸å¸¸é¢ä¸´å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€èµ„æºå¯†é›†å‹è¦æ±‚æˆ–æ”»å‡»ç­–ç•¥çš„å¤šæ ·æ€§æœ‰é™ã€‚æˆ‘ä»¬æå‡ºäº†RainbowPlusï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè¿›åŒ–è®¡ç®—çš„æ–°å‹çº¢é˜Ÿæ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”è´¨é‡å¤šæ ·æ€§ï¼ˆQDï¼‰æœç´¢å¢å¼ºå¯¹æŠ—æ€§æç¤ºç”Ÿæˆï¼Œæ‰©å±•äº†ç»å…¸çš„è¿›åŒ–ç®—æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRainbowPlusåœ¨æ”»å‡»æˆåŠŸç‡å’Œå¤šæ ·æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”Ÿæˆçš„ç‹¬ç‰¹æç¤ºæ•°é‡æ˜¾è‘—å¢åŠ ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨æ€§è¯„ä¼°ä¸­çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.14717",
            "title": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry",
            "url": "https://huggingface.co/papers/2504.14717",
            "abstract": "We introduce TAPIP3D, a novel approach for long-term 3D point tracking in monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized spatio-temporal feature clouds, leveraging depth and camera motion information to lift 2D video features into a 3D world space where camera motion is effectively canceled. TAPIP3D iteratively refines multi-frame 3D motion estimates within this stabilized representation, enabling robust tracking over extended periods. To manage the inherent irregularities of 3D point distributions, we propose a Local Pair Attention mechanism. This 3D contextualization strategy effectively exploits spatial relationships in 3D, forming informative feature neighborhoods for precise 3D trajectory estimation. Our 3D-centric approach significantly outperforms existing 3D point tracking methods and even enhances 2D tracking accuracy compared to conventional 2D pixel trackers when accurate depth is available. It supports inference in both camera coordinates (i.e., unstabilized) and world coordinates, and our results demonstrate that compensating for camera motion improves tracking performance. Our approach replaces the conventional 2D square correlation neighborhoods used in prior 2D and 3D trackers, leading to more robust and accurate results across various 3D point tracking benchmarks. Project Page: https://tapip3d.github.io",
            "score": 2,
            "issue_id": 3360,
            "pub_date": "2025-04-20",
            "pub_date_card": {
                "ru": "20 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 20",
                "zh": "4æœˆ20æ—¥"
            },
            "hash": "b8ab0510eb563950",
            "authors": [
                "Bowei Zhang",
                "Lei Ke",
                "Adam W. Harley",
                "Katerina Fragkiadaki"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Peking University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.14717.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#3d",
                    "#long_context"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ 3D Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²",
                    "desc": "TAPIP3D - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ 3D Ñ‚Ğ¾Ñ‡ĞµĞº Ğ² Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… RGB Ğ¸ RGB-D Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ ĞºĞ°Ğ¼ĞµÑ€Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. TAPIP3D Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ² Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ."
                },
                "en": {
                    "title": "Revolutionizing 3D Point Tracking with TAPIP3D",
                    "desc": "TAPIP3D is a new method designed for tracking 3D points over long periods using monocular RGB and RGB-D videos. It transforms videos into stabilized spatio-temporal feature clouds, which helps to eliminate the effects of camera motion. The method refines 3D motion estimates through a Local Pair Attention mechanism, allowing for better handling of irregular 3D point distributions. TAPIP3D not only improves the accuracy of 3D tracking but also enhances 2D tracking when depth information is available, outperforming traditional tracking methods."
                },
                "zh": {
                    "title": "TAPIP3Dï¼šæå‡3Dç‚¹è·Ÿè¸ªçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•TAPIP3Dï¼Œç”¨äºåœ¨å•ç›®RGBå’ŒRGB-Dè§†é¢‘ä¸­è¿›è¡Œé•¿æœŸ3Dç‚¹è·Ÿè¸ªã€‚TAPIP3Dé€šè¿‡æ·±åº¦å’Œç›¸æœºè¿åŠ¨ä¿¡æ¯ï¼Œå°†è§†é¢‘è¡¨ç¤ºä¸ºç¨³å®šçš„æ—¶ç©ºç‰¹å¾äº‘ï¼Œä»è€Œå°†2Dè§†é¢‘ç‰¹å¾æå‡åˆ°3Dä¸–ç•Œç©ºé—´ã€‚è¯¥æ–¹æ³•é€šè¿‡å±€éƒ¨å¯¹æ³¨æ„åŠ›æœºåˆ¶å¤„ç†3Dç‚¹åˆ†å¸ƒçš„ä¸è§„åˆ™æ€§ï¼Œæœ‰æ•ˆåˆ©ç”¨3Dç©ºé—´å…³ç³»ï¼Œå½¢æˆä¿¡æ¯ä¸°å¯Œçš„ç‰¹å¾é‚»åŸŸï¼Œä»¥å®ç°ç²¾ç¡®çš„3Dè½¨è¿¹ä¼°è®¡ã€‚æˆ‘ä»¬çš„3Dä¸­å¿ƒæ–¹æ³•åœ¨å¤šä¸ª3Dç‚¹è·Ÿè¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å‡†ç¡®æ·±åº¦å¯ç”¨æ—¶æé«˜äº†2Dè·Ÿè¸ªç²¾åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.13941",
            "title": "NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning",
            "url": "https://huggingface.co/papers/2504.13941",
            "abstract": "Large Language Models (LLMs) have shown strong reasoning capabilities, particularly when enhanced through Reinforcement Learning (RL). While prior work has successfully applied RL to mathematical reasoning -- where rules and correctness are well-defined -- generalizing these methods to broader reasoning domains remains challenging due to limited data, the lack of verifiable reward structures, and diverse task requirements. In this work, we propose NEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain corpora, including both synthetic and real-world question-answer pairs, into RL training to improve generalization across diverse reasoning tasks. NEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from varied sources spanning STEM, humanities, social sciences, etc.; (2) applying structured templates (e.g., multiple-choice and open-ended) to control answer-space complexity; (3) filtering for verifiable answers; and (4) optimizing data blending strategies that utilizes data from multiple sources effectively. Our approach enables scalable and verifiable reward modeling beyond mathematics and demonstrates improved accuracies on both math (MATH-500: +30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%, GPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover, NEMOTRON-CROSSTHINK exhibits significantly improved response efficiency -- using 28% fewer tokens for correct answers -- highlighting more focused and effective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that integrating multi-domain, multi-format data in RL leads to more accurate, efficient, and generalizable LLMs.",
            "score": 2,
            "issue_id": 3357,
            "pub_date": "2025-04-15",
            "pub_date_card": {
                "ru": "15 Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
                "en": "April 15",
                "zh": "4æœˆ15æ—¥"
            },
            "hash": "df91b810ea243fdc",
            "authors": [
                "Syeda Nahida Akter",
                "Shrimai Prabhumoye",
                "Matvei Novikov",
                "Seungju Han",
                "Ying Lin",
                "Evelina Bakhturi",
                "Eric Nyberg",
                "Yejin Choi",
                "Mostofa Patwary",
                "Mohammad Shoeybi",
                "Bryan Catanzaro"
            ],
            "affiliations": [
                "Boston University",
                "Carnegie Mellon University",
                "NVIDIA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.13941.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#reasoning",
                    "#data",
                    "#rl",
                    "#transfer_learning",
                    "#math"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞœĞ½Ğ¾Ğ³Ğ¾Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ NEMOTRON-CROSSTHINK - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ½ĞµĞ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. NEMOTRON-CROSSTHINK Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing LLM Reasoning with Multi-Domain Reinforcement Learning",
                    "desc": "This paper introduces NEMOTRON-CROSSTHINK, a new framework that enhances the reasoning abilities of Large Language Models (LLMs) using Reinforcement Learning (RL). It tackles the challenge of generalizing RL methods across various reasoning tasks by incorporating diverse datasets from multiple domains, including STEM and humanities. The framework employs structured templates to manage answer complexity and ensures the use of verifiable answers, leading to improved reward modeling. As a result, NEMOTRON-CROSSTHINK achieves significant accuracy gains on both mathematical and non-mathematical reasoning tasks while also improving response efficiency by reducing token usage."
                },
                "zh": {
                    "title": "å¤šé¢†åŸŸæ•°æ®åŠ©åŠ›æ¨ç†èƒ½åŠ›æå‡",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œå¢å¼ºã€‚ä»¥å¾€çš„ç ”ç©¶æˆåŠŸåœ°å°†RLåº”ç”¨äºæ•°å­¦æ¨ç†ï¼Œä½†å°†è¿™äº›æ–¹æ³•æ¨å¹¿åˆ°æ›´å¹¿æ³›çš„æ¨ç†é¢†åŸŸä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†NEMOTRON-CROSSTHINKæ¡†æ¶ï¼Œç³»ç»Ÿåœ°å°†å¤šé¢†åŸŸè¯­æ–™åº“çº³å…¥RLè®­ç»ƒï¼Œä»¥æé«˜åœ¨ä¸åŒæ¨ç†ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡å¤šæ ·åŒ–æ•°æ®æºã€ç»“æ„åŒ–æ¨¡æ¿ã€å¯éªŒè¯ç­”æ¡ˆè¿‡æ»¤å’Œä¼˜åŒ–æ•°æ®æ··åˆç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æ•°å­¦å’Œéæ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„å‡†ç¡®æ€§å’Œå“åº”æ•ˆç‡ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-04-21.html",
    "link_next": "2025-04-23.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "21.04",
        "en": "04/21",
        "zh": "4æœˆ21æ—¥"
    },
    "short_date_next": {
        "ru": "23.04",
        "en": "04/23",
        "zh": "4æœˆ23æ—¥"
    },
    "categories": {
        "#dataset": 4,
        "#data": 3,
        "#benchmark": 8,
        "#agents": 5,
        "#cv": 0,
        "#rl": 5,
        "#rlhf": 2,
        "#rag": 0,
        "#plp": 0,
        "#inference": 0,
        "#3d": 4,
        "#audio": 1,
        "#video": 1,
        "#multimodal": 5,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 6,
        "#robotics": 0,
        "#agi": 0,
        "#games": 2,
        "#interpretability": 0,
        "#reasoning": 7,
        "#transfer_learning": 2,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 2,
        "#optimization": 5,
        "#survey": 2,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æ„å»ºæœ‰æ•ˆæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†çš„å…³é”®å› ç´ ï¼šæ•°æ®è´¨é‡å’Œå¤šæ ·æ€§ã€‚éšç€å¼€æºæ•°æ®é›†çš„å¢åŠ ï¼Œè‡ªåŠ¨é€‰æ‹©é«˜è´¨é‡ä¸”å¤šæ ·çš„å­é›†å˜å¾—é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å®ä¾‹è´¨é‡ï¼Œä½¿ç”¨å¯å‘å¼è§„åˆ™ç»´æŒå¤šæ ·æ€§ï¼Œä½† often æ•ˆæœä¸ä½³ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºæ ‡ç­¾å›¾æ¥æ¨¡æ‹Ÿè¯­ä¹‰ç©ºé—´ï¼Œå¹¶åŸºäºå›¾ä¸­çš„ä¿¡æ¯åˆ†å¸ƒé‡åŒ–å¤šæ ·æ€§ã€‚å®éªŒæ˜¾ç¤ºï¼Œè¿™ç§æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†å’ŒåŸºç¡€æ¨¡å‹ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚",
        "title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing\n  Information Gain in Semantic Space",
        "pinyin": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†æ„å»ºæœ‰æ•ˆæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†çš„å…³é”®å› ç´ ï¼šæ•°æ®è´¨é‡å’Œå¤šæ ·æ€§ã€‚\nzhÃ¨ piÄn wÃ©n zhÄng tÇo lÃ¹n le gÃ²u jiÃ n yÇ’u xiÃ o zhÇ lÃ¬ng tiÃ¡o zhÄ›ng shÃ¹ jÃ¹ de guÇn jiÃ n yÄ«n sÃ¹: shÃ¹ jÃ¹ zhÃ¬ liÃ ng hÃ© duÅ yÃ ng xÃ¬ng.\n\néšç€å¼€æºæ•°æ®é›†çš„å¢åŠ ï¼Œè‡ªåŠ¨é€‰æ‹©é«˜è´¨é‡ä¸”å¤šæ ·çš„å­é›†å˜å¾—é‡è¦ã€‚\nsuÃ­ zhe kÄi yuÃ¡n shÃ¹ jÃ¹ jÃ­ de zÄ“ng jiÄ, zÃ¬ dÃ²ng xuÇn zÃ© gÄo zhÃ¬ liÃ ng qiÄ› duÅ yÃ ng de zÇ jÃ­ biÃ n de zhÃ²ng yÃ o.\n\nç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å®ä¾‹è´¨é‡ï¼Œä½¿ç”¨å¯å‘å¼è§„åˆ™ç»´æŒå¤šæ ·æ€§ï¼Œä½† often æ•ˆæœä¸ä½³ã€‚\nxiÃ n yÇ’u fÄng fÇ zhÇ” yÃ o guÄn zhÃ¹ shÃ­ lÃ¬ zhÃ¬ liÃ ng, shÇ yÃ²ng qÇ fÄ shÃ¬ guÄ« zÃ© wÃ©i chÃ­ duÅ yÃ ng xÃ¬ng, dÃ n often xiÃ o guÇ’ bÃ¹ jiÄ.\n\nä½œè€…æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºæ ‡ç­¾å›¾æ¥æ¨¡æ‹Ÿè¯­ä¹‰ç©ºé—´ï¼Œå¹¶åŸºäºå›¾ä¸­çš„ä¿¡æ¯åˆ†å¸ƒé‡åŒ–å¤šæ ·æ€§ã€‚\nzuÃ² zhÄ› tÃ­ chÅ« le yÄ« zhÇ’ng xÄ«n fÄng fÇ, tÅng guÃ² gÃ²u jiÃ n biÄo qiÄn tÃº lÃ¡i mÃ³ nÇ yÇ” yÃ¬ kÅng jiÄn, bÃ¬ng jÄ« yÃº tÃº zhÅng de xÃ¬n xÄ« fÄ“n bÃ¹ liÃ ng huÃ  duÅ yÃ ng xÃ¬ng.\n\nå®éªŒæ˜¾ç¤ºï¼Œè¿™ç§æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†å’ŒåŸºç¡€æ¨¡å‹ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚\nshÃ­ yÃ n xiÇn shÃ¬, zhÃ¨ zhÇ’ng fÄng fÇ zÃ i duÅ gÃ¨ shÃ¹ jÃ¹ jÃ­ hÃ© jÄ« chÇ” mÃ³ xÃ­ng shÃ ng dÅu yÅu yÃº xiÃ n yÇ’u fÄng fÇ.",
        "vocab": "[\n    {\"word\": \"æ„å»º\", \"pinyin\": \"gÃ²ujiÃ n\", \"trans\": \"construct\"},\n    {\"word\": \"æœ‰æ•ˆ\", \"pinyin\": \"yÇ’uxiÃ o\", \"trans\": \"effective\"},\n    {\"word\": \"æŒ‡ä»¤\", \"pinyin\": \"zhÇlÃ¬ng\", \"trans\": \"instruction\"},\n    {\"word\": \"è°ƒæ•´\", \"pinyin\": \"tiÃ¡ozhÄ›ng\", \"trans\": \"adjust\"},\n    {\"word\": \"æ•°æ®é›†\", \"pinyin\": \"shÃ¹jÃ¹jÃ­\", \"trans\": \"dataset\"},\n    {\"word\": \"å…³é”®å› ç´ \", \"pinyin\": \"guÇnjiÃ n yÄ«nsÃ¹\", \"trans\": \"key factors\"},\n    {\"word\": \"è´¨é‡\", \"pinyin\": \"zhÃ¬liÃ ng\", \"trans\": \"quality\"},\n    {\"word\": \"å¤šæ ·æ€§\", \"pinyin\": \"duÅyÃ ngxÃ¬ng\", \"trans\": \"diversity\"},\n    {\"word\": \"å¼€æº\", \"pinyin\": \"kÄiyuÃ¡n\", \"trans\": \"open-source\"},\n    {\"word\": \"è‡ªåŠ¨\", \"pinyin\": \"zÃ¬dÃ²ng\", \"trans\": \"automatic\"},\n    {\"word\": \"é€‰æ‹©\", \"pinyin\": \"xuÇnzÃ©\", \"trans\": \"select\"},\n    {\"word\": \"å­é›†\", \"pinyin\": \"zÇjÃ­\", \"trans\": \"subset\"},\n    {\"word\": \"å˜å¾—\", \"pinyin\": \"biÃ ndÃ©\", \"trans\": \"become\"},\n    {\"word\": \"é‡è¦\", \"pinyin\": \"zhÃ²ngyÃ o\", \"trans\": \"important\"},\n    {\"word\": \"ç°æœ‰\", \"pinyin\": \"xiÃ nyÇ’u\", \"trans\": \"existing\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄngfÇ\", \"trans\": \"method\"},\n    {\"word\": \"ä¸»è¦\", \"pinyin\": \"zhÇ”yÃ o\", \"trans\": \"main\"},\n    {\"word\": \"å…³æ³¨\", \"pinyin\": \"guÄnzhÃ¹\", \"trans\": \"focus on\"},\n    {\"word\": \"å®ä¾‹\", \"pinyin\": \"shÃ­lÃ¬\", \"trans\": \"instance\"},\n    {\"word\": \"å¯å‘å¼\", \"pinyin\": \"qÇfÄshÃ¬\", \"trans\": \"heuristic\"},\n    {\"word\": \"è§„åˆ™\", \"pinyin\": \"guÄ«zÃ©\", \"trans\": \"rule\"},\n    {\"word\": \"ç»´æŒ\", \"pinyin\": \"wÃ©ichÃ­\", \"trans\": \"maintain\"},\n    {\"word\": \"æ•ˆæœ\", \"pinyin\": \"xiÃ oguÇ’\", \"trans\": \"effect\"},\n    {\"word\": \"ä¸ä½³\", \"pinyin\": \"bÃ¹jiÄ\", \"trans\": \"poor\"},\n    {\"word\": \"ä½œè€…\", \"pinyin\": \"zuÃ²zhÄ›\", \"trans\": \"author\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"æ–°æ–¹æ³•\", \"pinyin\": \"xÄ«n fÄngfÇ\", \"trans\": \"new method\"},\n    {\"word\": \"é€šè¿‡\", \"pinyin\": \"tÅngguÃ²\", \"trans\": \"through\"},\n    {\"word\": \"æ ‡ç­¾\", \"pinyin\": \"biÄoqiÄn\", \"trans\": \"label\"},\n    {\"word\": \"å›¾\", \"pinyin\": \"tÃº\", \"trans\": \"graph\"},\n    {\"word\": \"æ¨¡æ‹Ÿ\", \"pinyin\": \"mÃ³nÇ\", \"trans\": \"simulate\"},\n    {\"word\": \"è¯­ä¹‰\", \"pinyin\": \"yÇ”yÃ¬\", \"trans\": \"semantic\"},\n    {\"word\": \"ç©ºé—´\", \"pinyin\": \"kÅngjiÄn\", \"trans\": \"space\"},\n    {\"word\": \"åŸºäº\", \"pinyin\": \"jÄ«yÃº\", \"trans\": \"based on\"},\n    {\"word\": \"ä¿¡æ¯\", \"pinyin\": \"xÃ¬nxÄ«\", \"trans\": \"information\"},\n    {\"word\": \"åˆ†å¸ƒ\", \"pinyin\": \"fÄ“nbÃ¹\", \"trans\": \"distribution\"},\n    {\"word\": \"é‡åŒ–\", \"pinyin\": \"liÃ nghuÃ \", \"trans\": \"quantify\"},\n    {\"word\": \"å®éªŒ\", \"pinyin\": \"shÃ­yÃ n\", \"trans\": \"experiment\"},\n    {\"word\": \"æ˜¾ç¤º\", \"pinyin\": \"xiÇnshÃ¬\", \"trans\": \"show\"},\n    {\"word\": \"ä¼˜äº\", \"pinyin\": \"yÅuyÃº\", \"trans\": \"superior to\"},\n    {\"word\": \"åŸºç¡€\", \"pinyin\": \"jÄ«chÇ”\", \"trans\": \"foundation\"},\n    {\"word\": \"æ¨¡å‹\", \"pinyin\": \"mÃ³xÃ­ng\", \"trans\": \"model\"}\n]",
        "trans": "This article discusses the key factors in constructing an effective dataset for instruction tuning: data quality and diversity. As the number of open-source datasets increases, it becomes important to automatically select high-quality and diverse subsets. Existing methods primarily focus on instance quality, using heuristic rules to maintain diversity, but often with limited effectiveness. The authors propose a new method that constructs a label graph to simulate semantic space and quantifies diversity based on the distribution of information in the graph. Experiments show that this method outperforms existing methods across multiple datasets and base models.",
        "update_ts": "2025-04-21 09:12"
    }
}