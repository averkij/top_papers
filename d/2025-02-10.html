
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 22 papers. February 10.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">10 февраля</span> | <span id="title-articles-count">22 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-02-07.html">⬅️ <span id="prev-date">07.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-02-11.html">➡️ <span id="next-date">11.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-02.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'};
        let feedDateNext = {'ru': '11.02', 'en': '02/11', 'zh': '2月11日'};
        let feedDatePrev = {'ru': '07.02', 'en': '02/07', 'zh': '2月7日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.03032', 'title': 'Analyze Feature Flow to Enhance Interpretation and Steering in Language Models', 'url': 'https://huggingface.co/papers/2502.03032', 'abstract': 'We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations. Crucially, we demonstrate how these cross-layer feature maps facilitate direct steering of model behavior by amplifying or suppressing chosen features, achieving targeted thematic control in text generation. Together, our findings highlight the utility of a causal, cross-layer interpretability framework that not only clarifies how features develop through forward passes but also provides new means for transparent manipulation of large language models.', 'score': 53, 'issue_id': 2090, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '030f362f419e9eb4', 'authors': ['Daniil Laptev', 'Nikita Balagansky', 'Yaroslav Aksenov', 'Daniil Gavrilov'], 'affiliations': ['1T-Tech', 'Moscow Institute of Physics and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.03032.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#training'], 'emoji': '🧠', 'ru': {'title': 'Прозрачное управление языковыми моделями через межслойный анализ признаков', 'desc': 'Представлен новый подход к систематическому отображению признаков, обнаруженных разреженным автоэнкодером, между последовательными слоями больших языковых моделей. Метод использует технику косинусного сходства без данных для отслеживания эволюции признаков на каждом этапе. Это позволяет создавать подробные графы потоков развития признаков, обеспечивая детальную интерпретируемость и механистическое понимание вычислений модели. Исследование демонстрирует, как межслойные карты признаков могут использоваться для прямого управления поведением модели путем усиления или подавления выбранных признаков.'}, 'en': {'title': 'Mapping and Manipulating Features in Language Models', 'desc': 'This paper presents a novel method for analyzing features in large language models using sparse autoencoders. It employs a data-free cosine similarity approach to track the evolution of features across different layers of the model. The resulting flow graphs provide detailed insights into how features change and interact during processing. Additionally, the method allows for targeted manipulation of model behavior by adjusting specific features, enhancing interpretability and control in text generation tasks.'}, 'zh': {'title': '特征映射：引导大型语言模型的新方法', 'desc': '本文提出了一种新方法，系统地映射稀疏自编码器在大型语言模型中各层发现的特征。我们使用无数据的余弦相似度技术，追踪特定特征在每个阶段的持续、转变或首次出现。这种方法生成了特征演变的细粒度流图，增强了模型计算的可解释性和机制洞察力。我们还展示了如何通过放大或抑制选定特征，直接引导模型行为，实现文本生成中的主题控制。'}}}, {'id': 'https://huggingface.co/papers/2502.03544', 'title': 'Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2', 'url': 'https://huggingface.co/papers/2502.03544', 'abstract': 'We present AlphaGeometry2, a significantly improved version of AlphaGeometry introduced in Trinh et al. (2024), which has now surpassed an average gold medalist in solving Olympiad geometry problems. To achieve this, we first extend the original AlphaGeometry language to tackle harder problems involving movements of objects, and problems containing linear equations of angles, ratios, and distances. This, together with other additions, has markedly improved the coverage rate of the AlphaGeometry language on International Math Olympiads (IMO) 2000-2024 geometry problems from 66% to 88%. The search process of AlphaGeometry2 has also been greatly improved through the use of Gemini architecture for better language modeling, and a novel knowledge-sharing mechanism that combines multiple search trees. Together with further enhancements to the symbolic engine and synthetic data generation, we have significantly boosted the overall solving rate of AlphaGeometry2 to 84% for all geometry problems over the last 25 years, compared to 54% previously. AlphaGeometry2 was also part of the system that achieved silver-medal standard at IMO 2024 https://dpmd.ai/imo-silver. Last but not least, we report progress towards using AlphaGeometry2 as a part of a fully automated system that reliably solves geometry problems directly from natural language input.', 'score': 33, 'issue_id': 2088, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '93cdc6bc9f5f22d7', 'authors': ['Yuri Chervonyi', 'Trieu H. Trinh', 'Miroslav Olšák', 'Xiaomeng Yang', 'Hoang Nguyen', 'Marcelo Menegali', 'Junehyuk Jung', 'Vikas Verma', 'Quoc V. Le', 'Thang Luong'], 'affiliations': ['Brown University', 'Georgia Institute of Technology', 'Google DeepMind', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2502.03544.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#math', '#training', '#architecture', '#optimization', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'ИИ превосходит человека в олимпиадной геометрии', 'desc': 'AlphaGeometry2 - это улучшенная версия системы для решения олимпиадных задач по геометрии, превзошедшая средний уровень золотого медалиста. Разработчики расширили язык AlphaGeometry для решения более сложных задач, включая движение объектов и линейные уравнения с углами, отношениями и расстояниями. Система использует архитектуру Gemini для улучшенного языкового моделирования и новый механизм обмена знаниями, комбинирующий несколько деревьев поиска. AlphaGeometry2 достигла 84% успешности решения геометрических задач за последние 25 лет Международных математических олимпиад.'}, 'en': {'title': 'AlphaGeometry2: Mastering Olympiad Geometry with Advanced AI', 'desc': "AlphaGeometry2 is an advanced version of the original AlphaGeometry, designed to solve complex Olympiad geometry problems more effectively. It enhances the language capabilities to handle intricate scenarios involving object movements and linear equations related to angles and distances. The model's performance has improved significantly, increasing its problem coverage from 66% to 88% for International Math Olympiad geometry problems. With a new search process utilizing Gemini architecture and a knowledge-sharing mechanism, AlphaGeometry2 achieves an impressive solving rate of 84%, marking a substantial leap from its predecessor."}, 'zh': {'title': 'AlphaGeometry2：几何问题解决的新突破', 'desc': 'AlphaGeometry2是对AlphaGeometry的显著改进版本，能够超越平均金牌得主在解决奥林匹克几何问题上的表现。它扩展了原有的语言，能够处理更复杂的对象运动和包含角度、比例及距离的线性方程问题。通过使用Gemini架构和新颖的知识共享机制，AlphaGeometry2的搜索过程得到了显著提升，几何问题的解决率达到了84%。该系统还朝着从自然语言输入直接解决几何问题的全自动化系统迈出了重要一步。'}}}, {'id': 'https://huggingface.co/papers/2502.03621', 'title': 'DynVFX: Augmenting Real Videos with Dynamic Content', 'url': 'https://huggingface.co/papers/2502.03621', 'abstract': 'We present a method for augmenting real-world videos with newly generated dynamic content. Given an input video and a simple user-provided text instruction describing the desired content, our method synthesizes dynamic objects or complex scene effects that naturally interact with the existing scene over time. The position, appearance, and motion of the new content are seamlessly integrated into the original footage while accounting for camera motion, occlusions, and interactions with other dynamic objects in the scene, resulting in a cohesive and realistic output video. We achieve this via a zero-shot, training-free framework that harnesses a pre-trained text-to-video diffusion transformer to synthesize the new content and a pre-trained Vision Language Model to envision the augmented scene in detail. Specifically, we introduce a novel inference-based method that manipulates features within the attention mechanism, enabling accurate localization and seamless integration of the new content while preserving the integrity of the original scene. Our method is fully automated, requiring only a simple user instruction. We demonstrate its effectiveness on a wide range of edits applied to real-world videos, encompassing diverse objects and scenarios involving both camera and object motion.', 'score': 25, 'issue_id': 2089, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '8c22f8cda7e633d5', 'authors': ['Danah Yatim', 'Rafail Fridman', 'Omer Bar-Tal', 'Tali Dekel'], 'affiliations': ['Pika Labs, USA', 'Weizmann Institute of Science, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2502.03621.jpg', 'data': {'categories': ['#inference', '#video', '#multimodal', '#synthetic', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Генерация динамического контента в видео по текстовым инструкциям', 'desc': 'Авторы представляют метод для дополнения реальных видео новым динамическим контентом на основе текстовых инструкций пользователя. Метод использует предобученные модели text-to-video и Vision Language Model для синтеза и интеграции нового контента в исходное видео. Предложен новый подход на основе манипуляции признаками в механизме внимания для точной локализации и бесшовной интеграции нового контента. Метод полностью автоматизирован и требует только простой инструкции от пользователя, демонстрируя эффективность на широком спектре редактирований реальных видео.'}, 'en': {'title': 'Seamless Video Augmentation with Dynamic Content', 'desc': 'This paper introduces a novel approach for enhancing real-world videos by adding dynamic content based on user instructions. The method utilizes a pre-trained text-to-video diffusion transformer to generate new objects and effects that interact naturally with the existing scene. It employs a zero-shot, training-free framework that ensures seamless integration of the new content while considering factors like camera motion and occlusions. The approach is fully automated, allowing users to simply provide text instructions to achieve realistic video augmentation.'}, 'zh': {'title': '自动化视频增强，轻松生成动态内容！', 'desc': '我们提出了一种增强现实视频的新方法，可以生成动态内容。用户只需提供简单的文本指令，我们的方法就能合成与原始场景自然互动的动态对象或复杂场景效果。新内容的位置、外观和运动与原始视频无缝结合，同时考虑了相机运动、遮挡和其他动态对象的互动。该方法采用零-shot、无训练的框架，利用预训练的文本到视频扩散变换器和视觉语言模型，实现了自动化的内容合成。'}}}, {'id': 'https://huggingface.co/papers/2502.04320', 'title': 'ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features', 'url': 'https://huggingface.co/papers/2502.04320', 'abstract': 'Do the rich representations of multi-modal diffusion transformers (DiTs) exhibit unique properties that enhance their interpretability? We introduce ConceptAttention, a novel method that leverages the expressive power of DiT attention layers to generate high-quality saliency maps that precisely locate textual concepts within images. Without requiring additional training, ConceptAttention repurposes the parameters of DiT attention layers to produce highly contextualized concept embeddings, contributing the major discovery that performing linear projections in the output space of DiT attention layers yields significantly sharper saliency maps compared to commonly used cross-attention mechanisms. Remarkably, ConceptAttention even achieves state-of-the-art performance on zero-shot image segmentation benchmarks, outperforming 11 other zero-shot interpretability methods on the ImageNet-Segmentation dataset and on a single-class subset of PascalVOC. Our work contributes the first evidence that the representations of multi-modal DiT models like Flux are highly transferable to vision tasks like segmentation, even outperforming multi-modal foundation models like CLIP.', 'score': 24, 'issue_id': 2101, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '043b9ff9f5eaf227', 'authors': ['Alec Helbling', 'Tuna Han Salih Meral', 'Ben Hoover', 'Pinar Yanardag', 'Duen Horng Chau'], 'affiliations': ['Georgia Tech', 'IBM Research', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2502.04320.jpg', 'data': {'categories': ['#interpretability', '#transfer_learning', '#dataset', '#multimodal', '#cv', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'ConceptAttention: новый взгляд на интерпретируемость мультимодальных моделей', 'desc': 'Статья представляет новый метод ConceptAttention, который использует возможности слоев внимания мультимодальных диффузионных трансформеров (DiT) для создания высококачественных карт важности. Этот метод позволяет точно локализовать текстовые концепции на изображениях без дополнительного обучения. ConceptAttention превосходит существующие методы в задаче сегментации изображений с нулевым обучением на наборах данных ImageNet-Segmentation и PascalVOC. Исследование показывает, что представления моделей DiT, таких как Flux, хорошо переносятся на задачи компьютерного зрения, превосходя даже мультимодальные базовые модели вроде CLIP.'}, 'en': {'title': 'Enhancing Interpretability with ConceptAttention in Multi-Modal Transformers', 'desc': 'This paper explores the unique properties of multi-modal diffusion transformers (DiTs) and their interpretability through a new method called ConceptAttention. ConceptAttention utilizes the attention layers of DiTs to create detailed saliency maps that accurately identify textual concepts in images without needing extra training. The study reveals that linear projections in the output space of DiT attention layers lead to sharper saliency maps compared to traditional cross-attention methods. Additionally, ConceptAttention demonstrates superior performance in zero-shot image segmentation tasks, surpassing other interpretability techniques and showing the strong transferability of DiT representations to vision applications.'}, 'zh': {'title': '多模态扩散变换器的可解释性新突破', 'desc': '本文探讨了多模态扩散变换器（DiTs）在可解释性方面的独特性质。我们提出了一种新方法ConceptAttention，利用DiT注意力层的表达能力生成高质量的显著性图，准确定位图像中的文本概念。通过对DiT注意力层输出空间进行线性投影，ConceptAttention生成的显著性图比常用的交叉注意力机制更清晰。我们的研究首次证明了多模态DiT模型的表示在视觉任务（如分割）中具有高度可转移性，甚至超越了像CLIP这样的多模态基础模型。'}}}, {'id': 'https://huggingface.co/papers/2502.04313', 'title': 'Great Models Think Alike and this Undermines AI Oversight', 'url': 'https://huggingface.co/papers/2502.04313', 'abstract': 'As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as "AI Oversight". We study how model similarity affects both aspects of AI oversight by proposing a probabilistic metric for LM similarity based on overlap in model mistakes. Using this metric, we first show that LLM-as-a-judge scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from "weak-to-strong generalization". As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend -- model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight.', 'score': 20, 'issue_id': 2091, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'a44111741eb33c43', 'authors': ['Shashwat Goel', 'Joschka Struber', 'Ilze Amanda Auzina', 'Karuna K Chandra', 'Ponnurangam Kumaraguru', 'Douwe Kiela', 'Ameya Prabhu', 'Matthias Bethge', 'Jonas Geiping'], 'affiliations': ['Contextual AI', 'ELLIS Institute Tubingen', 'IIIT Hyderabad', 'Max Planck Institute for Intelligent Systems', 'Stanford University', 'Tubingen AI Center', 'University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2502.04313.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#interpretability', '#training', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Сходство языковых моделей: проблемы и риски AI-надзора', 'desc': "В статье рассматривается проблема оценки и контроля языковых моделей (ЯМ) с помощью других ЯМ, что авторы называют 'AI Oversight'. Исследуется влияние сходства моделей на эффективность такого надзора с использованием вероятностной метрики, основанной на пересечении ошибок. Обнаружено, что оценки ЯМ-судей смещены в пользу похожих моделей, а при обучении на аннотациях ЯМ важную роль играет дополняющее знание. Авторы отмечают тревожную тенденцию: с ростом возможностей ЯМ их ошибки становятся более схожими, что указывает на риски коррелированных сбоев."}, 'en': {'title': 'Navigating AI Oversight: Understanding Model Similarity and Its Risks', 'desc': 'This paper explores the challenges of evaluating and supervising advanced language models (LMs) as their capabilities grow. It introduces a probabilistic metric to measure LM similarity based on the overlap in their mistakes, which aids in understanding AI oversight. The study reveals that when using one LM to evaluate another, models that are similar tend to score each other favorably, indicating a potential bias. Additionally, it highlights the risks of correlated failures among models as they become more capable, emphasizing the need for careful reporting and correction of model similarities in AI oversight practices.'}, 'zh': {'title': 'AI监督：模型相似性的重要性', 'desc': '随着语言模型（LM）能力的提升，人类对其进行评估和监督变得越来越困难。我们提出了一种基于模型错误重叠的概率度量来研究模型相似性对AI监督的影响。研究表明，作为评判者的语言模型更倾向于偏好与其相似的模型，这与最近的自我偏好结果一致。此外，弱监督者与强学生模型之间的互补知识在“弱到强的泛化”中起着关键作用。'}}}, {'id': 'https://huggingface.co/papers/2502.00473', 'title': 'Weak-to-Strong Diffusion with Reflection', 'url': 'https://huggingface.co/papers/2502.00473', 'abstract': 'The goal of diffusion generative models is to align the learned distribution with the real data distribution through gradient score matching. However, inherent limitations in training data quality, modeling strategies, and architectural design lead to inevitable gap between generated outputs and real data. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel framework that utilizes the estimated difference between existing weak and strong models (i.e., weak-to-strong difference) to approximate the gap between an ideal model and a strong model. By employing a reflective operation that alternates between denoising and inversion with weak-to-strong difference, we theoretically understand that W2SD steers latent variables along sampling trajectories toward regions of the real data distribution. W2SD is highly flexible and broadly applicable, enabling diverse improvements through the strategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5, good experts vs. bad experts in MoE). Extensive experiments demonstrate that W2SD significantly improves human preference, aesthetic quality, and prompt adherence, achieving SOTA performance across various modalities (e.g., image, video), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For example, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to 90% over the original results. Moreover, the performance gains achieved by W2SD markedly outweigh its additional computational overhead, while the cumulative improvements from different weak-to-strong difference further solidify its practical utility and deployability.', 'score': 19, 'issue_id': 2095, 'pub_date': '2025-02-01', 'pub_date_card': {'ru': '1 февраля', 'en': 'February 1', 'zh': '2月1日'}, 'hash': '82d22d57d66f1a7a', 'authors': ['Lichen Bai', 'Masashi Sugiyama', 'Zeke Xie'], 'affiliations': ['RIKEN AIP', 'The University of Tokyo', 'xLeaF Lab, The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.00473.jpg', 'data': {'categories': ['#training', '#multimodal', '#architecture', '#optimization', '#dataset', '#diffusion', '#benchmark'], 'emoji': '🚀', 'ru': {'title': 'W2SD: Преодоление разрыва между генеративными моделями и реальными данными', 'desc': 'Статья представляет новый фреймворк под названием Weak-to-Strong Diffusion (W2SD) для улучшения диффузионных генеративных моделей. W2SD использует разницу между слабыми и сильными моделями для аппроксимации разрыва между идеальной и сильной моделью. Метод применяет рефлексивную операцию, чередующую денойзинг и инверсию с учетом разницы weak-to-strong, что теоретически направляет латентные переменные к реальному распределению данных. Эксперименты показывают, что W2SD значительно улучшает предпочтения пользователей, эстетическое качество и соответствие промпту, достигая SOTA результатов в различных модальностях и архитектурах.'}, 'en': {'title': 'Bridging the Gap: Weak-to-Strong Diffusion for Enhanced Generative Models', 'desc': "This paper introduces Weak-to-Strong Diffusion (W2SD), a new framework designed to enhance diffusion generative models by addressing the gap between generated outputs and real data. W2SD leverages the differences between weak and strong models to better approximate the ideal model's performance. By alternating between denoising and inversion processes, it guides latent variables towards areas that closely resemble the real data distribution. The framework shows significant improvements in various applications, achieving state-of-the-art results while maintaining efficiency in computational resources."}, 'zh': {'title': '弱到强扩散：缩小生成与真实数据的差距', 'desc': '扩散生成模型的目标是通过梯度评分匹配将学习到的分布与真实数据分布对齐。然而，训练数据质量、建模策略和架构设计的固有限制导致生成输出与真实数据之间存在不可避免的差距。为了解决这个问题，我们提出了弱到强扩散（W2SD）框架，利用现有弱模型和强模型之间的差异来近似理想模型与强模型之间的差距。W2SD通过反射操作在去噪和反演之间交替，理论上引导潜在变量沿着采样轨迹朝向真实数据分布的区域，从而显著提高生成结果的质量和一致性。'}}}, {'id': 'https://huggingface.co/papers/2502.04328', 'title': 'Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment', 'url': 'https://huggingface.co/papers/2502.04328', 'abstract': 'Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts. The core design of Ola lies in its progressive modality alignment strategy that extends the supporting modality of the language model progressively. Our training pipeline begins with the most distinct modalities: image and text, then gradually expands the skill sets of the model using speech data that connects language and audio knowledge, and video data that connects all modalities. The progressive learning pipeline also enables us to maintain a relatively small size of the cross-modal alignment data, making developing omni-modal from existing vision-language models easy and less costly. Moreover, to unlock an advanced interactive experience like GPT-4o, we further design a sentence-wise decoding solution for streaming speech generation. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at https://github.com/Ola-Omni/Ola.', 'score': 19, 'issue_id': 2089, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '0e25e42e93e9e560', 'authors': ['Zuyan Liu', 'Yuhao Dong', 'Jiahui Wang', 'Ziwei Liu', 'Winston Hu', 'Jiwen Lu', 'Yongming Rao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2502.04328.jpg', 'data': {'categories': ['#audio', '#open_source', '#video', '#training', '#multimodal'], 'emoji': '🦉', 'ru': {'title': 'Ola: Прогрессивное обучение для создания мощной омнимодальной модели', 'desc': 'В статье представлена Ola - омнимодальная языковая модель, способная понимать изображения, видео и аудио на уровне специализированных моделей. Ключевая особенность Ola - стратегия прогрессивного выравнивания модальностей, начинающаяся с изображений и текста, затем расширяющаяся на речь и видео. Модель использует небольшой объем кросс-модальных данных для обучения, что делает ее разработку менее затратной. Эксперименты показывают, что Ola превосходит существующие открытые омнимодальные модели по всем модальностям.'}, 'en': {'title': 'Ola: Bridging Modalities for Superior Understanding', 'desc': "This paper introduces Ola, an omni-modal language model designed to understand and process multiple types of data, including images, videos, and audio. Ola employs a progressive modality alignment strategy, starting with image and text, and gradually incorporating speech and video data to enhance its capabilities. The model's training pipeline is efficient, allowing it to maintain a smaller dataset for cross-modal alignment, making it easier and more cost-effective to develop. Experimental results show that Ola outperforms existing open omni-modal models and competes well with specialized models, aiming to contribute to future research in omni-modal understanding."}, 'zh': {'title': 'Ola：全模态理解的新突破', 'desc': '本文介绍了一种名为Ola的全模态语言模型，能够在图像、视频和音频理解方面与专门的单模态模型竞争。Ola的核心设计是逐步模态对齐策略，首先从图像和文本开始训练，然后逐步引入语音和视频数据。这样的训练流程使得跨模态对齐数据的规模相对较小，降低了开发全模态模型的成本。通过广泛的实验，Ola在所有模态上超越了现有的开放全模态语言模型，并在与同类专门模型的竞争中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.04153', 'title': 'UltraIF: Advancing Instruction Following from the Wild', 'url': 'https://huggingface.co/papers/2502.04153', 'abstract': 'Instruction-following made modern large language models (LLMs) helpful assistants. However, the key to taming LLMs on complex instructions remains mysterious, for that there are huge gaps between models trained by open-source community and those trained by leading companies. To bridge the gap, we propose a simple and scalable approach UltraIF for building LLMs that can follow complex instructions with open-source data. UltraIF first decomposes real-world user prompts into simpler queries, constraints, and corresponding evaluation questions for the constraints. Then, we train an UltraComposer to compose constraint-associated prompts with evaluation questions. This prompt composer allows us to synthesize complicated instructions as well as filter responses with evaluation questions. In our experiment, for the first time, we successfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5 instruction-following benchmarks without any benchmark information, using only 8B model as response generator and evaluator. The aligned model also achieved competitive scores on other benchmarks. Moreover, we also show that UltraIF could further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating broader use cases for the method. Our code will be available at https://github.com/kkk-an/UltraIF.', 'score': 19, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '5c7c902f1effa8a3', 'authors': ['Kaikai An', 'Li Sheng', 'Ganqu Cui', 'Shuzheng Si', 'Ning Ding', 'Yu Cheng', 'Baobao Chang'], 'affiliations': ['Peking University', 'Shanghai AI Lab', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.04153.jpg', 'data': {'categories': ['#open_source', '#training', '#benchmark', '#alignment', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'UltraIF: простой метод для обучения LLM следовать сложным инструкциям', 'desc': 'Исследователи представили подход UltraIF для улучшения способности больших языковых моделей (LLM) следовать сложным инструкциям. Метод разбивает пользовательские запросы на более простые компоненты и использует специальную модель UltraComposer для составления сложных инструкций с вопросами для оценки. Эксперименты показали, что UltraIF позволяет значительно улучшить следование инструкциям у базовой модели LLaMA-3.1-8B без использования специальных данных. Подход также продемонстрировал возможность дальнейшего улучшения версии модели, уже обученной следовать инструкциям.'}, 'en': {'title': 'UltraIF: Simplifying Complex Instructions for LLMs', 'desc': 'This paper introduces UltraIF, a method designed to enhance large language models (LLMs) in following complex instructions using open-source data. The approach involves breaking down user prompts into simpler components, such as queries and constraints, and then training a model called UltraComposer to generate these structured prompts. By synthesizing complicated instructions and evaluating responses, UltraIF successfully aligns the LLaMA-3.1-8B-Base model with its instruct version on multiple benchmarks without prior benchmark data. Additionally, the method shows potential for improving existing instruct models through self-alignment, expanding its applicability in various scenarios.'}, 'zh': {'title': 'UltraIF：让大型语言模型更聪明的秘密武器', 'desc': '本文提出了一种名为UltraIF的方法，用于提高大型语言模型（LLMs）对复杂指令的理解能力。该方法通过将用户的真实请求分解为更简单的查询、约束和相应的评估问题来实现。接着，训练一个名为UltraComposer的模型，能够生成与约束相关的提示，并结合评估问题来过滤响应。实验表明，UltraIF成功地使LLaMA-3.1-8B-Base在多个指令跟随基准上与其指令版本对齐，展示了该方法的有效性和广泛应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2502.04235', 'title': 'MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion', 'url': 'https://huggingface.co/papers/2502.04235', 'abstract': "Despite the remarkable capabilities of large language models across various tasks, their continued scaling faces a critical challenge: the scarcity of high-quality pretraining data. While model architectures continue to evolve, the natural language data struggles to scale up. To tackle this bottleneck, we propose MAssive Genre-Audience~(MAGA) reformulation method, which systematic synthesizes diverse, contextually-rich pretraining data from existing corpus. This work makes three main contributions: (1) We propose MAGA reformulation method, a lightweight and scalable approach for pretraining corpus expansion, and build a 770B tokens MAGACorpus. (2) We evaluate MAGACorpus with different data budget scaling strategies, demonstrating consistent improvements across various model sizes (134M-13B), establishing the necessity for next-generation large-scale synthetic pretraining language models. (3) Through comprehensive analysis, we investigate prompt engineering's impact on synthetic training collapse and reveal limitations in conventional collapse detection metrics using validation losses. Our work shows that MAGA can substantially expand training datasets while maintaining quality, offering a reliably pathway for scaling models beyond data limitations.", 'score': 16, 'issue_id': 2089, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'b1c2fec586443af8', 'authors': ['Xintong Hao', 'Ke Shen', 'Chenggang Li'], 'affiliations': ['Seed-LLM, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2502.04235.jpg', 'data': {'categories': ['#optimization', '#data', '#dataset', '#training', '#synthetic'], 'emoji': '🚀', 'ru': {'title': 'MAGA: преодоление ограничений данных для масштабирования языковых моделей', 'desc': 'Статья представляет метод MAGA для синтеза разнообразных и контекстуально богатых данных для предобучения языковых моделей. Авторы создали корпус MAGACorpus объемом 770 миллиардов токенов и продемонстрировали улучшение результатов для моделей различных размеров. Исследование также анализирует влияние инженерии промптов на синтетический коллапс обучения. Работа показывает, что MAGA может значительно расширить наборы данных для обучения, сохраняя их качество.'}, 'en': {'title': 'Expanding Language Models with MAGA: A Path Beyond Data Limitations', 'desc': 'This paper addresses the challenge of limited high-quality pretraining data for large language models. It introduces the MAssive Genre-Audience (MAGA) reformulation method, which creates diverse and contextually-rich pretraining data from existing sources. The authors present a new dataset called MAGACorpus, containing 770 billion tokens, and demonstrate its effectiveness across various model sizes. Additionally, they analyze the effects of prompt engineering on training stability and highlight the shortcomings of traditional metrics for detecting training collapse.'}, 'zh': {'title': 'MAGA：突破数据限制的预训练新方法', 'desc': '尽管大型语言模型在各种任务中表现出色，但它们在扩展时面临高质量预训练数据稀缺的挑战。为了解决这个瓶颈，我们提出了MAssive Genre-Audience（MAGA）重构方法，该方法系统地从现有语料库中合成多样化且富有上下文的预训练数据。我们的研究贡献包括提出了一种轻量级且可扩展的预训练语料库扩展方法，并构建了一个包含7700亿个标记的MAGACorpus。通过对不同数据预算扩展策略的评估，我们证明了在各种模型规模下（134M-13B）的一致性改进，展示了下一代大规模合成预训练语言模型的必要性。'}}}, {'id': 'https://huggingface.co/papers/2502.03860', 'title': 'BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation', 'url': 'https://huggingface.co/papers/2502.03860', 'abstract': "Large language models (LLMs), such as o1 from OpenAI, have demonstrated remarkable reasoning capabilities. o1 generates a long chain-of-thought (LongCoT) before answering a question. LongCoT allows LLMs to analyze problems, devise plans, reflect, and backtrack effectively. These actions empower LLM to solve complex problems. After the release of o1, many teams have attempted to replicate its LongCoT and reasoning capabilities. In terms of methods, they primarily rely on knowledge distillation with data from existing models with LongCoT capacities (e.g., OpenAI-o1, Qwen-QwQ, DeepSeek-R1-Preview), leaving significant uncertainties on systematically developing such reasoning abilities. In terms of data domains, these works focus narrowly on math while a few others include coding, limiting their generalizability. This paper introduces a novel approach to enable LLM's LongCoT capacity without distillation from o1-like models or expensive human annotations, where we bootstrap LongCoT (BOLT) from a standard instruct model. BOLT involves three stages: 1) LongCoT data bootstrapping with in-context learning on a standard instruct model; 2) LongCoT supervised finetuning; 3) online training to further refine LongCoT capacities. In BOLT, only a few in-context examples need to be constructed during the bootstrapping stage; in our experiments, we created 10 examples, demonstrating the feasibility of this approach. We use Llama-3.1-70B-Instruct to bootstrap LongCoT and apply our method to various model scales (7B, 8B, 70B). We achieve impressive performance on a variety of benchmarks, Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500, which evaluate diverse task-solving and reasoning capabilities.", 'score': 16, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'b861ba86ae27e974', 'authors': ['Bo Pang', 'Hanze Dong', 'Jiacheng Xu', 'Silvio Savarese', 'Yingbo Zhou', 'Caiming Xiong'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.03860.jpg', 'data': {'categories': ['#training', '#benchmark', '#math', '#long_context', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Самообучение ИИ сложным рассуждениям без подсказок', 'desc': 'Эта статья представляет новый подход к обучению больших языковых моделей (LLM) способности генерировать длинные цепочки рассуждений (LongCoT) без использования дистилляции знаний от существующих моделей. Метод BOLT включает три этапа: начальную генерацию данных LongCoT с помощью обучения в контексте, супервизорную донастройку и онлайн-обучение для дальнейшего улучшения способностей. Авторы применили свой метод к моделям различных масштабов и достигли впечатляющих результатов на ряде бенчмарков, оценивающих разнообразные способности решения задач и рассуждений. Этот подход позволяет развивать способности LLM к сложным рассуждениям без необходимости в дорогостоящих аннотациях или данных от существующих продвинутых моделей.'}, 'en': {'title': 'Empowering LLMs with Efficient Long Chain-of-Thought Bootstrapping', 'desc': "This paper presents a new method called Bootstrapping Long Chain-of-Thought (BOLT) to enhance the reasoning abilities of large language models (LLMs) without relying on knowledge distillation from existing models. BOLT consists of three stages: bootstrapping LongCoT data using in-context learning, supervised fine-tuning for LongCoT, and online training for further refinement. The approach requires only a few examples to start the bootstrapping process, making it efficient and scalable across different model sizes. Experimental results show that BOLT significantly improves performance on various reasoning and problem-solving benchmarks, demonstrating its effectiveness in developing LLMs' LongCoT capabilities."}, 'zh': {'title': '引导长链思维，提升推理能力！', 'desc': '本文介绍了一种新方法，旨在使大型语言模型（LLM）具备长链思维（LongCoT）能力，而无需依赖于类似o1模型的知识蒸馏或昂贵的人类标注。该方法称为BOLT，分为三个阶段：首先通过上下文学习从标准指令模型引导LongCoT数据，其次进行LongCoT的监督微调，最后进行在线训练以进一步提升LongCoT能力。实验中，我们仅构建了10个示例，证明了该方法的可行性。我们在多个基准测试上取得了显著的性能，展示了该方法在解决复杂问题和推理能力方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2502.04128', 'title': 'Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis', 'url': 'https://huggingface.co/papers/2502.04128', 'abstract': 'Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available.', 'score': 15, 'issue_id': 2088, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '196e7cb6a29a44ea', 'authors': ['Zhen Ye', 'Xinfa Zhu', 'Chi-Min Chan', 'Xinsheng Wang', 'Xu Tan', 'Jiahe Lei', 'Yi Peng', 'Haohe Liu', 'Yizhu Jin', 'Zheqi DAI', 'Hongzhan Lin', 'Jianyi Chen', 'Xingjian Du', 'Liumeng Xue', 'Yunlin Chen', 'Zhifei Li', 'Lei Xie', 'Qiuqiang Kong', 'Yike Guo', 'Wei Xue'], 'affiliations': ['ASLP Lab, Northwestern Polytechnical University', 'Chinese University of Hong Kong', 'Hong Kong Baptist University', 'Independent Researcher', 'Shanghai Mobvoi Information Technology Co., Ltd.', 'The Hong Kong University of Science and Technology', 'University of Rochester', 'University of Science and Technology Beijing', 'University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2502.04128.jpg', 'data': {'categories': ['#open_source', '#dataset', '#audio', '#training', '#optimization'], 'emoji': '🗣️', 'ru': {'title': 'Llasa: масштабируемый синтез речи на основе единой языковой модели', 'desc': 'В статье представлена новая система синтеза речи Llasa, использующая одноуровневый векторный квантователь и единую архитектуру трансформера, аналогичную стандартным языковым моделям. Исследование показывает, что увеличение вычислительных ресурсов при обучении улучшает естественность синтезированной речи и позволяет генерировать более сложные просодические паттерны. Масштабирование вычислений во время вывода с использованием моделей понимания речи в качестве верификаторов улучшает эмоциональную выразительность, согласованность тембра и точность содержания. Авторы опубликовали контрольные точки и код обучения для своей модели синтеза речи.'}, 'en': {'title': 'Simplifying Speech Synthesis with Scalable LLMs', 'desc': 'This paper discusses advancements in text-to-speech (TTS) systems that utilize large language models (LLMs) like GPT and o1. It introduces Llasa, a new framework that simplifies speech synthesis by using a single-layer vector quantizer and a Transformer architecture, aligning it with standard LLMs. The research shows that increasing training compute enhances the naturalness and complexity of the generated speech. Additionally, it highlights how scaling inference compute can improve emotional expressiveness and accuracy by using speech understanding models as verifiers during the synthesis process.'}, 'zh': {'title': '简化语音合成，提升自然性与情感表达', 'desc': '本文探讨了文本基础的大型语言模型（LLMs）在语音合成中的应用，特别是GPT系列和o1模型的最新进展。我们提出了一种名为Llasa的简单框架，使用单层向量量化（VQ）编解码器和单一Transformer架构，旨在简化多阶段的语音合成过程。实验结果表明，增加训练时间的计算资源可以显著提高合成语音的自然性，并生成更复杂的韵律模式。此外，我们还发现，在推理时间增加计算资源可以改善情感表现、音色一致性和内容准确性。'}}}, {'id': 'https://huggingface.co/papers/2502.04306', 'title': 'ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization', 'url': 'https://huggingface.co/papers/2502.04306', 'abstract': 'Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: https://github.com/Gen-Verse/ScoreFlow', 'score': 15, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '51ace85d35c202d5', 'authors': ['Yinjie Wang', 'Ling Yang', 'Guohao Li', 'Mengdi Wang', 'Bryon Aragam'], 'affiliations': ['Princeton University', 'University of Chicago', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2502.04306.jpg', 'data': {'categories': ['#benchmark', '#inference', '#optimization', '#agents', '#rlhf', '#small_models'], 'emoji': '🚀', 'ru': {'title': 'ScoreFlow: эффективная оптимизация мультиагентных систем на основе ИИ', 'desc': 'В статье представлен новый фреймворк ScoreFlow для оптимизации рабочих процессов мультиагентных систем на основе больших языковых моделей. ScoreFlow использует градиентную оптимизацию в непрерывном пространстве, что позволяет преодолеть ограничения существующих методов. Ключевым компонентом является Score-DPO - новый вариант метода прямой оптимизации предпочтений, учитывающий количественную обратную связь. На шести тестовых задачах ScoreFlow показал улучшение результатов на 8.2% по сравнению с базовыми методами.'}, 'en': {'title': 'ScoreFlow: Optimizing Multi-Agent Systems with Continuous Gradient Techniques', 'desc': 'This paper introduces ScoreFlow, a new framework designed to enhance the performance of multi-agent systems in solving complex problems. It addresses the limitations of existing optimization methods by utilizing gradient-based optimization in a continuous space, which allows for greater flexibility and scalability. ScoreFlow features Score-DPO, a novel approach that incorporates quantitative feedback into the optimization process. The results show that ScoreFlow not only improves performance by 8.2% over previous methods but also enables smaller models to achieve better results than larger models at a lower cost.'}, 'zh': {'title': 'ScoreFlow：高效的多智能体优化框架', 'desc': '最近的研究利用大型语言模型的多智能体系统来解决复杂问题，同时努力减少构建这些系统所需的手动工作。现有方法由于表示限制、缺乏适应性和依赖离散优化技术而导致灵活性不足。我们提出了ScoreFlow，这是一个简单但高性能的框架，利用基于梯度的优化在连续空间中进行优化。ScoreFlow在六个基准测试中表现出色，超越了现有基线，并使较小的模型以更低的推理成本超越较大的模型。'}}}, {'id': 'https://huggingface.co/papers/2502.02358', 'title': 'MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm', 'url': 'https://huggingface.co/papers/2502.02358', 'abstract': 'Human motion generation and editing are key components of computer graphics and vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify motion-related tasks, these methods simply use different modalities as conditions to guide motion generation. Consequently, they lack editing capabilities, fine-grained control, and fail to facilitate knowledge sharing across tasks. To address these limitations and provide a versatile, unified framework capable of handling both human motion generation and editing, we introduce a novel paradigm: Motion-Condition-Motion, which enables the unified formulation of diverse tasks with three concepts: source motion, condition, and target motion. Based on this paradigm, we propose a unified framework, MotionLab, which incorporates rectified flows to learn the mapping from source motion to target motion, guided by the specified conditions. In MotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional generation and editing without task-specific modules; 2) Aligned Rotational Position Encoding} to guarantee the time synchronization between source motion and target motion; 3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for effective multi-task learning and knowledge sharing across tasks. Notably, our MotionLab demonstrates promising generalization capabilities and inference efficiency across multiple benchmarks for human motion. Our code and additional video results are available at: https://diouo.github.io/motionlab.github.io/.', 'score': 14, 'issue_id': 2088, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': '967ac00db9aae918', 'authors': ['Ziyan Guo', 'Zeyu Hu', 'Na Zhao', 'De Wen Soh'], 'affiliations': ['LightSpeed Studios, Singapore', 'Singapore University of Technology and Design, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.02358.jpg', 'data': {'categories': ['#training', '#multimodal', '#cv', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Универсальный подход к генерации и редактированию движений человека', 'desc': 'Статья представляет новый подход к генерации и редактированию движений человека в компьютерной графике и компьютерном зрении. Авторы предлагают парадигму Motion-Condition-Motion и унифицированный фреймворк MotionLab, использующий исправленные потоки для отображения исходного движения в целевое. MotionLab включает в себя MotionFlow Transformer, выровненное позиционное кодирование вращения, модуляцию инструкций для конкретных задач и учебную программу движения для эффективного обучения. Фреймворк демонстрирует многообещающие возможности обобщения и эффективность вывода на нескольких эталонных тестах для движений человека.'}, 'en': {'title': 'Unifying Human Motion Generation and Editing with MotionLab', 'desc': 'This paper presents a new approach to human motion generation and editing called MotionLab, which aims to unify various motion-related tasks into a single framework. The proposed paradigm, Motion-Condition-Motion, allows for the integration of source motion, conditions, and target motion, facilitating better control and editing capabilities. MotionLab employs advanced techniques like the MotionFlow Transformer and Aligned Rotational Position Encoding to enhance the generation process and ensure synchronization between motions. Overall, this framework shows improved efficiency and generalization across different benchmarks, making it a versatile tool for real-world applications in computer graphics and vision.'}, 'zh': {'title': '统一人类运动生成与编辑的创新框架', 'desc': '人类运动生成和编辑是计算机图形学和视觉的重要组成部分。现有的方法往往针对特定任务提供孤立的解决方案，效率低下且不适用于实际应用。为了解决这些问题，我们提出了一种新的范式：运动条件运动（Motion-Condition-Motion），它通过源运动、条件和目标运动三个概念统一处理多种任务。基于这一范式，我们开发了MotionLab框架，能够有效地进行人类运动的生成和编辑，并在多个基准测试中展示了良好的泛化能力和推理效率。'}}}, {'id': 'https://huggingface.co/papers/2502.04299', 'title': 'MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2502.04299', 'abstract': 'This paper presents a method that allows users to design cinematic video shots in the context of image-to-video generation. Shot design, a critical aspect of filmmaking, involves meticulously planning both camera movements and object motions in a scene. However, enabling intuitive shot design in modern image-to-video generation systems presents two main challenges: first, effectively capturing user intentions on the motion design, where both camera movements and scene-space object motions must be specified jointly; and second, representing motion information that can be effectively utilized by a video diffusion model to synthesize the image animations. To address these challenges, we introduce MotionCanvas, a method that integrates user-driven controls into image-to-video (I2V) generation models, allowing users to control both object and camera motions in a scene-aware manner. By connecting insights from classical computer graphics and contemporary video generation techniques, we demonstrate the ability to achieve 3D-aware motion control in I2V synthesis without requiring costly 3D-related training data. MotionCanvas enables users to intuitively depict scene-space motion intentions, and translates them into spatiotemporal motion-conditioning signals for video diffusion models. We demonstrate the effectiveness of our method on a wide range of real-world image content and shot-design scenarios, highlighting its potential to enhance the creative workflows in digital content creation and adapt to various image and video editing applications.', 'score': 10, 'issue_id': 2088, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '91b39568cf3793e2', 'authors': ['Jinbo Xing', 'Long Mai', 'Cusuh Ham', 'Jiahui Huang', 'Aniruddha Mahapatra', 'Chi-Wing Fu', 'Tien-Tsin Wong', 'Feng Liu'], 'affiliations': ['Adobe Research', 'Monash University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.04299.jpg', 'data': {'categories': ['#multimodal', '#video', '#diffusion', '#3d', '#games'], 'emoji': '🎬', 'ru': {'title': 'MotionCanvas: Интуитивное проектирование видеокадров с контролем движения', 'desc': 'Эта статья представляет метод MotionCanvas, позволяющий пользователям проектировать кинематографические видеокадры в контексте генерации видео из изображений. Метод интегрирует пользовательское управление в модели I2V, позволяя контролировать движения объектов и камеры с учетом сцены. MotionCanvas соединяет классическую компьютерную графику и современные методы генерации видео, обеспечивая 3D-осведомленное управление движением без необходимости в дорогостоящих 3D-данных для обучения. Метод позволяет интуитивно описывать намерения движения в пространстве сцены и преобразовывать их в пространственно-временные сигналы для обусловливания движения в диффузионных моделях видео.'}, 'en': {'title': 'Empowering Cinematic Creativity with MotionCanvas', 'desc': 'This paper introduces MotionCanvas, a novel method for designing cinematic video shots in image-to-video generation. It addresses two main challenges: capturing user intentions for both camera and object motions, and effectively representing this motion information for video diffusion models. By integrating user-driven controls, MotionCanvas allows for intuitive scene-aware motion design without the need for expensive 3D training data. The method enhances creative workflows in digital content creation, making it easier for users to specify and visualize their motion intentions in video synthesis.'}, 'zh': {'title': '直观设计电影镜头，提升视频生成体验', 'desc': '本文提出了一种方法，使用户能够在图像到视频生成的背景下设计电影镜头。镜头设计是电影制作中的关键环节，涉及到对相机运动和场景中物体运动的精心规划。我们介绍了MotionCanvas，这是一种将用户驱动的控制集成到图像到视频生成模型中的方法，允许用户以场景感知的方式控制物体和相机的运动。通过将经典计算机图形学的见解与现代视频生成技术相结合，我们展示了在不需要昂贵的3D训练数据的情况下，实现I2V合成中的3D感知运动控制的能力。'}}}, {'id': 'https://huggingface.co/papers/2502.04270', 'title': 'PILAF: Optimal Human Preference Sampling for Reward Modeling', 'url': 'https://huggingface.co/papers/2502.04270', 'abstract': 'As large language models increasingly drive real-world applications, aligning them with human values becomes paramount. Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique, translating preference data into reward models when oracle human values remain inaccessible. In practice, RLHF mostly relies on approximate reward models, which may not consistently guide the policy toward maximizing the underlying human values. We propose Policy-Interpolated Learning for Aligned Feedback (PILAF), a novel response sampling strategy for preference labeling that explicitly aligns preference learning with maximizing the underlying oracle reward. PILAF is theoretically grounded, demonstrating optimality from both an optimization and a statistical perspective. The method is straightforward to implement and demonstrates strong performance in iterative and online RLHF settings where feedback curation is critical.', 'score': 8, 'issue_id': 2088, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '5c0dd6afed760a2b', 'authors': ['Yunzhen Feng', 'Ariel Kwiatkowski', 'Kunhao Zheng', 'Julia Kempe', 'Yaqi Duan'], 'affiliations': ['Meta FAIR', 'NYU'], 'pdf_title_img': 'assets/pdf/title_img/2502.04270.jpg', 'data': {'categories': ['#rlhf', '#training', '#alignment', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'PILAF: точное согласование ИИ с человеческими ценностями', 'desc': 'Статья представляет новый метод обучения языковых моделей с учетом человеческих ценностей - PILAF (Policy-Interpolated Learning for Aligned Feedback). Этот подход улучшает существующую технику обучения с подкреплением на основе обратной связи от человека (RLHF). PILAF предлагает стратегию выборки ответов для маркировки предпочтений, которая явно согласует обучение предпочтениям с максимизацией базовой награды. Метод демонстрирует сильную производительность в итеративных и онлайн-настройках RLHF, где критически важна курация обратной связи.'}, 'en': {'title': 'Aligning AI with Human Values through PILAF', 'desc': 'This paper introduces Policy-Interpolated Learning for Aligned Feedback (PILAF), a new method for improving Reinforcement Learning from Human Feedback (RLHF). PILAF focuses on aligning preference learning with the true human values by using a response sampling strategy that enhances reward model accuracy. The approach is theoretically sound, showing optimal performance in both optimization and statistical contexts. It is easy to implement and performs well in scenarios where feedback curation is essential, making it a valuable tool for aligning large language models with human values.'}, 'zh': {'title': '政策插值学习：对齐人类反馈的新策略', 'desc': '随着大型语言模型在实际应用中的广泛使用，使其与人类价值观保持一致变得至关重要。强化学习与人类反馈（RLHF）成为了一种关键技术，它将偏好数据转化为奖励模型，尤其是在无法获取人类价值观的情况下。我们提出了一种新的响应采样策略，称为政策插值学习（PILAF），它明确将偏好学习与最大化基础奖励对齐。PILAF在理论上是有依据的，从优化和统计的角度都展示了其最优性，并且在反馈策划至关重要的迭代和在线RLHF环境中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2502.00989', 'title': 'ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution', 'url': 'https://huggingface.co/papers/2502.00989', 'abstract': 'Large Language Models (LLMs) can perform chart question-answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system orchestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through pre-filtering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLM-assisted chart QA and enables professionals to be more productive.', 'score': 7, 'issue_id': 2092, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '4d26f9419f20aadb', 'authors': ['Kanika Goswami', 'Puneet Mathur', 'Ryan Rossi', 'Franck Dernoncourt'], 'affiliations': ['ACM, New York, NY, USA', 'IGDTUW, Delhi India'], 'pdf_title_img': 'assets/pdf/title_img/2502.00989.jpg', 'data': {'categories': ['#interpretability', '#hallucinations', '#agents', '#cv', '#multimodal'], 'emoji': '📊', 'ru': {'title': 'Точные ответы по графикам с доказательствами от ИИ', 'desc': 'ChartCitor - это мультиагентная система для ответов на вопросы по графикам с использованием больших языковых моделей (LLM). Она решает проблему галлюцинаций и необоснованных ответов путем предоставления точных ссылок на области изображения графика. Система включает в себя извлечение данных из графика в таблицу, переформулировку вопроса, дополнение таблицы, поиск доказательств и сопоставление таблицы с графиком. ChartCitor превосходит существующие методы и повышает доверие пользователей к генеративному ИИ.'}, 'en': {'title': 'ChartCitor: Enhancing Trust in AI with Accurate Chart Question-Answering', 'desc': "This paper introduces ChartCitor, a multi-agent framework designed to improve the accuracy of question-answering tasks involving charts by addressing the issue of hallucinated responses from Large Language Models (LLMs). It enhances answer attribution by providing precise bounding box citations that link responses to specific parts of chart images, overcoming challenges related to visual-semantic context and complex layouts. The framework includes processes for chart-to-table extraction, answer reformulation, and evidence retrieval, which collectively improve the reliability of the generated answers. User studies indicate that ChartCitor not only boosts the performance of LLMs in chart QA tasks but also increases user trust and productivity by offering clearer explanations of the AI's reasoning."}, 'zh': {'title': 'ChartCitor：提升图表问答的可信度与效率', 'desc': '大型语言模型（LLMs）可以执行图表问答任务，但常常生成未经验证的虚假回答。现有的答案归属方法由于视觉语义上下文有限、复杂的视觉文本对齐要求以及在复杂布局中进行边界框预测的困难，难以将回答与源图表关联。我们提出了ChartCitor，一个多代理框架，通过识别图表图像中的支持证据来提供细粒度的边界框引用。ChartCitor在不同图表类型上超越了现有基准，增强了用户对生成式人工智能的信任，并提高了专业人士的工作效率。'}}}, {'id': 'https://huggingface.co/papers/2502.03639', 'title': 'Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach', 'url': 'https://huggingface.co/papers/2502.03639', 'abstract': 'We present a novel video generation framework that integrates 3-dimensional geometry and dynamic awareness. To achieve this, we augment 2D videos with 3D point trajectories and align them in pixel space. The resulting 3D-aware video dataset, PointVid, is then used to fine-tune a latent diffusion model, enabling it to track 2D objects with 3D Cartesian coordinates. Building on this, we regularize the shape and motion of objects in the video to eliminate undesired artifacts, \\eg, nonphysical deformation. Consequently, we enhance the quality of generated RGB videos and alleviate common issues like object morphing, which are prevalent in current video models due to a lack of shape awareness. With our 3D augmentation and regularization, our model is capable of handling contact-rich scenarios such as task-oriented videos. These videos involve complex interactions of solids, where 3D information is essential for perceiving deformation and contact. Furthermore, our model improves the overall quality of video generation by promoting the 3D consistency of moving objects and reducing abrupt changes in shape and motion.', 'score': 7, 'issue_id': 2088, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '7875c341693f4d1e', 'authors': ['Yunuo Chen', 'Junli Cao', 'Anil Kag', 'Vidit Goel', 'Sergei Korolev', 'Chenfanfu Jiang', 'Sergey Tulyakov', 'Jian Ren'], 'affiliations': ['Snap Inc.', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2502.03639.jpg', 'data': {'categories': ['#diffusion', '#video', '#3d'], 'emoji': '🎥', 'ru': {'title': '3D-осведомленная генерация видео с улучшенной геометрией и динамикой', 'desc': 'Представлена новая система генерации видео, объединяющая трехмерную геометрию и динамическое восприятие. Двумерные видео дополняются трехмерными траекториями точек и выравниваются в пиксельном пространстве. Полученный набор данных PointVid используется для дообучения модели латентной диффузии, что позволяет отслеживать 2D объекты с помощью 3D координат. Регуляризация формы и движения объектов улучшает качество генерируемых RGB видео и устраняет нежелательные артефакты.'}, 'en': {'title': 'Enhancing Video Generation with 3D Awareness', 'desc': 'This paper introduces a new framework for generating videos that combines 3D geometry with an understanding of dynamic movements. It creates a dataset called PointVid by enhancing 2D videos with 3D point trajectories, which helps the model learn to track objects in three dimensions. The framework uses a latent diffusion model that is fine-tuned to ensure that the shapes and motions of objects are realistic, reducing issues like nonphysical deformation. By focusing on 3D consistency, the model improves the quality of generated videos, especially in scenarios where objects interact closely, such as in task-oriented videos.'}, 'zh': {'title': '三维感知，提升视频生成质量', 'desc': '我们提出了一种新的视频生成框架，结合了三维几何和动态感知。通过在像素空间中对齐三维点轨迹，我们增强了二维视频，创建了一个三维感知的视频数据集PointVid。利用这个数据集，我们对潜在扩散模型进行微调，使其能够跟踪具有三维坐标的二维物体。我们的模型通过正则化物体的形状和运动，消除了不必要的伪影，提高了生成RGB视频的质量，特别是在处理复杂的接触场景时。'}}}, {'id': 'https://huggingface.co/papers/2502.04295', 'title': 'Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization', 'url': 'https://huggingface.co/papers/2502.04295', 'abstract': 'Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code will be available at https://github.com/HenryLau7/CFPO.', 'score': 7, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'f72f46ad2c1b9853', 'authors': ['Yuanye Liu', 'Jiahang Xu', 'Li Lyna Zhang', 'Qi Chen', 'Xuan Feng', 'Yang Chen', 'Zhongxin Guo', 'Yuqing Yang', 'Cheng Peng'], 'affiliations': ['Fudan University', 'Microsoft Research Asia'], 'pdf_title_img': 'assets/pdf/title_img/2502.04295.jpg', 'data': {'categories': ['#optimization', '#open_source', '#training'], 'emoji': '🧬', 'ru': {'title': 'Интегрированная оптимизация формы и содержания промптов повышает эффективность LLM', 'desc': 'Статья представляет новую методологию оптимизации промптов для больших языковых моделей (LLM), называемую CFPO. Этот подход оптимизирует как содержание, так и форматирование промптов через итеративный процесс уточнения. CFPO использует мутации естественного языка для исследования вариаций содержания и применяет динамическую стратегию исследования форматов. Результаты показывают значительное улучшение производительности по сравнению с методами оптимизации, ориентированными только на содержание.'}, 'en': {'title': 'Enhancing LLMs with Integrated Content and Format Optimization', 'desc': 'This paper presents a new method called Content-Format Integrated Prompt Optimization (CFPO) that improves the performance of Large Language Models (LLMs) by optimizing both the content and formatting of prompts. The authors argue that while prompt content has been the focus of recent research, the formatting aspect is equally important and has not been thoroughly explored. CFPO uses natural language mutations to create variations in content and a dynamic strategy to test different formatting options. The results show that this integrated approach leads to better performance in various tasks compared to methods that only optimize content.'}, 'zh': {'title': '内容与格式的完美结合，提升LLM性能！', 'desc': '大型语言模型（LLMs）在各种任务中表现出色，其实际效果往往依赖于提示设计。尽管最近的研究主要集中在优化提示内容上，但提示格式的作用却被忽视，缺乏系统性的研究。本文提出了一种新的方法——内容格式集成提示优化（CFPO），通过迭代优化过程同时优化提示内容和格式。我们的评估表明，CFPO在多个任务和开源LLMs上相较于仅优化内容的方法，显示出显著的性能提升，强调了内容与格式集成优化的重要性。'}}}, {'id': 'https://huggingface.co/papers/2502.00988', 'title': 'PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback', 'url': 'https://huggingface.co/papers/2502.00988', 'abstract': 'Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, a novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including a Query Planning Agent that breaks down complex user requests into executable steps, a Code Generation Agent that converts pseudocode into executable Python code, and three retrieval feedback agents - a Numeric Feedback Agent, a Lexical Feedback Agent, and a Visual Feedback Agent - that leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines, achieving a 4-6 percent improvement on the MatPlotBench dataset, leading to enhanced user trust in LLM-generated visualizations and improved novice productivity due to a reduction in debugging time needed for plot errors.', 'score': 5, 'issue_id': 2094, 'pub_date': '2025-02-03', 'pub_date_card': {'ru': '3 февраля', 'en': 'February 3', 'zh': '2月3日'}, 'hash': '9548b306478edf6d', 'authors': ['Kanika Goswami', 'Puneet Mathur', 'Ryan Rossi', 'Franck Dernoncourt'], 'affiliations': ['IGDTUW, Delhi India'], 'pdf_title_img': 'assets/pdf/title_img/2502.00988.jpg', 'data': {'categories': ['#cv', '#agents', '#science', '#dataset', '#multimodal'], 'emoji': '📊', 'ru': {'title': 'PlotGen: ИИ-помощник для создания точных научных визуализаций', 'desc': 'Статья представляет PlotGen - новую мультиагентную систему для автоматического создания научных визуализаций данных. Система использует несколько агентов на основе больших языковых моделей (LLM) для планирования, генерации кода и итеративного улучшения графиков. PlotGen включает агентов для разбиения сложных запросов, генерации Python-кода и многомодальной обратной связи для уточнения точности данных, текстовых меток и визуальной корректности. Эксперименты показывают, что PlotGen превосходит базовые методы на 4-6% на датасете MatPlotBench, повышая доверие пользователей к визуализациям, созданным с помощью LLM.'}, 'en': {'title': 'Automating Scientific Visualization with PlotGen', 'desc': 'This paper introduces PlotGen, a multi-agent framework designed to automate the creation of accurate scientific visualizations. It utilizes several Large Language Model (LLM)-based agents to streamline the process, including a Query Planning Agent for breaking down user requests and a Code Generation Agent for converting pseudocode into Python code. Additionally, it employs retrieval feedback agents that enhance the quality of visualizations by refining data accuracy and visual elements through iterative self-reflection. Experimental results demonstrate that PlotGen significantly improves visualization accuracy and reduces debugging time, thereby increasing user trust and productivity for novice users.'}, 'zh': {'title': '自动化科学可视化的未来', 'desc': '科学数据可视化对于将原始数据转化为易于理解的视觉表示至关重要，能够帮助识别模式和预测结果。新手用户在选择合适工具和掌握可视化技术时常常面临困难。本文提出了一种名为PlotGen的新型多代理框架，旨在自动化创建精确的科学可视化。通过多个基于大语言模型的代理，PlotGen能够有效地分解用户请求并生成高质量的可视化图表。'}}}, {'id': 'https://huggingface.co/papers/2502.04296', 'title': 'Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression', 'url': 'https://huggingface.co/papers/2502.04296', 'abstract': 'We propose Heterogeneous Masked Autoregression (HMA) for modeling action-video dynamics to generate high-quality data and evaluation in scaling robot learning. Building interactive video world models and policies for robotics is difficult due to the challenge of handling diverse settings while maintaining computational efficiency to run in real time. HMA uses heterogeneous pre-training from observations and action sequences across different robotic embodiments, domains, and tasks. HMA uses masked autoregression to generate quantized or soft tokens for video predictions. \\ourshort achieves better visual fidelity and controllability than the previous robotic video generation models with 15 times faster speed in the real world. After post-training, this model can be used as a video simulator from low-level action inputs for evaluating policies and generating synthetic data. See this link https://liruiw.github.io/hma for more information.', 'score': 5, 'issue_id': 2087, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'da9d11d5ea5d9d9e', 'authors': ['Lirui Wang', 'Kevin Zhao', 'Chaoqi Liu', 'Xinlei Chen'], 'affiliations': ['MIT', 'Meta, FAIR', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2502.04296.jpg', 'data': {'categories': ['#games', '#robotics', '#dataset', '#video', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'HMA: Быстрое и качественное моделирование видео для обучения роботов', 'desc': 'Статья представляет новый метод Heterogeneous Masked Autoregression (HMA) для моделирования динамики видео действий в робототехнике. HMA использует гетерогенное предобучение на наблюдениях и последовательностях действий из различных роботизированных воплощений, доменов и задач. Метод применяет маскированную авторегрессию для генерации квантованных или мягких токенов для предсказания видео. HMA достигает лучшего визуального качества и управляемости по сравнению с предыдущими моделями генерации видео для роботов, работая в 15 раз быстрее в реальном мире.'}, 'en': {'title': 'Revolutionizing Robot Learning with Heterogeneous Video Modeling', 'desc': 'The paper introduces Heterogeneous Masked Autoregression (HMA), a novel approach for modeling the dynamics of action videos to enhance robot learning. HMA addresses the challenges of diverse environments and the need for real-time computational efficiency by leveraging heterogeneous pre-training from various robotic tasks and domains. By employing masked autoregression, HMA generates high-quality video predictions using quantized or soft tokens. The results show that HMA significantly improves visual fidelity and controllability while operating 15 times faster than previous models, making it a valuable tool for simulating video from low-level actions and evaluating robotic policies.'}, 'zh': {'title': '异构掩蔽自回归：提升机器人学习的视频生成', 'desc': '我们提出了异构掩蔽自回归（HMA）模型，用于建模动作视频的动态，以生成高质量的数据并评估机器人学习的扩展性。构建交互式视频世界模型和机器人策略面临挑战，因为需要处理多样化的环境，同时保持实时运行的计算效率。HMA通过对不同机器人形态、领域和任务的观察和动作序列进行异构预训练，来提高模型的性能。经过后期训练，该模型可以作为视频模拟器，从低级动作输入中评估策略并生成合成数据。'}}}, {'id': 'https://huggingface.co/papers/2501.19085', 'title': 'Enhancing Code Generation for Low-Resource Languages: No Silver Bullet', 'url': 'https://huggingface.co/papers/2501.19085', 'abstract': "The advent of Large Language Models (LLMs) has significantly advanced the field of automated code generation. LLMs rely on large and diverse datasets to learn syntax, semantics, and usage patterns of programming languages. For low-resource languages (i.e., niche programming languages characterized by the scarcity of training data), the limited availability of such data hampers the models' ability to generalize effectively, resulting in poorer code generation performance as compared to high-resource languages. For this reason, there is a quest for techniques able to close this performance gap. We present an empirical study investigating the effectiveness of several approaches for boosting LLMs' performance on low-resource languages, namely: (i) a classic fine-tuning, which is however capped in size by the scarcity of training data; (ii) three variants of in-context learning, with prompts crafted to provide the LLM with additional information about the low-resource language (e.g., few-shot examples showcasing features of the targeted language); and (iii) a pre-training objective teaching the model how to translate between high- and low-resource languages. The context of our study are two low-resource languages (R and Racket) and six LLMs having different architectures and sizes. Our findings reveal that a fine-tuning is usually the best choice for smaller LLMs, possibly due to the fact that even a small dataset is sufficient to train their limited number of parameters. With the increase in size of the models, in-context learning becomes more and more effective, representing a safe and cheap bet (i.e., it always helps, but with different magnitudes). Differently, very large LLMs may deteriorate their performance on low-resource languages when fine-tuning is performed, possibly due to the lack of enough data needed to effectively update their weights.", 'score': 4, 'issue_id': 2094, 'pub_date': '2025-01-31', 'pub_date_card': {'ru': '31 января', 'en': 'January 31', 'zh': '1月31日'}, 'hash': '8c0ab784750b1038', 'authors': ['Alessandro Giagnorio', 'Alberto Martin-Lopez', 'Gabriele Bavota'], 'affiliations': ['Software Institute USI Università della Svizzera italiana, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2501.19085.jpg', 'data': {'categories': ['#training', '#low_resource', '#plp', '#dataset'], 'emoji': '🚀', 'ru': {'title': 'Повышение эффективности языковых моделей для малоресурсных языков программирования', 'desc': 'Исследование посвящено улучшению генерации кода языковыми моделями (ЯМ) для малоресурсных языков программирования. Авторы сравнивают эффективность различных подходов, включая дообучение, обучение в контексте и предобучение на задаче перевода. Результаты показывают, что для небольших ЯМ лучше всего работает дообучение, а для крупных - обучение в контексте. Очень большие ЯМ могут ухудшать производительность при дообучении из-за недостатка данных.'}, 'en': {'title': 'Boosting Code Generation for Low-Resource Languages with LLMs', 'desc': 'This paper explores how Large Language Models (LLMs) can be improved for generating code in low-resource programming languages, which have limited training data. It examines various methods such as fine-tuning, in-context learning, and a pre-training objective that translates between high- and low-resource languages. The study finds that smaller LLMs benefit most from fine-tuning, while larger models perform better with in-context learning due to their architecture. However, fine-tuning large LLMs can lead to worse performance on low-resource languages because they require more data to adjust their parameters effectively.'}, 'zh': {'title': '提升低资源语言代码生成的有效策略', 'desc': '大型语言模型（LLMs）的出现显著推动了自动代码生成领域的发展。这些模型依赖于大量多样化的数据集来学习编程语言的语法、语义和使用模式。然而，对于低资源语言（即训练数据稀缺的小众编程语言），数据的有限性限制了模型的泛化能力，导致代码生成性能较差。因此，本文研究了几种提升LLMs在低资源语言上表现的有效方法，包括经典的微调和几种上下文学习变体。'}}}, {'id': 'https://huggingface.co/papers/2502.04322', 'title': 'Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions', 'url': 'https://huggingface.co/papers/2502.04322', 'abstract': 'Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.', 'score': 3, 'issue_id': 2090, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '1ad4a9febd48be28', 'authors': ['Yik Siu Chan', 'Narutatsu Ri', 'Yuxin Xiao', 'Marzyeh Ghassemi'], 'affiliations': ['Brown University', 'Columbia University', 'Massachusetts Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.04322.jpg', 'data': {'categories': ['#dataset', '#ethics', '#multilingual', '#benchmark', '#security'], 'emoji': '🕵️', 'ru': {'title': 'Простое взаимодействие - скрытая угроза безопасности языковых моделей', 'desc': 'Статья посвящена проблеме уязвимостей больших языковых моделей (LLM) к атакам, направленным на обход систем безопасности. Авторы предлагают новую метрику HarmScore для оценки эффективности вредоносных ответов LLM и разрабатывают фреймворк атаки Speak Easy, основанный на многошаговом многоязычном взаимодействии. Исследование показывает, что простые паттерны взаимодействия могут быть легко использованы злоумышленниками для вредоносных целей. Результаты демонстрируют значительное увеличение успешности атак и показателя HarmScore при применении Speak Easy к различным LLM.'}, 'en': {'title': 'Uncovering Hidden Vulnerabilities in Language Models', 'desc': 'This paper addresses the vulnerability of large language models (LLMs) to jailbreak attacks that can lead to harmful behavior. It explores whether these attacks are useful for average users and if safety issues arise in typical human-LLM interactions. The authors introduce HarmScore, a metric to evaluate how well LLM responses can facilitate harmful actions, and Speak Easy, a framework for executing multi-step, multilingual attacks. Their findings indicate that malicious users can exploit common interaction patterns, significantly increasing the success rate of harmful actions.'}, 'zh': {'title': '揭示大型语言模型的安全漏洞', 'desc': '尽管已经进行了广泛的安全对齐工作，大型语言模型（LLMs）仍然容易受到越狱攻击，这会引发有害行为。本文探讨了两个关键问题：越狱响应是否真正帮助普通用户实施有害行为，以及在更常见的人类与LLM的简单互动中是否存在安全漏洞。我们提出了HarmScore，这是一种评估LLM响应如何有效促进有害行为的指标，并介绍了Speak Easy，一个简单的多步骤、多语言攻击框架。研究表明，恶意用户可以轻松利用常见的互动模式来实现有害意图。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (3)', '#agi', '#alignment (3)', '#architecture (3)', '#audio (2)', '#benchmark (8)', '#cv (4)', '#data (1)', '#dataset (10)', '#diffusion (4)', '#ethics (1)', '#games (2)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (2)', '#interpretability (4)', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math (2)', '#multilingual (1)', '#multimodal (8)', '#open_source (4)', '#optimization (8)', '#plp (1)', '#rag', '#reasoning (2)', '#rl', '#rlhf (3)', '#robotics (1)', '#science (1)', '#security (1)', '#small_models (1)', '#story_generation', '#survey', '#synthetic (4)', '#training (13)', '#transfer_learning (1)', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-02-10 00:47',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-10 00:47')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-10 00:47')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    