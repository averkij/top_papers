
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 17 papers. December 1.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">1 –¥–µ–∫–∞–±—Ä—è</span> | <span id="title-articles-count">17 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-11-28.html">‚¨ÖÔ∏è <span id="prev-date">28.11</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-12-02.html">‚û°Ô∏è <span id="next-date">02.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-12.html">üìà <span id='top-month-label'>–ú–µ—Å—è—Ü</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '1 –¥–µ–∫–∞–±—Ä—è', 'en': 'December 1', 'zh': '12Êúà1Êó•'};
        let feedDateNext = {'ru': '02.12', 'en': '12/02', 'zh': '12Êúà2Êó•'};
        let feedDatePrev = {'ru': '28.11', 'en': '11/28', 'zh': '11Êúà28Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2511.22699', 'title': 'Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer', 'url': 'https://huggingface.co/papers/2511.22699', 'abstract': 'Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the "scale-at-all-costs" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.', 'score': 183, 'issue_id': 1, 'pub_date': '2025-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': 'f95b1b395872e6e6', 'authors': ['Z-Image Team', 'Huanqia Cai', 'Sihan Cao', 'Ruoyi Du', 'Peng Gao', 'Steven Hoi', 'Zhaohui Hou', 'Shijie Huang', 'Dengyang Jiang', 'Xin Jin', 'Liangchen Li', 'Zhen Li', 'Zhong-Yu Li', 'David Liu', 'Dongyang Liu', 'Junhan Shi', 'Qilong Wu', 'Feng Yu', 'Chi Zhang', 'Shifeng Zhang', 'Shilin Zhou'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.22699.jpg', 'data': {'categories': ['#inference', '#optimization', '#open_source', '#small_models', '#diffusion', '#architecture', '#training', '#cv'], 'emoji': '‚ö°', 'ru': {'title': '–ö–∞—á–µ—Å—Ç–≤–æ –±–µ–∑ –∏–∑–±—ã—Ç–∫–∞: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —ç–ø–æ—Ö—É –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏', 'desc': 'Z-Image ‚Äî —ç—Ç–æ –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å 6 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ (S3-DiT), –∫–æ—Ç–æ—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –∑–∞—Ç—Ä–∞—Ç—ã –¥–æ 314 —Ç—ã—Å—è—á GPU-—á–∞—Å–æ–≤. –î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏–≤–µ–ª–∞ –∫ —Å–æ–∑–¥–∞–Ω–∏—é Z-Image-Turbo, –∫–æ—Ç–æ—Ä–∞—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã–≤–æ–¥ –º–µ–Ω–µ–µ —á–µ–º –∑–∞ —Å–µ–∫—É–Ω–¥—É –Ω–∞ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏ —Å –æ–±—ä—ë–º–æ–º –ø–∞–º—è—Ç–∏ –º–µ–Ω–µ–µ 16 –ì–ë. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–≥–æ —Å –≤–µ–¥—É—â–∏–º–∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Ñ–æ—Ç–æ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –¥–≤—É—è–∑—ã—á–Ω–æ–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–µ —Ç–µ–∫—Å—Ç–∞.'}, 'en': {'title': 'Efficient Image Generation with Z-Image: High Performance, Low Cost!', 'desc': "Z-Image is a new image generation model that uses a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture with only 6 billion parameters, making it efficient and suitable for consumer hardware. It achieves fast inference times of under a second while maintaining high-quality output, challenging the trend of requiring massive models for good performance. The model's training process is optimized to reduce costs and time, completing in about 314,000 GPU hours. Z-Image also includes an editing version that excels in following instructions, demonstrating that high-quality generative models can be developed with lower computational resources."}, 'zh': {'title': 'Z-ImageÔºöÈ´òÊïàÂõæÂÉèÁîüÊàêÁöÑÊñ∞ÈÄâÊã©', 'desc': 'Z-ImageÊòØ‰∏ÄÁßçÈ´òÊïàÁöÑ6BÂèÇÊï∞ÁîüÊàêÊ®°ÂûãÔºåÂü∫‰∫éÂèØÊâ©Â±ïÁöÑÂçïÊµÅÊâ©Êï£ÂèòÊç¢Âô®(S3-DiT)Êû∂ÊûÑÔºåÊó®Âú®Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨Âπ∂ÊèêÈ´òÂõæÂÉèÁîüÊàêÊÄßËÉΩ„ÄÇËØ•Ê®°ÂûãÂú®‰ºÅ‰∏öÁ∫ßH800 GPU‰∏äÂÆûÁé∞‰∫Ü‰∫öÁßíÁ∫ßÊé®ÁêÜÂª∂ËøüÔºåÂêåÊó∂ÂÖºÂÆπÊ∂àË¥πËÄÖÁ∫ßÁ°¨‰ª∂ÔºåÈÄÇÂêàÂπøÊ≥õÂ∫îÁî®„ÄÇÈÄöËøá‰ºòÂåñÊ®°ÂûãÁîüÂëΩÂë®ÊúüÂíåËÆ≠ÁªÉÊµÅÁ®ãÔºåZ-ImageÂú®ËÆ≠ÁªÉÊïàÁéá‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåËÉΩÂ§ü‰∏éÂ§ßÂûãÊ®°ÂûãÁ´û‰∫â„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåZ-ImageÂú®ÁîüÊàêÈÄºÁúüÂõæÂÉèÂíåÂèåËØ≠ÊñáÊú¨Ê∏≤ÊüìÊñπÈù¢ÁöÑËÉΩÂäõÔºåËÉΩÂ§ü‰∏éÈ°∂Á∫ßÂïÜ‰∏öÊ®°ÂûãÁõ∏Â™≤ÁæéÔºåÂ±ïÁ§∫‰∫ÜÂú®ÊòæËëóÈôç‰ΩéËÆ°ÁÆóÂºÄÈîÄÁöÑÊÉÖÂÜµ‰∏ãÂÆûÁé∞ÊúÄÂÖàËøõÁªìÊûúÁöÑÂèØËÉΩÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2511.22570', 'title': 'DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2511.22570', 'abstract': "A self-verifying large language model for theorem proving improves mathematical reasoning by incentivizing rigorous step-by-step derivations and achieves high scores in international competitions.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced. By scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year. However, this approach faces fundamental limitations. Pursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning. Moreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable. To push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning. Self-verification is particularly important for scaling test-time compute, especially for open problems without known solutions. Towards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving. We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier. Our resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.", 'score': 70, 'issue_id': 1, 'pub_date': '2025-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': '8b1110df3818640d', 'authors': ['Zhihong Shao', 'Yuxiang Luo', 'Chengda Lu', 'Z. Z. Ren', 'Jiewen Hu', 'Tian Ye', 'Zhibin Gou', 'Shirong Ma', 'Xiaokang Zhang'], 'affiliations': ['DeepSeek-AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.22570.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#rl', '#training', '#math', '#science'], 'emoji': 'üßÆ', 'ru': {'title': '–°–∞–º–æ–ø—Ä–æ–≤–µ—Ä—è—é—â–∞—è—Å—è –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ç—Ä–æ–≥–æ–≥–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä—è—é—â–µ–≥–æ—Å—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ —Ç–µ–æ—Ä–µ–º. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –≥–¥–µ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–ª—É–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏–µ–π –Ω–∞–≥—Ä–∞–¥—ã, —Å—Ç–∏–º—É–ª–∏—Ä—É—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –≤—ã—è–≤–ª—è—Ç—å –∏ –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å –æ—à–∏–±–∫–∏ –≤ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–∞–±–æ—Ç–∞—Ö. –î–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö —É–ª—É—á—à–∞–µ—Ç—Å—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞. –ü–æ–ª—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å DeepSeekMath-V2 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∑–æ–ª–æ—Ç–æ–≥–æ —É—Ä–æ–≤–Ω—è –Ω–∞ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –æ–ª–∏–º–ø–∏–∞–¥–∞—Ö –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, –≤–∫–ª—é—á–∞—è IMO 2025 –∏ CMO 2024.'}, 'en': {'title': 'Empowering Theorem Proving with Self-Verification', 'desc': 'This paper presents a self-verifying large language model (LLM) designed to enhance mathematical reasoning, particularly in theorem proving. The model, named DeepSeekMath-V2, utilizes reinforcement learning to incentivize rigorous step-by-step derivations rather than just focusing on final answers. By implementing a verification mechanism, the model ensures that the reasoning process is both accurate and comprehensive, addressing the limitations of previous approaches. The results show significant improvements in competitive mathematical reasoning tasks, achieving high scores in prestigious competitions.'}, 'zh': {'title': 'Ëá™ÊàëÈ™åËØÅÊ®°ÂûãÔºåÊèêÂçáÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçËá™ÊàëÈ™åËØÅÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÁî®‰∫éÂÆöÁêÜËØÅÊòéÔºåÊó®Âú®ÊèêÈ´òÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ„ÄÇÈÄöËøáÊøÄÂä±‰∏•Ê†ºÁöÑÈÄêÊ≠•Êé®ÂØºÔºåËØ•Ê®°ÂûãÂú®ÂõΩÈôÖÁ´ûËµõ‰∏≠ÂèñÂæó‰∫ÜÈ´òÂàÜ„ÄÇÂ∞ΩÁÆ°Áé∞ÊúâÊñπÊ≥ïÂú®ÊúÄÁªàÁ≠îÊ°àÁöÑÂáÜÁ°ÆÊÄß‰∏äÊúâÊâÄÊèêÂçáÔºå‰ΩÜÂπ∂Êú™Ëß£ÂÜ≥Êé®ÁêÜËøáÁ®ãÁöÑÊ≠£Á°ÆÊÄßÈóÆÈ¢ò„ÄÇËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆ≠ÁªÉÊñπÊ≥ïÔºåÈÄöËøáËá™ÊàëÈ™åËØÅÊù•Â¢ûÂº∫Ê®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºåÊúÄÁªàÂÆûÁé∞‰∫ÜÂú®Â§ö‰∏™Êï∞Â≠¶Á´ûËµõ‰∏≠ÁöÑ‰ºòÂºÇË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2511.22625', 'title': 'REASONEDIT: Towards Reasoning-Enhanced Image Editing Models', 'url': 'https://huggingface.co/papers/2511.22625', 'abstract': 'Integrating reasoning mechanisms into image editing models enhances performance by improving instruction understanding and result correction.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in image editing models have shown remarkable progress. A common architectural design couples a multimodal large language model (MLLM) encoder with a diffusion decoder, as seen in systems such as Step1X-Edit and Qwen-Image-Edit, where the MLLM encodes both the reference image and the instruction but remains frozen during training. In this work, we demonstrate that unlocking the reasoning capabilities of MLLM can further push the boundaries of editing models. Specifically, we explore two reasoning mechanisms, thinking and reflection, which enhance instruction understanding and editing accuracy. Based on that, our proposed framework enables image editing in a thinking-editing-reflection loop: the thinking mechanism leverages the world knowledge of MLLM to interpret abstract instructions, while the reflection reviews editing results, automatically corrects unintended manipulations, and identifies the stopping round. Extensive experiments demonstrate that our reasoning approach achieves significant performance gains, with improvements of ImgEdit (+4.3%), GEdit (+4.7%), and Kris (+8.2%) when initializing our DiT from the Step1X-Edit (ReasonEdit-S), and also outperforms previous open-source methods on both GEdit and Kris when integrated with Qwen-Image-Edit (ReasonEdit-Q).', 'score': 45, 'issue_id': 1, 'pub_date': '2025-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': 'f2cb78babe98bca3', 'authors': ['Fukun Yin', 'Shiyu Liu', 'Yucheng Han', 'Zhibo Wang', 'Peng Xing', 'Rui Wang', 'Wei Cheng', 'Yingming Wang', 'Aojie Li', 'Zixin Yin', 'Pengtao Chen', 'Xiangyu Zhang', 'Daxin Jiang', 'Xianfang Zeng', 'Gang Yu'], 'affiliations': ['StepFun'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.22625.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#diffusion', '#interpretability', '#architecture', '#cv'], 'emoji': 'ü§î', 'ru': {'title': '–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –≤ —Ü–∏–∫–ª–µ: –º—ã—à–ª–µ–Ω–∏–µ –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º–æ–¥–µ–ª–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, —Å–æ—Å—Ç–æ—è—â–µ–π –∏–∑ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (MLLM) –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ –¥–µ–∫–æ–¥–µ—Ä–∞. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑–±–ª–æ–∫–∏—Ä—É—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ MLLM –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é, –≤–Ω–µ–¥—Ä—è—è –¥–≤–∞ –º–µ—Ö–∞–Ω–∏–∑–º–∞: thinking –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ reflection –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ ReasonEdit —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Ü–∏–∫–ª –º—ã—à–ª–µ–Ω–∏–µ-—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ-—Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞—Ç—å –∫–æ–º–∞–Ω–¥—ã –∏ –∏–∑–±–µ–≥–∞—Ç—å –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —Å –ø—Ä–∏—Ä–æ—Å—Ç–æ–º —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–æ 8.2%.'}, 'en': {'title': 'Enhancing Image Editing with Reasoning Mechanisms', 'desc': 'This paper discusses how integrating reasoning mechanisms into image editing models can improve their performance. The authors focus on a framework that combines a multimodal large language model (MLLM) with a diffusion decoder, allowing for better understanding of instructions and correction of results. They introduce two reasoning processes: thinking, which helps interpret instructions, and reflection, which reviews and corrects editing outcomes. The results show significant performance improvements in various image editing tasks, demonstrating the effectiveness of their approach.'}, 'zh': {'title': 'Êé®ÁêÜÊú∫Âà∂ÊèêÂçáÂõæÂÉèÁºñËæëÊ®°ÂûãÁöÑÊÄßËÉΩ', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ∞ÜÊé®ÁêÜÊú∫Âà∂Êï¥ÂêàÂà∞ÂõæÂÉèÁºñËæëÊ®°Âûã‰∏≠Ôºå‰ª•ÊèêÂçáÂÖ∂ÊÄßËÉΩ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÊÄùËÄÉÂíåÂèçÊÄù‰∏§ÁßçÊé®ÁêÜÊú∫Âà∂ÔºåÂ¢ûÂº∫‰∫ÜÂØπÊåá‰ª§ÁöÑÁêÜËß£ÂíåÁºñËæëÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊÄùËÄÉÊú∫Âà∂Âà©Áî®Â§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∏ñÁïåÁü•ËØÜÊù•Ëß£ËØªÊäΩË±°Êåá‰ª§ÔºåËÄåÂèçÊÄùÊú∫Âà∂ÂàôÂÆ°Êü•ÁºñËæëÁªìÊûúÔºåËá™Âä®Á∫†Ê≠£ÊÑèÂ§ñÁöÑÊìç‰Ωú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™ÂõæÂÉèÁºñËæë‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2511.23475', 'title': 'AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement', 'url': 'https://huggingface.co/papers/2511.23475', 'abstract': "The proposed AnyTalker framework generates high-quality multi-person talking videos by extending Diffusion Transformer with identity-aware attention, leveraging single-person videos for training, and using a specialized dataset for evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.", 'score': 41, 'issue_id': 1, 'pub_date': '2025-11-28', 'pub_date_card': {'ru': '28 –Ω–æ—è–±—Ä—è', 'en': 'November 28', 'zh': '11Êúà28Êó•'}, 'hash': 'c92624f175f73ab5', 'authors': ['Zhizhou Zhong', 'Yicheng Ji', 'Zhe Kong', 'Yiying Liu', 'Jiarui Wang', 'Jiasun Feng', 'Lupeng Liu', 'Xiangyi Wang', 'Yanjia Li', 'Yuqing She', 'Ying Qin', 'Huan Li', 'Shuiyang Mao', 'Wei Liu', 'Wenhan Luo'], 'affiliations': ['Beijing Jiaotong University', 'Hong Kong University of Science and Technology', 'Video Rebirth', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.23475.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#synthetic', '#benchmark', '#diffusion', '#video', '#architecture'], 'emoji': 'üé¨', 'ru': {'title': '–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–Ω—ã—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –≤–∏–¥–µ–æ –∏–∑ –æ–¥–Ω–æ–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ AnyTalker –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –≥–æ–≤–æ—Ä—è—â–∏–º–∏ –ª—é–¥—å–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç Diffusion Transformer —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º –º–µ—Ö–∞–Ω–∏–∑–º–æ–º identity-aware attention. –ö–ª—é—á–µ–≤–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ - –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –≤–∏–¥–µ–æ —Å –æ–¥–Ω–∏–º —á–µ–ª–æ–≤–µ–∫–æ–º, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º—É–ª—å—Ç–∏–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä–∏—è. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –±–ª–∞–≥–æ–¥–∞—Ä—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–∞—Ä –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å-–∞—É–¥–∏–æ —á–µ—Ä–µ–∑ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –Ω–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É –∏ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –≥—É–± –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏ –≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ.'}, 'en': {'title': 'AnyTalker: Revolutionizing Multi-Person Video Generation with Identity-Aware Attention', 'desc': 'The AnyTalker framework is designed to create high-quality videos of multiple people talking by enhancing the Diffusion Transformer model with a unique identity-aware attention mechanism. This approach allows the model to process audio and identity information together, enabling it to generate videos with various characters while maintaining coherent interactions. Instead of relying on large datasets of multi-person videos, AnyTalker trains on single-person videos and fine-tunes its performance with a limited number of multi-person clips. The framework also introduces a new evaluation metric and dataset to assess the realism and interactivity of the generated videos, achieving impressive results in lip synchronization and visual fidelity.'}, 'zh': {'title': 'AnyTalkerÔºöÈ´òÊïàÁîüÊàêÂ§ö‰∫∫Áâ©ÂØπËØùËßÜÈ¢ëÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'AnyTalkerÊ°ÜÊû∂ÈÄöËøáÊâ©Â±ïÊâ©Êï£ÂèòÊç¢Âô®ÁöÑË∫´‰ªΩÊÑüÁü•Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁîüÊàêÈ´òË¥®ÈáèÁöÑÂ§ö‰∫∫Áâ©ÂØπËØùËßÜÈ¢ë„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®Âçï‰∫∫ËßÜÈ¢ëËøõË°åËÆ≠ÁªÉÔºåÂπ∂‰ΩøÁî®‰∏ìÈó®ÁöÑÊï∞ÊçÆÈõÜËøõË°åËØÑ‰º∞Ôºå‰ªéËÄåËß£ÂÜ≥‰∫ÜÂ§ö‰∫∫Áâ©Êï∞ÊçÆÊî∂ÈõÜÁöÑÈ´òÊàêÊú¨ÂíåÂ§öË∫´‰ªΩ‰∫íÂä®ÁöÑÈöæÈ¢ò„ÄÇAnyTalkerÁöÑÂ§öÊµÅÂ§ÑÁêÜÊû∂ÊûÑÂÖÅËÆ∏ÁÅµÊ¥ªÊâ©Â±ïÂèØÈ©±Âä®ÁöÑË∫´‰ªΩÔºåÂπ∂ÈÄöËøáÂ∞ëÈáèÁúüÂÆûÁöÑÂ§ö‰∫∫Áâ©ÁâáÊÆµÊù•‰ºòÂåñ‰∫íÂä®ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAnyTalkerÂú®ÂîáÂêåÊ≠•„ÄÅËßÜËßâË¥®ÈáèÂíåËá™ÁÑ∂‰∫íÂä®ÊÄßÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÊàêÂäüÂπ≥Ë°°‰∫ÜÊï∞ÊçÆÊàêÊú¨ÂíåË∫´‰ªΩÂèØÊâ©Â±ïÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2511.18890', 'title': 'Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models', 'url': 'https://huggingface.co/papers/2511.18890', 'abstract': "The study identifies key architectural factors and efficient operators to optimize small language models for real-device latency, introducing the Nemotron-Flash family for improved accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficient deployment of small language models (SLMs) is essential for numerous real-world applications with stringent latency constraints. While previous work on SLM design has primarily focused on reducing the number of parameters to achieve parameter-optimal SLMs, parameter efficiency does not necessarily translate into proportional real-device speed-ups. This work aims to identify the key determinants of SLMs' real-device latency and offer generalizable principles and methodologies for SLM design and training when real-device latency is the primary consideration. Specifically, we identify two central architectural factors: depth-width ratios and operator choices. The former is crucial for small-batch-size latency, while the latter affects both latency and large-batch-size throughput. In light of this, we first study latency-optimal depth-width ratios, with the key finding that although deep-thin models generally achieve better accuracy under the same parameter budget, they may not lie on the accuracy-latency trade-off frontier. Next, we explore emerging efficient attention alternatives to evaluate their potential as candidate building operators. Using the identified promising operators, we construct an evolutionary search framework to automatically discover latency-optimal combinations of these operators within hybrid SLMs, thereby advancing the accuracy-latency frontier. In addition to architectural improvements, we further enhance SLM training using a weight normalization technique that enables more effective weight updates and improves final convergence. Combining these methods, we introduce a new family of hybrid SLMs, called Nemotron-Flash, which significantly advances the accuracy-efficiency frontier of state-of-the-art SLMs, e.g., achieving over +5.5% average accuracy, 1.3x/1.9x lower latency, and 18.7x/45.6x higher throughput compared to Qwen3-1.7B/0.6B, respectively.", 'score': 29, 'issue_id': 1, 'pub_date': '2025-11-24', 'pub_date_card': {'ru': '24 –Ω–æ—è–±—Ä—è', 'en': 'November 24', 'zh': '11Êúà24Êó•'}, 'hash': '7cd0a2bfd5155b52', 'authors': ['Yonggan Fu', 'Xin Dong', 'Shizhe Diao', 'Matthijs Van keirsbilck', 'Hanrong Ye', 'Wonmin Byeon', 'Yashaswi Karnati', 'Lucas Liebenwein', 'Hannah Zhang', 'Nikolaus Binder', 'Maksim Khadkevich', 'Alexander Keller', 'Jan Kautz', 'Yingyan Celine Lin', 'Pavlo Molchanov'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.18890.jpg', 'data': {'categories': ['#inference', '#optimization', '#small_models', '#architecture', '#training'], 'emoji': '‚ö°', 'ru': {'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –∑–∞–¥–µ—Ä–∂–∫–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö', 'desc': '–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –∫–ª—é—á–µ–≤—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –ø—Ä–∏ —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã –∏ —à–∏—Ä–∏–Ω—ã —Å–µ—Ç–∏ –∏ –≤—ã–±–æ—Ä –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–∞–∫ –∑–∞–¥–µ—Ä–∂–∫–∏ –ø—Ä–∏ –º–∞–ª—ã—Ö –±–∞—Ç—á–∞—Ö, —Ç–∞–∫ –∏ –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –±–∞—Ç—á–∞—Ö. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π Nemotron-Flash, –∫–æ—Ç–æ—Ä–æ–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ñ—Ä–æ–Ω—Ç–∏–µ—Ä —Ç–æ—á–Ω–æ—Å—Ç—å-—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Optimizing Small Language Models for Real-World Latency', 'desc': 'This paper focuses on optimizing small language models (SLMs) for better performance on real devices, particularly under strict latency requirements. It identifies key architectural factors, such as depth-width ratios and operator choices, that significantly influence the latency and throughput of SLMs. The authors propose a new family of models called Nemotron-Flash, which combines efficient operators and improved training techniques to enhance both accuracy and efficiency. The results show that these models outperform existing SLMs, achieving higher accuracy and lower latency, making them suitable for real-world applications.'}, 'zh': {'title': '‰ºòÂåñÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÊèêÂçáÁúüÂÆûËÆæÂ§áÊÄßËÉΩ', 'desc': 'Êú¨Á†îÁ©∂ËØÜÂà´‰∫Ü‰ºòÂåñÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàSLMsÔºâÂú®ÁúüÂÆûËÆæÂ§á‰∏äÂª∂ËøüÁöÑÂÖ≥ÈîÆÊû∂ÊûÑÂõ†Á¥†ÂíåÈ´òÊïàÊìç‰ΩúÁ¨¶ÔºåÊèêÂá∫‰∫ÜNemotron-FlashÁ≥ªÂàó‰ª•ÊèêÈ´òÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ‰ª•ÂæÄÁöÑSLMËÆæËÆ°‰∏ªË¶ÅÂÖ≥Ê≥®ÂáèÂ∞ëÂèÇÊï∞Êï∞ÈáèÔºå‰ΩÜÂèÇÊï∞ÊïàÁéáÂπ∂‰∏ç‰∏ÄÂÆöËÉΩÂ∏¶Êù•Áõ∏Â∫îÁöÑÈÄüÂ∫¶ÊèêÂçá„ÄÇÊàë‰ª¨ÂèëÁé∞Ê∑±Â∫¶-ÂÆΩÂ∫¶ÊØîÂíåÊìç‰ΩúÁ¨¶ÈÄâÊã©ÊòØÂΩ±ÂìçSLMÁúüÂÆûËÆæÂ§áÂª∂ËøüÁöÑ‰∏§‰∏™ÈáçË¶ÅÂõ†Á¥†ÔºåÂπ∂ÊèêÂá∫‰∫ÜÁõ∏Â∫îÁöÑËÆæËÆ°ÂíåËÆ≠ÁªÉÂéüÂàô„ÄÇÈÄöËøáËøô‰∫õÊñπÊ≥ïÔºåÊàë‰ª¨ÊûÑÂª∫‰∫ÜÊñ∞ÁöÑÊ∑∑ÂêàSLMÁ≥ªÂàóÔºåÊòæËëóÊèêÂçá‰∫ÜÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2511.21025', 'title': 'CaptionQA: Is Your Caption as Useful as the Image Itself?', 'url': 'https://huggingface.co/papers/2511.21025', 'abstract': 'CaptionQA evaluates caption utility by measuring their effectiveness in supporting downstream tasks, revealing significant gaps compared to traditional image-QA benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Image captions serve as efficient surrogates for visual content in multimodal systems such as retrieval, recommendation, and multi-step agentic inference pipelines. Yet current evaluation practices miss a fundamental question: Can captions stand-in for images in real downstream tasks? We propose a utility-based benchmark, CaptionQA, to evaluate model-generated captions, where caption quality is measured by how well it supports downstream tasks. CaptionQA is an extensible domain-dependent benchmark covering 4 domains--Natural, Document, E-commerce, and Embodied AI--each with fine-grained taxonomies (25 top-level and 69 subcategories) that identify useful information for domain-specific tasks. CaptionQA builds 33,027 densely annotated multiple-choice questions (50.3 per image on average) that explicitly require visual information to answer, providing a comprehensive probe of caption utility. In our evaluation protocol, an LLM answers these questions using captions alone, directly measuring whether captions preserve image-level utility and are utilizable by a downstream LLM. Evaluating state-of-the-art MLLMs reveals substantial gaps between the image and its caption utility. Notably, models nearly identical on traditional image-QA benchmarks lower by up to 32% in caption utility. We release CaptionQA along with an open-source pipeline for extension to new domains. The code is available at https://github.com/bronyayang/CaptionQA.', 'score': 25, 'issue_id': 1, 'pub_date': '2025-11-26', 'pub_date_card': {'ru': '26 –Ω–æ—è–±—Ä—è', 'en': 'November 26', 'zh': '11Êúà26Êó•'}, 'hash': 'c0f311411875caef', 'authors': ['Shijia Yang', 'Yunong Liu', 'Bohan Zhai', 'Ximeng Sun', 'Zicheng Liu', 'Emad Barsoum', 'Manling Li', 'Chenfeng Xu'], 'affiliations': ['ADVANCED MICRO DEVICES, INC.', 'INDEPENDENT RESEARCHER', 'NORTHWESTERN UNIVERSITY', 'STANFORD UNIVERSITY', 'UT AUSTIN'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.21025.jpg', 'data': {'categories': ['#dataset', '#agents', '#open_source', '#multimodal', '#benchmark', '#cv'], 'emoji': 'üì∏', 'ru': {'title': '–ü–æ–¥–ø–∏—Å–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –Ω–µ –º–æ–≥—É—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–º–µ–Ω–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—ã –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CaptionQA –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ –±–æ–ª–µ–µ —á–µ–º 33 —Ç—ã—Å—è—á –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —á–µ—Ç—ã—Ä–µ –ø—Ä–µ–¥–º–µ—Ç–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏, –≥–¥–µ –æ—Ç–≤–µ—Ç—ã —Ç—Ä–µ–±—É—é—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–µ —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ö—É–∂–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –∑–∞–¥–∞—á–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –≤–º–µ—Å—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –≠—Ç–∞ —Ä–∞–±–æ—Ç–∞ –≤—ã—è–≤–ª—è–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–¥–ø–∏—Å–µ–π –∏ –∏—Ö —Ä–µ–∞–ª—å–Ω–æ–π –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å—é –≤ —Å–ª–æ–∂–Ω—ã—Ö –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö.'}, 'en': {'title': 'Evaluating Caption Utility for Real-World Tasks with CaptionQA', 'desc': 'CaptionQA is a new benchmark designed to assess the effectiveness of image captions in supporting various downstream tasks, such as retrieval and recommendation. It highlights the shortcomings of traditional image-QA benchmarks by showing that captions often do not perform as well as images in practical applications. The benchmark includes a large dataset of 33,027 annotated questions across four domains, allowing for a detailed evaluation of caption quality. Results indicate that state-of-the-art models show a significant drop in performance when relying solely on captions, underscoring the need for improved caption generation methods.'}, 'zh': {'title': 'CaptionQAÔºöËØÑ‰º∞ÂõæÂÉèÊèèËø∞Âú®‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑÊïàÁî®', 'desc': 'CaptionQA ÊòØ‰∏Ä‰∏™ËØÑ‰º∞ÂõæÂÉèÊèèËø∞ÊúâÊïàÊÄßÁöÑÂü∫ÂáÜÔºå‰∏ªË¶ÅÈÄöËøáÊµãÈáèÂÖ∂Âú®‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑÊîØÊåÅËÉΩÂäõÊù•Êè≠Á§∫‰∏é‰º†ÁªüÂõæÂÉèÈóÆÁ≠îÂü∫ÂáÜ‰πãÈó¥ÁöÑÊòæËëóÂ∑ÆË∑ù„ÄÇËØ•Âü∫ÂáÜÊ∂µÁõñËá™ÁÑ∂„ÄÅÊñáÊ°£„ÄÅÁîµÂ≠êÂïÜÂä°ÂíåÂÖ∑Ë∫´‰∫∫Â∑•Êô∫ËÉΩÁ≠âÂõõ‰∏™È¢ÜÂüüÔºåÊèê‰æõ‰∫ÜÁªÜËá¥ÁöÑÂàÜÁ±ª‰ΩìÁ≥ªÔºå‰ª•ËØÜÂà´ÁâπÂÆö‰ªªÂä°ÊâÄÈúÄÁöÑ‰ø°ÊÅØ„ÄÇÈÄöËøáÊûÑÂª∫ 33,027 ‰∏™ÂØÜÈõÜÊ≥®ÈáäÁöÑÂ§öÈ°πÈÄâÊã©È¢òÔºåCaptionQA Áõ¥Êé•ÊµãÈáèÊèèËø∞ÊòØÂê¶‰øùÁïô‰∫ÜÂõæÂÉèÁ∫ßÂà´ÁöÑÊïàÁî®ÔºåÂπ∂ËÉΩË¢´‰∏ãÊ∏∏Â§ßËØ≠Ë®ÄÊ®°ÂûãÊúâÊïàÂà©Áî®„ÄÇÊàë‰ª¨ÁöÑËØÑ‰º∞ÊòæÁ§∫ÔºåÂ∞ΩÁÆ°Âú®‰º†ÁªüÂõæÂÉèÈóÆÁ≠îÂü∫ÂáÜ‰∏äË°®Áé∞Áõ∏‰ººÔºåÊ®°ÂûãÂú®ÊèèËø∞ÊïàÁî®‰∏äÂç¥‰ΩéËá≥ 32%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2511.22677', 'title': 'Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield', 'url': 'https://huggingface.co/papers/2511.22677', 'abstract': "The study reveals that in text-to-image generation, CFG Augmentation is the primary driver of few-step distillation in Distribution Matching Distillation (DMD), while the distribution matching term acts as a regularizer.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks like text-to-image generation, where CFG is typically required for desirable few-step performance, the primary driver of few-step distillation is not distribution matching, but a previously overlooked component we identify as CFG Augmentation (CA). We demonstrate that this term acts as the core ``engine'' of distillation, while the Distribution Matching (DM) term functions as a ``regularizer'' that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding further enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains. Notably, our method has been adopted by the Z-Image ( https://github.com/Tongyi-MAI/Z-Image ) project to develop a top-tier 8-step image generation model, empirically validating the generalization and robustness of our findings.", 'score': 23, 'issue_id': 1, 'pub_date': '2025-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': 'dfbb44ed75494dc0', 'authors': ['Dongyang Liu', 'Peng Gao', 'David Liu', 'Ruoyi Du', 'Zhen Li', 'Qilong Wu', 'Xin Jin', 'Sihan Cao', 'Shifeng Zhang', 'Hongsheng Li', 'Steven Hoi'], 'affiliations': ['The Chinese University of Hong Kong', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.22677.jpg', 'data': {'categories': ['#inference', '#optimization', '#open_source', '#multimodal', '#diffusion', '#training'], 'emoji': '‚ö°', 'ru': {'title': 'CFG Augmentation ‚Äî –∏—Å—Ç–∏–Ω–Ω—ã–π –¥–≤–∏–≥–∞—Ç–µ–ª—å –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª—è–µ—Ç—Å—è —Ä–æ–ª—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤ Distribution Matching Distillation (DMD) –¥–ª—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –≤ –∑–∞–¥–∞—á–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é –æ—Å–Ω–æ–≤–Ω–æ–π –¥–≤–∏–∂—É—â–µ–π —Å–∏–ª–æ–π —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —è–≤–ª—è–µ—Ç—Å—è CFG Augmentation, –∞ –Ω–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–æ–≤ —É—á–µ–Ω–∏–∫–∞ –∏ —É—á–∏—Ç–µ–ª—è. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–æ–≤ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ç–æ—Ä, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ —Å–Ω–∏–∂–∞—é—â–∏–π –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω—ã —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏, –≤–∫–ª—é—á–∞—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–π —à—É–º–æ–≤ –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ç–æ—Ä–∞.'}, 'en': {'title': 'Unlocking the Power of CFG Augmentation in Distillation', 'desc': 'This paper investigates the mechanisms behind few-step distillation in text-to-image generation, specifically focusing on Distribution Matching Distillation (DMD). It reveals that CFG Augmentation (CA) is the main factor driving performance, rather than the traditional emphasis on distribution matching. The authors show that while distribution matching serves as a stabilizing regularizer, it can be replaced by simpler methods without losing effectiveness. Their findings lead to new strategies for improving distillation processes, which have been successfully implemented in a state-of-the-art image generation model.'}, 'zh': {'title': 'CFGÂ¢ûÂº∫ÔºöÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁöÑÊ†∏ÂøÉÈ©±Âä®Âäõ', 'desc': 'Êú¨Á†îÁ©∂Êè≠Á§∫‰∫ÜÂú®ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê‰∏≠ÔºåCFGÂ¢ûÂº∫ÊòØÂàÜÂ∏ÉÂåπÈÖçËí∏È¶èÔºàDMDÔºâ‰∏≠Â∞ëÊ≠•Ëí∏È¶èÁöÑ‰∏ªË¶ÅÈ©±Âä®Âõ†Á¥†ÔºåËÄåÂàÜÂ∏ÉÂåπÈÖçÈ°πÂàôÂÖÖÂΩìÊ≠£ÂàôÂåñÂô®„ÄÇÊàë‰ª¨ÈÄöËøáÂØπDMDËÆ≠ÁªÉÁõÆÊ†áÁöÑ‰∏•Ê†ºÂàÜËß£ÔºåÊåëÊàò‰∫Ü‰º†ÁªüÁêÜËß£ÔºåÊåáÂá∫Âú®Â§çÊùÇ‰ªªÂä°‰∏≠ÔºåCFGÂ¢ûÂº∫ÊòØËí∏È¶èÁöÑÊ†∏ÂøÉÂºïÊìé„ÄÇÊàë‰ª¨È™åËØÅ‰∫ÜËøô‰∏ÄÂàÜÁ¶ªÔºåË°®ÊòéËôΩÁÑ∂ÂàÜÂ∏ÉÂåπÈÖçÈ°πÊòØÊúâÊïàÁöÑÊ≠£ÂàôÂåñÂô®Ôºå‰ΩÜÂπ∂ÈùûÂîØ‰∏ÄÔºåÂÖ∂‰ªñÁÆÄÂçïÁöÑÈùûÂèÇÊï∞Á∫¶ÊùüÊàñÂü∫‰∫éGANÁöÑÁõÆÊ†á‰πüËÉΩÂÆûÁé∞Áõ∏‰ººÁöÑÁ®≥ÂÆöÂäüËÉΩ„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂‰∏∫Ëí∏È¶èËøáÁ®ãÁöÑÂéüÂàôÊÄß‰øÆÊîπÊèê‰æõ‰∫ÜÂü∫Á°ÄÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫ÜÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2511.22475', 'title': 'Adversarial Flow Models', 'url': 'https://huggingface.co/papers/2511.22475', 'abstract': 'Adversarial flow models unify adversarial and flow-based generative models, offering stable training, efficient generation, and high performance on image datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t We present adversarial flow models, a class of generative models that unifies adversarial models and flow models. Our method supports native one-step or multi-step generation and is trained using the adversarial objective. Unlike traditional GANs, where the generator learns an arbitrary transport plan between the noise and the data distributions, our generator learns a deterministic noise-to-data mapping, which is the same optimal transport as in flow-matching models. This significantly stabilizes adversarial training. Also, unlike consistency-based methods, our model directly learns one-step or few-step generation without needing to learn the intermediate timesteps of the probability flow for propagation. This saves model capacity, reduces training iterations, and avoids error accumulation. Under the same 1NFE setting on ImageNet-256px, our B/2 model approaches the performance of consistency-based XL/2 models, while our XL/2 model creates a new best FID of 2.38. We additionally show the possibility of end-to-end training of 56-layer and 112-layer models through depth repetition without any intermediate supervision, and achieve FIDs of 2.08 and 1.94 using a single forward pass, surpassing their 2NFE and 4NFE counterparts.', 'score': 21, 'issue_id': 1, 'pub_date': '2025-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': '6dc43b8b78da2d3a', 'authors': ['Shanchuan Lin', 'Ceyuan Yang', 'Zhijie Lin', 'Hao Chen', 'Haoqi Fan'], 'affiliations': ['Bytedance'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.22475.jpg', 'data': {'categories': ['#inference', '#optimization', '#diffusion', '#architecture', '#training'], 'emoji': '‚ö°', 'ru': {'title': '–°—Ç–∞–±–∏–ª—å–Ω–æ–µ —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã adversarial flow –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç adversarial –ø–æ–¥—Ö–æ–¥ –∏ flow-based –º–µ—Ç–æ–¥—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ–±—É—á–∞–µ—Ç—Å—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—é –∏–∑ —à—É–º–æ–≤–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç, —á—Ç–æ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç adversarial –æ–±—É—á–µ–Ω–∏–µ. –ú–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–¥–Ω–æ—à–∞–≥–æ–≤—É—é –∏–ª–∏ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏–∑—É—á–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —à–∞–≥–∏, —á—Ç–æ —ç–∫–æ–Ω–æ–º–∏—Ç —ë–º–∫–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ —Å–Ω–∏–∂–∞–µ—ÇÁ¥ØÁßØ–æ—à–∏–±–∫—É. –ù–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ ImageNet-256px –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–æ–≤—ã—Ö –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å FID 2.38, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –≤—ã—Å–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥.'}, 'en': {'title': 'Unifying Adversarial and Flow Models for Efficient Image Generation', 'desc': 'Adversarial flow models are a new type of generative model that combine the strengths of adversarial and flow-based approaches. They allow for stable training and efficient image generation by learning a direct mapping from noise to data distributions, rather than relying on complex transport plans. This method reduces the need for intermediate steps in generation, which saves resources and minimizes errors. The models achieve high performance on image datasets, demonstrating their effectiveness in generating high-quality images with fewer training iterations.'}, 'zh': {'title': 'ÂØπÊäóÊµÅÊ®°ÂûãÔºöÁ®≥ÂÆöÈ´òÊïàÁöÑÁîüÊàêÊñ∞ÊñπÊ≥ï', 'desc': 'ÂØπÊäóÊµÅÊ®°ÂûãÊòØ‰∏ÄÁßçÁîüÊàêÊ®°ÂûãÔºåÁªìÂêà‰∫ÜÂØπÊäóÊ®°ÂûãÂíåÊµÅÊ®°ÂûãÁöÑ‰ºòÁÇπ„ÄÇËØ•ÊñπÊ≥ïÊîØÊåÅ‰∏ÄÊ≠•ÊàñÂ§öÊ≠•ÁîüÊàêÔºåÂπ∂‰ΩøÁî®ÂØπÊäóÁõÆÊ†áËøõË°åËÆ≠ÁªÉ„ÄÇ‰∏é‰º†ÁªüÁöÑÁîüÊàêÂØπÊäóÁΩëÁªúÔºàGANÔºâ‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÁîüÊàêÂô®Â≠¶‰π†ÁöÑÊòØÁ°ÆÂÆöÊÄßÁöÑÂô™Â£∞Âà∞Êï∞ÊçÆÁöÑÊò†Â∞ÑÔºå‰ªéËÄåÊòæËëóÁ®≥ÂÆö‰∫ÜÂØπÊäóËÆ≠ÁªÉ„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÁõ¥Êé•Â≠¶‰π†‰∏ÄÊ≠•ÊàñÂ∞ëÊ≠•ÁîüÊàêÔºåËäÇÁúÅ‰∫ÜÊ®°ÂûãÂÆπÈáèÔºåÂáèÂ∞ë‰∫ÜËÆ≠ÁªÉËø≠‰ª£ÔºåÂπ∂ÈÅøÂÖç‰∫ÜËØØÂ∑ÆÁ¥ØÁßØ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2511.22815', 'title': 'Captain Safari: A World Engine', 'url': 'https://huggingface.co/papers/2511.22815', 'abstract': 'Captain Safari, a pose-conditioned world engine using a dynamic local memory and retriever for pose-aligned world tokens, generates high-quality, 3D-consistent long videos with accurate camera maneuvers, outperforming existing methods across quality, consistency, and trajectory following.  \t\t\t\t\tAI-generated summary \t\t\t\t World engines aim to synthesize long, 3D-consistent videos that support interactive exploration of a scene under user-controlled camera motion. However, existing systems struggle under aggressive 6-DoF trajectories and complex outdoor layouts: they lose long-range geometric coherence, deviate from the target path, or collapse into overly conservative motion. To this end, we introduce Captain Safari, a pose-conditioned world engine that generates videos by retrieving from a persistent world memory. Given a camera path, our method maintains a dynamic local memory and uses a retriever to fetch pose-aligned world tokens, which then condition video generation along the trajectory. This design enables the model to maintain stable 3D structure while accurately executing challenging camera maneuvers. To evaluate this setting, we curate OpenSafari, a new in-the-wild FPV dataset containing high-dynamic drone videos with verified camera trajectories, constructed through a multi-stage geometric and kinematic validation pipeline. Across video quality, 3D consistency, and trajectory following, Captain Safari substantially outperforms state-of-the-art camera-controlled generators. It reduces MEt3R from 0.3703 to 0.3690, improves AUC@30 from 0.181 to 0.200, and yields substantially lower FVD than all camera-controlled baselines. More importantly, in a 50-participant, 5-way human study where annotators select the best result among five anonymized models, 67.6% of preferences favor our method across all axes. Our results demonstrate that pose-conditioned world memory is a powerful mechanism for long-horizon, controllable video generation and provide OpenSafari as a challenging new benchmark for future world-engine research.', 'score': 9, 'issue_id': 1, 'pub_date': '2025-11-28', 'pub_date_card': {'ru': '28 –Ω–æ—è–±—Ä—è', 'en': 'November 28', 'zh': '11Êúà28Êó•'}, 'hash': '01688d8243558dc1', 'authors': ['Yu-Cheng Chou', 'Xingrui Wang', 'Yitong Li', 'Jiahao Wang', 'Hanting Liu', 'Cihang Xie', 'Alan Yuille', 'Junfei Xiao'], 'affiliations': ['Johns Hopkins University', 'Tsinghua University', 'UC Santa Cruz'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.22815.jpg', 'data': {'categories': ['#dataset', '#open_source', '#benchmark', '#video', '#3d'], 'emoji': 'üöÅ', 'ru': {'title': '–ú–∏—Ä–æ–≤–æ–π –¥–≤–∏–∂–æ–∫ —Å –ø–∞–º—è—Ç—å—é –¥–ª—è —É–ø—Ä–∞–≤–ª—è–µ–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–æ–ª–≥–∏—Ö –≤–∏–¥–µ–æ', 'desc': 'Captain Safari ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —Å —Ç–æ—á–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–∞–º–µ—Ä—ã, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–º—è—Ç—å –∏ –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤, –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã—Ö –ø–æ –ø–æ–ª–æ–∂–µ–Ω–∏—é –∫–∞–º–µ—Ä—ã. –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—É—é 3D-—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å—Ü–µ–Ω—ã –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –º–∞–Ω–µ–≤—Ä–æ–≤ –∫–∞–º–µ—Ä—ã –≤ —à–µ—Å—Ç–∏ —Å—Ç–µ–ø–µ–Ω—è—Ö —Å–≤–æ–±–æ–¥—ã, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∞–∂–µ –Ω–∞ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç OpenSafari —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –≤–∏–¥–µ–æ –¥—Ä–æ–Ω–æ–≤ –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–º–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ –∫–∞–º–µ—Ä—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –≤ –¥–∏–∫–æ–π –ø—Ä–∏—Ä–æ–¥–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ—Ç–æ–¥–∞ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ, 3D-—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω–æ–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–æ–π –≤ 67.6% –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π.'}, 'en': {'title': 'Revolutionizing 3D Video Generation with Pose-Conditioned Memory', 'desc': 'Captain Safari is a novel pose-conditioned world engine designed to generate long, 3D-consistent videos that allow for interactive exploration with user-controlled camera movements. It addresses the limitations of existing systems that struggle with complex trajectories and outdoor environments by utilizing a dynamic local memory and a retriever to fetch pose-aligned world tokens. This approach ensures stable 3D structure and accurate camera maneuvers, significantly improving video quality and trajectory adherence. The introduction of the OpenSafari dataset further validates the effectiveness of Captain Safari, showcasing its superior performance in comparison to state-of-the-art methods.'}, 'zh': {'title': 'ÂßøÊÄÅÈ©±Âä®ÁöÑ‰∏ñÁïåÂºïÊìéÔºåÁîüÊàêÈ´òË¥®ÈáèÈïøËßÜÈ¢ë', 'desc': 'Captain Safari ÊòØ‰∏ÄÁßçÂü∫‰∫éÂßøÊÄÅÁöÑ‰∏ñÁïåÂºïÊìéÔºåÂà©Áî®Âä®ÊÄÅÊú¨Âú∞ËÆ∞ÂøÜÂíåÊ£ÄÁ¥¢Âô®ÁîüÊàêÈ´òË¥®Èáè„ÄÅ‰∏âÁª¥‰∏ÄËá¥ÁöÑÈïøËßÜÈ¢ë„ÄÇËØ•ÊñπÊ≥ïÈÄöËøá‰ªéÊåÅ‰πÖÁöÑ‰∏ñÁïåËÆ∞ÂøÜ‰∏≠Ê£ÄÁ¥¢‰∏éÂßøÊÄÅÂØπÈΩêÁöÑ‰∏ñÁïåÊ†áËÆ∞ÔºåÁ°Æ‰øùÂú®Â§çÊùÇÁöÑÁõ∏Êú∫Ë∑ØÂæÑ‰∏ã‰øùÊåÅÁ®≥ÂÆöÁöÑ‰∏âÁª¥ÁªìÊûÑ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåCaptain Safari Âú®ËßÜÈ¢ëË¥®Èáè„ÄÅ‰∏ÄËá¥ÊÄßÂíåËΩ®ËøπË∑üË∏™ÊñπÈù¢Ë°®Áé∞‰ºòË∂äÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁîüÊàêËßÜÈ¢ëÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨ËøòÊûÑÂª∫‰∫Ü OpenSafari Êï∞ÊçÆÈõÜÔºå‰∏∫Êú™Êù•ÁöÑ‰∏ñÁïåÂºïÊìéÁ†îÁ©∂Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊåëÊàòÂü∫ÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2511.22787', 'title': 'World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models', 'url': 'https://huggingface.co/papers/2511.22787', 'abstract': 'LVLMs struggle with preserving cultural identities in mixed visual scenes, but supervised fine-tuning with culture mixing datasets improves their performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In a globalized world, cultural elements from diverse origins frequently appear together within a single visual scene. We refer to these as culture mixing scenarios, yet how Large Vision-Language Models (LVLMs) perceive them remains underexplored. We investigate culture mixing as a critical challenge for LVLMs and examine how current models behave when cultural items from multiple regions appear together. To systematically analyze these behaviors, we construct CultureMix, a food Visual Question Answering (VQA) benchmark with 23k diffusion-generated, human-verified culture mixing images across four subtasks: (1) food-only, (2) food+food, (3) food+background, and (4) food+food+background. Evaluating 10 LVLMs, we find consistent failures to preserve individual cultural identities in mixed settings. Models show strong background reliance, with accuracy dropping 14% when cultural backgrounds are added to food-only baselines, and they produce inconsistent predictions for identical foods across different contexts. To address these limitations, we explore three robustness strategies. We find supervised fine-tuning using a diverse culture mixing dataset substantially improve model consistency and reduce background sensitivity. We call for increased attention to culture mixing scenarios as a critical step toward developing LVLMs capable of operating reliably in culturally diverse real-world environments.', 'score': 8, 'issue_id': 1, 'pub_date': '2025-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': '875721f95f9deab4', 'authors': ['Eunsu Kim', 'Junyeong Park', 'Na Min An', 'Junseong Kim', 'Hitesh Laxmichand Patel', 'Jiho Jin', 'Julia Kruk', 'Amit Agarwal', 'Srikant Panda', 'Fenal Ashokbhai Ilasariya', 'Hyunjung Shim', 'Alice Oh'], 'affiliations': ['KAIST', 'Meta', 'Oracle', 'Stevens Institute of Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.22787.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#synthetic', '#benchmark', '#ethics', '#training', '#cv'], 'emoji': 'üåç', 'ru': {'title': '–û–±—É—á–µ–Ω–∏–µ LVLM –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ö', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –º–æ–¥–µ–ª—è–º–∏ LVLM —Å–º–µ—à–∞–Ω–Ω—ã—Ö –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö —Å—Ü–µ–Ω, –≥–¥–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Ä–∞–∑–Ω—ã—Ö –∫—É–ª—å—Ç—É—Ä –ø–æ—è–≤–ª—è—é—Ç—Å—è –≤–º–µ—Å—Ç–µ –Ω–∞ –æ–¥–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ CultureMix —Å 23 —Ç—ã—Å—è—á–∞–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –ø–∏—Ç–∞–Ω–∏—è –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫—É–ª—å—Ç—É—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –∫—É–ª—å—Ç—É—Ä–Ω—É—é –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å –≤ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ö. –ß–µ—Ä–µ–∑ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 10 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö LVLM –≤—ã—è–≤–ª–µ–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –ø–ª–æ—Ö–æ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –∫—É–ª—å—Ç—É—Ä–Ω—É—é –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏ —Å–∏–ª—å–Ω–æ –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Ñ–æ–Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ü–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ supervised fine-tuning –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö —Å –∫—É–ª—å—Ç—É—Ä–Ω—ã–º —Å–º–µ—à–∏–≤–∞–Ω–∏–µ–º –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π –∏ —Å–Ω–∏–∂–∞–µ—Ç –∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç —Ñ–æ–Ω–∞.'}, 'en': {'title': 'Enhancing LVLMs for Culturally Diverse Visual Understanding', 'desc': 'This paper addresses the challenges that Large Vision-Language Models (LVLMs) face when interpreting mixed cultural elements in visual scenes. It introduces a benchmark called CultureMix, which consists of 23,000 images designed to test LVLMs on various food-related tasks involving cultural mixing. The study reveals that LVLMs often fail to maintain the distinct identities of different cultures when they are presented together, particularly showing a reliance on background context. To improve performance, the authors propose supervised fine-tuning with diverse culture mixing datasets, which significantly enhances model consistency and reduces sensitivity to background information.'}, 'zh': {'title': 'ÊèêÂçáLVLMsÂú®ÊñáÂåñÊ∑∑ÂêàÂú∫ÊôØ‰∏≠ÁöÑË°®Áé∞', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâÂú®Ê∑∑ÂêàÊñáÂåñÂú∫ÊôØ‰∏≠‰øùÊåÅÊñáÂåñË∫´‰ªΩÁöÑÊåëÊàò„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂΩì‰∏çÂêåÊñáÂåñÂÖÉÁ¥†ÂêåÊó∂Âá∫Áé∞Âú®ËßÜËßâÂú∫ÊôØ‰∏≠Êó∂ÔºåÁé∞ÊúâÊ®°ÂûãÁöÑË°®Áé∞‰∏ç‰Ω≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®ËÉåÊôØ‰ø°ÊÅØÁöÑÂΩ±Âìç‰∏ã„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºå‰ΩúËÄÖÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫CultureMixÁöÑËßÜËßâÈóÆÁ≠îÂü∫ÂáÜÔºåÂπ∂ÈÄöËøáÁõëÁù£ÂæÆË∞ÉÊñáÂåñÊ∑∑ÂêàÊï∞ÊçÆÈõÜÊù•ÊèêÈ´òÊ®°ÂûãÁöÑ‰∏ÄËá¥ÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ¢ûÂº∫ÂØπÊñáÂåñÊ∑∑ÂêàÂú∫ÊôØÁöÑÂÖ≥Ê≥®ÊòØÊèêÂçáLVLMsÂú®Â§öÂÖÉÊñáÂåñÁéØÂ¢É‰∏≠ÂèØÈù†ÊÄßÁöÑÂÖ≥ÈîÆ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2511.22055', 'title': 'OralGPT-Omni: A Versatile Dental Multimodal Large Language Model', 'url': 'https://huggingface.co/papers/2511.22055', 'abstract': "OralGPT-Omni, a dental-specialized multimodal large language model, enhances dental image analysis through TRACE-CoT reasoning supervision and achieves superior performance on MMOral-Uni benchmark compared to GPT-5.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have exhibited immense potential across numerous medical specialties; yet, dentistry remains underexplored, in part due to limited domain-specific data, scarce dental expert annotations, insufficient modality-specific modeling, and challenges in reliability. In this paper, we present OralGPT-Omni, the first dental-specialized MLLM designed for comprehensive and trustworthy analysis across diverse dental imaging modalities and clinical tasks. To explicitly capture dentists' diagnostic reasoning, we construct TRACE-CoT, a clinically grounded chain-of-thought dataset that mirrors dental radiologists' decision-making processes. This reasoning supervision, combined with our proposed four-stage training paradigm, substantially strengthens the model's capacity for dental image understanding and analysis. In parallel, we introduce MMOral-Uni, the first unified multimodal benchmark for dental image analysis. It comprises 2,809 open-ended question-answer pairs spanning five modalities and five tasks, offering a comprehensive evaluation suite to date for MLLMs in digital dentistry. OralGPT-Omni achieves an overall score of 51.84 on the MMOral-Uni benchmark and 45.31 on the MMOral-OPG benchmark, dramatically outperforming the scores of GPT-5. Our work promotes intelligent dentistry and paves the way for future advances in dental image analysis. All code, benchmark, and models will be made publicly available.", 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': 'e10e022898c2e810', 'authors': ['Jing Hao', 'Yuci Liang', 'Lizhuo Lin', 'Yuxuan Fan', 'Wenkai Zhou', 'Kaixin Guo', 'Zanting Ye', 'Yanpeng Sun', 'Xinyu Zhang', 'Yanqi Yang', 'Qiankun Li', 'Hao Tang', 'James Kit-Hon Tsoi', 'Linlin Shen', 'Kuo Feng Hung'], 'affiliations': ['College of Artificial Intelligence, Shenzhen University', 'College of Computer Science and Software Engineering, Shenzhen University', 'Faculty of Dentistry, The University of Hong Kong', 'School of Biomedical Engineering, Southern Medical University', 'School of Computer Science, Peking University', 'Singapore University of Technology and Design', 'The Hong Kong University of Science and Technology (GZ)', 'University of Auckland', 'University of Science and Technology of China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.22055.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#open_source', '#multimodal', '#benchmark', '#healthcare', '#training', '#science'], 'emoji': 'ü¶∑', 'ru': {'title': '–°—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ò–ò —Å –∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–π –ª–æ–≥–∏–∫–æ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': 'OralGPT-Omni ‚Äî —ç—Ç–æ –ø–µ—Ä–≤–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (MLLM) –¥–ª—è —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TRACE-CoT, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —Ü–µ–ø–æ—á–∫–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –æ—Ç—Ä–∞–∂–∞—é—â–∏–º–∏ –ª–æ–≥–∏–∫—É –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–æ–≤-—Ä–µ–Ω—Ç–≥–µ–Ω–æ–ª–æ–≥–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–ª–∏ MMOral-Uni ‚Äî –ø–µ—Ä–≤—ã–π —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –±–æ–ª–µ–µ 2800 –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ –ø—è—Ç–∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º. OralGPT-Omni –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç GPT-5 –∏ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ, –ø—Ä–æ–¥–≤–∏–≥–∞—è —Ä–∞–∑–≤–∏—Ç–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏–∏.'}, 'en': {'title': 'Revolutionizing Dental Image Analysis with OralGPT-Omni', 'desc': 'OralGPT-Omni is a specialized multimodal large language model (MLLM) designed to improve dental image analysis. It utilizes TRACE-CoT reasoning supervision to better mimic the diagnostic processes of dental professionals, enhancing its understanding of dental images. The model was evaluated using the MMOral-Uni benchmark, where it significantly outperformed GPT-5, achieving high scores in various dental tasks. This research aims to advance intelligent dentistry by providing a robust framework for analyzing dental images and making the tools publicly available for further development.'}, 'zh': {'title': 'Êô∫ËÉΩÁâôÁßëÁöÑÊú™Êù•ÔºöOralGPT-OmniÁöÑÂ¥õËµ∑', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜOralGPT-OmniÔºåËøôÊòØ‰∏Ä‰∏™‰∏ìÈó®ÈíàÂØπÁâôÁßëÁöÑÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºåÊó®Âú®ÊèêÂçáÁâôÁßëÂõæÂÉèÂàÜÊûêÁöÑËÉΩÂäõ„ÄÇÈÄöËøáTRACE-CoTÊé®ÁêÜÁõëÁù£ÔºåËØ•Ê®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâÁâôÂåªÁöÑËØäÊñ≠Êé®ÁêÜËøáÁ®ãÔºå‰ªéËÄåÊèêÈ´òÂØπÁâôÁßëÂõæÂÉèÁöÑÁêÜËß£ÂíåÂàÜÊûêËÉΩÂäõ„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜMMOral-UniÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Áªü‰∏ÄÁöÑÁâôÁßëÂõæÂÉèÂàÜÊûêÂü∫ÂáÜÔºåÂåÖÂê´Â§öÁßçÊ®°ÊÄÅÂíå‰ªªÂä°ÁöÑÂºÄÊîæÂºèÈóÆÁ≠îÂØπ„ÄÇOralGPT-OmniÂú®ËØ•Âü∫ÂáÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåÊòæËëóË∂ÖË∂ä‰∫ÜGPT-5ÁöÑÊàêÁª©ÔºåÊé®Âä®‰∫ÜÊô∫ËÉΩÁâôÁßëÁöÑÂèëÂ±ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2511.21750', 'title': 'SO-Bench: A Structural Output Evaluation of Multimodal LLMs', 'url': 'https://huggingface.co/papers/2511.21750', 'abstract': "A benchmark evaluates schema-grounded information extraction and reasoning over visual inputs for multimodal large language models, revealing gaps and guiding future improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.", 'score': 5, 'issue_id': 1, 'pub_date': '2025-11-23', 'pub_date_card': {'ru': '23 –Ω–æ—è–±—Ä—è', 'en': 'November 23', 'zh': '11Êúà23Êó•'}, 'hash': '6368fe988a89fff8', 'authors': ['Di Feng', 'Kaixin Ma', 'Feng Nan', 'Haofeng Chen', 'Bohan Zhai', 'David Griffiths', 'Mingfei Gao', 'Zhe Gan', 'Eshan Verma', 'Yinfei Yang', 'Zhifeng Chen', 'Afshin Dehghan'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.21750.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#open_source', '#multimodal', '#benchmark', '#survey'], 'emoji': 'üìã', 'ru': {'title': '–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ—Ü–µ–Ω–∫–∞ –∏ —É–ª—É—á—à–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω SO-Bench ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Ö–æ–¥—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º–∏ —Å—Ö–µ–º–∞–º–∏ JSON. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —á–µ—Ç—ã—Ä–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–∞: UI-—ç–∫—Ä–∞–Ω—ã, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏, –≤–∫–ª—é—á–∞—è –±–æ–ª–µ–µ 6,5 —Ç—ã—Å—è—á —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ö–µ–º –∏ 1,8 —Ç—ã—Å—è—á–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Å—Ö–µ–º–∞. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π –≤—ã—è–≤–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–æ—á–Ω—ã–µ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ö–µ–º–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—ã—Ö–æ–¥—ã –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π —É–ª—É—á—à–µ–Ω–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –∏ –ø–ª–∞–Ω–∏—Ä—É—é—Ç —Å–¥–µ–ª–∞—Ç—å –±–µ–Ω—á–º–∞—Ä–∫ –¥–æ—Å—Ç—É–ø–Ω—ã–º –¥–ª—è —Å–æ–æ–±—â–µ—Å—Ç–≤–∞.'}, 'en': {'title': 'Bridging Gaps in Multimodal Schema-Grounded Reasoning', 'desc': 'This paper introduces a benchmark called SO-Bench, designed to evaluate how well multimodal large language models (MLLMs) can extract information and reason about visual inputs while adhering to specific data schemas. The benchmark includes a diverse set of over 6,500 JSON schemas and 1,800 image-schema pairs, covering various visual domains such as UI screens and natural images. The study reveals significant gaps in the ability of current MLLMs to produce accurate outputs that comply with these schemas, indicating a need for improved structured reasoning capabilities. Additionally, the authors conduct training experiments to enhance these capabilities and plan to share the benchmark with the research community.'}, 'zh': {'title': 'ÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÁªìÊûÑÂåñÊé®ÁêÜËÉΩÂäõ', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÂü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ËßÜËßâËæìÂÖ•‰∏äÁöÑ‰ø°ÊÅØÊèêÂèñÂíåÊé®ÁêÜËÉΩÂäõ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂ∞ΩÁÆ°Âú®ÊñáÊú¨ÁîüÊàêÊñπÈù¢ÂèñÂæó‰∫Ü‰∏Ä‰∫õËøõÂ±ïÔºå‰ΩÜÂú®ËßÜËßâËæìÂÖ•ÁöÑÁªìÊûÑÂåñÁîüÊàê‰∏ä‰ªçÂ≠òÂú®ÊòæËëóÂ∑ÆË∑ù„ÄÇÈÄöËøáËÆæËÆ°SO-BenchÂü∫ÂáÜÔºåÊ∂µÁõñ‰∫ÜÁî®Êà∑ÁïåÈù¢„ÄÅËá™ÁÑ∂ÂõæÂÉè„ÄÅÊñáÊ°£ÂíåÂõæË°®Á≠âÂõõ‰∏™ËßÜËßâÈ¢ÜÂüüÔºå‰ΩøÁî®‰∫ÜË∂ÖËøá6500‰∏™Â§öÊ†∑ÂåñÁöÑJSONÊ®°ÂºèÂíå1800‰∏™ÁªèËøá‰∫∫Â∑•È™åËØÅÁöÑÂõæÂÉè-Ê®°ÂºèÂØπ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂΩìÂâçÊ®°ÂûãÂú®ÁîüÊàêÁ¨¶ÂêàÈ¢ÑÂÆö‰πâÊï∞ÊçÆÊ®°ÂºèÁöÑËæìÂá∫ÊñπÈù¢‰ªçÊúâÂæÖÊîπËøõÔºåÂº∫Ë∞É‰∫ÜÂ§öÊ®°ÊÄÅÁªìÊûÑÂåñÊé®ÁêÜËÉΩÂäõÁöÑÊèêÂçáÈúÄÊ±Ç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2511.22688', 'title': 'Test-time scaling of diffusions with flow maps', 'url': 'https://huggingface.co/papers/2511.22688', 'abstract': 'The proposed Flow Map Trajectory Tilting (FMTT) algorithm improves diffusion models at test-time by leveraging flow maps to better align with user-specified rewards, enabling more effective sampling and image editing.  \t\t\t\t\tAI-generated summary \t\t\t\t A common recipe to improve diffusion models at test-time so that samples score highly against a user-specified reward is to introduce the gradient of the reward into the dynamics of the diffusion itself. This procedure is often ill posed, as user-specified rewards are usually only well defined on the data distribution at the end of generation. While common workarounds to this problem are to use a denoiser to estimate what a sample would have been at the end of generation, we propose a simple solution to this problem by working directly with a flow map. By exploiting a relationship between the flow map and velocity field governing the instantaneous transport, we construct an algorithm, Flow Map Trajectory Tilting (FMTT), which provably performs better ascent on the reward than standard test-time methods involving the gradient of the reward. The approach can be used to either perform exact sampling via importance weighting or principled search that identifies local maximizers of the reward-tilted distribution. We demonstrate the efficacy of our approach against other look-ahead techniques, and show how the flow map enables engagement with complicated reward functions that make possible new forms of image editing, e.g. by interfacing with vision language models.', 'score': 4, 'issue_id': 1, 'pub_date': '2025-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': '454c4582d6e71d10', 'authors': ['Amirmojtaba Sabour', 'Michael S. Albergo', 'Carles Domingo-Enrich', 'Nicholas M. Boffi', 'Sanja Fidler', 'Karsten Kreis', 'Eric Vanden-Eijnden'], 'affiliations': ['Carnegie Mellon University', 'Courant Institute, New York University', 'Harvard University', 'IAIFI', 'Kempner Institute', 'ML Lab at Capital Fund Management (CFM)', 'Microsoft Research', 'NVIDIA', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.22688.jpg', 'data': {'categories': ['#inference', '#multimodal', '#optimization', '#diffusion'], 'emoji': 'üéØ', 'ru': {'title': '–¢—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã —á–µ—Ä–µ–∑ –∫–∞—Ä—Ç—É –ø–æ—Ç–æ–∫–∞', 'desc': '–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º Flow Map Trajectory Tilting (FMTT) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —ç—Ç–∞–ø–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç flow map –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –Ω–∞–≥—Ä–∞–¥—ã, –∑–∞–¥–∞–Ω–Ω–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º, —Ä–µ—à–∞—è –ø—Ä–æ–±–ª–µ–º—É —Ç–æ–≥–æ, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –∫–æ–Ω—Ü–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ê–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ —Å flow map –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–≥–æ —Å–≤—è–∑—å —Å velocity field, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –ª—É—á—à–µ–≥–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—è –ø–æ –Ω–∞–≥—Ä–∞–¥–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –ü–æ–¥—Ö–æ–¥ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ —Ç–æ—á–Ω–æ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ importance weighting, —Ç–∞–∫ –∏ —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∞–∫—Å–∏–º—É–º–æ–≤, –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –∫ —Å–ª–æ–∂–Ω—ã–º —Ñ—É–Ω–∫—Ü–∏—è–º –Ω–∞–≥—Ä–∞–¥—ã, –≤–∫–ª—é—á–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å vision language –º–æ–¥–µ–ª—è–º–∏.'}, 'en': {'title': 'Enhancing Diffusion Models with Flow Map Trajectory Tilting', 'desc': 'The Flow Map Trajectory Tilting (FMTT) algorithm enhances diffusion models during testing by utilizing flow maps to align better with user-defined rewards. This method addresses the challenge of incorporating reward gradients, which are often poorly defined during the diffusion process. By directly manipulating the flow map, FMTT allows for more effective sampling and image editing, outperforming traditional methods that rely on reward gradients. The algorithm also facilitates interaction with complex reward functions, enabling innovative applications in image generation and editing.'}, 'zh': {'title': 'ÊµÅÂõæËΩ®ËøπÂÄæÊñúÔºöÊèêÂçáÊâ©Êï£Ê®°ÂûãÁöÑÊô∫ËÉΩÈááÊ†∑‰∏éÁºñËæë', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÊµÅÂõæËΩ®ËøπÂÄæÊñúÔºàFMTTÔºâÁöÑÁÆóÊ≥ïÔºåÊó®Âú®ÈÄöËøáÂà©Áî®ÊµÅÂõæÂú®ÊµãËØïÊó∂ÊîπËøõÊâ©Êï£Ê®°ÂûãÔºå‰ª•Êõ¥Â•ΩÂú∞‰∏éÁî®Êà∑ÊåáÂÆöÁöÑÂ•ñÂä±ÂØπÈΩêÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÊúâÊïàÁöÑÈááÊ†∑ÂíåÂõæÂÉèÁºñËæë„ÄÇ‰º†ÁªüÊñπÊ≥ïÂú®Êâ©Êï£ËøáÁ®ã‰∏≠ÂºïÂÖ•Â•ñÂä±ÁöÑÊ¢ØÂ∫¶Ôºå‰ΩÜÂ∏∏Â∏∏‰ºöÂØºËá¥ÈóÆÈ¢òÔºåÂõ†‰∏∫Áî®Êà∑ÊåáÂÆöÁöÑÂ•ñÂä±ÈÄöÂ∏∏Âè™Âú®ÁîüÊàêÁªìÊùüÊó∂ÊâçÊòéÁ°ÆÂÆö‰πâ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÁõ¥Êé•‰ΩøÁî®ÊµÅÂõæÔºåÂà©Áî®ÊµÅÂõæ‰∏éÁû¨Êó∂‰º†ËæìÈÄüÂ∫¶Âú∫‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºåÊûÑÂª∫‰∫ÜFMTTÁÆóÊ≥ïÔºåËØÅÊòéÂÖ∂Âú®Â•ñÂä±‰∏äÊØîÊ†áÂáÜÊµãËØïÊñπÊ≥ïË°®Áé∞Êõ¥‰Ω≥„ÄÇËØ•ÊñπÊ≥ïÂèØ‰ª•ÈÄöËøáÈáçË¶ÅÊÄßÂä†ÊùÉËøõË°åÁ≤æÁ°ÆÈááÊ†∑ÔºåÊàñÈÄöËøáÂéüÂàôÊÄßÊêúÁ¥¢ËØÜÂà´Â•ñÂä±ÂÄæÊñúÂàÜÂ∏ÉÁöÑÂ±ÄÈÉ®ÊûÅÂ§ßÂÄºÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§çÊùÇÂ•ñÂä±ÂáΩÊï∞‰∏ãÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2511.22805', 'title': 'From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images', 'url': 'https://huggingface.co/papers/2511.22805', 'abstract': "CogIP-Bench evaluates MLLMs on image cognitive properties, revealing a gap that post-training can bridge, enhancing human-like perception and improving creative tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t While Multimodal Large Language Models (MLLMs) are adept at answering what is in an image-identifying objects and describing scenes-they often lack the ability to understand how an image feels to a human observer. This gap is most evident when considering subjective cognitive properties, such as what makes an image memorable, funny, aesthetically pleasing, or emotionally evocative. To systematically address this challenge, we introduce CogIP-Bench, a comprehensive benchmark for evaluating MLLMs on such image cognitive properties. Our evaluation reveals a significant gap: current models are poorly aligned with human perception of these nuanced properties. We then demonstrate that a post-training phase can effectively bridge this gap, significantly enhancing the model's alignment with human judgments. Furthermore, we show that this learned cognitive alignment is not merely predictive but also transferable to downstream creative tasks. By integrating our cognitively-aligned MLLM into an image generation pipeline, we can guide the synthesis process to produce images that better embody desired traits, such as being more memorable or visually appealing. Our work provides a benchmark to measure this human-like perception, a post-training pipeline to enhance it, and a demonstration that this alignment unlocks more human-centric AI.", 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': '72dc20b8e577714a', 'authors': ['Yiming Chen', 'Junlin Han', 'Tianyi Bai', 'Shengbang Tong', 'Filippos Kokkinos', 'Philip Torr'], 'affiliations': ['HKUST', 'New York University', 'Oxford University', 'University College London'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.22805.jpg', 'data': {'categories': ['#multimodal', '#alignment', '#benchmark', '#interpretability', '#training'], 'emoji': 'üé®', 'ru': {'title': '–ö–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ CogIP-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–≤–æ–π—Å—Ç–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∑–∞–ø–æ–º–∏–Ω–∞–µ–º–æ—Å—Ç—å, —é–º–æ—Ä –∏ —ç—Å—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Ç–µ–∫—É—â–∏–º–∏ MLLM –∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º —á–µ–ª–æ–≤–µ–∫–∞ –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ —ç—Ç–∏—Ö —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —ç—Ç–∞–ø –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (post-training) —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç —ç—Ç–æ—Ç —Ä–∞–∑—Ä—ã–≤ –∏ —É–ª—É—á—à–∞–µ—Ç –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ —Å—É–∂–¥–µ–Ω–∏—è–º–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ, —á—Ç–æ –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ–Ω–∏–º–æ –∫ –∑–∞–¥–∞—á–∞–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø–æ–∑–≤–æ–ª—è—è —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å –∂–µ–ª–∞–µ–º—ã–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏.'}, 'en': {'title': "Bridging the Gap: Enhancing AI's Understanding of Image Perception", 'desc': "The paper introduces CogIP-Bench, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on their understanding of cognitive properties of images. It highlights a significant gap in current MLLMs, which can identify objects in images but struggle with subjective qualities like emotional impact and aesthetic appeal. The authors propose a post-training phase that enhances the models' alignment with human perception, allowing them to better understand and generate images that resonate on a cognitive level. This improved alignment not only aids in image generation but also enhances the models' performance in creative tasks, making AI more human-centric."}, 'zh': {'title': 'Áº©Â∞è‰∫∫Á±ª‰∏éAIÁöÑËÆ§Áü•Â∑ÆË∑ù', 'desc': 'CogIP-Bench ÊòØ‰∏Ä‰∏™ËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÂõæÂÉèËÆ§Áü•Â±ûÊÄß‰∏äÁöÑÂü∫ÂáÜ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁé∞ÊúâÊ®°ÂûãÂú®ÁêÜËß£ÂõæÂÉèÁöÑ‰∏ªËßÇËÆ§Áü•Â±ûÊÄßÊñπÈù¢‰∏é‰∫∫Á±ªÊÑüÁü•Â≠òÂú®ÊòæËëóÂ∑ÆË∑ù„ÄÇÈÄöËøáÂêéÊúüËÆ≠ÁªÉÔºåÂèØ‰ª•ÊúâÊïàÁº©Â∞èËøô‰∏ÄÂ∑ÆË∑ùÔºå‰ªéËÄåÊèêÂçáÊ®°Âûã‰∏é‰∫∫Á±ªÂà§Êñ≠ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊ≠§Â§ñÔºåËøôÁßçËÆ§Áü•ÂØπÈΩê‰∏ç‰ªÖÂÖ∑ÊúâÈ¢ÑÊµãÊÄßÔºåËøòÂèØ‰ª•ËΩ¨ÁßªÂà∞‰∏ãÊ∏∏ÁöÑÂàõÊÑè‰ªªÂä°‰∏≠„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2511.19990', 'title': 'OmniRefiner: Reinforcement-Guided Local Diffusion Refinement', 'url': 'https://huggingface.co/papers/2511.19990', 'abstract': 'A detail-aware refinement framework using single-image diffusion and reinforcement learning enhances reference-guided image generation, improving detail preservation and consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Reference-guided image generation has progressed rapidly, yet current diffusion models still struggle to preserve fine-grained visual details when refining a generated image using a reference. This limitation arises because VAE-based latent compression inherently discards subtle texture information, causing identity- and attribute-specific cues to vanish. Moreover, post-editing approaches that amplify local details based on existing methods often produce results inconsistent with the original image in terms of lighting, texture, or shape. To address this, we introduce , a detail-aware refinement framework that performs two consecutive stages of reference-driven correction to enhance pixel-level consistency. We first adapt a single-image diffusion editor by fine-tuning it to jointly ingest the draft image and the reference image, enabling globally coherent refinement while maintaining structural fidelity. We then apply reinforcement learning to further strengthen localized editing capability, explicitly optimizing for detail accuracy and semantic consistency. Extensive experiments demonstrate that  significantly improves reference alignment and fine-grained detail preservation, producing faithful and visually coherent edits that surpass both open-source and commercial models on challenging reference-guided restoration benchmarks.', 'score': 3, 'issue_id': 1, 'pub_date': '2025-11-25', 'pub_date_card': {'ru': '25 –Ω–æ—è–±—Ä—è', 'en': 'November 25', 'zh': '11Êúà25Êó•'}, 'hash': 'eadd286d549aa44a', 'authors': ['Yaoli Liu', 'Ziheng Ouyang', 'Shengtao Lou', 'Yiren Song'], 'affiliations': ['Creatly.ai', 'Nankai University', 'National University of Singapore', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.19990.jpg', 'data': {'categories': ['#optimization', '#rl', '#benchmark', '#diffusion', '#cv'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —É–ø—Ä–∞–≤–ª—è–µ–º–æ–π —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–æ–º, –∏—Å–ø–æ–ª—å–∑—É—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É –ø–æ—Ç–µ—Ä–∏ –º–µ–ª–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π, –≤–æ–∑–Ω–∏–∫–∞—é—â—É—é –∏–∑-–∑–∞ —Å–∂–∞—Ç–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ VAE-–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –∏ –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–µ. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö —ç—Ç–∞–ø–æ–≤: —Å–Ω–∞—á–∞–ª–∞ –∞–¥–∞–ø—Ç–∞—Ü–∏—è –æ–¥–Ω–æ–∫–∞–¥—Ä–æ–≤–æ–≥–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–Ω–æ–≤–∏–∫–∞ –∏ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–∞, –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–µ—Ç–∞–ª–µ–π –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏.'}, 'en': {'title': 'Enhancing Image Generation with Detail-Aware Refinement', 'desc': 'This paper presents a new framework for improving image generation that uses reference images to guide the process. It addresses the common problem of losing fine details during image refinement by using a two-step approach: first, it refines the image globally while keeping its structure intact, and then it enhances local details through reinforcement learning. The framework specifically targets the preservation of textures and attributes that are often lost in traditional methods. Experiments show that this approach significantly outperforms existing models in maintaining visual consistency and detail accuracy.'}, 'zh': {'title': 'ÁªÜËäÇÊÑüÁü•ÔºåÊèêÂçáÂõæÂÉèÁîüÊàêË¥®Èáè', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªÜËäÇÊÑüÁü•ÁöÑÂõæÂÉè‰øÆÂ§çÊ°ÜÊû∂ÔºåÂà©Áî®ÂçïÂõæÂÉèÊâ©Êï£ÂíåÂº∫ÂåñÂ≠¶‰π†Êù•Â¢ûÂº∫ÂèÇËÄÉÂºïÂØºÁöÑÂõæÂÉèÁîüÊàê„ÄÇÂΩìÂâçÁöÑÊâ©Êï£Ê®°ÂûãÂú®‰ΩøÁî®ÂèÇËÄÉÂõæÂÉè‰øÆÂ§çÁîüÊàêÂõæÂÉèÊó∂ÔºåÈöæ‰ª•‰øùÁïôÁªÜËá¥ÁöÑËßÜËßâÁªÜËäÇ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈÄöËøáÂØπÂçïÂõæÂÉèÊâ©Êï£ÁºñËæëÂô®ËøõË°åÂæÆË∞ÉÔºå‰ΩøÂÖ∂ËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜËçâÂõæÂõæÂÉèÂíåÂèÇËÄÉÂõæÂÉèÔºå‰ªéËÄåÂÆûÁé∞ÂÖ®Â±Ä‰∏ÄËá¥ÁöÑ‰øÆÂ§ç„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºåÊàë‰ª¨Ëøõ‰∏ÄÊ≠•‰ºòÂåñ‰∫ÜÂ±ÄÈÉ®ÁºñËæëËÉΩÂäõÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁªÜËäÇÂáÜÁ°ÆÊÄßÂíåËØ≠‰πâ‰∏ÄËá¥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2511.22533', 'title': 'Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration', 'url': 'https://huggingface.co/papers/2511.22533', 'abstract': 'Fast3Dcache accelerates 3D diffusion model inference with minimal geometric quality degradation by using a geometry-aware caching framework with dynamic cache quotas and spatiotemporal stability criteria.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have achieved impressive generative quality across modalities like 2D images, videos, and 3D shapes, but their inference remains computationally expensive due to the iterative denoising process. While recent caching-based methods effectively reuse redundant computations to speed up 2D and video generation, directly applying these techniques to 3D diffusion models can severely disrupt geometric consistency. In 3D synthesis, even minor numerical errors in cached latent features accumulate, causing structural artifacts and topological inconsistencies. To overcome this limitation, we propose Fast3Dcache, a training-free geometry-aware caching framework that accelerates 3D diffusion inference while preserving geometric fidelity. Our method introduces a Predictive Caching Scheduler Constraint (PCSC) to dynamically determine cache quotas according to voxel stabilization patterns and a Spatiotemporal Stability Criterion (SSC) to select stable features for reuse based on velocity magnitude and acceleration criterion. Comprehensive experiments show that Fast3Dcache accelerates inference significantly, achieving up to a 27.12% speed-up and a 54.8% reduction in FLOPs, with minimal degradation in geometric quality as measured by Chamfer Distance (2.48%) and F-Score (1.95%).', 'score': 1, 'issue_id': 1, 'pub_date': '2025-11-27', 'pub_date_card': {'ru': '27 –Ω–æ—è–±—Ä—è', 'en': 'November 27', 'zh': '11Êúà27Êó•'}, 'hash': 'd520e1b6cb263e94', 'authors': ['Mengyu Yang', 'Yanming Yang', 'Chenyi Xu', 'Chenxi Song', 'Yufan Zuo', 'Tong Zhao', 'Ruibo Li', 'Chi Zhang'], 'affiliations': ['AGI Lab, Westlake University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.22533.jpg', 'data': {'categories': ['#inference', '#3d', '#optimization', '#diffusion'], 'emoji': '‚ö°', 'ru': {'title': '–ë—ã—Å—Ç—Ä–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è 3D –¥–∏—Ñ—Ñ—É–∑–∏–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –≥–µ–æ–º–µ—Ç—Ä–∏–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Fast3Dcache ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è inference –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –ø—Ä—è–º–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–æ–µ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è 2D –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –Ω–∞—Ä—É—à–∞–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å 3D –º–æ–¥–µ–ª–µ–π –∏–∑-–∑–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –≤ —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç —ç—Ç—É –∑–∞–¥–∞—á—É —á–µ—Ä–µ–∑ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏ –æ—Å–≤–µ–¥–æ–º–ª—ë–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥: –∏—Å–ø–æ–ª—å–∑—É—é—Ç Predictive Caching Scheduler Constraint –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–±—ä—ë–º–∞ –∫—ç—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≤–æ–∫—Å–µ–ª–µ–π –∏ Spatiotemporal Stability Criterion –¥–ª—è –æ—Ç–±–æ—Ä–∞ –Ω–∞–¥—ë–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–æ 27% –∏ —Å–Ω–∏–∂–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ 55% —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º —Å–Ω–∏–∂–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–æ–º–µ—Ç—Ä–∏–∏.'}, 'en': {'title': 'Accelerating 3D Diffusion with Geometry-Aware Caching', 'desc': 'Fast3Dcache is a novel framework designed to speed up the inference process of 3D diffusion models while maintaining high geometric quality. It utilizes a geometry-aware caching system that dynamically adjusts cache quotas based on the stability of voxel features over time. By implementing a Predictive Caching Scheduler Constraint (PCSC) and a Spatiotemporal Stability Criterion (SSC), the framework effectively reuses stable features, minimizing errors that could lead to geometric inconsistencies. Experimental results demonstrate that Fast3Dcache can significantly enhance inference speed and reduce computational load, achieving notable improvements in performance metrics without compromising the structural integrity of the generated 3D shapes.'}, 'zh': {'title': 'Âä†ÈÄü3DÊé®ÁêÜÔºå‰øùÊåÅÂá†‰ΩïË¥®ÈáèÁöÑÂàõÊñ∞ÊñπÊ°à', 'desc': 'Fast3Dcache ÊòØ‰∏ÄÁßçÂá†‰ΩïÊÑüÁü•ÁºìÂ≠òÊ°ÜÊû∂ÔºåÊó®Âú®Âä†ÈÄü 3D Êâ©Êï£Ê®°ÂûãÁöÑÊé®ÁêÜÔºåÂêåÊó∂‰øùÊåÅÂá†‰ΩïË¥®Èáè„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂä®ÊÄÅÁ°ÆÂÆöÁºìÂ≠òÈÖçÈ¢ùÂíåÁ©∫Èó¥Êó∂Èó¥Á®≥ÂÆöÊÄßÊ†áÂáÜÔºåÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÂºÄÈîÄ„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ïÁõ∏ÊØîÔºåFast3Dcache ËÉΩÂ§üÊòæËëóÊèêÈ´òÊé®ÁêÜÈÄüÂ∫¶ÔºåÊúÄÈ´òÂèØËææ 27.12% ÁöÑÂä†ÈÄüÂíå 54.8% ÁöÑ FLOPs ÂáèÂ∞ë„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂá†‰ΩïË¥®ÈáèÁöÑÈôçËß£ÈùûÂ∏∏Â∞èÔºåChamfer Ë∑ùÁ¶ªÂíå F-Score ÁöÑÂèòÂåñÂàÜÂà´‰ªÖ‰∏∫ 2.48% Âíå 1.95%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2511.16854', 'title': 'MRI Super-Resolution with Deep Learning: A Comprehensive Survey', 'url': 'https://huggingface.co/papers/2511.16854', 'abstract': 'A survey examines deep learning-based super-resolution techniques in MRI, addressing challenges and providing resources for generating high-resolution images from low-resolution scans.  \t\t\t\t\tAI-generated summary \t\t\t\t High-resolution (HR) magnetic resonance imaging (MRI) is crucial for many clinical and research applications. However, achieving it remains costly and constrained by technical trade-offs and experimental limitations. Super-resolution (SR) presents a promising computational approach to overcome these challenges by generating HR images from more affordable low-resolution (LR) scans, potentially improving diagnostic accuracy and efficiency without requiring additional hardware. This survey reviews recent advances in MRI SR techniques, with a focus on deep learning (DL) approaches. It examines DL-based MRI SR methods from the perspectives of computer vision, computational imaging, inverse problems, and MR physics, covering theoretical foundations, architectural designs, learning strategies, benchmark datasets, and performance metrics. We propose a systematic taxonomy to categorize these methods and present an in-depth study of both established and emerging SR techniques applicable to MRI, considering unique challenges in clinical and research contexts. We also highlight open challenges and directions that the community needs to address. Additionally, we provide a collection of essential open-access resources, tools, and tutorials, available on our GitHub: https://github.com/mkhateri/Awesome-MRI-Super-Resolution.   IEEE keywords: MRI, Super-Resolution, Deep Learning, Computational Imaging, Inverse Problem, Survey.', 'score': 0, 'issue_id': 1, 'pub_date': '2025-11-20', 'pub_date_card': {'ru': '20 –Ω–æ—è–±—Ä—è', 'en': 'November 20', 'zh': '11Êúà20Êó•'}, 'hash': 'b16bad51b44ffab8', 'authors': ['Mohammad Khateri', 'Serge Vasylechko', 'Morteza Ghahremani', 'Liam Timms', 'Deniz Kocanaogullari', 'Simon K. Warfield', 'Camilo Jaimes', 'Davood Karimi', 'Alejandra Sierra', 'Jussi Tohka', 'Sila Kurugol', 'Onur Afacan'], 'affiliations': ['A. I. Virtanen Institute for Molecular Sciences, Faculty of Health Sciences, University of Eastern Finland', 'Artificial Intelligence in Medical Imaging group at the Department of Radiology, Technical University of Munich', 'Department of Radiology, Massachusetts General Hospital', 'Harvard Medical School and Boston Childrens Hospital'], 'pdf_title_img': 'assets\\pdf\\title_img\\2511.16854.jpg', 'data': {'categories': ['#dataset', '#open_source', '#benchmark', '#healthcare', '#survey', '#cv', '#science'], 'emoji': 'üî¨', 'ru': {'title': '–û—Ç –ø–∏–∫—Å–µ–ª–µ–π –∫ –¥–µ—Ç–∞–ª—è–º: –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ú–†–¢', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –ú–†–¢-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ä–µ—à–∞—è –ø—Ä–æ–±–ª–µ–º—É –ø–æ–ª—É—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–Ω–∏–º–∫–æ–≤ –∏–∑ –Ω–∏–∑–∫–æ—Ä–µ–∑–æ–ª—é—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∫–∞–Ω–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –ø–æ–¥—Ö–æ–¥—ã —Å—É–ø–µ—Ä—Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —Å –ø–æ–∑–∏—Ü–∏–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–º–æ–≥—Ä–∞—Ñ–∏–∏ –∏ –æ–±—Ä–∞—Ç–Ω—ã—Ö –∑–∞–¥–∞—á, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—É—á–µ–Ω–∏—è. –í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ —Å—É–ø–µ—Ä—Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –¥–ª—è –ú–†–¢ —Å —É—á—ë—Ç–æ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –æ—Ç–∫—Ä—ã—Ç—ã–µ —Ä–µ—Å—É—Ä—Å—ã –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Å–æ–æ–±—â–µ—Å—Ç–≤–µ.'}, 'en': {'title': 'Enhancing MRI with Deep Learning Super-Resolution', 'desc': 'This paper surveys deep learning-based super-resolution (SR) techniques specifically for magnetic resonance imaging (MRI). It highlights the importance of generating high-resolution images from low-resolution scans to enhance diagnostic accuracy while minimizing costs and technical limitations. The authors categorize various deep learning methods, discussing their theoretical foundations, architectures, and performance metrics. Additionally, the paper identifies ongoing challenges in the field and provides valuable resources for researchers and practitioners.'}, 'zh': {'title': 'Ê∑±Â∫¶Â≠¶‰π†Âä©ÂäõMRIË∂ÖÂàÜËæ®ÁéáÊäÄÊúØÁöÑÊú™Êù•', 'desc': 'ËøôÁØáËÆ∫ÊñáË∞ÉÊü•‰∫ÜÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑMRIË∂ÖÂàÜËæ®ÁéáÊäÄÊúØÔºåÊó®Âú®Ëß£ÂÜ≥‰ªé‰ΩéÂàÜËæ®ÁéáÊâ´ÊèèÁîüÊàêÈ´òÂàÜËæ®ÁéáÂõæÂÉèÁöÑÊåëÊàò„ÄÇË∂ÖÂàÜËæ®ÁéáÊäÄÊúØÊèê‰æõ‰∫Ü‰∏ÄÁßçËÆ°ÁÆóÊñπÊ≥ïÔºåÂèØ‰ª•Âú®‰∏çÂ¢ûÂä†Á°¨‰ª∂ÊàêÊú¨ÁöÑÊÉÖÂÜµ‰∏ãÊèêÈ´òËØäÊñ≠ÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇËÆ∫ÊñáÂõûÈ°æ‰∫ÜMRIË∂ÖÂàÜËæ®ÁéáÊäÄÊúØÁöÑÊúÄÊñ∞ËøõÂ±ïÔºåÈáçÁÇπÂÖ≥Ê≥®Ê∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ïÔºåÂπ∂‰ªéËÆ°ÁÆóÊú∫ËßÜËßâ„ÄÅËÆ°ÁÆóÊàêÂÉèÂíåÈÄÜÈóÆÈ¢òÁ≠âËßíÂ∫¶ËøõË°åÂàÜÊûê„ÄÇÊúÄÂêéÔºå‰ΩúËÄÖÊèêÂá∫‰∫ÜÁ≥ªÁªüÁöÑÂàÜÁ±ªÊ≥ïÔºåÂπ∂Êèê‰æõ‰∫ÜÂºÄÊîæËé∑ÂèñÁöÑËµÑÊ∫êÂíåÂ∑•ÂÖ∑Ôºå‰ª•ÊîØÊåÅËØ•È¢ÜÂüüÁöÑÁ†îÁ©∂„ÄÇ'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (1)', '#agi', '#alignment (1)', '#architecture (5)', '#audio', '#benchmark (9)', '#cv (6)', '#data', '#dataset (7)', '#diffusion (8)', '#ethics (1)', '#games', '#graphs', '#hallucinations', '#healthcare (2)', '#inference (6)', '#interpretability (2)', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (9)', '#open_source (7)', '#optimization (8)', '#plp', '#rag', '#reasoning (4)', '#rl (2)', '#rlhf', '#robotics', '#science (3)', '#security', '#small_models (2)', '#story_generation', '#survey (2)', '#synthetic (2)', '#training (8)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2025-12-01 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-12-01 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-12-01 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    