{
    "date": {
        "ru": "23 октября",
        "en": "October 23",
        "zh": "10月23日"
    },
    "date_en": "23 October",
    "time_utc": "2024-10-23 20:13",
    "weekday": 2,
    "issue_id": 238,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.17247",
            "title": "PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction",
            "url": "https://huggingface.co/papers/2410.17247",
            "abstract": "In large vision-language models (LVLMs), images serve as inputs that carry a wealth of information. As the idiom \"A picture is worth a thousand words\" implies, representing a single image in current LVLMs can require hundreds or even thousands of tokens. This results in significant computational costs, which grow quadratically as input image resolution increases, thereby severely impacting the efficiency of both training and inference. Previous approaches have attempted to reduce the number of image tokens either before or within the early layers of LVLMs. However, these strategies inevitably result in the loss of crucial image information, ultimately diminishing model performance. To address this challenge, we conduct an empirical study revealing that all visual tokens are necessary for LVLMs in the shallow layers, and token redundancy progressively increases in the deeper layers of the model. To this end, we propose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost their efficiency in both training and inference with neglectable performance loss. Specifically, we partition the LVLM into several stages and drop part of the image tokens at the end of each stage with a pre-defined ratio, creating pyramid-like visual tokens across model layers. The dropping is based on a lightweight similarity calculation with a negligible time overhead. Extensive experiments demonstrate that PyramidDrop can achieve a 40% training time and 55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance. Besides, the PyramidDrop could also serve as a plug-and-play strategy for inference acceleration without training, with better performance and lower inference cost than counterparts. We hope that the insights and approach introduced by PyramidDrop will inspire future research to further investigate the role of image tokens in LVLMs.",
            "score": 43,
            "issue_id": 222,
            "pub_date": "2024-10-22",
            "pub_date_ru": "22 октября",
            "hash": "b399abce7351d374",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#inference"
                ],
                "emoji": "🔺",
                "ru": {
                    "title": "PyramidDrop: Эффективное сокращение визуальной избыточности в LVLM",
                    "desc": "Статья представляет новый метод PyramidDrop для повышения эффективности крупных визуально-языковых моделей (LVLM). Авторы обнаружили, что токены изображений становятся избыточными в глубоких слоях модели. PyramidDrop постепенно уменьшает количество токенов изображения на разных этапах обработки, создавая пирамидальную структуру. Эксперименты показывают, что метод позволяет ускорить обучение на 40% и снизить вычислительные затраты при инференсе на 55% без значительной потери производительности."
                },
                "en": {
                    "title": "\"PyramidDrop: Streamlining Image Processing in Vision-Language Models\"",
                    "desc": "The paper discusses the challenge of handling large amounts of image data in large vision-language models (LVLMs), which can be computationally expensive. It introduces PyramidDrop, a method that reduces the number of image tokens progressively through the model layers, maintaining essential information while improving efficiency. This approach allows for significant reductions in training time and inference computations without sacrificing model performance. PyramidDrop can also be used as a standalone strategy to speed up inference, offering better performance and lower costs compared to existing methods."
                },
                "zh": {
                    "title": "PyramidDrop：提升视觉语言模型效率的新策略",
                    "desc": "在大型视觉语言模型中，图像作为输入携带大量信息，但这会导致计算成本高昂。PyramidDrop是一种减少视觉冗余的策略，通过在模型的不同阶段丢弃部分图像标记来提高效率。实验表明，PyramidDrop可以在保持性能的同时显著加速训练和推理过程。此方法不仅能在训练中使用，还能作为推理加速的即插即用策略。"
                }
            },
            "pub_date_card": {
                "ru": "22 октября",
                "en": "October 22",
                "zh": "10月22日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.17249",
            "title": "SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes",
            "url": "https://huggingface.co/papers/2410.17249",
            "abstract": "We present SpectroMotion, a novel approach that combines 3D Gaussian Splatting (3DGS) with physically-based rendering (PBR) and deformation fields to reconstruct dynamic specular scenes. Previous methods extending 3DGS to model dynamic scenes have struggled to accurately represent specular surfaces. Our method addresses this limitation by introducing a residual correction technique for accurate surface normal computation during deformation, complemented by a deformable environment map that adapts to time-varying lighting conditions. We implement a coarse-to-fine training strategy that significantly enhances both scene geometry and specular color prediction. We demonstrate that our model outperforms prior methods for view synthesis of scenes containing dynamic specular objects and that it is the only existing 3DGS method capable of synthesizing photorealistic real-world dynamic specular scenes, outperforming state-of-the-art methods in rendering complex, dynamic, and specular scenes.",
            "score": 39,
            "issue_id": 222,
            "pub_date": "2024-10-22",
            "pub_date_ru": "22 октября",
            "hash": "17c17b383cd97344",
            "data": {
                "categories": [
                    "#3d",
                    "#cv"
                ],
                "emoji": "✨",
                "ru": {
                    "title": "Реалистичный рендеринг динамических зеркальных объектов",
                    "desc": "SpectroMotion - это новый подход, объединяющий 3D гауссово сплаттинг (3DGS) с физически корректным рендерингом (PBR) и полями деформации для реконструкции динамических зеркальных сцен. Метод вводит технику остаточной коррекции для точного вычисления нормалей поверхности при деформации, а также деформируемую карту окружения для адаптации к изменяющимся условиям освещения. Используется стратегия обучения от грубого к точному, значительно улучшающая геометрию сцены и предсказание зеркального цвета. SpectroMotion превосходит существующие методы в синтезе фотореалистичных динамических зеркальных сцен реального мира."
                },
                "en": {
                    "title": "SpectroMotion: Shining a New Light on Dynamic 3D Scenes",
                    "desc": "SpectroMotion is a new method that improves how we create 3D images of shiny, moving objects by combining advanced techniques like 3D Gaussian Splatting and physically-based rendering. It solves previous problems with accurately showing shiny surfaces by using a special correction method to get the surface details right, even when the object moves. The method also uses a flexible map that changes with the lighting, making the scenes look more realistic. By training the model in stages from simple to complex, it becomes better at predicting both the shape and the shiny colors of the objects, outperforming other methods in creating lifelike images of moving, shiny scenes."
                },
                "zh": {
                    "title": "SpectroMotion：动态镜面场景的真实感重建",
                    "desc": "SpectroMotion 是一种新方法，结合了 3D 高斯散射和基于物理的渲染，用于重建动态镜面场景。以前的方法在处理动态场景时难以准确表示镜面表面。我们的方法通过引入残差校正技术来解决这一问题，确保在变形过程中准确计算表面法线，并使用可变形环境图适应时间变化的光照条件。我们采用由粗到细的训练策略，大大提高了场景几何和镜面颜色预测的准确性。"
                }
            },
            "pub_date_card": {
                "ru": "22 октября",
                "en": "October 22",
                "zh": "10月22日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.17131",
            "title": "Aligning Large Language Models via Self-Steering Optimization",
            "url": "https://huggingface.co/papers/2410.17131",
            "abstract": "Automated alignment develops alignment systems with minimal human intervention. The key to automated alignment lies in providing learnable and accurate preference signals for preference learning without human annotation. In this paper, we introduce Self-Steering Optimization (SSO), an algorithm that autonomously generates high-quality preference signals based on predefined principles during iterative training, eliminating the need for manual annotation. SSO maintains the accuracy of signals by ensuring a consistent gap between chosen and rejected responses while keeping them both on-policy to suit the current policy model's learning capacity. SSO can benefit the online and offline training of the policy model, as well as enhance the training of reward models. We validate the effectiveness of SSO with two foundation models, Qwen2 and Llama3.1, indicating that it provides accurate, on-policy preference signals throughout iterative training. Without any manual annotation or external models, SSO leads to significant performance improvements across six subjective or objective benchmarks. Besides, the preference data generated by SSO significantly enhanced the performance of the reward model on Rewardbench. Our work presents a scalable approach to preference optimization, paving the way for more efficient and effective automated alignment.",
            "score": 19,
            "issue_id": 223,
            "pub_date": "2024-10-22",
            "pub_date_ru": "22 октября",
            "hash": "3823bd06ecf4a69a",
            "data": {
                "categories": [
                    "#alignment",
                    "#benchmark",
                    "#rlhf"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Автоматическое выравнивание ИИ без участия человека",
                    "desc": "Статья представляет алгоритм Self-Steering Optimization (SSO) для автоматизированного выравнивания языковых моделей. SSO генерирует качественные сигналы предпочтений без ручной разметки, поддерживая постоянный разрыв между выбранными и отвергнутыми ответами. Алгоритм улучшает онлайн и офлайн обучение политики модели, а также обучение моделей вознаграждения. Эксперименты с моделями Qwen2 и Llama3.1 показали значительное повышение производительности на шести бенчмарках."
                },
                "en": {
                    "title": "Automated Alignment: Let Machines Learn Their Own Preferences",
                    "desc": "The paper introduces Self-Steering Optimization (SSO), an algorithm that autonomously generates preference signals for machine learning models without human annotation. SSO ensures these signals are accurate by maintaining a consistent gap between chosen and rejected responses, aligning with the model's learning capacity. This approach benefits both online and offline training, improving the performance of policy and reward models. The effectiveness of SSO is validated with models like Qwen2 and Llama3.1, showing significant improvements across various benchmarks."
                },
                "zh": {
                    "title": "自引导优化：实现自动化对齐的新路径",
                    "desc": "这篇论文介绍了一种名为自引导优化（SSO）的算法，它可以在训练过程中自动生成高质量的偏好信号，而无需人工标注。SSO通过确保选择和拒绝的响应之间保持一致的差距来维持信号的准确性，并使它们都符合当前策略模型的学习能力。实验表明，SSO在不需要人工标注或外部模型的情况下，显著提高了多个基准测试的性能。SSO生成的偏好数据也显著提升了奖励模型在Rewardbench上的表现。"
                }
            },
            "pub_date_card": {
                "ru": "22 октября",
                "en": "October 22",
                "zh": "10月22日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.16198",
            "title": "Improve Vision Language Model Chain-of-thought Reasoning",
            "url": "https://huggingface.co/papers/2410.16198",
            "abstract": "Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with minimal rationales. In this work, we show that training VLM on short answers does not generalize well to reasoning tasks that require more detailed responses. To address this, we propose a two-fold approach. First, we distill rationales from GPT-4o model to enrich the training data and fine-tune VLMs, boosting their CoT performance. Second, we apply reinforcement learning to further calibrate reasoning quality. Specifically, we construct positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, by comparing their predictions with annotated short answers. Using this pairwise data, we apply the Direct Preference Optimization algorithm to refine the model's reasoning abilities. Our experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction as well. This work emphasizes the importance of incorporating detailed rationales in training and leveraging reinforcement learning to strengthen the reasoning capabilities of VLMs.",
            "score": 16,
            "issue_id": 234,
            "pub_date": "2024-10-21",
            "pub_date_ru": "21 октября",
            "hash": "f0a749332feba0df",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#rlhf"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Усиление рассуждений в визуально-языковых моделях: от коротких ответов к глубокому пониманию",
                    "desc": "Статья представляет новый подход к улучшению рассуждений в визуально-языковых моделях (VLM). Авторы предлагают обогащать обучающие данные подробными объяснениями, полученными от GPT-4, и применять обучение с подкреплением для калибровки качества рассуждений. Эксперименты показывают значительное улучшение способностей VLM к рассуждениям на эталонных наборах данных. Работа подчеркивает важность включения детальных обоснований в процесс обучения моделей машинного обучения."
                },
                "en": {
                    "title": "\"Boosting VLMs: From Short Answers to Deep Reasoning\"",
                    "desc": "The paper discusses how current vision language models (VLMs) struggle with reasoning tasks due to a lack of detailed rationale in training data. To improve this, the authors propose enriching training data with rationales distilled from a more advanced model, GPT-4o, and fine-tuning VLMs. They also use reinforcement learning to enhance reasoning by comparing correct and incorrect reasoning chains and applying the Direct Preference Optimization algorithm. The study shows that these methods significantly improve the models' reasoning abilities and their ability to generalize to new tasks."
                },
                "zh": {
                    "title": "细化推理，提升视觉语言模型的可信度",
                    "desc": "这篇论文探讨了视觉语言模型中链式推理的重要性，强调其对模型解释性和可信度的提升。研究发现，现有的训练数据多为简短注释，缺乏详细的推理过程，导致模型在复杂推理任务中的表现不佳。为解决这一问题，作者提出了从GPT-4o模型中提取推理过程以丰富训练数据，并通过强化学习进一步优化推理质量。实验结果表明，这种方法显著提高了模型在基准数据集上的推理能力，并改善了直接答案预测的泛化能力。"
                }
            },
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.16267",
            "title": "xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs",
            "url": "https://huggingface.co/papers/2410.16267",
            "abstract": "We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for videos, particularly designed to efficiently capture temporal information over multiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in addition to the conventional visual tokenizer, which maps a sequence of tokens over multiple frames into a compact set of visual tokens. This enables BLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32 vs. 4608 tokens). We explore different types of temporal encoders, including learnable spatio-temporal pooling as well as sequential models like Token Turing Machines. We experimentally confirm that BLIP-3-Video obtains video question-answering accuracies comparable to much larger state-of-the-art models (e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using fewer visual tokens. The project website is at https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html",
            "score": 14,
            "issue_id": 233,
            "pub_date": "2024-10-21",
            "pub_date_ru": "21 октября",
            "hash": "7e29d42e819fbb96",
            "data": {
                "categories": [
                    "#architecture",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Эффективный анализ видео с меньшим количеством токенов",
                    "desc": "В статье представлена мультимодальная языковая модель xGen-MM-Vid (BLIP-3-Video) для обработки видео. Модель использует 'временной кодировщик' в дополнение к обычному визуальному токенизатору, что позволяет эффективно захватывать временную информацию из нескольких кадров. BLIP-3-Video требует значительно меньше визуальных токенов по сравнению с конкурирующими моделями. Экспериментально подтверждено, что BLIP-3-Video достигает точности ответов на вопросы по видео, сопоставимой с гораздо более крупными современными моделями, при этом являясь меньше по размеру и эффективнее."
                },
                "en": {
                    "title": "Efficient Video Understanding with Fewer Tokens",
                    "desc": "The paper introduces xGen-MM-Vid (BLIP-3-Video), a multimodal language model designed to efficiently process video data by capturing temporal information across multiple frames. It uses a 'temporal encoder' alongside a visual tokenizer to convert sequences of frames into a compact set of visual tokens, significantly reducing the number of tokens needed compared to other models. The model explores various temporal encoders, including spatio-temporal pooling and Token Turing Machines, to enhance its efficiency. Experimental results show that BLIP-3-Video achieves competitive video question-answering performance with fewer resources, being smaller and more efficient than larger models."
                },
                "zh": {
                    "title": "高效捕捉视频时间信息的多模态语言模型",
                    "desc": "这篇论文介绍了一种名为xGen-MM-Vid（BLIP-3-Video）的多模态语言模型，专为视频设计，能够高效捕捉多帧的时间信息。BLIP-3-Video利用了“时间编码器”，结合传统的视觉标记器，将多帧序列映射为紧凑的视觉标记集。与其他模型相比，它使用的视觉标记数量大大减少（例如，32个对比4608个）。实验结果表明，BLIP-3-Video在视频问答准确性上与更大规模的模型相当，但其规模更小且效率更高。"
                }
            },
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.15926",
            "title": "Mitigating Object Hallucination via Concentric Causal Attention",
            "url": "https://huggingface.co/papers/2410.15926",
            "abstract": "Recent Large Vision Language Models (LVLMs) present remarkable zero-shot conversational and reasoning capabilities given multimodal queries. Nevertheless, they suffer from object hallucination, a phenomenon where LVLMs are prone to generate textual responses not factually aligned with image inputs. Our pilot study reveals that object hallucination is closely tied with Rotary Position Encoding (RoPE), a widely adopted positional dependency modeling design in existing LVLMs. Due to the long-term decay in RoPE, LVLMs tend to hallucinate more when relevant visual cues are distant from instruction tokens in the multimodal input sequence. Additionally, we observe a similar effect when reversing the sequential order of visual tokens during multimodal alignment. Our tests indicate that long-term decay in RoPE poses challenges to LVLMs while capturing visual-instruction interactions across long distances. We propose Concentric Causal Attention (CCA), a simple yet effective positional alignment strategy that mitigates the impact of RoPE long-term decay in LVLMs by naturally reducing relative distance between visual and instruction tokens. With CCA, visual tokens can better interact with instruction tokens, thereby enhancing model's perception capability and alleviating object hallucination. Without bells and whistles, our positional alignment method surpasses existing hallucination mitigation strategies by large margins on multiple object hallucination benchmarks.",
            "score": 14,
            "issue_id": 226,
            "pub_date": "2024-10-21",
            "pub_date_ru": "21 октября",
            "hash": "9dfb7677b0acecf1",
            "data": {
                "categories": [
                    "#alignment",
                    "#cv",
                    "#hallucinations",
                    "#long_context",
                    "#rlhf"
                ],
                "emoji": "👁️",
                "ru": {
                    "title": "Борьба с галлюцинациями в мультимодальных ИИ-моделях",
                    "desc": "Исследователи обнаружили, что большие мультимодальные языковые модели (LVLMs) склонны к галлюцинациям объектов из-за особенностей позиционного кодирования RoPE. Эта проблема усугубляется, когда визуальные подсказки находятся далеко от текстовых инструкций во входной последовательности. Авторы предложили метод Concentric Causal Attention (CCA) для уменьшения относительного расстояния между визуальными и текстовыми токенами. CCA значительно превзошел существующие методы по снижению галлюцинаций на нескольких бенчмарках."
                },
                "en": {
                    "title": "Enhancing Visual Perception in LVLMs: Tackling Object Hallucination with CCA",
                    "desc": "The paper discusses how Large Vision Language Models (LVLMs) can sometimes generate incorrect textual responses that don't match the images they are analyzing, a problem known as object hallucination. This issue is linked to Rotary Position Encoding (RoPE), which struggles with long-term decay, causing LVLMs to hallucinate when visual cues are far from instruction tokens. The authors propose a new method called Concentric Causal Attention (CCA) to address this, which helps align visual and instruction tokens more effectively. By using CCA, the model's ability to perceive and interpret images improves, significantly reducing object hallucination compared to other methods."
                },
                "zh": {
                    "title": "同心因果注意力：消除对象幻觉的有效策略",
                    "desc": "这篇论文研究了大型视觉语言模型（LVLMs）在处理多模态查询时出现的对象幻觉问题。研究发现，这种现象与广泛使用的旋转位置编码（RoPE）有关，因为RoPE的长期衰减导致LVLMs在视觉线索与指令标记距离较远时更容易出现幻觉。为了解决这个问题，作者提出了一种新的位置对齐策略，称为同心因果注意力（CCA），可以有效减少视觉标记与指令标记之间的相对距离。通过这种方法，模型的感知能力得到增强，对象幻觉现象得到显著缓解。"
                }
            },
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.17250",
            "title": "JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation",
            "url": "https://huggingface.co/papers/2410.17250",
            "abstract": "Accelerating research on Large Multimodal Models (LMMs) in non-English languages is crucial for enhancing user experiences across broader populations. In this paper, we introduce JMMMU (Japanese MMMU), the first large-scale Japanese benchmark designed to evaluate LMMs on expert-level tasks based on the Japanese cultural context. To facilitate comprehensive culture-aware evaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA) subset, where the culture-independent subjects (e.g., Math) are selected and translated into Japanese, enabling one-to-one comparison with its English counterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly crafted subjects that reflect Japanese cultural context. Using the CA subset, we observe performance drop in many LMMs when evaluated in Japanese, which is purely attributable to language variation. Using the CS subset, we reveal their inadequate Japanese cultural understanding. Further, by combining both subsets, we identify that some LMMs perform well on the CA subset but not on the CS subset, exposing a shallow understanding of the Japanese language that lacks depth in cultural understanding. We hope this work will not only help advance LMM performance in Japanese but also serve as a guideline to create high-standard, culturally diverse benchmarks for multilingual LMM development. The project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.",
            "score": 12,
            "issue_id": 222,
            "pub_date": "2024-10-22",
            "pub_date_ru": "22 октября",
            "hash": "245b35927b3cf09b",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multilingual"
                ],
                "emoji": "🗾",
                "ru": {
                    "title": "Культурно-ориентированная оценка мультимодальных моделей на японском языке",
                    "desc": "Статья представляет JMMMU - первый масштабный японский бенчмарк для оценки больших мультимодальных моделей (LMM) на экспертных задачах в японском культурном контексте. Бенчмарк состоит из двух подмножеств: культурно-агностического (CA) и культурно-специфического (CS). Исследование выявило снижение производительности многих LMM при оценке на японском языке, а также их недостаточное понимание японской культуры. Авторы надеются, что эта работа поможет улучшить производительность LMM на японском языке и послужит руководством для создания качественных, культурно разнообразных бенчмарков для многоязычного развития LMM."
                },
                "en": {
                    "title": "Bridging Language and Culture in AI: The JMMMU Benchmark",
                    "desc": "The paper introduces JMMMU, a benchmark for evaluating Large Multimodal Models (LMMs) in Japanese, focusing on both culture-agnostic and culture-specific tasks. It highlights that many LMMs show reduced performance in Japanese due to language differences, and struggle with tasks requiring deep cultural understanding. The study reveals that some models perform well on general tasks but fail on culturally nuanced ones, indicating a lack of cultural depth. This work aims to improve LMMs in Japanese and guide the creation of culturally diverse benchmarks for multilingual models."
                },
                "zh": {
                    "title": "推动多语言模型的文化深度理解",
                    "desc": "这篇论文介绍了JMMMU，这是第一个用于评估大型多模态模型在日本文化背景下表现的基准。JMMMU包括两个子集：文化无关子集和文化特定子集。研究发现，许多模型在日语环境下表现下降，尤其是在文化特定子集上表现不佳，显示出对日本文化理解的不足。通过这项研究，作者希望推动多语言模型在日语中的表现，并为创建高标准的多文化基准提供指导。"
                }
            },
            "pub_date_card": {
                "ru": "22 октября",
                "en": "October 22",
                "zh": "10月22日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.17215",
            "title": "MiniPLM: Knowledge Distillation for Pre-Training Language Models",
            "url": "https://huggingface.co/papers/2410.17215",
            "abstract": "Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. While effective in fine-tuning, KD during pre-training faces challenges in efficiency, flexibility, and effectiveness. Existing methods either incur high computational costs due to online teacher inference, require tokenization matching between teacher and student LMs, or risk losing the difficulty and diversity of the teacher-generated training data. To address these issues, we propose MiniPLM, a KD framework for pre-training LMs by refining the training data distribution with the teacher's knowledge. For efficiency, MiniPLM performs offline teacher LM inference, allowing KD for multiple student LMs without adding training-time costs. For flexibility, MiniPLM operates solely on the training corpus, enabling KD across model families. For effectiveness, MiniPLM leverages the differences between large and small LMs to enhance the difficulty and diversity of the training data, helping student LMs acquire versatile and sophisticated knowledge. Extensive experiments demonstrate that MiniPLM boosts the student LMs' performance on 9 widely used downstream tasks, improves the language modeling capabilities, and reduces pre-training computation. The benefit of MiniPLM extends to large pre-training scales, evidenced by the extrapolation of the scaling curves. Further analysis reveals that MiniPLM supports KD across model families and enhances the utilization of pre-training data. Our model, code, and data are available at https://github.com/thu-coai/MiniPLM.",
            "score": 12,
            "issue_id": 221,
            "pub_date": "2024-10-22",
            "pub_date_ru": "22 октября",
            "hash": "50ff5ac54c5a78a2",
            "data": {
                "categories": [
                    "#dataset"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "MiniPLM: Эффективная дистилляция знаний для предобучения языковых моделей",
                    "desc": "Статья представляет MiniPLM - новый фреймворк для дистилляции знаний при предварительном обучении языковых моделей. MiniPLM решает проблемы эффективности, гибкости и эффективности существующих методов, выполняя офлайн-вывод учительской модели и работая только с обучающим корпусом. Этот подход улучшает распределение обучающих данных, повышая их сложность и разнообразие. Эксперименты показывают, что MiniPLM улучшает производительность ученических моделей на различных задачах и поддерживает дистилляцию знаний между разными семействами моделей."
                },
                "en": {
                    "title": "MiniPLM: Efficient and Flexible Knowledge Distillation for Smarter Language Models",
                    "desc": "The paper introduces MiniPLM, a knowledge distillation framework designed to improve the pre-training of small language models by using the knowledge from larger models. MiniPLM enhances efficiency by performing offline teacher model inference, which reduces computational costs and allows for training multiple student models simultaneously. It also increases flexibility by working directly with the training corpus, enabling knowledge distillation across different model families. The framework improves the effectiveness of training by refining the data distribution, which helps student models learn more complex and diverse information, leading to better performance on various tasks."
                },
                "zh": {
                    "title": "MiniPLM：高效灵活的知识蒸馏框架",
                    "desc": "知识蒸馏（KD）常用于通过大型教师语言模型（LMs）训练小型高性能学生语言模型。MiniPLM是一种新的KD框架，通过离线教师推理提高效率，减少训练时间成本。它仅依赖训练语料库，增强了模型间的灵活性，并通过丰富训练数据的难度和多样性提高学生模型的效果。实验表明，MiniPLM在多项任务中提升了学生模型的表现，并减少了预训练计算量。"
                }
            },
            "pub_date_card": {
                "ru": "22 октября",
                "en": "October 22",
                "zh": "10月22日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.14649",
            "title": "EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search",
            "url": "https://huggingface.co/papers/2410.14649",
            "abstract": "The high computational costs of large language models (LLMs) have led to a flurry of research on LLM compression, via methods such as quantization, sparsification, or structured pruning. A new frontier in this area is given by dynamic, non-uniform compression methods, which adjust the compression levels (e.g., sparsity) per-block or even per-layer in order to minimize accuracy loss, while guaranteeing a global compression threshold. Yet, current methods rely on heuristics for identifying the \"importance\" of a given layer towards the loss, based on assumptions such as error monotonicity, i.e. that the end-to-end model compression error is proportional to the sum of layer-wise errors. In this paper, we revisit this area, and propose a new and general approach for dynamic compression that is provably optimal in a given input range. We begin from the motivating observation that, in general, error monotonicity does not hold for LLMs: compressed models with lower sum of per-layer errors can perform worse than models with higher error sums. To address this, we propose a new general evolutionary framework for dynamic LLM compression called EvoPress, which has provable convergence, and low sample and evaluation complexity. We show that these theoretical guarantees lead to highly competitive practical performance for dynamic compression of Llama, Mistral and Phi models. Via EvoPress, we set new state-of-the-art results across all compression approaches: structural pruning (block/layer dropping), unstructured sparsity, as well as quantization with dynamic bitwidths. Our code is available at https://github.com/IST-DASLab/EvoPress.",
            "score": 7,
            "issue_id": 222,
            "pub_date": "2024-10-18",
            "pub_date_ru": "18 октября",
            "hash": "8c7a7ff7f03a5de4",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization"
                ],
                "emoji": "🗜️",
                "ru": {
                    "title": "EvoPress: оптимальное динамическое сжатие языковых моделей",
                    "desc": "Статья представляет новый подход к динамической компрессии больших языковых моделей (LLM) под названием EvoPress. Авторы опровергают предположение о монотонности ошибок в LLM и предлагают эволюционный фреймворк с доказуемой сходимостью и низкой вычислительной сложностью. EvoPress показывает высокую эффективность при сжатии моделей Llama, Mistral и Phi, устанавливая новые стандарты в структурном прунинге, неструктурированном прореживании и квантизации с динамической разрядностью. Код реализации доступен в открытом репозитории GitHub."
                },
                "en": {
                    "title": "EvoPress: Revolutionizing LLM Compression with Dynamic Precision",
                    "desc": "The paper addresses the challenge of reducing the computational costs of large language models by introducing a new method called EvoPress for dynamic compression. Unlike traditional methods that rely on assumptions like error monotonicity, EvoPress uses an evolutionary framework to adjust compression levels per block or layer, ensuring minimal accuracy loss. This approach is proven to be optimal within a specific input range and demonstrates superior performance across various compression techniques, including pruning and quantization. The authors provide theoretical guarantees for EvoPress, which translate into practical improvements for models like Llama, Mistral, and Phi."
                },
                "zh": {
                    "title": "EvoPress：大语言模型压缩的新突破",
                    "desc": "这篇论文探讨了大语言模型的压缩问题，提出了一种新的动态压缩方法。传统方法依赖于假设层的重要性来进行压缩，而这种新方法则不依赖于这些假设。作者提出了一种名为EvoPress的进化框架，能够在给定输入范围内实现最优压缩。实验结果表明，EvoPress在多种压缩方法中都达到了最新的性能水平。"
                }
            },
            "pub_date_card": {
                "ru": "18 октября",
                "en": "October 18",
                "zh": "10月18日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.16930",
            "title": "Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes",
            "url": "https://huggingface.co/papers/2410.16930",
            "abstract": "Math reasoning is a highly active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be isolated within a model. Doing so could allow targeted intervention to improve math performance without altering non-math behavior and foster understanding of how models encode math reasoning. We introduce Math Neurosurgery (MathNeuro), a method for isolating math-specific parameters in LLMs using only forward passes. MathNeuro builds on existing work by using weights and activations to calculate parameter importance, but isolates math-specific parameters by removing those important for general language tasks. Pruning parameters MathNeuro identifies deletes a LLM's math reasoning ability without destroying its general language ability. Scaling these parameters by a small constant improves a pretrained or instruction-tuned LLM's performance by 4-17% on GSM8K while leaving non-math behavior unaltered. MathNeuro is also data efficient: most of its effectiveness holds when identifying math-specific parameters using a single sample. MathNeuro highlights the potential for future work to intervene on math-specific parameters.",
            "score": 5,
            "issue_id": 226,
            "pub_date": "2024-10-22",
            "pub_date_ru": "22 октября",
            "hash": "13a2560869429449",
            "data": {
                "categories": [
                    "#interpretability",
                    "#math"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Нейрохирургия для ИИ: точечное улучшение математических способностей языковых моделей",
                    "desc": "Статья представляет метод MathNeuro для выделения параметров, отвечающих за математические рассуждения в больших языковых моделях (LLM). Этот метод позволяет целенаправленно улучшать математические способности модели, не затрагивая другие языковые навыки. Исследователи показали, что удаление выявленных параметров уничтожает математические способности LLM, в то время как их масштабирование улучшает производительность на 4-17% в тесте GSM8K. MathNeuro эффективен даже при использовании единственного образца данных, что открывает перспективы для дальнейших исследований в области целенаправленного вмешательства в параметры LLM."
                },
                "en": {
                    "title": "\"Unlocking Math Genius in AI: Isolate, Enhance, Excel!\"",
                    "desc": "The paper introduces Math Neurosurgery (MathNeuro), a method to isolate math-specific parameters in Large Language Models (LLMs) using only forward passes. MathNeuro identifies and prunes parameters crucial for math reasoning without affecting general language abilities. By scaling these parameters, the method enhances math performance by 4-17% on GSM8K, while keeping non-math behavior unchanged. This approach is data-efficient, requiring only a single sample to identify math-specific parameters, paving the way for targeted improvements in math reasoning."
                },
                "zh": {
                    "title": "MathNeuro：精准提升数学推理能力的利器",
                    "desc": "这篇论文介绍了一种名为MathNeuro的方法，用于在大型语言模型中隔离数学推理参数。MathNeuro通过前向传播计算参数的重要性，并去除对一般语言任务重要的参数，从而专注于数学推理。通过调整这些参数，模型的数学性能可以提高4-17%，而不影响其语言能力。MathNeuro展示了在不改变非数学行为的情况下，如何有效地提升数学推理能力。"
                }
            },
            "pub_date_card": {
                "ru": "22 октября",
                "en": "October 22",
                "zh": "10月22日"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.16266",
            "title": "3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors",
            "url": "https://huggingface.co/papers/2410.16266",
            "abstract": "Novel-view synthesis aims to generate novel views of a scene from multiple input images or videos, and recent advancements like 3D Gaussian splatting (3DGS) have achieved notable success in producing photorealistic renderings with efficient pipelines. However, generating high-quality novel views under challenging settings, such as sparse input views, remains difficult due to insufficient information in under-sampled areas, often resulting in noticeable artifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing the representation quality of 3DGS representations. We leverage 2D video diffusion priors to address the challenging 3D view consistency problem, reformulating it as achieving temporal consistency within a video generation process. 3DGS-Enhancer restores view-consistent latent features of rendered novel views and integrates them with the input views through a spatial-temporal decoder. The enhanced views are then used to fine-tune the initial 3DGS model, significantly improving its rendering performance. Extensive experiments on large-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields superior reconstruction performance and high-fidelity rendering results compared to state-of-the-art methods. The project webpage is https://xiliu8006.github.io/3DGS-Enhancer-project .",
            "score": 2,
            "issue_id": 237,
            "pub_date": "2024-10-21",
            "pub_date_ru": "21 октября",
            "hash": "f00040bffceab087",
            "data": {
                "categories": [
                    "#3d",
                    "#benchmark",
                    "#dataset",
                    "#video"
                ],
                "emoji": "🎥",
                "ru": {
                    "title": "Улучшение синтеза новых ракурсов с помощью видео-диффузии",
                    "desc": "Статья представляет 3DGS-Enhancer - новый метод улучшения качества представления сцен в задаче синтеза новых ракурсов. Авторы используют 2D видео-диффузионные приоры для решения проблемы согласованности ракурсов в 3D, переформулируя ее как достижение временной согласованности в процессе генерации видео. Метод восстанавливает согласованные латентные признаки отрендеренных новых ракурсов и интегрирует их с исходными видами через пространственно-временной декодер. Улучшенные виды затем используются для дообучения исходной 3DGS модели, значительно повышая качество рендеринга."
                },
                "en": {
                    "title": "Enhancing 3D Views with Temporal Consistency",
                    "desc": "The paper introduces 3DGS-Enhancer, a new method to improve the quality of 3D Gaussian splatting (3DGS) for novel-view synthesis, especially in challenging scenarios with sparse input views. By using 2D video diffusion priors, the approach addresses the issue of maintaining 3D view consistency by ensuring temporal consistency in video generation. The method enhances the latent features of rendered views and combines them with input views using a spatial-temporal decoder, which is then used to refine the original 3DGS model. Experiments show that 3DGS-Enhancer significantly outperforms existing methods in rendering high-quality, photorealistic scenes."
                },
                "zh": {
                    "title": "3D视图一致性的新突破：3DGS-Enhancer",
                    "desc": "这篇论文介绍了一种名为3DGS-Enhancer的新方法，用于提升3D高斯点云表示的质量。通过利用2D视频扩散先验，该方法解决了3D视图一致性的问题，将其重新定义为视频生成过程中的时间一致性。3DGS-Enhancer通过空间-时间解码器将渲染的新视图与输入视图结合，恢复视图一致的潜在特征。实验表明，该方法在大规模数据集上表现出色，显著提高了渲染性能。"
                }
            },
            "pub_date_card": {
                "ru": "21 октября",
                "en": "October 21",
                "zh": "10月21日"
            }
        }
    ],
    "link_prev": "2024-10-22.html",
    "link_next": "2024-10-24.html",
    "date_prev": "22 октября",
    "date_next": "24 октября",
    "short_date_prev": {
        "ru": "22.10",
        "en": "10/22",
        "zh": "10月22日"
    },
    "short_date_next": {
        "ru": "24.10",
        "en": "10/24",
        "zh": "10月24日"
    },
    "zh": {
        "text": "这篇文章讨论了大规模视觉-语言模型（LVLMs）中图像表示的计算成本问题。图像包含大量信息，表示一张图像需要大量的标记，计算成本随图像分辨率增加而增加。为解决这一问题，作者提出了PyramidDrop策略，在模型的不同阶段逐步减少图像标记，从而提高训练和推理效率，且性能损失可忽略。实验结果显示，PyramidDrop可显著加快训练和推理速度，并可作为即插即用的策略进一步加速推理。",
        "title": "PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction",
        "pinyin": "Zhè piān wénzhāng tǎolùn le dàguīmó shìjué-yǔyán móxíng (LVLMs) zhōng túxiàng biǎoshì de jìsuàn chéngběn wènti. Túxiàng bāohán dàliàng xìnxī, biǎoshì yī zhāng túxiàng xūyào dàliàng de biāojì, jìsuàn chéngběn suí túxiàng fēnbiànlǜ zēngjiā ér zēngjiā. Wèi jiějué zhè yī wènti, zuòzhě tíchū le PyramidDrop cèlüè, zài móxíng de bùtóng jiēduàn zhúbù jiǎnshǎo túxiàng biāojì, cóng'ér tǐgāo xùnliàn hé tuīlǐ xiàolǜ, érqiě xìngnéng sǔnshī kě hūlüè. Shíyàn jiéguǒ xiǎnshì, PyramidDrop kě xiǎnzhù jiākuài xùnliàn hé tuīlǐ sùdù, bìng kě zuòwéi jíchājíyòng de cèlüè jìnxíng jiāsù tuīlǐ.",
        "vocab": "[\n    {\"word\": \"讨论\", \"pinyin\": \"tǎo lùn\", \"trans\": \"discuss\"},\n    {\"word\": \"大规模\", \"pinyin\": \"dà guī mó\", \"trans\": \"large-scale\"},\n    {\"word\": \"视觉-语言模型\", \"pinyin\": \"shì jué yǔ yán mó xìng\", \"trans\": \"vision-language model\"},\n    {\"word\": \"计算成本\", \"pinyin\": \"jì suàn chéng běn\", \"trans\": \"computational cost\"},\n    {\"word\": \"表示\", \"pinyin\": \"biǎo shì\", \"trans\": \"representation\"},\n    {\"word\": \"标记\", \"pinyin\": \"biāo jì\", \"trans\": \"token\"},\n    {\"word\": \"分辨率\", \"pinyin\": \"fēn biàn lǜ\", \"trans\": \"resolution\"},\n    {\"word\": \"提出\", \"pinyin\": \"tí chū\", \"trans\": \"propose\"},\n    {\"word\": \"策略\", \"pinyin\": \"cè lüè\", \"trans\": \"strategy\"},\n    {\"word\": \"逐步\", \"pinyin\": \"zhú bù\", \"trans\": \"gradually\"},\n    {\"word\": \"减少\", \"pinyin\": \"jiǎn shǎo\", \"trans\": \"reduce\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùn liàn\", \"trans\": \"training\"},\n    {\"word\": \"推理\", \"pinyin\": \"tuī lǐ\", \"trans\": \"inference\"},\n    {\"word\": \"效率\", \"pinyin\": \"xiào lǜ\", \"trans\": \"efficiency\"},\n    {\"word\": \"损失\", \"pinyin\": \"sǔn shī\", \"trans\": \"loss\"},\n    {\"word\": \"可忽略\", \"pinyin\": \"kě hū lüè\", \"trans\": \"negligible\"},\n    {\"word\": \"显著\", \"pinyin\": \"xiǎn zhù\", \"trans\": \"significant\"},\n    {\"word\": \"加快\", \"pinyin\": \"jiā kuài\", \"trans\": \"accelerate\"},\n    {\"word\": \"速度\", \"pinyin\": \"sù dù\", \"trans\": \"speed\"},\n    {\"word\": \"即插即用\", \"pinyin\": \"jí chā jí yòng\", \"trans\": \"plug-and-play\"},\n    {\"word\": \"进一步\", \"pinyin\": \"jìn yī bù\", \"trans\": \"further\"}\n]",
        "trans": "This article discusses the computational cost issue of image representation in large-scale vision-language models (LVLMs). Images contain a large amount of information, and representing an image requires a large number of tokens, with the computational cost increasing as the image resolution increases. To address this problem, the authors propose the PyramidDrop strategy, which gradually reduces image tokens at different stages of the model, thereby improving training and inference efficiency with negligible performance loss. Experimental results show that PyramidDrop can significantly accelerate training and inference speeds and can be used as a plug-and-play strategy to further speed up inference.",
        "update_ts": "2024-10-23 09:59"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 3,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 2,
        "#medicine": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#translation": 0,
        "#nlp": 0
    },
    "link_month": "2024-10.html"
}