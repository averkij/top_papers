{
    "date": {
        "ru": "23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 23",
        "zh": "10æœˆ23æ—¥"
    },
    "date_en": "23 October",
    "time_utc": "2024-10-23 20:13",
    "weekday": 2,
    "issue_id": 238,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.17247",
            "title": "PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction",
            "url": "https://huggingface.co/papers/2410.17247",
            "abstract": "In large vision-language models (LVLMs), images serve as inputs that carry a wealth of information. As the idiom \"A picture is worth a thousand words\" implies, representing a single image in current LVLMs can require hundreds or even thousands of tokens. This results in significant computational costs, which grow quadratically as input image resolution increases, thereby severely impacting the efficiency of both training and inference. Previous approaches have attempted to reduce the number of image tokens either before or within the early layers of LVLMs. However, these strategies inevitably result in the loss of crucial image information, ultimately diminishing model performance. To address this challenge, we conduct an empirical study revealing that all visual tokens are necessary for LVLMs in the shallow layers, and token redundancy progressively increases in the deeper layers of the model. To this end, we propose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost their efficiency in both training and inference with neglectable performance loss. Specifically, we partition the LVLM into several stages and drop part of the image tokens at the end of each stage with a pre-defined ratio, creating pyramid-like visual tokens across model layers. The dropping is based on a lightweight similarity calculation with a negligible time overhead. Extensive experiments demonstrate that PyramidDrop can achieve a 40% training time and 55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance. Besides, the PyramidDrop could also serve as a plug-and-play strategy for inference acceleration without training, with better performance and lower inference cost than counterparts. We hope that the insights and approach introduced by PyramidDrop will inspire future research to further investigate the role of image tokens in LVLMs.",
            "score": 43,
            "issue_id": 222,
            "pub_date": "2024-10-22",
            "pub_date_ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "hash": "b399abce7351d374",
            "data": {
                "categories": [
                    "#architecture",
                    "#cv",
                    "#inference"
                ],
                "emoji": "ğŸ”º",
                "ru": {
                    "title": "PyramidDrop: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² LVLM",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ PyramidDrop Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. PyramidDrop Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¿Ğ¸Ñ€Ğ°Ğ¼Ğ¸Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 40% Ğ¸ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ½Ğ° 55% Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "\"PyramidDrop: Streamlining Image Processing in Vision-Language Models\"",
                    "desc": "The paper discusses the challenge of handling large amounts of image data in large vision-language models (LVLMs), which can be computationally expensive. It introduces PyramidDrop, a method that reduces the number of image tokens progressively through the model layers, maintaining essential information while improving efficiency. This approach allows for significant reductions in training time and inference computations without sacrificing model performance. PyramidDrop can also be used as a standalone strategy to speed up inference, offering better performance and lower costs compared to existing methods."
                },
                "zh": {
                    "title": "PyramidDropï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹æ•ˆç‡çš„æ–°ç­–ç•¥",
                    "desc": "åœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ï¼Œå›¾åƒä½œä¸ºè¾“å…¥æºå¸¦å¤§é‡ä¿¡æ¯ï¼Œä½†è¿™ä¼šå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚PyramidDropæ˜¯ä¸€ç§å‡å°‘è§†è§‰å†—ä½™çš„ç­–ç•¥ï¼Œé€šè¿‡åœ¨æ¨¡å‹çš„ä¸åŒé˜¶æ®µä¸¢å¼ƒéƒ¨åˆ†å›¾åƒæ ‡è®°æ¥æé«˜æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒPyramidDropå¯ä»¥åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—åŠ é€Ÿè®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚æ­¤æ–¹æ³•ä¸ä»…èƒ½åœ¨è®­ç»ƒä¸­ä½¿ç”¨ï¼Œè¿˜èƒ½ä½œä¸ºæ¨ç†åŠ é€Ÿçš„å³æ’å³ç”¨ç­–ç•¥ã€‚"
                }
            },
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.17249",
            "title": "SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes",
            "url": "https://huggingface.co/papers/2410.17249",
            "abstract": "We present SpectroMotion, a novel approach that combines 3D Gaussian Splatting (3DGS) with physically-based rendering (PBR) and deformation fields to reconstruct dynamic specular scenes. Previous methods extending 3DGS to model dynamic scenes have struggled to accurately represent specular surfaces. Our method addresses this limitation by introducing a residual correction technique for accurate surface normal computation during deformation, complemented by a deformable environment map that adapts to time-varying lighting conditions. We implement a coarse-to-fine training strategy that significantly enhances both scene geometry and specular color prediction. We demonstrate that our model outperforms prior methods for view synthesis of scenes containing dynamic specular objects and that it is the only existing 3DGS method capable of synthesizing photorealistic real-world dynamic specular scenes, outperforming state-of-the-art methods in rendering complex, dynamic, and specular scenes.",
            "score": 39,
            "issue_id": 222,
            "pub_date": "2024-10-22",
            "pub_date_ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "hash": "17c17b383cd97344",
            "data": {
                "categories": [
                    "#3d",
                    "#cv"
                ],
                "emoji": "âœ¨",
                "ru": {
                    "title": "Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·ĞµÑ€ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²",
                    "desc": "SpectroMotion - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³ (3DGS) Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ¾Ğ¼ (PBR) Ğ¸ Ğ¿Ğ¾Ğ»ÑĞ¼Ğ¸ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·ĞµÑ€ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¼ÑÑ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼Ñƒ, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ·ĞµÑ€ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ†Ğ²ĞµÑ‚Ğ°. SpectroMotion Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·ĞµÑ€ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°."
                },
                "en": {
                    "title": "SpectroMotion: Shining a New Light on Dynamic 3D Scenes",
                    "desc": "SpectroMotion is a new method that improves how we create 3D images of shiny, moving objects by combining advanced techniques like 3D Gaussian Splatting and physically-based rendering. It solves previous problems with accurately showing shiny surfaces by using a special correction method to get the surface details right, even when the object moves. The method also uses a flexible map that changes with the lighting, making the scenes look more realistic. By training the model in stages from simple to complex, it becomes better at predicting both the shape and the shiny colors of the objects, outperforming other methods in creating lifelike images of moving, shiny scenes."
                },
                "zh": {
                    "title": "SpectroMotionï¼šåŠ¨æ€é•œé¢åœºæ™¯çš„çœŸå®æ„Ÿé‡å»º",
                    "desc": "SpectroMotion æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œç»“åˆäº† 3D é«˜æ–¯æ•£å°„å’ŒåŸºäºç‰©ç†çš„æ¸²æŸ“ï¼Œç”¨äºé‡å»ºåŠ¨æ€é•œé¢åœºæ™¯ã€‚ä»¥å‰çš„æ–¹æ³•åœ¨å¤„ç†åŠ¨æ€åœºæ™¯æ—¶éš¾ä»¥å‡†ç¡®è¡¨ç¤ºé•œé¢è¡¨é¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¼•å…¥æ®‹å·®æ ¡æ­£æŠ€æœ¯æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç¡®ä¿åœ¨å˜å½¢è¿‡ç¨‹ä¸­å‡†ç¡®è®¡ç®—è¡¨é¢æ³•çº¿ï¼Œå¹¶ä½¿ç”¨å¯å˜å½¢ç¯å¢ƒå›¾é€‚åº”æ—¶é—´å˜åŒ–çš„å…‰ç…§æ¡ä»¶ã€‚æˆ‘ä»¬é‡‡ç”¨ç”±ç²—åˆ°ç»†çš„è®­ç»ƒç­–ç•¥ï¼Œå¤§å¤§æé«˜äº†åœºæ™¯å‡ ä½•å’Œé•œé¢é¢œè‰²é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚"
                }
            },
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.17131",
            "title": "Aligning Large Language Models via Self-Steering Optimization",
            "url": "https://huggingface.co/papers/2410.17131",
            "abstract": "Automated alignment develops alignment systems with minimal human intervention. The key to automated alignment lies in providing learnable and accurate preference signals for preference learning without human annotation. In this paper, we introduce Self-Steering Optimization (SSO), an algorithm that autonomously generates high-quality preference signals based on predefined principles during iterative training, eliminating the need for manual annotation. SSO maintains the accuracy of signals by ensuring a consistent gap between chosen and rejected responses while keeping them both on-policy to suit the current policy model's learning capacity. SSO can benefit the online and offline training of the policy model, as well as enhance the training of reward models. We validate the effectiveness of SSO with two foundation models, Qwen2 and Llama3.1, indicating that it provides accurate, on-policy preference signals throughout iterative training. Without any manual annotation or external models, SSO leads to significant performance improvements across six subjective or objective benchmarks. Besides, the preference data generated by SSO significantly enhanced the performance of the reward model on Rewardbench. Our work presents a scalable approach to preference optimization, paving the way for more efficient and effective automated alignment.",
            "score": 19,
            "issue_id": 223,
            "pub_date": "2024-10-22",
            "pub_date_ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "hash": "3823bd06ecf4a69a",
            "data": {
                "categories": [
                    "#alignment",
                    "#benchmark",
                    "#rlhf"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Self-Steering Optimization (SSO) Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. SSO Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ€Ğ³Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Qwen2 Ğ¸ Llama3.1 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Automated Alignment: Let Machines Learn Their Own Preferences",
                    "desc": "The paper introduces Self-Steering Optimization (SSO), an algorithm that autonomously generates preference signals for machine learning models without human annotation. SSO ensures these signals are accurate by maintaining a consistent gap between chosen and rejected responses, aligning with the model's learning capacity. This approach benefits both online and offline training, improving the performance of policy and reward models. The effectiveness of SSO is validated with models like Qwen2 and Llama3.1, showing significant improvements across various benchmarks."
                },
                "zh": {
                    "title": "è‡ªå¼•å¯¼ä¼˜åŒ–ï¼šå®ç°è‡ªåŠ¨åŒ–å¯¹é½çš„æ–°è·¯å¾„",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºè‡ªå¼•å¯¼ä¼˜åŒ–ï¼ˆSSOï¼‰çš„ç®—æ³•ï¼Œå®ƒå¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„åå¥½ä¿¡å·ï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚SSOé€šè¿‡ç¡®ä¿é€‰æ‹©å’Œæ‹’ç»çš„å“åº”ä¹‹é—´ä¿æŒä¸€è‡´çš„å·®è·æ¥ç»´æŒä¿¡å·çš„å‡†ç¡®æ€§ï¼Œå¹¶ä½¿å®ƒä»¬éƒ½ç¬¦åˆå½“å‰ç­–ç•¥æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒSSOåœ¨ä¸éœ€è¦äººå·¥æ ‡æ³¨æˆ–å¤–éƒ¨æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜äº†å¤šä¸ªåŸºå‡†æµ‹è¯•çš„æ€§èƒ½ã€‚SSOç”Ÿæˆçš„åå¥½æ•°æ®ä¹Ÿæ˜¾è‘—æå‡äº†å¥–åŠ±æ¨¡å‹åœ¨Rewardbenchä¸Šçš„è¡¨ç°ã€‚"
                }
            },
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.16198",
            "title": "Improve Vision Language Model Chain-of-thought Reasoning",
            "url": "https://huggingface.co/papers/2410.16198",
            "abstract": "Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with minimal rationales. In this work, we show that training VLM on short answers does not generalize well to reasoning tasks that require more detailed responses. To address this, we propose a two-fold approach. First, we distill rationales from GPT-4o model to enrich the training data and fine-tune VLMs, boosting their CoT performance. Second, we apply reinforcement learning to further calibrate reasoning quality. Specifically, we construct positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, by comparing their predictions with annotated short answers. Using this pairwise data, we apply the Direct Preference Optimization algorithm to refine the model's reasoning abilities. Our experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction as well. This work emphasizes the importance of incorporating detailed rationales in training and leveraging reinforcement learning to strengthen the reasoning capabilities of VLMs.",
            "score": 16,
            "issue_id": 234,
            "pub_date": "2024-10-21",
            "pub_date_ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "hash": "f0a749332feba0df",
            "data": {
                "categories": [
                    "#benchmark",
                    "#interpretability",
                    "#rlhf"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ¾Ñ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚ GPT-4, Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ VLM Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "\"Boosting VLMs: From Short Answers to Deep Reasoning\"",
                    "desc": "The paper discusses how current vision language models (VLMs) struggle with reasoning tasks due to a lack of detailed rationale in training data. To improve this, the authors propose enriching training data with rationales distilled from a more advanced model, GPT-4o, and fine-tuning VLMs. They also use reinforcement learning to enhance reasoning by comparing correct and incorrect reasoning chains and applying the Direct Preference Optimization algorithm. The study shows that these methods significantly improve the models' reasoning abilities and their ability to generalize to new tasks."
                },
                "zh": {
                    "title": "ç»†åŒ–æ¨ç†ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯ä¿¡åº¦",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ä¸­é“¾å¼æ¨ç†çš„é‡è¦æ€§ï¼Œå¼ºè°ƒå…¶å¯¹æ¨¡å‹è§£é‡Šæ€§å’Œå¯ä¿¡åº¦çš„æå‡ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„è®­ç»ƒæ•°æ®å¤šä¸ºç®€çŸ­æ³¨é‡Šï¼Œç¼ºä¹è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä»GPT-4oæ¨¡å‹ä¸­æå–æ¨ç†è¿‡ç¨‹ä»¥ä¸°å¯Œè®­ç»ƒæ•°æ®ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–æ¨ç†è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶æ”¹å–„äº†ç›´æ¥ç­”æ¡ˆé¢„æµ‹çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            },
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.16267",
            "title": "xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs",
            "url": "https://huggingface.co/papers/2410.16267",
            "abstract": "We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for videos, particularly designed to efficiently capture temporal information over multiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in addition to the conventional visual tokenizer, which maps a sequence of tokens over multiple frames into a compact set of visual tokens. This enables BLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32 vs. 4608 tokens). We explore different types of temporal encoders, including learnable spatio-temporal pooling as well as sequential models like Token Turing Machines. We experimentally confirm that BLIP-3-Video obtains video question-answering accuracies comparable to much larger state-of-the-art models (e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using fewer visual tokens. The project website is at https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html",
            "score": 14,
            "issue_id": 233,
            "pub_date": "2024-10-21",
            "pub_date_ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "hash": "7e29d42e819fbb96",
            "data": {
                "categories": [
                    "#architecture",
                    "#multimodal",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ xGen-MM-Vid (BLIP-3-Video) Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 'Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº' Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğº Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². BLIP-3-Video Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ BLIP-3-Video Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑÑÑÑŒ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñƒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ."
                },
                "en": {
                    "title": "Efficient Video Understanding with Fewer Tokens",
                    "desc": "The paper introduces xGen-MM-Vid (BLIP-3-Video), a multimodal language model designed to efficiently process video data by capturing temporal information across multiple frames. It uses a 'temporal encoder' alongside a visual tokenizer to convert sequences of frames into a compact set of visual tokens, significantly reducing the number of tokens needed compared to other models. The model explores various temporal encoders, including spatio-temporal pooling and Token Turing Machines, to enhance its efficiency. Experimental results show that BLIP-3-Video achieves competitive video question-answering performance with fewer resources, being smaller and more efficient than larger models."
                },
                "zh": {
                    "title": "é«˜æ•ˆæ•æ‰è§†é¢‘æ—¶é—´ä¿¡æ¯çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºxGen-MM-Vidï¼ˆBLIP-3-Videoï¼‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œä¸“ä¸ºè§†é¢‘è®¾è®¡ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ•æ‰å¤šå¸§çš„æ—¶é—´ä¿¡æ¯ã€‚BLIP-3-Videoåˆ©ç”¨äº†â€œæ—¶é—´ç¼–ç å™¨â€ï¼Œç»“åˆä¼ ç»Ÿçš„è§†è§‰æ ‡è®°å™¨ï¼Œå°†å¤šå¸§åºåˆ—æ˜ å°„ä¸ºç´§å‡‘çš„è§†è§‰æ ‡è®°é›†ã€‚ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒä½¿ç”¨çš„è§†è§‰æ ‡è®°æ•°é‡å¤§å¤§å‡å°‘ï¼ˆä¾‹å¦‚ï¼Œ32ä¸ªå¯¹æ¯”4608ä¸ªï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBLIP-3-Videoåœ¨è§†é¢‘é—®ç­”å‡†ç¡®æ€§ä¸Šä¸æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ç›¸å½“ï¼Œä½†å…¶è§„æ¨¡æ›´å°ä¸”æ•ˆç‡æ›´é«˜ã€‚"
                }
            },
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.15926",
            "title": "Mitigating Object Hallucination via Concentric Causal Attention",
            "url": "https://huggingface.co/papers/2410.15926",
            "abstract": "Recent Large Vision Language Models (LVLMs) present remarkable zero-shot conversational and reasoning capabilities given multimodal queries. Nevertheless, they suffer from object hallucination, a phenomenon where LVLMs are prone to generate textual responses not factually aligned with image inputs. Our pilot study reveals that object hallucination is closely tied with Rotary Position Encoding (RoPE), a widely adopted positional dependency modeling design in existing LVLMs. Due to the long-term decay in RoPE, LVLMs tend to hallucinate more when relevant visual cues are distant from instruction tokens in the multimodal input sequence. Additionally, we observe a similar effect when reversing the sequential order of visual tokens during multimodal alignment. Our tests indicate that long-term decay in RoPE poses challenges to LVLMs while capturing visual-instruction interactions across long distances. We propose Concentric Causal Attention (CCA), a simple yet effective positional alignment strategy that mitigates the impact of RoPE long-term decay in LVLMs by naturally reducing relative distance between visual and instruction tokens. With CCA, visual tokens can better interact with instruction tokens, thereby enhancing model's perception capability and alleviating object hallucination. Without bells and whistles, our positional alignment method surpasses existing hallucination mitigation strategies by large margins on multiple object hallucination benchmarks.",
            "score": 14,
            "issue_id": 226,
            "pub_date": "2024-10-21",
            "pub_date_ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "hash": "9dfb7677b0acecf1",
            "data": {
                "categories": [
                    "#alignment",
                    "#cv",
                    "#hallucinations",
                    "#long_context",
                    "#rlhf"
                ],
                "emoji": "ğŸ‘ï¸",
                "ru": {
                    "title": "Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LVLMs) ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğº Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ·-Ğ·Ğ° Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ RoPE. Ğ­Ñ‚Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑƒÑÑƒĞ³ÑƒĞ±Ğ»ÑĞµÑ‚ÑÑ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ´Ğ°Ğ»ĞµĞºĞ¾ Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Concentric Causal Attention (CCA) Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸. CCA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Enhancing Visual Perception in LVLMs: Tackling Object Hallucination with CCA",
                    "desc": "The paper discusses how Large Vision Language Models (LVLMs) can sometimes generate incorrect textual responses that don't match the images they are analyzing, a problem known as object hallucination. This issue is linked to Rotary Position Encoding (RoPE), which struggles with long-term decay, causing LVLMs to hallucinate when visual cues are far from instruction tokens. The authors propose a new method called Concentric Causal Attention (CCA) to address this, which helps align visual and instruction tokens more effectively. By using CCA, the model's ability to perceive and interpret images improves, significantly reducing object hallucination compared to other methods."
                },
                "zh": {
                    "title": "åŒå¿ƒå› æœæ³¨æ„åŠ›ï¼šæ¶ˆé™¤å¯¹è±¡å¹»è§‰çš„æœ‰æ•ˆç­–ç•¥",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å¤„ç†å¤šæ¨¡æ€æŸ¥è¯¢æ—¶å‡ºç°çš„å¯¹è±¡å¹»è§‰é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™ç§ç°è±¡ä¸å¹¿æ³›ä½¿ç”¨çš„æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰æœ‰å…³ï¼Œå› ä¸ºRoPEçš„é•¿æœŸè¡°å‡å¯¼è‡´LVLMsåœ¨è§†è§‰çº¿ç´¢ä¸æŒ‡ä»¤æ ‡è®°è·ç¦»è¾ƒè¿œæ—¶æ›´å®¹æ˜“å‡ºç°å¹»è§‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„ä½ç½®å¯¹é½ç­–ç•¥ï¼Œç§°ä¸ºåŒå¿ƒå› æœæ³¨æ„åŠ›ï¼ˆCCAï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘è§†è§‰æ ‡è®°ä¸æŒ‡ä»¤æ ‡è®°ä¹‹é—´çš„ç›¸å¯¹è·ç¦»ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›å¾—åˆ°å¢å¼ºï¼Œå¯¹è±¡å¹»è§‰ç°è±¡å¾—åˆ°æ˜¾è‘—ç¼“è§£ã€‚"
                }
            },
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.17250",
            "title": "JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation",
            "url": "https://huggingface.co/papers/2410.17250",
            "abstract": "Accelerating research on Large Multimodal Models (LMMs) in non-English languages is crucial for enhancing user experiences across broader populations. In this paper, we introduce JMMMU (Japanese MMMU), the first large-scale Japanese benchmark designed to evaluate LMMs on expert-level tasks based on the Japanese cultural context. To facilitate comprehensive culture-aware evaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA) subset, where the culture-independent subjects (e.g., Math) are selected and translated into Japanese, enabling one-to-one comparison with its English counterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly crafted subjects that reflect Japanese cultural context. Using the CA subset, we observe performance drop in many LMMs when evaluated in Japanese, which is purely attributable to language variation. Using the CS subset, we reveal their inadequate Japanese cultural understanding. Further, by combining both subsets, we identify that some LMMs perform well on the CA subset but not on the CS subset, exposing a shallow understanding of the Japanese language that lacks depth in cultural understanding. We hope this work will not only help advance LMM performance in Japanese but also serve as a guideline to create high-standard, culturally diverse benchmarks for multilingual LMM development. The project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.",
            "score": 12,
            "issue_id": 222,
            "pub_date": "2024-10-22",
            "pub_date_ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "hash": "245b35927b3cf09b",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multilingual"
                ],
                "emoji": "ğŸ—¾",
                "ru": {
                    "title": "ĞšÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ¿Ğ¾Ğ½ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ JMMMU - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾Ğ½ÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LMM) Ğ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² ÑĞ¿Ğ¾Ğ½ÑĞºĞ¾Ğ¼ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²: ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ (CA) Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ (CS). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… LMM Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ½Ğ° ÑĞ¿Ğ¾Ğ½ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ñ… Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑĞ¿Ğ¾Ğ½ÑĞºĞ¾Ğ¹ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ°Ğ´ĞµÑÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ LMM Ğ½Ğ° ÑĞ¿Ğ¾Ğ½ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ¸ Ğ¿Ğ¾ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ…, ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ LMM."
                },
                "en": {
                    "title": "Bridging Language and Culture in AI: The JMMMU Benchmark",
                    "desc": "The paper introduces JMMMU, a benchmark for evaluating Large Multimodal Models (LMMs) in Japanese, focusing on both culture-agnostic and culture-specific tasks. It highlights that many LMMs show reduced performance in Japanese due to language differences, and struggle with tasks requiring deep cultural understanding. The study reveals that some models perform well on general tasks but fail on culturally nuanced ones, indicating a lack of cultural depth. This work aims to improve LMMs in Japanese and guide the creation of culturally diverse benchmarks for multilingual models."
                },
                "zh": {
                    "title": "æ¨åŠ¨å¤šè¯­è¨€æ¨¡å‹çš„æ–‡åŒ–æ·±åº¦ç†è§£",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†JMMMUï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ—¥æœ¬æ–‡åŒ–èƒŒæ™¯ä¸‹è¡¨ç°çš„åŸºå‡†ã€‚JMMMUåŒ…æ‹¬ä¸¤ä¸ªå­é›†ï¼šæ–‡åŒ–æ— å…³å­é›†å’Œæ–‡åŒ–ç‰¹å®šå­é›†ã€‚ç ”ç©¶å‘ç°ï¼Œè®¸å¤šæ¨¡å‹åœ¨æ—¥è¯­ç¯å¢ƒä¸‹è¡¨ç°ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨æ–‡åŒ–ç‰¹å®šå­é›†ä¸Šè¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºå‡ºå¯¹æ—¥æœ¬æ–‡åŒ–ç†è§£çš„ä¸è¶³ã€‚é€šè¿‡è¿™é¡¹ç ”ç©¶ï¼Œä½œè€…å¸Œæœ›æ¨åŠ¨å¤šè¯­è¨€æ¨¡å‹åœ¨æ—¥è¯­ä¸­çš„è¡¨ç°ï¼Œå¹¶ä¸ºåˆ›å»ºé«˜æ ‡å‡†çš„å¤šæ–‡åŒ–åŸºå‡†æä¾›æŒ‡å¯¼ã€‚"
                }
            },
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.17215",
            "title": "MiniPLM: Knowledge Distillation for Pre-Training Language Models",
            "url": "https://huggingface.co/papers/2410.17215",
            "abstract": "Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. While effective in fine-tuning, KD during pre-training faces challenges in efficiency, flexibility, and effectiveness. Existing methods either incur high computational costs due to online teacher inference, require tokenization matching between teacher and student LMs, or risk losing the difficulty and diversity of the teacher-generated training data. To address these issues, we propose MiniPLM, a KD framework for pre-training LMs by refining the training data distribution with the teacher's knowledge. For efficiency, MiniPLM performs offline teacher LM inference, allowing KD for multiple student LMs without adding training-time costs. For flexibility, MiniPLM operates solely on the training corpus, enabling KD across model families. For effectiveness, MiniPLM leverages the differences between large and small LMs to enhance the difficulty and diversity of the training data, helping student LMs acquire versatile and sophisticated knowledge. Extensive experiments demonstrate that MiniPLM boosts the student LMs' performance on 9 widely used downstream tasks, improves the language modeling capabilities, and reduces pre-training computation. The benefit of MiniPLM extends to large pre-training scales, evidenced by the extrapolation of the scaling curves. Further analysis reveals that MiniPLM supports KD across model families and enhances the utilization of pre-training data. Our model, code, and data are available at https://github.com/thu-coai/MiniPLM.",
            "score": 12,
            "issue_id": 221,
            "pub_date": "2024-10-22",
            "pub_date_ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "hash": "50ff5ac54c5a78a2",
            "data": {
                "categories": [
                    "#dataset"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "MiniPLM: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MiniPLM - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. MiniPLM Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ²Ñ‹Ğ²Ğ¾Ğ´ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ¾Ğ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MiniPLM ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑƒÑ‡ĞµĞ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "MiniPLM: Efficient and Flexible Knowledge Distillation for Smarter Language Models",
                    "desc": "The paper introduces MiniPLM, a knowledge distillation framework designed to improve the pre-training of small language models by using the knowledge from larger models. MiniPLM enhances efficiency by performing offline teacher model inference, which reduces computational costs and allows for training multiple student models simultaneously. It also increases flexibility by working directly with the training corpus, enabling knowledge distillation across different model families. The framework improves the effectiveness of training by refining the data distribution, which helps student models learn more complex and diverse information, leading to better performance on various tasks."
                },
                "zh": {
                    "title": "MiniPLMï¼šé«˜æ•ˆçµæ´»çš„çŸ¥è¯†è’¸é¦æ¡†æ¶",
                    "desc": "çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰å¸¸ç”¨äºé€šè¿‡å¤§å‹æ•™å¸ˆè¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰è®­ç»ƒå°å‹é«˜æ€§èƒ½å­¦ç”Ÿè¯­è¨€æ¨¡å‹ã€‚MiniPLMæ˜¯ä¸€ç§æ–°çš„KDæ¡†æ¶ï¼Œé€šè¿‡ç¦»çº¿æ•™å¸ˆæ¨ç†æé«˜æ•ˆç‡ï¼Œå‡å°‘è®­ç»ƒæ—¶é—´æˆæœ¬ã€‚å®ƒä»…ä¾èµ–è®­ç»ƒè¯­æ–™åº“ï¼Œå¢å¼ºäº†æ¨¡å‹é—´çš„çµæ´»æ€§ï¼Œå¹¶é€šè¿‡ä¸°å¯Œè®­ç»ƒæ•°æ®çš„éš¾åº¦å’Œå¤šæ ·æ€§æé«˜å­¦ç”Ÿæ¨¡å‹çš„æ•ˆæœã€‚å®éªŒè¡¨æ˜ï¼ŒMiniPLMåœ¨å¤šé¡¹ä»»åŠ¡ä¸­æå‡äº†å­¦ç”Ÿæ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶å‡å°‘äº†é¢„è®­ç»ƒè®¡ç®—é‡ã€‚"
                }
            },
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.14649",
            "title": "EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search",
            "url": "https://huggingface.co/papers/2410.14649",
            "abstract": "The high computational costs of large language models (LLMs) have led to a flurry of research on LLM compression, via methods such as quantization, sparsification, or structured pruning. A new frontier in this area is given by dynamic, non-uniform compression methods, which adjust the compression levels (e.g., sparsity) per-block or even per-layer in order to minimize accuracy loss, while guaranteeing a global compression threshold. Yet, current methods rely on heuristics for identifying the \"importance\" of a given layer towards the loss, based on assumptions such as error monotonicity, i.e. that the end-to-end model compression error is proportional to the sum of layer-wise errors. In this paper, we revisit this area, and propose a new and general approach for dynamic compression that is provably optimal in a given input range. We begin from the motivating observation that, in general, error monotonicity does not hold for LLMs: compressed models with lower sum of per-layer errors can perform worse than models with higher error sums. To address this, we propose a new general evolutionary framework for dynamic LLM compression called EvoPress, which has provable convergence, and low sample and evaluation complexity. We show that these theoretical guarantees lead to highly competitive practical performance for dynamic compression of Llama, Mistral and Phi models. Via EvoPress, we set new state-of-the-art results across all compression approaches: structural pruning (block/layer dropping), unstructured sparsity, as well as quantization with dynamic bitwidths. Our code is available at https://github.com/IST-DASLab/EvoPress.",
            "score": 7,
            "issue_id": 222,
            "pub_date": "2024-10-18",
            "pub_date_ru": "18 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "hash": "8c7a7ff7f03a5de4",
            "data": {
                "categories": [
                    "#inference",
                    "#optimization"
                ],
                "emoji": "ğŸ—œï¸",
                "ru": {
                    "title": "EvoPress: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ EvoPress. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ¼Ğ¾Ğ½Ğ¾Ñ‚Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² LLM Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ Ğ´Ğ¾ĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ. EvoPress Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Llama, Mistral Ğ¸ Phi, ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³Ğµ, Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€ÑĞ´Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞšĞ¾Ğ´ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ GitHub."
                },
                "en": {
                    "title": "EvoPress: Revolutionizing LLM Compression with Dynamic Precision",
                    "desc": "The paper addresses the challenge of reducing the computational costs of large language models by introducing a new method called EvoPress for dynamic compression. Unlike traditional methods that rely on assumptions like error monotonicity, EvoPress uses an evolutionary framework to adjust compression levels per block or layer, ensuring minimal accuracy loss. This approach is proven to be optimal within a specific input range and demonstrates superior performance across various compression techniques, including pruning and quantization. The authors provide theoretical guarantees for EvoPress, which translate into practical improvements for models like Llama, Mistral, and Phi."
                },
                "zh": {
                    "title": "EvoPressï¼šå¤§è¯­è¨€æ¨¡å‹å‹ç¼©çš„æ–°çªç ´",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹çš„å‹ç¼©é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€å‹ç¼©æ–¹æ³•ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå‡è®¾å±‚çš„é‡è¦æ€§æ¥è¿›è¡Œå‹ç¼©ï¼Œè€Œè¿™ç§æ–°æ–¹æ³•åˆ™ä¸ä¾èµ–äºè¿™äº›å‡è®¾ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸ºEvoPressçš„è¿›åŒ–æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ç»™å®šè¾“å…¥èŒƒå›´å†…å®ç°æœ€ä¼˜å‹ç¼©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEvoPressåœ¨å¤šç§å‹ç¼©æ–¹æ³•ä¸­éƒ½è¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ã€‚"
                }
            },
            "pub_date_card": {
                "ru": "18 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 18",
                "zh": "10æœˆ18æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.16930",
            "title": "Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes",
            "url": "https://huggingface.co/papers/2410.16930",
            "abstract": "Math reasoning is a highly active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be isolated within a model. Doing so could allow targeted intervention to improve math performance without altering non-math behavior and foster understanding of how models encode math reasoning. We introduce Math Neurosurgery (MathNeuro), a method for isolating math-specific parameters in LLMs using only forward passes. MathNeuro builds on existing work by using weights and activations to calculate parameter importance, but isolates math-specific parameters by removing those important for general language tasks. Pruning parameters MathNeuro identifies deletes a LLM's math reasoning ability without destroying its general language ability. Scaling these parameters by a small constant improves a pretrained or instruction-tuned LLM's performance by 4-17% on GSM8K while leaving non-math behavior unaltered. MathNeuro is also data efficient: most of its effectiveness holds when identifying math-specific parameters using a single sample. MathNeuro highlights the potential for future work to intervene on math-specific parameters.",
            "score": 5,
            "issue_id": 226,
            "pub_date": "2024-10-22",
            "pub_date_ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "hash": "13a2560869429449",
            "data": {
                "categories": [
                    "#interpretability",
                    "#math"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "ĞĞµĞ¹Ñ€Ğ¾Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ˜Ğ˜: Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ MathNeuro Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ·Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ğ³Ğ¸Ğ²Ğ°Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ÑƒĞ½Ğ¸Ñ‡Ñ‚Ğ¾Ğ¶Ğ°ĞµÑ‚ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ¸Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 4-17% Ğ² Ñ‚ĞµÑÑ‚Ğµ GSM8K. MathNeuro ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞµĞ´Ğ¸Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ LLM."
                },
                "en": {
                    "title": "\"Unlocking Math Genius in AI: Isolate, Enhance, Excel!\"",
                    "desc": "The paper introduces Math Neurosurgery (MathNeuro), a method to isolate math-specific parameters in Large Language Models (LLMs) using only forward passes. MathNeuro identifies and prunes parameters crucial for math reasoning without affecting general language abilities. By scaling these parameters, the method enhances math performance by 4-17% on GSM8K, while keeping non-math behavior unchanged. This approach is data-efficient, requiring only a single sample to identify math-specific parameters, paving the way for targeted improvements in math reasoning."
                },
                "zh": {
                    "title": "MathNeuroï¼šç²¾å‡†æå‡æ•°å­¦æ¨ç†èƒ½åŠ›çš„åˆ©å™¨",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMathNeuroçš„æ–¹æ³•ï¼Œç”¨äºåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­éš”ç¦»æ•°å­¦æ¨ç†å‚æ•°ã€‚MathNeuroé€šè¿‡å‰å‘ä¼ æ’­è®¡ç®—å‚æ•°çš„é‡è¦æ€§ï¼Œå¹¶å»é™¤å¯¹ä¸€èˆ¬è¯­è¨€ä»»åŠ¡é‡è¦çš„å‚æ•°ï¼Œä»è€Œä¸“æ³¨äºæ•°å­¦æ¨ç†ã€‚é€šè¿‡è°ƒæ•´è¿™äº›å‚æ•°ï¼Œæ¨¡å‹çš„æ•°å­¦æ€§èƒ½å¯ä»¥æé«˜4-17%ï¼Œè€Œä¸å½±å“å…¶è¯­è¨€èƒ½åŠ›ã€‚MathNeuroå±•ç¤ºäº†åœ¨ä¸æ”¹å˜éæ•°å­¦è¡Œä¸ºçš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°æå‡æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚"
                }
            },
            "pub_date_card": {
                "ru": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 22",
                "zh": "10æœˆ22æ—¥"
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.16266",
            "title": "3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors",
            "url": "https://huggingface.co/papers/2410.16266",
            "abstract": "Novel-view synthesis aims to generate novel views of a scene from multiple input images or videos, and recent advancements like 3D Gaussian splatting (3DGS) have achieved notable success in producing photorealistic renderings with efficient pipelines. However, generating high-quality novel views under challenging settings, such as sparse input views, remains difficult due to insufficient information in under-sampled areas, often resulting in noticeable artifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing the representation quality of 3DGS representations. We leverage 2D video diffusion priors to address the challenging 3D view consistency problem, reformulating it as achieving temporal consistency within a video generation process. 3DGS-Enhancer restores view-consistent latent features of rendered novel views and integrates them with the input views through a spatial-temporal decoder. The enhanced views are then used to fine-tune the initial 3DGS model, significantly improving its rendering performance. Extensive experiments on large-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields superior reconstruction performance and high-fidelity rendering results compared to state-of-the-art methods. The project webpage is https://xiliu8006.github.io/3DGS-Enhancer-project .",
            "score": 2,
            "issue_id": 237,
            "pub_date": "2024-10-21",
            "pub_date_ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
            "hash": "f00040bffceab087",
            "data": {
                "categories": [
                    "#3d",
                    "#benchmark",
                    "#dataset",
                    "#video"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ 3DGS-Enhancer - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ 2D Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ² 3D, Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑ ĞµĞµ ĞºĞ°Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¾Ñ‚Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€. Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´Ñ‹ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ 3DGS Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°."
                },
                "en": {
                    "title": "Enhancing 3D Views with Temporal Consistency",
                    "desc": "The paper introduces 3DGS-Enhancer, a new method to improve the quality of 3D Gaussian splatting (3DGS) for novel-view synthesis, especially in challenging scenarios with sparse input views. By using 2D video diffusion priors, the approach addresses the issue of maintaining 3D view consistency by ensuring temporal consistency in video generation. The method enhances the latent features of rendered views and combines them with input views using a spatial-temporal decoder, which is then used to refine the original 3DGS model. Experiments show that 3DGS-Enhancer significantly outperforms existing methods in rendering high-quality, photorealistic scenes."
                },
                "zh": {
                    "title": "3Dè§†å›¾ä¸€è‡´æ€§çš„æ–°çªç ´ï¼š3DGS-Enhancer",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸º3DGS-Enhancerçš„æ–°æ–¹æ³•ï¼Œç”¨äºæå‡3Dé«˜æ–¯ç‚¹äº‘è¡¨ç¤ºçš„è´¨é‡ã€‚é€šè¿‡åˆ©ç”¨2Dè§†é¢‘æ‰©æ•£å…ˆéªŒï¼Œè¯¥æ–¹æ³•è§£å†³äº†3Dè§†å›¾ä¸€è‡´æ€§çš„é—®é¢˜ï¼Œå°†å…¶é‡æ–°å®šä¹‰ä¸ºè§†é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ—¶é—´ä¸€è‡´æ€§ã€‚3DGS-Enhanceré€šè¿‡ç©ºé—´-æ—¶é—´è§£ç å™¨å°†æ¸²æŸ“çš„æ–°è§†å›¾ä¸è¾“å…¥è§†å›¾ç»“åˆï¼Œæ¢å¤è§†å›¾ä¸€è‡´çš„æ½œåœ¨ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†æ¸²æŸ“æ€§èƒ½ã€‚"
                }
            },
            "pub_date_card": {
                "ru": "21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 21",
                "zh": "10æœˆ21æ—¥"
            }
        }
    ],
    "link_prev": "2024-10-22.html",
    "link_next": "2024-10-24.html",
    "date_prev": "22 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
    "date_next": "24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
    "short_date_prev": {
        "ru": "22.10",
        "en": "10/22",
        "zh": "10æœˆ22æ—¥"
    },
    "short_date_next": {
        "ru": "24.10",
        "en": "10/24",
        "zh": "10æœˆ24æ—¥"
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« è®¨è®ºäº†å¤§è§„æ¨¡è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸­å›¾åƒè¡¨ç¤ºçš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚å›¾åƒåŒ…å«å¤§é‡ä¿¡æ¯ï¼Œè¡¨ç¤ºä¸€å¼ å›¾åƒéœ€è¦å¤§é‡çš„æ ‡è®°ï¼Œè®¡ç®—æˆæœ¬éšå›¾åƒåˆ†è¾¨ç‡å¢åŠ è€Œå¢åŠ ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†PyramidDropç­–ç•¥ï¼Œåœ¨æ¨¡å‹çš„ä¸åŒé˜¶æ®µé€æ­¥å‡å°‘å›¾åƒæ ‡è®°ï¼Œä»è€Œæé«˜è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ï¼Œä¸”æ€§èƒ½æŸå¤±å¯å¿½ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPyramidDropå¯æ˜¾è‘—åŠ å¿«è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ï¼Œå¹¶å¯ä½œä¸ºå³æ’å³ç”¨çš„ç­–ç•¥è¿›ä¸€æ­¥åŠ é€Ÿæ¨ç†ã€‚",
        "title": "PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction",
        "pinyin": "ZhÃ¨ piÄn wÃ©nzhÄng tÇolÃ¹n le dÃ guÄ«mÃ³ shÃ¬juÃ©-yÇ”yÃ¡n mÃ³xÃ­ng (LVLMs) zhÅng tÃºxiÃ ng biÇoshÃ¬ de jÃ¬suÃ n chÃ©ngbÄ›n wÃ¨nti. TÃºxiÃ ng bÄohÃ¡n dÃ liÃ ng xÃ¬nxÄ«, biÇoshÃ¬ yÄ« zhÄng tÃºxiÃ ng xÅ«yÃ o dÃ liÃ ng de biÄojÃ¬, jÃ¬suÃ n chÃ©ngbÄ›n suÃ­ tÃºxiÃ ng fÄ“nbiÃ nlÇœ zÄ“ngjiÄ Ã©r zÄ“ngjiÄ. WÃ¨i jiÄ›juÃ© zhÃ¨ yÄ« wÃ¨nti, zuÃ²zhÄ› tÃ­chÅ« le PyramidDrop cÃ¨lÃ¼Ã¨, zÃ i mÃ³xÃ­ng de bÃ¹tÃ³ng jiÄ“duÃ n zhÃºbÃ¹ jiÇnshÇo tÃºxiÃ ng biÄojÃ¬, cÃ³ng'Ã©r tÇgÄo xÃ¹nliÃ n hÃ© tuÄ«lÇ xiÃ olÇœ, Ã©rqiÄ› xÃ¬ngnÃ©ng sÇ”nshÄ« kÄ› hÅ«lÃ¼Ã¨. ShÃ­yÃ n jiÃ©guÇ’ xiÇnshÃ¬, PyramidDrop kÄ› xiÇnzhÃ¹ jiÄkuÃ i xÃ¹nliÃ n hÃ© tuÄ«lÇ sÃ¹dÃ¹, bÃ¬ng kÄ› zuÃ²wÃ©i jÃ­chÄjÃ­yÃ²ng de cÃ¨lÃ¼Ã¨ jÃ¬nxÃ­ng jiÄsÃ¹ tuÄ«lÇ.",
        "vocab": "[\n    {\"word\": \"è®¨è®º\", \"pinyin\": \"tÇo lÃ¹n\", \"trans\": \"discuss\"},\n    {\"word\": \"å¤§è§„æ¨¡\", \"pinyin\": \"dÃ  guÄ« mÃ³\", \"trans\": \"large-scale\"},\n    {\"word\": \"è§†è§‰-è¯­è¨€æ¨¡å‹\", \"pinyin\": \"shÃ¬ juÃ© yÇ” yÃ¡n mÃ³ xÃ¬ng\", \"trans\": \"vision-language model\"},\n    {\"word\": \"è®¡ç®—æˆæœ¬\", \"pinyin\": \"jÃ¬ suÃ n chÃ©ng bÄ›n\", \"trans\": \"computational cost\"},\n    {\"word\": \"è¡¨ç¤º\", \"pinyin\": \"biÇo shÃ¬\", \"trans\": \"representation\"},\n    {\"word\": \"æ ‡è®°\", \"pinyin\": \"biÄo jÃ¬\", \"trans\": \"token\"},\n    {\"word\": \"åˆ†è¾¨ç‡\", \"pinyin\": \"fÄ“n biÃ n lÇœ\", \"trans\": \"resolution\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"ç­–ç•¥\", \"pinyin\": \"cÃ¨ lÃ¼Ã¨\", \"trans\": \"strategy\"},\n    {\"word\": \"é€æ­¥\", \"pinyin\": \"zhÃº bÃ¹\", \"trans\": \"gradually\"},\n    {\"word\": \"å‡å°‘\", \"pinyin\": \"jiÇn shÇo\", \"trans\": \"reduce\"},\n    {\"word\": \"è®­ç»ƒ\", \"pinyin\": \"xÃ¹n liÃ n\", \"trans\": \"training\"},\n    {\"word\": \"æ¨ç†\", \"pinyin\": \"tuÄ« lÇ\", \"trans\": \"inference\"},\n    {\"word\": \"æ•ˆç‡\", \"pinyin\": \"xiÃ o lÇœ\", \"trans\": \"efficiency\"},\n    {\"word\": \"æŸå¤±\", \"pinyin\": \"sÇ”n shÄ«\", \"trans\": \"loss\"},\n    {\"word\": \"å¯å¿½ç•¥\", \"pinyin\": \"kÄ› hÅ« lÃ¼Ã¨\", \"trans\": \"negligible\"},\n    {\"word\": \"æ˜¾è‘—\", \"pinyin\": \"xiÇn zhÃ¹\", \"trans\": \"significant\"},\n    {\"word\": \"åŠ å¿«\", \"pinyin\": \"jiÄ kuÃ i\", \"trans\": \"accelerate\"},\n    {\"word\": \"é€Ÿåº¦\", \"pinyin\": \"sÃ¹ dÃ¹\", \"trans\": \"speed\"},\n    {\"word\": \"å³æ’å³ç”¨\", \"pinyin\": \"jÃ­ chÄ jÃ­ yÃ²ng\", \"trans\": \"plug-and-play\"},\n    {\"word\": \"è¿›ä¸€æ­¥\", \"pinyin\": \"jÃ¬n yÄ« bÃ¹\", \"trans\": \"further\"}\n]",
        "trans": "This article discusses the computational cost issue of image representation in large-scale vision-language models (LVLMs). Images contain a large amount of information, and representing an image requires a large number of tokens, with the computational cost increasing as the image resolution increases. To address this problem, the authors propose the PyramidDrop strategy, which gradually reduces image tokens at different stages of the model, thereby improving training and inference efficiency with negligible performance loss. Experimental results show that PyramidDrop can significantly accelerate training and inference speeds and can be used as a plug-and-play strategy to further speed up inference.",
        "update_ts": "2024-10-23 09:59"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 3,
        "#rl": 0,
        "#rlhf": 3,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 2,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 1,
        "#math": 1,
        "#multilingual": 1,
        "#architecture": 2,
        "#medicine": 0,
        "#training": 0,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 0,
        "#alignment": 2,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#translation": 0,
        "#nlp": 0
    },
    "link_month": "2024-10.html"
}