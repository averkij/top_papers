
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 35 papers. August 7.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°</span> | <span id="title-articles-count">35 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-08-06.html">â¬…ï¸ <span id="prev-date">06.08</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-08-08.html">â¡ï¸ <span id="next-date">08.08</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-08.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '7 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 7', 'zh': '8æœˆ7æ—¥'};
        let feedDateNext = {'ru': '08.08', 'en': '08/08', 'zh': '8æœˆ8æ—¥'};
        let feedDatePrev = {'ru': '06.08', 'en': '08/06', 'zh': '8æœˆ6æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.04026', 'title': 'VeriGUI: Verifiable Long-Chain GUI Dataset', 'url': 'https://huggingface.co/papers/2508.04026', 'abstract': 'VeriGUI is a novel dataset for evaluating GUI agents in long-horizon tasks, emphasizing long-chain complexity and subtask-level verifiability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have delved into constructing autonomous agents capable of performing complex Graphical User Interface (GUI)-based computer tasks, with the potential to revolutionize human-computer interaction. Despite encouraging results, existing efforts mainly focus on short-term interactions and rely on outcome-only verification, thereby limiting their scalability in real-world GUI applications that demand long-horizon task decomposition and execution. In this work, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed to facilitate the development and evaluation of generalist GUI agents operating in realistic computer environments. Our dataset emphasizes two critical dimensions: (1) long-chain complexity, with tasks decomposed into a sequence of interdependent subtasks spanning hundreds of steps, explicitly designed to allow any subtask to serve as a valid starting point; and (2) subtask-level verifiability, which enables diverse exploration strategies within each subtask, while ensuring that each subtask-level goal remains verifiable and consistent. The dataset consists of GUI task trajectories across both desktop and web, annotated by human experts. Extensive experiments on VeriGUI using various agents with different foundation models reveal significant performance gaps in handling long-horizon tasks, highlighting the need for more robust planning and decision-making capabilities in GUI agents.', 'score': 112, 'issue_id': 5221, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '2692487ee60e017e', 'authors': ['Shunyu Liu', 'Minghao Liu', 'Huichi Zhou', 'Zhenyu Cui', 'Yang Zhou', 'Yuhao Zhou', 'Wendong Fan', 'Ge Zhang', 'Jiajun Shi', 'Weihao Xuan', 'Jiaxing Huang', 'Shuang Luo', 'Fang Wu', 'Heli Qi', 'Qingcheng Zeng', 'Ziqi Ren', 'Jialiang Gao', 'Jindi Lv', 'Junjie Wang', 'Aosong Feng', 'Heng Zhou', 'Wangchunshu Zhou', 'Zhenfei Yin', 'Wenlong Zhang', 'Guohao Li', 'Wenhao Yu', 'Irene Li', 'Lei Ma', 'Lei Bai', 'Qunshu Lin', 'Mingli Song', 'Dacheng Tao'], 'affiliations': ['VeriGUI Team'], 'pdf_title_img': 'assets/pdf/title_img/2508.04026.jpg', 'data': {'categories': ['#games', '#agents', '#dataset', '#long_context'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'VeriGUI: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'VeriGUI - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ GUI-Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ğ´ĞµÑĞºÑ‚Ğ¾Ğ¿Ğ½Ñ‹Ñ… Ğ¸ Ğ²ĞµĞ±-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° VeriGUI Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering GUI Agents for Complex Tasks with VeriGUI', 'desc': 'VeriGUI is a new dataset created to test GUI agents on complex tasks that require many steps. It focuses on breaking down tasks into smaller, manageable subtasks that can be verified individually. This approach allows agents to start from any subtask, making it easier to explore different strategies. The dataset includes detailed task examples from desktop and web environments, showing that current agents struggle with long-term planning and decision-making.'}, 'zh': {'title': 'VeriGUIï¼šé•¿æ—¶é—´ä»»åŠ¡ä¸­çš„æ™ºèƒ½ä½“è¯„ä¼°æ–°æ ‡å‡†', 'desc': 'VeriGUIæ˜¯ä¸€ä¸ªæ–°é¢–çš„æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­æ‰§è¡Œå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ“ä½œçš„æ™ºèƒ½ä½“ã€‚è¯¥æ•°æ®é›†å¼ºè°ƒäº†é•¿é“¾å¤æ‚æ€§å’Œå­ä»»åŠ¡çº§å¯éªŒè¯æ€§ï¼Œå…è®¸ä»»åŠ¡è¢«åˆ†è§£ä¸ºæ•°ç™¾ä¸ªç›¸äº’ä¾èµ–çš„å­ä»»åŠ¡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä»»ä½•å­ä»»åŠ¡éƒ½å¯ä»¥ä½œä¸ºæœ‰æ•ˆçš„èµ·ç‚¹ï¼Œä¿ƒè¿›äº†å¤šæ ·åŒ–çš„æ¢ç´¢ç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰æ™ºèƒ½ä½“åœ¨å¤„ç†é•¿æ—¶é—´ä»»åŠ¡æ—¶å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œè¡¨æ˜éœ€è¦æ›´å¼ºå¤§çš„è§„åˆ’å’Œå†³ç­–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01191', 'title': 'Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens', 'url': 'https://huggingface.co/papers/2508.01191', 'abstract': 'CoT reasoning in LLMs is found to be limited by the distribution discrepancy between training and test data, suggesting it is not a robust form of reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.', 'score': 102, 'issue_id': 5220, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 2', 'zh': '8æœˆ2æ—¥'}, 'hash': '427ac75c7123b50a', 'authors': ['Chengshuai Zhao', 'Zhen Tan', 'Pingchuan Ma', 'Dawei Li', 'Bohan Jiang', 'Yancheng Wang', 'Yingzhen Yang', 'Huan Liu'], 'affiliations': ['Arizona State University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.01191.jpg', 'data': {'categories': ['#reasoning', '#data', '#training', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ CoT-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM: Ğ¼Ğ¸Ñ€Ğ°Ğ¶ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ (CoT) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ñ‹ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑÑ€ĞµĞ´Ñƒ DataAlchemy Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ CoT-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚Ñ€ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼: Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°, Ğ´Ğ»Ğ¸Ğ½Ğ° Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ CoT-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½ĞµÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹ Ğ¸ Ğ¸ÑÑ‡ĞµĞ·Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğµ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM.'}, 'en': {'title': 'Unmasking the Fragility of CoT Reasoning in LLMs', 'desc': 'This paper examines the limitations of Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) by analyzing the impact of distribution discrepancies between training and test data. It suggests that while CoT prompting can enhance performance by mimicking human-like reasoning, this reasoning may not be as robust as it seems. The authors introduce a framework called DataAlchemy to systematically investigate how CoT reasoning varies across different tasks, lengths, and formats under controlled conditions. Their findings indicate that CoT reasoning is fragile and often fails when faced with data that deviates from what the model was trained on, highlighting the need for more reliable reasoning mechanisms in LLMs.'}, 'zh': {'title': 'é“¾å¼æ€ç»´æ¨ç†çš„å±€é™æ€§ä¸æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡æ¢è®¨äº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯è®­ç»ƒæ•°æ®ä¸æµ‹è¯•æ•°æ®ä¹‹é—´çš„åˆ†å¸ƒå·®å¼‚å¯¹å…¶å½±å“ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒCoTæ¨ç†å¹¶ä¸æ˜¯ä¸€ç§ç¨³å¥çš„æ¨ç†å½¢å¼ï¼Œå› ä¸ºå®ƒçš„æœ‰æ•ˆæ€§å—åˆ°è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æŸ¥è¯¢ä¹‹é—´åˆ†å¸ƒå·®å¼‚çš„é™åˆ¶ã€‚é€šè¿‡è®¾è®¡ä¸€ä¸ªåä¸ºDataAlchemyçš„æ§åˆ¶ç¯å¢ƒï¼Œä½œè€…ç³»ç»Ÿæ€§åœ°åˆ†æäº†CoTæ¨ç†åœ¨ä¸åŒä»»åŠ¡ã€é•¿åº¦å’Œæ ¼å¼ä¸‹çš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå½“CoTæ¨ç†è¶…å‡ºè®­ç»ƒåˆ†å¸ƒæ—¶ï¼Œå…¶æ•ˆæœä¼šæ˜¾è‘—ä¸‹é™ï¼Œæ­ç¤ºäº†å®ç°çœŸæ­£å¯æ¨å¹¿æ¨ç†çš„æŒç»­æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02694', 'title': 'Efficient Agents: Building Effective Agents While Reducing Cost', 'url': 'https://huggingface.co/papers/2508.02694', 'abstract': 'A study on the efficiency-effectiveness trade-off in LLM-driven agent systems identifies optimal agent framework design to reduce costs while maintaining performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable capabilities of Large Language Model (LLM)-driven agents have enabled sophisticated systems to tackle complex, multi-step tasks, but their escalating costs threaten scalability and accessibility. This work presents the first systematic study of the efficiency-effectiveness trade-off in modern agent systems, addressing the critical need for cost-effective designs without sacrificing performance. We investigate three key questions: (1) How much complexity do agentic tasks inherently require? (2) When do additional modules yield diminishing returns? (3) How much efficiency can be gained through the design of efficient agent frameworks? Through an empirical analysis on the GAIA benchmark, we evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies. Using the cost-of-pass metric, we quantify the efficiency-performance trade-off across these dimensions. Our findings inform the development of Efficient Agents , a novel agent framework that has an optimal complexity to task requirements. Efficient Agents retains 96.7% of the performance of OWL, one leading open-source agent framework, while reducing operational costs from 0.398 to 0.228, resulting in a 28.4% improvement in cost-of-pass. Our work provides actionable insights for designing efficient, high-performing agent systems, advancing the accessibility and sustainability of AI-driven solutions.', 'score': 45, 'issue_id': 5222, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ»Ñ', 'en': 'July 24', 'zh': '7æœˆ24æ—¥'}, 'hash': '0c518e4f5949dae3', 'authors': ['Ningning Wang', 'Xavier Hu', 'Pai Liu', 'He Zhu', 'Yue Hou', 'Heyuan Huang', 'Shengyu Zhang', 'Jian Yang', 'Jiaheng Liu', 'Ge Zhang', 'Changwang Zhang', 'Jun Wang', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou'], 'affiliations': ['OPPO', 'OPPO-PersonalAI'], 'pdf_title_img': 'assets/pdf/title_img/2508.02694.jpg', 'data': {'categories': ['#agents', '#optimization', '#training', '#open_source', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ñ… Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ GAIA, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° LLM, Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ² Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Efficient Agents, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğ¹ 96.7% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞ´ÑƒÑ‰ĞµĞ³Ğ¾ open-source Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° 28.4%.'}, 'en': {'title': 'Balancing Cost and Performance in AI Agents', 'desc': 'This paper explores how to balance efficiency and effectiveness in systems powered by Large Language Models (LLMs). It identifies the optimal design for agent frameworks that can lower costs while still performing well. The study answers key questions about task complexity, the diminishing returns of adding modules, and how to enhance efficiency through better design. The results show that the proposed Efficient Agents framework maintains high performance while significantly reducing operational costs, making AI solutions more accessible and sustainable.'}, 'zh': {'title': 'é«˜æ•ˆä»£ç†ç³»ç»Ÿï¼šé™ä½æˆæœ¬ä¸ä¿æŒæ€§èƒ½çš„å¹³è¡¡', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„ä»£ç†ç³»ç»Ÿä¸­çš„æ•ˆç‡ä¸æ•ˆæœä¹‹é—´çš„æƒè¡¡ï¼Œæ—¨åœ¨è®¾è®¡å‡ºæ—¢èƒ½é™ä½æˆæœ¬åˆèƒ½ä¿æŒæ€§èƒ½çš„æœ€ä½³ä»£ç†æ¡†æ¶ã€‚æˆ‘ä»¬åˆ†æäº†ä»£ç†ä»»åŠ¡çš„å¤æ‚æ€§ã€é¢å¤–æ¨¡å—çš„è¾¹é™…æ•ˆç›Šä»¥åŠé€šè¿‡é«˜æ•ˆä»£ç†æ¡†æ¶è®¾è®¡æ‰€èƒ½è·å¾—çš„æ•ˆç‡æå‡ã€‚é€šè¿‡å¯¹GAIAåŸºå‡†çš„å®è¯åˆ†æï¼Œæˆ‘ä»¬è¯„ä¼°äº†LLMéª¨å¹²é€‰æ‹©ã€ä»£ç†æ¡†æ¶è®¾è®¡å’Œæµ‹è¯•æ—¶æ‰©å±•ç­–ç•¥çš„å½±å“ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¼€å‘é«˜æ•ˆä»£ç†ç³»ç»Ÿå¯ä»¥åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½è¿è¥æˆæœ¬ï¼Œæ¨åŠ¨AIè§£å†³æ–¹æ¡ˆçš„å¯åŠæ€§å’Œå¯æŒç»­æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04700', 'title': 'SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from\n  Experience', 'url': 'https://huggingface.co/papers/2508.04700', 'abstract': "SEAgent, an agentic self-evolving framework, enables computer-use agents to autonomously master novel software through experiential learning and a curriculum of tasks, achieving superior performance compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agent's policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS.", 'score': 36, 'issue_id': 5221, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '6c7450255f7c28bc', 'authors': ['Zeyi Sun', 'Ziyu Liu', 'Yuhang Zang', 'Yuhang Cao', 'Xiaoyi Dong', 'Tong Wu', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.04700.jpg', 'data': {'categories': ['#agents', '#agi', '#open_source', '#optimization', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¾ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞŸĞ', 'desc': 'SEAgent - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ğ¾Ğµ ĞŸĞ Ñ‡ĞµÑ€ĞµĞ· ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ°ÑÑ‰ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. SEAgent Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ ÑƒÑ‡ĞµĞ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering Agents to Learn and Evolve Autonomously', 'desc': 'SEAgent is a self-evolving framework designed for computer-use agents (CUAs) to autonomously learn and master new software through experiential learning. It allows agents to explore unfamiliar software environments and learn from their experiences by tackling tasks that increase in complexity. The framework includes a World State Model for assessing agent performance and a Curriculum Generator for creating diverse tasks. By integrating insights from specialist agents, SEAgent develops a robust generalist CUA that outperforms traditional methods, achieving a notable increase in success rates across various software environments.'}, 'zh': {'title': 'SEAgentï¼šè‡ªä¸»è¿›åŒ–çš„æ™ºèƒ½ä½“æ¡†æ¶', 'desc': 'SEAgentæ˜¯ä¸€ç§è‡ªæˆ‘è¿›åŒ–çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œèƒ½å¤Ÿä½¿è®¡ç®—æœºä½¿ç”¨ä»£ç†é€šè¿‡ä½“éªŒå­¦ä¹ è‡ªä¸»æŒæ¡æ–°è½¯ä»¶ã€‚è¯¥æ¡†æ¶é€šè¿‡é€æ­¥çš„ä»»åŠ¡è¯¾ç¨‹ï¼Œå¸®åŠ©ä»£ç†åœ¨æ²¡æœ‰äººç±»æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œæ¢ç´¢å’Œå­¦ä¹ é™Œç”Ÿçš„è½¯ä»¶ç¯å¢ƒã€‚SEAgentè®¾è®¡äº†ä¸–ç•ŒçŠ¶æ€æ¨¡å‹å’Œè¯¾ç¨‹ç”Ÿæˆå™¨ï¼Œä»¥ä¾¿ä»£ç†èƒ½å¤Ÿé€šè¿‡è¯•é”™å­¦ä¹ ä¸æ–­æé«˜å…¶èƒ½åŠ›ã€‚æœ€ç»ˆï¼ŒSEAgentçš„è¡¨ç°è¶…è¶Šäº†å¤šä¸ªä¸“é—¨åŒ–ä»£ç†çš„ç»„åˆï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ–°è½¯ä»¶ç¯å¢ƒä¸­çš„ä¼˜è¶Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03501', 'title': 'Training Long-Context, Multi-Turn Software Engineering Agents with\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2508.03501', 'abstract': "Research on applications of Reinforcement Learning (RL) to Large Language Models (LLMs) has mostly been focused on single-turn problems, such as mathematical reasoning or single-shot code generation. While these problems can be viewed as token-level multi-turn MDPs, this view corresponds to a degenerate case of multi-turn interaction where the environment provides no feedback. This contrasts with many real-world domains, such as software engineering (SWE), which require rich multi-turn interactions with a stateful environment that responds to each action with a non-trivial observation.   To bridge this gap, we demonstrate the successful application of RL to this general regime. Using a modified Decoupled Advantage Policy Optimization (DAPO) algorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world software engineering tasks. Our approach increases the agent's success rate on the SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to 39%, without relying on any teacher models. On SWE-rebench, our agent matches or outperforms leading open-weight models such as DeepSeek-V3-0324 and Qwen3-235B-A22B using an identical scaffolding, offering a viable path toward building more capable autonomous agents for complex real-world problems based on open models.", 'score': 27, 'issue_id': 5228, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '9e2fbad63802fc98', 'authors': ['Alexander Golubev', 'Maria Trofimova', 'Sergei Polezhaev', 'Ibragim Badertdinov', 'Maksim Nekrashevich', 'Anton Shevtsov', 'Simon Karasik', 'Sergey Abramov', 'Andrei Andriushchenko', 'Filipp Fisin', 'Sergei Skvortsov', 'Boris Yangel'], 'affiliations': ['Humanoid', 'Nebius AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.03501.jpg', 'data': {'categories': ['#open_source', '#agents', '#benchmark', '#optimization', '#rl', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'RL Ğ´Ğ»Ñ LLM: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL) Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (LLM) Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ DAPO, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Qwen2.5-72B-Instruct Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² ÑÑ‚Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-bench Verified Ñ 20% Ğ´Ğ¾ 39%. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-rebench Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Empowering Language Models with Reinforcement Learning for Real-World Software Engineering', 'desc': 'This paper explores the application of Reinforcement Learning (RL) to Large Language Models (LLMs) in multi-turn interactions, particularly in software engineering tasks. Unlike previous studies that focused on single-turn problems, this research addresses the need for agents to operate in environments that provide feedback after each action. The authors introduce a modified Decoupled Advantage Policy Optimization (DAPO) algorithm to train an agent, Qwen2.5-72B-Instruct, achieving a significant improvement in success rates on software engineering benchmarks. This work demonstrates the potential of RL in enhancing the capabilities of autonomous agents for complex, real-world applications without relying on teacher models.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ åŠ©åŠ›å¤§å‹è¯­è¨€æ¨¡å‹è§£å†³å¤æ‚ä»»åŠ¡', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¤šè½®äº¤äº’çš„çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›çš„è§£è€¦ä¼˜åŠ¿ç­–ç•¥ä¼˜åŒ–ï¼ˆDAPOï¼‰ç®—æ³•ï¼ŒæˆåŠŸè®­ç»ƒäº†ä¸€ä¸ªåŸºäºQwen2.5-72B-Instructçš„æ™ºèƒ½ä½“ï¼Œä»¥è§£å†³è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ™ºèƒ½ä½“åœ¨SWE-bench VerifiedåŸºå‡†æµ‹è¯•ä¸­çš„æˆåŠŸç‡ä»20%æå‡è‡³39%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ„å»ºæ›´å¼ºå¤§çš„è‡ªä¸»æ™ºèƒ½ä½“æä¾›äº†å¯è¡Œçš„è·¯å¾„ï¼Œèƒ½å¤Ÿåº”å¯¹å¤æ‚çš„ç°å®é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04280', 'title': 'Enhancing Vision-Language Model Training with Reinforcement Learning in\n  Synthetic Worlds for Real-World Success', 'url': 'https://huggingface.co/papers/2508.04280', 'abstract': 'A lightweight, hyperparameter-free RL algorithm, VL-DAC, enables VLMs to learn generalized policies from inexpensive simulators, improving performance on real-world benchmarks without sacrificing image understanding accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Interactive multimodal agents must convert raw visual observations into coherent sequences of language-conditioned actions -- a capability that current vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL) efforts could, in principle, endow VLMs with such skills, but they have seldom tested whether the learned behaviours generalize beyond their training simulators, and they depend either on brittle hyperparameter tuning or on dense-reward environments with low state variability. We introduce Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight, hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens while learning value only at the environment-step level: an arrangement, to our knowledge, not previously explored for large VLMs or LLMs. This simple decoupling removes unstable weighting terms and yields faster, more reliable convergence. Training a single VLM with VL-DAC in one inexpensive simulator at a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies that generalize widely: +50\\% relative on BALROG (game-centric agentic control), +5\\% relative on the hardest part of VSI-Bench (spatial planning), and +2\\% on VisualWebBench (web navigation), all without degrading general image understanding accuracy. These results provide the first evidence that a simple RL algorithm can train VLMs entirely in cheap synthetic worlds while delivering measurable gains on real-image agentic, spatial-reasoning, and web-navigation benchmarks.', 'score': 25, 'issue_id': 5227, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '007849b63760d8ee', 'authors': ['George Bredis', 'Stanislav Dereka', 'Viacheslav Sinii', 'Ruslan Rakhimov', 'Daniil Gavrilov'], 'affiliations': ['T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2508.04280.jpg', 'data': {'categories': ['#rl', '#training', '#transfer_learning', '#synthetic', '#games', '#rlhf', '#multimodal', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğµ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'VL-DAC - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ°Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. VL-DAC Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ PPO Ğº Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² ÑÑ€ĞµĞ´Ñ‹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'VL-DAC: Simplifying RL for Enhanced Vision-Language Learning', 'desc': 'The paper presents VL-DAC, a novel reinforcement learning (RL) algorithm designed for vision-language models (VLMs) that operates without the need for hyperparameter tuning. This algorithm allows VLMs to learn effective policies from low-cost simulators, enhancing their performance on real-world tasks while maintaining image understanding accuracy. By decoupling the learning of action tokens and value estimation, VL-DAC achieves faster and more stable convergence compared to previous methods. The results demonstrate that training with VL-DAC leads to significant improvements in various benchmarks, showcasing its potential for developing multimodal agents capable of complex tasks.'}, 'zh': {'title': 'è½»é‡çº§å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è½»é‡çº§çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•VL-DACï¼Œè¯¥ç®—æ³•æ— éœ€è¶…å‚æ•°è°ƒæ•´ï¼Œèƒ½å¤Ÿä½¿è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä»ä½æˆæœ¬çš„æ¨¡æ‹Ÿå™¨ä¸­å­¦ä¹ é€šç”¨ç­–ç•¥ã€‚VL-DACé€šè¿‡å¯¹åŠ¨ä½œä»¤ç‰Œåº”ç”¨PPOæ›´æ–°ï¼ŒåŒæ—¶ä»…åœ¨ç¯å¢ƒæ­¥éª¤çº§åˆ«å­¦ä¹ ä»·å€¼ï¼Œä»è€Œå®ç°äº†ç®€å•çš„è§£è€¦ï¼Œé¿å…äº†ä¸ç¨³å®šçš„åŠ æƒé¡¹ï¼Œä¿ƒè¿›äº†æ›´å¿«ã€æ›´å¯é çš„æ”¶æ•›ã€‚é€šè¿‡åœ¨å•ä¸ªä½æˆæœ¬æ¨¡æ‹Ÿå™¨ä¸­è®­ç»ƒVLMï¼ŒVL-DACèƒ½å¤Ÿåœ¨å¤šä¸ªçœŸå®ä¸–ç•ŒåŸºå‡†ä¸Šæ˜¾è‘—æé«˜æ€§èƒ½ï¼Œè€Œä¸å½±å“å›¾åƒç†è§£çš„å‡†ç¡®æ€§ã€‚è¿™äº›ç»“æœé¦–æ¬¡è¯æ˜äº†ç®€å•çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯ä»¥åœ¨å»‰ä»·çš„åˆæˆç¯å¢ƒä¸­å®Œå…¨è®­ç»ƒVLMï¼Œå¹¶åœ¨çœŸå®å›¾åƒçš„ä»£ç†æ§åˆ¶ã€ç©ºé—´æ¨ç†å’Œç½‘é¡µå¯¼èˆªåŸºå‡†ä¸Šå–å¾—å¯æµ‹é‡çš„æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03680', 'title': 'Agent Lightning: Train ANY AI Agents with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2508.03680', 'abstract': "Agent Lightning is a flexible RL framework for training LLMs in various agents, using a hierarchical RL algorithm and decoupling execution from training to handle complex interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, we define an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing us to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, we introduce a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the framework's potential for real-world agent training and deployment.", 'score': 20, 'issue_id': 5221, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '78a8398db0f71f63', 'authors': ['Xufang Luo', 'Yuge Zhang', 'Zhiyuan He', 'Zilong Wang', 'Siyun Zhao', 'Dongsheng Li', 'Luna K. Qiu', 'Yuqing Yang'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2508.03680.jpg', 'data': {'categories': ['#agents', '#rag', '#games', '#math', '#training', '#optimization', '#rl'], 'emoji': 'âš¡', 'ru': {'title': 'Agent Lightning: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Agent Lightning - ÑÑ‚Ğ¾ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ°Ğ¼Ğ¸, Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… text-to-SQL, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Decoupling Training and Execution for Enhanced AI Agent Performance', 'desc': 'Agent Lightning is a versatile framework designed for training Large Language Models (LLMs) using Reinforcement Learning (RL) techniques. It separates the training process from agent execution, allowing for easier integration with various existing AI agents without significant code changes. By modeling agent execution as a Markov decision process, it introduces a hierarchical RL algorithm called LightningRL, which effectively manages complex interactions and multi-agent scenarios. The framework has shown promising results in tasks like text-to-SQL and retrieval-augmented generation, indicating its effectiveness for real-world applications.'}, 'zh': {'title': 'Agent Lightningï¼šçµæ´»çš„æ™ºèƒ½ä½“è®­ç»ƒæ¡†æ¶', 'desc': 'Agent Lightningæ˜¯ä¸€ä¸ªçµæ´»çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºå„ç§æ™ºèƒ½ä½“è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚å®ƒé€šè¿‡å°†æ‰§è¡Œä¸è®­ç»ƒè§£è€¦ï¼Œä½¿ç”¨å±‚æ¬¡åŒ–çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„äº¤äº’é€»è¾‘ã€‚è¯¥æ¡†æ¶å…è®¸ä¸ç°æœ‰æ™ºèƒ½ä½“çš„æ— ç¼é›†æˆï¼Œå‡ ä¹ä¸éœ€è¦ä»£ç ä¿®æ”¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgent Lightningåœ¨æ–‡æœ¬åˆ°SQLã€å¢å¼ºç”Ÿæˆå’Œæ•°å­¦å·¥å…·ä½¿ç”¨ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºç¨³å®šçš„æŒç»­æ”¹è¿›ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…æ™ºèƒ½ä½“è®­ç»ƒå’Œéƒ¨ç½²ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03159', 'title': 'CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and\n  Prediction', 'url': 'https://huggingface.co/papers/2508.03159', 'abstract': "CoTox, a framework integrating LLMs with chain-of-thought reasoning, enhances multi-toxicity prediction by incorporating chemical structure data, biological pathways, and gene ontology terms, improving interpretability and predictive performance in drug development.  \t\t\t\t\tAI-generated summary \t\t\t\t Drug toxicity remains a major challenge in pharmaceutical development. Recent machine learning models have improved in silico toxicity prediction, but their reliance on annotated data and lack of interpretability limit their applicability. This limits their ability to capture organ-specific toxicities driven by complex biological mechanisms. Large language models (LLMs) offer a promising alternative through step-by-step reasoning and integration of textual data, yet prior approaches lack biological context and transparent rationale. To address this issue, we propose CoTox, a novel framework that integrates LLM with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox combines chemical structure data, biological pathways, and gene ontology (GO) terms to generate interpretable toxicity predictions through step-by-step reasoning. Using GPT-4o, we show that CoTox outperforms both traditional machine learning and deep learning model. We further examine its performance across various LLMs to identify where CoTox is most effective. Additionally, we find that representing chemical structures with IUPAC names, which are easier for LLMs to understand than SMILES, enhances the model's reasoning ability and improves predictive performance. To demonstrate its practical utility in drug development, we simulate the treatment of relevant cell types with drug and incorporated the resulting biological context into the CoTox framework. This approach allow CoTox to generate toxicity predictions aligned with physiological responses, as shown in case study. This result highlights the potential of LLM-based frameworks to improve interpretability and support early-stage drug safety assessment. The code and prompt used in this work are available at https://github.com/dmis-lab/CoTox.", 'score': 18, 'issue_id': 5222, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '229c53307b839ab7', 'authors': ['Jueon Park', 'Yein Park', 'Minju Song', 'Soyon Park', 'Donghyeon Lee', 'Seungheun Baek', 'Jaewoo Kang'], 'affiliations': ['AIGEN Sciences, Seoul 04778, Republic of Korea', 'Department of Computer Science and Engineering, Korea University, Seoul 17035, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2508.03159.jpg', 'data': {'categories': ['#science', '#interpretability', '#healthcare', '#reasoning', '#data', '#multimodal'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'CoTox: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM', 'desc': 'CoTox - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ, Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿ÑƒÑ‚ÑÑ… Ğ¸ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ñ‹ Ğ³ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ½Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. CoTox Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ² Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ´Ğ¸ÑÑ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸.'}, 'en': {'title': 'CoTox: Enhancing Drug Toxicity Prediction with LLMs and Chain-of-Thought Reasoning', 'desc': "CoTox is a new framework that combines large language models (LLMs) with chain-of-thought reasoning to predict multi-toxicity in drugs. It enhances the prediction process by integrating chemical structure data, biological pathways, and gene ontology terms, which helps in making the predictions more interpretable. By using step-by-step reasoning, CoTox improves the model's ability to understand complex biological mechanisms and organ-specific toxicities. The framework has shown superior performance compared to traditional machine learning and deep learning models, making it a valuable tool for early-stage drug safety assessment."}, 'zh': {'title': 'CoToxï¼šæå‡è¯ç‰©æ¯’æ€§é¢„æµ‹çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'CoToxæ˜¯ä¸€ä¸ªå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸é“¾å¼æ¨ç†ç›¸ç»“åˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šç§æ¯’æ€§é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚å®ƒé€šè¿‡æ•´åˆåŒ–å­¦ç»“æ„æ•°æ®ã€ç”Ÿç‰©é€šè·¯å’ŒåŸºå› æœ¬ä½“æœ¯è¯­ï¼Œç”Ÿæˆå¯è§£é‡Šçš„æ¯’æ€§é¢„æµ‹ã€‚ä¸ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸æ¯”ï¼ŒCoToxåœ¨è¯ç‰©å¼€å‘ä¸­è¡¨ç°å‡ºæ›´å¥½çš„é¢„æµ‹æ€§èƒ½ã€‚è¯¥æ¡†æ¶çš„è®¾è®¡ä½¿å¾—æ¯’æ€§é¢„æµ‹ä¸ç”Ÿç†ååº”ç›¸ä¸€è‡´ï¼Œå±•ç¤ºäº†LLMæ¡†æ¶åœ¨è¯ç‰©å®‰å…¨æ€§è¯„ä¼°ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03905', 'title': 'Sotopia-RL: Reward Design for Social Intelligence', 'url': 'https://huggingface.co/papers/2508.03905', 'abstract': 'Sotopia-RL, a novel reinforcement learning framework, enhances social intelligence in large language models by refining feedback into utterance-level, multi-dimensional rewards, improving performance in social tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as accommodation, persuasion, collaboration, and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions. However, social interactions have two key characteristics that set barriers for RL training: (1) partial observability, where utterances have indirect and delayed effects that complicate credit assignment, and (2) multi-dimensionality, where behaviors such as rapport-building or knowledge-seeking contribute indirectly to goal achievement. These characteristics make Markov decision process (MDP)-based RL with single-dimensional episode-level rewards inefficient and unstable. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment mitigates partial observability by attributing outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in Sotopia, an open-ended social learning environment, demonstrate that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training. Our implementation is publicly available at: https://github.com/sotopia-lab/sotopia-rl.', 'score': 15, 'issue_id': 5220, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '815208c1d75b8f05', 'authors': ['Haofei Yu', 'Zhengyang Qi', 'Yining Zhao', 'Kolby Nottingham', 'Keyang Xuan', 'Bodhisattwa Prasad Majumder', 'Hao Zhu', 'Paul Pu Liang', 'Jiaxuan You'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Carnegie Mellon University', 'Massachusetts Institute of Technology', 'Stanford University', 'University of California Irvine', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2508.03905.jpg', 'data': {'categories': ['#games', '#alignment', '#rlhf', '#rl'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Sotopia-RL: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ Ğ´Ğ»Ñ Ğ˜Ğ˜', 'desc': 'Sotopia-RL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Sotopia-RL Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Enhancing Social Intelligence in LLMs with Sotopia-RL', 'desc': "Sotopia-RL is a new reinforcement learning framework designed to improve social intelligence in large language models (LLMs). It addresses challenges in social interactions, such as partial observability and multi-dimensionality, by refining feedback into more precise utterance-level, multi-dimensional rewards. This approach allows for better credit assignment to individual utterances, enhancing the model's ability to learn from social interactions. Experiments show that Sotopia-RL significantly outperforms existing methods in achieving social goals, demonstrating its effectiveness in training socially intelligent agents."}, 'zh': {'title': 'æå‡ç¤¾äº¤æ™ºèƒ½çš„å¼ºåŒ–å­¦ä¹ æ–°æ¡†æ¶', 'desc': 'Sotopia-RLæ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¤¾äº¤æ™ºèƒ½ã€‚å®ƒé€šè¿‡å°†åé¦ˆç»†åŒ–ä¸ºå‘è¨€çº§åˆ«çš„å¤šç»´å¥–åŠ±ï¼Œæ¥æ”¹å–„æ¨¡å‹åœ¨ç¤¾äº¤ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç¤¾äº¤äº’åŠ¨ä¸­çš„éƒ¨åˆ†å¯è§‚å¯Ÿæ€§å’Œå¤šç»´æ€§é—®é¢˜ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å­¦ä¹ å¤æ‚çš„ç¤¾äº¤ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSotopia-RLåœ¨ç¤¾äº¤ç›®æ ‡å®Œæˆè¯„åˆ†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01858', 'title': 'Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web\n  Agents', 'url': 'https://huggingface.co/papers/2508.01858', 'abstract': 'A framework for web agents decomposes their capabilities into knowledge content learning and cognitive processes, using a structured dataset and a novel reasoning framework to enhance generalization and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent\'s capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent\'s processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the "what" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the "how" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent\'s conceptual grounding-the "nouns" upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner', 'score': 14, 'issue_id': 5220, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 3', 'zh': '8æœˆ3æ—¥'}, 'hash': 'ad30239b3abef884', 'authors': ['Yuhan Guo', 'Cong Guo', 'Aiwen Sun', 'Hongliang He', 'Xinyu Yang', 'Yue Lu', 'Yingji Zhang', 'Xuntao Guo', 'Dong Zhang', 'Jianzhuang Liu', 'Jiang Duan', 'Yijia Xiao', 'Liangjian Wen', 'Hai-Ming Xu', 'Yong Dai'], 'affiliations': ['Central South University', 'Fudan University', 'Harbin Institute of Technology', 'Hithink Research', 'Shanghai Jiao Tong University', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'Southwestern University of Finance and Economics', 'University of Adelaide', 'University of California, Los Angeles', 'University of Manchester', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01858.jpg', 'data': {'categories': ['#agents', '#benchmark', '#dataset', '#reasoning', '#open_source'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰ÑƒÑ Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Web-CogKnowledge Framework, ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğº Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ, ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ğµ. Ğ”Ğ»Ñ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Web-CogDataset Ğ¸Ğ· 14 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµĞ±-ÑĞ°Ğ¹Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Web-CogReasoner, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought), Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Empowering Web Agents through Structured Knowledge and Reasoning', 'desc': 'This paper presents a framework for web agents that separates their abilities into two main areas: learning knowledge content and performing cognitive processes. The authors introduce the Web-CogKnowledge Framework, which categorizes knowledge into Factual, Conceptual, and Procedural types, essential for effective reasoning. They also create the Web-CogDataset, a structured dataset from real-world websites to help agents learn necessary knowledge. The proposed Web-CogReasoner utilizes a novel Chain-of-Thought reasoning approach, demonstrating improved performance in generalizing to new tasks compared to existing models.'}, 'zh': {'title': 'æ™ºèƒ½ä½“èƒ½åŠ›çš„åŒé‡åˆ†è§£ï¼šçŸ¥è¯†ä¸è®¤çŸ¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç½‘ç»œæ™ºèƒ½ä½“çš„æ¡†æ¶ï¼Œå°†å…¶èƒ½åŠ›åˆ†è§£ä¸ºçŸ¥è¯†å†…å®¹å­¦ä¹ å’Œè®¤çŸ¥è¿‡ç¨‹ã€‚æˆ‘ä»¬å®šä¹‰äº†Web-CogKnowledgeæ¡†æ¶ï¼Œå°†çŸ¥è¯†åˆ†ä¸ºäº‹å®æ€§ã€æ¦‚å¿µæ€§å’Œç¨‹åºæ€§ä¸‰ç±»ï¼Œä»¥æ”¯æŒæ™ºèƒ½ä½“çš„å­¦ä¹ å’Œæ¨ç†ã€‚é€šè¿‡æ„å»ºWeb-CogDatasetï¼Œæˆ‘ä»¬ä¸ºæ™ºèƒ½ä½“æä¾›äº†ç³»ç»ŸåŒ–çš„çŸ¥è¯†åŸºç¡€ï¼Œå¸®åŠ©å…¶æŒæ¡å¿…è¦çš„æ ¸å¿ƒçŸ¥è¯†ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†Web-CogReasonerï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶åœ¨å¤„ç†æ–°ä»»åŠ¡æ—¶çš„ä¼˜è¶Šæ€§ï¼Œå°¤å…¶æ˜¯åœ¨ç»“æ„åŒ–çŸ¥è¯†è‡³å…³é‡è¦çš„æƒ…å†µä¸‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03560', 'title': 'LaTCoder: Converting Webpage Design to Code with Layout-as-Thought', 'url': 'https://huggingface.co/papers/2508.03560', 'abstract': 'LaTCoder enhances layout preservation in design-to-code tasks by dividing webpage designs into blocks and using Chain-of-Thought reasoning with MLLMs, achieving significant improvements in metrics and human preference.  \t\t\t\t\tAI-generated summary \t\t\t\t Converting webpage designs into code (design-to-code) plays a vital role in User Interface (UI) development for front-end developers, bridging the gap between visual design and functional implementation. While recent Multimodal Large Language Models (MLLMs) have shown significant potential in design-to-code tasks, they often fail to accurately preserve the layout during code generation. To this end, we draw inspiration from the Chain-of-Thought (CoT) reasoning in human cognition and propose LaTCoder, a novel approach that enhances layout preservation in webpage design during code generation with Layout-as-Thought (LaT). Specifically, we first introduce a simple yet efficient algorithm to divide the webpage design into image blocks. Next, we prompt MLLMs using a CoTbased approach to generate code for each block. Finally, we apply two assembly strategies-absolute positioning and an MLLM-based method-followed by dynamic selection to determine the optimal output. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs (i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly introduced, more challenging benchmark (CC-HARD) that features complex layouts. The experimental results on automatic metrics demonstrate significant improvements. Specifically, TreeBLEU scores increased by 66.67% and MAE decreased by 38% when using DeepSeek-VL2, compared to direct prompting. Moreover, the human preference evaluation results indicate that annotators favor the webpages generated by LaTCoder in over 60% of cases, providing strong evidence of the effectiveness of our method.', 'score': 12, 'issue_id': 5221, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '14b52fe9021b5b26', 'authors': ['Yi Gui', 'Zhen Li', 'Zhongyi Zhang', 'Guohao Wang', 'Tianpeng Lv', 'Gaoyang Jiang', 'Yi Liu', 'Dongping Chen', 'Yao Wan', 'Hongyu Zhang', 'Wenbin Jiang', 'Xuanhua Shi', 'Hai Jin'], 'affiliations': ['Chongqing University, Chongqing, China', 'Huazhong University of Science and Technology, Wuhan, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.03560.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#architecture', '#multimodal', '#optimization'], 'emoji': 'ğŸ§©', 'ru': {'title': 'LaTCoder: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ĞºĞµÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸Ğ· Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†', 'desc': 'LaTCoder - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ² ĞºĞ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°ĞºĞµÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ½Ğ° Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ (Chain-of-Thought) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). LaTCoder Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº TreeBLEU Ğ¸ MAE. Ğ’ Ñ…Ğ¾Ğ´Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ LaTCoder, Ğ±Ñ‹Ğ»Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ñ‹ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 60% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ².'}, 'en': {'title': 'Enhancing Layout Preservation in Design-to-Code with LaTCoder', 'desc': "LaTCoder is a novel approach designed to improve the accuracy of converting webpage designs into code while preserving the layout. It utilizes Chain-of-Thought reasoning to enhance the performance of Multimodal Large Language Models (MLLMs) by breaking down designs into manageable blocks. The method employs two assembly strategies to optimize the final output, ensuring that the generated code closely matches the intended design. Experimental results show significant improvements in both automatic metrics and human preference, indicating LaTCoder's effectiveness in design-to-code tasks."}, 'zh': {'title': 'æå‡ç½‘é¡µè®¾è®¡å¸ƒå±€ä¿ç•™çš„LaTCoder', 'desc': 'LaTCoderæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ç½‘é¡µè®¾è®¡åˆ°ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¸ƒå±€ä¿ç•™èƒ½åŠ›ã€‚å®ƒé€šè¿‡å°†ç½‘é¡µè®¾è®¡åˆ†å‰²æˆå¤šä¸ªå›¾åƒå—ï¼Œå¹¶ä½¿ç”¨åŸºäºæ€ç»´é“¾çš„æ¨ç†æ–¹æ³•æ¥ç”Ÿæˆæ¯ä¸ªå—çš„ä»£ç ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç»å¯¹å®šä½å’ŒåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç»„è£…ç­–ç•¥ï¼Œä»¥é€‰æ‹©æœ€ä½³è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaTCoderåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å’Œäººç±»åå¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23785', 'title': 'Gaussian Variation Field Diffusion for High-fidelity Video-to-4D\n  Synthesis', 'url': 'https://huggingface.co/papers/2507.23785', 'abstract': 'A novel framework uses a Direct 4DMesh-to-GS Variation Field VAE and Gaussian Variation Field diffusion model to generate high-quality dynamic 3D content from single video inputs, demonstrating superior quality and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: https://gvfdiffusion.github.io/.', 'score': 12, 'issue_id': 5220, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': '47c7686978b9c4dc', 'authors': ['Bowen Zhang', 'Sicheng Xu', 'Chuxin Wang', 'Jiaolong Yang', 'Feng Zhao', 'Dong Chen', 'Baining Guo'], 'affiliations': ['Microsoft Research Asia', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23785.jpg', 'data': {'categories': ['#video', '#synthetic', '#3d', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ VAE Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ğ½Ğ¾Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ»Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ¸Ğ· Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Objaverse. Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ²Ñ…Ğ¾Ğ´Ñ‹, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Transforming Videos into Dynamic 3D Worlds', 'desc': 'This paper introduces a new framework for generating dynamic 3D content from single video inputs using advanced machine learning techniques. It employs a Direct 4DMesh-to-GS Variation Field Variational Autoencoder (VAE) to efficiently encode 3D shapes and their motion without needing to fit each instance individually. The framework also incorporates a Gaussian Variation Field diffusion model that leverages a temporal-aware Diffusion Transformer, allowing it to conditionally generate high-quality animations based on input videos. The model shows impressive performance and generalization capabilities, even when trained on synthetic data, making it a significant advancement in video-to-4D generation.'}, 'zh': {'title': 'ä»è§†é¢‘ç”Ÿæˆé«˜è´¨é‡åŠ¨æ€3Då†…å®¹çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç”¨äºä»å•ä¸ªè§†é¢‘è¾“å…¥ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨æ€3Då†…å®¹ã€‚æˆ‘ä»¬å¼•å…¥äº†ç›´æ¥çš„4DMeshåˆ°é«˜æ–¯æ ·æ¡ï¼ˆGSï¼‰å˜åˆ†åœºå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œèƒ½å¤Ÿç›´æ¥ç¼–ç 3DåŠ¨ç”»æ•°æ®ä¸­çš„é«˜æ–¯æ ·æ¡åŠå…¶æ—¶é—´å˜åŒ–ã€‚é€šè¿‡è¿™ç§é«˜æ•ˆçš„è¡¨ç¤ºæ–¹å¼ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªåŸºäºæ—¶é—´æ„ŸçŸ¥çš„é«˜æ–¯å˜åˆ†åœºæ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥è§†é¢‘å’Œé«˜æ–¯æ ·æ¡ç”ŸæˆåŠ¨æ€å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆè´¨é‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨å¤„ç†çœŸå®è§†é¢‘è¾“å…¥æ—¶è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03789', 'title': 'HPSv3: Towards Wide-Spectrum Human Preference Score', 'url': 'https://huggingface.co/papers/2508.03789', 'abstract': 'HPSv3, a human preference score using a wide-spectrum dataset and uncertainty-aware ranking loss, enhances text-to-image generation quality through iterative refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating text-to-image generation models requires alignment with human perception, yet existing human-centric metrics are constrained by limited data coverage, suboptimal feature extraction, and inefficient loss functions. To address these challenges, we introduce Human Preference Score v3 (HPSv3). (1) We release HPDv3, the first wide-spectrum human preference dataset integrating 1.08M text-image pairs and 1.17M annotated pairwise comparisons from state-of-the-art generative models and low to high-quality real-world images. (2) We introduce a VLM-based preference model trained using an uncertainty-aware ranking loss for fine-grained ranking. Besides, we propose Chain-of-Human-Preference (CoHP), an iterative image refinement method that enhances quality without extra data, using HPSv3 to select the best image at each step. Extensive experiments demonstrate that HPSv3 serves as a robust metric for wide-spectrum image evaluation, and CoHP offers an efficient and human-aligned approach to improve image generation quality. The code and dataset are available at the HPSv3 Homepage.', 'score': 11, 'issue_id': 5220, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': 'a2a0678cfc88e0ef', 'authors': ['Yuhang Ma', 'Xiaoshi Wu', 'Keqiang Sun', 'Hongsheng Li'], 'affiliations': ['CPII, InnoHK', 'CUHK MMLab', 'Kings College London', 'Mizzen AI', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.03789.jpg', 'data': {'categories': ['#alignment', '#data', '#benchmark', '#dataset', '#cv', '#open_source'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'HPSv3: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'HPSv3 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ HPDv3, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 1.08 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ğ°Ñ€ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ 1.17 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ VLM, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Chain-of-Human-Preference Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Image Generation with Human Preference Score v3', 'desc': 'The paper presents Human Preference Score v3 (HPSv3), a new metric designed to improve the evaluation of text-to-image generation models by aligning them more closely with human preferences. It introduces a comprehensive dataset, HPDv3, containing over 1 million text-image pairs and annotated comparisons, which enhances the training of preference models. The authors also propose an uncertainty-aware ranking loss for fine-grained image ranking and a method called Chain-of-Human-Preference (CoHP) that iteratively refines images to boost quality without needing additional data. Through extensive testing, HPSv3 is shown to be a reliable metric for image evaluation, while CoHP effectively enhances image generation quality in a way that resonates with human judgment.'}, 'zh': {'title': 'æå‡å›¾åƒç”Ÿæˆè´¨é‡çš„HPSv3ä¸CoHPæ–¹æ³•', 'desc': 'HPSv3æ˜¯ä¸€ç§äººç±»åå¥½è¯„åˆ†ï¼Œåˆ©ç”¨å¹¿æ³›çš„æ•°æ®é›†å’Œè€ƒè™‘ä¸ç¡®å®šæ€§çš„æ’åæŸå¤±ï¼Œæå‡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„è´¨é‡ã€‚æˆ‘ä»¬å‘å¸ƒäº†HPDv3ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŒ…å«108ä¸‡å¯¹æ–‡æœ¬-å›¾åƒå’Œ117ä¸‡å¯¹æ ‡æ³¨æ¯”è¾ƒçš„å¹¿æ³›äººç±»åå¥½æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„åå¥½æ¨¡å‹ï¼Œä½¿ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„æ’åæŸå¤±è¿›è¡Œç»†è‡´æ’åã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†äººç±»åå¥½é“¾ï¼ˆCoHPï¼‰ï¼Œé€šè¿‡è¿­ä»£å›¾åƒä¼˜åŒ–ï¼Œåœ¨æ¯ä¸€æ­¥é€‰æ‹©æœ€ä½³å›¾åƒï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02215', 'title': 'LeanK: Learnable K Cache Channel Pruning for Efficient Decoding', 'url': 'https://huggingface.co/papers/2508.02215', 'abstract': 'LeanK, a learning-based method, prunes unimportant key cache channels in large language models to reduce memory usage and accelerate decoding without sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK.', 'score': 9, 'issue_id': 5220, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': 'caa8de613517d011', 'authors': ['Yike Zhang', 'Zhiyuan He', 'Huiqiang Jiang', 'Chengruidong Zhang', 'Yuqing Yang', 'Jianyong Wang', 'Lili Qiu'], 'affiliations': ['Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02215.jpg', 'data': {'categories': ['#training', '#long_context', '#inference', '#optimization'], 'emoji': 'ğŸ”ª', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¸Ğµ ĞºÑÑˆĞ° Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'LeanK - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑƒĞ´Ğ°Ğ»ÑÑ Ğ½ĞµĞ²Ğ°Ğ¶Ğ½Ñ‹Ğµ ĞºĞ°Ğ½Ğ°Ğ»Ñ‹ Ğ² ĞºÑÑˆĞµ ĞºĞ»ÑÑ‡ĞµĞ¹. LeanK Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ², ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ÑÑÑ‰ĞµĞ¹ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ´Ğ¾ 70% Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² 1,3 Ñ€Ğ°Ğ·Ğ°.'}, 'en': {'title': 'LeanK: Pruning for Efficient Language Model Performance', 'desc': 'LeanK is a novel method designed to enhance the efficiency of large language models by pruning unnecessary key cache channels. It utilizes a learning-based approach to identify and remove unimportant key (K) cache channels while maintaining model accuracy. The method employs a two-stage training process to create a static mask that meets specific sparsity and hardware requirements. As a result, LeanK achieves significant reductions in GPU memory usage and accelerates decoding times, demonstrating up to 70% reduction in K cache and a 1.3x speedup in attention computation.'}, 'zh': {'title': 'LeanKï¼šé«˜æ•ˆè§£ç çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–æ–¹æ¡ˆ', 'desc': 'LeanKæ˜¯ä¸€ç§åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä¸é‡è¦çš„å…³é”®ç¼“å­˜é€šé“ï¼Œä»è€Œé™ä½å†…å­˜ä½¿ç”¨å¹¶åŠ é€Ÿè§£ç ï¼ŒåŒæ—¶ä¸å½±å“å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é™æ€é€šé“ç¨€ç–æ€§ï¼Œé€šè¿‡ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œå­¦ä¹ æ»¡è¶³ç‰¹å®šç¨€ç–æ¯”å’Œç¡¬ä»¶å¯¹é½è¦æ±‚çš„é€šé“é™æ€æ©ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLeanKå¯ä»¥å‡å°‘é«˜è¾¾70%çš„Kç¼“å­˜å’Œ16%-18%çš„Vç¼“å­˜å†…å­˜ï¼Œå¹¶ä¸”è‡ªå®šä¹‰è§£ç å†…æ ¸ä½¿æ³¨æ„åŠ›è®¡ç®—é€Ÿåº¦æé«˜äº†1.3å€ã€‚é€šè¿‡åˆ†æå­¦ä¹ åˆ°çš„é‡è¦æ€§åˆ†å¸ƒï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å¯¹é•¿ä¸Šä¸‹æ–‡æ¨ç†è¿‡ç¨‹ä¸­æ¨¡å‹é€šé“å’Œæ³¨æ„åŠ›å¤´çš„æ·±å…¥è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.02807', 'title': 'DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a\n  Stage-Wise Diffusion Transformer Framework', 'url': 'https://huggingface.co/papers/2508.02807', 'abstract': 'DreamVVT, a two-stage framework using Diffusion Transformers and LoRA adapters, enhances video virtual try-on by leveraging unpaired human-centric data and pretrained models to preserve garment details and temporal consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Video virtual try-on (VVT) technology has garnered considerable academic interest owing to its promising applications in e-commerce advertising and entertainment. However, most existing end-to-end methods rely heavily on scarce paired garment-centric datasets and fail to effectively leverage priors of advanced visual models and test-time inputs, making it challenging to accurately preserve fine-grained garment details and maintain temporal consistency in unconstrained scenarios. To address these challenges, we propose DreamVVT, a carefully designed two-stage framework built upon Diffusion Transformers (DiTs), which is inherently capable of leveraging diverse unpaired human-centric data to enhance adaptability in real-world scenarios. To further leverage prior knowledge from pretrained models and test-time inputs, in the first stage, we sample representative frames from the input video and utilize a multi-frame try-on model integrated with a vision-language model (VLM), to synthesize high-fidelity and semantically consistent keyframe try-on images. These images serve as complementary appearance guidance for subsequent video generation. In the second stage, skeleton maps together with fine-grained motion and appearance descriptions are extracted from the input content, and these along with the keyframe try-on images are then fed into a pretrained video generation model enhanced with LoRA adapters. This ensures long-term temporal coherence for unseen regions and enables highly plausible dynamic motions. Extensive quantitative and qualitative experiments demonstrate that DreamVVT surpasses existing methods in preserving detailed garment content and temporal stability in real-world scenarios. Our project page https://virtu-lab.github.io/', 'score': 7, 'issue_id': 5224, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 4', 'zh': '8æœˆ4æ—¥'}, 'hash': 'f3f693bca2e57a94', 'authors': ['Tongchun Zuo', 'Zaiyu Huang', 'Shuliang Ning', 'Ente Lin', 'Chao Liang', 'Zerong Zheng', 'Jianwen Jiang', 'Yuan Zhang', 'Mingyuan Gao', 'Xin Dong'], 'affiliations': ['ByteDance Intelligent Creation', 'Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02807.jpg', 'data': {'categories': ['#video', '#cv', '#synthetic', '#multimodal', '#diffusion'], 'emoji': 'ğŸ‘š', 'ru': {'title': 'DreamVVT: Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ° Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'DreamVVT - ÑÑ‚Ğ¾ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¸ LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ¿Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¾Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ĞºĞ°Ğ´Ñ€Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Video Try-On with DreamVVT: Consistency Meets Detail', 'desc': 'DreamVVT is a two-stage framework designed to improve video virtual try-on (VVT) by using Diffusion Transformers and LoRA adapters. It effectively utilizes unpaired human-centric data and pretrained models to maintain garment details and ensure temporal consistency in videos. The first stage generates high-quality keyframe images using a multi-frame try-on model and a vision-language model, while the second stage focuses on video generation by incorporating motion and appearance data. This innovative approach allows DreamVVT to outperform existing methods in preserving garment fidelity and achieving smooth motion in dynamic scenarios.'}, 'zh': {'title': 'DreamVVTï¼šæå‡è§†é¢‘è™šæ‹Ÿè¯•ç©¿çš„åˆ›æ–°æ¡†æ¶', 'desc': 'DreamVVTæ˜¯ä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£å˜æ¢å™¨å’ŒLoRAé€‚é…å™¨ï¼Œæå‡è§†é¢‘è™šæ‹Ÿè¯•ç©¿æŠ€æœ¯ã€‚è¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨æœªé…å¯¹çš„äººä½“ä¸­å¿ƒæ•°æ®å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿ç•™æœè£…ç»†èŠ‚å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡å¤šå¸§è¯•ç©¿æ¨¡å‹ç”Ÿæˆé«˜ä¿çœŸå…³é”®å¸§å›¾åƒï¼Œç¬¬äºŒé˜¶æ®µåˆ™åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ç¡®ä¿åŠ¨æ€è¿åŠ¨çš„è¿è´¯æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDreamVVTåœ¨çœŸå®åœºæ™¯ä¸­ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°ä¿æŒæœè£…å†…å®¹çš„ç»†èŠ‚å’Œæ—¶é—´ç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04664', 'title': 'Sculptor: Empowering LLMs with Cognitive Agency via Active Context\n  Management', 'url': 'https://huggingface.co/papers/2508.04664', 'abstract': "Sculptor, a framework for Active Context Management, enhances LLM performance on long contexts by enabling proactive attention and memory control, reducing proactive interference and improving reasoning reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs' capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs' inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale.", 'score': 6, 'issue_id': 5220, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '1575b65bda95c5f9', 'authors': ['Mo Li', 'L. H. Xu', 'Qitai Tan', 'Ting Cao', 'Yunxin Liu'], 'affiliations': ['Independent Researcher', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04664.jpg', 'data': {'categories': ['#multimodal', '#long_context', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Sculptor: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Sculptor Ğ´Ğ»Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). Sculptor Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, ÑÑƒĞ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Sculptor Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ LLM Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Sculptor: Mastering Memory for Better Long-Context Reasoning', 'desc': 'The paper introduces Sculptor, a framework designed to enhance the performance of Large Language Models (LLMs) when dealing with long contexts. It addresses the issue of proactive interference, where irrelevant information disrupts reasoning and memory recall. Sculptor provides LLMs with Active Context Management (ACM) tools, allowing them to manage their internal memory more effectively by fragmenting context, summarizing information, and intelligently searching for relevant data. Experimental results show that Sculptor improves reasoning reliability and performance on long-context tasks without requiring additional training, emphasizing the importance of context-control strategies.'}, 'zh': {'title': 'ä¸»åŠ¨ä¸Šä¸‹æ–‡ç®¡ç†ï¼Œæå‡LLMæ€§èƒ½ï¼', 'desc': 'Sculptoræ˜¯ä¸€ä¸ªç”¨äºä¸»åŠ¨ä¸Šä¸‹æ–‡ç®¡ç†çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸»åŠ¨ç®¡ç†æ³¨æ„åŠ›å’Œå·¥ä½œè®°å¿†ï¼Œå‡å°‘äº†å‰æœŸä¿¡æ¯çš„å¹²æ‰°ï¼Œä»è€Œæ”¹å–„æ¨ç†çš„å¯é æ€§ã€‚Sculptoræä¾›äº†ä¸‰ç§å·¥å…·ï¼šä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–ã€æ‘˜è¦ã€éšè—ä¸æ¢å¤ï¼Œä»¥åŠæ™ºèƒ½æœç´¢ï¼Œå¸®åŠ©LLMæ›´æœ‰æ•ˆåœ°å¤„ç†ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSculptoråœ¨æ²¡æœ‰ç‰¹å®šè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¼ºè°ƒäº†ä¸»åŠ¨ä¸Šä¸‹æ–‡ç®¡ç†çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04586', 'title': 'Position: The Current AI Conference Model is Unsustainable! Diagnosing\n  the Crisis of Centralized AI Conference', 'url': 'https://huggingface.co/papers/2508.04586', 'abstract': 'The paper diagnoses structural issues in AI conferences, including publication rates, carbon footprint, negative community sentiment, and logistical challenges, and proposes a Community-Federated Conference model to address these issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Artificial Intelligence (AI) conferences are essential for advancing research, sharing knowledge, and fostering academic community. However, their rapid expansion has rendered the centralized conference model increasingly unsustainable. This paper offers a data-driven diagnosis of a structural crisis that threatens the foundational goals of scientific dissemination, equity, and community well-being. We identify four key areas of strain: (1) scientifically, with per-author publication rates more than doubling over the past decade to over 4.5 papers annually; (2) environmentally, with the carbon footprint of a single conference exceeding the daily emissions of its host city; (3) psychologically, with 71% of online community discourse reflecting negative sentiment and 35% referencing mental health concerns; and (4) logistically, with attendance at top conferences such as NeurIPS 2024 beginning to outpace venue capacity. These pressures point to a system that is misaligned with its core mission. In response, we propose the Community-Federated Conference (CFC) model, which separates peer review, presentation, and networking into globally coordinated but locally organized components, offering a more sustainable, inclusive, and resilient path forward for AI research.', 'score': 6, 'issue_id': 5220, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '0e9d3ff69536a24d', 'authors': ['Nuo Chen', 'Moming Duan', 'Andre Huikai Lin', 'Qian Wang', 'Jiaying Wu', 'Bingsheng He'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2508.04586.jpg', 'data': {'categories': ['#ethics', '#survey'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¹ Ğ˜Ğ˜: Ğ¾Ñ‚ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğº Ñ„ĞµĞ´ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ñƒ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ¾ÑÑ‚ Ñ‡Ğ¸ÑĞ»Ğ° Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹, ÑƒĞ³Ğ»ĞµÑ€Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ÑĞ»ĞµĞ´, Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ°Ğ¿Ñ€ÑĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ°ÑƒÑ‡Ğ½ÑƒÑ, ÑĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ, Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Community-Federated Conference (CFC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½ĞµÑ‚Ğ²Ğ¾Ñ€ĞºĞ¸Ğ½Ğ³ Ğ½Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ, Ğ½Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¹, Ğ¸Ğ½ĞºĞ»ÑĞ·Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ˜Ğ˜.'}, 'en': {'title': 'Towards Sustainable AI Conferences: A Community-Federated Approach', 'desc': 'This paper highlights critical issues facing AI conferences, such as unsustainable publication rates, high carbon emissions, negative community sentiment, and logistical challenges. It reveals that the number of papers published per author has significantly increased, leading to a strain on the scientific community. Additionally, the environmental impact of conferences is alarming, with emissions surpassing those of host cities. To address these challenges, the authors propose a Community-Federated Conference model that decentralizes the conference structure, enhancing sustainability and inclusivity in AI research.'}, 'zh': {'title': 'æ„å»ºå¯æŒç»­çš„äººå·¥æ™ºèƒ½ä¼šè®®æ–°æ¨¡å¼', 'desc': 'è¿™ç¯‡è®ºæ–‡è¯Šæ–­äº†äººå·¥æ™ºèƒ½ä¼šè®®çš„ç»“æ„æ€§é—®é¢˜ï¼ŒåŒ…æ‹¬å‘è¡¨ç‡ã€ç¢³è¶³è¿¹ã€è´Ÿé¢ç¤¾åŒºæƒ…ç»ªå’Œåå‹¤æŒ‘æˆ˜ã€‚éšç€ä¼šè®®æ•°é‡çš„å¿«é€Ÿå¢é•¿ï¼Œä¼ ç»Ÿçš„é›†ä¸­å¼ä¼šè®®æ¨¡å¼å˜å¾—è¶Šæ¥è¶Šä¸å¯æŒç»­ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç¤¾åŒºçš„è”åˆä¼šè®®æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œä¿ƒè¿›ç§‘å­¦ä¼ æ’­çš„å…¬å¹³æ€§å’Œç¤¾åŒºçš„ç¦ç¥‰ã€‚è¯¥æ¨¡å‹å°†åŒè¡Œè¯„å®¡ã€å±•ç¤ºå’Œç½‘ç»œäº¤æµåˆ†å¼€ï¼Œæä¾›äº†ä¸€ç§æ›´å¯æŒç»­ã€åŒ…å®¹å’Œæœ‰éŸ§æ€§çš„äººå·¥æ™ºèƒ½ç ”ç©¶å‘å±•è·¯å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01928', 'title': 'IAUNet: Instance-Aware U-Net', 'url': 'https://huggingface.co/papers/2508.01928', 'abstract': 'IAUNet, a query-based U-Net architecture with a lightweight convolutional Pixel decoder and Transformer decoder, outperforms state-of-the-art models in biomedical instance segmentation.  \t\t\t\t\tAI-generated summary \t\t\t\t Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often overlap and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been a go-to architecture in medical image segmentation, its potential in query-based approaches remains largely unexplored. In this work, we present IAUNet, a novel query-based U-Net architecture. The core design features a full U-Net architecture, enhanced by a novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose a Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource with detailed annotations of overlapping cell cytoplasm in brightfield images, setting a new benchmark for biomedical instance segmentation. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, transformer-based, and query-based models and cell segmentation-specific models, setting a strong baseline for cell instance segmentation tasks. Code is available at https://github.com/SlavkoPrytula/IAUNet', 'score': 5, 'issue_id': 5226, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 3', 'zh': '8æœˆ3æ—¥'}, 'hash': '0f9cff1ae25f175b', 'authors': ['Yaroslav Prytula', 'Illia Tsiporenko', 'Ali Zeynalli', 'Dmytro Fishman'], 'affiliations': ['Institute of Computer Science, University of Tartu', 'STACC U, Tartu, Estonia', 'Ukrainian Catholic University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01928.jpg', 'data': {'categories': ['#architecture', '#cv', '#games', '#benchmark', '#science', '#dataset', '#optimization', '#healthcare'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'IAUNet: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ»ĞµÑ‚Ğ¾Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹', 'desc': 'IAUNet - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ U-Net Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ¾Ğ² Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ ĞºĞ»ĞµÑ‚Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ»ĞµÑ‚Ğ¾Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½ĞµÑ‚ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ¼ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'IAUNet: Revolutionizing Biomedical Instance Segmentation with Query-Based U-Net', 'desc': 'IAUNet is a new architecture designed for biomedical instance segmentation, which helps in identifying and separating individual cells in images. It combines a traditional U-Net structure with a lightweight convolutional Pixel decoder and a Transformer decoder to enhance performance. This model efficiently reduces parameters while improving the segmentation of overlapping objects. The introduction of the 2025 Revvity Full Cell Segmentation Dataset provides a valuable resource for training and benchmarking segmentation models in this field.'}, 'zh': {'title': 'IAUNetï¼šç”Ÿç‰©åŒ»å­¦å®ä¾‹åˆ†å‰²çš„æ–°æ ‡æ†', 'desc': 'IAUNetæ˜¯ä¸€ç§åŸºäºæŸ¥è¯¢çš„U-Netæ¶æ„ï¼Œç»“åˆäº†è½»é‡çº§å·ç§¯åƒç´ è§£ç å™¨å’ŒTransformerè§£ç å™¨ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿç‰©åŒ»å­¦å®ä¾‹åˆ†å‰²ä¸­è¶…è¶Šç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚å®ä¾‹åˆ†å‰²åœ¨ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå¯ä»¥å‡†ç¡®åŒºåˆ†é‡å ä¸”å¤§å°ä¸ä¸€çš„ç»†èƒã€‚IAUNeté€šè¿‡å…¨æ–°çš„è½»é‡çº§å·ç§¯åƒç´ è§£ç å™¨æé«˜äº†æ¨¡å‹çš„æ•ˆç‡ï¼Œå¹¶å‡å°‘äº†å‚æ•°æ•°é‡ï¼ŒåŒæ—¶å¼•å…¥çš„Transformerè§£ç å™¨èƒ½å¤Ÿåœ¨å¤šä¸ªå°ºåº¦ä¸Šç»†åŒ–ç‰¹å®šå¯¹è±¡çš„ç‰¹å¾ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†2025 Revvityå…¨ç»†èƒåˆ†å‰²æ•°æ®é›†ï¼Œä¸ºç”Ÿç‰©åŒ»å­¦å®ä¾‹åˆ†å‰²è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04295', 'title': 'EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust\n  Translation', 'url': 'https://huggingface.co/papers/2508.04295', 'abstract': "EvoC2Rust is an automated framework that translates entire C projects to Rust using a skeleton-guided approach, combining rule-based and LLM-based methods to improve syntax, semantics, and safety.  \t\t\t\t\tAI-generated summary \t\t\t\t Rust's compile-time safety guarantees make it ideal for safety-critical systems, creating demand for translating legacy C codebases to Rust. While various approaches have emerged for this task, they face inherent trade-offs: rule-based solutions face challenges in meeting code safety and idiomaticity requirements, while LLM-based solutions often fail to generate semantically equivalent Rust code, due to the heavy dependencies of modules across the entire codebase. Recent studies have revealed that both solutions are limited to small-scale programs. In this paper, we propose EvoC2Rust, an automated framework for converting entire C projects to equivalent Rust ones. EvoC2Rust employs a skeleton-guided translation strategy for project-level translation. The pipeline consists of three evolutionary stages: 1) it first decomposes the C project into functional modules, employs a feature-mapping-enhanced LLM to transform definitions and macros and generates type-checked function stubs, which form a compilable Rust skeleton; 2) it then incrementally translates the function, replacing the corresponding stub placeholder; 3) finally, it repairs compilation errors by integrating LLM and static analysis. Through evolutionary augmentation, EvoC2Rust combines the advantages of both rule-based and LLM-based solutions. Our evaluation on open-source benchmarks and six industrial projects demonstrates EvoC2Rust's superior performance in project-level C-to-Rust translation. On average, it achieves 17.24% and 14.32% improvements in syntax and semantic accuracy over the LLM-based approaches, along with a 96.79% higher code safety rate than the rule-based tools. At the module level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates on industrial projects, even for complex codebases and long functions.", 'score': 4, 'issue_id': 5221, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '0dc09e7a8e2bad95', 'authors': ['Chaofan Wang', 'Tingrui Yu', 'Jie Wang', 'Dong Chen', 'Wenrui Zhang', 'Yuling Shi', 'Xiaodong Gu', 'Beijun Shen'], 'affiliations': ['Huawei Technologies Co., Ltd', 'Shanghai Jiao Tong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.04295.jpg', 'data': {'categories': ['#open_source', '#architecture', '#plp', '#optimization'], 'emoji': 'ğŸ”„', 'ru': {'title': 'EvoC2Rust: Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ñƒ C Ğ² Rust', 'desc': 'EvoC2Rust - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° C Ğ² Rust, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºĞµĞ»ĞµÑ‚Ğ° ĞºĞ¾Ğ´Ğ°. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸ÑĞ°, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°, Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸. ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ EvoC2Rust Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'EvoC2Rust: Bridging C to Rust with Safety and Precision', 'desc': 'EvoC2Rust is a novel framework designed to automate the translation of entire C projects into Rust, leveraging a skeleton-guided approach that integrates both rule-based and LLM-based techniques. This method addresses the limitations of existing solutions by ensuring that the translated code maintains both safety and idiomatic Rust syntax. The framework operates in three stages: it first breaks down the C project into modules, then translates functions incrementally, and finally resolves any compilation errors using a combination of LLM and static analysis. Evaluation results show that EvoC2Rust significantly outperforms previous methods in terms of syntax, semantic accuracy, and code safety, making it a robust solution for converting legacy C code to Rust.'}, 'zh': {'title': 'EvoC2Rustï¼šé«˜æ•ˆçš„Cåˆ°Rustè‡ªåŠ¨è½¬æ¢æ¡†æ¶', 'desc': 'EvoC2Rustæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨å°†æ•´ä¸ªCé¡¹ç›®è½¬æ¢ä¸ºRustä»£ç ã€‚å®ƒé‡‡ç”¨äº†éª¨æ¶å¼•å¯¼çš„æ–¹æ³•ï¼Œç»“åˆäº†åŸºäºè§„åˆ™å’ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ï¼Œä»¥æé«˜ä»£ç çš„è¯­æ³•ã€è¯­ä¹‰å’Œå®‰å…¨æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªè¿›åŒ–é˜¶æ®µè¿›è¡Œé¡¹ç›®çº§ç¿»è¯‘ï¼Œé¦–å…ˆå°†Cé¡¹ç›®åˆ†è§£ä¸ºåŠŸèƒ½æ¨¡å—ï¼Œç„¶åé€æ­¥ç¿»è¯‘å‡½æ•°ï¼Œæœ€åé€šè¿‡é›†æˆLLMå’Œé™æ€åˆ†æä¿®å¤ç¼–è¯‘é”™è¯¯ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒEvoC2Ruståœ¨Cåˆ°Rustçš„ç¿»è¯‘ä¸­è¡¨ç°ä¼˜è¶Šï¼Œè¯­æ³•å’Œè¯­ä¹‰å‡†ç¡®æ€§åˆ†åˆ«æé«˜äº†17.24%å’Œ14.32%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00222', 'title': 'RL-PLUS: Countering Capability Boundary Collapse of LLMs in\n  Reinforcement Learning with Hybrid-policy Optimization', 'url': 'https://huggingface.co/papers/2508.00222', 'abstract': "RL-PLUS, a hybrid-policy optimization approach, enhances LLM reasoning capabilities by integrating Multiple Importance Sampling and Exploration-Based Advantage Function, outperforming RLVR on various benchmarks and resolving capability boundary collapse.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its essentially on-policy strategy coupled with LLM's immense action space and sparse reward. Critically, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel hybrid-policy optimization approach for LLMs that synergizes internal exploitation with external data to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components, i.e., Multiple Importance Sampling to address distributional mismatch from external data, and Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. Compared with existing RLVR methods, RL-PLUS achieves 1) state-of-the-art performance on six math reasoning benchmarks; 2) superior performance on six out-of-distribution reasoning tasks; 3) consistent and significant gains across diverse model families, with average relative improvements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates that RL-PLUS effectively resolves the capability boundary collapse problem.", 'score': 4, 'issue_id': 5224, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': 'ec45fc053d8a3704', 'authors': ['Yihong Dong', 'Xue Jiang', 'Yongding Tao', 'Huanyu Liu', 'Kechi Zhang', 'Lili Mou', 'Rongyu Cao', 'Yingwei Ma', 'Jue Chen', 'Binhua Li', 'Zhi Jin', 'Fei Huang', 'Yongbin Li', 'Ge Li'], 'affiliations': ['Department of Computing Science, University of Alberta', 'School of Computer Science, Peking University', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2508.00222.jpg', 'data': {'categories': ['#training', '#benchmark', '#optimization', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'RL-PLUS: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RL-PLUS - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). RL-PLUS Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ RLVR Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. RL-PLUS Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½ÑƒÑ Ğ´Ğ»Ñ RLVR.'}, 'en': {'title': 'Breaking Boundaries in LLM Reasoning with RL-PLUS', 'desc': "The paper introduces RL-PLUS, a new hybrid-policy optimization method designed to improve the reasoning abilities of Large Language Models (LLMs). It combines Multiple Importance Sampling and Exploration-Based Advantage Function to enhance the model's performance beyond the limitations of traditional Reinforcement Learning with Verifiable Reward (RLVR). By addressing the issues of distributional mismatch and guiding exploration towards valuable reasoning paths, RL-PLUS effectively prevents capability boundary collapse. Extensive experiments show that RL-PLUS outperforms existing methods on various benchmarks, achieving significant improvements in reasoning tasks."}, 'zh': {'title': 'RL-PLUSï¼šçªç ´æ¨ç†èƒ½åŠ›è¾¹ç•Œçš„åˆ›æ–°æ–¹æ³•', 'desc': 'RL-PLUSæ˜¯ä¸€ç§æ··åˆç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡ç»“åˆå¤šé‡é‡è¦æ€§é‡‡æ ·å’ŒåŸºäºæ¢ç´¢çš„ä¼˜åŠ¿å‡½æ•°ï¼Œå…‹æœäº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚RLVRï¼‰åœ¨èƒ½åŠ›è¾¹ç•Œå´©æºƒæ–¹é¢çš„å±€é™ã€‚RL-PLUSèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¤–éƒ¨æ•°æ®ï¼ŒæŒ‡å¯¼æ¨¡å‹æ¢ç´¢é«˜ä»·å€¼çš„æ¨ç†è·¯å¾„ï¼Œä»è€Œå®ç°æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRL-PLUSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.21974', 'title': 'Reasoning Language Models for Root Cause Analysis in 5G Wireless\n  Networks', 'url': 'https://huggingface.co/papers/2507.21974', 'abstract': 'A lightweight framework using Large Language Models (LLMs) with TeleLogs dataset and a two-stage training methodology improves Root Cause Analysis (RCA) in mobile networks by enhancing interpretability and reasoning quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Root Cause Analysis (RCA) in mobile networks remains a challenging task due to the need for interpretability, domain expertise, and causal reasoning. In this work, we propose a lightweight framework that leverages Large Language Models (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of annotated troubleshooting problems designed to benchmark RCA capabilities. Our evaluation reveals that existing open-source reasoning LLMs struggle with these problems, underscoring the need for domain-specific adaptation. To address this issue, we propose a two-stage training methodology that combines supervised fine-tuning with reinforcement learning to improve the accuracy and reasoning quality of LLMs. The proposed approach fine-tunes a series of RCA models to integrate domain knowledge and generate structured, multi-step diagnostic explanations, improving both interpretability and effectiveness. Extensive experiments across multiple LLM sizes show significant performance gains over state-of-the-art reasoning and non-reasoning models, including strong generalization to randomized test variants. These results demonstrate the promise of domain-adapted, reasoning-enhanced LLMs for practical and explainable RCA in network operation and management.', 'score': 4, 'issue_id': 5227, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 Ğ¸ÑĞ»Ñ', 'en': 'July 29', 'zh': '7æœˆ29æ—¥'}, 'hash': '91b127bf8dd7d610', 'authors': ['Mohamed Sana', 'Nicola Piovesan', 'Antonio De Domenico', 'Yibin Kang', 'Haozhe Zhang', 'Merouane Debbah', 'Fadhel Ayed'], 'affiliations': ['Huawei Technologies, China', 'Khalifa University of Science and Technology, Abu Dhabi, UAE', 'Paris Research Center, Huawei Technologies, Boulogne-Billancourt, France'], 'pdf_title_img': 'assets/pdf/title_img/2507.21974.jpg', 'data': {'categories': ['#reasoning', '#rl', '#interpretability', '#dataset', '#training', '#open_source', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½ Ğ² Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… TeleLogs Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½ (RCA) Ğ² Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ±ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing RCA in Mobile Networks with Domain-Specific LLMs', 'desc': 'This paper presents a lightweight framework that utilizes Large Language Models (LLMs) to enhance Root Cause Analysis (RCA) in mobile networks. It introduces the TeleLogs dataset, which contains annotated troubleshooting problems to evaluate RCA capabilities effectively. The authors propose a two-stage training methodology that combines supervised fine-tuning and reinforcement learning to improve the reasoning quality and interpretability of LLMs. Experimental results show that this approach significantly outperforms existing models, demonstrating the potential of domain-adapted LLMs for practical RCA applications.'}, 'zh': {'title': 'é¢†åŸŸé€‚åº”ä¸æ¨ç†å¢å¼ºçš„æ ¹æœ¬åŸå› åˆ†ææ–°æ–¹æ³•', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è½»é‡çº§æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥æ”¹å–„ç§»åŠ¨ç½‘ç»œä¸­çš„æ ¹æœ¬åŸå› åˆ†æï¼ˆRCAï¼‰ã€‚æˆ‘ä»¬å¼•å…¥äº†TeleLogsæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡æ³¨é‡Šçš„æ•…éšœæ’é™¤é—®é¢˜é›†åˆï¼Œæ—¨åœ¨è¯„ä¼°RCAèƒ½åŠ›ã€‚é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œç»“åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œæå‡äº†LLMsçš„å‡†ç¡®æ€§å’Œæ¨ç†è´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç»è¿‡é¢†åŸŸé€‚åº”çš„LLMsåœ¨ç½‘ç»œæ“ä½œå’Œç®¡ç†ä¸­çš„RCAä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01197', 'title': 'A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding', 'url': 'https://huggingface.co/papers/2508.01197', 'abstract': 'A benchmark and model for 3D occupancy grounding using natural language and voxel-level annotations improve object perception in autonomous driving.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual grounding aims to identify objects or regions in a scene based on natural language descriptions, essential for spatially aware perception in autonomous driving. However, existing visual grounding tasks typically depend on bounding boxes that often fail to capture fine-grained details. Not all voxels within a bounding box are occupied, resulting in inaccurate object representations. To address this, we introduce a benchmark for 3D occupancy grounding in challenging outdoor scenes. Built on the nuScenes dataset, it integrates natural language with voxel-level occupancy annotations, offering more precise object perception compared to the traditional grounding task. Moreover, we propose GroundingOcc, an end-to-end model designed for 3D occupancy grounding through multi-modal learning. It combines visual, textual, and point cloud features to predict object location and occupancy information from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder for feature extraction, an occupancy head for voxel-wise predictions, and a grounding head to refine localization. Additionally, a 2D grounding module and a depth estimation module enhance geometric understanding, thereby boosting model performance. Extensive experiments on the benchmark demonstrate that our method outperforms existing baselines on 3D occupancy grounding. The dataset is available at https://github.com/RONINGOD/GroundingOcc.', 'score': 3, 'issue_id': 5220, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 2', 'zh': '8æœˆ2æ—¥'}, 'hash': 'd7be41190836a7cc', 'authors': ['Zhan Shi', 'Song Wang', 'Junbo Chen', 'Jianke Zhu'], 'affiliations': ['College of Computer Science, Zhejiang University, Hangzhou 310027, China', 'College of Software Technology, Zhejiang University', 'Udeer.ai, Hangzhou 310000, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.01197.jpg', 'data': {'categories': ['#multimodal', '#games', '#benchmark', '#3d', '#optimization'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 3D Ğ³Ñ€ÑƒĞ½Ñ‚Ğ¾Ğ²ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ 3D Ğ³Ñ€ÑƒĞ½Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GroundingOcc, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ 2D Ğ³Ñ€ÑƒĞ½Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ 3D Ğ³Ñ€ÑƒĞ½Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ½ÑÑ‚Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Object Perception with 3D Occupancy Grounding', 'desc': 'This paper presents a new approach to 3D occupancy grounding, which is crucial for improving object perception in autonomous driving. It introduces a benchmark that uses voxel-level annotations combined with natural language descriptions, allowing for more accurate identification of objects in complex outdoor environments. The proposed model, GroundingOcc, employs multi-modal learning to integrate visual, textual, and point cloud data, enhancing the precision of object localization and occupancy predictions. Experimental results show that this method significantly outperforms existing techniques in the field, demonstrating its effectiveness in real-world applications.'}, 'zh': {'title': 'æå‡è‡ªåŠ¨é©¾é©¶ç‰©ä½“æ„ŸçŸ¥çš„3Då ç”¨åŸºç¡€è§†è§‰å®šä½', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†å’Œæ¨¡å‹ï¼Œç”¨äºé€šè¿‡è‡ªç„¶è¯­è¨€å’Œä½“ç´ çº§æ³¨é‡Šè¿›è¡Œ3Då ç”¨åŸºç¡€çš„è§†è§‰å®šä½ï¼Œæ—¨åœ¨æé«˜è‡ªåŠ¨é©¾é©¶ä¸­çš„ç‰©ä½“æ„ŸçŸ¥èƒ½åŠ›ã€‚ç°æœ‰çš„è§†è§‰å®šä½ä»»åŠ¡é€šå¸¸ä¾èµ–äºè¾¹ç•Œæ¡†ï¼Œè¿™ç§æ–¹æ³•æ— æ³•æ•æ‰åˆ°ç»†ç²’åº¦çš„ç»†èŠ‚ï¼Œå¯¼è‡´ç‰©ä½“è¡¨ç¤ºä¸å‡†ç¡®ã€‚æˆ‘ä»¬å¼•å…¥çš„GroundingOccæ¨¡å‹é€šè¿‡å¤šæ¨¡æ€å­¦ä¹ ï¼Œç»“åˆè§†è§‰ã€æ–‡æœ¬å’Œç‚¹äº‘ç‰¹å¾ï¼Œä»ç²—åˆ°ç»†åœ°é¢„æµ‹ç‰©ä½“ä½ç½®å’Œå ç”¨ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨3Då ç”¨åŸºç¡€çš„è§†è§‰å®šä½ä»»åŠ¡ä¸­ä¼˜äºç°æœ‰çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04632', 'title': 'IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with\n  Verifiable Rewards', 'url': 'https://huggingface.co/papers/2508.04632', 'abstract': 'Instruction Following Decorator enhances RLVR by improving sample efficiency, intent alignment, and reducing reward hacking in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction following capabilities of large language models (LLMs), but suffers from training inefficiency due to inadequate difficulty assessment. Moreover, RLVR is prone to over-optimization, where LLMs exploit verification shortcuts without aligning to the actual intent of user instructions. We introduce Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR training into a robust and sample-efficient pipeline. It consists of three components: (1) a cooperative-adversarial data flywheel that co-evolves instructions and hybrid verifications, generating progressively more challenging instruction-verification pairs; (2) IntentCheck, a bypass module enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that detects reward hacking via trap instructions, which trigger and capture shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves 87.43% accuracy on IFEval, outperforming larger proprietary models such as GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench while preserving general capabilities. Our trip wires show significant reductions in reward hacking rates. We will release models, code, and data for future research.', 'score': 2, 'issue_id': 5228, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '1e91bd1d8a3387f3', 'authors': ['Xu Guo', 'Tianyi Liang', 'Tong Jian', 'Xiaogui Yang', 'Ling-I Wu', 'Chenhui Li', 'Zhihui Lu', 'Qipeng Guo', 'Kai Chen'], 'affiliations': ['East China Normal University', 'Fudan University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2508.04632.jpg', 'data': {'categories': ['#rlhf', '#open_source', '#alignment', '#training', '#optimization', '#security', '#rl'], 'emoji': 'ğŸ“', 'ru': {'title': 'IFDecorator: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Instruction Following Decorator (IFDecorator) - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. IFDecorator Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾-ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ÑÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ IFDecorator.'}, 'en': {'title': 'Enhancing Instruction Following with Robust Reinforcement Learning', 'desc': 'The paper presents the Instruction Following Decorator (IFDecorator), a framework designed to enhance Reinforcement Learning with Verifiable Rewards (RLVR) for large language models (LLMs). IFDecorator improves sample efficiency by using a cooperative-adversarial data flywheel that generates increasingly difficult instruction-verification pairs. It also includes IntentCheck, which ensures that the model aligns with user intent, and trip wires that detect and mitigate reward hacking behaviors. The results show that the Qwen2.5-32B-Instruct-IFDecorator model achieves high accuracy and reduces reward hacking, outperforming larger models like GPT-4o.'}, 'zh': {'title': 'æå‡æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºæŒ‡ä»¤è·Ÿéšè£…é¥°å™¨ï¼ˆIFDecoratorï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä¸‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸‰ä¸ªä¸»è¦ç»„ä»¶æ¥å¢å¼ºæ ·æœ¬æ•ˆç‡å’Œæ„å›¾å¯¹é½ï¼Œå‡å°‘å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚é¦–å…ˆï¼Œå®ƒåˆ©ç”¨åˆä½œå¯¹æŠ—çš„æ•°æ®é£è½®ç”Ÿæˆè¶Šæ¥è¶Šå…·æŒ‘æˆ˜æ€§çš„æŒ‡ä»¤-éªŒè¯å¯¹ï¼›å…¶æ¬¡ï¼ŒIntentCheckæ¨¡å—ç¡®ä¿æ¨¡å‹çš„æ„å›¾å¯¹é½ï¼›æœ€åï¼Œtrip wiresæœºåˆ¶æ£€æµ‹å¹¶æ•æ‰å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨IFDecoratorçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡å’Œæ•´ä½“èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04010', 'title': 'HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive\n  Policy Enhancement and Dual-Objective Optimization', 'url': 'https://huggingface.co/papers/2508.04010', 'abstract': 'HarmonyGuard is a multi-agent framework that enhances policy compliance and task completion in web environments by adaptively updating security policies and optimizing dual objectives of safety and utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models enable agents to autonomously perform tasks in open web environments. However, as hidden threats within the web evolve, web agents face the challenge of balancing task performance with emerging risks during long-sequence operations. Although this challenge is critical, current research remains limited to single-objective optimization or single-turn scenarios, lacking the capability for collaborative optimization of both safety and utility in web environments. To address this gap, we propose HarmonyGuard, a multi-agent collaborative framework that leverages policy enhancement and objective optimization to jointly improve both utility and safety. HarmonyGuard features a multi-agent architecture characterized by two fundamental capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent within HarmonyGuard, which automatically extracts and maintains structured security policies from unstructured external documents, while continuously updating policies in response to evolving threats. (2) Dual-Objective Optimization: Based on the dual objectives of safety and utility, the Utility Agent integrated within HarmonyGuard performs the Markovian real-time reasoning to evaluate the objectives and utilizes metacognitive capabilities for their optimization. Extensive evaluations on multiple benchmarks show that HarmonyGuard improves policy compliance by up to 38% and task completion by up to 20% over existing baselines, while achieving over 90% policy compliance across all tasks. Our project is available here: https://github.com/YurunChen/HarmonyGuard.', 'score': 2, 'issue_id': 5232, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '9a431a1adec12655', 'authors': ['Yurun Chen', 'Xavier Hu', 'Yuhan Liu', 'Keting Yin', 'Juncheng Li', 'Zhuosheng Zhang', 'Shengyu Zhang'], 'affiliations': ['Shanghai Jiao Tong University', 'Xiamen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04010.jpg', 'data': {'categories': ['#optimization', '#security', '#benchmark', '#architecture', '#agents'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ“Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ²ĞµĞ±-ÑÑ€ĞµĞ´Ğµ', 'desc': 'HarmonyGuard - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ†ĞµĞ»Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ°ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Balancing Safety and Utility in Web Tasks with HarmonyGuard', 'desc': 'HarmonyGuard is a multi-agent framework designed to improve how web agents comply with security policies while completing tasks. It addresses the challenge of balancing safety and utility in dynamic web environments by adaptively updating security policies. The framework includes a Policy Agent that extracts and updates security policies from external documents and a Utility Agent that optimizes task performance based on safety and utility objectives. Evaluations show that HarmonyGuard significantly enhances policy compliance and task completion compared to existing methods.'}, 'zh': {'title': 'HarmonyGuardï¼šå®‰å…¨ä¸æ•ˆç”¨çš„åŒé‡ä¼˜åŒ–', 'desc': 'HarmonyGuardæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªé€‚åº”æ›´æ–°å®‰å…¨ç­–ç•¥å’Œä¼˜åŒ–å®‰å…¨æ€§ä¸æ•ˆç”¨çš„åŒé‡ç›®æ ‡ï¼Œå¢å¼ºç½‘ç»œç¯å¢ƒä¸­çš„æ”¿ç­–åˆè§„æ€§å’Œä»»åŠ¡å®Œæˆåº¦ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†æ”¿ç­–ä»£ç†ï¼Œèƒ½å¤Ÿä»éç»“æ„åŒ–æ–‡æ¡£ä¸­æå–å’Œç»´æŠ¤ç»“æ„åŒ–çš„å®‰å…¨æ”¿ç­–ï¼Œå¹¶æ ¹æ®ä¸æ–­å˜åŒ–çš„å¨èƒæŒç»­æ›´æ–°è¿™äº›æ”¿ç­–ã€‚åŒæ—¶ï¼Œæ•ˆç”¨ä»£ç†é€šè¿‡é©¬å°”å¯å¤«å®æ—¶æ¨ç†è¯„ä¼°å®‰å…¨æ€§å’Œæ•ˆç”¨çš„åŒé‡ç›®æ ‡ï¼Œå¹¶åˆ©ç”¨å…ƒè®¤çŸ¥èƒ½åŠ›è¿›è¡Œä¼˜åŒ–ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒHarmonyGuardåœ¨æ”¿ç­–åˆè§„æ€§å’Œä»»åŠ¡å®Œæˆåº¦æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæ”¿ç­–åˆè§„æ€§è¶…è¿‡90%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01778', 'title': 'DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving\n  via Online HD Map Diffusion', 'url': 'https://huggingface.co/papers/2508.01778', 'abstract': 'DiffSemanticFusion enhances autonomous driving by fusing semantic raster and graph-based representations using a map diffusion module, improving trajectory prediction and end-to-end driving performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous driving requires accurate scene understanding, including road geometry, traffic agents, and their semantic relationships. In online HD map generation scenarios, raster-based representations are well-suited to vision models but lack geometric precision, while graph-based representations retain structural detail but become unstable without precise maps. To harness the complementary strengths of both, we propose DiffSemanticFusion -- a fusion framework for multimodal trajectory prediction and planning. Our approach reasons over a semantic raster-fused BEV space, enhanced by a map diffusion module that improves both the stability and expressiveness of online HD map representations. We validate our framework on two downstream tasks: trajectory prediction and planning-oriented end-to-end autonomous driving. Experiments on real-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrate improved performance over several state-of-the-art methods. For the prediction task on nuScenes, we integrate DiffSemanticFusion with the online HD map informed QCNet, achieving a 5.1\\% performance improvement. For end-to-end autonomous driving in NAVSIM, DiffSemanticFusion achieves state-of-the-art results, with a 15\\% performance gain in NavHard scenarios. In addition, extensive ablation and sensitivity studies show that our map diffusion module can be seamlessly integrated into other vector-based approaches to enhance performance. All artifacts are available at https://github.com/SunZhigang7/DiffSemanticFusion.', 'score': 2, 'issue_id': 5232, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 3', 'zh': '8æœˆ3æ—¥'}, 'hash': 'd5f701aadba28943', 'authors': ['Zhigang Sun', 'Yiru Wang', 'Anqing Jiang', 'Shuo Wang', 'Yu Gao', 'Yuwen Heng', 'Shouyi Zhang', 'An He', 'Hao Jiang', 'Jinhao Chai', 'Zichong Gu', 'Wang Jijun', 'Shichen Tang', 'Lavdim Halilaj', 'Juergen Luettin', 'Hao Sun'], 'affiliations': ['AIR, Tsinghua University, Beijing, China', 'Bosch Corporate Research, Bosch (China) Investment Ltd., Shanghai, China', 'Robert Bosch GmbH', 'School of Communication and Information Engineering, Shanghai University, Shanghai, China', 'Shanghai Jiaotong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.01778.jpg', 'data': {'categories': ['#video', '#optimization', '#games', '#benchmark', '#multimodal', '#agents'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ¡Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ°', 'desc': 'DiffSemanticFusion - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ ĞºĞ°Ñ€Ñ‚Ñ‹. ĞĞ½ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ HD-ĞºĞ°Ñ€Ñ‚, Ğ³Ğ´Ğµ Ğ¾Ğ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Fusing Data for Smarter Autonomous Driving', 'desc': 'DiffSemanticFusion is a novel framework designed to improve autonomous driving by combining two types of data representations: semantic raster and graph-based models. The framework uses a map diffusion module to enhance the stability and detail of high-definition maps, which are crucial for accurate trajectory prediction and driving performance. By integrating these representations, DiffSemanticFusion leverages their strengths to provide better scene understanding and decision-making for autonomous vehicles. Experiments show that this approach significantly outperforms existing methods in real-world driving scenarios, demonstrating its effectiveness in both trajectory prediction and end-to-end driving tasks.'}, 'zh': {'title': 'èåˆè¯­ä¹‰ä¸å›¾å½¢ï¼Œæå‡è‡ªä¸»é©¾é©¶æ€§èƒ½', 'desc': 'DiffSemanticFusion æ˜¯ä¸€ç§å¢å¼ºè‡ªä¸»é©¾é©¶çš„æ¡†æ¶ï¼Œé€šè¿‡èåˆè¯­ä¹‰æ …æ ¼å’ŒåŸºäºå›¾çš„è¡¨ç¤ºï¼Œåˆ©ç”¨åœ°å›¾æ‰©æ•£æ¨¡å—æ¥æé«˜è½¨è¿¹é¢„æµ‹å’Œç«¯åˆ°ç«¯é©¾é©¶æ€§èƒ½ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ …æ ¼è¡¨ç¤ºçš„è§†è§‰æ¨¡å‹ä¼˜åŠ¿å’Œå›¾è¡¨ç¤ºçš„ç»“æ„ç»†èŠ‚ï¼Œè§£å†³äº†åœ¨çº¿é«˜æ¸…åœ°å›¾ç”Ÿæˆä¸­çš„ä¸ç¨³å®šæ€§é—®é¢˜ã€‚æˆ‘ä»¬åœ¨çœŸå®ä¸–ç•Œçš„è‡ªä¸»é©¾é©¶åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†è¯¥æ¡†æ¶ï¼Œç»“æœæ˜¾ç¤ºåœ¨è½¨è¿¹é¢„æµ‹å’Œè§„åˆ’ä»»åŠ¡ä¸Šå‡ä¼˜äºå¤šç§å…ˆè¿›æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒDiffSemanticFusion åœ¨å¤šä¸ªåœºæ™¯ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„è‡ªä¸»é©¾é©¶è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01630', 'title': 'OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers\n  for Biomedical NER Across 12 Public Datasets', 'url': 'https://huggingface.co/papers/2508.01630', 'abstract': 'OpenMed NER, a suite of open-source transformer models using DAPT and LoRA, achieves state-of-the-art performance on diverse biomedical NER benchmarks with high efficiency and low computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Named-entity recognition (NER) is fundamental to extracting structured information from the >80% of healthcare data that resides in unstructured clinical notes and biomedical literature. Despite recent advances with large language models, achieving state-of-the-art performance across diverse entity types while maintaining computational efficiency remains a significant challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted transformer models that combine lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced, publicly available research repositories and de-identified clinical notes (PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA backbones. This is followed by task-specific fine-tuning with LoRA, which updates less than 1.5% of model parameters. We evaluate our models on 12 established biomedical NER benchmarks spanning chemicals, diseases, genes, and species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of these 12 datasets, with substantial gains across diverse entity types. Our models advance the state-of-the-art on foundational disease and chemical benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger improvements of over 5.3 and 9.7 percentage points on more specialized gene and clinical cell line corpora. This work demonstrates that strategically adapted open-source models can surpass closed-source solutions. This performance is achieved with remarkable efficiency: training completes in under 12 hours on a single GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively licensed, open-source checkpoints designed to help practitioners facilitate compliance with emerging data protection and AI regulations, such as the EU AI Act.', 'score': 2, 'issue_id': 5222, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 3', 'zh': '8æœˆ3æ—¥'}, 'hash': 'aca28561e07e250a', 'authors': ['Maziyar Panahi'], 'affiliations': ['CNRS Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2508.01630.jpg', 'data': {'categories': ['#healthcare', '#ethics', '#dataset', '#training', '#transfer_learning', '#open_source', '#benchmark', '#data'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'OpenMed NER: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'OpenMed NER - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ DAPT Ğ¸ LoRA Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 10 Ğ¸Ğ· 12 ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²ĞµÑ‰ĞµÑÑ‚Ğ²Ğ°, Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ñ, Ğ³ĞµĞ½Ñ‹ Ğ¸ Ğ²Ğ¸Ğ´Ñ‹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ - Ğ¼ĞµĞ½ĞµĞµ 12 Ñ‡Ğ°ÑĞ¾Ğ² Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ ÑƒĞ³Ğ»ĞµÑ€Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ÑĞ»ĞµĞ´Ğ¾Ğ¼. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ NER.'}, 'en': {'title': 'OpenMed NER: Efficiently Transforming Biomedical NER with Open-Source Innovation', 'desc': 'OpenMed NER is a collection of open-source transformer models designed for named-entity recognition (NER) in the biomedical field. It utilizes domain-adaptive pre-training (DAPT) and Low-Rank Adaptation (LoRA) to achieve high performance while being computationally efficient. The models were trained on a large dataset of clinical notes and research papers, and they excelled in identifying various biomedical entities, outperforming existing models on multiple benchmarks. This work highlights the potential of open-source solutions to achieve superior results compared to proprietary models, all while maintaining a low environmental impact.'}, 'zh': {'title': 'OpenMed NERï¼šé«˜æ•ˆçš„ç”Ÿç‰©åŒ»å­¦å‘½åå®ä½“è¯†åˆ«è§£å†³æ–¹æ¡ˆ', 'desc': 'OpenMed NER æ˜¯ä¸€å¥—å¼€æºçš„å˜æ¢å™¨æ¨¡å‹ï¼Œç»“åˆäº†è½»é‡çº§çš„é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼ˆDAPTï¼‰å’Œå‚æ•°é«˜æ•ˆçš„ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ï¼Œåœ¨ç”Ÿç‰©åŒ»å­¦å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥æ¨¡å‹åœ¨350,000æ®µæ¥è‡ªä¼¦ç†æ¥æºçš„ä¸´åºŠç¬”è®°å’Œç ”ç©¶æ–‡çŒ®çš„è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿé«˜æ•ˆæå–åŒ»ç–—æ•°æ®ä¸­çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚OpenMed NER åœ¨12ä¸ªç”Ÿç‰©åŒ»å­¦NERåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†10ä¸ªæ•°æ®é›†çš„æ–°æœ€ä¼˜å¾®F1åˆ†æ•°ï¼Œå°¤å…¶åœ¨ç–¾ç—…å’ŒåŒ–å­¦åŸºå‡†ä¸Šæœ‰æ˜¾è‘—æå‡ã€‚è¯¥æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡é«˜ï¼Œå•ä¸ªGPUä¸‹è®­ç»ƒæ—¶é—´å°‘äº12å°æ—¶ï¼Œä¸”ç¢³è¶³è¿¹ä½äº1.2å…¬æ–¤CO2eï¼Œé€‚åˆå¸®åŠ©ä»ä¸šè€…éµå®ˆæ•°æ®ä¿æŠ¤å’Œäººå·¥æ™ºèƒ½æ³•è§„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00599', 'title': 'DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior', 'url': 'https://huggingface.co/papers/2508.00599', 'abstract': "DPoser-X, a diffusion-based model, addresses the complexity of 3D human poses using variational diffusion sampling and a novel truncated timestep scheduling method, outperforming existing models across various pose benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present DPoser-X, a diffusion-based prior model for 3D whole-body human poses. Building a versatile and robust full-body human pose prior remains challenging due to the inherent complexity of articulated human poses and the scarcity of high-quality whole-body pose datasets. To address these limitations, we introduce a Diffusion model as body Pose prior (DPoser) and extend it to DPoser-X for expressive whole-body human pose modeling. Our approach unifies various pose-centric tasks as inverse problems, solving them through variational diffusion sampling. To enhance performance on downstream applications, we introduce a novel truncated timestep scheduling method specifically designed for pose data characteristics. We also propose a masked training mechanism that effectively combines whole-body and part-specific datasets, enabling our model to capture interdependencies between body parts while avoiding overfitting to specific actions. Extensive experiments demonstrate DPoser-X's robustness and versatility across multiple benchmarks for body, hand, face, and full-body pose modeling. Our model consistently outperforms state-of-the-art alternatives, establishing a new benchmark for whole-body human pose prior modeling.", 'score': 2, 'issue_id': 5235, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 1', 'zh': '8æœˆ1æ—¥'}, 'hash': '52b3dd3f31a8d796', 'authors': ['Junzhe Lu', 'Jing Lin', 'Hongkun Dou', 'Ailing Zeng', 'Yue Deng', 'Xian Liu', 'Zhongang Cai', 'Lei Yang', 'Yulun Zhang', 'Haoqian Wang', 'Ziwei Liu'], 'affiliations': ['Beihang University', 'Independent Researcher', 'NVIDIA Research', 'Nanyang Technological University', 'SenseTime Research', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00599.jpg', 'data': {'categories': ['#benchmark', '#3d', '#diffusion'], 'emoji': 'ğŸ•º', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'DPoser-X - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ·Ğ°Ğ¼Ğ¸, ĞºĞ°Ğº Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¸Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. DPoser-X Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ·.'}, 'en': {'title': 'Revolutionizing 3D Human Pose Generation with DPoser-X', 'desc': 'DPoser-X is a diffusion-based model designed to improve the generation of 3D human poses by utilizing variational diffusion sampling. It addresses the challenges of modeling complex articulated poses and the lack of high-quality datasets by introducing a novel truncated timestep scheduling method tailored for pose data. The model treats various pose-related tasks as inverse problems, allowing it to effectively learn from both whole-body and part-specific datasets. Extensive testing shows that DPoser-X outperforms existing models, setting a new standard for whole-body human pose modeling.'}, 'zh': {'title': 'DPoser-Xï¼šçªç ´3Däººä½“å§¿æ€å»ºæ¨¡çš„æé™', 'desc': 'DPoser-Xæ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³3Däººä½“å§¿æ€å»ºæ¨¡çš„å¤æ‚æ€§ã€‚å®ƒé€šè¿‡å˜åˆ†æ‰©æ•£é‡‡æ ·å’Œæ–°é¢–çš„æˆªæ–­æ—¶é—´è°ƒåº¦æ–¹æ³•ï¼Œæå‡äº†åœ¨å„ç§å§¿æ€åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹å°†å¤šç§å§¿æ€ç›¸å…³ä»»åŠ¡ç»Ÿä¸€ä¸ºé€†é—®é¢˜ï¼Œé€šè¿‡å˜åˆ†æ‰©æ•£é‡‡æ ·è¿›è¡Œæ±‚è§£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDPoser-Xåœ¨å…¨èº«ã€æ‰‹éƒ¨ã€é¢éƒ¨å’Œå…¨èº«å§¿æ€å»ºæ¨¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03983', 'title': 'MiDashengLM: Efficient Audio Understanding with General Audio Captions', 'url': 'https://huggingface.co/papers/2508.03983', 'abstract': 'MiDashengLM is an open audio-language model using general audio captions for efficient and comprehensive audio understanding, offering faster processing and higher throughput compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Current approaches for large audio language models (LALMs) often rely on closed data sources or proprietary models, limiting their generalization and accessibility. This paper introduces MiDashengLM, a novel open audio-language model designed for efficient and comprehensive audio understanding through the use of general audio captions using our novel ACAVCaps training dataset. MiDashengLM exclusively relies on publicly available pretraining and supervised fine-tuning (SFT) datasets, ensuring full transparency and reproducibility. At its core, MiDashengLM integrates Dasheng, an open-source audio encoder, specifically engineered to process diverse auditory information effectively. Unlike previous works primarily focused on Automatic Speech Recognition (ASR) based audio-text alignment, our strategy centers on general audio captions, fusing speech, sound and music information into one textual representation, enabling a holistic textual representation of complex audio scenes. Lastly, MiDashengLM provides an up to 4x speedup in terms of time-to-first-token (TTFT) and up to 20x higher throughput than comparable models. Checkpoints are available online at https://huggingface.co/mispeech/midashenglm-7b and https://github.com/xiaomi-research/dasheng-lm.', 'score': 1, 'issue_id': 5227, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '6455762dc1eab458', 'authors': ['Heinrich Dinkel', 'Gang Li', 'Jizhong Liu', 'Jian Luan', 'Yadong Niu', 'Xingwei Sun', 'Tianzi Wang', 'Qiyang Xiao', 'Junbo Zhang', 'Jiahao Zhou'], 'affiliations': ['Horizon Team, MiLM Plus Xiaomi Inc., China'], 'pdf_title_img': 'assets/pdf/title_img/2508.03983.jpg', 'data': {'categories': ['#audio', '#dataset', '#open_source'], 'emoji': 'ğŸ§', 'ru': {'title': 'MiDashengLM: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ°', 'desc': 'MiDashengLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Dasheng, Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚, ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (ASR), ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ MiDashengLM Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑÑ…, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ€ĞµÑ‡Ğ¸, Ğ·Ğ²ÑƒĞºĞµ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞµ Ğ² Ğ¾Ğ´Ğ½Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ. MiDashengLM Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 4 Ñ€Ğ°Ğ· Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° (TTFT) Ğ¸ Ğ´Ğ¾ 20 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Audio Understanding with MiDashengLM', 'desc': 'MiDashengLM is an innovative open audio-language model that enhances audio understanding by utilizing general audio captions. It leverages a unique training dataset called ACAVCaps, which is built from publicly available data, ensuring transparency and reproducibility in its development. Unlike traditional models that focus mainly on Automatic Speech Recognition (ASR), MiDashengLM integrates various audio elements like speech, sound, and music into a unified textual representation. This model significantly improves processing speed, achieving up to 4 times faster time-to-first-token and up to 20 times higher throughput compared to existing models.'}, 'zh': {'title': 'é«˜æ•ˆéŸ³é¢‘ç†è§£çš„å¼€æ”¾æ¨¡å‹', 'desc': 'MiDashengLMæ˜¯ä¸€ç§å¼€æ”¾çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ä½¿ç”¨é€šç”¨éŸ³é¢‘æ ‡é¢˜å®ç°é«˜æ•ˆå’Œå…¨é¢çš„éŸ³é¢‘ç†è§£ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒæä¾›äº†æ›´å¿«çš„å¤„ç†é€Ÿåº¦å’Œæ›´é«˜çš„ååé‡ã€‚è¯¥æ¨¡å‹ä¾èµ–äºå…¬å¼€å¯ç”¨çš„é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒæ•°æ®é›†ï¼Œç¡®ä¿äº†é€æ˜æ€§å’Œå¯é‡å¤æ€§ã€‚MiDashengLMå°†è¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹ä¿¡æ¯èåˆä¸ºä¸€ä¸ªæ–‡æœ¬è¡¨ç¤ºï¼Œèƒ½å¤Ÿå…¨é¢æè¿°å¤æ‚çš„éŸ³é¢‘åœºæ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03448', 'title': 'SonicMaster: Towards Controllable All-in-One Music Restoration and\n  Mastering', 'url': 'https://huggingface.co/papers/2508.03448', 'abstract': "SonicMaster, a unified generative model, improves music audio quality by addressing various artifacts using text-based control and a flow-matching generative training paradigm.  \t\t\t\t\tAI-generated summary \t\t\t\t Music recordings often suffer from audio quality issues such as excessive reverberation, distortion, clipping, tonal imbalances, and a narrowed stereo image, especially when created in non-professional settings without specialized equipment or expertise. These problems are typically corrected using separate specialized tools and manual adjustments. In this paper, we introduce SonicMaster, the first unified generative model for music restoration and mastering that addresses a broad spectrum of audio artifacts with text-based control. SonicMaster is conditioned on natural language instructions to apply targeted enhancements, or can operate in an automatic mode for general restoration. To train this model, we construct the SonicMaster dataset, a large dataset of paired degraded and high-quality tracks by simulating common degradation types with nineteen degradation functions belonging to five enhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our approach leverages a flow-matching generative training paradigm to learn an audio transformation that maps degraded inputs to their cleaned, mastered versions guided by text prompts. Objective audio quality metrics demonstrate that SonicMaster significantly improves sound quality across all artifact categories. Furthermore, subjective listening tests confirm that listeners prefer SonicMaster's enhanced outputs over the original degraded audio, highlighting the effectiveness of our unified approach.", 'score': 1, 'issue_id': 5223, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': '534674700c0af141', 'authors': ['Jan Melechovsky', 'Ambuj Mehrish', 'Dorien Herremans'], 'affiliations': ['Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2508.03448.jpg', 'data': {'categories': ['#audio', '#dataset', '#synthetic'], 'emoji': 'ğŸµ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'SonicMaster - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ² Ğ¼ÑƒĞ·Ñ‹ĞºĞµ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. SonicMaster Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ¿Ğ°Ñ€Ñ‹ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞºĞ¾Ğ².'}, 'en': {'title': 'SonicMaster: Revolutionizing Music Restoration with AI', 'desc': 'SonicMaster is a novel generative model designed to enhance music audio quality by correcting various audio artifacts. It utilizes text-based control to allow users to specify desired improvements, making it versatile for both targeted and automatic restoration. The model is trained on a large dataset that pairs degraded audio tracks with their high-quality counterparts, using a flow-matching generative training approach. Results show that SonicMaster significantly enhances sound quality, as confirmed by both objective metrics and subjective listener preferences.'}, 'zh': {'title': 'SonicMasterï¼šéŸ³ä¹éŸ³é¢‘è´¨é‡çš„ç»Ÿä¸€ç”Ÿæˆæ¨¡å‹', 'desc': 'SonicMasteræ˜¯ä¸€ç§ç»Ÿä¸€çš„ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡æ–‡æœ¬æ§åˆ¶å’ŒæµåŒ¹é…ç”Ÿæˆè®­ç»ƒèŒƒå¼æ¥æ”¹å–„éŸ³ä¹éŸ³é¢‘è´¨é‡ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤šç§éŸ³é¢‘ä¼ªå½±ï¼Œå¦‚æ··å“è¿‡åº¦ã€å¤±çœŸå’ŒéŸ³è°ƒä¸å¹³è¡¡ç­‰ï¼Œå°¤å…¶é€‚ç”¨äºéä¸“ä¸šç¯å¢ƒä¸‹å½•åˆ¶çš„éŸ³ä¹ã€‚SonicMasteré€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¢å¼ºï¼Œæˆ–åœ¨è‡ªåŠ¨æ¨¡å¼ä¸‹è¿›è¡Œä¸€èˆ¬ä¿®å¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSonicMasteråœ¨æ‰€æœ‰ä¼ªå½±ç±»åˆ«ä¸­æ˜¾è‘—æé«˜äº†éŸ³è´¨ï¼Œä¸”å¬ä¼—æ›´å–œæ¬¢å…¶å¢å¼ºçš„è¾“å‡ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.01311', 'title': 'C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with\n  Learnable Advisor', 'url': 'https://huggingface.co/papers/2508.01311', 'abstract': 'A continual learning framework for 3D anomaly detection uses Kernel Attention mechanisms and parameter perturbation to handle multiple and emerging classes of point clouds.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D Anomaly Detection (AD) has shown great potential in detecting anomalies or defects of high-precision industrial products. However, existing methods are typically trained in a class-specific manner and also lack the capability of learning from emerging classes. In this study, we proposed a continual learning framework named Continual 3D Anomaly Detection (C3D-AD), which can not only learn generalized representations for multi-class point clouds but also handle new classes emerging over time.Specifically, in the feature extraction module, to extract generalized local features from diverse product types of different tasks efficiently, Kernel Attention with random feature Layer (KAL) is introduced, which normalizes the feature space. Then, to reconstruct data correctly and continually, an efficient Kernel Attention with learnable Advisor (KAA) mechanism is proposed, which learns the information from new categories while discarding redundant old information within both the encoder and decoder. Finally, to keep the representation consistency over tasks, a Reconstruction with Parameter Perturbation (RPP) module is proposed by designing a representation rehearsal loss function, which ensures that the model remembers previous category information and returns category-adaptive representation.Extensive experiments on three public datasets demonstrate the effectiveness of the proposed method, achieving an average performance of 66.4%, 83.1%, and 63.4% AUROC on Real3D-AD, Anomaly-ShapeNet, and MulSen-AD, respectively.', 'score': 1, 'issue_id': 5230, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 2', 'zh': '8æœˆ2æ—¥'}, 'hash': 'af26fb044fbca284', 'authors': ['Haoquan Lu', 'Hanzhe Liang', 'Jie Zhang', 'Chenxi Hu', 'Jinbao Wang', 'Can Gao'], 'affiliations': ['College of Computer Science and Software Engineering, Shenzhen University', 'Faculty of Applied Sciences, Macao Polytechnic University', 'Guangdong Provincial Key Laboratory of Intelligent Information Processing', 'Ningbo Institute of Digital Twin, Eastern Institute of Technology', 'School of Artificial Intelligence, Shenzhen University', 'Shenzhen Audencia Financial Technology Institute, Shenzhen University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01311.jpg', 'data': {'categories': ['#training', '#3d', '#dataset', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ 3D-Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹: Ğ¾Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ»Ğ°ÑÑĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ĞºĞ»Ğ°ÑÑĞ°Ğ¼', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ C3D-AD Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ² 3D-Ğ¾Ğ±Ğ»Ğ°ĞºĞ°Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ´ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ»Ğ°ÑÑÑ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Adapting to Anomalies: Continual Learning in 3D Detection', 'desc': 'This paper presents a continual learning framework called Continual 3D Anomaly Detection (C3D-AD) designed for detecting anomalies in 3D point clouds. It addresses the limitations of traditional methods that are class-specific and unable to adapt to new classes over time. The framework utilizes Kernel Attention mechanisms to efficiently extract generalized features and incorporates a parameter perturbation strategy to maintain representation consistency across tasks. Experimental results on multiple datasets show that C3D-AD significantly improves anomaly detection performance while adapting to emerging classes.'}, 'zh': {'title': 'æŒç»­å­¦ä¹ ï¼Œæ™ºèƒ½æ£€æµ‹3Då¼‚å¸¸', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºC3D-ADçš„æŒç»­å­¦ä¹ æ¡†æ¶ï¼Œç”¨äº3Då¼‚å¸¸æ£€æµ‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ ¸æ³¨æ„åŠ›æœºåˆ¶å’Œå‚æ•°æ‰°åŠ¨ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç±»ç‚¹äº‘æ•°æ®å¹¶é€‚åº”æ–°å‡ºç°çš„ç±»åˆ«ã€‚é€šè¿‡å¼•å…¥éšæœºç‰¹å¾å±‚çš„æ ¸æ³¨æ„åŠ›ï¼Œæ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆæå–å¤šæ ·åŒ–äº§å“ç±»å‹çš„ç‰¹å¾ã€‚åŒæ—¶ï¼Œé‡å»ºæ¨¡å—é€šè¿‡å­¦ä¹ æ–°ç±»åˆ«çš„ä¿¡æ¯ï¼Œç¡®ä¿æ¨¡å‹åœ¨å¤„ç†æ–°ä»»åŠ¡æ—¶ä»èƒ½ä¿æŒå¯¹æ—§ç±»åˆ«çš„è®°å¿†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00428', 'title': 'Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D\n  Generation', 'url': 'https://huggingface.co/papers/2508.00428', 'abstract': 'Sel3DCraft enhances text-to-3D generation through a dual-branch retrieval and generation system, multi-view hybrid scoring with MLLMs, and prompt-driven visual analytics, improving designer creativity.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-3D (T23D) generation has transformed digital content creation, yet remains bottlenecked by blind trial-and-error prompting processes that yield unpredictable results. While visual prompt engineering has advanced in text-to-image domains, its application to 3D generation presents unique challenges requiring multi-view consistency evaluation and spatial understanding. We present Sel3DCraft, a visual prompt engineering system for T23D that transforms unstructured exploration into a guided visual process. Our approach introduces three key innovations: a dual-branch structure combining retrieval and generation for diverse candidate exploration; a multi-view hybrid scoring approach that leverages MLLMs with innovative high-level metrics to assess 3D models with human-expert consistency; and a prompt-driven visual analytics suite that enables intuitive defect identification and refinement. Extensive testing and user studies demonstrate that Sel3DCraft surpasses other T23D systems in supporting creativity for designers.', 'score': 1, 'issue_id': 5229, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 1', 'zh': '8æœˆ1æ—¥'}, 'hash': '7c82937efeb350cb', 'authors': ['Nan Xiang', 'Tianyi Liang', 'Haiwen Huang', 'Shiqi Jiang', 'Hao Huang', 'Yifei Huang', 'Liangyu Chen', 'Changbo Wang', 'Chenhui Li'], 'affiliations': ['East China Normal University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00428.jpg', 'data': {'categories': ['#games', '#3d', '#optimization', '#multimodal'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Sel3DCraft: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Sel3DCraft - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² 3D, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğ¹ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. Sel3DCraft Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ´ĞµÑ„ĞµĞºÑ‚Ñ‹.'}, 'en': {'title': 'Transforming Text-to-3D Generation for Enhanced Creativity', 'desc': 'Sel3DCraft is a novel system designed to improve text-to-3D (T23D) generation by integrating a dual-branch approach that combines retrieval and generation methods. This system addresses the challenges of 3D model creation by implementing multi-view hybrid scoring, which utilizes machine learning language models (MLLMs) to ensure consistency and quality in the generated outputs. Additionally, Sel3DCraft features a prompt-driven visual analytics tool that helps designers identify and refine defects in their 3D models more intuitively. Overall, extensive testing shows that Sel3DCraft significantly enhances the creative process for designers compared to existing T23D systems.'}, 'zh': {'title': 'æå‡è®¾è®¡å¸ˆåˆ›é€ åŠ›çš„ä¸‰ç»´ç”Ÿæˆç³»ç»Ÿ', 'desc': 'Sel3DCraft æ˜¯ä¸€ä¸ªå¢å¼ºæ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆçš„ç³»ç»Ÿï¼Œé‡‡ç”¨åŒåˆ†æ”¯æ£€ç´¢å’Œç”Ÿæˆç»“æ„ï¼Œæå‡è®¾è®¡å¸ˆçš„åˆ›é€ åŠ›ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¤šè§†è§’æ··åˆè¯„åˆ†å’Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¥è¯„ä¼°ä¸‰ç»´æ¨¡å‹çš„ä¸€è‡´æ€§ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„ç›²ç›®è¯•é”™é—®é¢˜ã€‚å®ƒè¿˜å¼•å…¥äº†åŸºäºæç¤ºçš„è§†è§‰åˆ†æå·¥å…·ï¼Œå¸®åŠ©ç”¨æˆ·ç›´è§‚åœ°è¯†åˆ«å’Œæ”¹è¿›ç¼ºé™·ã€‚ç»è¿‡å¹¿æ³›æµ‹è¯•ï¼ŒSel3DCraft åœ¨æ”¯æŒè®¾è®¡å¸ˆåˆ›é€ åŠ›æ–¹é¢è¡¨ç°ä¼˜äºå…¶ä»–æ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.23313', 'title': 'The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in\n  Text-to-Image Models', 'url': 'https://huggingface.co/papers/2507.23313', 'abstract': 'Transformer-based text-to-image diffusion models show varying degrees of content-style separation in generated artworks, as revealed by cross-attention heatmaps.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models have demonstrated remarkable capabilities in generating artistic content by learning from billions of images, including popular artworks. However, the fundamental question of how these models internally represent concepts, such as content and style in paintings, remains unexplored. Traditional computer vision assumes content and style are orthogonal, but diffusion models receive no explicit guidance about this distinction during training. In this work, we investigate how transformer-based text-to-image diffusion models encode content and style concepts when generating artworks. We leverage cross-attention heatmaps to attribute pixels in generated images to specific prompt tokens, enabling us to isolate image regions influenced by content-describing versus style-describing tokens. Our findings reveal that diffusion models demonstrate varying degrees of content-style separation depending on the specific artistic prompt and style requested. In many cases, content tokens primarily influence object-related regions while style tokens affect background and texture areas, suggesting an emergent understanding of the content-style distinction. These insights contribute to our understanding of how large-scale generative models internally represent complex artistic concepts without explicit supervision. We share the code and dataset, together with an exploratory tool for visualizing attention maps at https://github.com/umilISLab/artistic-prompt-interpretation.', 'score': 1, 'issue_id': 5221, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': 'c584e9c932383ec6', 'authors': ['Alfio Ferrara', 'Sergio Picascia', 'Elisabetta Rocchetti'], 'affiliations': ['Department of Computer Science, UniversitÃ  degli Studi di Milano, Via Celoria, 18, 20133 Milan, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2507.23313.jpg', 'data': {'categories': ['#dataset', '#open_source', '#multimodal', '#interpretability', '#cv', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°: ĞºĞ°Ğº Ğ˜Ğ˜ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑÑ‚Ğ¸Ğ»ÑŒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸ÑÑ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ğ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»ÑÑ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞ¿Ğ»Ğ¾Ğ²Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ½ĞµÑÑ‚Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑÑ‚Ğ¸Ğ»Ñ Ğ²Ğ¾Ğ·Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ½Ğ° Ñ„Ğ¾Ğ½ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹. Ğ­Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ¼ Ğ±ĞµĞ· ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¼Ñƒ.'}, 'en': {'title': 'Decoding Art: Understanding Content and Style in AI-Generated Images', 'desc': 'This paper explores how transformer-based text-to-image diffusion models generate artworks by analyzing their internal representation of content and style. Using cross-attention heatmaps, the authors investigate how different prompt tokens influence specific regions of generated images, revealing a separation between content and style. The study finds that content tokens mainly affect object-related areas, while style tokens influence backgrounds and textures, indicating an emergent understanding of these concepts. This research enhances our knowledge of generative models and their ability to represent complex artistic ideas without direct supervision.'}, 'zh': {'title': 'æ¢ç´¢å†…å®¹ä¸é£æ ¼çš„åˆ†ç¦»ï¼šæ‰©æ•£æ¨¡å‹çš„è‰ºæœ¯ç”Ÿæˆ', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºå˜æ¢å™¨çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè‰ºæœ¯ä½œå“æ—¶å¦‚ä½•ç¼–ç å†…å®¹å’Œé£æ ¼çš„æ¦‚å¿µã€‚é€šè¿‡äº¤å‰æ³¨æ„åŠ›çƒ­å›¾ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå°†ç”Ÿæˆå›¾åƒä¸­çš„åƒç´ å½’å› äºç‰¹å®šçš„æç¤ºä»¤ç‰Œï¼Œä»è€ŒåŒºåˆ†å—å†…å®¹æè¿°å’Œé£æ ¼æè¿°å½±å“çš„å›¾åƒåŒºåŸŸã€‚ç ”ç©¶å‘ç°ï¼Œæ‰©æ•£æ¨¡å‹åœ¨ä¸åŒè‰ºæœ¯æç¤ºå’Œé£æ ¼è¯·æ±‚ä¸‹è¡¨ç°å‡ºä¸åŒç¨‹åº¦çš„å†…å®¹ä¸é£æ ¼åˆ†ç¦»ã€‚ç»“æœè¡¨æ˜ï¼Œå†…å®¹ä»¤ç‰Œä¸»è¦å½±å“ä¸ç‰©ä½“ç›¸å…³çš„åŒºåŸŸï¼Œè€Œé£æ ¼ä»¤ç‰Œåˆ™å½±å“èƒŒæ™¯å’Œçº¹ç†åŒºåŸŸï¼Œæ˜¾ç¤ºå‡ºæ¨¡å‹å¯¹å†…å®¹ä¸é£æ ¼åŒºåˆ†çš„ç†è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.04440', 'title': 'StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs\n  through Knowledge-Reasoning Fusion', 'url': 'https://huggingface.co/papers/2508.04440', 'abstract': 'ThinkingF, a data synthesis and training pipeline, enhances autoformalization by improving formal knowledge and informal-to-formal reasoning, achieving state-of-the-art results in formalization tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoformalization aims to translate natural-language mathematical statements into a formal language. While LLMs have accelerated progress in this area, existing methods still suffer from low accuracy. We identify two key abilities for effective autoformalization: comprehensive mastery of formal-language domain knowledge, and reasoning capability of natural language problem understanding and informal-formal alignment. Without the former, a model cannot identify the correct formal objects; without the latter, it struggles to interpret real-world contexts and map them precisely into formal expressions. To address these gaps, we introduce ThinkingF, a data synthesis and training pipeline that improves both abilities. First, we construct two datasets: one by distilling and selecting large-scale examples rich in formal knowledge, and another by generating informal-to-formal reasoning trajectories guided by expert-designed templates. We then apply SFT and RLVR with these datasets to further fuse and refine the two abilities. The resulting 7B and 32B models exhibit both comprehensive formal knowledge and strong informal-to-formal reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5% on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior general-purpose and specialized models.', 'score': 0, 'issue_id': 5230, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 6', 'zh': '8æœˆ6æ—¥'}, 'hash': '6d2a57f491626d3a', 'authors': ['Yutong Wu', 'Di Huang', 'Ruosi Wan', 'Yue Peng', 'Shijie Shang', 'Chenrui Cao', 'Lei Qi', 'Rui Zhang', 'Zidong Du', 'Jie Yan', 'Xing Hu'], 'affiliations': ['SKL of Processors, Institute of Computing Technology, CAS', 'StepFun Inc.', 'University of Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.04440.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#reasoning', '#data', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ThinkingF: Ğ¼Ğ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¾Ğ¹', 'desc': 'ThinkingF - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğº Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²ÑƒÑ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: Ğ¾Ğ´Ğ¸Ğ½ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸, Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğ¼Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸, Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğº Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ (SFT) Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RLVR) Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… FormalMATH-Lite Ğ¸ ProverBench.'}, 'en': {'title': 'ThinkingF: Bridging Natural Language and Formal Knowledge', 'desc': 'The paper presents ThinkingF, a novel data synthesis and training pipeline designed to enhance autoformalization, which is the process of converting natural-language mathematical statements into formal language. It identifies two critical skills necessary for effective autoformalization: mastery of formal-language knowledge and the ability to reason from informal to formal contexts. To improve these skills, ThinkingF constructs two specialized datasets and employs techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning with Value Regression (RLVR). The resulting models, particularly StepFun-Formalizer-32B, achieve state-of-the-art performance on formalization tasks, demonstrating significant advancements over previous models.'}, 'zh': {'title': 'æå‡è‡ªåŠ¨å½¢å¼åŒ–çš„ThinkingF', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ThinkingFï¼Œä¸€ä¸ªæ•°æ®åˆæˆå’Œè®­ç»ƒç®¡é“ï¼Œæ—¨åœ¨æå‡è‡ªåŠ¨å½¢å¼åŒ–çš„èƒ½åŠ›ã€‚è‡ªåŠ¨å½¢å¼åŒ–çš„ç›®æ ‡æ˜¯å°†è‡ªç„¶è¯­è¨€æ•°å­¦é™ˆè¿°è½¬æ¢ä¸ºå½¢å¼è¯­è¨€ã€‚ç ”ç©¶å‘ç°ï¼ŒæˆåŠŸçš„è‡ªåŠ¨å½¢å¼åŒ–éœ€è¦å¯¹å½¢å¼è¯­è¨€é¢†åŸŸçŸ¥è¯†çš„å…¨é¢æŒæ¡å’Œè‡ªç„¶è¯­è¨€é—®é¢˜ç†è§£çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºä¸¤ä¸ªæ•°æ®é›†å¹¶åº”ç”¨SFTå’ŒRLVRï¼ŒThinkingFæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å½¢å¼åŒ–ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨FormalMATH-Liteå’ŒProverBenchä¸Šå–å¾—äº†é¢†å…ˆçš„æˆç»©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.03178', 'title': 'Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and\n  Self-Checking for Complex Instruction Following', 'url': 'https://huggingface.co/papers/2508.03178', 'abstract': 'A framework using entropy-preserving supervised fine-tuning and token-wise entropy-adaptive reinforcement learning improves instruction adherence in LLMs by fostering rigorous reasoning processes.  \t\t\t\t\tAI-generated summary \t\t\t\t While advancements in the reasoning abilities of LLMs have significantly enhanced their performance in solving mathematical problems, coding tasks, and general puzzles, their effectiveness in accurately adhering to instructions remains inconsistent, particularly with more complex directives. Our investigation identifies lazy reasoning during the thinking stage as the primary factor contributing to poor instruction adherence. To mitigate this issue, we propose a comprehensive framework designed to enable rigorous reasoning processes involving preview and self-checking, essential for satisfying strict instruction constraints. Specifically, we first generate instructions with complex constraints and apply a filtering process to obtain valid prompts, resulting in three distinct prompt datasets categorized as hard, easy, and pass. Then, we employ rejection sampling on the pass prompts to curate a small yet high-quality dataset, enabling a cold-start initialization of the model and facilitating its adaptation to effective reasoning patterns. Subsequently, we employ an entropy-preserving supervised fine-tuning (Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL) reinforcement learning guided by rule-based dense rewards. This approach encourages the model to transform its reasoning mechanism, ultimately fostering generalizable reasoning abilities that encompass preview and self-checking. Extensive experiments conducted on instruction-following benchmarks demonstrate remarkable performance improvements across various model scales. Notably, our Light-IF-32B model surpasses both larger open-source models such as DeepSeek-R1 and closed-source models like Doubao-1.6.', 'score': 0, 'issue_id': 5229, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 5', 'zh': '8æœˆ5æ—¥'}, 'hash': 'b68fc5d4f6ed55e1', 'authors': ['Chenyang Wang', 'Liang Wen', 'Shousheng Jia', 'Xiangzheng Zhang', 'Liang Xu'], 'affiliations': ['CLUE', 'Harbin Institute of Technology', 'Qiyuan Tech'], 'pdf_title_img': 'assets/pdf/title_img/2508.03178.jpg', 'data': {'categories': ['#optimization', '#rl', '#dataset', '#benchmark', '#training', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾-ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰ĞµĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ´Ğ»Ñ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Instruction Adherence in LLMs through Rigorous Reasoning', 'desc': 'This paper presents a new framework that enhances the ability of large language models (LLMs) to follow complex instructions by improving their reasoning processes. The authors identify that poor instruction adherence is often due to lazy reasoning, particularly during the initial thinking phase. To address this, they introduce an entropy-preserving supervised fine-tuning method and a token-wise entropy-adaptive reinforcement learning strategy, which together promote rigorous reasoning through self-checking and previewing. Their experiments show that this approach significantly boosts performance on instruction-following tasks, outperforming both larger open-source and closed-source models.'}, 'zh': {'title': 'æå‡æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„æ¨ç†æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œé€šè¿‡ä¿æŒç†µçš„ç›‘ç£å¾®è°ƒå’Œé€è¯ç†µè‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ ï¼Œæ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹æŒ‡ä»¤çš„éµå¾ªèƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œæ‡’æƒ°æ¨ç†æ˜¯å¯¼è‡´æŒ‡ä»¤éµå¾ªä¸ä½³çš„ä¸»è¦åŸå› ï¼Œå› æ­¤æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼Œä¿ƒè¿›ä¸¥æ ¼çš„æ¨ç†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é¢„è§ˆå’Œè‡ªæˆ‘æ£€æŸ¥ã€‚æˆ‘ä»¬ç”Ÿæˆäº†å…·æœ‰å¤æ‚çº¦æŸçš„æŒ‡ä»¤ï¼Œå¹¶é€šè¿‡è¿‡æ»¤è¿‡ç¨‹è·å¾—æœ‰æ•ˆçš„æç¤ºï¼Œæœ€ç»ˆå½¢æˆäº†ä¸‰ä¸ªä¸åŒçš„æç¤ºæ•°æ®é›†ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨éµå¾ªæŒ‡ä»¤çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯æˆ‘ä»¬çš„Light-IF-32Bæ¨¡å‹è¶…è¶Šäº†è®¸å¤šæ›´å¤§çš„å¼€æºå’Œé—­æºæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.00109', 'title': 'FACTORY: A Challenging Human-Verified Prompt Set for Long-Form\n  Factuality', 'url': 'https://huggingface.co/papers/2508.00109', 'abstract': 'FACTORY, a human-verified prompt set, evaluates the factuality of long-form responses from language models, revealing higher factual accuracy compared to existing datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-form factuality evaluation assesses the ability of models to generate accurate, comprehensive responses to short prompts. Existing benchmarks often lack human verification, leading to potential quality issues. To address this limitation, we introduce FACTORY, a large-scale, human-verified prompt set. Developed using a model-in-the-loop approach and refined by humans, FACTORY includes challenging prompts that are fact-seeking, answerable, and unambiguous. We conduct human evaluations on 6 state-of-the-art language models using FACTORY and existing datasets. Our results show that FACTORY is a challenging benchmark: approximately 40% of the claims made in the responses of SOTA models are not factual, compared to only 10% for other datasets. Our analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing its reliability and the necessity for models to reason across long-tailed facts.', 'score': 0, 'issue_id': 5234, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 Ğ¸ÑĞ»Ñ', 'en': 'July 31', 'zh': '7æœˆ31æ—¥'}, 'hash': '6dd83d8a170db99d', 'authors': ['Mingda Chen', 'Yang Li', 'Xilun Chen', 'Adina Williams', 'Gargi Ghosh', 'Scott Yih'], 'affiliations': ['Meta'], 'pdf_title_img': 'assets/pdf/title_img/2508.00109.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#dataset', '#long_context', '#hallucinations'], 'emoji': 'ğŸ”', 'ru': {'title': 'FACTORY: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'FACTORY - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° model-in-the-loop Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½ Ğ»ÑĞ´ÑŒĞ¼Ğ¸. FACTORY Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ñ‹, Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¸ Ğ½Ğµ Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°ÑÑ‚ Ğ´Ğ²ÑƒÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FACTORY ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ¼: Ğ¾ĞºĞ¾Ğ»Ğ¾ 40% ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹.'}, 'en': {'title': 'FACTORY: Elevating Factual Accuracy in Language Models', 'desc': 'The paper introduces FACTORY, a new benchmark for evaluating the factual accuracy of long-form responses generated by language models. Unlike existing datasets, FACTORY is human-verified, ensuring higher quality and reliability in assessing model outputs. The study reveals that state-of-the-art models struggle with factual accuracy, with around 40% of their claims being incorrect when evaluated with FACTORY. This highlights the importance of using robust, human-verified datasets to improve the reasoning capabilities of language models across diverse factual scenarios.'}, 'zh': {'title': 'FACTORYï¼šæå‡è¯­è¨€æ¨¡å‹çš„äº‹å®å‡†ç¡®æ€§', 'desc': 'FACTORYæ˜¯ä¸€ä¸ªç»è¿‡äººå·¥éªŒè¯çš„æç¤ºé›†ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹ç”Ÿæˆé•¿æ–‡æœ¬å“åº”çš„äº‹å®å‡†ç¡®æ€§ã€‚ä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼ŒFACTORYæ˜¾ç¤ºå‡ºæ›´é«˜çš„äº‹å®å‡†ç¡®æ€§ï¼Œå› ä¸ºå®ƒé‡‡ç”¨äº†æ¨¡å‹å¾ªç¯çš„æ–¹æ³•ï¼Œå¹¶ç»è¿‡äººç±»çš„ç²¾ç»†è°ƒæ•´ã€‚é€šè¿‡å¯¹å…­ç§æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å¤§çº¦40%çš„å“åº”å£°æ˜å¹¶ä¸çœŸå®ï¼Œè€Œå…¶ä»–æ•°æ®é›†çš„è¿™ä¸€æ¯”ä¾‹ä»…ä¸º10%ã€‚è¿™è¡¨æ˜FACTORYåœ¨è¯„ä¼°æ¨¡å‹çš„é•¿å°¾äº‹å®æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰æ›´é«˜çš„å¯é æ€§ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (5)', '#agents (8)', '#agi (1)', '#alignment (3)', '#architecture (4)', '#audio (2)', '#benchmark (14)', '#cv (4)', '#data (5)', '#dataset (14)', '#diffusion (4)', '#ethics (2)', '#games (8)', '#graphs', '#hallucinations (1)', '#healthcare (3)', '#inference (1)', '#interpretability (5)', '#leakage', '#long_context (4)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (10)', '#open_source (11)', '#optimization (18)', '#plp (1)', '#rag (1)', '#reasoning (10)', '#rl (10)', '#rlhf (3)', '#robotics', '#science (2)', '#security (2)', '#small_models', '#story_generation', '#survey (1)', '#synthetic (4)', '#training (13)', '#transfer_learning (2)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-08-07 18:18',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-08-07 18:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-08-07 18:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    