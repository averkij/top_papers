{
    "date": {
        "ru": "18 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        "en": "October 18",
        "zh": "10æœˆ18æ—¥"
    },
    "time_utc": "2024-10-18 09:00",
    "weekday": 4,
    "issue_id": 175,
    "home_page_url": "https://huggingface.co/papers?date=2024-10-18",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2410.13720",
            "title": "Movie Gen: A Cast of Media Foundation Models",
            "url": "https://huggingface.co/papers/2410.13720",
            "abstract": "We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user's image. Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models. All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.",
            "score": 88,
            "issue_id": 147,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "086b8ff148ce7df3",
            "authors": [
                "Adam Polyak",
                "Amit Zohar",
                "Andrew Brown",
                "Andros Tjandra",
                "Animesh Sinha",
                "Ann Lee",
                "Apoorv Vyas",
                "Bowen Shi",
                "Chih-Yao Ma",
                "Ching-Yao Chuang",
                "David Yan",
                "Dhruv Choudhary",
                "Dingkang Wang",
                "Geet Sethi",
                "Guan Pang",
                "Haoyu Ma",
                "Ishan Misra",
                "Ji Hou",
                "Jialiang Wang",
                "Kiran Jagadeesh",
                "Kunpeng Li",
                "Luxin Zhang",
                "Mannat Singh",
                "Mary Williamson",
                "Matt Le",
                "Matthew Yu",
                "Mitesh Kumar Singh",
                "Peizhao Zhang",
                "Peter Vajda",
                "Quentin Duval",
                "Rohit Girdhar",
                "Roshan Sumbaly",
                "Sai Saketh Rambhatla",
                "Sam Tsai",
                "Samaneh Azadi",
                "Samyak Datta",
                "Sanyuan Chen",
                "Sean Bell",
                "Sharadh Ramaswamy",
                "Shelly Sheynin",
                "Siddharth Bhattacharya",
                "Simran Motwani",
                "Tao Xu",
                "Tianhe Li",
                "Tingbo Hou",
                "Wei-Ning Hsu",
                "Xi Yin",
                "Xiaoliang Dai",
                "Yaniv Taigman",
                "Yaqiao Luo",
                "Yen-Cheng Liu",
                "Yi-Chiao Wu",
                "Yue Zhao",
                "Yuval Kirstain",
                "Zecheng He",
                "Zijian He",
                "Albert Pumarola",
                "Ali Thabet",
                "Artsiom Sanakoyeu",
                "Arun Mallya",
                "Baishan Guo",
                "Boris Araya",
                "Breena Kerr",
                "Carleigh Wood",
                "Ce Liu",
                "Cen Peng",
                "Dimitry Vengertsev",
                "Edgar Schonfeld",
                "Elliot Blanchard",
                "Felix Juefei-Xu",
                "Fraylie Nord",
                "Jeff Liang",
                "John Hoffman",
                "Jonas Kohler",
                "Kaolin Fire",
                "Karthik Sivakumar",
                "Lawrence Chen",
                "Licheng Yu",
                "Luya Gao",
                "Markos Georgopoulos",
                "Rashel Moritz",
                "Sara K. Sampson",
                "Shikai Li",
                "Simone Parmeggiani",
                "Steve Fine",
                "Tara Fowler",
                "Vladan Petrovic",
                "Yuming Du"
            ],
            "affiliations": [
                "Meta"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13720.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#synthetic",
                    "#inference",
                    "#video",
                    "#optimization",
                    "#multimodal",
                    "#data",
                    "#training",
                    "#open_source",
                    "#audio",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "MovieGen: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼ĞµĞ´Ğ¸Ğ°",
                    "desc": "MovieGen - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ 1080p HD Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞšÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ 30 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 16-ÑĞµĞºÑƒĞ½Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ€ÑĞ´ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼ĞµĞ´Ğ¸Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Video Creation with Movie Gen",
                    "desc": "Movie Gen introduces advanced foundation models capable of generating high-quality videos with synchronized audio, offering new capabilities in video editing and personalization. The models excel in tasks like text-to-video synthesis and video-to-audio generation, setting a new benchmark in the field. With a 30 billion parameter transformer, the system can produce 16-second videos at 16 frames per second, showcasing significant technical innovations. These advancements aim to push forward the research and development of large-scale media generation models."
                },
                "zh": {
                    "title": "Movie Genï¼šå¼•é¢†é«˜æ¸…è§†é¢‘ç”Ÿæˆæ–°æ ‡å‡†",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºMovie Gençš„åŸºç¡€æ¨¡å‹é›†ï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„1080pé«˜æ¸…è§†é¢‘ï¼Œå¹¶æ”¯æŒä¸åŒçš„å®½é«˜æ¯”å’ŒåŒæ­¥éŸ³é¢‘ã€‚è¯¥æ¨¡å‹è¿˜å…·å¤‡ç²¾ç¡®çš„æŒ‡ä»¤è§†é¢‘ç¼–è¾‘å’ŒåŸºäºç”¨æˆ·å›¾åƒç”Ÿæˆä¸ªæ€§åŒ–è§†é¢‘çš„èƒ½åŠ›ã€‚Movie Genåœ¨å¤šé¡¹ä»»åŠ¡ä¸Šè®¾ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡å‡†ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°è§†é¢‘åˆæˆã€è§†é¢‘ä¸ªæ€§åŒ–ã€è§†é¢‘ç¼–è¾‘ã€è§†é¢‘åˆ°éŸ³é¢‘ç”Ÿæˆå’Œæ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆã€‚é€šè¿‡å¤šé¡¹æŠ€æœ¯åˆ›æ–°å’Œç®€åŒ–ï¼Œè¯¥æ¨¡å‹åœ¨æ¶æ„ã€æ½œåœ¨ç©ºé—´ã€è®­ç»ƒç›®æ ‡ã€æ•°æ®ç­–åˆ’ç­‰æ–¹é¢å–å¾—äº†çªç ´ï¼Œæ¨åŠ¨äº†å¤§è§„æ¨¡åª’ä½“ç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13754",
            "title": "MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures",
            "url": "https://huggingface.co/papers/2410.13754",
            "abstract": "Perceiving and generating diverse modalities are crucial for AI models to effectively learn from and engage with real-world signals, necessitating reliable evaluations for their development. We identify two major issues in current evaluations: (1) inconsistent standards, shaped by different communities with varying protocols and maturity levels; and (2) significant query, grading, and generalization biases. To address these, we introduce MixEval-X, the first any-to-any real-world benchmark designed to optimize and standardize evaluations across input and output modalities. We propose multi-modal benchmark mixture and adaptation-rectification pipelines to reconstruct real-world task distributions, ensuring evaluations generalize effectively to real-world use cases. Extensive meta-evaluations show our approach effectively aligns benchmark samples with real-world task distributions and the model rankings correlate strongly with that of crowd-sourced real-world evaluations (up to 0.98). We provide comprehensive leaderboards to rerank existing models and organizations and offer insights to enhance understanding of multi-modal evaluations and inform future research.",
            "score": 74,
            "issue_id": 148,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "82517ad6fbb54273",
            "authors": [
                "Jinjie Ni",
                "Yifan Song",
                "Deepanway Ghosal",
                "Bo Li",
                "David Junhao Zhang",
                "Xiang Yue",
                "Fuzhao Xue",
                "Zian Zheng",
                "Kaichen Zhang",
                "Mahir Shah",
                "Kabir Jain",
                "Yang You",
                "Michael Shieh"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Independent Researcher",
                "Nanyang Technological University",
                "National University of Singapore",
                "Peking University",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13754.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#optimization",
                    "#multimodal",
                    "#survey",
                    "#alignment"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "MixEval-X: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MixEval-X - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ² ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ ĞºÑ€Ğ°ÑƒĞ´ÑĞ¾Ñ€ÑĞ¸Ğ½Ğ³Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "MixEval-X: Bridging the Gap Between AI Benchmarks and Real-World Performance",
                    "desc": "The paper introduces MixEval-X, a benchmark designed to standardize evaluations across different input and output modalities in AI models. It addresses issues of inconsistent evaluation standards and biases in current methods by proposing a multi-modal benchmark mixture and adaptation-rectification pipelines. These pipelines help align benchmark samples with real-world task distributions, ensuring that evaluations are more representative of real-world scenarios. The approach shows strong correlation with real-world evaluations, providing valuable insights for improving multi-modal evaluations and guiding future research."
                },
                "zh": {
                    "title": "MixEval-Xï¼šä¼˜åŒ–å¤šæ¨¡æ€è¯„ä¼°çš„å…¨æ–°åŸºå‡†",
                    "desc": "è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†AIæ¨¡å‹åœ¨å¤„ç†å¤šç§ä¿¡å·æ—¶éœ€è¦å¯é çš„è¯„ä¼°æ–¹æ³•ã€‚å½“å‰è¯„ä¼°å­˜åœ¨æ ‡å‡†ä¸ä¸€è‡´å’Œåå·®é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†MixEval-Xï¼Œä¸€ä¸ªç”¨äºä¼˜åŒ–å’Œæ ‡å‡†åŒ–å¤šæ¨¡æ€è¯„ä¼°çš„åŸºå‡†ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œè¯„ä¼°ç»“æœæ›´è´´è¿‘çœŸå®ä¸–ç•Œçš„ä»»åŠ¡åˆ†å¸ƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.12784",
            "title": "JudgeBench: A Benchmark for Evaluating LLM-based Judges",
            "url": "https://huggingface.co/papers/2410.12784",
            "abstract": "LLM-based judges have emerged as a scalable alternative to human evaluation and are increasingly used to assess, compare, and improve models. However, the reliability of LLM-based judges themselves is rarely scrutinized. As LLMs become more advanced, their responses grow more sophisticated, requiring stronger judges to evaluate them. Existing benchmarks primarily focus on a judge's alignment with human preferences, but often fail to account for more challenging tasks where crowdsourced human preference is a poor indicator of factual and logical correctness. To address this, we propose a novel evaluation framework to objectively evaluate LLM-based judges. Based on this framework, we propose JudgeBench, a benchmark for evaluating LLM-based judges on challenging response pairs spanning knowledge, reasoning, math, and coding. JudgeBench leverages a novel pipeline for converting existing difficult datasets into challenging response pairs with preference labels reflecting objective correctness. Our comprehensive evaluation on a collection of prompted judges, fine-tuned judges, multi-agent judges, and reward models shows that JudgeBench poses a significantly greater challenge than previous benchmarks, with many strong models (e.g., GPT-4o) performing just slightly better than random guessing. Overall, JudgeBench offers a reliable platform for assessing increasingly advanced LLM-based judges. Data and code are available at https://github.com/ScalerLab/JudgeBench .",
            "score": 42,
            "issue_id": 160,
            "pub_date": "2024-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "a81030e9f379736a",
            "authors": [
                "Sijun Tan",
                "Siyuan Zhuang",
                "Kyle Montgomery",
                "William Y. Tang",
                "Alejandro Cuadron",
                "Chenguang Wang",
                "Raluca Ada Popa",
                "Ion Stoica"
            ],
            "affiliations": [
                "UC Berkeley",
                "Washington University in St. Louis"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.12784.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#benchmark",
                    "#math",
                    "#plp",
                    "#data",
                    "#training",
                    "#dataset",
                    "#open_source",
                    "#architecture",
                    "#alignment"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "JudgeBench: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜-ÑÑƒĞ´ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑƒĞ´ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ JudgeBench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM-ÑÑƒĞ´ĞµĞ¹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. JudgeBench Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ñ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ JudgeBench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‡ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ñ‹, Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğ¼Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ»Ğ¸ÑˆÑŒ Ğ½ĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "JudgeBench: Raising the Bar for LLM Evaluation",
                    "desc": "The paper introduces JudgeBench, a new benchmark designed to evaluate the reliability of LLM-based judges, which are used to assess and improve machine learning models. Unlike existing benchmarks that focus on alignment with human preferences, JudgeBench emphasizes objective correctness in challenging tasks like reasoning and coding. The framework converts difficult datasets into response pairs with preference labels, providing a more rigorous test for LLM-based judges. Results show that even advanced models struggle with JudgeBench, highlighting its effectiveness in assessing the capabilities of these judges."
                },
                "zh": {
                    "title": "JudgeBenchï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹è¯„åˆ¤è€…çš„æ–°åŸºå‡†",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå®¢è§‚åœ°è¯„ä¼°åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„åˆ¤è€…ã€‚ç ”ç©¶è€…å¼€å‘äº†ä¸€ä¸ªåä¸ºJudgeBenchçš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è¿™äº›è¯„åˆ¤è€…åœ¨çŸ¥è¯†ã€æ¨ç†ã€æ•°å­¦å’Œç¼–ç¨‹ç­‰æ–¹é¢çš„æŒ‘æˆ˜æ€§å“åº”å¯¹ã€‚JudgeBenché€šè¿‡ä¸€ä¸ªæ–°é¢–çš„æµç¨‹ï¼Œå°†ç°æœ‰çš„å›°éš¾æ•°æ®é›†è½¬æ¢ä¸ºå…·æœ‰åå¥½æ ‡ç­¾çš„æŒ‘æˆ˜æ€§å“åº”å¯¹ï¼Œä»¥åæ˜ å®¢è§‚çš„æ­£ç¡®æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒJudgeBenchæ¯”ä»¥å¾€çš„åŸºå‡†æ›´å…·æŒ‘æˆ˜æ€§ï¼Œè®¸å¤šå¼ºå¤§çš„æ¨¡å‹åœ¨æ­¤åŸºå‡†ä¸Šçš„è¡¨ç°ä»…ç•¥ä¼˜äºéšæœºçŒœæµ‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13863",
            "title": "Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens",
            "url": "https://huggingface.co/papers/2410.13863",
            "abstract": "Scaling up autoregressive models in vision has not proven as beneficial as in large language models. In this work, we investigate this scaling problem in the context of text-to-image generation, focusing on two critical factors: whether models use discrete or continuous tokens, and whether tokens are generated in a random or fixed raster order using BERT- or GPT-like transformer architectures. Our empirical results show that, while all models scale effectively in terms of validation loss, their evaluation performance -- measured by FID, GenEval score, and visual quality -- follows different trends. Models based on continuous tokens achieve significantly better visual quality than those using discrete tokens. Furthermore, the generation order and attention mechanisms significantly affect the GenEval score: random-order models achieve notably better GenEval scores compared to raster-order models. Inspired by these findings, we train Fluid, a random-order autoregressive model on continuous tokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16 on MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope our findings and results will encourage future efforts to further bridge the scaling gap between vision and language models.",
            "score": 35,
            "issue_id": 160,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "3fa9a449112a391b",
            "authors": [
                "Lijie Fan",
                "Tianhong Li",
                "Siyang Qin",
                "Yuanzhen Li",
                "Chen Sun",
                "Michael Rubinstein",
                "Deqing Sun",
                "Kaiming He",
                "Yonglong Tian"
            ],
            "affiliations": [
                "Google DeepMind",
                "MIT"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13863.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#benchmark",
                    "#cv",
                    "#optimization",
                    "#games",
                    "#architecture"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ½Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ GenEval. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Fluid, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ÑˆĞ°Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "Bridging the Gap: Continuous Tokens and Random Order in Vision Models",
                    "desc": "This paper explores the challenges of scaling autoregressive models for text-to-image generation, focusing on the use of discrete versus continuous tokens and the order of token generation. The study finds that models using continuous tokens produce higher visual quality images compared to those using discrete tokens. Additionally, models that generate tokens in a random order outperform those using a fixed raster order in terms of GenEval scores. The authors introduce Fluid, a random-order autoregressive model with continuous tokens, which sets new benchmarks in zero-shot FID and GenEval scores, suggesting a promising direction for future research in bridging the gap between vision and language models."
                },
                "zh": {
                    "title": "çªç ´è§†è§‰ä¸è¯­è¨€æ¨¡å‹æ‰©å±•çš„ç•Œé™",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åœ¨å›¾åƒç”Ÿæˆä¸­è‡ªå›å½’æ¨¡å‹çš„æ‰©å±•é—®é¢˜ï¼Œç‰¹åˆ«å…³æ³¨ä½¿ç”¨ç¦»æ•£æˆ–è¿ç»­çš„æ ‡è®°ï¼Œä»¥åŠæ ‡è®°ç”Ÿæˆçš„é¡ºåºã€‚ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨è¿ç»­æ ‡è®°çš„æ¨¡å‹åœ¨è§†è§‰è´¨é‡ä¸Šæ˜æ˜¾ä¼˜äºä½¿ç”¨ç¦»æ•£æ ‡è®°çš„æ¨¡å‹ã€‚ç”Ÿæˆé¡ºåºå’Œæ³¨æ„åŠ›æœºåˆ¶å¯¹GenEvalè¯„åˆ†æœ‰æ˜¾è‘—å½±å“ï¼Œéšæœºé¡ºåºçš„æ¨¡å‹åœ¨GenEvalè¯„åˆ†ä¸Šè¡¨ç°æ›´å¥½ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œä½œè€…è®­ç»ƒäº†Fluidæ¨¡å‹ï¼Œåœ¨MS-COCO 30Kæ•°æ®é›†ä¸Šå–å¾—äº†æ–°çš„é›¶æ ·æœ¬FIDè®°å½•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13268",
            "title": "Roadmap towards Superhuman Speech Understanding using Large Language Models",
            "url": "https://huggingface.co/papers/2410.13268",
            "abstract": "The success of large language models (LLMs) has prompted efforts to integrate speech and audio data, aiming to create general foundation models capable of processing both textual and non-textual inputs. Recent advances, such as GPT-4o, highlight the potential for end-to-end speech LLMs, which preserves non-semantic information and world knowledge for deeper speech understanding. To guide the development of speech LLMs, we propose a five-level roadmap, ranging from basic automatic speech recognition (ASR) to advanced superhuman models capable of integrating non-semantic information with abstract acoustic knowledge for complex tasks. Moreover, we design a benchmark, SAGI Bechmark, that standardizes critical aspects across various tasks in these five levels, uncovering challenges in using abstract acoustic knowledge and completeness of capability. Our findings reveal gaps in handling paralinguistic cues and abstract acoustic knowledge, and we offer future directions. This paper outlines a roadmap for advancing speech LLMs, introduces a benchmark for evaluation, and provides key insights into their current limitations and potential.",
            "score": 33,
            "issue_id": 153,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "929ec80dcb105705",
            "authors": [
                "Fan Bu",
                "Yuhao Zhang",
                "Xidong Wang",
                "Benyou Wang",
                "Qun Liu",
                "Haizhou Li"
            ],
            "affiliations": [
                "Noahs Ark Lab, Huawei",
                "The Chinese University of Hong Kong, Shenzhen"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13268.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#benchmark",
                    "#agi",
                    "#multimodal",
                    "#survey",
                    "#audio",
                    "#architecture"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "Ğ”Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… LLM: Ğ¾Ñ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğº ÑĞ²ĞµÑ€Ñ…Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ° Ğ¸Ğ· Ğ¿ÑÑ‚Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµÑ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SAGI Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… LLM."
                },
                "en": {
                    "title": "Bridging Text and Sound: The Future of Speech Language Models",
                    "desc": "This paper explores the integration of speech and audio data into large language models (LLMs) to create versatile models that can handle both text and non-text inputs. It introduces a five-level roadmap for developing speech LLMs, from basic automatic speech recognition (ASR) to advanced models that incorporate non-semantic information and abstract acoustic knowledge. The authors also present the SAGI Benchmark, which evaluates these models across various tasks and highlights challenges in processing paralinguistic cues and abstract acoustic knowledge. The paper provides insights into the current limitations of speech LLMs and suggests future research directions to enhance their capabilities."
                },
                "zh": {
                    "title": "è¯­éŸ³å¤§æ¨¡å‹çš„æœªæ¥ï¼šä»åŸºç¡€åˆ°è¶…äºº",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å°†è¯­éŸ³å’ŒéŸ³é¢‘æ•°æ®æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¯èƒ½æ€§ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿå¤„ç†æ–‡æœ¬å’Œéæ–‡æœ¬è¾“å…¥çš„é€šç”¨åŸºç¡€æ¨¡å‹ã€‚ç ”ç©¶æå‡ºäº†ä¸€ä¸ªäº”çº§è·¯çº¿å›¾ï¼Œä»åŸºæœ¬çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«åˆ°èƒ½å¤Ÿå¤„ç†å¤æ‚ä»»åŠ¡çš„è¶…äººæ¨¡å‹ã€‚ä½œè€…è¿˜è®¾è®¡äº†ä¸€ä¸ªåä¸ºSAGIçš„åŸºå‡†ï¼Œç”¨äºæ ‡å‡†åŒ–è¿™äº›äº”ä¸ªçº§åˆ«ä¸­å„ç§ä»»åŠ¡çš„å…³é”®æ–¹é¢ã€‚ç ”ç©¶å‘ç°å½“å‰æ¨¡å‹åœ¨å¤„ç†å‰¯è¯­è¨€çº¿ç´¢å’ŒæŠ½è±¡å£°å­¦çŸ¥è¯†æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¹¶æä¾›äº†æœªæ¥çš„å‘å±•æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13757",
            "title": "MobA: A Two-Level Agent System for Efficient Mobile Task Automation",
            "url": "https://huggingface.co/papers/2410.13757",
            "abstract": "Current mobile assistants are limited by dependence on system APIs or struggle with complex user instructions and diverse interfaces due to restricted comprehension and decision-making abilities. To address these challenges, we propose MobA, a novel Mobile phone Agent powered by multimodal large language models that enhances comprehension and planning capabilities through a sophisticated two-level agent architecture. The high-level Global Agent (GA) is responsible for understanding user commands, tracking history memories, and planning tasks. The low-level Local Agent (LA) predicts detailed actions in the form of function calls, guided by sub-tasks and memory from the GA. Integrating a Reflection Module allows for efficient task completion and enables the system to handle previously unseen complex tasks. MobA demonstrates significant improvements in task execution efficiency and completion rate in real-life evaluations, underscoring the potential of MLLM-empowered mobile assistants.",
            "score": 31,
            "issue_id": 150,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "a4a73fb090d1a0ae",
            "authors": [
                "Zichen Zhu",
                "Hao Tang",
                "Yansi Li",
                "Kunyao Lan",
                "Yixuan Jiang",
                "Hao Zhou",
                "Yixiao Wang",
                "Situo Zhang",
                "Liangtai Sun",
                "Lu Chen",
                "Kai Yu"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University, China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13757.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#agi",
                    "#multimodal",
                    "#agents",
                    "#architecture",
                    "#alignment"
                ],
                "emoji": "ğŸ“±",
                "ru": {
                    "title": "MobA: Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ",
                    "desc": "MobA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹. MobA Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Revolutionizing Mobile Assistance with Multimodal Intelligence",
                    "desc": "The paper introduces MobA, a mobile assistant that uses multimodal large language models to improve understanding and task planning. It features a two-level agent architecture with a Global Agent for command comprehension and task planning, and a Local Agent for executing detailed actions. A Reflection Module is integrated to enhance the system's ability to handle complex and novel tasks. Real-life tests show MobA's improved efficiency and success in completing tasks, highlighting the potential of advanced language models in mobile assistants."
                },
                "zh": {
                    "title": "å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åŠ©åŠ›ç§»åŠ¨åŠ©æ‰‹æ–°çªç ´",
                    "desc": "å½“å‰çš„ç§»åŠ¨åŠ©æ‰‹ç”±äºå¯¹ç³»ç»ŸAPIçš„ä¾èµ–å’Œå¯¹å¤æ‚ç”¨æˆ·æŒ‡ä»¤çš„ç†è§£èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥å¤„ç†å¤šæ ·åŒ–çš„ç•Œé¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MobAï¼Œè¿™æ˜¯ä¸€ç§ç”±å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨çš„ç§»åŠ¨ä»£ç†ï¼Œé€šè¿‡å¤æ‚çš„åŒå±‚ä»£ç†æ¶æ„å¢å¼ºç†è§£å’Œè§„åˆ’èƒ½åŠ›ã€‚é«˜å±‚çš„å…¨å±€ä»£ç†è´Ÿè´£ç†è§£ç”¨æˆ·å‘½ä»¤ã€è·Ÿè¸ªå†å²è®°å¿†å’Œè§„åˆ’ä»»åŠ¡ï¼Œè€Œä½å±‚çš„æœ¬åœ°ä»£ç†åˆ™æ ¹æ®å­ä»»åŠ¡å’Œå…¨å±€ä»£ç†çš„è®°å¿†é¢„æµ‹è¯¦ç»†çš„åŠ¨ä½œã€‚é€šè¿‡é›†æˆåæ€æ¨¡å—ï¼Œç³»ç»Ÿèƒ½å¤Ÿé«˜æ•ˆå®Œæˆä»»åŠ¡ï¼Œå¹¶å¤„ç†ä»¥å‰æœªè§è¿‡çš„å¤æ‚ä»»åŠ¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.12705",
            "title": "WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines",
            "url": "https://huggingface.co/papers/2410.12705",
            "abstract": "Vision Language Models (VLMs) often struggle with culture-specific knowledge, particularly in languages other than English and in underrepresented cultural contexts. To evaluate their understanding of such knowledge, we introduce WorldCuisines, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes a visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural VQA benchmark to date. It includes tasks for identifying dish names and their origins. We provide evaluation datasets in two sizes (12k and 60k instances) alongside a training dataset (1 million instances). Our findings show that while VLMs perform better with correct location context, they struggle with adversarial contexts and predicting specific regional cuisines and languages. To support future research, we release a knowledge base with annotated food entries and images along with the VQA data.",
            "score": 29,
            "issue_id": 157,
            "pub_date": "2024-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "6829d8490ef2d294",
            "authors": [
                "Genta Indra Winata",
                "Frederikus Hudi",
                "Patrick Amadeus Irawan",
                "David Anugraha",
                "Rifki Afina Putri",
                "Yutong Wang",
                "Adam Nohejl",
                "Ubaidillah Ariq Prathama",
                "Nedjma Ousidhoum",
                "Afifa Amriani",
                "Anar Rzayev",
                "Anirban Das",
                "Ashmari Pramodya",
                "Aulia Adila",
                "Bryan Wilie",
                "Candy Olivia Mawalim",
                "Ching Lam Cheng",
                "Daud Abolade",
                "Emmanuele Chersoni",
                "Enrico Santus",
                "Fariz Ikhwantri",
                "Garry Kuwanto",
                "Hanyang Zhao",
                "Haryo Akbarianto Wibowo",
                "Holy Lovenia",
                "Jan Christian Blaise Cruz",
                "Jan Wira Gotama Putra",
                "Junho Myung",
                "Lucky Susanto",
                "Maria Angelica Riera Machin",
                "Marina Zhukova",
                "Michael Anugraha",
                "Muhammad Farid Adilazuarda",
                "Natasha Santosa",
                "Peerat Limkonchotiwat",
                "Raj Dabre",
                "Rio Alexander Audino",
                "Samuel Cahyawijaya",
                "Shi-Xiong Zhang",
                "Stephanie Yulia Salim",
                "Yi Zhou",
                "Yinxuan Gui",
                "David Ifeoluwa Adelani",
                "En-Shiun Annie Lee",
                "Shogo Okada",
                "Ayu Purwarianti",
                "Alham Fikri Aji",
                "Taro Watanabe",
                "Derry Tanti Wijaya",
                "Alice Oh",
                "Chong-Wah Ngo"
            ],
            "affiliations": [
                "AI Singapore",
                "Boston University",
                "Capital One",
                "Cardiff University",
                "Cohere",
                "Columbia University",
                "HK PolyU",
                "HKUST",
                "ITB",
                "Independent",
                "JAIST",
                "KAIST",
                "MBZUAI",
                "MILA",
                "Masakhane",
                "McGill",
                "Monash University",
                "NAIST",
                "NICT",
                "Ontario Tech",
                "SEACrowd",
                "SMU",
                "Tokyo Tech",
                "UCSB",
                "University of Lagos",
                "UofT"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.12705.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#multilingual",
                    "#cv",
                    "#graphs",
                    "#multimodal",
                    "#dataset",
                    "#open_source",
                    "#games",
                    "#low_resource"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "WorldCuisines: Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ½Ğ° ĞºÑƒĞ»Ğ¸Ğ½Ğ°Ñ€Ğ½ÑƒÑ ÑÑ€ÑƒĞ´Ğ¸Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº WorldCuisines Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ (VQA) Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 30 ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 9 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞµĞ¼ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² Ğ´Ğ²ÑƒÑ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ· 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºÑƒÑ…Ğ¾Ğ½ÑŒ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²."
                },
                "en": {
                    "title": "\"WorldCuisines: Bridging Cultural Gaps in Vision Language Models\"",
                    "desc": "The paper introduces WorldCuisines, a large-scale benchmark designed to test Vision Language Models (VLMs) on their ability to understand culture-specific knowledge across multiple languages and dialects. This benchmark includes a Visual Question Answering (VQA) dataset with over 1 million text-image pairs, making it the largest of its kind for multicultural contexts. The study reveals that while VLMs can perform well when given correct location context, they face challenges with adversarial contexts and accurately predicting regional cuisines and languages. To aid further research, the authors provide a comprehensive knowledge base with annotated food entries and images."
                },
                "zh": {
                    "title": "è·¨æ–‡åŒ–è§†è§‰è¯­è¨€ç†è§£çš„æ–°åŸºå‡†",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºWorldCuisinesçš„å¤§è§„æ¨¡åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€å’Œå¤šæ–‡åŒ–èƒŒæ™¯ä¸‹çš„ç†è§£èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…æ‹¬ä¸€ä¸ªè§†è§‰é—®ç­”æ•°æ®é›†ï¼Œæ¶µç›–30ç§è¯­è¨€å’Œæ–¹è¨€ï¼Œæ¶‰åŠ9ä¸ªè¯­è¨€å®¶æ—ï¼Œæ‹¥æœ‰è¶…è¿‡100ä¸‡ä¸ªæ•°æ®ç‚¹ã€‚ç ”ç©¶å‘ç°ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ­£ç¡®çš„åœ°ç†èƒŒæ™¯ä¸‹è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨å¯¹æŠ—æ€§èƒŒæ™¯å’Œé¢„æµ‹ç‰¹å®šåœ°åŒºçš„èœè‚´å’Œè¯­è¨€æ—¶è¡¨ç°è¾ƒå·®ã€‚ä¸ºäº†æ”¯æŒæœªæ¥çš„ç ”ç©¶ï¼Œä½œè€…è¿˜å‘å¸ƒäº†ä¸€ä¸ªåŒ…å«æ³¨é‡Šé£Ÿå“æ¡ç›®å’Œå›¾åƒçš„çŸ¥è¯†åº“ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13824",
            "title": "Harnessing Webpage UIs for Text-Rich Visual Understanding",
            "url": "https://huggingface.co/papers/2410.13824",
            "abstract": "Text-rich visual understanding-the ability to process environments where dense textual content is integrated with visuals-is crucial for multimodal large language models (MLLMs) to interact effectively with structured environments. To enhance this capability, we propose synthesizing general multimodal instructions from webpage UIs using text-based large language models (LLMs). Despite lacking direct visual input, text-based LLMs are able to process structured text representations from webpage accessibility trees. These instructions are then paired with UI screenshots to train multimodal models. We introduce MultiUI, a dataset containing 7.3 million samples from 1 million websites, covering diverse multimodal tasks and UI layouts. Models trained on MultiUI not only excel in web UI tasks-achieving up to a 48\\% improvement on VisualWebBench and a 19.1\\% boost in action accuracy on a web agent dataset Mind2Web-but also generalize surprisingly well to non-web UI tasks and even to non-UI domains, such as document understanding, OCR, and chart interpretation. These results highlight the broad applicability of web UI data for advancing text-rich visual understanding across various scenarios.",
            "score": 29,
            "issue_id": 146,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "7d1ade016ff53a03",
            "authors": [
                "Junpeng Liu",
                "Tianyue Ou",
                "Yifan Song",
                "Yuxiao Qu",
                "Wai Lam",
                "Chenyan Xiong",
                "Wenhu Chen",
                "Graham Neubig",
                "Xiang Yue"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Peking University",
                "The Chinese University of Hong Kong",
                "University of Waterloo"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13824.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#synthetic",
                    "#benchmark",
                    "#cv",
                    "#graphs",
                    "#optimization",
                    "#multimodal",
                    "#data",
                    "#training",
                    "#dataset",
                    "#transfer_learning",
                    "#games",
                    "#architecture"
                ],
                "emoji": "ğŸŒ",
                "ru": {
                    "title": "Ğ¡Ğ¸Ğ½Ñ‚ĞµĞ· Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ²ĞµĞ±-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MultiUI, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 7,3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ¸Ğ· 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ²ĞµĞ±-ÑĞ°Ğ¹Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° MultiUI, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²ĞµĞ±-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ² Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ ÑĞ²Ğ¾Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹."
                },
                "en": {
                    "title": "Unlocking Text-Rich Visual Understanding with Web UI Data",
                    "desc": "The paper introduces a method to improve multimodal large language models (MLLMs) by synthesizing instructions from webpage UIs using text-based large language models (LLMs). These models, despite not having direct visual input, can process structured text from webpage accessibility trees and are trained with UI screenshots. The authors present MultiUI, a dataset with 7.3 million samples from 1 million websites, which helps models excel in web UI tasks and generalize to other domains like document understanding and OCR. The study demonstrates that web UI data can significantly enhance text-rich visual understanding across various applications."
                },
                "zh": {
                    "title": "ç½‘é¡µUIæ•°æ®ï¼šæå‡å¤šæ¨¡æ€è§†è§‰ç†è§£çš„å…³é”®",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡ç½‘é¡µçš„å¯è®¿é—®æ€§æ ‘ç”Ÿæˆå¤šæ¨¡æ€æŒ‡ä»¤ï¼Œæ¥å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ä¸°å¯Œè§†è§‰ç†è§£èƒ½åŠ›ã€‚ç ”ç©¶ä¸­ä½¿ç”¨äº†ä¸€ä¸ªåä¸ºMultiUIçš„æ•°æ®é›†ï¼ŒåŒ…å«äº†æ¥è‡ª100ä¸‡ä¸ªç½‘ç«™çš„730ä¸‡æ ·æœ¬ï¼Œç”¨äºè®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MultiUIè®­ç»ƒçš„æ¨¡å‹åœ¨ç½‘é¡µUIä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ä¸”åœ¨éç½‘é¡µUIä»»åŠ¡å’Œå…¶ä»–é¢†åŸŸå¦‚æ–‡æ¡£ç†è§£ã€OCRå’Œå›¾è¡¨è§£é‡Šä¸­ä¹Ÿæœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™è¡¨æ˜ç½‘é¡µUIæ•°æ®åœ¨æå‡å¤šç§åœºæ™¯ä¸‹çš„æ–‡æœ¬ä¸°å¯Œè§†è§‰ç†è§£æ–¹é¢å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13848",
            "title": "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation",
            "url": "https://huggingface.co/papers/2410.13848",
            "abstract": "In this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal understanding and generation, this approach can lead to suboptimal performance, particularly in multimodal understanding. To address this issue, we decouple visual encoding into separate pathways, while still leveraging a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder's roles in understanding and generation, but also enhances the framework's flexibility. For instance, both the multimodal understanding and generation components can independently select their most suitable encoding methods. Experiments show that Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.",
            "score": 27,
            "issue_id": 148,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "8b28045f373976ba",
            "authors": [
                "Chengyue Wu",
                "Xiaokang Chen",
                "Zhiyu Wu",
                "Yiyang Ma",
                "Xingchao Liu",
                "Zizheng Pan",
                "Wen Liu",
                "Zhenda Xie",
                "Xingkai Yu",
                "Chong Ruan",
                "Ping Luo"
            ],
            "affiliations": [
                "DeepSeek-AI",
                "Peking University",
                "The University of Hong Kong"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13848.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#interpretability",
                    "#games",
                    "#architecture"
                ],
                "emoji": "ğŸ”€",
                "ru": {
                    "title": "Janus: ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Janus - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Janus Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ Ğ¾Ğ±Ğ¾Ğ¸Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Janus Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ½Ğµ ÑƒÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
                },
                "en": {
                    "title": "Janus: A New Era in Multimodal Intelligence",
                    "desc": "The paper introduces Janus, a new framework that improves how machines understand and create content using different types of data, like images and text. Unlike previous models that used one visual encoder for both understanding and generating, Janus separates these tasks into different pathways, which helps improve performance. By using a single transformer architecture, Janus allows each task to choose the best way to process information, making it more flexible and effective. Experiments show that Janus not only outperforms previous models but also competes well with models designed for specific tasks."
                },
                "zh": {
                    "title": "Janusï¼šå¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆçš„å…¨æ–°ç»Ÿä¸€æ¡†æ¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†Janusï¼Œä¸€ä¸ªç»Ÿä¸€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„è‡ªå›å½’æ¡†æ¶ã€‚ä»¥å¾€çš„ç ”ç©¶é€šå¸¸ä½¿ç”¨å•ä¸€çš„è§†è§‰ç¼–ç å™¨æ¥å¤„ç†è¿™ä¸¤é¡¹ä»»åŠ¡ï¼Œä½†ç”±äºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆæ‰€éœ€çš„ä¿¡æ¯ç²’åº¦ä¸åŒï¼Œè¿™ç§æ–¹æ³•å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒJanuså°†è§†è§‰ç¼–ç è§£è€¦ä¸ºç‹¬ç«‹çš„è·¯å¾„ï¼ŒåŒæ—¶ä»ç„¶ä½¿ç”¨ç»Ÿä¸€çš„Transformeræ¶æ„è¿›è¡Œå¤„ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒJanusä¸ä»…è¶…è¶Šäº†ä¹‹å‰çš„ç»Ÿä¸€æ¨¡å‹ï¼Œè¿˜èƒ½åŒ¹æ•Œæˆ–è¶…è¿‡ç‰¹å®šä»»åŠ¡æ¨¡å‹çš„è¡¨ç°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13830",
            "title": "DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control",
            "url": "https://huggingface.co/papers/2410.13830",
            "abstract": "Recent advances in customized video generation have enabled users to create videos tailored to both specific subjects and motion trajectories. However, existing methods often require complicated test-time fine-tuning and struggle with balancing subject learning and motion control, limiting their real-world applications. In this paper, we present DreamVideo-2, a zero-shot video customization framework capable of generating videos with a specific subject and motion trajectory, guided by a single image and a bounding box sequence, respectively, and without the need for test-time fine-tuning. Specifically, we introduce reference attention, which leverages the model's inherent capabilities for subject learning, and devise a mask-guided motion module to achieve precise motion control by fully utilizing the robust motion signal of box masks derived from bounding boxes. While these two components achieve their intended functions, we empirically observe that motion control tends to dominate over subject learning. To address this, we propose two key designs: 1) the masked reference attention, which integrates a blended latent mask modeling scheme into reference attention to enhance subject representations at the desired positions, and 2) a reweighted diffusion loss, which differentiates the contributions of regions inside and outside the bounding boxes to ensure a balance between subject and motion control. Extensive experimental results on a newly curated dataset demonstrate that DreamVideo-2 outperforms state-of-the-art methods in both subject customization and motion control. The dataset, code, and models will be made publicly available.",
            "score": 23,
            "issue_id": 146,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "67dc892195cd59d6",
            "authors": [
                "Yujie Wei",
                "Shiwei Zhang",
                "Hangjie Yuan",
                "Xiang Wang",
                "Haonan Qiu",
                "Rui Zhao",
                "Yutong Feng",
                "Feng Liu",
                "Zhizhong Huang",
                "Jiaxin Ye",
                "Yingya Zhang",
                "Hongming Shan"
            ],
            "affiliations": [
                "Alibaba Group",
                "Fudan University",
                "Michigan State University",
                "Nanyang Technological University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13830.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#video",
                    "#training",
                    "#dataset",
                    "#open_source",
                    "#games",
                    "#architecture"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ñ‰ĞµĞ»Ñ‡ĞºĞ¾Ğ¼",
                    "desc": "DreamVideo-2 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ¼Ğ¾Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DreamVideo-2 Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Effortless Video Customization with DreamVideo-2",
                    "desc": "DreamVideo-2 is a new framework for creating customized videos without needing complex adjustments during testing. It uses a single image and a sequence of bounding boxes to guide video generation, focusing on both the subject and its motion. The framework introduces reference attention and a mask-guided motion module to improve subject learning and motion control. To balance these aspects, it employs masked reference attention and reweighted diffusion loss, achieving superior results compared to existing methods."
                },
                "zh": {
                    "title": "DreamVideo-2ï¼šæ— å¾®è°ƒçš„è§†é¢‘å®šåˆ¶æ–°çªç ´",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDreamVideo-2çš„è§†é¢‘å®šåˆ¶æ¡†æ¶ï¼Œå¯ä»¥åœ¨ä¸éœ€è¦æµ‹è¯•æ—¶å¾®è°ƒçš„æƒ…å†µä¸‹ç”Ÿæˆç‰¹å®šä¸»é¢˜å’Œè¿åŠ¨è½¨è¿¹çš„è§†é¢‘ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥å‚è€ƒæ³¨æ„åŠ›å’Œæ©ç å¼•å¯¼è¿åŠ¨æ¨¡å—ï¼Œå®ç°äº†å¯¹ä¸»é¢˜å­¦ä¹ å’Œè¿åŠ¨æ§åˆ¶çš„å¹³è¡¡ã€‚ç ”ç©¶å‘ç°ï¼Œè¿åŠ¨æ§åˆ¶å¾€å¾€ä¼šå‹å€’ä¸»é¢˜å­¦ä¹ ï¼Œå› æ­¤æå‡ºäº†æ©ç å‚è€ƒæ³¨æ„åŠ›å’Œé‡åŠ æƒæ‰©æ•£æŸå¤±æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDreamVideo-2åœ¨ä¸»é¢˜å®šåˆ¶å’Œè¿åŠ¨æ§åˆ¶æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.11842",
            "title": "MoH: Multi-Head Attention as Mixture-of-Head Attention",
            "url": "https://huggingface.co/papers/2410.11842",
            "abstract": "In this work, we upgrade the multi-head attention mechanism, the core of the Transformer model, to improve efficiency while maintaining or surpassing the previous accuracy level. We show that multi-head attention can be expressed in the summation form. Drawing on the insight that not all attention heads hold equal significance, we propose Mixture-of-Head attention (MoH), a new architecture that treats attention heads as experts in the Mixture-of-Experts (MoE) mechanism. MoH has two significant advantages: First, MoH enables each token to select the appropriate attention heads, enhancing inference efficiency without compromising accuracy or increasing the number of parameters. Second, MoH replaces the standard summation in multi-head attention with a weighted summation, introducing flexibility to the attention mechanism and unlocking extra performance potential. Extensive experiments on ViT, DiT, and LLMs demonstrate that MoH outperforms multi-head attention by using only 50%-90% of the attention heads. Moreover, we demonstrate that pre-trained multi-head attention models, such as LLaMA3-8B, can be further continue-tuned into our MoH models. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14 benchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the attention heads. We believe the proposed MoH is a promising alternative to multi-head attention and provides a strong foundation for developing advanced and efficient attention-based models.",
            "score": 20,
            "issue_id": 148,
            "pub_date": "2024-10-15",
            "pub_date_card": {
                "ru": "15 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 15",
                "zh": "10æœˆ15æ—¥"
            },
            "hash": "4a94e557d3f7a79a",
            "authors": [
                "Peng Jin",
                "Bo Zhu",
                "Li Yuan",
                "Shuicheng Yan"
            ],
            "affiliations": [
                "Kunlun 2050 Research & Skywork AI, Singapore",
                "Peng Cheng Laboratory, Shenzhen, China",
                "Rabbitpre Intelligence, Shenzhen, China",
                "School of Electronic and Computer Engineering, Peking University, Shenzhen, China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.11842.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#inference",
                    "#optimization",
                    "#training",
                    "#transfer_learning",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ¡Ğ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Mixture-of-Head (MoH), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. MoH Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ Ñ‚Ğ¾ĞºĞµĞ½Ñƒ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ViT, DiT Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MoH Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ğ¸ÑˆÑŒ 50-90% Ğ³Ğ¾Ğ»Ğ¾Ğ². ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº LLaMA3-8B, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MoH Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "\"MoH: Elevating Attention Efficiency and Accuracy\"",
                    "desc": "This paper introduces Mixture-of-Head attention (MoH), an enhancement to the multi-head attention mechanism in Transformer models, aimed at improving efficiency and accuracy. MoH treats attention heads as experts, allowing each token to select the most relevant heads, which boosts inference efficiency without increasing parameters. By replacing the standard summation with a weighted summation, MoH adds flexibility and unlocks additional performance potential. Experiments show that MoH outperforms traditional multi-head attention, achieving higher accuracy with fewer attention heads, and can be applied to pre-trained models like LLaMA3-8B for further improvements."
                },
                "zh": {
                    "title": "æ··åˆå¤´æ³¨æ„åŠ›ï¼šé«˜æ•ˆçš„Transformeræ–°é€‰æ‹©",
                    "desc": "è¿™é¡¹ç ”ç©¶æ”¹è¿›äº†Transformeræ¨¡å‹ä¸­çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæˆ–è¶…è¿‡äº†ä¹‹å‰çš„å‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¤šå¤´æ³¨æ„åŠ›å¯ä»¥ç”¨æ±‚å’Œå½¢å¼è¡¨ç¤ºï¼Œå¹¶æå‡ºäº†æ··åˆå¤´æ³¨æ„åŠ›ï¼ˆMoHï¼‰æ¶æ„ï¼Œå°†æ³¨æ„åŠ›å¤´è§†ä¸ºä¸“å®¶ã€‚MoHå…è®¸æ¯ä¸ªæ ‡è®°é€‰æ‹©åˆé€‚çš„æ³¨æ„åŠ›å¤´ï¼Œæé«˜æ¨ç†æ•ˆç‡è€Œä¸å¢åŠ å‚æ•°æ•°é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒMoHåœ¨ä½¿ç”¨è¾ƒå°‘æ³¨æ„åŠ›å¤´çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼˜äºä¼ ç»Ÿå¤šå¤´æ³¨æ„åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13085",
            "title": "MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models",
            "url": "https://huggingface.co/papers/2410.13085",
            "abstract": "Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for interactive diagnostic tools. However, these models often suffer from factual hallucination, which can lead to incorrect diagnoses. Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to address these issues. However, the amount of high-quality data and distribution shifts between training data and deployment data limit the application of fine-tuning methods. Although RAG is lightweight and effective, existing RAG-based approaches are not sufficiently general to different medical domains and can potentially cause misalignment issues, both between modalities and between the model and the ground truth. In this paper, we propose a versatile multimodal RAG system, MMed-RAG, designed to enhance the factuality of Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an adaptive retrieved contexts selection method, and a provable RAG-based preference fine-tuning strategy. These innovations make the RAG process sufficiently general and reliable, significantly improving alignment when introducing retrieved contexts. Experimental results across five medical datasets (involving radiology, ophthalmology, pathology) on medical VQA and report generation demonstrate that MMed-RAG can achieve an average improvement of 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available in https://github.com/richard-peng-xia/MMed-RAG.",
            "score": 20,
            "issue_id": 146,
            "pub_date": "2024-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "8ef96c4ea4d54ffd",
            "authors": [
                "Peng Xia",
                "Kangyu Zhu",
                "Haoran Li",
                "Tianze Wang",
                "Weijia Shi",
                "Sheng Wang",
                "Linjun Zhang",
                "James Zou",
                "Huaxiu Yao"
            ],
            "affiliations": [
                "Brown University",
                "PloyU",
                "Rutgers University",
                "Stanford University",
                "UNC-Chapel Hill",
                "University of Washington"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13085.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#hallucinations",
                    "#benchmark",
                    "#cv",
                    "#multimodal",
                    "#healthcare",
                    "#data",
                    "#training",
                    "#dataset",
                    "#open_source",
                    "#alignment"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "MMed-RAG: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… AI-Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾Ğ·Ğ¾Ğ²",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° MMed-RAG, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (Med-LVLMs). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ RAG. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ€ĞµĞ´Ğ½ĞµĞµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Med-LVLMs Ğ½Ğ° 43.8%. MMed-RAG Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹, Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Enhancing Medical AI: MMed-RAG Boosts Diagnostic Accuracy",
                    "desc": "The paper discusses the development of MMed-RAG, a new system designed to improve the accuracy of Medical Large Vision-Language Models (Med-LVLMs) by addressing issues of factual hallucination. MMed-RAG uses a domain-aware retrieval mechanism and an adaptive context selection method to enhance the reliability of retrieval-augmented generation (RAG) processes. This approach ensures better alignment between the model's outputs and the ground truth across various medical domains. Experimental results show that MMed-RAG significantly boosts the factual accuracy of Med-LVLMs by 43.8% on average across multiple medical datasets."
                },
                "zh": {
                    "title": "MMed-RAGï¼šæå‡åŒ»å­¦AIæ¨¡å‹å‡†ç¡®æ€§çš„å¤šæ¨¡æ€è§£å†³æ–¹æ¡ˆ",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿï¼Œç§°ä¸ºMMed-RAGï¼Œæ—¨åœ¨æé«˜åŒ»å­¦å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å¼•å…¥é¢†åŸŸæ„ŸçŸ¥çš„æ£€ç´¢æœºåˆ¶ã€è‡ªé€‚åº”çš„æ£€ç´¢ä¸Šä¸‹æ–‡é€‰æ‹©æ–¹æ³•ï¼Œä»¥åŠå¯è¯æ˜çš„åå¥½å¾®è°ƒç­–ç•¥æ¥å¢å¼ºæ¨¡å‹çš„å¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMMed-RAGåœ¨äº”ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—æé«˜äº†43.8%çš„äº‹å®å‡†ç¡®æ€§ã€‚æ­¤ç ”ç©¶ä¸ºåŒ»å­¦è¯Šæ–­å·¥å…·çš„å¼€å‘æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13804",
            "title": "BenTo: Benchmark Task Reduction with In-Context Transferability",
            "url": "https://huggingface.co/papers/2410.13804",
            "abstract": "Evaluating large language models (LLMs) is costly: it requires the generation and examination of LLM outputs on a large-scale benchmark of various tasks. This paper investigates how to efficiently reduce the tasks used to benchmark LLMs without affecting the evaluation quality. Our study reveals that task transferability and relevance provide critical information to identify the most representative subset of tasks via optimizing a facility location function. We propose a practically efficient metric for estimating the transferability between two tasks via in-context learning (ICL). By analyzing the pairwise transferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or FLAN) to 5% while inducing only a <4% difference to the evaluation on the original benchmark. Compared to prior works, our method is training-free, gradient-free, and highly efficient requiring ICL only.",
            "score": 19,
            "issue_id": 147,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "e8177fd577296e7e",
            "authors": [
                "Hongyu Zhao",
                "Ming Li",
                "Lichao Sun",
                "Tianyi Zhou"
            ],
            "affiliations": [
                "Lehigh University",
                "University of Maryland, College Park"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13804.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#transfer_learning",
                    "#benchmark"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ° Ğ¶Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ (ICL). ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… LLM Ğ´Ğ¾ 5% Ñ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†ĞµĞ¹ Ğ¼ĞµĞ½ĞµĞµ 4% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼."
                },
                "en": {
                    "title": "Efficient LLM Evaluation: Less is More",
                    "desc": "This paper explores a method to make evaluating large language models (LLMs) more efficient by reducing the number of tasks needed for benchmarking. It introduces a way to identify the most important tasks using task transferability and relevance, optimizing a facility location function. The authors propose a metric to estimate how well tasks transfer to each other using in-context learning, which helps in selecting a smaller set of tasks. Their approach can cut down the tasks in benchmarks like MMLU or FLAN to just 5% of the original, with minimal impact on evaluation quality, and it doesn't require any training or gradients."
                },
                "zh": {
                    "title": "é«˜æ•ˆè¯„ä¼°ï¼šå‡å°‘ä»»åŠ¡ï¼Œä¿æŒè´¨é‡",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶å¦‚ä½•åœ¨ä¸å½±å“è¯„ä¼°è´¨é‡çš„æƒ…å†µä¸‹ï¼Œå‡å°‘ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡æ•°é‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»»åŠ¡çš„å¯è½¬ç§»æ€§å’Œç›¸å…³æ€§æ˜¯è¯†åˆ«æœ€å…·ä»£è¡¨æ€§ä»»åŠ¡å­é›†çš„å…³é”®ã€‚é€šè¿‡ä¼˜åŒ–è®¾æ–½ä½ç½®å‡½æ•°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æŒ‡æ ‡æ¥ä¼°è®¡ä»»åŠ¡ä¹‹é—´çš„å¯è½¬ç§»æ€§ã€‚åˆ†æä»»åŠ¡é—´çš„å¯è½¬ç§»æ€§ï¼Œå¯ä»¥å°†ç°ä»£LLMåŸºå‡†ä¸­çš„ä»»åŠ¡å‡å°‘åˆ°5%ï¼Œè€Œè¯„ä¼°ç»“æœä»…æœ‰ä¸åˆ°4%çš„å·®å¼‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13785",
            "title": "PopAlign: Diversifying Contrasting Patterns for a More Comprehensive Alignment",
            "url": "https://huggingface.co/papers/2410.13785",
            "abstract": "Alignment of large language models (LLMs) involves training models on preference-contrastive output pairs to adjust their responses according to human preferences. To obtain such contrastive pairs, traditional methods like RLHF and RLAIF rely on limited contrasting patterns, such as varying model variants or decoding temperatures. This singularity leads to two issues: (1) alignment is not comprehensive; and thereby (2) models are susceptible to jailbreaking attacks. To address these issues, we investigate how to construct more comprehensive and diversified contrasting patterns to enhance preference data (RQ1) and verify the impact of the diversification of contrasting patterns on model alignment (RQ2). For RQ1, we propose PopAlign, a framework that integrates diversified contrasting patterns across the prompt, model, and pipeline levels, introducing six contrasting strategies that do not require additional feedback labeling procedures. Regarding RQ2, we conduct thorough experiments demonstrating that PopAlign significantly outperforms existing methods, leading to more comprehensive alignment.",
            "score": 18,
            "issue_id": 147,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "d458841995668004",
            "authors": [
                "Zekun Moore Wang",
                "Shawn Wang",
                "Kang Zhu",
                "Jiaheng Liu",
                "Ke Xu",
                "Jie Fu",
                "Wangchunshu Zhou",
                "Wenhao Huang"
            ],
            "affiliations": [
                "201.AI",
                "AIWaves",
                "Beihang University",
                "HKUST",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13785.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#training",
                    "#security",
                    "#architecture",
                    "#alignment"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "PopAlign: ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ PopAlign. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ…. PopAlign Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº RLHF Ğ¸ RLAIF, Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ñ‚Ğ¸Ğ¿Ğ° jailbreaking. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PopAlign Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "PopAlign: Diversifying Patterns for Better Model Alignment",
                    "desc": "The paper discusses improving the alignment of large language models (LLMs) by using a new framework called PopAlign. Traditional methods like RLHF and RLAIF have limitations due to their reliance on limited contrasting patterns, which can make models vulnerable to jailbreaking attacks. PopAlign introduces diversified contrasting patterns across different levels, such as prompt, model, and pipeline, without needing extra feedback labeling. Experiments show that PopAlign enhances model alignment more effectively than existing methods, making models more robust and comprehensive."
                },
                "zh": {
                    "title": "PopAlignï¼šå¤šæ ·åŒ–å¯¹æ¯”ç­–ç•¥æå‡æ¨¡å‹å¯¹é½",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¦‚ä½•é€šè¿‡å¯¹æ¯”æ¨¡å¼æ¥æ›´å¥½åœ°è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼Œä½¿å…¶æ›´ç¬¦åˆäººç±»çš„åå¥½ã€‚ä¼ ç»Ÿæ–¹æ³•å¦‚RLHFå’ŒRLAIFåœ¨å¯¹æ¯”æ¨¡å¼ä¸Šå­˜åœ¨å±€é™æ€§ï¼Œå¯¼è‡´æ¨¡å‹å¯¹æ”»å‡»çš„è„†å¼±æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†PopAlignæ¡†æ¶ï¼Œé€šè¿‡åœ¨æç¤ºã€æ¨¡å‹å’Œæµç¨‹å±‚é¢å¼•å…¥å¤šæ ·åŒ–çš„å¯¹æ¯”ç­–ç•¥æ¥å¢å¼ºåå¥½æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPopAlignæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æ›´å…¨é¢çš„æ¨¡å‹å¯¹é½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13639",
            "title": "A Comparative Study on Reasoning Patterns of OpenAI's o1 Model",
            "url": "https://huggingface.co/papers/2410.13639",
            "abstract": "Enabling Large Language Models (LLMs) to handle a wider range of complex tasks (e.g., coding, math) has drawn great attention from many researchers. As LLMs continue to evolve, merely increasing the number of model parameters yields diminishing performance improvements and heavy computational costs. Recently, OpenAI's o1 model has shown that inference strategies (i.e., Test-time Compute methods) can also significantly enhance the reasoning capabilities of LLMs. However, the mechanisms behind these methods are still unexplored. In our work, to investigate the reasoning patterns of o1, we compare o1 with existing Test-time Compute methods (BoN, Step-wise BoN, Agent Workflow, and Self-Refine) by using OpenAI's GPT-4o as a backbone on general reasoning benchmarks in three domains (i.e., math, coding, commonsense reasoning). Specifically, first, our experiments show that the o1 model has achieved the best performance on most datasets. Second, as for the methods of searching diverse responses (e.g., BoN), we find the reward models' capability and the search space both limit the upper boundary of these methods. Third, as for the methods that break the problem into many sub-problems, the Agent Workflow has achieved better performance than Step-wise BoN due to the domain-specific system prompt for planning better reasoning processes. Fourth, it is worth mentioning that we have summarized six reasoning patterns of o1, and provided a detailed analysis on several reasoning benchmarks.",
            "score": 16,
            "issue_id": 162,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "39b4ff88e70ccaf2",
            "authors": [
                "Siwei Wu",
                "Zhongyuan Peng",
                "Xinrun Du",
                "Tuney Zheng",
                "Minghao Liu",
                "Jialong Wu",
                "Jiachen Ma",
                "Yizhi Li",
                "Jian Yang",
                "Wangchunshu Zhou",
                "Qunshu Lin",
                "Junbo Zhao",
                "Zhaoxiang Zhang",
                "Wenhao Huang",
                "Ge Zhang",
                "Chenghua Lin",
                "J. H. Liu"
            ],
            "affiliations": [
                "2077AI",
                "Abaka AI",
                "M-A-P",
                "OpenO1 Team",
                "University of Chinese Academy of Sciences",
                "University of Manchester",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13639.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rl",
                    "#benchmark",
                    "#inference",
                    "#optimization",
                    "#math",
                    "#plp"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞµĞºÑ€ĞµÑ‚Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ o1 Ğ¾Ñ‚ OpenAI Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑˆĞµÑÑ‚ÑŒ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ o1 Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¾ Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ o1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM."
                },
                "en": {
                    "title": "Smarter, Not Bigger: Enhancing LLMs with Inference Strategies",
                    "desc": "This paper explores how Large Language Models (LLMs) can be improved not just by adding more parameters, but by using smarter inference strategies. The study focuses on OpenAI's o1 model, comparing it with other Test-time Compute methods to understand its reasoning capabilities. The research finds that o1 outperforms other models in reasoning tasks across math, coding, and commonsense domains. It also identifies six reasoning patterns in o1, providing insights into how these models can be optimized for complex tasks."
                },
                "zh": {
                    "title": "æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ–°ç­–ç•¥",
                    "desc": "è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¦‚ä½•è®©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ›´å¥½åœ°å¤„ç†å¤æ‚ä»»åŠ¡ï¼Œå¦‚ç¼–ç¨‹å’Œæ•°å­¦ã€‚ç ”ç©¶å‘ç°ï¼Œä»…ä»…å¢åŠ æ¨¡å‹å‚æ•°çš„æ•°é‡å¹¶ä¸èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œåè€Œä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚é€šè¿‡æ¯”è¾ƒä¸åŒçš„æ¨ç†ç­–ç•¥ï¼Œå‘ç°o1æ¨¡å‹åœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³ï¼Œå¹¶æ€»ç»“äº†å…­ç§æ¨ç†æ¨¡å¼ã€‚ç ”ç©¶è¿˜æŒ‡å‡ºï¼Œæœç´¢å¤šæ ·åŒ–å“åº”çš„æ–¹æ³•å’Œå°†é—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæœ‰ä¸åŒçš„é™åˆ¶å’Œä¼˜åŠ¿ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13841",
            "title": "A Unified View of Delta Parameter Editing in Post-Trained Large-Scale Models",
            "url": "https://huggingface.co/papers/2410.13841",
            "abstract": "Post-training has emerged as a crucial paradigm for adapting large-scale pre-trained models to various tasks, whose effects are fully reflected by delta parameters (i.e., the disparity between post-trained and pre-trained parameters). While numerous studies have explored delta parameter properties via operations like pruning, quantization, low-rank approximation, and extrapolation, a unified framework for systematically examining these characteristics has been lacking. In this paper, we propose a novel perspective based on Riemann sum approximation of the loss function to elucidate delta parameter editing operations. Our analysis categorizes existing methods into three classes based on their post-editing performance: competitive, decreased, and improved, explaining how they are expressed by the Riemann sum approximation term and how they alter the model performance. Extensive experiments on both visual and language models, including ViT, LLaMA 3, Qwen 2, and Mistral, corroborate our theoretical findings. Furthermore, we introduce extensions to existing techniques like DARE and BitDelta, highlighting their limitations in leveraging the properties of delta parameters and reorganizing them into general expressions to enhance the applicability and effectiveness of delta parameter editing in post-trained models.",
            "score": 14,
            "issue_id": 146,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "d5678be0144ffffb",
            "authors": [
                "Qiaoyu Tang",
                "Le Yu",
                "Bowen Yu",
                "Hongyu Lin",
                "Keming Lu",
                "Yaojie Lu",
                "Xianpei Han",
                "Le Sun"
            ],
            "affiliations": [
                "Alibaba Group",
                "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13841.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#inference",
                    "#optimization",
                    "#training",
                    "#transfer_learning",
                    "#architecture"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ´ĞµĞ»ÑŒÑ‚Ğ°-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹: Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ Ğ¸Ğ¼Ğ°Ğ½Ğ° Ğ² Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ´ĞµĞ»ÑŒÑ‚Ğ°-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ Ğ¸Ğ¼Ğ°Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ»ÑŒÑ‚Ğ°-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ‚Ñ€Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¸Ñ… Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº ViT, LLaMA 3, Qwen 2 Ğ¸ Mistral. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ»ÑŒÑ‚Ğ°-Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…."
                },
                "en": {
                    "title": "Unlocking the Power of Delta Parameters in AI Models",
                    "desc": "The paper introduces a new way to understand how changes made to large pre-trained models, called delta parameters, affect their performance. By using a mathematical tool called Riemann sum approximation, the authors categorize different methods of editing these parameters into three groups based on how they impact the model's performance. They test their ideas on various models, showing that their approach helps explain why some methods work better than others. Additionally, they suggest improvements to existing techniques to make them more effective in adjusting these delta parameters."
                },
                "zh": {
                    "title": "æ­ç¤ºåè®­ç»ƒæ¨¡å‹ä¸­deltaå‚æ•°çš„å¥¥ç§˜",
                    "desc": "è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºé»æ›¼å’Œè¿‘ä¼¼æŸå¤±å‡½æ•°çš„æ–°è§†è§’æ¥è§£é‡Šåè®­ç»ƒæ¨¡å‹ä¸­çš„deltaå‚æ•°ç¼–è¾‘æ“ä½œã€‚ç ”ç©¶å°†ç°æœ‰æ–¹æ³•æ ¹æ®ç¼–è¾‘åæ€§èƒ½åˆ†ä¸ºä¸‰ç±»ï¼šç«äº‰æ€§ã€ä¸‹é™å’Œæå‡ï¼Œå¹¶é€šè¿‡é»æ›¼å’Œè¿‘ä¼¼é¡¹è§£é‡Šè¿™äº›æ–¹æ³•å¦‚ä½•å½±å“æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡åœ¨è§†è§‰å’Œè¯­è¨€æ¨¡å‹ä¸Šçš„å¤§é‡å®éªŒï¼ŒéªŒè¯äº†ç†è®ºå‘ç°çš„æ­£ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¯¹ç°æœ‰æŠ€æœ¯å¦‚DAREå’ŒBitDeltaè¿›è¡Œäº†æ‰©å±•ï¼ŒæŒ‡å‡ºå…¶åœ¨åˆ©ç”¨deltaå‚æ•°ç‰¹æ€§ä¸Šçš„å±€é™æ€§ï¼Œå¹¶å°†å…¶é‡æ–°ç»„ç»‡ä¸ºé€šç”¨è¡¨è¾¾å¼ä»¥æé«˜åè®­ç»ƒæ¨¡å‹ä¸­deltaå‚æ•°ç¼–è¾‘çš„é€‚ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13334",
            "title": "Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems",
            "url": "https://huggingface.co/papers/2410.13334",
            "abstract": "Although large language models (LLMs) demonstrate impressive proficiency in various tasks, they present potential safety risks, such as `jailbreaks', where malicious inputs can coerce LLMs into generating harmful content. To address these issues, many LLM developers have implemented various safety measures to align these models. This alignment involves several techniques, including data filtering during pre-training, supervised fine-tuning, reinforcement learning from human feedback, and red-teaming exercises. These methods often introduce deliberate and intentional biases similar to Political Correctness (PC) to ensure the ethical behavior of LLMs. In this paper, we delve into the intentional biases injected into LLMs for safety purposes and examine methods to circumvent these safety alignment techniques. Notably, these intentional biases result in a jailbreaking success rate in GPT-4o models that differs by 20% between non-binary and cisgender keywords and by 16% between white and black keywords, even when the other parts of the prompts are identical. We introduce the concept of PCJailbreak, highlighting the inherent risks posed by these safety-induced biases. Additionally, we propose an efficient defense method PCDefense, which prevents jailbreak attempts by injecting defense prompts prior to generation. PCDefense stands as an appealing alternative to Guard Models, such as Llama-Guard, that require additional inference cost after text generation. Our findings emphasize the urgent need for LLM developers to adopt a more responsible approach when designing and implementing safety measures.",
            "score": 12,
            "issue_id": 149,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "cc9053b7f6f516ca",
            "authors": [
                "Isack Lee",
                "Haebin Seong"
            ],
            "affiliations": [
                "Theori Inc."
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13334.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#inference",
                    "#ethics",
                    "#security",
                    "#architecture",
                    "#alignment"
                ],
                "emoji": "ğŸ›¡ï¸",
                "ru": {
                    "title": "Ğ‘Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ½Ğ¸: Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ğ¸ĞºĞ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ñ… Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ PCJailbreak, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ Ñ€Ğ¸ÑĞºĞ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ² LLM Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ñƒ Ğ² ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ GPT-4 Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ PCDefense, Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ğ²Ğ·Ğ»Ğ¾Ğ¼Ğ° Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Balancing Safety and Bias: Navigating the Ethical Landscape of LLMs",
                    "desc": "This paper explores the safety risks associated with large language models (LLMs), particularly focusing on 'jailbreaks' where malicious inputs lead to harmful outputs. It discusses how developers use techniques like data filtering, supervised fine-tuning, and reinforcement learning from human feedback to align LLMs with ethical standards, often introducing biases similar to Political Correctness. The study reveals that these biases can affect the success rate of jailbreaks, showing significant differences based on keywords related to gender and race. To counteract these vulnerabilities, the paper introduces PCDefense, a method that injects defense prompts to prevent jailbreaks without the additional costs associated with other guard models."
                },
                "zh": {
                    "title": "ç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨ä¸é“å¾·è¡Œä¸º",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ‰§è¡Œå„ç§ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†ä¹Ÿå­˜åœ¨å®‰å…¨é£é™©ï¼Œå¦‚é€šè¿‡æ¶æ„è¾“å…¥è¯±å¯¼ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œå¼€å‘è€…é‡‡ç”¨äº†æ•°æ®è¿‡æ»¤ã€ç›‘ç£å¾®è°ƒå’Œäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ç­‰æ–¹æ³•æ¥å¯¹é½æ¨¡å‹ã€‚è¿™äº›æ–¹æ³•å¼•å…¥äº†ç±»ä¼¼æ”¿æ²»æ­£ç¡®æ€§çš„åè§ï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„é“å¾·è¡Œä¸ºã€‚æœ¬æ–‡æ¢è®¨äº†è¿™äº›åè§çš„å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºPCDefenseçš„é˜²å¾¡æ–¹æ³•ï¼Œä»¥é˜²æ­¢æ¨¡å‹è¢«ç ´è§£ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13832",
            "title": "VidPanos: Generative Panoramic Videos from Casual Panning Videos",
            "url": "https://huggingface.co/papers/2410.13832",
            "abstract": "Panoramic image stitching provides a unified, wide-angle view of a scene that extends beyond the camera's field of view. Stitching frames of a panning video into a panoramic photograph is a well-understood problem for stationary scenes, but when objects are moving, a still panorama cannot capture the scene. We present a method for synthesizing a panoramic video from a casually-captured panning video, as if the original video were captured with a wide-angle camera. We pose panorama synthesis as a space-time outpainting problem, where we aim to create a full panoramic video of the same length as the input video. Consistent completion of the space-time volume requires a powerful, realistic prior over video content and motion, for which we adapt generative video models. Existing generative models do not, however, immediately extend to panorama completion, as we show. We instead apply video generation as a component of our panorama synthesis system, and demonstrate how to exploit the strengths of the models while minimizing their limitations. Our system can create video panoramas for a range of in-the-wild scenes including people, vehicles, and flowing water, as well as stationary background features.",
            "score": 12,
            "issue_id": 147,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "bbef29ff3d52bc13",
            "authors": [
                "Jingwei Ma",
                "Erika Lu",
                "Roni Paiss",
                "Shiran Zada",
                "Aleksander Holynski",
                "Tali Dekel",
                "Brian Curless",
                "Michael Rubinstein",
                "Forrester Cole"
            ],
            "affiliations": [
                "Google DeepMind, Israel",
                "Google DeepMind, USA",
                "UC Berkeley, USA",
                "University of Washington, USA",
                "Weitzmann Institute of Science, Israel"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13832.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#video",
                    "#games",
                    "#3d",
                    "#architecture"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞŸĞ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ¾Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ ÑÑŠĞµĞ¼ĞºĞ¸ Ğº ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ñƒ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ½ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ¼Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸ĞµÑÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ğ»ÑĞ´ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Expanding Horizons: Creating Panoramic Videos from Panning Footage",
                    "desc": "The paper introduces a novel method for creating panoramic videos from panning video footage, even when the scene includes moving objects. This is achieved by treating the problem as a space-time outpainting task, where the goal is to generate a complete panoramic video that matches the length of the original footage. The authors adapt generative video models to handle the complex task of filling in the missing parts of the video, ensuring realistic motion and content. They demonstrate the effectiveness of their system across various dynamic scenes, overcoming the limitations of existing generative models."
                },
                "zh": {
                    "title": "åŠ¨æ€åœºæ™¯çš„å…¨æ™¯è§†é¢‘åˆæˆæ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§ä»æ™®é€šè§†é¢‘åˆæˆå…¨æ™¯è§†é¢‘çš„æ–¹æ³•ï¼Œè§£å†³äº†ä¼ ç»Ÿå…¨æ™¯ç…§ç‰‡æ— æ³•æ•æ‰åŠ¨æ€åœºæ™¯çš„é—®é¢˜ã€‚ä½œè€…å°†å…¨æ™¯åˆæˆè§†ä¸ºä¸€ä¸ªæ—¶ç©ºå¤–æ¨é—®é¢˜ï¼Œåˆ©ç”¨ç”Ÿæˆè§†é¢‘æ¨¡å‹æ¥å®ç°è§†é¢‘å†…å®¹å’Œè¿åŠ¨çš„é€¼çœŸè¡¥å…¨ã€‚ç°æœ‰çš„ç”Ÿæˆæ¨¡å‹ä¸èƒ½ç›´æ¥ç”¨äºå…¨æ™¯è¡¥å…¨ï¼Œå› æ­¤ä½œè€…å°†å…¶ä½œä¸ºå…¨æ™¯åˆæˆç³»ç»Ÿçš„ä¸€éƒ¨åˆ†ï¼Œå‘æŒ¥æ¨¡å‹çš„ä¼˜åŠ¿å¹¶å‡å°‘å…¶å±€é™æ€§ã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿä¸ºåŒ…æ‹¬äººã€è½¦è¾†å’Œæµæ°´ç­‰åŠ¨æ€åœºæ™¯ç”Ÿæˆå…¨æ™¯è§†é¢‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09426",
            "title": "FlatQuant: Flatness Matters for LLM Quantization",
            "url": "https://huggingface.co/papers/2410.09426",
            "abstract": "Recently, quantization has been widely used for the compression and acceleration of large language models~(LLMs). Due to the outliers in LLMs, it is crucial to flatten weights and activations to minimize quantization error with the equally spaced quantization points. Prior research explores various pre-quantization transformations to suppress outliers, such as per-channel scaling and Hadamard transformation. However, we observe that these transformed weights and activations can still remain steep and outspread. In this paper, we propose FlatQuant (Fast and Learnable Affine Transformation), a new post-training quantization approach to enhance flatness of weights and activations. Our approach identifies optimal affine transformations tailored to each linear layer, calibrated in hours via a lightweight objective. To reduce runtime overhead, we apply Kronecker decomposition to the transformation matrices, and fuse all operations in FlatQuant into a single kernel. Extensive experiments show that FlatQuant sets up a new state-of-the-art quantization benchmark. For instance, it achieves less than 1% accuracy drop for W4A4 quantization on the LLaMA-3-70B model, surpassing SpinQuant by 7.5%. For inference latency, FlatQuant reduces the slowdown induced by pre-quantization transformation from 0.26x of QuaRot to merely 0.07x, bringing up to 2.3x speedup for prefill and 1.7x speedup for decoding, respectively. Code is available at: https://github.com/ruikangliu/FlatQuant.",
            "score": 12,
            "issue_id": 146,
            "pub_date": "2024-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "edfd47f3735a6a6e",
            "authors": [
                "Yuxuan Sun",
                "Ruikang Liu",
                "Haoli Bai",
                "Han Bao",
                "Kang Zhao",
                "Yuening Li",
                "Jiaxin Hu",
                "Xianzhi Yu",
                "Lu Hou",
                "Chun Yuan",
                "Xin Jiang",
                "Wulong Liu",
                "Jun Yao"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "Shenzhen International Graduate School, Tsinghua University",
                "The Chinese University of Hong Kong"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.09426.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#inference",
                    "#optimization",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ”¢",
                "ru": {
                    "title": "FlatQuant: Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "FlatQuant - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ³Ğ»Ğ°Ğ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ñ„Ñ„Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ, ĞºĞ°Ğ»Ğ¸Ğ±Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ·Ğ° Ñ‡Ğ°ÑÑ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸. Ğ”Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ ĞšÑ€Ğ¾Ğ½ĞµĞºĞµÑ€Ğ° Ğº Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğ°Ğ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. FlatQuant ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¼ĞµĞ½ĞµĞµ 1% Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ W4A4 Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaMA-3-70B."
                },
                "en": {
                    "title": "FlatQuant: Revolutionizing Model Quantization with Speed and Precision",
                    "desc": "The paper introduces FlatQuant, a novel post-training quantization method designed to improve the flatness of weights and activations in large language models. By using learnable affine transformations tailored to each linear layer, FlatQuant minimizes quantization errors and enhances model performance. The approach employs Kronecker decomposition to streamline operations, significantly reducing runtime overhead and improving inference speed. Extensive testing demonstrates that FlatQuant achieves superior accuracy and speed compared to existing methods, setting a new benchmark in model quantization."
                },
                "zh": {
                    "title": "FlatQuantï¼šæå‡é‡åŒ–ç²¾åº¦ä¸é€Ÿåº¦çš„æ–°æ–¹æ³•",
                    "desc": "é‡åŒ–æŠ€æœ¯è¢«å¹¿æ³›ç”¨äºå‹ç¼©å’ŒåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½†ç”±äºå¼‚å¸¸å€¼çš„å­˜åœ¨ï¼Œé‡åŒ–è¯¯å·®è¾ƒå¤§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åè®­ç»ƒé‡åŒ–æ–¹æ³•FlatQuantï¼Œé€šè¿‡å¿«é€Ÿå¯å­¦ä¹ çš„ä»¿å°„å˜æ¢æ¥å¢å¼ºæƒé‡å’Œæ¿€æ´»çš„å¹³å¦æ€§ã€‚FlatQuanté€šè¿‡å…‹ç½—å†…å…‹åˆ†è§£å‡å°‘è¿è¡Œæ—¶å¼€é”€ï¼Œå¹¶å°†æ‰€æœ‰æ“ä½œèåˆåˆ°ä¸€ä¸ªå†…æ ¸ä¸­ã€‚å®éªŒè¡¨æ˜ï¼ŒFlatQuantåœ¨é‡åŒ–åŸºå‡†ä¸Šè¾¾åˆ°äº†æ–°çš„æ°´å¹³ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ¨ç†é€Ÿåº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13198",
            "title": "Failing Forward: Improving Generative Error Correction for ASR with Synthetic Data and Retrieval Augmentation",
            "url": "https://huggingface.co/papers/2410.13198",
            "abstract": "Generative Error Correction (GEC) has emerged as a powerful post-processing method to enhance the performance of Automatic Speech Recognition (ASR) systems. However, we show that GEC models struggle to generalize beyond the specific types of errors encountered during training, limiting their ability to correct new, unseen errors at test time, particularly in out-of-domain (OOD) scenarios. This phenomenon amplifies with named entities (NEs), where, in addition to insufficient contextual information or knowledge about the NEs, novel NEs keep emerging. To address these issues, we propose DARAG (Data- and Retrieval-Augmented Generative Error Correction), a novel approach designed to improve GEC for ASR in in-domain (ID) and OOD scenarios. We augment the GEC training dataset with synthetic data generated by prompting LLMs and text-to-speech models, thereby simulating additional errors from which the model can learn. For OOD scenarios, we simulate test-time errors from new domains similarly and in an unsupervised fashion. Additionally, to better handle named entities, we introduce retrieval-augmented correction by augmenting the input with entities retrieved from a database. Our approach is simple, scalable, and both domain- and language-agnostic. We experiment on multiple datasets and settings, showing that DARAG outperforms all our baselines, achieving 8\\% -- 30\\% relative WER improvements in ID and 10\\% -- 33\\% improvements in OOD settings.",
            "score": 9,
            "issue_id": 146,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "0385e3248b804521",
            "authors": [
                "Sreyan Ghosh",
                "Mohammad Sadegh Rasooli",
                "Michael Levit",
                "Peidong Wang",
                "Jian Xue",
                "Dinesh Manocha",
                "Jinyu Li"
            ],
            "affiliations": [
                "Microsoft, USA",
                "University of Maryland, College Park, USA"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13198.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#synthetic",
                    "#multilingual",
                    "#data",
                    "#dataset",
                    "#transfer_learning",
                    "#games",
                    "#audio"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "DARAG: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº ASR Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ DARAG Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº (GEC) Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (ASR). DARAG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ text-to-speech, Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹. DARAG Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ²."
                },
                "en": {
                    "title": "DARAG: Revolutionizing Error Correction in Speech Recognition",
                    "desc": "The paper discusses a new method called DARAG, which enhances Generative Error Correction (GEC) for Automatic Speech Recognition (ASR) systems. Traditional GEC models struggle with unseen errors, especially in out-of-domain scenarios, but DARAG addresses this by using synthetic data and retrieval-augmented correction. By generating additional training data and retrieving relevant named entities, DARAG improves the model's ability to correct errors in both familiar and new contexts. Experiments show that DARAG significantly reduces word error rates, making it a robust solution for diverse ASR applications."
                },
                "zh": {
                    "title": "DARAGï¼šæå‡è¯­éŸ³è¯†åˆ«é”™è¯¯æ ¡æ­£çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDARAGçš„æ–°æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„ç”Ÿæˆå¼é”™è¯¯æ ¡æ­£ã€‚ä¼ ç»Ÿçš„ç”Ÿæˆå¼é”™è¯¯æ ¡æ­£æ¨¡å‹åœ¨é¢å¯¹æœªè§è¿‡çš„é”™è¯¯æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨è·¨é¢†åŸŸåœºæ™¯ä¸­ã€‚DARAGé€šè¿‡ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®æ¥å¢å¼ºè®­ç»ƒæ•°æ®é›†ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ›´å¥½åœ°å¤„ç†å‘½åå®ä½“ï¼ŒDARAGå¼•å…¥äº†æ£€ç´¢å¢å¼ºæ ¡æ­£ï¼Œé€šè¿‡ä»æ•°æ®åº“ä¸­æ£€ç´¢å®ä½“æ¥ä¸°å¯Œè¾“å…¥ä¿¡æ¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13854",
            "title": "Can MLLMs Understand the Deep Implication Behind Chinese Images?",
            "url": "https://huggingface.co/papers/2410.13854",
            "abstract": "As the capabilities of Multimodal Large Language Models (MLLMs) continue to improve, the need for higher-order capability evaluation of MLLMs is increasing. However, there is a lack of work evaluating MLLM for higher-order perception and understanding of Chinese visual content. To fill the gap, we introduce the **C**hinese **I**mage **I**mplication understanding **Bench**mark, **CII-Bench**, which aims to assess the higher-order perception and understanding capabilities of MLLMs for Chinese images. CII-Bench stands out in several ways compared to existing benchmarks. Firstly, to ensure the authenticity of the Chinese context, images in CII-Bench are sourced from the Chinese Internet and manually reviewed, with corresponding answers also manually crafted. Additionally, CII-Bench incorporates images that represent Chinese traditional culture, such as famous Chinese traditional paintings, which can deeply reflect the model's understanding of Chinese traditional culture. Through extensive experiments on CII-Bench across multiple MLLMs, we have made significant findings. Initially, a substantial gap is observed between the performance of MLLMs and humans on CII-Bench. The highest accuracy of MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at an impressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditional culture images, suggesting limitations in their ability to understand high-level semantics and lack a deep knowledge base of Chinese traditional culture. Finally, it is observed that most models exhibit enhanced accuracy when image emotion hints are incorporated into the prompts. We believe that CII-Bench will enable MLLMs to gain a better understanding of Chinese semantics and Chinese-specific images, advancing the journey towards expert artificial general intelligence (AGI). Our project is publicly available at https://cii-bench.github.io/.",
            "score": 8,
            "issue_id": 150,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "bea0da184db20058",
            "authors": [
                "Chenhao Zhang",
                "Xi Feng",
                "Yuelin Bai",
                "Xinrun Du",
                "Jinchang Hou",
                "Kaixin Deng",
                "Guangzeng Han",
                "Qinrui Li",
                "Bingli Wang",
                "Jiaheng Liu",
                "Xingwei Qu",
                "Yifei Zhang",
                "Qixuan Zhao",
                "Yiming Liang",
                "Ziqiang Liu",
                "Feiteng Fang",
                "Min Yang",
                "Wenhao Huang",
                "Chenghua Lin",
                "Ge Zhang",
                "Shiwen Ni"
            ],
            "affiliations": [
                "CDUT",
                "Huazhong University of Science and Technology",
                "M-A-P 501.ai",
                "SICAU",
                "SWU",
                "Shenzhen Institute of Advanced Technology, CAS",
                "UCAS",
                "University of California, Santa Barbara",
                "University of Manchester",
                "University of Memphis",
                "University of Science and Technology of China"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13854.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#benchmark",
                    "#multilingual",
                    "#cv",
                    "#agi",
                    "#multimodal",
                    "#open_source"
                ],
                "emoji": "ğŸ‡¨ğŸ‡³",
                "ru": {
                    "title": "CII-Bench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ MLLM",
                    "desc": "CII-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°ÑƒÑ‚ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ² Ñ‚Ğ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ MLLM Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ½Ğ° CII-Bench, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¹ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Bridging the Cultural Gap in AI Understanding",
                    "desc": "The paper introduces CII-Bench, a benchmark designed to evaluate the higher-order perception and understanding capabilities of Multimodal Large Language Models (MLLMs) for Chinese visual content. CII-Bench uses images sourced from the Chinese Internet and includes traditional cultural elements to test the models' understanding of Chinese culture. Experiments reveal that MLLMs perform significantly worse than humans, especially on images related to Chinese traditional culture, indicating a gap in high-level semantic understanding. The study also finds that MLLMs improve in accuracy when emotional cues are included in prompts, suggesting a potential area for enhancing model performance."
                },
                "zh": {
                    "title": "æå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ä¸­æ–‡å›¾åƒç†è§£èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCII-Benchçš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å¯¹ä¸­æ–‡å›¾åƒçš„é«˜çº§æ„ŸçŸ¥å’Œç†è§£èƒ½åŠ›ã€‚CII-Benchçš„å›¾åƒæ¥è‡ªä¸­å›½äº’è”ç½‘ï¼Œå¹¶ç»è¿‡äººå·¥å®¡æ ¸ï¼Œç¡®ä¿äº†ä¸­æ–‡è¯­å¢ƒçš„çœŸå®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨ç†è§£ä¸­å›½ä¼ ç»Ÿæ–‡åŒ–å›¾åƒæ—¶è¡¨ç°è¾ƒå·®ï¼Œè¡¨æ˜å…¶åœ¨é«˜å±‚æ¬¡è¯­ä¹‰ç†è§£å’Œæ–‡åŒ–çŸ¥è¯†æ–¹é¢çš„ä¸è¶³ã€‚é€šè¿‡CII-Benchçš„æµ‹è¯•ï¼Œå¯ä»¥å¸®åŠ©è¿™äº›æ¨¡å‹æ›´å¥½åœ°ç†è§£ä¸­æ–‡è¯­ä¹‰å’Œç‰¹å®šçš„ä¸­æ–‡å›¾åƒã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13360",
            "title": "Remember, Retrieve and Generate: Understanding Infinite Visual Concepts as Your Personalized Assistant",
            "url": "https://huggingface.co/papers/2410.13360",
            "abstract": "The development of large language models (LLMs) has significantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user-specific knowledge still restricts their application in human's daily life. In this paper, we introduce the Retrieval Augmented Personalization (RAP) framework for MLLMs' personalization. Starting from a general MLLM, we turn it into a personalized assistant in three steps. (a) Remember: We design a key-value database to store user-related information, e.g., user's name, avatar and other attributes. (b) Retrieve: When the user initiates a conversation, RAP will retrieve relevant information from the database using a multimodal retriever. (c) Generate: The input query and retrieved concepts' information are fed into MLLMs to generate personalized, knowledge-augmented responses. Unlike previous methods, RAP allows real-time concept editing via updating the external database. To further improve generation quality and alignment with user-specific information, we design a pipeline for data collection and create a specialized dataset for personalized training of MLLMs. Based on the dataset, we train a series of MLLMs as personalized multimodal assistants. By pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual concepts without additional finetuning. Our models demonstrate outstanding flexibility and generation quality across a variety of tasks, such as personalized image captioning, question answering and visual recognition. The code, data and models are available at https://github.com/Hoar012/RAP-MLLM.",
            "score": 8,
            "issue_id": 150,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "2355c49dae930058",
            "authors": [
                "Haoran Hao",
                "Jiaming Han",
                "Changsheng Li",
                "Yu-Feng Li",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "Beijing Institute of Technology",
                "MMLab, The Chinese University of Hong Kong",
                "National Key Laboratory for Novel Software Technology, Nanjing University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13360.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#synthetic",
                    "#agi",
                    "#multimodal",
                    "#data",
                    "#training",
                    "#dataset",
                    "#open_source",
                    "#alignment"
                ],
                "emoji": "ğŸ‘¤",
                "ru": {
                    "title": "ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RAP",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Retrieval Augmented Personalization (RAP) Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). RAP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğµ, Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ MLLM Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ."
                },
                "en": {
                    "title": "Personalized AI: Tailoring Multimodal Models to You",
                    "desc": "The paper introduces the Retrieval Augmented Personalization (RAP) framework to enhance the personalization of multimodal large language models (MLLMs). RAP uses a key-value database to store user-specific information, which is retrieved during interactions to generate personalized responses. This approach allows for real-time updates and editing of user data, improving the model's ability to align with user-specific needs. The framework is trained on a specialized dataset, enabling the models to perform tasks like personalized image captioning and question answering with high flexibility and quality."
                },
                "zh": {
                    "title": "RAPï¼šä¸ªæ€§åŒ–å¤šæ¨¡æ€åŠ©æ‰‹çš„æœªæ¥",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºRAPçš„æ¡†æ¶ï¼Œç”¨äºä¸ªæ€§åŒ–å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚RAPé€šè¿‡è®°å¿†ã€æ£€ç´¢å’Œç”Ÿæˆä¸‰ä¸ªæ­¥éª¤ï¼Œå°†é€šç”¨çš„MLLMè½¬å˜ä¸ºä¸ªæ€§åŒ–åŠ©æ‰‹ã€‚å®ƒä½¿ç”¨ä¸€ä¸ªé”®å€¼æ•°æ®åº“å­˜å‚¨ç”¨æˆ·ä¿¡æ¯ï¼Œå¹¶åœ¨å¯¹è¯æ—¶æ£€ç´¢ç›¸å…³ä¿¡æ¯ä»¥ç”Ÿæˆä¸ªæ€§åŒ–å“åº”ã€‚RAPçš„åˆ›æ–°åœ¨äºå…è®¸å®æ—¶ç¼–è¾‘æ¦‚å¿µï¼Œå¹¶é€šè¿‡ä¸“é—¨çš„æ•°æ®é›†æé«˜ç”Ÿæˆè´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09019",
            "title": "MedMobile: A mobile-sized language model with expert-level clinical capabilities",
            "url": "https://huggingface.co/papers/2410.09019",
            "abstract": "Language models (LMs) have demonstrated expert-level reasoning and recall abilities in medicine. However, computational costs and privacy concerns are mounting barriers to wide-scale implementation. We introduce a parsimonious adaptation of phi-3-mini, MedMobile, a 3.8 billion parameter LM capable of running on a mobile device, for medical applications. We demonstrate that MedMobile scores 75.7% on the MedQA (USMLE), surpassing the passing mark for physicians (~60%), and approaching the scores of models 100 times its size. We subsequently perform a careful set of ablations, and demonstrate that chain of thought, ensembling, and fine-tuning lead to the greatest performance gains, while unexpectedly retrieval augmented generation fails to demonstrate significant improvements",
            "score": 8,
            "issue_id": 147,
            "pub_date": "2024-10-11",
            "pub_date_card": {
                "ru": "11 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 11",
                "zh": "10æœˆ11æ—¥"
            },
            "hash": "dcae17e6b8ab798e",
            "authors": [
                "Krithik Vishwanath",
                "Jaden Stryker",
                "Anton Alyakin",
                "Daniel Alexander Alber",
                "Eric Karl Oermann"
            ],
            "affiliations": [
                "Center for Data Science, New York University, New York, New York, 10016",
                "Department of Aerospace Engineering and Engineering Mechanics, The University of Texas at Austin, Austin, Texas, 78712",
                "Department of Neurological Surgery, NYU Langone Medical Center, New York, New York, 10016",
                "Department of Neurosurgery, Washington University School of Medicine in St. Louis, St. Louis, Missouri, 63110",
                "Department of Radiology, NYU Langone Medical Center, New York, New York, 10016"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.09019.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#rag",
                    "#small_models",
                    "#reasoning",
                    "#optimization",
                    "#healthcare",
                    "#training"
                ],
                "emoji": "ğŸ©º",
                "ru": {
                    "title": "ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²Ñ€Ğ°Ñ‡ĞµĞ¹ Ğ² Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MedMobile - ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° phi-3-mini, Ğ¸Ğ¼ĞµĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 3,8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ½Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğµ MedQA (USMLE), Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ»Ğ» Ğ´Ğ»Ñ Ğ²Ñ€Ğ°Ñ‡ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¼Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ retrieval augmented generation Ğ½Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹."
                },
                "en": {
                    "title": "MedMobile: Compact Power for Medical AI on the Go",
                    "desc": "The paper introduces MedMobile, a compact language model designed for medical applications that can operate on mobile devices, addressing issues of computational cost and privacy. MedMobile, with 3.8 billion parameters, achieves a 75.7% score on the MedQA exam, surpassing the physician passing mark and nearing the performance of much larger models. The study finds that techniques like chain of thought, ensembling, and fine-tuning significantly enhance the model's performance. Interestingly, retrieval augmented generation does not provide notable improvements, contrary to expectations."
                },
                "zh": {
                    "title": "å°è€Œå¼ºå¤§çš„åŒ»å­¦è¯­è¨€æ¨¡å‹ï¼šMedMobile",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMedMobileçš„è¯­è¨€æ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¿è¡Œï¼Œå¹¶åœ¨åŒ»å­¦åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ã€‚MedMobileåœ¨MedQAæµ‹è¯•ä¸­å¾—åˆ†75.7%ï¼Œè¶…è¿‡äº†åŒ»ç”Ÿçš„åŠæ ¼çº¿ï¼Œå¹¶æ¥è¿‘æ¯”å®ƒå¤§100å€çš„æ¨¡å‹çš„å¾—åˆ†ã€‚ç ”ç©¶å‘ç°ï¼Œæ€ç»´é“¾ã€é›†æˆå’Œå¾®è°ƒæ˜¯æé«˜æ€§èƒ½çš„å…³é”®ï¼Œè€Œæ£€ç´¢å¢å¼ºç”Ÿæˆæœªèƒ½æ˜¾è‘—æ”¹å–„ç»“æœã€‚è¯¥æ¨¡å‹çš„è®¾è®¡æ—¨åœ¨é™ä½è®¡ç®—æˆæœ¬å’Œè§£å†³éšç§é—®é¢˜ï¼Œä½¿å…¶æ›´æ˜“äºå¹¿æ³›åº”ç”¨ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13852",
            "title": "Retrospective Learning from Interactions",
            "url": "https://huggingface.co/papers/2410.13852",
            "abstract": "Multi-turn interactions between large language models (LLMs) and users naturally include implicit feedback signals. If an LLM responds in an unexpected way to an instruction, the user is likely to signal it by rephrasing the request, expressing frustration, or pivoting to an alternative task. Such signals are task-independent and occupy a relatively constrained subspace of language, allowing the LLM to identify them even if it fails on the actual task. This creates an avenue for continually learning from interactions without additional annotations. We introduce ReSpect, a method to learn from such signals in past interactions via retrospection. We deploy ReSpect in a new multimodal interaction scenario, where humans instruct an LLM to solve an abstract reasoning task with a combinatorial solution space. Through thousands of interactions with humans, we show how ReSpect gradually improves task completion rate from 31% to 82%, all without any external annotation.",
            "score": 8,
            "issue_id": 146,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "e59c253e0efcaa3b",
            "authors": [
                "Zizhao Chen",
                "Mustafa Omer Gul",
                "Yiwei Chen",
                "Gloria Geng",
                "Anne Wu",
                "Yoav Artzi"
            ],
            "affiliations": [
                "Department of Computer Science and Cornell Tech, Cornell University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13852.jpg",
            "data": {
                "categories": [
                    "#reasoning",
                    "#rlhf",
                    "#multimodal",
                    "#training",
                    "#alignment"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ReSpect, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (LLM) ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ² Ñ…Ğ¾Ğ´Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ ReSpect Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ³Ğ´Ğµ Ğ»ÑĞ´Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‚ LLM Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ReSpect Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ 31% Ğ´Ğ¾ 82% Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ­Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸."
                },
                "en": {
                    "title": "Learning from Implicit Feedback: Enhancing LLMs with ReSpect",
                    "desc": "The paper discusses how large language models (LLMs) can learn from implicit feedback during multi-turn interactions with users. When users rephrase requests or express frustration, these signals can be detected by the LLM, even if it doesn't understand the task. The authors introduce a method called ReSpect, which allows LLMs to learn from these signals retrospectively, improving their performance over time. In a multimodal interaction scenario, ReSpect significantly increased task completion rates from 31% to 82% without needing additional annotations."
                },
                "zh": {
                    "title": "é€šè¿‡å›é¡¾ä¿¡å·ï¼Œæå‡è¯­è¨€æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºReSpectçš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡å›é¡¾è¿‡å»çš„äº’åŠ¨ä¿¡å·æ¥å­¦ä¹ ã€‚ç”¨æˆ·åœ¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹äº’åŠ¨æ—¶ï¼Œå¯èƒ½ä¼šé€šè¿‡é‡æ–°æªè¾æˆ–è¡¨è¾¾æ²®ä¸§æ¥æä¾›éšæ€§åé¦ˆã€‚ReSpectåˆ©ç”¨è¿™äº›ä¿¡å·æ¥æé«˜æ¨¡å‹çš„ä»»åŠ¡å®Œæˆç‡ï¼Œä»31%æå‡åˆ°82%ã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦é¢å¤–çš„æ ‡æ³¨ï¼Œå±•ç¤ºäº†åœ¨å¤šæ¨¡æ€äº’åŠ¨åœºæ™¯ä¸­æŒç»­å­¦ä¹ çš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13859",
            "title": "$Î³-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models",
            "url": "https://huggingface.co/papers/2410.13859",
            "abstract": "Despite the significant progress in multimodal large language models (MLLMs), their high computational cost remains a barrier to real-world deployment. Inspired by the mixture of depths (MoDs) in natural language processing, we aim to address this limitation from the perspective of ``activated tokens''. Our key insight is that if most tokens are redundant for the layer computation, then can be skipped directly via the MoD layer. However, directly converting the dense layers of MLLMs to MoD layers leads to substantial performance degradation. To address this issue, we propose an innovative MoD adaptation strategy for existing MLLMs called gamma-MoD. In gamma-MoD, a novel metric is proposed to guide the deployment of MoDs in the MLLM, namely rank of attention maps (ARank). Through ARank, we can effectively identify which layer is redundant and should be replaced with the MoD layer. Based on ARank, we further propose two novel designs to maximize the computational sparsity of MLLM while maintaining its performance, namely shared vision-language router and masked routing learning. With these designs, more than 90% dense layers of the MLLM can be effectively converted to the MoD ones. To validate our method, we apply it to three popular MLLMs, and conduct extensive experiments on 9 benchmark datasets. Experimental results not only validate the significant efficiency benefit of gamma-MoD to existing MLLMs but also confirm its generalization ability on various MLLMs. For example, with a minor performance drop, i.e., -1.5%, gamma-MoD can reduce the training and inference time of LLaVA-HR by 31.0% and 53.2%, respectively.",
            "score": 7,
            "issue_id": 150,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "f393a253d43ee89d",
            "authors": [
                "Yaxin Luo",
                "Gen Luo",
                "Jiayi Ji",
                "Yiyi Zhou",
                "Xiaoshuai Sun",
                "Zhiqiang Shen",
                "Rongrong Ji"
            ],
            "affiliations": [
                "MBZUAI",
                "National University of Singapore",
                "OpenGVLab, Shanghai AI Laboratory",
                "Technical University Of Denmark",
                "Xiamen University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13859.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#inference",
                    "#optimization",
                    "#multimodal",
                    "#training",
                    "#architecture"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ gamma-MoD Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Mixture of Depths (MoD) Ğ´Ğ»Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… MLLM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ñ€Ğ°Ğ½Ğ³Ğ° ĞºĞ°Ñ€Ñ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ (ARank) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 9 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ MLLM Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Efficient Multimodal Models: Cutting Costs, Not Performance",
                    "desc": "The paper addresses the high computational cost of multimodal large language models (MLLMs) by introducing a method called gamma-MoD, inspired by the mixture of depths (MoDs) concept. Gamma-MoD uses a novel metric, ARank, to identify and replace redundant layers in MLLMs with more efficient MoD layers, significantly reducing computational demands. The approach includes innovative designs like shared vision-language router and masked routing learning to maintain model performance while achieving computational sparsity. Experiments on various MLLMs demonstrate that gamma-MoD can reduce training and inference times substantially with minimal performance loss."
                },
                "zh": {
                    "title": "gamma-MoDï¼šæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ•ˆç‡çš„æ–°ç­–ç•¥",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºgamma-MoDçš„åˆ›æ–°ç­–ç•¥ã€‚é€šè¿‡å¼•å…¥æ³¨æ„åŠ›å›¾çš„ç§©ï¼ˆARankï¼‰ä½œä¸ºæŒ‡æ ‡ï¼Œgamma-MoDèƒ½å¤Ÿè¯†åˆ«å“ªäº›å±‚æ˜¯å†—ä½™çš„ï¼Œå¹¶ç”¨æ··åˆæ·±åº¦ï¼ˆMoDï¼‰å±‚æ›¿æ¢ã€‚è¯¥æ–¹æ³•é€šè¿‡å…±äº«è§†è§‰-è¯­è¨€è·¯ç”±å™¨å’Œæ©ç è·¯ç”±å­¦ä¹ è®¾è®¡ï¼ŒæˆåŠŸå°†è¶…è¿‡90%çš„å¯†é›†å±‚è½¬æ¢ä¸ºMoDå±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œgamma-MoDåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œå¤§å¹…æé«˜äº†MLLMsçš„è®¡ç®—æ•ˆç‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.12957",
            "title": "MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic Synchronization",
            "url": "https://huggingface.co/papers/2410.12957",
            "abstract": "Generating music that aligns with the visual content of a video has been a challenging task, as it requires a deep understanding of visual semantics and involves generating music whose melody, rhythm, and dynamics harmonize with the visual narratives. This paper presents MuVi, a novel framework that effectively addresses these challenges to enhance the cohesion and immersive experience of audio-visual content. MuVi analyzes video content through a specially designed visual adaptor to extract contextually and temporally relevant features. These features are used to generate music that not only matches the video's mood and theme but also its rhythm and pacing. We also introduce a contrastive music-visual pre-training scheme to ensure synchronization, based on the periodicity nature of music phrases. In addition, we demonstrate that our flow-matching-based music generator has in-context learning ability, allowing us to control the style and genre of the generated music. Experimental results show that MuVi demonstrates superior performance in both audio quality and temporal synchronization. The generated music video samples are available at https://muvi-v2m.github.io.",
            "score": 7,
            "issue_id": 149,
            "pub_date": "2024-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "6540fab2fcc644f2",
            "authors": [
                "Ruiqi Li",
                "Siqi Zheng",
                "Xize Cheng",
                "Ziang Zhang",
                "Shengpeng Ji",
                "Zhou Zhao"
            ],
            "affiliations": [
                "Alibaba Group",
                "Zhejiang University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.12957.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#synthetic",
                    "#video",
                    "#multimodal",
                    "#training",
                    "#transfer_learning",
                    "#games",
                    "#audio",
                    "#architecture"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "MuVi: Ğ“Ğ°Ñ€Ğ¼Ğ¾Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚",
                    "desc": "MuVi - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ¼. ĞĞ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº. Ğ—Ğ°Ñ‚ĞµĞ¼ ÑÑ‚Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞ¼Ğµ, Ñ€Ğ¸Ñ‚Ğ¼Ñƒ Ğ¸ Ñ‚ĞµĞ¼Ğ¿Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ."
                },
                "en": {
                    "title": "Harmonizing Visuals and Sound: MuVi's Magic",
                    "desc": "The paper introduces MuVi, a framework designed to generate music that aligns with the visual content of videos by understanding visual semantics. MuVi uses a visual adaptor to extract features from videos, ensuring the generated music matches the video's mood, theme, rhythm, and pacing. A contrastive music-visual pre-training scheme is employed to synchronize music with video, leveraging the periodic nature of music phrases. The framework also allows for control over the style and genre of the music, demonstrating superior performance in audio quality and synchronization."
                },
                "zh": {
                    "title": "MuViï¼šè®©éŸ³ä¹ä¸è§†è§‰å®Œç¾åŒæ­¥çš„åˆ›æ–°æ¡†æ¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºMuViçš„æ–°æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆä¸è§†é¢‘å†…å®¹ç›¸åŒ¹é…çš„éŸ³ä¹ã€‚MuVié€šè¿‡ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„è§†è§‰é€‚é…å™¨åˆ†æè§†é¢‘å†…å®¹ï¼Œæå–ä¸ä¸Šä¸‹æ–‡å’Œæ—¶é—´ç›¸å…³çš„ç‰¹å¾ã€‚è¿™äº›ç‰¹å¾ç”¨äºç”Ÿæˆä¸è§†é¢‘æƒ…ç»ªã€ä¸»é¢˜ã€èŠ‚å¥å’ŒèŠ‚æ‹ç›¸åŒ¹é…çš„éŸ³ä¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯¹æ¯”éŸ³ä¹-è§†è§‰é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œä»¥ç¡®ä¿éŸ³ä¹ä¸è§†é¢‘çš„åŒæ­¥æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.12771",
            "title": "Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models",
            "url": "https://huggingface.co/papers/2410.12771",
            "abstract": "The ability to discover new materials with desirable properties is critical for numerous applications from helping mitigate climate change to advances in next generation computing hardware. AI has the potential to accelerate materials discovery and design by more effectively exploring the chemical space compared to other computational methods or by trial-and-error. While substantial progress has been made on AI for materials data, benchmarks, and models, a barrier that has emerged is the lack of publicly available training data and open pre-trained models. To address this, we present a Meta FAIR release of the Open Materials 2024 (OMat24) large-scale open dataset and an accompanying set of pre-trained models. OMat24 contains over 110 million density functional theory (DFT) calculations focused on structural and compositional diversity. Our EquiformerV2 models achieve state-of-the-art performance on the Matbench Discovery leaderboard and are capable of predicting ground-state stability and formation energies to an F1 score above 0.9 and an accuracy of 20 meV/atom, respectively. We explore the impact of model size, auxiliary denoising objectives, and fine-tuning on performance across a range of datasets including OMat24, MPtraj, and Alexandria. The open release of the OMat24 dataset and models enables the research community to build upon our efforts and drive further advancements in AI-assisted materials science.",
            "score": 6,
            "issue_id": 154,
            "pub_date": "2024-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "a98981f16e047445",
            "authors": [
                "Luis Barroso-Luque",
                "Muhammed Shuaibi",
                "Xiang Fu",
                "Brandon M. Wood",
                "Misko Dzamba",
                "Meng Gao",
                "Ammar Rizvi",
                "C. Lawrence Zitnick",
                "Zachary W. Ulissi"
            ],
            "affiliations": [
                "Fundamental AI Research (FAIR) at Meta"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.12771.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#benchmark",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ§ª",
                "ru": {
                    "title": "Ğ˜Ğ˜ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Open Materials 2024 (OMat24) Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. OMat24 ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 110 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ¾Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»Ğ° Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ (DFT), Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¼ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ EquiformerV2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ»Ğ¸Ğ´ĞµÑ€Ğ±Ğ¾Ñ€Ğ´Ğµ Matbench Discovery, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…."
                },
                "en": {
                    "title": "Unlocking Material Innovation with AI: The OMat24 Revolution",
                    "desc": "The paper discusses the use of AI to accelerate the discovery of new materials by exploring chemical space more efficiently than traditional methods. It introduces the Open Materials 2024 (OMat24) dataset, which includes over 110 million density functional theory calculations, and a set of pre-trained models called EquiformerV2. These models achieve high performance in predicting material properties, such as ground-state stability, with impressive accuracy. The open release of this dataset and models aims to overcome the barrier of limited training data and foster further advancements in AI-driven materials science."
                },
                "zh": {
                    "title": "AIåŠ é€Ÿææ–™ç§‘å­¦æ–°çªç ´",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤§è§„æ¨¡å¼€æ”¾æ•°æ®é›†OMat24å’Œä¸€ç»„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç”¨äºåŠ é€Ÿææ–™å‘ç°å’Œè®¾è®¡ã€‚OMat24åŒ…å«è¶…è¿‡1.1äº¿ä¸ªå¯†åº¦æ³›å‡½ç†è®ºè®¡ç®—ï¼Œä¸“æ³¨äºç»“æ„å’Œæˆåˆ†çš„å¤šæ ·æ€§ã€‚EquiformerV2æ¨¡å‹åœ¨Matbench Discoveryæ’è¡Œæ¦œä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿé«˜æ•ˆé¢„æµ‹ææ–™çš„åŸºæ€ç¨³å®šæ€§å’Œå½¢æˆèƒ½ã€‚é€šè¿‡å…¬å¼€OMat24æ•°æ®é›†å’Œæ¨¡å‹ï¼Œç ”ç©¶ç¤¾åŒºå¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ¨åŠ¨AIè¾…åŠ©ææ–™ç§‘å­¦çš„å‘å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13618",
            "title": "LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for Parameter-Efficient Fine-Tuning",
            "url": "https://huggingface.co/papers/2410.13618",
            "abstract": "The rapid growth of model scale has necessitated substantial computational resources for fine-tuning. Existing approach such as Low-Rank Adaptation (LoRA) has sought to address the problem of handling the large updated parameters in full fine-tuning. However, LoRA utilize random initialization and optimization of low-rank matrices to approximate updated weights, which can result in suboptimal convergence and an accuracy gap compared to full fine-tuning. To address these issues, we propose LoLDU, a Parameter-Efficient Fine-Tuning (PEFT) approach that significantly reduces trainable parameters by 2600 times compared to regular PEFT methods while maintaining comparable performance. LoLDU leverages Lower-Diag-Upper Decomposition (LDU) to initialize low-rank matrices for faster convergence and orthogonality. We focus on optimizing the diagonal matrix for scaling transformations. To the best of our knowledge, LoLDU has the fewest parameters among all PEFT approaches. We conducted extensive experiments across 4 instruction-following datasets, 6 natural language understanding (NLU) datasets, 8 image classification datasets, and image generation datasets with multiple model types (LLaMA2, RoBERTa, ViT, and Stable Diffusion), providing a comprehensive and detailed analysis. Our open-source code can be accessed at https://github.com/SKDDJ/LoLDU{https://github.com/SKDDJ/LoLDU}.",
            "score": 6,
            "issue_id": 147,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "446dc7eb391be2b2",
            "authors": [
                "Yiming Shi",
                "Jiwei Wei",
                "Yujia Wu",
                "Ran Ran",
                "Chengwei Sun",
                "Shiyuan He",
                "Yang Yang"
            ],
            "affiliations": [],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13618.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#cv",
                    "#optimization",
                    "#training",
                    "#dataset",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "LoLDU: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² (PEFT) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LoLDU. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LDU-Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ³Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ñ€Ñ‚Ğ¾Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. LoLDU Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ PEFT, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "LoLDU: Fine-Tuning with Fewer Parameters, Same Power",
                    "desc": "The paper introduces LoLDU, a new method for fine-tuning machine learning models that uses fewer parameters than traditional methods. LoLDU employs a technique called Lower-Diag-Upper Decomposition to improve the initialization of low-rank matrices, which helps in achieving faster convergence and better performance. This approach significantly reduces the number of trainable parameters by 2600 times compared to other parameter-efficient fine-tuning methods, without sacrificing accuracy. Extensive experiments across various datasets and model types demonstrate the effectiveness of LoLDU in maintaining performance while being more efficient."
                },
                "zh": {
                    "title": "LoLDUï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒçš„æ–°çªç ´",
                    "desc": "éšç€æ¨¡å‹è§„æ¨¡çš„å¿«é€Ÿå¢é•¿ï¼Œå¾®è°ƒéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚ç°æœ‰çš„æ–¹æ³•å¦‚ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¯•å›¾è§£å†³å…¨é‡å¾®è°ƒä¸­å‚æ•°æ›´æ–°è¿‡å¤§çš„é—®é¢˜ï¼Œä½†å…¶éšæœºåˆå§‹åŒ–å¯èƒ½å¯¼è‡´æ”¶æ•›ä¸ä½³å’Œç²¾åº¦å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†LoLDUï¼Œä¸€ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡ä¸‹å¯¹è§’ä¸Šåˆ†è§£ï¼ˆLDUï¼‰æ¥åˆå§‹åŒ–ä½ç§©çŸ©é˜µï¼Œå®ç°æ›´å¿«çš„æ”¶æ•›å’Œæ­£äº¤æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒLoLDUåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå‚æ•°é‡æ˜¯æ‰€æœ‰PEFTæ–¹æ³•ä¸­æœ€å°‘çš„ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.12781",
            "title": "Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats",
            "url": "https://huggingface.co/papers/2410.12781",
            "abstract": "We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is capable of reconstructing a large scene from a long sequence of input images. Specifically, our model can process 32 source images at 960x540 resolution within only 1.3 seconds on a single A100 80G GPU. Our architecture features a mixture of the recent Mamba2 blocks and the classical transformer blocks which allowed many more tokens to be processed than prior work, enhanced by efficient token merging and Gaussian pruning steps that balance between quality and efficiency. Unlike previous feed-forward models that are limited to processing 1~4 input images and can only reconstruct a small portion of a large scene, Long-LRM reconstructs the entire scene in a single feed-forward step. On large-scale scene datasets such as DL3DV-140 and Tanks and Temples, our method achieves performance comparable to optimization-based approaches while being two orders of magnitude more efficient. Project page: https://arthurhero.github.io/projects/llrm",
            "score": 5,
            "issue_id": 150,
            "pub_date": "2024-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "0f75e8070394f973",
            "authors": [
                "Chen Ziwen",
                "Hao Tan",
                "Kai Zhang",
                "Sai Bi",
                "Fujun Luan",
                "Yicong Hong",
                "Li Fuxin",
                "Zexiang Xu"
            ],
            "affiliations": [
                "Adobe Research",
                "Oregon State University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.12781.jpg",
            "data": {
                "categories": [
                    "#long_context",
                    "#benchmark",
                    "#inference",
                    "#optimization",
                    "#dataset",
                    "#3d",
                    "#architecture"
                ],
                "emoji": "ğŸ™ï¸",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Long-LRM - ÑÑ‚Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑÑ†ĞµĞ½Ñ‹ Ğ¸Ğ· Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ 32 Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 960x540 Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 1,3 ÑĞµĞºÑƒĞ½Ğ´Ñ‹ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU A100 80G. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ±Ğ»Ğ¾ĞºĞ¸ Mamba2 Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. Long-LRM Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ğ²ÑÑ ÑÑ†ĞµĞ½Ñƒ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ² 100 Ñ€Ğ°Ğ· ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ."
                },
                "en": {
                    "title": "Revolutionizing 3D Scene Reconstruction with Speed and Efficiency",
                    "desc": "Long-LRM is a 3D Gaussian reconstruction model designed to handle large scenes from a sequence of images efficiently. It processes 32 high-resolution images in just 1.3 seconds using a single GPU, thanks to its innovative architecture combining Mamba2 and transformer blocks. The model uses token merging and Gaussian pruning to maintain a balance between quality and speed, allowing it to reconstruct entire scenes in one step. Long-LRM achieves results similar to optimization-based methods but is significantly faster, making it ideal for large-scale datasets."
                },
                "zh": {
                    "title": "é«˜æ•ˆé‡å»ºï¼šLong-LRMè®©å¤§åœºæ™¯é‡å»ºæ›´ç®€å•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLong-LRMçš„3Dé«˜æ–¯é‡å»ºæ¨¡å‹ï¼Œå¯ä»¥ä»é•¿åºåˆ—çš„è¾“å…¥å›¾åƒä¸­é‡å»ºå¤§å‹åœºæ™¯ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å•ä¸ªA100 80G GPUä¸Šä»¥1.3ç§’çš„é€Ÿåº¦å¤„ç†32å¼ 960x540åˆ†è¾¨ç‡çš„æºå›¾åƒã€‚å…¶æ¶æ„ç»“åˆäº†æœ€æ–°çš„Mamba2æ¨¡å—å’Œç»å…¸çš„Transformeræ¨¡å—ï¼Œé€šè¿‡é«˜æ•ˆçš„ä»¤ç‰Œåˆå¹¶å’Œé«˜æ–¯å‰ªææ­¥éª¤ï¼Œåœ¨è´¨é‡å’Œæ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ä¸ä¹‹å‰åªèƒ½å¤„ç†1åˆ°4å¼ å›¾åƒçš„å‰é¦ˆæ¨¡å‹ä¸åŒï¼ŒLong-LRMå¯ä»¥åœ¨ä¸€æ¬¡å‰é¦ˆæ­¥éª¤ä¸­é‡å»ºæ•´ä¸ªåœºæ™¯ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.09347",
            "title": "Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment",
            "url": "https://huggingface.co/papers/2410.09347",
            "abstract": "Classifier-Free Guidance (CFG) is a critical technique for enhancing the sample quality of visual generative models. However, in autoregressive (AR) multi-modal generation, CFG introduces design inconsistencies between language and visual content, contradicting the design philosophy of unifying different modalities for visual AR. Motivated by language model alignment methods, we propose Condition Contrastive Alignment (CCA) to facilitate guidance-free AR visual generation with high performance and analyze its theoretical connection with guided sampling methods. Unlike guidance methods that alter the sampling process to achieve the ideal sampling distribution, CCA directly fine-tunes pretrained models to fit the same distribution target. Experimental results show that CCA can significantly enhance the guidance-free performance of all tested models with just one epoch of fine-tuning (sim 1\\% of pretraining epochs) on the pretraining dataset, on par with guided sampling methods. This largely removes the need for guided sampling in AR visual generation and cuts the sampling cost by half. Moreover, by adjusting training parameters, CCA can achieve trade-offs between sample diversity and fidelity similar to CFG. This experimentally confirms the strong theoretical connection between language-targeted alignment and visual-targeted guidance methods, unifying two previously independent research fields. Code and model weights: https://github.com/thu-ml/CCA.",
            "score": 4,
            "issue_id": 150,
            "pub_date": "2024-10-12",
            "pub_date_card": {
                "ru": "12 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 12",
                "zh": "10æœˆ12æ—¥"
            },
            "hash": "aea2fc78baf79a07",
            "authors": [
                "Huayu Chen",
                "Hang Su",
                "Peize Sun",
                "Jun Zhu"
            ],
            "affiliations": [
                "The University of Hong Kong",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.09347.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#cv",
                    "#multimodal",
                    "#training",
                    "#open_source",
                    "#architecture",
                    "#alignment"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· guidance Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Condition Contrastive Alignment (CCA) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Classifier-Free Guidance (CFG). CCA Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CCA Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ guidance, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ²Ğ´Ğ²Ğ¾Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ², Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾ CFG."
                },
                "en": {
                    "title": "Unifying Visual and Language Models with CCA: A New Era of Efficient AR Generation",
                    "desc": "The paper introduces Condition Contrastive Alignment (CCA), a method designed to improve autoregressive visual generation without the need for classifier-free guidance. CCA fine-tunes pretrained models to match the desired sampling distribution, enhancing performance with minimal additional training. This approach reduces the reliance on guided sampling, effectively cutting sampling costs in half while maintaining high-quality outputs. The study demonstrates that CCA can balance sample diversity and fidelity, bridging the gap between language and visual model alignment techniques."
                },
                "zh": {
                    "title": "æ¡ä»¶å¯¹æ¯”å¯¹é½ï¼šç»Ÿä¸€è¯­è¨€ä¸è§†è§‰ç”Ÿæˆçš„åˆ›æ–°æ–¹æ³•",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºæ¡ä»¶å¯¹æ¯”å¯¹é½ï¼ˆCCAï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºåœ¨è‡ªå›å½’å¤šæ¨¡æ€ç”Ÿæˆä¸­å®ç°æ— æŒ‡å¯¼çš„é«˜æ€§èƒ½è§†è§‰ç”Ÿæˆã€‚ä¼ ç»Ÿçš„åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼ï¼ˆCFGï¼‰åœ¨è¯­è¨€å’Œè§†è§‰å†…å®¹ä¹‹é—´å¼•å…¥äº†è®¾è®¡ä¸ä¸€è‡´ï¼Œè€ŒCCAé€šè¿‡ç›´æ¥å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCCAå¯ä»¥åœ¨ä»…éœ€ä¸€æ¬¡å¾®è°ƒçš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜æ¨¡å‹çš„æ— æŒ‡å¯¼æ€§èƒ½ï¼Œä¸æŒ‡å¯¼é‡‡æ ·æ–¹æ³•ç›¸å½“ã€‚é€šè¿‡è°ƒæ•´è®­ç»ƒå‚æ•°ï¼ŒCCAè¿˜å¯ä»¥åœ¨æ ·æœ¬å¤šæ ·æ€§å’Œä¿çœŸåº¦ä¹‹é—´å®ç°ç±»ä¼¼CFGçš„å¹³è¡¡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13060",
            "title": "AERO: Softmax-Only LLMs for Efficient Private Inference",
            "url": "https://huggingface.co/papers/2410.13060",
            "abstract": "The pervasiveness of proprietary language models has raised privacy concerns for users' sensitive data, emphasizing the need for private inference (PI), where inference is performed directly on encrypted inputs. However, current PI methods face prohibitively higher communication and latency overheads, primarily due to nonlinear operations. In this paper, we present a comprehensive analysis to understand the role of nonlinearities in transformer-based decoder-only language models. We introduce AERO, a four-step architectural optimization framework that refines the existing LLM architecture for efficient PI by systematically removing nonlinearities such as LayerNorm and GELU and reducing FLOPs counts. For the first time, we propose a Softmax-only architecture with significantly fewer FLOPs tailored for efficient PI. Furthermore, we devise a novel entropy regularization technique to improve the performance of Softmax-only models. AERO achieves up to 4.23times communication and 1.94times latency reduction. We validate the effectiveness of AERO by benchmarking it against the state-of-the-art.",
            "score": 4,
            "issue_id": 146,
            "pub_date": "2024-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "f664135f7701d39c",
            "authors": [
                "Nandan Kumar Jha",
                "Brandon Reagen"
            ],
            "affiliations": [
                "New York University"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13060.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#inference",
                    "#optimization",
                    "#security",
                    "#architecture"
                ],
                "emoji": "ğŸ”’",
                "ru": {
                    "title": "AERO: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ¾Ğ»Ğ¸ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ AERO - Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº LayerNorm Ğ¸ GELU, Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ FLOP. Ğ’Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Softmax, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° (PI). ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Softmax."
                },
                "en": {
                    "title": "Streamlining Privacy: AERO's Leap in Efficient Language Model Inference",
                    "desc": "The paper addresses privacy concerns in language models by focusing on private inference, which allows computations on encrypted data. Current methods struggle with high communication and latency due to nonlinear operations. The authors introduce AERO, an optimization framework that removes nonlinearities like LayerNorm and GELU, and proposes a Softmax-only architecture to reduce computational demands. AERO significantly improves efficiency, achieving notable reductions in communication and latency, and is validated against existing methods."
                },
                "zh": {
                    "title": "AEROï¼šæå‡éšç§æ¨ç†æ•ˆç‡çš„æ–°æ¡†æ¶",
                    "desc": "è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†åœ¨åŠ å¯†è¾“å…¥ä¸Šè¿›è¡Œæ¨ç†çš„éšç§æ¨ç†ï¼ˆPIï¼‰é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºAEROçš„ä¼˜åŒ–æ¡†æ¶ã€‚AEROé€šè¿‡å»é™¤éçº¿æ€§æ“ä½œå¦‚LayerNormå’ŒGELUï¼Œå‡å°‘è®¡ç®—é‡ï¼Œä»è€Œæé«˜PIçš„æ•ˆç‡ã€‚ä½œè€…é¦–æ¬¡æå‡ºäº†ä¸€ç§ä»…ä½¿ç”¨Softmaxçš„æ¶æ„ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—é‡ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„ç†µæ­£åˆ™åŒ–æŠ€æœ¯æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAEROåœ¨é€šä¿¡å’Œå»¶è¿Ÿæ–¹é¢åˆ†åˆ«å‡å°‘äº†4.23å€å’Œ1.94å€ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.10210",
            "title": "Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as the Key",
            "url": "https://huggingface.co/papers/2410.10210",
            "abstract": "As large language models rapidly evolve to support longer context, there is a notable disparity in their capability to generate output at greater lengths. Recent study suggests that the primary cause for this imbalance may arise from the lack of data with long-output during alignment training. In light of this observation, attempts are made to re-align foundation models with data that fills the gap, which result in models capable of generating lengthy output when instructed. In this paper, we explore the impact of data-quality in tuning a model for long output, and the possibility of doing so from the starting points of human-aligned (instruct or chat) models. With careful data curation, we show that it possible to achieve similar performance improvement in our tuned models, with only a small fraction of training data instances and compute. In addition, we assess the generalizability of such approaches by applying our tuning-recipes to several models. our findings suggest that, while capacities for generating long output vary across different models out-of-the-box, our approach to tune them with high-quality data using lite compute, consistently yields notable improvement across all models we experimented on. We have made public our curated dataset for tuning long-writing capability, the implementations of model tuning and evaluation, as well as the fine-tuned models, all of which can be openly-accessed.",
            "score": 3,
            "issue_id": 150,
            "pub_date": "2024-10-14",
            "pub_date_card": {
                "ru": "14 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 14",
                "zh": "10æœˆ14æ—¥"
            },
            "hash": "2fa998e2c3dc73e1",
            "authors": [
                "Yingda Chen",
                "Xingjun Wang",
                "Jintao Huang",
                "Yunlin Mao",
                "Daoze Zhang",
                "Yuze Zhao"
            ],
            "affiliations": [
                "ModelScope Team, Alibaba Group"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.10210.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#synthetic",
                    "#data",
                    "#training",
                    "#dataset",
                    "#open_source",
                    "#long_context",
                    "#alignment"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "Ğ”Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½ĞµÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ ÑĞ²Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ğ´ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°."
                },
                "en": {
                    "title": "Unlocking Long-Form Language: Tuning Models for Extended Output",
                    "desc": "The paper discusses how large language models often struggle to generate long outputs due to insufficient training data that includes lengthy examples. By re-aligning these models with carefully curated data that emphasizes long outputs, the researchers were able to enhance the models' ability to produce extended text. They found that even with a small amount of high-quality data and minimal computational resources, significant improvements could be achieved. The study also demonstrated that this approach works across various models, suggesting a generalizable method for improving long-output generation."
                },
                "zh": {
                    "title": "ç”¨é«˜è´¨é‡æ•°æ®æå‡é•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆé•¿æ–‡æœ¬æ—¶èƒ½åŠ›ä¸å‡è¡¡çš„é—®é¢˜ï¼Œä¸»è¦åŸå› æ˜¯è®­ç»ƒæ—¶ç¼ºä¹é•¿è¾“å‡ºçš„æ•°æ®ã€‚ç ”ç©¶é€šè¿‡é‡æ–°è°ƒæ•´åŸºç¡€æ¨¡å‹çš„æ•°æ®ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆé•¿æ–‡æœ¬ã€‚é€šè¿‡ç²¾å¿ƒçš„æ•°æ®é€‰æ‹©ï¼Œç ”ç©¶è¡¨æ˜åªéœ€å°‘é‡æ•°æ®å’Œè®¡ç®—èµ„æºå°±èƒ½æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶è¿˜å±•ç¤ºäº†è¿™ç§æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹ä¸Šçš„æ™®éé€‚ç”¨æ€§ï¼Œå¹¶å…¬å¼€äº†ç›¸å…³æ•°æ®é›†å’Œæ¨¡å‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.12183",
            "title": "TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration",
            "url": "https://huggingface.co/papers/2410.12183",
            "abstract": "Vision-language foundation models (such as CLIP) have recently shown their power in transfer learning, owing to large-scale image-text pre-training. However, target domain data in the downstream tasks can be highly different from the pre-training phase, which makes it hard for such a single model to generalize well. Alternatively, there exists a wide range of expert models that contain diversified vision and/or language knowledge pre-trained on different modalities, tasks, networks, and datasets. Unfortunately, these models are \"isolated agents\" with heterogeneous structures, and how to integrate their knowledge for generalizing CLIP-like models has not been fully explored. To bridge this gap, we propose a general and concise TransAgent framework, which transports the knowledge of the isolated agents in a unified manner, and effectively guides CLIP to generalize with multi-source knowledge distillation. With such a distinct framework, we flexibly collaborate with 11 heterogeneous agents to empower vision-language foundation models, without further cost in the inference phase. Finally, our TransAgent achieves state-of-the-art performance on 11 visual recognition datasets. Under the same low-shot setting, it outperforms the popular CoOp with around 10% on average, and 20% on EuroSAT which contains large domain shifts.",
            "score": 3,
            "issue_id": 150,
            "pub_date": "2024-10-16",
            "pub_date_card": {
                "ru": "16 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 16",
                "zh": "10æœˆ16æ—¥"
            },
            "hash": "8e00415319fa6d49",
            "authors": [
                "Yiwei Guo",
                "Shaobin Zhuang",
                "Kunchang Li",
                "Yu Qiao",
                "Yali Wang"
            ],
            "affiliations": [
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University",
                "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences",
                "University of Chinese Academy of Sciences"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.12183.jpg",
            "data": {
                "categories": [
                    "#benchmark",
                    "#cv",
                    "#graphs",
                    "#multimodal",
                    "#training",
                    "#transfer_learning",
                    "#agents",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "TransAgent: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TransAgent - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ñ‚Ğ¸Ğ¿Ğ° CLIP. TransAgent Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ CLIP-Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 11 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²."
                },
                "en": {
                    "title": "\"Unifying Isolated Experts: TransAgent's Leap in Vision-Language Models\"",
                    "desc": "The paper introduces TransAgent, a framework designed to enhance vision-language models like CLIP by integrating knowledge from various expert models. These expert models, pre-trained on different tasks and datasets, are typically isolated and have diverse structures. TransAgent unifies their knowledge through multi-source knowledge distillation, allowing CLIP to generalize better across different domains. This approach significantly improves performance on multiple visual recognition tasks, outperforming existing methods like CoOp, especially in scenarios with large domain shifts."
                },
                "zh": {
                    "title": "TransAgentï¼šæ•´åˆå¤šæºçŸ¥è¯†ï¼Œæå‡è§†è§‰-è¯­è¨€æ¨¡å‹æ³›åŒ–èƒ½åŠ›",
                    "desc": "è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTransAgentçš„æ¡†æ¶ï¼Œç”¨äºæ•´åˆä¸åŒä¸“å®¶æ¨¡å‹çš„çŸ¥è¯†æ¥å¢å¼ºè§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚TransAgenté€šè¿‡å¤šæºçŸ¥è¯†è’¸é¦çš„æ–¹å¼ï¼Œå°†å­¤ç«‹çš„ä¸“å®¶æ¨¡å‹çš„çŸ¥è¯†ç»Ÿä¸€ä¼ è¾“ç»™CLIPç­‰æ¨¡å‹ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨ä¸å¢åŠ æ¨ç†é˜¶æ®µæˆæœ¬çš„æƒ…å†µä¸‹ï¼Œä¸11ä¸ªå¼‚æ„ä»£ç†çµæ´»åä½œã€‚æœ€ç»ˆï¼ŒTransAgentåœ¨11ä¸ªè§†è§‰è¯†åˆ«æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨EuroSATæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2410.13293",
            "title": "SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2410.13293",
            "abstract": "Many students struggle with math word problems (MWPs), often finding it difficult to identify key information and select the appropriate mathematical operations.Schema-based instruction (SBI) is an evidence-based strategy that helps students categorize problems based on their structure, improving problem-solving accuracy. Building on this, we propose a Schema-Based Instruction Retrieval-Augmented Generation (SBI-RAG) framework that incorporates a large language model (LLM).Our approach emphasizes step-by-step reasoning by leveraging schemas to guide solution generation. We evaluate its performance on the GSM8K dataset, comparing it with GPT-4 and GPT-3.5 Turbo, and introduce a \"reasoning score\" metric to assess solution quality. Our findings suggest that SBI-RAG enhances reasoning clarity and problem-solving accuracy, potentially providing educational benefits for students",
            "score": 2,
            "issue_id": 148,
            "pub_date": "2024-10-17",
            "pub_date_card": {
                "ru": "17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
                "en": "October 17",
                "zh": "10æœˆ17æ—¥"
            },
            "hash": "edc83779c70890eb",
            "authors": [
                "Prakhar Dixit",
                "Tim Oates"
            ],
            "affiliations": [
                "Department of Computer Science University of Maryland Baltimore County"
            ],
            "pdf_title_img": "assets\\pdf\\title_img\\2410.13293.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#reasoning",
                    "#benchmark",
                    "#math",
                    "#dataset",
                    "#education"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ¡Ñ…ĞµĞ¼Ñ‹ + Ğ˜Ğ˜ = Ğ›ÑƒÑ‡ÑˆĞµĞµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ SBI-RAG (Schema-Based Instruction Retrieval-Augmented Generation). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ğµ ÑÑ…ĞµĞ¼Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ (SBI) Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLM) Ğ´Ğ»Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ SBI-RAG Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GSM8K, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ ĞµĞ³Ğ¾ Ñ GPT-4 Ğ¸ GPT-3.5 Turbo. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SBI-RAG ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½ĞµÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ñƒ ÑƒÑ‡Ğ°Ñ‰Ğ¸Ğ¼ÑÑ."
                },
                "en": {
                    "title": "Unlocking Math Word Problems with Schema-Based AI",
                    "desc": "The paper introduces a new framework called Schema-Based Instruction Retrieval-Augmented Generation (SBI-RAG) to help students solve math word problems more effectively. This approach uses a large language model to guide students through step-by-step reasoning by categorizing problems based on their structure. The framework was tested on the GSM8K dataset and compared with existing models like GPT-4 and GPT-3.5 Turbo, using a new 'reasoning score' metric to evaluate solution quality. Results indicate that SBI-RAG improves both the clarity of reasoning and the accuracy of problem-solving, offering potential educational benefits."
                },
                "zh": {
                    "title": "ç»“æ„å¼•å¯¼ï¼Œæå‡è§£é¢˜èƒ½åŠ›",
                    "desc": "è®¸å¤šå­¦ç”Ÿåœ¨è§£å†³æ•°å­¦æ–‡å­—é¢˜æ—¶æ„Ÿåˆ°å›°éš¾ï¼Œå¸¸å¸¸éš¾ä»¥è¯†åˆ«å…³é”®ä¿¡æ¯å¹¶é€‰æ‹©åˆé€‚çš„æ•°å­¦è¿ç®—ã€‚åŸºäºç»“æ„çš„æ•™å­¦ï¼ˆSBIï¼‰æ˜¯ä¸€ç§åŸºäºè¯æ®çš„ç­–ç•¥ï¼Œå¸®åŠ©å­¦ç”Ÿæ ¹æ®é—®é¢˜çš„ç»“æ„è¿›è¡Œåˆ†ç±»ï¼Œä»è€Œæé«˜è§£é¢˜å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºäºç»“æ„çš„æ•™å­¦æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆSBI-RAGï¼‰æ¡†æ¶ï¼Œå¼ºè°ƒé€šè¿‡åˆ©ç”¨ç»“æ„æŒ‡å¯¼è§£å†³æ–¹æ¡ˆç”Ÿæˆçš„é€æ­¥æ¨ç†ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒSBI-RAGå¯ä»¥æé«˜æ¨ç†çš„æ¸…æ™°åº¦å’Œè§£é¢˜çš„å‡†ç¡®æ€§ï¼Œä¸ºå­¦ç”Ÿæä¾›æ½œåœ¨çš„æ•™è‚²ç›Šå¤„ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-10-17.html",
    "link_next": "2024-10-21.html",
    "link_month": "2024-10.html",
    "short_date_prev": {
        "ru": "17.10",
        "en": "10/17",
        "zh": "10æœˆ17æ—¥"
    },
    "short_date_next": {
        "ru": "21.10",
        "en": "10/21",
        "zh": "10æœˆ21æ—¥"
    },
    "categories": {
        "#dataset": 12,
        "#data": 7,
        "#benchmark": 17,
        "#agents": 2,
        "#cv": 10,
        "#rl": 1,
        "#rlhf": 3,
        "#rag": 5,
        "#plp": 2,
        "#inference": 9,
        "#3d": 2,
        "#audio": 4,
        "#video": 4,
        "#multimodal": 15,
        "#math": 3,
        "#multilingual": 3,
        "#architecture": 22,
        "#healthcare": 2,
        "#training": 19,
        "#robotics": 0,
        "#agi": 4,
        "#games": 8,
        "#interpretability": 1,
        "#reasoning": 6,
        "#transfer_learning": 7,
        "#graphs": 3,
        "#ethics": 1,
        "#security": 3,
        "#optimization": 16,
        "#survey": 2,
        "#diffusion": 6,
        "#alignment": 10,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 6,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 12,
        "#small_models": 4,
        "#science": 5,
        "#low_resource": 1,
        "#education": 1
    }
}