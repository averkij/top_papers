{
    "date": {
        "ru": "6 сентября",
        "en": "September 6",
        "zh": "9月6日"
    },
    "time_utc": "2024-09-06 09:00",
    "weekday": 4,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-06",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.01322",
            "title": "Guide-and-Rescale: Self-Guidance Mechanism for Effective Tuning-Free Real Image Editing",
            "url": "https://huggingface.co/papers/2409.01322",
            "abstract": "Despite recent advances in large-scale text-to-image generative models, manipulating real images with these models remains a challenging problem. The main limitations of existing editing methods are that they either fail to perform with consistent quality on a wide range of image edits or require time-consuming hyperparameter tuning or fine-tuning of the diffusion model to preserve the image-specific appearance of the input image. We propose a novel approach that is built upon a modified diffusion sampling process via the guidance mechanism. In this work, we explore the self-guidance technique to preserve the overall structure of the input image and its local regions appearance that should not be edited. In particular, we explicitly introduce layout-preserving energy functions that are aimed to save local and global structures of the source image. Additionally, we propose a noise rescaling mechanism that allows to preserve noise distribution by balancing the norms of classifier-free guidance and our proposed guiders during generation. Such a guiding approach does not require fine-tuning the diffusion model and exact inversion process. As a result, the proposed method provides a fast and high-quality editing mechanism. In our experiments, we show through human evaluation and quantitative analysis that the proposed method allows to produce desired editing which is more preferable by humans and also achieves a better trade-off between editing quality and preservation of the original image. Our code is available at https://github.com/FusionBrainLab/Guide-and-Rescale.",
            "score": 94,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "18431c1f871794ad",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Улучшенное редактирование изображений с помощью самонаправляемой диффузии",
                    "desc": "Эта статья представляет новый подход к редактированию изображений с использованием диффузионных моделей. Авторы предлагают метод self-guidance для сохранения структуры исходного изображения и внешнего вида областей, которые не должны редактироваться. Они вводят функции энергии для сохранения локальных и глобальных структур, а также механизм перемасштабирования шума. Эксперименты показывают, что предложенный метод обеспечивает более качественное редактирование и лучший баланс между качеством редактирования и сохранением оригинального изображения."
                },
                "en": {
                    "title": "Effortless Image Editing with Structure Preservation",
                    "desc": "This paper presents a new method for editing images using text-to-image generative models, addressing the limitations of existing techniques that struggle with quality and require extensive tuning. The authors introduce a modified diffusion sampling process that utilizes self-guidance to maintain the structure and appearance of the original image while allowing for effective edits. They implement layout-preserving energy functions to ensure that both local and global features of the source image are retained during the editing process. The proposed noise rescaling mechanism balances guidance norms, enabling fast and high-quality image editing without the need for fine-tuning the model."
                },
                "zh": {
                    "title": "高效图像编辑的新方法",
                    "desc": "本文提出了一种新颖的方法来改善大规模文本到图像生成模型在真实图像编辑中的表现。我们通过修改扩散采样过程，引入自我引导技术，以保持输入图像的整体结构和局部区域的外观。特别地，我们引入了布局保持能量函数，以保护源图像的局部和全局结构。此外，我们提出了一种噪声重标定机制，能够在生成过程中平衡分类器自由引导和我们提出的引导器的范数，从而实现快速高质量的图像编辑。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.03752",
            "title": "Attention Heads of Large Language Models: A Survey",
            "url": "https://huggingface.co/papers/2409.03752",
            "abstract": "Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in various tasks but remain largely as black-box systems. Consequently, their development relies heavily on data-driven approaches, limiting performance enhancement through changes in internal architecture and reasoning pathways. As a result, many researchers have begun exploring the potential internal mechanisms of LLMs, aiming to identify the essence of their reasoning bottlenecks, with most studies focusing on attention heads. Our survey aims to shed light on the internal reasoning processes of LLMs by concentrating on the interpretability and underlying mechanisms of attention heads. We first distill the human thought process into a four-stage framework: Knowledge Recalling, In-Context Identification, Latent Reasoning, and Expression Preparation. Using this framework, we systematically review existing research to identify and categorize the functions of specific attention heads. Furthermore, we summarize the experimental methodologies used to discover these special heads, dividing them into two categories: Modeling-Free methods and Modeling-Required methods. Also, we outline relevant evaluation methods and benchmarks. Finally, we discuss the limitations of current research and propose several potential future directions. Our reference list is open-sourced at https://github.com/IAAR-Shanghai/Awesome-Attention-Heads.",
            "score": 87,
            "issue_id": 1,
            "pub_date": "2024-09-05",
            "pub_date_card": {
                "ru": "5 сентября",
                "en": "September 5",
                "zh": "9月5日"
            },
            "hash": "d79e296c0a5e1b88",
            "data": {
                "categories": [
                    "#reasoning",
                    "#survey",
                    "#cv",
                    "#interpretability",
                    "#benchmark",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Раскрывая тайны механизмов внимания в больших языковых моделях",
                    "desc": "Эта статья представляет собой обзор исследований внутренних механизмов работы больших языковых моделей (LLM), фокусируясь на интерпретируемости и функциях механизмов внимания. Авторы предлагают четырехэтапную структуру человеческого мышления и используют ее для категоризации функций механизмов внимания в LLM. В работе также обсуждаются методологии экспериментов для обнаружения специальных механизмов внимания, разделяя их на методы без моделирования и методы с моделированием. Статья завершается обсуждением ограничений текущих исследований и предложением направлений для будущих работ."
                },
                "en": {
                    "title": "Unlocking the Black Box: Understanding Attention Heads in LLMs",
                    "desc": "This paper explores the internal reasoning mechanisms of Large Language Models (LLMs), particularly focusing on attention heads. It introduces a four-stage framework that mirrors human thought processes: Knowledge Recalling, In-Context Identification, Latent Reasoning, and Expression Preparation. The authors systematically review existing studies to categorize the functions of attention heads and discuss the methodologies used to identify them, distinguishing between Modeling-Free and Modeling-Required approaches. Additionally, the paper highlights current research limitations and suggests future research directions to enhance the interpretability of LLMs."
                },
                "zh": {
                    "title": "揭示大型语言模型的推理机制",
                    "desc": "本论文探讨了大型语言模型（LLMs）内部推理过程的可解释性，特别关注注意力头的机制。我们提出了一个四阶段框架，分别是知识回忆、上下文识别、潜在推理和表达准备，以帮助理解人类思维过程。通过系统回顾现有研究，我们识别并分类了特定注意力头的功能，并总结了发现这些特殊头部的实验方法。最后，我们讨论了当前研究的局限性，并提出了未来的研究方向。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.01944",
            "title": "FuzzCoder: Byte-level Fuzzing Test via Large Language Model",
            "url": "https://huggingface.co/papers/2409.01944",
            "abstract": "Fuzzing is an important dynamic program analysis technique designed for finding vulnerabilities in complex software. Fuzzing involves presenting a target program with crafted malicious input to cause crashes, buffer overflows, memory errors, and exceptions. Crafting malicious inputs in an efficient manner is a difficult open problem and the best approaches often apply uniform random mutations to pre-existing valid inputs. In this work, we propose to adopt fine-tuned large language models (FuzzCoder) to learn patterns in the input files from successful attacks to guide future fuzzing explorations. Specifically, we develop a framework to leverage the code LLMs to guide the mutation process of inputs in fuzzing. The mutation process is formulated as the sequence-to-sequence modeling, where LLM receives a sequence of bytes and then outputs the mutated byte sequence. FuzzCoder is fine-tuned on the created instruction dataset (Fuzz-Instruct), where the successful fuzzing history is collected from the heuristic fuzzing tool. FuzzCoder can predict mutation locations and strategies locations in input files to trigger abnormal behaviors of the program. Experimental results show that FuzzCoder based on AFL (American Fuzzy Lop) gain significant improvements in terms of effective proportion of mutation (EPM) and number of crashes (NC) for various input formats including ELF, JPG, MP3, and XML.",
            "score": 44,
            "issue_id": 1,
            "pub_date": "2024-09-03",
            "pub_date_card": {
                "ru": "3 сентября",
                "en": "September 3",
                "zh": "9月3日"
            },
            "hash": "e7eb6566405186be",
            "data": {
                "categories": [
                    "#dataset",
                    "#security",
                    "#training",
                    "#optimization",
                    "#plp"
                ],
                "emoji": "🐞",
                "ru": {
                    "title": "Умный фаззинг с помощью языковых моделей",
                    "desc": "Данная статья представляет новый подход к фаззингу - технике анализа программ для поиска уязвимостей. Авторы предлагают использовать большие языковые модели (LLM) для создания более эффективных мутаций входных данных. Модель FuzzCoder обучается на успешных примерах атак и может предсказывать оптимальные места для мутаций. Эксперименты показывают значительное улучшение эффективности фаззинга по сравнению с традиционными методами для различных форматов файлов."
                },
                "en": {
                    "title": "Enhancing Fuzzing with Language Models for Better Vulnerability Detection",
                    "desc": "This paper introduces FuzzCoder, a novel approach to improve fuzzing techniques in software vulnerability detection. By utilizing fine-tuned large language models, FuzzCoder learns from successful attack patterns to enhance the input mutation process. The mutation is treated as a sequence-to-sequence problem, where the model generates new byte sequences based on existing inputs. Experimental results demonstrate that FuzzCoder significantly increases the effectiveness of fuzzing, leading to more crashes and better coverage across different file formats."
                },
                "zh": {
                    "title": "利用大语言模型提升模糊测试效率",
                    "desc": "模糊测试是一种动态程序分析技术，旨在发现复杂软件中的漏洞。本文提出了一种名为FuzzCoder的模型，利用大语言模型学习成功攻击中的输入文件模式，以指导未来的模糊测试探索。我们将输入的变异过程建模为序列到序列的任务，模型接收字节序列并输出变异后的字节序列。实验结果表明，基于FuzzCoder的模糊测试在有效变异比例和崩溃次数方面显著提高，适用于多种输入格式。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.03512",
            "title": "From MOOC to MAIC: Reshaping Online Teaching and Learning through LLM-driven Agents",
            "url": "https://huggingface.co/papers/2409.03512",
            "abstract": "Since the first instances of online education, where courses were uploaded to accessible and shared online platforms, this form of scaling the dissemination of human knowledge to reach a broader audience has sparked extensive discussion and widespread adoption. Recognizing that personalized learning still holds significant potential for improvement, new AI technologies have been continuously integrated into this learning format, resulting in a variety of educational AI applications such as educational recommendation and intelligent tutoring. The emergence of intelligence in large language models (LLMs) has allowed for these educational enhancements to be built upon a unified foundational model, enabling deeper integration. In this context, we propose MAIC (Massive AI-empowered Course), a new form of online education that leverages LLM-driven multi-agent systems to construct an AI-augmented classroom, balancing scalability with adaptivity. Beyond exploring the conceptual framework and technical innovations, we conduct preliminary experiments at Tsinghua University, one of China's leading universities. Drawing from over 100,000 learning records of more than 500 students, we obtain a series of valuable observations and initial analyses. This project will continue to evolve, ultimately aiming to establish a comprehensive open platform that supports and unifies research, technology, and applications in exploring the possibilities of online education in the era of large model AI. We envision this platform as a collaborative hub, bringing together educators, researchers, and innovators to collectively explore the future of AI-driven online education.",
            "score": 26,
            "issue_id": 1,
            "pub_date": "2024-09-05",
            "pub_date_card": {
                "ru": "5 сентября",
                "en": "September 5",
                "zh": "9月5日"
            },
            "hash": "63d56825655d908a",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#multilingual",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "MAIC: Революция в онлайн-образовании с помощью AI и многоагентных систем",
                    "desc": "Статья представляет новую форму онлайн-образования под названием MAIC (Massive AI-empowered Course), использующую системы с несколькими агентами на основе больших языковых моделей (LLM) для создания AI-расширенной виртуальной аудитории. Авторы провели предварительные эксперименты в Университете Цинхуа, анализируя более 100 000 записей об обучении более 500 студентов. Проект направлен на создание открытой платформы, объединяющей исследования, технологии и приложения в области онлайн-образования с использованием AI. Цель платформы - стать центром сотрудничества для педагогов, исследователей и новаторов в изучении будущего AI-ориентированного онлайн-образования."
                },
                "en": {
                    "title": "Revolutionizing Online Learning with AI-Driven Personalization",
                    "desc": "This paper introduces MAIC (Massive AI-empowered Course), a novel approach to online education that utilizes large language models (LLMs) and multi-agent systems to create an AI-enhanced learning environment. The integration of AI technologies aims to improve personalized learning experiences while maintaining scalability for a larger audience. Preliminary experiments conducted at Tsinghua University analyzed over 100,000 learning records from more than 500 students, providing insights into the effectiveness of this approach. The ultimate goal is to develop a comprehensive open platform that fosters collaboration among educators, researchers, and innovators in the field of AI-driven online education."
                },
                "zh": {
                    "title": "大规模AI赋能，重塑在线教育未来",
                    "desc": "本文提出了一种新的在线教育形式，称为MAIC（大规模人工智能赋能课程），它利用大型语言模型驱动的多智能体系统来构建增强型课堂。通过这种方式，MAIC在可扩展性和适应性之间取得了平衡，旨在提升个性化学习的效果。我们在清华大学进行的初步实验基于超过100,000条学习记录，分析了500多名学生的学习情况，获得了一系列有价值的观察结果。最终目标是建立一个综合开放平台，支持研究、技术和应用的统一，探索大模型人工智能时代在线教育的可能性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.03718",
            "title": "Geometry Image Diffusion: Fast and Data-Efficient Text-to-3D with Image-Based Surface Representation",
            "url": "https://huggingface.co/papers/2409.03718",
            "abstract": "Generating high-quality 3D objects from textual descriptions remains a challenging problem due to computational cost, the scarcity of 3D data, and complex 3D representations. We introduce Geometry Image Diffusion (GIMDiffusion), a novel Text-to-3D model that utilizes geometry images to efficiently represent 3D shapes using 2D images, thereby avoiding the need for complex 3D-aware architectures. By integrating a Collaborative Control mechanism, we exploit the rich 2D priors of existing Text-to-Image models such as Stable Diffusion. This enables strong generalization even with limited 3D training data (allowing us to use only high-quality training data) as well as retaining compatibility with guidance techniques such as IPAdapter. In short, GIMDiffusion enables the generation of 3D assets at speeds comparable to current Text-to-Image models. The generated objects consist of semantically meaningful, separate parts and include internal structures, enhancing both usability and versatility.",
            "score": 25,
            "issue_id": 1,
            "pub_date": "2024-09-05",
            "pub_date_card": {
                "ru": "5 сентября",
                "en": "September 5",
                "zh": "9月5日"
            },
            "hash": "eaae7513d598cd8a",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#diffusion",
                    "#architecture",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "Быстрое и качественное создание 3D из текста с помощью 2D-технологий",
                    "desc": "GIMDiffusion - это новая модель для создания 3D-объектов из текстовых описаний. Она использует геометрические изображения для эффективного представления 3D-форм в виде 2D-изображений. Модель интегрирует механизм совместного контроля, используя богатые 2D-приоры существующих моделей Text-to-Image. GIMDiffusion позволяет генерировать 3D-объекты со скоростью, сравнимой с современными моделями Text-to-Image, при этом создавая семантически значимые отдельные части и внутренние структуры."
                },
                "en": {
                    "title": "Transforming Text to 3D: Fast and Flexible with GIMDiffusion",
                    "desc": "The paper presents Geometry Image Diffusion (GIMDiffusion), a new model for generating 3D objects from text descriptions. It uses geometry images to represent 3D shapes in a 2D format, simplifying the process and reducing computational demands. By leveraging existing Text-to-Image models, GIMDiffusion can generalize well even with limited 3D data, ensuring high-quality outputs. This approach allows for the rapid creation of detailed 3D assets that are both functional and adaptable."
                },
                "zh": {
                    "title": "高效生成三维对象的新方法",
                    "desc": "本文介绍了一种新的文本到三维模型，称为几何图像扩散（GIMDiffusion），它利用几何图像高效地表示三维形状。通过使用二维图像，GIMDiffusion避免了复杂的三维架构，从而降低了计算成本。该模型结合了协作控制机制，利用现有文本到图像模型的丰富二维先验知识，实现了强大的泛化能力。最终，GIMDiffusion能够以与当前文本到图像模型相当的速度生成具有语义意义的三维对象，提升了可用性和多功能性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.03420",
            "title": "mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding",
            "url": "https://huggingface.co/papers/2409.03420",
            "abstract": "Multimodel Large Language Models(MLLMs) have achieved promising OCR-free Document Understanding performance by increasing the supported resolution of document images. However, this comes at the cost of generating thousands of visual tokens for a single document image, leading to excessive GPU memory and slower inference times, particularly in multi-page document comprehension. In this work, to address these challenges, we propose a High-resolution DocCompressor module to compress each high-resolution document image into 324 tokens, guided by low-resolution global visual features. With this compression module, to strengthen multi-page document comprehension ability and balance both token efficiency and question-answering performance, we develop the DocOwl2 under a three-stage training framework: Single-image Pretraining, Multi-image Continue-pretraining, and Multi-task Finetuning. DocOwl2 sets a new state-of-the-art across multi-page document understanding benchmarks and reduces first token latency by more than 50%, demonstrating advanced capabilities in multi-page questioning answering, explanation with evidence pages, and cross-page structure understanding. Additionally, compared to single-image MLLMs trained on similar data, our DocOwl2 achieves comparable single-page understanding performance with less than 20% of the visual tokens. Our codes, models, and data are publicly available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2.",
            "score": 23,
            "issue_id": 1,
            "pub_date": "2024-09-05",
            "pub_date_card": {
                "ru": "5 сентября",
                "en": "September 5",
                "zh": "9月5日"
            },
            "hash": "0a6e9a759ec89f2f",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#cv",
                    "#training",
                    "#inference",
                    "#optimization",
                    "#benchmark",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "📄",
                "ru": {
                    "title": "Эффективное сжатие и анализ многостраничных документов с помощью ИИ",
                    "desc": "Статья представляет DocOwl2 - мультимодальную языковую модель для понимания документов без OCR. Авторы предлагают модуль High-resolution DocCompressor для сжатия изображений документов высокого разрешения в 324 токена. Модель обучается в три этапа: предобучение на одном изображении, продолжение предобучения на нескольких изображениях и мультизадачная донастройка. DocOwl2 достигает наилучших результатов в понимании многостраничных документов, значительно сокращая задержку вывода первого токена."
                },
                "en": {
                    "title": "Efficient Multi-Page Document Understanding with DocOwl2",
                    "desc": "This paper introduces the High-resolution DocCompressor module, which reduces the number of visual tokens generated from high-resolution document images to improve efficiency in multi-page document understanding. The proposed DocOwl2 model utilizes a three-stage training framework to enhance its ability to comprehend and answer questions about documents while maintaining a balance between token efficiency and performance. By achieving a significant reduction in first token latency and setting new benchmarks in multi-page document understanding, DocOwl2 demonstrates its advanced capabilities. Furthermore, it maintains competitive performance in single-page understanding with significantly fewer visual tokens compared to traditional models."
                },
                "zh": {
                    "title": "高效压缩，提升文档理解能力",
                    "desc": "多模态大型语言模型（MLLMs）在无OCR文档理解方面取得了良好的效果，但生成大量视觉标记导致GPU内存消耗过大和推理速度变慢。为了解决这些问题，本文提出了一种高分辨率文档压缩模块，将高分辨率文档图像压缩为324个标记，并通过低分辨率的全局视觉特征进行指导。我们在三阶段训练框架下开发了DocOwl2，显著提高了多页文档理解能力，并在多页问答和跨页结构理解方面设立了新的基准。与单图像MLLMs相比，DocOwl2在单页理解性能上表现相当，但视觉标记数量减少了20%以下。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.03643",
            "title": "CDM: A Reliable Metric for Fair and Accurate Formula Recognition Evaluation",
            "url": "https://huggingface.co/papers/2409.03643",
            "abstract": "Formula recognition presents significant challenges due to the complicated structure and varied notation of mathematical expressions. Despite continuous advancements in formula recognition models, the evaluation metrics employed by these models, such as BLEU and Edit Distance, still exhibit notable limitations. They overlook the fact that the same formula has diverse representations and is highly sensitive to the distribution of training data, thereby causing the unfairness in formula recognition evaluation. To this end, we propose a Character Detection Matching (CDM) metric, ensuring the evaluation objectivity by designing a image-level rather than LaTex-level metric score. Specifically, CDM renders both the model-predicted LaTeX and the ground-truth LaTeX formulas into image-formatted formulas, then employs visual feature extraction and localization techniques for precise character-level matching, incorporating spatial position information. Such a spatially-aware and character-matching method offers a more accurate and equitable evaluation compared with previous BLEU and Edit Distance metrics that rely solely on text-based character matching. Experimentally, we evaluated various formula recognition models using CDM, BLEU, and ExpRate metrics. Their results demonstrate that the CDM aligns more closely with human evaluation standards and provides a fairer comparison across different models by eliminating discrepancies caused by diverse formula representations.",
            "score": 18,
            "issue_id": 1,
            "pub_date": "2024-09-05",
            "pub_date_card": {
                "ru": "5 сентября",
                "en": "September 5",
                "zh": "9月5日"
            },
            "hash": "9036259aa729330f",
            "data": {
                "categories": [
                    "#cv",
                    "#math",
                    "#ethics",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Справедливая оценка распознавания формул: от LaTeX к изображениям",
                    "desc": "Статья представляет новую метрику оценки качества распознавания математических формул - Character Detection Matching (CDM). В отличие от традиционных метрик, CDM работает на уровне изображений, а не LaTeX-кода, что позволяет учитывать разнообразие представлений одной и той же формулы. Метод использует извлечение визуальных признаков и локализацию для точного сопоставления символов с учетом их пространственного положения. Эксперименты показали, что CDM лучше соответствует человеческим стандартам оценки и обеспечивает более справедливое сравнение различных моделей распознавания формул."
                },
                "en": {
                    "title": "Revolutionizing Formula Recognition Evaluation with CDM",
                    "desc": "This paper addresses the challenges in evaluating formula recognition models due to the complex nature of mathematical expressions and their varied notations. The authors highlight the limitations of traditional evaluation metrics like BLEU and Edit Distance, which fail to account for different representations of the same formula and are sensitive to training data distribution. To improve evaluation fairness, they introduce a new metric called Character Detection Matching (CDM), which evaluates formulas based on image representations rather than text. CDM utilizes visual feature extraction and spatial localization for character-level matching, resulting in a more accurate assessment that aligns better with human evaluations compared to existing metrics."
                },
                "zh": {
                    "title": "公式识别的新标准：字符检测匹配指标",
                    "desc": "公式识别面临着复杂结构和多样符号的挑战。现有的评估指标如BLEU和编辑距离存在明显局限，无法公平评估公式识别的效果。为此，我们提出了一种字符检测匹配（CDM）指标，通过图像级别的评估方法提高评估的客观性。CDM通过将预测的LaTeX和真实的LaTeX公式转化为图像格式，利用视觉特征提取和定位技术进行精确的字符级匹配，从而提供更准确和公平的评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.03753",
            "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
            "url": "https://huggingface.co/papers/2409.03753",
            "abstract": "The increasing availability of real-world conversation data offers exciting opportunities for researchers to study user-chatbot interactions. However, the sheer volume of this data makes manually examining individual conversations impractical. To overcome this challenge, we introduce WildVis, an interactive tool that enables fast, versatile, and large-scale conversation analysis. WildVis provides search and visualization capabilities in the text and embedding spaces based on a list of criteria. To manage million-scale datasets, we implemented optimizations including search index construction, embedding precomputation and compression, and caching to ensure responsive user interactions within seconds. We demonstrate WildVis's utility through three case studies: facilitating chatbot misuse research, visualizing and comparing topic distributions across datasets, and characterizing user-specific conversation patterns. WildVis is open-source and designed to be extendable, supporting additional datasets and customized search and visualization functionalities.",
            "score": 18,
            "issue_id": 1,
            "pub_date": "2024-09-05",
            "pub_date_card": {
                "ru": "5 сентября",
                "en": "September 5",
                "zh": "9月5日"
            },
            "hash": "b58bcce018642f84",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#training",
                    "#data",
                    "#optimization",
                    "#benchmark",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Визуализация больших диалоговых данных для исследования взаимодействия человека и ИИ",
                    "desc": "WildVis - это интерактивный инструмент для быстрого и масштабного анализа диалогов между пользователями и чат-ботами. Он позволяет осуществлять поиск и визуализацию в текстовом и эмбеддинговом пространствах на основе заданных критериев. Для обработки миллионов диалогов авторы реализовали оптимизации, включая построение поискового индекса и сжатие эмбеддингов. WildVis применим для исследования неправильного использования чат-ботов, визуализации распределения тем и анализа паттернов общения пользователей."
                },
                "en": {
                    "title": "WildVis: Revolutionizing Large-Scale Conversation Analysis",
                    "desc": "This paper presents WildVis, an innovative tool designed for analyzing large-scale conversation data between users and chatbots. It addresses the challenge of manually reviewing extensive datasets by providing efficient search and visualization features in both text and embedding spaces. WildVis incorporates optimizations like search index construction and embedding precomputation to ensure quick responses, even with millions of conversations. The tool is open-source and allows for customization, making it suitable for various research applications, including chatbot misuse analysis and user interaction pattern exploration."
                },
                "zh": {
                    "title": "WildVis：高效分析聊天数据的工具",
                    "desc": "随着真实对话数据的增加，研究人员可以更好地研究用户与聊天机器人的互动。然而，数据量庞大使得手动检查每个对话变得不切实际。为了解决这个问题，我们推出了WildVis，这是一个交互式工具，可以快速、灵活地进行大规模对话分析。WildVis提供基于多种标准的文本和嵌入空间的搜索和可视化功能，支持对百万级数据集的高效管理。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02392",
            "title": "Building Math Agents with Multi-Turn Iterative Preference Learning",
            "url": "https://huggingface.co/papers/2409.02392",
            "abstract": "Recent studies have shown that large language models' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model's performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2024-09-04",
            "pub_date_card": {
                "ru": "4 сентября",
                "en": "September 4",
                "zh": "9月4日"
            },
            "hash": "9cb2dcc6706cca00",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#training",
                    "#math",
                    "#optimization",
                    "#rlhf",
                    "#architecture",
                    "#synthetic"
                ],
                "emoji": "🧮",
                "ru": {
                    "title": "Улучшение математических навыков ИИ через обучение на основе предпочтений",
                    "desc": "Это исследование представляет новый подход к улучшению математических способностей больших языковых моделей (LLM) с использованием многоходового обучения с подкреплением на основе предпочтений. Авторы разработали фреймворк, который оптимизирует траектории рассуждений модели, используя обратную связь от интерпретаторов кода. Эксперименты показали значительное улучшение производительности моделей Gemma на наборах данных GSM8K и MATH после применения этого метода. Предложенный подход дополняет существующие методы, такие как генерация синтетических данных и обучение с учителем."
                },
                "en": {
                    "title": "Enhancing LLMs with Multi-Turn Direct Preference Learning for Math Problem Solving",
                    "desc": "This paper explores how to enhance large language models' (LLMs) ability to solve mathematical problems by using external tools and multi-turn Chain-of-Thought (CoT) reasoning. It introduces a new approach called multi-turn direct preference learning, which is designed to handle the complexities of multi-turn interactions and tool integration. The framework includes specific implementations like multi-turn DPO and multi-turn KTO, which optimize preferences based on feedback from code interpreters. The results show significant performance improvements in LLMs when tested on the GSM8K and MATH datasets, indicating the effectiveness of this new learning framework."
                },
                "zh": {
                    "title": "提升数学推理能力的多轮学习框架",
                    "desc": "本研究探讨了如何通过整合外部工具和多轮推理来提升大型语言模型（LLMs）在数学问题解决中的能力。我们提出了一种多轮直接偏好学习框架，专门针对工具集成的数学推理任务，利用代码解释器的反馈来优化模型的表现。该框架包括多轮直接偏好优化（DPO）和多轮知识转移优化（KTO）作为具体实现。实验结果表明，使用该框架训练的模型在GSM8K和MATH数据集上的表现显著提升。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.03525",
            "title": "FrozenSeg: Harmonizing Frozen Foundation Models for Open-Vocabulary Segmentation",
            "url": "https://huggingface.co/papers/2409.03525",
            "abstract": "Open-vocabulary segmentation poses significant challenges, as it requires segmenting and recognizing objects across an open set of categories in unconstrained environments. Building on the success of powerful vision-language (ViL) foundation models, such as CLIP, recent efforts sought to harness their zero-short capabilities to recognize unseen categories. Despite notable performance improvements, these models still encounter the critical issue of generating precise mask proposals for unseen categories and scenarios, resulting in inferior segmentation performance eventually. To address this challenge, we introduce a novel approach, FrozenSeg, designed to integrate spatial knowledge from a localization foundation model (e.g., SAM) and semantic knowledge extracted from a ViL model (e.g., CLIP), in a synergistic framework. Taking the ViL model's visual encoder as the feature backbone, we inject the space-aware feature into the learnable queries and CLIP features within the transformer decoder. In addition, we devise a mask proposal ensemble strategy for further improving the recall rate and mask quality. To fully exploit pre-trained knowledge while minimizing training overhead, we freeze both foundation models, focusing optimization efforts solely on a lightweight transformer decoder for mask proposal generation-the performance bottleneck. Extensive experiments demonstrate that FrozenSeg advances state-of-the-art results across various segmentation benchmarks, trained exclusively on COCO panoptic data, and tested in a zero-shot manner. Code is available at https://github.com/chenxi52/FrozenSeg.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-09-05",
            "pub_date_card": {
                "ru": "5 сентября",
                "en": "September 5",
                "zh": "9月5日"
            },
            "hash": "d324ecd311d0064b",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#transfer_learning",
                    "#benchmark",
                    "#open_source",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "🧊",
                "ru": {
                    "title": "FrozenSeg: синергия локализации и семантики для открытой сегментации",
                    "desc": "Статья представляет новый подход к открытой сегментации изображений под названием FrozenSeg. Метод объединяет пространственные знания из модели локализации (например, SAM) и семантические знания из модели vision-language (например, CLIP). FrozenSeg использует замороженные предобученные модели и оптимизирует только легковесный трансформерный декодер для генерации маскированных предложений. Эксперименты показывают, что FrozenSeg достигает state-of-the-art результатов на различных бенчмарках сегментации при обучении только на данных COCO panoptic."
                },
                "en": {
                    "title": "FrozenSeg: Bridging Spatial and Semantic Knowledge for Open-Vocabulary Segmentation",
                    "desc": "This paper presents FrozenSeg, a new method for open-vocabulary segmentation that combines spatial and semantic knowledge from different foundation models. By leveraging the visual encoder of a vision-language model like CLIP and integrating it with a localization model, FrozenSeg enhances the generation of mask proposals for unseen object categories. The approach focuses on optimizing a lightweight transformer decoder while keeping the foundation models frozen, which reduces training time and complexity. Experimental results show that FrozenSeg achieves state-of-the-art performance on segmentation tasks, particularly in zero-shot scenarios."
                },
                "zh": {
                    "title": "融合空间与语义知识的分割新方法",
                    "desc": "开放词汇分割面临重大挑战，因为它需要在不受限制的环境中对开放类别集合中的物体进行分割和识别。基于强大的视觉-语言（ViL）基础模型（如CLIP）的成功，最近的研究试图利用其零样本能力来识别未见类别。尽管性能有所提升，这些模型在生成未见类别和场景的精确掩码提议方面仍然存在关键问题，导致分割性能不佳。为了解决这个问题，我们提出了一种新方法FrozenSeg，旨在将定位基础模型（如SAM）的空间知识与从ViL模型（如CLIP）提取的语义知识结合在一个协同框架中。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00844",
            "title": "Report Cards: Qualitative Evaluation of Language Models Using Natural Language Summaries",
            "url": "https://huggingface.co/papers/2409.00844",
            "abstract": "The rapid development and dynamic nature of large language models (LLMs) make it difficult for conventional quantitative benchmarks to accurately assess their capabilities. We propose report cards, which are human-interpretable, natural language summaries of model behavior for specific skills or topics. We develop a framework to evaluate report cards based on three criteria: specificity (ability to distinguish between models), faithfulness (accurate representation of model capabilities), and interpretability (clarity and relevance to humans). We also propose an iterative algorithm for generating report cards without human supervision and explore its efficacy by ablating various design choices. Through experimentation with popular LLMs, we demonstrate that report cards provide insights beyond traditional benchmarks and can help address the need for a more interpretable and holistic evaluation of LLMs.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-09-01",
            "pub_date_card": {
                "ru": "1 сентября",
                "en": "September 1",
                "zh": "9月1日"
            },
            "hash": "7d104d8e5fa06209",
            "data": {
                "categories": [
                    "#survey",
                    "#interpretability",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "📊",
                "ru": {
                    "title": "Табели успеваемости: новый способ оценки языковых моделей",
                    "desc": "В статье предлагается новый подход к оценке больших языковых моделей (LLM) с помощью 'табелей успеваемости'. Это человекочитаемые текстовые резюме, описывающие поведение модели для конкретных навыков или тем. Авторы разработали критерии оценки этих табелей: специфичность, достоверность и интерпретируемость. Также представлен итеративный алгоритм для автоматической генерации табелей без участия человека."
                },
                "en": {
                    "title": "Report Cards: A New Way to Evaluate Language Models",
                    "desc": "This paper introduces a new method called report cards to evaluate large language models (LLMs) in a more interpretable way. Unlike traditional benchmarks, report cards provide clear summaries of a model's abilities in specific areas, making it easier to understand their performance. The authors establish criteria for assessing these report cards, focusing on how well they differentiate models, accurately reflect their capabilities, and are understandable to humans. They also present an algorithm to create these report cards automatically and show through experiments that this approach offers deeper insights into LLMs than conventional methods."
                },
                "zh": {
                    "title": "报告卡：评估大型语言模型的新方式",
                    "desc": "随着大型语言模型（LLMs）的快速发展，传统的定量基准难以准确评估其能力。我们提出了报告卡，这是一种人类可理解的自然语言总结，专注于模型在特定技能或主题上的表现。我们开发了一个评估报告卡的框架，基于特异性、真实性和可解释性三个标准进行评估。通过对流行的LLMs进行实验，我们证明报告卡提供了超越传统基准的见解，有助于实现对LLMs更可解释和全面的评估。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00921",
            "title": "Statically Contextualizing Large Language Models with Typed Holes",
            "url": "https://huggingface.co/papers/2409.00921",
            "abstract": "Large language models (LLMs) have reshaped the landscape of program synthesis. However, contemporary LLM-based code completion systems often hallucinate broken code because they lack appropriate context, particularly when working with definitions not in the training data nor near the cursor. This paper demonstrates that tight integration with the type and binding structure of a language, as exposed by its language server, can address this contextualization problem in a token-efficient manner. In short, we contend that AIs need IDEs, too! In particular, we integrate LLM code generation into the Hazel live program sketching environment. The Hazel Language Server identifies the type and typing context of the hole being filled, even in the presence of errors, ensuring that a meaningful program sketch is always available. This allows prompting with codebase-wide contextual information not lexically local to the cursor, nor necessarily in the same file, but that is likely to be semantically local to the developer's goal. Completions synthesized by the LLM are then iteratively refined via further dialog with the language server. To evaluate these techniques, we introduce MVUBench, a dataset of model-view-update (MVU) web applications. These applications serve as challenge problems due to their reliance on application-specific data structures. We find that contextualization with type definitions is particularly impactful. After introducing our ideas in the context of Hazel we duplicate our techniques and port MVUBench to TypeScript in order to validate the applicability of these methods to higher-resource languages. Finally, we outline ChatLSP, a conservative extension to the Language Server Protocol (LSP) that language servers can implement to expose capabilities that AI code completion systems of various designs can use to incorporate static context when generating prompts for an LLM.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 сентября",
                "en": "September 2",
                "zh": "9月2日"
            },
            "hash": "424294faa72337b6",
            "data": {
                "categories": [
                    "#dataset",
                    "#hallucinations",
                    "#long_context",
                    "#inference",
                    "#low_resource",
                    "#plp",
                    "#open_source",
                    "#architecture",
                    "#synthetic"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "ИИ нужны IDE: улучшение генерации кода с помощью языковых серверов",
                    "desc": "Эта статья описывает новый подход к интеграции больших языковых моделей (LLM) с языковыми серверами для улучшения синтеза кода. Авторы демонстрируют, как использование типов и структуры связывания языка может решить проблему контекстуализации при генерации кода. Они интегрируют LLM в среду Hazel и вводят набор данных MVUBench для оценки эффективности своего метода. Результаты показывают, что контекстуализация с определениями типов особенно эффективна для улучшения качества генерируемого кода."
                },
                "en": {
                    "title": "Empowering AI Code Completion with Contextual Awareness!",
                    "desc": "This paper addresses the limitations of large language models (LLMs) in code completion, particularly their tendency to generate incorrect code due to insufficient context. It proposes a solution by integrating LLMs with the type and binding structure provided by a language server, which enhances contextual awareness during code generation. The authors demonstrate this approach within the Hazel live program sketching environment, allowing for more accurate code completions by utilizing broader contextual information. They also introduce MVUBench, a dataset for evaluating these techniques, and suggest a new extension to the Language Server Protocol to improve AI-assisted coding."
                },
                "zh": {
                    "title": "让AI也需要集成开发环境！",
                    "desc": "大型语言模型（LLMs）在程序合成领域带来了重大变革。然而，现有的基于LLM的代码补全系统常常因为缺乏适当的上下文而生成错误的代码，尤其是在处理不在训练数据中的定义时。本文展示了与语言的类型和绑定结构紧密集成，可以有效解决这一上下文问题。我们提出将LLM代码生成集成到Hazel实时程序草图环境中，以确保在填补代码空缺时始终提供有意义的程序草图。"
                }
            }
        }
    ],
    "link_prev": "2024-09-05.html",
    "link_next": "2024-09-09.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "05.09",
        "en": "09/05",
        "zh": "9月5日"
    },
    "short_date_next": {
        "ru": "09.09",
        "en": "09/09",
        "zh": "9月9日"
    },
    "categories": {
        "#dataset": 6,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 7,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 2,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 6,
        "#survey": 2,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 7,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 1
    }
}