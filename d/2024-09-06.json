{
    "date": {
        "ru": "6 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 6",
        "zh": "9æœˆ6æ—¥"
    },
    "time_utc": "2024-09-06 09:00",
    "weekday": 4,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-06",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.01322",
            "title": "Guide-and-Rescale: Self-Guidance Mechanism for Effective Tuning-Free Real Image Editing",
            "url": "https://huggingface.co/papers/2409.01322",
            "abstract": "Despite recent advances in large-scale text-to-image generative models, manipulating real images with these models remains a challenging problem. The main limitations of existing editing methods are that they either fail to perform with consistent quality on a wide range of image edits or require time-consuming hyperparameter tuning or fine-tuning of the diffusion model to preserve the image-specific appearance of the input image. We propose a novel approach that is built upon a modified diffusion sampling process via the guidance mechanism. In this work, we explore the self-guidance technique to preserve the overall structure of the input image and its local regions appearance that should not be edited. In particular, we explicitly introduce layout-preserving energy functions that are aimed to save local and global structures of the source image. Additionally, we propose a noise rescaling mechanism that allows to preserve noise distribution by balancing the norms of classifier-free guidance and our proposed guiders during generation. Such a guiding approach does not require fine-tuning the diffusion model and exact inversion process. As a result, the proposed method provides a fast and high-quality editing mechanism. In our experiments, we show through human evaluation and quantitative analysis that the proposed method allows to produce desired editing which is more preferable by humans and also achieves a better trade-off between editing quality and preservation of the original image. Our code is available at https://github.com/FusionBrainLab/Guide-and-Rescale.",
            "score": 94,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "18431c1f871794ad",
            "data": {
                "categories": [
                    "#open_source",
                    "#diffusion",
                    "#architecture",
                    "#cv"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ°Ğ¼Ğ¾Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ self-guidance Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿ĞµÑ€ĞµĞ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Effortless Image Editing with Structure Preservation",
                    "desc": "This paper presents a new method for editing images using text-to-image generative models, addressing the limitations of existing techniques that struggle with quality and require extensive tuning. The authors introduce a modified diffusion sampling process that utilizes self-guidance to maintain the structure and appearance of the original image while allowing for effective edits. They implement layout-preserving energy functions to ensure that both local and global features of the source image are retained during the editing process. The proposed noise rescaling mechanism balances guidance norms, enabling fast and high-quality image editing without the need for fine-tuning the model."
                },
                "zh": {
                    "title": "é«˜æ•ˆå›¾åƒç¼–è¾‘çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•æ¥æ”¹å–„å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨çœŸå®å›¾åƒç¼–è¾‘ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬é€šè¿‡ä¿®æ”¹æ‰©æ•£é‡‡æ ·è¿‡ç¨‹ï¼Œå¼•å…¥è‡ªæˆ‘å¼•å¯¼æŠ€æœ¯ï¼Œä»¥ä¿æŒè¾“å…¥å›¾åƒçš„æ•´ä½“ç»“æ„å’Œå±€éƒ¨åŒºåŸŸçš„å¤–è§‚ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¸ƒå±€ä¿æŒèƒ½é‡å‡½æ•°ï¼Œä»¥ä¿æŠ¤æºå›¾åƒçš„å±€éƒ¨å’Œå…¨å±€ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å™ªå£°é‡æ ‡å®šæœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¹³è¡¡åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼å’Œæˆ‘ä»¬æå‡ºçš„å¼•å¯¼å™¨çš„èŒƒæ•°ï¼Œä»è€Œå®ç°å¿«é€Ÿé«˜è´¨é‡çš„å›¾åƒç¼–è¾‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.03752",
            "title": "Attention Heads of Large Language Models: A Survey",
            "url": "https://huggingface.co/papers/2409.03752",
            "abstract": "Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in various tasks but remain largely as black-box systems. Consequently, their development relies heavily on data-driven approaches, limiting performance enhancement through changes in internal architecture and reasoning pathways. As a result, many researchers have begun exploring the potential internal mechanisms of LLMs, aiming to identify the essence of their reasoning bottlenecks, with most studies focusing on attention heads. Our survey aims to shed light on the internal reasoning processes of LLMs by concentrating on the interpretability and underlying mechanisms of attention heads. We first distill the human thought process into a four-stage framework: Knowledge Recalling, In-Context Identification, Latent Reasoning, and Expression Preparation. Using this framework, we systematically review existing research to identify and categorize the functions of specific attention heads. Furthermore, we summarize the experimental methodologies used to discover these special heads, dividing them into two categories: Modeling-Free methods and Modeling-Required methods. Also, we outline relevant evaluation methods and benchmarks. Finally, we discuss the limitations of current research and propose several potential future directions. Our reference list is open-sourced at https://github.com/IAAR-Shanghai/Awesome-Attention-Heads.",
            "score": 87,
            "issue_id": 1,
            "pub_date": "2024-09-05",
            "pub_date_card": {
                "ru": "5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 5",
                "zh": "9æœˆ5æ—¥"
            },
            "hash": "d79e296c0a5e1b88",
            "data": {
                "categories": [
                    "#reasoning",
                    "#survey",
                    "#cv",
                    "#interpretability",
                    "#benchmark",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑÑ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ ĞµĞµ Ğ´Ğ»Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² LLM. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ğ¸Ñ… Ğ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ±ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ÑÑ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚."
                },
                "en": {
                    "title": "Unlocking the Black Box: Understanding Attention Heads in LLMs",
                    "desc": "This paper explores the internal reasoning mechanisms of Large Language Models (LLMs), particularly focusing on attention heads. It introduces a four-stage framework that mirrors human thought processes: Knowledge Recalling, In-Context Identification, Latent Reasoning, and Expression Preparation. The authors systematically review existing studies to categorize the functions of attention heads and discuss the methodologies used to identify them, distinguishing between Modeling-Free and Modeling-Required approaches. Additionally, the paper highlights current research limitations and suggests future research directions to enhance the interpretability of LLMs."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æœºåˆ¶",
                    "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å†…éƒ¨æ¨ç†è¿‡ç¨‹çš„å¯è§£é‡Šæ€§ï¼Œç‰¹åˆ«å…³æ³¨æ³¨æ„åŠ›å¤´çš„æœºåˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå››é˜¶æ®µæ¡†æ¶ï¼Œåˆ†åˆ«æ˜¯çŸ¥è¯†å›å¿†ã€ä¸Šä¸‹æ–‡è¯†åˆ«ã€æ½œåœ¨æ¨ç†å’Œè¡¨è¾¾å‡†å¤‡ï¼Œä»¥å¸®åŠ©ç†è§£äººç±»æ€ç»´è¿‡ç¨‹ã€‚é€šè¿‡ç³»ç»Ÿå›é¡¾ç°æœ‰ç ”ç©¶ï¼Œæˆ‘ä»¬è¯†åˆ«å¹¶åˆ†ç±»äº†ç‰¹å®šæ³¨æ„åŠ›å¤´çš„åŠŸèƒ½ï¼Œå¹¶æ€»ç»“äº†å‘ç°è¿™äº›ç‰¹æ®Šå¤´éƒ¨çš„å®éªŒæ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å½“å‰ç ”ç©¶çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.01944",
            "title": "FuzzCoder: Byte-level Fuzzing Test via Large Language Model",
            "url": "https://huggingface.co/papers/2409.01944",
            "abstract": "Fuzzing is an important dynamic program analysis technique designed for finding vulnerabilities in complex software. Fuzzing involves presenting a target program with crafted malicious input to cause crashes, buffer overflows, memory errors, and exceptions. Crafting malicious inputs in an efficient manner is a difficult open problem and the best approaches often apply uniform random mutations to pre-existing valid inputs. In this work, we propose to adopt fine-tuned large language models (FuzzCoder) to learn patterns in the input files from successful attacks to guide future fuzzing explorations. Specifically, we develop a framework to leverage the code LLMs to guide the mutation process of inputs in fuzzing. The mutation process is formulated as the sequence-to-sequence modeling, where LLM receives a sequence of bytes and then outputs the mutated byte sequence. FuzzCoder is fine-tuned on the created instruction dataset (Fuzz-Instruct), where the successful fuzzing history is collected from the heuristic fuzzing tool. FuzzCoder can predict mutation locations and strategies locations in input files to trigger abnormal behaviors of the program. Experimental results show that FuzzCoder based on AFL (American Fuzzy Lop) gain significant improvements in terms of effective proportion of mutation (EPM) and number of crashes (NC) for various input formats including ELF, JPG, MP3, and XML.",
            "score": 44,
            "issue_id": 1,
            "pub_date": "2024-09-03",
            "pub_date_card": {
                "ru": "3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 3",
                "zh": "9æœˆ3æ—¥"
            },
            "hash": "e7eb6566405186be",
            "data": {
                "categories": [
                    "#dataset",
                    "#security",
                    "#training",
                    "#optimization",
                    "#plp"
                ],
                "emoji": "ğŸ",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ·Ğ·Ğ¸Ğ½Ğ³ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ„Ğ°Ğ·Ğ·Ğ¸Ğ½Ğ³Ñƒ - Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ÑƒÑ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ FuzzCoder Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒÑ‚Ğ°Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ°Ğ·Ğ·Ğ¸Ğ½Ğ³Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ² Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²."
                },
                "en": {
                    "title": "Enhancing Fuzzing with Language Models for Better Vulnerability Detection",
                    "desc": "This paper introduces FuzzCoder, a novel approach to improve fuzzing techniques in software vulnerability detection. By utilizing fine-tuned large language models, FuzzCoder learns from successful attack patterns to enhance the input mutation process. The mutation is treated as a sequence-to-sequence problem, where the model generates new byte sequences based on existing inputs. Experimental results demonstrate that FuzzCoder significantly increases the effectiveness of fuzzing, leading to more crashes and better coverage across different file formats."
                },
                "zh": {
                    "title": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹æå‡æ¨¡ç³Šæµ‹è¯•æ•ˆç‡",
                    "desc": "æ¨¡ç³Šæµ‹è¯•æ˜¯ä¸€ç§åŠ¨æ€ç¨‹åºåˆ†ææŠ€æœ¯ï¼Œæ—¨åœ¨å‘ç°å¤æ‚è½¯ä»¶ä¸­çš„æ¼æ´ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFuzzCoderçš„æ¨¡å‹ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å­¦ä¹ æˆåŠŸæ”»å‡»ä¸­çš„è¾“å…¥æ–‡ä»¶æ¨¡å¼ï¼Œä»¥æŒ‡å¯¼æœªæ¥çš„æ¨¡ç³Šæµ‹è¯•æ¢ç´¢ã€‚æˆ‘ä»¬å°†è¾“å…¥çš„å˜å¼‚è¿‡ç¨‹å»ºæ¨¡ä¸ºåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œæ¨¡å‹æ¥æ”¶å­—èŠ‚åºåˆ—å¹¶è¾“å‡ºå˜å¼‚åçš„å­—èŠ‚åºåˆ—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºFuzzCoderçš„æ¨¡ç³Šæµ‹è¯•åœ¨æœ‰æ•ˆå˜å¼‚æ¯”ä¾‹å’Œå´©æºƒæ¬¡æ•°æ–¹é¢æ˜¾è‘—æé«˜ï¼Œé€‚ç”¨äºå¤šç§è¾“å…¥æ ¼å¼ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.03512",
            "title": "From MOOC to MAIC: Reshaping Online Teaching and Learning through LLM-driven Agents",
            "url": "https://huggingface.co/papers/2409.03512",
            "abstract": "Since the first instances of online education, where courses were uploaded to accessible and shared online platforms, this form of scaling the dissemination of human knowledge to reach a broader audience has sparked extensive discussion and widespread adoption. Recognizing that personalized learning still holds significant potential for improvement, new AI technologies have been continuously integrated into this learning format, resulting in a variety of educational AI applications such as educational recommendation and intelligent tutoring. The emergence of intelligence in large language models (LLMs) has allowed for these educational enhancements to be built upon a unified foundational model, enabling deeper integration. In this context, we propose MAIC (Massive AI-empowered Course), a new form of online education that leverages LLM-driven multi-agent systems to construct an AI-augmented classroom, balancing scalability with adaptivity. Beyond exploring the conceptual framework and technical innovations, we conduct preliminary experiments at Tsinghua University, one of China's leading universities. Drawing from over 100,000 learning records of more than 500 students, we obtain a series of valuable observations and initial analyses. This project will continue to evolve, ultimately aiming to establish a comprehensive open platform that supports and unifies research, technology, and applications in exploring the possibilities of online education in the era of large model AI. We envision this platform as a collaborative hub, bringing together educators, researchers, and innovators to collectively explore the future of AI-driven online education.",
            "score": 26,
            "issue_id": 1,
            "pub_date": "2024-09-05",
            "pub_date_card": {
                "ru": "5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 5",
                "zh": "9æœˆ5æ—¥"
            },
            "hash": "63d56825655d908a",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#multilingual",
                    "#agents",
                    "#open_source"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "MAIC: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ AI Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MAIC (Massive AI-empowered Course), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ AI-Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ¸Ñ‚ĞµÑ‚Ğµ Ğ¦Ğ¸Ğ½Ñ…ÑƒĞ°, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ±Ğ¾Ğ»ĞµĞµ 100 000 Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ¾Ğ± Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 500 ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ¾Ğ². ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ĞµĞ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ AI. Ğ¦ĞµĞ»ÑŒ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹ - ÑÑ‚Ğ°Ñ‚ÑŒ Ñ†ĞµĞ½Ñ‚Ñ€Ğ¾Ğ¼ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ¿ĞµĞ´Ğ°Ğ³Ğ¾Ğ³Ğ¾Ğ², Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ½Ğ¾Ğ²Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ AI-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Online Learning with AI-Driven Personalization",
                    "desc": "This paper introduces MAIC (Massive AI-empowered Course), a novel approach to online education that utilizes large language models (LLMs) and multi-agent systems to create an AI-enhanced learning environment. The integration of AI technologies aims to improve personalized learning experiences while maintaining scalability for a larger audience. Preliminary experiments conducted at Tsinghua University analyzed over 100,000 learning records from more than 500 students, providing insights into the effectiveness of this approach. The ultimate goal is to develop a comprehensive open platform that fosters collaboration among educators, researchers, and innovators in the field of AI-driven online education."
                },
                "zh": {
                    "title": "å¤§è§„æ¨¡AIèµ‹èƒ½ï¼Œé‡å¡‘åœ¨çº¿æ•™è‚²æœªæ¥",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åœ¨çº¿æ•™è‚²å½¢å¼ï¼Œç§°ä¸ºMAICï¼ˆå¤§è§„æ¨¡äººå·¥æ™ºèƒ½èµ‹èƒ½è¯¾ç¨‹ï¼‰ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¥æ„å»ºå¢å¼ºå‹è¯¾å ‚ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒMAICåœ¨å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ä¹‹é—´å–å¾—äº†å¹³è¡¡ï¼Œæ—¨åœ¨æå‡ä¸ªæ€§åŒ–å­¦ä¹ çš„æ•ˆæœã€‚æˆ‘ä»¬åœ¨æ¸…åå¤§å­¦è¿›è¡Œçš„åˆæ­¥å®éªŒåŸºäºè¶…è¿‡100,000æ¡å­¦ä¹ è®°å½•ï¼Œåˆ†æäº†500å¤šåå­¦ç”Ÿçš„å­¦ä¹ æƒ…å†µï¼Œè·å¾—äº†ä¸€ç³»åˆ—æœ‰ä»·å€¼çš„è§‚å¯Ÿç»“æœã€‚æœ€ç»ˆç›®æ ‡æ˜¯å»ºç«‹ä¸€ä¸ªç»¼åˆå¼€æ”¾å¹³å°ï¼Œæ”¯æŒç ”ç©¶ã€æŠ€æœ¯å’Œåº”ç”¨çš„ç»Ÿä¸€ï¼Œæ¢ç´¢å¤§æ¨¡å‹äººå·¥æ™ºèƒ½æ—¶ä»£åœ¨çº¿æ•™è‚²çš„å¯èƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.03718",
            "title": "Geometry Image Diffusion: Fast and Data-Efficient Text-to-3D with Image-Based Surface Representation",
            "url": "https://huggingface.co/papers/2409.03718",
            "abstract": "Generating high-quality 3D objects from textual descriptions remains a challenging problem due to computational cost, the scarcity of 3D data, and complex 3D representations. We introduce Geometry Image Diffusion (GIMDiffusion), a novel Text-to-3D model that utilizes geometry images to efficiently represent 3D shapes using 2D images, thereby avoiding the need for complex 3D-aware architectures. By integrating a Collaborative Control mechanism, we exploit the rich 2D priors of existing Text-to-Image models such as Stable Diffusion. This enables strong generalization even with limited 3D training data (allowing us to use only high-quality training data) as well as retaining compatibility with guidance techniques such as IPAdapter. In short, GIMDiffusion enables the generation of 3D assets at speeds comparable to current Text-to-Image models. The generated objects consist of semantically meaningful, separate parts and include internal structures, enhancing both usability and versatility.",
            "score": 25,
            "issue_id": 1,
            "pub_date": "2024-09-05",
            "pub_date_card": {
                "ru": "5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 5",
                "zh": "9æœˆ5æ—¥"
            },
            "hash": "eaae7513d598cd8a",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#diffusion",
                    "#architecture",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ 3D Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 2D-Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹",
                    "desc": "GIMDiffusion - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D-Ñ„Ğ¾Ñ€Ğ¼ Ğ² Ğ²Ğ¸Ğ´Ğµ 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ 2D-Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Text-to-Image. GIMDiffusion Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ ÑĞ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Text-to-Image, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹."
                },
                "en": {
                    "title": "Transforming Text to 3D: Fast and Flexible with GIMDiffusion",
                    "desc": "The paper presents Geometry Image Diffusion (GIMDiffusion), a new model for generating 3D objects from text descriptions. It uses geometry images to represent 3D shapes in a 2D format, simplifying the process and reducing computational demands. By leveraging existing Text-to-Image models, GIMDiffusion can generalize well even with limited 3D data, ensuring high-quality outputs. This approach allows for the rapid creation of detailed 3D assets that are both functional and adaptable."
                },
                "zh": {
                    "title": "é«˜æ•ˆç”Ÿæˆä¸‰ç»´å¯¹è±¡çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°ä¸‰ç»´æ¨¡å‹ï¼Œç§°ä¸ºå‡ ä½•å›¾åƒæ‰©æ•£ï¼ˆGIMDiffusionï¼‰ï¼Œå®ƒåˆ©ç”¨å‡ ä½•å›¾åƒé«˜æ•ˆåœ°è¡¨ç¤ºä¸‰ç»´å½¢çŠ¶ã€‚é€šè¿‡ä½¿ç”¨äºŒç»´å›¾åƒï¼ŒGIMDiffusioné¿å…äº†å¤æ‚çš„ä¸‰ç»´æ¶æ„ï¼Œä»è€Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚è¯¥æ¨¡å‹ç»“åˆäº†åä½œæ§åˆ¶æœºåˆ¶ï¼Œåˆ©ç”¨ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ä¸°å¯ŒäºŒç»´å…ˆéªŒçŸ¥è¯†ï¼Œå®ç°äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ€ç»ˆï¼ŒGIMDiffusionèƒ½å¤Ÿä»¥ä¸å½“å‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ç›¸å½“çš„é€Ÿåº¦ç”Ÿæˆå…·æœ‰è¯­ä¹‰æ„ä¹‰çš„ä¸‰ç»´å¯¹è±¡ï¼Œæå‡äº†å¯ç”¨æ€§å’Œå¤šåŠŸèƒ½æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.03420",
            "title": "mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding",
            "url": "https://huggingface.co/papers/2409.03420",
            "abstract": "Multimodel Large Language Models(MLLMs) have achieved promising OCR-free Document Understanding performance by increasing the supported resolution of document images. However, this comes at the cost of generating thousands of visual tokens for a single document image, leading to excessive GPU memory and slower inference times, particularly in multi-page document comprehension. In this work, to address these challenges, we propose a High-resolution DocCompressor module to compress each high-resolution document image into 324 tokens, guided by low-resolution global visual features. With this compression module, to strengthen multi-page document comprehension ability and balance both token efficiency and question-answering performance, we develop the DocOwl2 under a three-stage training framework: Single-image Pretraining, Multi-image Continue-pretraining, and Multi-task Finetuning. DocOwl2 sets a new state-of-the-art across multi-page document understanding benchmarks and reduces first token latency by more than 50%, demonstrating advanced capabilities in multi-page questioning answering, explanation with evidence pages, and cross-page structure understanding. Additionally, compared to single-image MLLMs trained on similar data, our DocOwl2 achieves comparable single-page understanding performance with less than 20% of the visual tokens. Our codes, models, and data are publicly available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2.",
            "score": 23,
            "issue_id": 1,
            "pub_date": "2024-09-05",
            "pub_date_card": {
                "ru": "5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 5",
                "zh": "9æœˆ5æ—¥"
            },
            "hash": "0a6e9a759ec89f2f",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#cv",
                    "#training",
                    "#inference",
                    "#optimization",
                    "#benchmark",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "ğŸ“„",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DocOwl2 - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· OCR. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ High-resolution DocCompressor Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² 324 Ñ‚Ğ¾ĞºĞµĞ½Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸, Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ°Ñ Ğ´Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°. DocOwl2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°."
                },
                "en": {
                    "title": "Efficient Multi-Page Document Understanding with DocOwl2",
                    "desc": "This paper introduces the High-resolution DocCompressor module, which reduces the number of visual tokens generated from high-resolution document images to improve efficiency in multi-page document understanding. The proposed DocOwl2 model utilizes a three-stage training framework to enhance its ability to comprehend and answer questions about documents while maintaining a balance between token efficiency and performance. By achieving a significant reduction in first token latency and setting new benchmarks in multi-page document understanding, DocOwl2 demonstrates its advanced capabilities. Furthermore, it maintains competitive performance in single-page understanding with significantly fewer visual tokens compared to traditional models."
                },
                "zh": {
                    "title": "é«˜æ•ˆå‹ç¼©ï¼Œæå‡æ–‡æ¡£ç†è§£èƒ½åŠ›",
                    "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ— OCRæ–‡æ¡£ç†è§£æ–¹é¢å–å¾—äº†è‰¯å¥½çš„æ•ˆæœï¼Œä½†ç”Ÿæˆå¤§é‡è§†è§‰æ ‡è®°å¯¼è‡´GPUå†…å­˜æ¶ˆè€—è¿‡å¤§å’Œæ¨ç†é€Ÿåº¦å˜æ…¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜åˆ†è¾¨ç‡æ–‡æ¡£å‹ç¼©æ¨¡å—ï¼Œå°†é«˜åˆ†è¾¨ç‡æ–‡æ¡£å›¾åƒå‹ç¼©ä¸º324ä¸ªæ ‡è®°ï¼Œå¹¶é€šè¿‡ä½åˆ†è¾¨ç‡çš„å…¨å±€è§†è§‰ç‰¹å¾è¿›è¡ŒæŒ‡å¯¼ã€‚æˆ‘ä»¬åœ¨ä¸‰é˜¶æ®µè®­ç»ƒæ¡†æ¶ä¸‹å¼€å‘äº†DocOwl2ï¼Œæ˜¾è‘—æé«˜äº†å¤šé¡µæ–‡æ¡£ç†è§£èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šé¡µé—®ç­”å’Œè·¨é¡µç»“æ„ç†è§£æ–¹é¢è®¾ç«‹äº†æ–°çš„åŸºå‡†ã€‚ä¸å•å›¾åƒMLLMsç›¸æ¯”ï¼ŒDocOwl2åœ¨å•é¡µç†è§£æ€§èƒ½ä¸Šè¡¨ç°ç›¸å½“ï¼Œä½†è§†è§‰æ ‡è®°æ•°é‡å‡å°‘äº†20%ä»¥ä¸‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.03643",
            "title": "CDM: A Reliable Metric for Fair and Accurate Formula Recognition Evaluation",
            "url": "https://huggingface.co/papers/2409.03643",
            "abstract": "Formula recognition presents significant challenges due to the complicated structure and varied notation of mathematical expressions. Despite continuous advancements in formula recognition models, the evaluation metrics employed by these models, such as BLEU and Edit Distance, still exhibit notable limitations. They overlook the fact that the same formula has diverse representations and is highly sensitive to the distribution of training data, thereby causing the unfairness in formula recognition evaluation. To this end, we propose a Character Detection Matching (CDM) metric, ensuring the evaluation objectivity by designing a image-level rather than LaTex-level metric score. Specifically, CDM renders both the model-predicted LaTeX and the ground-truth LaTeX formulas into image-formatted formulas, then employs visual feature extraction and localization techniques for precise character-level matching, incorporating spatial position information. Such a spatially-aware and character-matching method offers a more accurate and equitable evaluation compared with previous BLEU and Edit Distance metrics that rely solely on text-based character matching. Experimentally, we evaluated various formula recognition models using CDM, BLEU, and ExpRate metrics. Their results demonstrate that the CDM aligns more closely with human evaluation standards and provides a fairer comparison across different models by eliminating discrepancies caused by diverse formula representations.",
            "score": 18,
            "issue_id": 1,
            "pub_date": "2024-09-05",
            "pub_date_card": {
                "ru": "5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 5",
                "zh": "9æœˆ5æ—¥"
            },
            "hash": "9036259aa729330f",
            "data": {
                "categories": [
                    "#cv",
                    "#math",
                    "#ethics",
                    "#optimization",
                    "#benchmark"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ¡Ğ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»: Ğ¾Ñ‚ LaTeX Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ» - Character Detection Matching (CDM). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº, CDM Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ½Ğµ LaTeX-ĞºĞ¾Ğ´Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¸ Ñ‚Ğ¾Ğ¹ Ğ¶Ğµ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ CDM Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ°Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»."
                },
                "en": {
                    "title": "Revolutionizing Formula Recognition Evaluation with CDM",
                    "desc": "This paper addresses the challenges in evaluating formula recognition models due to the complex nature of mathematical expressions and their varied notations. The authors highlight the limitations of traditional evaluation metrics like BLEU and Edit Distance, which fail to account for different representations of the same formula and are sensitive to training data distribution. To improve evaluation fairness, they introduce a new metric called Character Detection Matching (CDM), which evaluates formulas based on image representations rather than text. CDM utilizes visual feature extraction and spatial localization for character-level matching, resulting in a more accurate assessment that aligns better with human evaluations compared to existing metrics."
                },
                "zh": {
                    "title": "å…¬å¼è¯†åˆ«çš„æ–°æ ‡å‡†ï¼šå­—ç¬¦æ£€æµ‹åŒ¹é…æŒ‡æ ‡",
                    "desc": "å…¬å¼è¯†åˆ«é¢ä¸´ç€å¤æ‚ç»“æ„å’Œå¤šæ ·ç¬¦å·çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡å¦‚BLEUå’Œç¼–è¾‘è·ç¦»å­˜åœ¨æ˜æ˜¾å±€é™ï¼Œæ— æ³•å…¬å¹³è¯„ä¼°å…¬å¼è¯†åˆ«çš„æ•ˆæœã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å­—ç¬¦æ£€æµ‹åŒ¹é…ï¼ˆCDMï¼‰æŒ‡æ ‡ï¼Œé€šè¿‡å›¾åƒçº§åˆ«çš„è¯„ä¼°æ–¹æ³•æé«˜è¯„ä¼°çš„å®¢è§‚æ€§ã€‚CDMé€šè¿‡å°†é¢„æµ‹çš„LaTeXå’ŒçœŸå®çš„LaTeXå…¬å¼è½¬åŒ–ä¸ºå›¾åƒæ ¼å¼ï¼Œåˆ©ç”¨è§†è§‰ç‰¹å¾æå–å’Œå®šä½æŠ€æœ¯è¿›è¡Œç²¾ç¡®çš„å­—ç¬¦çº§åŒ¹é…ï¼Œä»è€Œæä¾›æ›´å‡†ç¡®å’Œå…¬å¹³çš„è¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.03753",
            "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
            "url": "https://huggingface.co/papers/2409.03753",
            "abstract": "The increasing availability of real-world conversation data offers exciting opportunities for researchers to study user-chatbot interactions. However, the sheer volume of this data makes manually examining individual conversations impractical. To overcome this challenge, we introduce WildVis, an interactive tool that enables fast, versatile, and large-scale conversation analysis. WildVis provides search and visualization capabilities in the text and embedding spaces based on a list of criteria. To manage million-scale datasets, we implemented optimizations including search index construction, embedding precomputation and compression, and caching to ensure responsive user interactions within seconds. We demonstrate WildVis's utility through three case studies: facilitating chatbot misuse research, visualizing and comparing topic distributions across datasets, and characterizing user-specific conversation patterns. WildVis is open-source and designed to be extendable, supporting additional datasets and customized search and visualization functionalities.",
            "score": 18,
            "issue_id": 1,
            "pub_date": "2024-09-05",
            "pub_date_card": {
                "ru": "5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 5",
                "zh": "9æœˆ5æ—¥"
            },
            "hash": "b58bcce018642f84",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#training",
                    "#data",
                    "#optimization",
                    "#benchmark",
                    "#open_source",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜",
                    "desc": "WildVis - ÑÑ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ². Ğ”Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ğ´ĞµĞºÑĞ° Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². WildVis Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞ¼ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "WildVis: Revolutionizing Large-Scale Conversation Analysis",
                    "desc": "This paper presents WildVis, an innovative tool designed for analyzing large-scale conversation data between users and chatbots. It addresses the challenge of manually reviewing extensive datasets by providing efficient search and visualization features in both text and embedding spaces. WildVis incorporates optimizations like search index construction and embedding precomputation to ensure quick responses, even with millions of conversations. The tool is open-source and allows for customization, making it suitable for various research applications, including chatbot misuse analysis and user interaction pattern exploration."
                },
                "zh": {
                    "title": "WildVisï¼šé«˜æ•ˆåˆ†æèŠå¤©æ•°æ®çš„å·¥å…·",
                    "desc": "éšç€çœŸå®å¯¹è¯æ•°æ®çš„å¢åŠ ï¼Œç ”ç©¶äººå‘˜å¯ä»¥æ›´å¥½åœ°ç ”ç©¶ç”¨æˆ·ä¸èŠå¤©æœºå™¨äººçš„äº’åŠ¨ã€‚ç„¶è€Œï¼Œæ•°æ®é‡åºå¤§ä½¿å¾—æ‰‹åŠ¨æ£€æŸ¥æ¯ä¸ªå¯¹è¯å˜å¾—ä¸åˆ‡å®é™…ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†WildVisï¼Œè¿™æ˜¯ä¸€ä¸ªäº¤äº’å¼å·¥å…·ï¼Œå¯ä»¥å¿«é€Ÿã€çµæ´»åœ°è¿›è¡Œå¤§è§„æ¨¡å¯¹è¯åˆ†æã€‚WildVisæä¾›åŸºäºå¤šç§æ ‡å‡†çš„æ–‡æœ¬å’ŒåµŒå…¥ç©ºé—´çš„æœç´¢å’Œå¯è§†åŒ–åŠŸèƒ½ï¼Œæ”¯æŒå¯¹ç™¾ä¸‡çº§æ•°æ®é›†çš„é«˜æ•ˆç®¡ç†ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.02392",
            "title": "Building Math Agents with Multi-Turn Iterative Preference Learning",
            "url": "https://huggingface.co/papers/2409.02392",
            "abstract": "Recent studies have shown that large language models' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model's performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2024-09-04",
            "pub_date_card": {
                "ru": "4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 4",
                "zh": "9æœˆ4æ—¥"
            },
            "hash": "9cb2dcc6706cca00",
            "data": {
                "categories": [
                    "#reasoning",
                    "#dataset",
                    "#training",
                    "#math",
                    "#optimization",
                    "#rlhf",
                    "#architecture",
                    "#synthetic"
                ],
                "emoji": "ğŸ§®",
                "ru": {
                    "title": "Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² ĞºĞ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Gemma Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GSM8K Ğ¸ MATH Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼."
                },
                "en": {
                    "title": "Enhancing LLMs with Multi-Turn Direct Preference Learning for Math Problem Solving",
                    "desc": "This paper explores how to enhance large language models' (LLMs) ability to solve mathematical problems by using external tools and multi-turn Chain-of-Thought (CoT) reasoning. It introduces a new approach called multi-turn direct preference learning, which is designed to handle the complexities of multi-turn interactions and tool integration. The framework includes specific implementations like multi-turn DPO and multi-turn KTO, which optimize preferences based on feedback from code interpreters. The results show significant performance improvements in LLMs when tested on the GSM8K and MATH datasets, indicating the effectiveness of this new learning framework."
                },
                "zh": {
                    "title": "æå‡æ•°å­¦æ¨ç†èƒ½åŠ›çš„å¤šè½®å­¦ä¹ æ¡†æ¶",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡æ•´åˆå¤–éƒ¨å·¥å…·å’Œå¤šè½®æ¨ç†æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šè½®ç›´æ¥åå¥½å­¦ä¹ æ¡†æ¶ï¼Œä¸“é—¨é’ˆå¯¹å·¥å…·é›†æˆçš„æ•°å­¦æ¨ç†ä»»åŠ¡ï¼Œåˆ©ç”¨ä»£ç è§£é‡Šå™¨çš„åé¦ˆæ¥ä¼˜åŒ–æ¨¡å‹çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬å¤šè½®ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œå¤šè½®çŸ¥è¯†è½¬ç§»ä¼˜åŒ–ï¼ˆKTOï¼‰ä½œä¸ºå…·ä½“å®ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¥æ¡†æ¶è®­ç»ƒçš„æ¨¡å‹åœ¨GSM8Kå’ŒMATHæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—æå‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.03525",
            "title": "FrozenSeg: Harmonizing Frozen Foundation Models for Open-Vocabulary Segmentation",
            "url": "https://huggingface.co/papers/2409.03525",
            "abstract": "Open-vocabulary segmentation poses significant challenges, as it requires segmenting and recognizing objects across an open set of categories in unconstrained environments. Building on the success of powerful vision-language (ViL) foundation models, such as CLIP, recent efforts sought to harness their zero-short capabilities to recognize unseen categories. Despite notable performance improvements, these models still encounter the critical issue of generating precise mask proposals for unseen categories and scenarios, resulting in inferior segmentation performance eventually. To address this challenge, we introduce a novel approach, FrozenSeg, designed to integrate spatial knowledge from a localization foundation model (e.g., SAM) and semantic knowledge extracted from a ViL model (e.g., CLIP), in a synergistic framework. Taking the ViL model's visual encoder as the feature backbone, we inject the space-aware feature into the learnable queries and CLIP features within the transformer decoder. In addition, we devise a mask proposal ensemble strategy for further improving the recall rate and mask quality. To fully exploit pre-trained knowledge while minimizing training overhead, we freeze both foundation models, focusing optimization efforts solely on a lightweight transformer decoder for mask proposal generation-the performance bottleneck. Extensive experiments demonstrate that FrozenSeg advances state-of-the-art results across various segmentation benchmarks, trained exclusively on COCO panoptic data, and tested in a zero-shot manner. Code is available at https://github.com/chenxi52/FrozenSeg.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-09-05",
            "pub_date_card": {
                "ru": "5 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 5",
                "zh": "9æœˆ5æ—¥"
            },
            "hash": "d324ecd311d0064b",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#transfer_learning",
                    "#benchmark",
                    "#open_source",
                    "#architecture",
                    "#multimodal"
                ],
                "emoji": "ğŸ§Š",
                "ru": {
                    "title": "FrozenSeg: ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FrozenSeg. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, SAM) Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ vision-language (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, CLIP). FrozenSeg Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FrozenSeg Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… COCO panoptic."
                },
                "en": {
                    "title": "FrozenSeg: Bridging Spatial and Semantic Knowledge for Open-Vocabulary Segmentation",
                    "desc": "This paper presents FrozenSeg, a new method for open-vocabulary segmentation that combines spatial and semantic knowledge from different foundation models. By leveraging the visual encoder of a vision-language model like CLIP and integrating it with a localization model, FrozenSeg enhances the generation of mask proposals for unseen object categories. The approach focuses on optimizing a lightweight transformer decoder while keeping the foundation models frozen, which reduces training time and complexity. Experimental results show that FrozenSeg achieves state-of-the-art performance on segmentation tasks, particularly in zero-shot scenarios."
                },
                "zh": {
                    "title": "èåˆç©ºé—´ä¸è¯­ä¹‰çŸ¥è¯†çš„åˆ†å‰²æ–°æ–¹æ³•",
                    "desc": "å¼€æ”¾è¯æ±‡åˆ†å‰²é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒéœ€è¦åœ¨ä¸å—é™åˆ¶çš„ç¯å¢ƒä¸­å¯¹å¼€æ”¾ç±»åˆ«é›†åˆä¸­çš„ç‰©ä½“è¿›è¡Œåˆ†å‰²å’Œè¯†åˆ«ã€‚åŸºäºå¼ºå¤§çš„è§†è§‰-è¯­è¨€ï¼ˆViLï¼‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„æˆåŠŸï¼Œæœ€è¿‘çš„ç ”ç©¶è¯•å›¾åˆ©ç”¨å…¶é›¶æ ·æœ¬èƒ½åŠ›æ¥è¯†åˆ«æœªè§ç±»åˆ«ã€‚å°½ç®¡æ€§èƒ½æœ‰æ‰€æå‡ï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆæœªè§ç±»åˆ«å’Œåœºæ™¯çš„ç²¾ç¡®æ©ç æè®®æ–¹é¢ä»ç„¶å­˜åœ¨å…³é”®é—®é¢˜ï¼Œå¯¼è‡´åˆ†å‰²æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•FrozenSegï¼Œæ—¨åœ¨å°†å®šä½åŸºç¡€æ¨¡å‹ï¼ˆå¦‚SAMï¼‰çš„ç©ºé—´çŸ¥è¯†ä¸ä»ViLæ¨¡å‹ï¼ˆå¦‚CLIPï¼‰æå–çš„è¯­ä¹‰çŸ¥è¯†ç»“åˆåœ¨ä¸€ä¸ªååŒæ¡†æ¶ä¸­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00844",
            "title": "Report Cards: Qualitative Evaluation of Language Models Using Natural Language Summaries",
            "url": "https://huggingface.co/papers/2409.00844",
            "abstract": "The rapid development and dynamic nature of large language models (LLMs) make it difficult for conventional quantitative benchmarks to accurately assess their capabilities. We propose report cards, which are human-interpretable, natural language summaries of model behavior for specific skills or topics. We develop a framework to evaluate report cards based on three criteria: specificity (ability to distinguish between models), faithfulness (accurate representation of model capabilities), and interpretability (clarity and relevance to humans). We also propose an iterative algorithm for generating report cards without human supervision and explore its efficacy by ablating various design choices. Through experimentation with popular LLMs, we demonstrate that report cards provide insights beyond traditional benchmarks and can help address the need for a more interpretable and holistic evaluation of LLMs.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-09-01",
            "pub_date_card": {
                "ru": "1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 1",
                "zh": "9æœˆ1æ—¥"
            },
            "hash": "7d104d8e5fa06209",
            "data": {
                "categories": [
                    "#survey",
                    "#interpretability",
                    "#architecture",
                    "#benchmark"
                ],
                "emoji": "ğŸ“Š",
                "ru": {
                    "title": "Ğ¢Ğ°Ğ±ĞµĞ»Ğ¸ ÑƒÑĞ¿ĞµĞ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 'Ñ‚Ğ°Ğ±ĞµĞ»ĞµĞ¹ ÑƒÑĞ¿ĞµĞ²Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸'. Ğ­Ñ‚Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ, Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ¸Ñ… Ñ‚Ğ°Ğ±ĞµĞ»ĞµĞ¹: ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ°Ğ±ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°."
                },
                "en": {
                    "title": "Report Cards: A New Way to Evaluate Language Models",
                    "desc": "This paper introduces a new method called report cards to evaluate large language models (LLMs) in a more interpretable way. Unlike traditional benchmarks, report cards provide clear summaries of a model's abilities in specific areas, making it easier to understand their performance. The authors establish criteria for assessing these report cards, focusing on how well they differentiate models, accurately reflect their capabilities, and are understandable to humans. They also present an algorithm to create these report cards automatically and show through experiments that this approach offers deeper insights into LLMs than conventional methods."
                },
                "zh": {
                    "title": "æŠ¥å‘Šå¡ï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æ–¹å¼",
                    "desc": "éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œä¼ ç»Ÿçš„å®šé‡åŸºå‡†éš¾ä»¥å‡†ç¡®è¯„ä¼°å…¶èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†æŠ¥å‘Šå¡ï¼Œè¿™æ˜¯ä¸€ç§äººç±»å¯ç†è§£çš„è‡ªç„¶è¯­è¨€æ€»ç»“ï¼Œä¸“æ³¨äºæ¨¡å‹åœ¨ç‰¹å®šæŠ€èƒ½æˆ–ä¸»é¢˜ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè¯„ä¼°æŠ¥å‘Šå¡çš„æ¡†æ¶ï¼ŒåŸºäºç‰¹å¼‚æ€§ã€çœŸå®æ€§å’Œå¯è§£é‡Šæ€§ä¸‰ä¸ªæ ‡å‡†è¿›è¡Œè¯„ä¼°ã€‚é€šè¿‡å¯¹æµè¡Œçš„LLMsè¿›è¡Œå®éªŒï¼Œæˆ‘ä»¬è¯æ˜æŠ¥å‘Šå¡æä¾›äº†è¶…è¶Šä¼ ç»ŸåŸºå‡†çš„è§è§£ï¼Œæœ‰åŠ©äºå®ç°å¯¹LLMsæ›´å¯è§£é‡Šå’Œå…¨é¢çš„è¯„ä¼°ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.00921",
            "title": "Statically Contextualizing Large Language Models with Typed Holes",
            "url": "https://huggingface.co/papers/2409.00921",
            "abstract": "Large language models (LLMs) have reshaped the landscape of program synthesis. However, contemporary LLM-based code completion systems often hallucinate broken code because they lack appropriate context, particularly when working with definitions not in the training data nor near the cursor. This paper demonstrates that tight integration with the type and binding structure of a language, as exposed by its language server, can address this contextualization problem in a token-efficient manner. In short, we contend that AIs need IDEs, too! In particular, we integrate LLM code generation into the Hazel live program sketching environment. The Hazel Language Server identifies the type and typing context of the hole being filled, even in the presence of errors, ensuring that a meaningful program sketch is always available. This allows prompting with codebase-wide contextual information not lexically local to the cursor, nor necessarily in the same file, but that is likely to be semantically local to the developer's goal. Completions synthesized by the LLM are then iteratively refined via further dialog with the language server. To evaluate these techniques, we introduce MVUBench, a dataset of model-view-update (MVU) web applications. These applications serve as challenge problems due to their reliance on application-specific data structures. We find that contextualization with type definitions is particularly impactful. After introducing our ideas in the context of Hazel we duplicate our techniques and port MVUBench to TypeScript in order to validate the applicability of these methods to higher-resource languages. Finally, we outline ChatLSP, a conservative extension to the Language Server Protocol (LSP) that language servers can implement to expose capabilities that AI code completion systems of various designs can use to incorporate static context when generating prompts for an LLM.",
            "score": 3,
            "issue_id": 1,
            "pub_date": "2024-09-02",
            "pub_date_card": {
                "ru": "2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 2",
                "zh": "9æœˆ2æ—¥"
            },
            "hash": "424294faa72337b6",
            "data": {
                "categories": [
                    "#dataset",
                    "#hallucinations",
                    "#long_context",
                    "#inference",
                    "#low_resource",
                    "#plp",
                    "#open_source",
                    "#architecture",
                    "#synthetic"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ˜Ğ˜ Ğ½ÑƒĞ¶Ğ½Ñ‹ IDE: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ²",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞµÑ€Ğ²ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ĞºĞ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. ĞĞ½Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ LLM Ğ² ÑÑ€ĞµĞ´Ñƒ Hazel Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MVUBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ²Ğ¾ĞµĞ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Empowering AI Code Completion with Contextual Awareness!",
                    "desc": "This paper addresses the limitations of large language models (LLMs) in code completion, particularly their tendency to generate incorrect code due to insufficient context. It proposes a solution by integrating LLMs with the type and binding structure provided by a language server, which enhances contextual awareness during code generation. The authors demonstrate this approach within the Hazel live program sketching environment, allowing for more accurate code completions by utilizing broader contextual information. They also introduce MVUBench, a dataset for evaluating these techniques, and suggest a new extension to the Language Server Protocol to improve AI-assisted coding."
                },
                "zh": {
                    "title": "è®©AIä¹Ÿéœ€è¦é›†æˆå¼€å‘ç¯å¢ƒï¼",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¨‹åºåˆæˆé¢†åŸŸå¸¦æ¥äº†é‡å¤§å˜é©ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºLLMçš„ä»£ç è¡¥å…¨ç³»ç»Ÿå¸¸å¸¸å› ä¸ºç¼ºä¹é€‚å½“çš„ä¸Šä¸‹æ–‡è€Œç”Ÿæˆé”™è¯¯çš„ä»£ç ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ä¸åœ¨è®­ç»ƒæ•°æ®ä¸­çš„å®šä¹‰æ—¶ã€‚æœ¬æ–‡å±•ç¤ºäº†ä¸è¯­è¨€çš„ç±»å‹å’Œç»‘å®šç»“æ„ç´§å¯†é›†æˆï¼Œå¯ä»¥æœ‰æ•ˆè§£å†³è¿™ä¸€ä¸Šä¸‹æ–‡é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºå°†LLMä»£ç ç”Ÿæˆé›†æˆåˆ°Hazelå®æ—¶ç¨‹åºè‰å›¾ç¯å¢ƒä¸­ï¼Œä»¥ç¡®ä¿åœ¨å¡«è¡¥ä»£ç ç©ºç¼ºæ—¶å§‹ç»ˆæä¾›æœ‰æ„ä¹‰çš„ç¨‹åºè‰å›¾ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-09-05.html",
    "link_next": "2024-09-09.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "05.09",
        "en": "09/05",
        "zh": "9æœˆ5æ—¥"
    },
    "short_date_next": {
        "ru": "09.09",
        "en": "09/09",
        "zh": "9æœˆ9æ—¥"
    },
    "categories": {
        "#dataset": 6,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 1,
        "#cv": 7,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 2,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 0,
        "#multimodal": 3,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 7,
        "#healthcare": 0,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 2,
        "#transfer_learning": 1,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 1,
        "#optimization": 6,
        "#survey": 2,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 3,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 7,
        "#small_models": 0,
        "#science": 2,
        "#low_resource": 1
    }
}