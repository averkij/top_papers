{
    "date": {
        "ru": "11 февраля",
        "en": "February 11",
        "zh": "2月11日"
    },
    "time_utc": "2025-02-11 03:15",
    "weekday": 1,
    "issue_id": 2140,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.03628",
            "title": "The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering",
            "url": "https://huggingface.co/papers/2502.03628",
            "abstract": "Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits rankings throughout the generation process, revealing three key patterns in how LVLMs process information: (1) gradual visual information loss -- visually grounded tokens gradually become less favored throughout generation, and (2) early excitation -- semantically meaningful tokens achieve peak activation in the layers earlier than the final layer. (3) hidden genuine information -- visually grounded tokens though not being eventually decided still retain relatively high rankings at inference. Based on these insights, we propose VISTA (Visual Information Steering with Token-logit Augmentation), a training-free inference-time intervention framework that reduces hallucination while promoting genuine information. VISTA works by combining two complementary approaches: reinforcing visual information in activation space and leveraging early layer activations to promote semantically meaningful decoding. Compared to existing methods, VISTA requires no external supervision and is applicable to various decoding strategies. Extensive experiments show that VISTA on average reduces hallucination by abount 40% on evaluated open-ended generation task, and it consistently outperforms existing methods on four benchmarks across four architectures under three decoding strategies.",
            "score": 4,
            "issue_id": 2140,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 февраля",
                "en": "February 5",
                "zh": "2月5日"
            },
            "hash": "04b182d80abf9219",
            "authors": [
                "Zhuowei Li",
                "Haizhou Shi",
                "Yunhe Gao",
                "Di Liu",
                "Zhenting Wang",
                "Yuxiao Chen",
                "Ting Liu",
                "Long Zhao",
                "Hao Wang",
                "Dimitris N. Metaxas"
            ],
            "affiliations": [
                "Google DeepMind",
                "Rutgers University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03628.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#hallucinations",
                    "#benchmark",
                    "#inference",
                    "#interpretability",
                    "#multimodal"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Борьба с галлюцинациями в визуально-языковых моделях: метод VISTA",
                    "desc": "Эта статья исследует проблему галлюцинаций в крупных визуально-языковых моделях (LVLM) при обработке текстовых и визуальных входных данных. Авторы анализируют внутренние механизмы возникновения галлюцинаций, изучая ранжирование логитов токенов в процессе генерации. На основе выявленных закономерностей предлагается метод VISTA для уменьшения галлюцинаций и усиления достоверной информации во время вывода. Эксперименты показывают, что VISTA в среднем снижает уровень галлюцинаций на 40% в задачах открытой генерации и превосходит существующие методы на четырех бенчмарках."
                },
                "en": {
                    "title": "VISTA: Reducing Hallucination in Vision-Language Models",
                    "desc": "This paper explores the issue of hallucination in Large Vision-Language Models (LVLMs), where the models generate plausible text that does not correspond to visual inputs. The authors identify three patterns in the generation process: a gradual loss of visual information, early activation of semantically meaningful tokens, and the presence of high-ranking visually grounded tokens that are not ultimately chosen. To address these issues, they introduce VISTA, a framework that enhances visual information during inference without requiring additional training. VISTA effectively reduces hallucination by about 40% and outperforms existing methods across multiple benchmarks and architectures."
                },
                "zh": {
                    "title": "减少幻觉，提升真实信息的VISTA框架",
                    "desc": "大型视觉语言模型（LVLMs）能够有效地处理文本和视觉输入，但它们往往会产生语法上连贯但视觉上不真实的内容。本文研究了幻觉的内部动态，发现LVLMs在生成过程中处理信息的三种关键模式：逐渐丧失视觉信息、早期激活和隐藏的真实信息。基于这些发现，我们提出了VISTA（视觉信息引导与标记逻辑增强），这是一种无需训练的推理时干预框架，旨在减少幻觉并促进真实信息的生成。实验表明，VISTA在开放式生成任务中平均减少了约40%的幻觉，并在四个基准测试中在三种解码策略下始终优于现有方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06155",
            "title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile",
            "url": "https://huggingface.co/papers/2502.06155",
            "abstract": "Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify a prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 2) Shorten the sampling process by adopting existing multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities. We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities. Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video generation with a marginal performance trade-off in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91x speedup when running on 4 GPUs with sequence parallelism.",
            "score": 1,
            "issue_id": 2140,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 февраля",
                "en": "February 10",
                "zh": "2月10日"
            },
            "hash": "2f8d5e54db328d39",
            "authors": [
                "Hangliang Ding",
                "Dacheng Li",
                "Runlong Su",
                "Peiyuan Zhang",
                "Zhijie Deng",
                "Ion Stoica",
                "Hao Zhang"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University of California, Berkeley",
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06155.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#diffusion",
                    "#optimization",
                    "#inference",
                    "#video"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Ускорение генерации видео: эффективность без потери качества",
                    "desc": "Статья представляет новый подход к ускорению генерации видео с помощью Диффузионных Трансформеров (DiTs). Авторы предлагают прореживание 3D-внимания на основе избыточности видеоданных и сокращение процесса сэмплирования с помощью многошаговой дистилляции согласованности. Разработан трехэтапный процесс обучения для объединения внимания с низкой сложностью и возможностей генерации за несколько шагов. Результаты показывают ускорение в 7.4-7.8 раз для генерации видео 720p с 29 и 93 кадрами при использовании всего 0.1% данных предобучения."
                },
                "en": {
                    "title": "Speeding Up Video Generation with Efficient Attention Mechanisms",
                    "desc": "This paper presents a solution to the inefficiency of Diffusion Transformers (DiTs) in generating high-fidelity videos. It introduces a method to prune 3D full attention by recognizing repetitive patterns in video data, leading to a sparse attention mechanism that reduces computational complexity. Additionally, the authors propose a multi-step consistency distillation approach to shorten the sampling process, allowing for faster video generation. The resulting model, Open-Sora-Plan-1.2, achieves significant speed improvements while maintaining performance, especially when utilizing distributed inference across multiple GPUs."
                },
                "zh": {
                    "title": "高效视频生成的新方法",
                    "desc": "本论文提出了一种改进的Diffusion Transformers（DiTs）模型，以解决生成高保真视频时的效率问题。我们通过识别视频数据中的冗余，提出了一种稀疏的3D注意力机制，使其在视频帧数量上具有线性复杂度。其次，我们采用多步一致性蒸馏技术，缩短了采样过程，从而实现了更快速的视频生成。最终，我们的模型在使用极少的预训练数据时，生成速度提高了7.4到7.8倍，同时保持了良好的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06049",
            "title": "LM2: Large Memory Models",
            "url": "https://huggingface.co/papers/2502.06049",
            "abstract": "This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates a memory module that acts as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformers general-purpose capabilities, LM2 maintains the original information flow while integrating a complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures.",
            "score": 0,
            "issue_id": 2140,
            "pub_date": "2025-02-09",
            "pub_date_card": {
                "ru": "9 февраля",
                "en": "February 9",
                "zh": "2月9日"
            },
            "hash": "5f62d7e814a6918f",
            "authors": [
                "Jikun Kang",
                "Wenqi Wu",
                "Filippos Christianos",
                "Alex J. Chan",
                "Fraser Greenlee",
                "George Thomas",
                "Marvin Purtorab",
                "Andy Toulis"
            ],
            "affiliations": [
                "Convergence Labs Ltd."
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06049.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#benchmark",
                    "#interpretability",
                    "#long_context",
                    "#reasoning"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "LM2: Трансформер с памятью для улучшенных рассуждений",
                    "desc": "В статье представлена модель Large Memory Model (LM2), архитектура декодер-трансформер с дополнительным модулем памяти. LM2 решает проблемы стандартных трансформеров в многошаговых рассуждениях и обработке длинных контекстов. Модель показала значительное улучшение производительности на бенчмарке BABILong по сравнению с базовыми моделями. LM2 также продемонстрировала улучшенные возможности в многоходовых выводах, числовых вычислениях и ответах на вопросы с большим контекстом."
                },
                "en": {
                    "title": "Enhancing Transformers with Memory for Superior Reasoning",
                    "desc": "The paper presents the Large Memory Model (LM2), a new type of Transformer designed to improve multi-step reasoning and information synthesis over long contexts. LM2 features an auxiliary memory module that stores contextual information and interacts with input data through cross attention, allowing it to update its memory dynamically. This model retains the original capabilities of Transformers while adding a memory pathway that enhances performance on complex tasks. Experimental results show that LM2 significantly outperforms existing models in reasoning tasks and maintains strong performance on general tasks, highlighting the value of integrating explicit memory into Transformer architectures."
                },
                "zh": {
                    "title": "大型记忆模型：提升Transformer推理能力的关键",
                    "desc": "本文介绍了一种名为大型记忆模型（LM2）的解码器仅Transformer架构，旨在解决标准Transformer在多步推理、关系论证和长上下文信息综合方面的局限性。LM2引入了一个辅助记忆模块，作为上下文表示的存储库，通过交叉注意力与输入标记交互，并通过门控机制进行更新。实验结果表明，LM2在BABILong基准测试中，平均性能比记忆增强的RMT模型提高了37.1%，比基线Llama-3.2模型提高了86.3%。LM2在多跳推理、数值推理和大上下文问答方面表现出色，证明了显式记忆在增强Transformer架构中的重要性。"
                }
            }
        }
    ],
    "link_prev": "2025-02-10.html",
    "link_next": "2025-02-12.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "10.02",
        "en": "02/10",
        "zh": "2月10日"
    },
    "short_date_next": {
        "ru": "12.02",
        "en": "02/12",
        "zh": "2月12日"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一种新的注意力机制，称为滑动磁贴注意力（STA），以解决扩散变压器（DiTs）在视频生成中的计算成本问题。传统的全注意力机制在生成短视频时耗时极长。STA通过在局部3D窗口内滑动和关注，消除了全注意力中的冗余。与传统的滑动窗口注意力不同，STA采用硬件感知的设计，提高了效率。实验显示，STA在不降低质量的情况下，显著减少了视频生成的延迟。",
        "title": "Fast Video Generation with Sliding Tile Attention",
        "pinyin": "这篇文章介绍了一种新的注意力机制，称为滑动磁贴注意力（STA），以解决扩散变压器（DiTs）在视频生成中的计算成本问题。传统的全注意力机制在生成短视频时耗时极长。STA通过在局部3D窗口内滑动和关注，消除了全注意力中的冗余。与传统的滑动窗口注意力不同，STA采用硬件感知的设计，提高了效率。实验显示，STA在不降低质量的情况下，显著减少了视频生成的延迟。\n\nzhè piān wén zhāng jiè shào le yī zhǒng xīn de zhù yì lì jī zhì, chēng wéi huá dòng cí tiē zhù yì lì (STA), yǐ jiě jué kuò sàn biàn shū qì (DiTs) zài shì pín shēng chéng zhōng de jì suàn chéng běn wèn tí. chuán tǒng de quán zhù yì lì jī zhì zài shēng chéng duǎn shì pín shí hào shí jí cháng. STA tōng guò zài jú bù 3D chuāng kǒu nèi huá dòng hé guān zhù, xiāo chú le quán zhù yì lì zhōng de róng yù. yǔ chuán tǒng de huá dòng chuāng kǒu zhù yì lì bù tóng, STA cǎi yòng yìng jiàn gǎn zhī de shè jì, tí gāo le xiào lǜ. shí yàn xiǎn shì, STA zài bù jiàng dī zhì liàng de qíng kuàng xià, xiǎn zhù jiǎn shǎo le shì pín shēng chéng de yán chí.",
        "vocab": "[{'word': '注意力', 'pinyin': 'zhùyìlì', 'trans': 'attention'},\n{'word': '机制', 'pinyin': 'jīzhì', 'trans': 'mechanism'},\n{'word': '滑动', 'pinyin': 'huádòng', 'trans': 'sliding'},\n{'word': '磁贴', 'pinyin': 'cítiē', 'trans': 'magnetic tile'},\n{'word': '扩散', 'pinyin': 'kuòsàn', 'trans': 'diffusion'},\n{'word': '变压器', 'pinyin': 'biànyāqì', 'trans': 'transformer'},\n{'word': '计算', 'pinyin': 'jìsuàn', 'trans': 'computation'},\n{'word': '成本', 'pinyin': 'chéngběn', 'trans': 'cost'},\n{'word': '传统', 'pinyin': 'chuántǒng', 'trans': 'traditional'},\n{'word': '全', 'pinyin': 'quán', 'trans': 'full'},\n{'word': '耗时', 'pinyin': 'hàoshí', 'trans': 'time-consuming'},\n{'word': '极长', 'pinyin': 'jícháng', 'trans': 'extremely long'},\n{'word': '局部', 'pinyin': 'júbù', 'trans': 'local'},\n{'word': '3D', 'pinyin': '', 'trans': '3D'},\n{'word': '窗口', 'pinyin': 'chuāngkǒu', 'trans': 'window'},\n{'word': '冗余', 'pinyin': 'rǒngyú', 'trans': 'redundancy'},\n{'word': '硬件', 'pinyin': 'yìngjiàn', 'trans': 'hardware'},\n{'word': '感知', 'pinyin': 'gǎnzhī', 'trans': 'perception'},\n{'word': '设计', 'pinyin': 'shèjì', 'trans': 'design'},\n{'word': '效率', 'pinyin': 'xiàolǜ', 'trans': 'efficiency'},\n{'word': '实验', 'pinyin': 'shíyàn', 'trans': 'experiment'},\n{'word': '显示', 'pinyin': 'xiǎnshì', 'trans': 'display'},\n{'word': '显著', 'pinyin': 'xiǎnzhù', 'trans': 'significant'},\n{'word': '减少', 'pinyin': 'jiǎnshǎo', 'trans': 'reduce'},\n{'word': '延迟', 'pinyin': 'yánchí', 'trans': 'latency'}]",
        "trans": "This article introduces a new attention mechanism called Sliding Tile Attention (STA) to address the computational cost issues of Diffusion Transformers (DiTs) in video generation. Traditional full attention mechanisms take an extremely long time to generate short videos. STA eliminates redundancy in full attention by sliding and focusing within local 3D windows. Unlike traditional sliding window attention, STA employs a hardware-aware design to enhance efficiency. Experiments show that STA significantly reduces the latency of video generation without compromising quality.",
        "update_ts": "2025-02-10 09:11"
    }
}