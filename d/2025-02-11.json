{
    "date": {
        "ru": "11 —Ñ–µ–≤—Ä–∞–ª—è",
        "en": "February 11",
        "zh": "2Êúà11Êó•"
    },
    "time_utc": "2025-02-11 05:10",
    "weekday": 1,
    "issue_id": 2142,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.06781",
            "title": "Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning",
            "url": "https://huggingface.co/papers/2502.06781",
            "abstract": "Reasoning abilities, especially those for solving complex math problems, are crucial components of general intelligence. Recent advances by proprietary companies, such as o-series models of OpenAI, have made remarkable progress on reasoning tasks. However, the complete technical details remain unrevealed, and the techniques that are believed certainly to be adopted are only reinforcement learning (RL) and the long chain of thoughts. This paper proposes a new RL framework, termed OREAL, to pursue the performance limit that can be achieved through Outcome REwArd-based reinforcement Learning for mathematical reasoning tasks, where only binary outcome rewards are easily accessible. We theoretically prove that behavior cloning on positive trajectories from best-of-N (BoN) sampling is sufficient to learn the KL-regularized optimal policy in binary feedback environments. This formulation further implies that the rewards of negative samples should be reshaped to ensure the gradient consistency between positive and negative samples. To alleviate the long-existing difficulties brought by sparse rewards in RL, which are even exacerbated by the partial correctness of the long chain of thought for reasoning tasks, we further apply a token-level reward model to sample important tokens in reasoning trajectories for learning. With OREAL, for the first time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL, being on par with 32B models. OREAL-32B also surpasses previous 32B models trained by distillation with 95.0 pass@1 accuracy on MATH-500. Our investigation also indicates the importance of initial policy models and training queries for RL. Code, models, and data will be released to benefit future researchhttps://github.com/InternLM/OREAL.",
            "score": 13,
            "issue_id": 2142,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 10",
                "zh": "2Êúà10Êó•"
            },
            "hash": "9cd2694b7c865b94",
            "authors": [
                "Chengqi Lyu",
                "Songyang Gao",
                "Yuzhe Gu",
                "Wenwei Zhang",
                "Jianfei Gao",
                "Kuikun Liu",
                "Ziyi Wang",
                "Shuaibin Li",
                "Qian Zhao",
                "Haian Huang",
                "Weihan Cao",
                "Jiangning Liu",
                "Hongwei Liu",
                "Junnan Liu",
                "Songyang Zhang",
                "Dahua Lin",
                "Kai Chen"
            ],
            "affiliations": [
                "HKGAI under InnoHK",
                "MMLab, The Chinese University of Hong Kong",
                "Shanghai AI Laboratory",
                "Shanghai Jiao Tong University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06781.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#open_source",
                    "#rl",
                    "#reasoning",
                    "#optimization",
                    "#math"
                ],
                "emoji": "üßÆ",
                "ru": {
                    "title": "OREAL: –ü—Ä–æ—Ä—ã–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º OREAL –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –¥–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è –Ω–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö –∏–∑ –≤—ã–±–æ—Ä–∫–∏ best-of-N –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –≤ —Å—Ä–µ–¥–∞—Ö —Å –±–∏–Ω–∞—Ä–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é. –î–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –Ω–∞–≥—Ä–∞–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤. –° –ø–æ–º–æ—â—å—é OREAL –º–æ–¥–µ–ª—å —Ä–∞–∑–º–µ—Ä–æ–º 7B –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ 94.0% pass@1 –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ MATH-500, —á—Ç–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ 32B –º–æ–¥–µ–ª–µ–π."
                },
                "en": {
                    "title": "OREAL: Advancing AI Reasoning with Outcome-Based Reinforcement Learning",
                    "desc": "This paper introduces a new reinforcement learning framework called OREAL, designed to enhance mathematical reasoning capabilities in AI models. OREAL focuses on using binary outcome rewards to improve learning efficiency, particularly in environments where feedback is sparse. The authors demonstrate that behavior cloning from positive examples can effectively learn optimal policies, while also reshaping negative rewards to maintain gradient consistency. The results show that OREAL achieves high accuracy on mathematical tasks, outperforming larger models and highlighting the significance of initial policy models in the training process."
                },
                "zh": {
                    "title": "OREALÔºöÊï∞Â≠¶Êé®ÁêÜÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "ËøôÁØáËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁß∞‰∏∫OREALÔºåÊó®Âú®ÊèêÈ´òÊï∞Â≠¶Êé®ÁêÜ‰ªªÂä°ÁöÑÊÄßËÉΩ„ÄÇOREAL‰ΩøÁî®Âü∫‰∫éÁªìÊûúÁöÑÂ•ñÂä±Êú∫Âà∂Ôºå‰∏ìÊ≥®‰∫é‰∫åÂÖÉÁªìÊûúÂ•ñÂä±Ôºå‰ª•Ëß£ÂÜ≥Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÁ®ÄÁñèÂ•ñÂä±ÈóÆÈ¢ò„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈÄöËøáÂØπÊúÄ‰Ω≥Ê†∑Êú¨ËøõË°åË°å‰∏∫ÂÖãÈöÜÔºåÂèØ‰ª•ÊúâÊïàÂ≠¶‰π†ÊúÄ‰ºòÁ≠ñÁï•ÔºåÂπ∂‰∏îÈúÄË¶ÅÂØπË¥üÊ†∑Êú¨ÁöÑÂ•ñÂä±ËøõË°åÈáçÂ°ë‰ª•‰øùÊåÅÊ¢ØÂ∫¶‰∏ÄËá¥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå‰ΩøÁî®OREALÁöÑ7BÊ®°ÂûãÂú®MATH-500‰∏äËææÂà∞‰∫Ü94.0ÁöÑÂáÜÁ°ÆÁéáÔºåË°®Áé∞‰∏é32BÊ®°ÂûãÁõ∏ÂΩìÔºå‰∏îOREAL-32BÂú®Âêå‰∏Ä‰ªªÂä°‰∏äË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑ32BÊ®°Âûã„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05609",
            "title": "Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding",
            "url": "https://huggingface.co/papers/2502.05609",
            "abstract": "Accelerating inference in Large Language Models (LLMs) is critical for real-time interactions, as they have been widely incorporated into real-world services. Speculative decoding, a fully algorithmic solution, has gained attention for improving inference speed by drafting and verifying tokens, thereby generating multiple tokens in a single forward pass. However, current drafting strategies usually require significant fine-tuning or have inconsistent performance across tasks. To address these challenges, we propose Hierarchy Drafting (HD), a novel lossless drafting approach that organizes various token sources into multiple databases in a hierarchical framework based on temporal locality. In the drafting step, HD sequentially accesses multiple databases to obtain draft tokens from the highest to the lowest locality, ensuring consistent acceleration across diverse tasks and minimizing drafting latency. Our experiments on Spec-Bench using LLMs with 7B and 13B parameters demonstrate that HD outperforms existing database drafting methods, achieving robust inference speedups across model sizes, tasks, and temperatures.",
            "score": 8,
            "issue_id": 2141,
            "pub_date": "2025-02-08",
            "pub_date_card": {
                "ru": "8 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 8",
                "zh": "2Êúà8Êó•"
            },
            "hash": "6f559083c224138c",
            "authors": [
                "Sukmin Cho",
                "Sangjin Choi",
                "Taeho Hwang",
                "Jeongyeon Seo",
                "Soyeong Jeong",
                "Huije Lee",
                "Hoyun Song",
                "Jong C. Park",
                "Youngjin Kwon"
            ],
            "affiliations": [
                "School of Computing, Graduate School of AI, Korea Advanced Institute of Science and Technology"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05609.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#architecture",
                    "#inference",
                    "#optimization"
                ],
                "emoji": "üöÄ",
                "ru": {
                    "title": "–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —á–µ—Ä–Ω–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é –≤—ã–≤–æ–¥–∞ –≤ LLM",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã–≤–æ–¥–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Hierarchy Drafting (HD). HD –æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞—â–∞–µ—Ç—Å—è –∫ –±–∞–∑–∞–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —á–µ—Ä–Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ HD –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —á–µ—Ä–Ω–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –Ω–∞–¥–µ–∂–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤—ã–≤–æ–¥–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, –∑–∞–¥–∞—á –∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä."
                },
                "en": {
                    "title": "Boosting Inference Speed with Hierarchy Drafting in LLMs",
                    "desc": "This paper focuses on improving the speed of inference in Large Language Models (LLMs) for real-time applications. It introduces a new method called Hierarchy Drafting (HD), which organizes token sources into a hierarchical structure to enhance the drafting process. By accessing these token databases based on their temporal locality, HD ensures faster and more consistent token generation across various tasks. Experimental results show that HD significantly outperforms existing methods, providing robust speed improvements for LLMs of different sizes and tasks."
                },
                "zh": {
                    "title": "Â±ÇÊ¨°ËçâÊãüÔºöÂä†ÈÄüÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Âä†ÈÄüÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊé®ÁêÜÂØπ‰∫éÂÆûÊó∂‰∫§‰∫íËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊó†ÊçüËçâÊãüÊñπÊ≥ïÔºåÁß∞‰∏∫Â±ÇÊ¨°ËçâÊãüÔºàHDÔºâÔºåÂÆÉÈÄöËøáÂü∫‰∫éÊó∂Èó¥Â±ÄÈÉ®ÊÄßÁöÑÂ±ÇÊ¨°Ê°ÜÊû∂ÁªÑÁªáÂ§öÁßç‰ª§ÁâåÊ∫ê„ÄÇHDÂú®ËçâÊãüÊ≠•È™§‰∏≠‰æùÊ¨°ËÆøÈóÆÂ§ö‰∏™Êï∞ÊçÆÂ∫ìÔºå‰ªéÊúÄÈ´òÂà∞ÊúÄ‰ΩéÁöÑÂ±ÄÈÉ®ÊÄßËé∑ÂèñËçâÊãü‰ª§ÁâåÔºå‰ªéËÄåÁ°Æ‰øùÂú®‰∏çÂêå‰ªªÂä°‰∏≠‰∏ÄËá¥ÁöÑÂä†ÈÄüÊïàÊûú„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHDÂú®Êé®ÁêÜÈÄüÂ∫¶‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊï∞ÊçÆÂ∫ìËçâÊãüÊñπÊ≥ïÔºåÈÄÇÁî®‰∫é‰∏çÂêåËßÑÊ®°ÁöÑÊ®°ÂûãÂíå‰ªªÂä°„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.03628",
            "title": "The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering",
            "url": "https://huggingface.co/papers/2502.03628",
            "abstract": "Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits rankings throughout the generation process, revealing three key patterns in how LVLMs process information: (1) gradual visual information loss -- visually grounded tokens gradually become less favored throughout generation, and (2) early excitation -- semantically meaningful tokens achieve peak activation in the layers earlier than the final layer. (3) hidden genuine information -- visually grounded tokens though not being eventually decided still retain relatively high rankings at inference. Based on these insights, we propose VISTA (Visual Information Steering with Token-logit Augmentation), a training-free inference-time intervention framework that reduces hallucination while promoting genuine information. VISTA works by combining two complementary approaches: reinforcing visual information in activation space and leveraging early layer activations to promote semantically meaningful decoding. Compared to existing methods, VISTA requires no external supervision and is applicable to various decoding strategies. Extensive experiments show that VISTA on average reduces hallucination by abount 40% on evaluated open-ended generation task, and it consistently outperforms existing methods on four benchmarks across four architectures under three decoding strategies.",
            "score": 7,
            "issue_id": 2140,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 5",
                "zh": "2Êúà5Êó•"
            },
            "hash": "04b182d80abf9219",
            "authors": [
                "Zhuowei Li",
                "Haizhou Shi",
                "Yunhe Gao",
                "Di Liu",
                "Zhenting Wang",
                "Yuxiao Chen",
                "Ting Liu",
                "Long Zhao",
                "Hao Wang",
                "Dimitris N. Metaxas"
            ],
            "affiliations": [
                "Google DeepMind",
                "Rutgers University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03628.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#hallucinations",
                    "#benchmark",
                    "#inference",
                    "#interpretability",
                    "#multimodal"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–ë–æ—Ä—å–±–∞ —Å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º–∏ –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö: –º–µ—Ç–æ–¥ VISTA",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –∫—Ä—É–ø–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LVLM) –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π, –∏–∑—É—á–∞—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–≥–∏—Ç–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ VISTA –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –∏ —É—Å–∏–ª–µ–Ω–∏—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ VISTA –≤ —Å—Ä–µ–¥–Ω–µ–º —Å–Ω–∏–∂–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –Ω–∞ 40% –≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–∫—Ä—ã—Ç–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –Ω–∞ —á–µ—Ç—ã—Ä–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö."
                },
                "en": {
                    "title": "VISTA: Reducing Hallucination in Vision-Language Models",
                    "desc": "This paper explores the issue of hallucination in Large Vision-Language Models (LVLMs), where the models generate plausible text that does not correspond to visual inputs. The authors identify three patterns in the generation process: a gradual loss of visual information, early activation of semantically meaningful tokens, and the presence of high-ranking visually grounded tokens that are not ultimately chosen. To address these issues, they introduce VISTA, a framework that enhances visual information during inference without requiring additional training. VISTA effectively reduces hallucination by about 40% and outperforms existing methods across multiple benchmarks and architectures."
                },
                "zh": {
                    "title": "ÂáèÂ∞ëÂπªËßâÔºåÊèêÂçáÁúüÂÆû‰ø°ÊÅØÁöÑVISTAÊ°ÜÊû∂",
                    "desc": "Â§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàLVLMsÔºâËÉΩÂ§üÊúâÊïàÂú∞Â§ÑÁêÜÊñáÊú¨ÂíåËßÜËßâËæìÂÖ•Ôºå‰ΩÜÂÆÉ‰ª¨ÂæÄÂæÄ‰ºö‰∫ßÁîüËØ≠Ê≥ï‰∏äËøûË¥Ø‰ΩÜËßÜËßâ‰∏ä‰∏çÁúüÂÆûÁöÑÂÜÖÂÆπ„ÄÇÊú¨ÊñáÁ†îÁ©∂‰∫ÜÂπªËßâÁöÑÂÜÖÈÉ®Âä®ÊÄÅÔºåÂèëÁé∞LVLMsÂú®ÁîüÊàêËøáÁ®ã‰∏≠Â§ÑÁêÜ‰ø°ÊÅØÁöÑ‰∏âÁßçÂÖ≥ÈîÆÊ®°ÂºèÔºöÈÄêÊ∏ê‰∏ßÂ§±ËßÜËßâ‰ø°ÊÅØ„ÄÅÊó©ÊúüÊøÄÊ¥ªÂíåÈöêËóèÁöÑÁúüÂÆû‰ø°ÊÅØ„ÄÇÂü∫‰∫éËøô‰∫õÂèëÁé∞ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜVISTAÔºàËßÜËßâ‰ø°ÊÅØÂºïÂØº‰∏éÊ†áËÆ∞ÈÄªËæëÂ¢ûÂº∫ÔºâÔºåËøôÊòØ‰∏ÄÁßçÊó†ÈúÄËÆ≠ÁªÉÁöÑÊé®ÁêÜÊó∂Âπ≤È¢ÑÊ°ÜÊû∂ÔºåÊó®Âú®ÂáèÂ∞ëÂπªËßâÂπ∂‰øÉËøõÁúüÂÆû‰ø°ÊÅØÁöÑÁîüÊàê„ÄÇÂÆûÈ™åË°®ÊòéÔºåVISTAÂú®ÂºÄÊîæÂºèÁîüÊàê‰ªªÂä°‰∏≠Âπ≥ÂùáÂáèÂ∞ë‰∫ÜÁ∫¶40%ÁöÑÂπªËßâÔºåÂπ∂Âú®Âõõ‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Âú®‰∏âÁßçËß£Á†ÅÁ≠ñÁï•‰∏ãÂßãÁªà‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06772",
            "title": "ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates",
            "url": "https://huggingface.co/papers/2502.06772",
            "abstract": "We present that hierarchical LLM reasoning via scaling thought templates can effectively optimize the reasoning search space and outperform the mathematical reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3. We train our ReasonFlux-32B model with only 8 GPUs and introduces three innovations: (i) a structured and generic thought template library, containing around 500 high-level thought templates capable of generalizing to similar or relevant reasoning problems; (ii) performing hierarchical reinforcement learning on a sequence of thought templates instead of long CoTs, optimizing a base LLM to plan out an optimal template trajectory for gradually handling complex problems; (iii) a brand new inference scaling system that enables hierarchical LLM reasoning by adaptively scaling thought templates at inference time. With a template trajectory containing sequential thought templates, our ReasonFlux-32B significantly advances math reasoning capabilities to state-of-the-art levels. Notably, on the MATH benchmark, it achieves an accuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad (AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems, surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code: https://github.com/Gen-Verse/ReasonFlux",
            "score": 4,
            "issue_id": 2141,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 10",
                "zh": "2Êúà10Êó•"
            },
            "hash": "1ac59597c9610fb2",
            "authors": [
                "Ling Yang",
                "Zhaochen Yu",
                "Bin Cui",
                "Mengdi Wang"
            ],
            "affiliations": [
                "Peking University",
                "Princeton University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06772.jpg",
            "data": {
                "categories": [
                    "#rl",
                    "#optimization",
                    "#training",
                    "#reasoning",
                    "#math",
                    "#benchmark",
                    "#inference"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –ò–ò: –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –Ω–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ",
                    "desc": "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å ReasonFlux-32B, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º —à–∞–±–ª–æ–Ω–æ–≤ –º—ã—à–ª–µ–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞ —Ä–µ—à–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ—â–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ OpenAI o1-preview –∏ DeepSeek V3. ReasonFlux-32B –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É —à–∞–±–ª–æ–Ω–æ–≤ –º—ã—à–ª–µ–Ω–∏—è –∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —à–∞–±–ª–æ–Ω–æ–≤. –ù–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MATH –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ 91.2%, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è o1-preview –Ω–∞ 6.7%."
                },
                "en": {
                    "title": "Revolutionizing Math Reasoning with Hierarchical Thought Templates",
                    "desc": "This paper introduces ReasonFlux-32B, a model that enhances mathematical reasoning in large language models (LLMs) by using hierarchical reasoning with thought templates. It features a library of 500 structured thought templates that help generalize reasoning across similar problems. The model employs hierarchical reinforcement learning to optimize the sequence of thought templates, allowing it to tackle complex problems more effectively. With these innovations, ReasonFlux-32B achieves state-of-the-art performance on math benchmarks, significantly outperforming existing models like OpenAI o1-preview and DeepSeek V3."
                },
                "zh": {
                    "title": "Â±ÇÊ¨°ÂåñÊé®ÁêÜÔºåÊï∞Â≠¶ËÉΩÂäõÊñ∞Á™ÅÁ†¥",
                    "desc": "Êú¨ÊñáÊèêÂá∫ÈÄöËøáÊâ©Â±ïÊÄùÁª¥Ê®°ÊùøÁöÑÂ±ÇÊ¨°ÂåñÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊé®ÁêÜÔºåÂèØ‰ª•ÊúâÊïà‰ºòÂåñÊé®ÁêÜÊêúÁ¥¢Á©∫Èó¥ÔºåÂπ∂Ë∂ÖË∂äÂº∫Â§ßÁöÑLLMÂ¶ÇOpenAI o1-previewÂíåDeepSeek V3ÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõ„ÄÇÊàë‰ª¨ËÆ≠ÁªÉÁöÑReasonFlux-32BÊ®°Âûã‰ªÖ‰ΩøÁî®8‰∏™GPUÔºåÂπ∂ÂºïÂÖ•‰∫Ü‰∏âÈ°πÂàõÊñ∞Ôºö‰∏ÄÊòØÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´Á∫¶500‰∏™È´òÂ±ÇÊ¨°ÊÄùÁª¥Ê®°ÊùøÁöÑÁªìÊûÑÂåñÈÄöÁî®Ê®°ÊùøÂ∫ìÔºåËÉΩÂ§üÊé®ÂπøÂà∞Á±ª‰ººÁöÑÊé®ÁêÜÈóÆÈ¢òÔºõ‰∫åÊòØÂØπÊÄùÁª¥Ê®°ÊùøÂ∫èÂàóËøõË°åÂ±ÇÊ¨°ÂåñÂº∫ÂåñÂ≠¶‰π†ÔºåËÄå‰∏çÊòØÈïøÈìæÁöÑÊÄùÁª¥ÔºàCoTsÔºâÔºå‰ºòÂåñÂü∫Á°ÄLLM‰ª•ËßÑÂàíÂá∫Â§ÑÁêÜÂ§çÊùÇÈóÆÈ¢òÁöÑÊúÄ‰Ω≥Ê®°ÊùøËΩ®ËøπÔºõ‰∏âÊòØÂÖ®Êñ∞ÁöÑÊé®ÁêÜÊâ©Â±ïÁ≥ªÁªüÔºåÈÄöËøáÂú®Êé®ÁêÜÊó∂Ëá™ÈÄÇÂ∫îÊâ©Â±ïÊÄùÁª¥Ê®°ÊùøÔºåÂÆûÁé∞Â±ÇÊ¨°ÂåñLLMÊé®ÁêÜ„ÄÇÈÄöËøáÂåÖÂê´È°∫Â∫èÊÄùÁª¥Ê®°ÊùøÁöÑÊ®°ÊùøËΩ®ËøπÔºåReasonFlux-32BÂú®Êï∞Â≠¶Êé®ÁêÜËÉΩÂäõ‰∏äÊòæËëóÊèêÂçáÔºåËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊ∞¥Âπ≥„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06788",
            "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models",
            "url": "https://huggingface.co/papers/2502.06788",
            "abstract": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the performance gap with their encoder-based counterparts, highlighting the promising potential for unified multimodal systems with structural simplicity and efficient deployment. We systematically clarify the performance gap between VLMs using pre-trained vision encoders, discrete tokenizers, and minimalist visual layers from scratch, deeply excavating the under-examined characteristics of encoder-free VLMs. We develop efficient strategies for encoder-free VLMs that rival mainstream encoder-based ones. After an in-depth investigation, we launch EVEv2.0, a new and improved family of encoder-free VLMs. We show that: (i) Properly decomposing and hierarchically associating vision and language within a unified model reduces interference between modalities. (ii) A well-designed training strategy enables effective optimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0 represents a thorough study for developing a decoder-only architecture across modalities, demonstrating superior data efficiency and strong vision-reasoning capability. Code is publicly available at: https://github.com/baaivision/EVE.",
            "score": 4,
            "issue_id": 2141,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 10",
                "zh": "2Êúà10Êó•"
            },
            "hash": "4374edb93ca102c6",
            "authors": [
                "Haiwen Diao",
                "Xiaotong Li",
                "Yufeng Cui",
                "Yueze Wang",
                "Haoge Deng",
                "Ting Pan",
                "Wenxuan Wang",
                "Huchuan Lu",
                "Xinlong Wang"
            ],
            "affiliations": [
                "BAAI",
                "BUPT",
                "CASIA",
                "DLUT",
                "PKU",
                "UCAS"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06788.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#architecture",
                    "#agi",
                    "#multimodal"
                ],
                "emoji": "üî¨",
                "ru": {
                    "title": "–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏: EVEv2.0 - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–µ–∑ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è EVEv2.0, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ —Ç–µ–∫—Å—Ç–æ–º –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —ç–Ω–∫–æ–¥–µ—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å—Å–ª–µ–¥—É—é—Ç —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ —Å —ç–Ω–∫–æ–¥–µ—Ä–∞–º–∏ –∏ –±–µ–∑ –Ω–∏—Ö, —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö. –û–Ω–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∞—Å—Å–æ—Ü–∏–∞—Ü–∏—è –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ –≤ –µ–¥–∏–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å–Ω–∏–∂–∞–µ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ—Ä–µ–Ω—Ü–∏—é –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏. EVEv2.0 –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ —Å–∏–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é."
                },
                "en": {
                    "title": "EVEv2.0: Bridging the Gap in Vision-Language Models Without Encoders",
                    "desc": "This paper discusses the advancements in encoder-free vision-language models (VLMs) that are closing the performance gap with traditional encoder-based models. The authors explore the characteristics of these encoder-free VLMs and propose efficient strategies to enhance their performance. They introduce EVEv2.0, a new family of encoder-free VLMs that effectively integrates vision and language while minimizing interference. The study demonstrates that a well-structured training approach and hierarchical association of modalities lead to improved data efficiency and vision-reasoning capabilities."
                },
                "zh": {
                    "title": "Êó†ÁºñÁ†ÅÂô®VLMÁöÑÊΩúÂäõ‰∏éÂàõÊñ∞",
                    "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÊó†ÁºñÁ†ÅÂô®ÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®ÊÄßËÉΩ‰∏ä‰∏éÂü∫‰∫éÁºñÁ†ÅÂô®ÁöÑÊ®°Âûã‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇÊàë‰ª¨Á≥ªÁªüÊÄßÂú∞ÂàÜÊûê‰∫Ü‰ΩøÁî®È¢ÑËÆ≠ÁªÉËßÜËßâÁºñÁ†ÅÂô®ÂíåÁÆÄÁ∫¶ËßÜËßâÂ±ÇÁöÑÊó†ÁºñÁ†ÅÂô®VLMsÁöÑÁâπÊÄß„ÄÇÈÄöËøáÂºÄÂèëÈ´òÊïàÁöÑÁ≠ñÁï•ÔºåÊàë‰ª¨Êé®Âá∫‰∫ÜEVEv2.0Ôºå‰∏Ä‰∏™ÊîπËøõÁöÑÊó†ÁºñÁ†ÅÂô®VLMÁ≥ªÂàóÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Êï∞ÊçÆÊïàÁéáÂíåËßÜËßâÊé®ÁêÜËÉΩÂäõ‰∏äÁöÑ‰ºòÂäø„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂêàÁêÜÂàÜËß£ÂíåÂ±ÇÊ¨°ÂÖ≥ËÅîËßÜËßâ‰∏éËØ≠Ë®ÄÂèØ‰ª•ÂáèÂ∞ëÊ®°ÊÄÅ‰πãÈó¥ÁöÑÂπ≤Êâ∞ÔºåÂπ∂ÈÄöËøáËâØÂ•ΩÁöÑËÆ≠ÁªÉÁ≠ñÁï•ÂÆûÁé∞ÊúâÊïà‰ºòÂåñ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06635",
            "title": "Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM",
            "url": "https://huggingface.co/papers/2502.06635",
            "abstract": "Steel-LLM is a Chinese-centric language model developed from scratch with the goal of creating a high-quality, open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey. Steel-LLM has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. This paper provides a comprehensive summary of the project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs. The model checkpoints and training script are available at https://github.com/zhanshijinwat/Steel-LLM.",
            "score": 2,
            "issue_id": 2141,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 10",
                "zh": "2Êúà10Êó•"
            },
            "hash": "2deb5075264d7660",
            "authors": [
                "Qingshui Gu",
                "Shu Li",
                "Tianyu Zheng",
                "Zhaoxiang Zhang"
            ],
            "affiliations": [
                "Beijing University of Posts and Telecommunications",
                "Institute of Automation, Chinese Academy of Sciences",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06635.jpg",
            "data": {
                "categories": [
                    "#small_models",
                    "#multilingual",
                    "#training",
                    "#data",
                    "#benchmark",
                    "#open_source",
                    "#dataset",
                    "#low_resource"
                ],
                "emoji": "üá®üá≥",
                "ru": {
                    "title": "–°–æ–∑–¥–∞–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∫–∏—Ç–∞–π—Å–∫–æ—è–∑—ã—á–Ω–æ–π LLM —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º",
                    "desc": "Steel-LLM - —ç—Ç–æ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏–π —è–∑—ã–∫, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è —Å –Ω—É–ª—è –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö. –ú–æ–¥–µ–ª—å —Å 1 –º–∏–ª–ª–∏–∞—Ä–¥–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ. Steel-LLM –ø–æ–∫–∞–∑–∞–ª–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö CEVAL –∏ CMMLU, –ø—Ä–µ–≤–∑–æ–π–¥—è —Ä–∞–Ω–Ω–∏–µ –º–æ–¥–µ–ª–∏ –æ—Ç –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã—Ö –∏–Ω—Å—Ç–∏—Ç—É—Ç–æ–≤. –°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –æ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏, –≤–∫–ª—é—á–∞—è —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –¥–∏–∑–∞–π–Ω –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ –æ–±—É—á–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Empowering Chinese NLP with Steel-LLM: Open-Source Innovation",
                    "desc": "Steel-LLM is a language model specifically designed for the Chinese language, built from the ground up to be open-source and accessible. It features 1 billion parameters and was trained on a large dataset primarily consisting of Chinese text, with some English data to fill existing gaps. The model has shown strong performance on various benchmarks, surpassing earlier models from larger organizations. This paper details the project's contributions, including data collection, model architecture, training techniques, and the challenges faced, serving as a guide for others in the field of language model development."
                },
                "zh": {
                    "title": "ÊâìÈÄ†‰∏≠Êñá‰ºòË¥®ÂºÄÊ∫êËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé¢Á¥¢",
                    "desc": "Steel-LLMÊòØ‰∏Ä‰∏™‰ª•‰∏≠Êñá‰∏∫‰∏≠ÂøÉÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºåÊó®Âú®Âú®ÊúâÈôêÁöÑËÆ°ÁÆóËµÑÊ∫ê‰∏ãÂºÄÂèëÂá∫È´òË¥®ÈáèÁöÑÂºÄÊ∫êÊ®°Âûã„ÄÇËØ•È°πÁõÆ‰∫é2024Âπ¥3ÊúàÂêØÂä®ÔºåËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™Êã•Êúâ10‰∫øÂèÇÊï∞ÁöÑÂ§ßËßÑÊ®°Ê®°ÂûãÔºåÈáçÁÇπÂÖ≥Ê≥®ÈÄèÊòéÂ∫¶ÂíåÂÆûÁî®ËßÅËß£ÁöÑÂàÜ‰∫´„ÄÇËÆ≠ÁªÉËøáÁ®ã‰∏≠‰∏ªË¶Å‰ΩøÁî®‰∏≠ÊñáÊï∞ÊçÆÔºåÂπ∂ÈÄÇÈáèÂåÖÂê´Ëã±ÊñáÊï∞ÊçÆÔºåÂ°´Ë°•‰∫ÜÁé∞ÊúâÂºÄÊ∫êÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ©∫ÁôΩ„ÄÇSteel-LLMÂú®CEVALÂíåCMMLUÁ≠âÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåË∂ÖË∂ä‰∫ÜÂ§ßÂûãÊú∫ÊûÑÁöÑÊó©ÊúüÊ®°ÂûãÔºå‰∏∫Á†îÁ©∂‰∫∫ÂëòÂíåÂÆûË∑µËÄÖÊèê‰æõ‰∫ÜÂÆùË¥µÁöÑËµÑÊ∫ê„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06049",
            "title": "LM2: Large Memory Models",
            "url": "https://huggingface.co/papers/2502.06049",
            "abstract": "This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates a memory module that acts as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformers general-purpose capabilities, LM2 maintains the original information flow while integrating a complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures.",
            "score": 2,
            "issue_id": 2140,
            "pub_date": "2025-02-09",
            "pub_date_card": {
                "ru": "9 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 9",
                "zh": "2Êúà9Êó•"
            },
            "hash": "5f62d7e814a6918f",
            "authors": [
                "Jikun Kang",
                "Wenqi Wu",
                "Filippos Christianos",
                "Alex J. Chan",
                "Fraser Greenlee",
                "George Thomas",
                "Marvin Purtorab",
                "Andy Toulis"
            ],
            "affiliations": [
                "Convergence Labs Ltd."
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06049.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#benchmark",
                    "#interpretability",
                    "#long_context",
                    "#reasoning"
                ],
                "emoji": "üß†",
                "ru": {
                    "title": "LM2: –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —Å –ø–∞–º—è—Ç—å—é –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π",
                    "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Large Memory Model (LM2), –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–µ–∫–æ–¥–µ—Ä-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –º–æ–¥—É–ª–µ–º –ø–∞–º—è—Ç–∏. LM2 —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –≤ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤. –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ BABILong –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. LM2 —Ç–∞–∫–∂–µ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ —É–ª—É—á—à–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –≤—ã–≤–æ–¥–∞—Ö, —á–∏—Å–ª–æ–≤—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö –∏ –æ—Ç–≤–µ—Ç–∞—Ö –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å –±–æ–ª—å—à–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."
                },
                "en": {
                    "title": "Enhancing Transformers with Memory for Superior Reasoning",
                    "desc": "The paper presents the Large Memory Model (LM2), a new type of Transformer designed to improve multi-step reasoning and information synthesis over long contexts. LM2 features an auxiliary memory module that stores contextual information and interacts with input data through cross attention, allowing it to update its memory dynamically. This model retains the original capabilities of Transformers while adding a memory pathway that enhances performance on complex tasks. Experimental results show that LM2 significantly outperforms existing models in reasoning tasks and maintains strong performance on general tasks, highlighting the value of integrating explicit memory into Transformer architectures."
                },
                "zh": {
                    "title": "Â§ßÂûãËÆ∞ÂøÜÊ®°ÂûãÔºöÊèêÂçáTransformerÊé®ÁêÜËÉΩÂäõÁöÑÂÖ≥ÈîÆ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫Â§ßÂûãËÆ∞ÂøÜÊ®°ÂûãÔºàLM2ÔºâÁöÑËß£Á†ÅÂô®‰ªÖTransformerÊû∂ÊûÑÔºåÊó®Âú®Ëß£ÂÜ≥Ê†áÂáÜTransformerÂú®Â§öÊ≠•Êé®ÁêÜ„ÄÅÂÖ≥Á≥ªËÆ∫ËØÅÂíåÈïø‰∏ä‰∏ãÊñá‰ø°ÊÅØÁªºÂêàÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄß„ÄÇLM2ÂºïÂÖ•‰∫Ü‰∏Ä‰∏™ËæÖÂä©ËÆ∞ÂøÜÊ®°ÂùóÔºå‰Ωú‰∏∫‰∏ä‰∏ãÊñáË°®Á§∫ÁöÑÂ≠òÂÇ®Â∫ìÔºåÈÄöËøá‰∫§ÂèâÊ≥®ÊÑèÂäõ‰∏éËæìÂÖ•Ê†áËÆ∞‰∫§‰∫íÔºåÂπ∂ÈÄöËøáÈó®ÊéßÊú∫Âà∂ËøõË°åÊõ¥Êñ∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLM2Âú®BABILongÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÂπ≥ÂùáÊÄßËÉΩÊØîËÆ∞ÂøÜÂ¢ûÂº∫ÁöÑRMTÊ®°ÂûãÊèêÈ´ò‰∫Ü37.1%ÔºåÊØîÂü∫Á∫øLlama-3.2Ê®°ÂûãÊèêÈ´ò‰∫Ü86.3%„ÄÇLM2Âú®Â§öË∑≥Êé®ÁêÜ„ÄÅÊï∞ÂÄºÊé®ÁêÜÂíåÂ§ß‰∏ä‰∏ãÊñáÈóÆÁ≠îÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåËØÅÊòé‰∫ÜÊòæÂºèËÆ∞ÂøÜÂú®Â¢ûÂº∫TransformerÊû∂ÊûÑ‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06155",
            "title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile",
            "url": "https://huggingface.co/papers/2502.06155",
            "abstract": "Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify a prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 2) Shorten the sampling process by adopting existing multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities. We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities. Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video generation with a marginal performance trade-off in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91x speedup when running on 4 GPUs with sequence parallelism.",
            "score": 2,
            "issue_id": 2140,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 10",
                "zh": "2Êúà10Êó•"
            },
            "hash": "2f8d5e54db328d39",
            "authors": [
                "Hangliang Ding",
                "Dacheng Li",
                "Runlong Su",
                "Peiyuan Zhang",
                "Zhijie Deng",
                "Ion Stoica",
                "Hao Zhang"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University of California, Berkeley",
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06155.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#diffusion",
                    "#optimization",
                    "#inference",
                    "#video"
                ],
                "emoji": "üé¨",
                "ru": {
                    "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É—Å–∫–æ—Ä–µ–Ω–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ (DiTs). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø—Ä–æ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ 3D-–≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω —Ç—Ä–µ—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è —Å –Ω–∏–∑–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 7.4-7.8 —Ä–∞–∑ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ 720p —Å 29 –∏ 93 –∫–∞–¥—Ä–∞–º–∏ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤—Å–µ–≥–æ 0.1% –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è."
                },
                "en": {
                    "title": "Speeding Up Video Generation with Efficient Attention Mechanisms",
                    "desc": "This paper presents a solution to the inefficiency of Diffusion Transformers (DiTs) in generating high-fidelity videos. It introduces a method to prune 3D full attention by recognizing repetitive patterns in video data, leading to a sparse attention mechanism that reduces computational complexity. Additionally, the authors propose a multi-step consistency distillation approach to shorten the sampling process, allowing for faster video generation. The resulting model, Open-Sora-Plan-1.2, achieves significant speed improvements while maintaining performance, especially when utilizing distributed inference across multiple GPUs."
                },
                "zh": {
                    "title": "È´òÊïàËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊîπËøõÁöÑDiffusion TransformersÔºàDiTsÔºâÊ®°ÂûãÔºå‰ª•Ëß£ÂÜ≥ÁîüÊàêÈ´ò‰øùÁúüËßÜÈ¢ëÊó∂ÁöÑÊïàÁéáÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÈÄöËøáËØÜÂà´ËßÜÈ¢ëÊï∞ÊçÆ‰∏≠ÁöÑÂÜó‰ΩôÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ®ÄÁñèÁöÑ3DÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰ΩøÂÖ∂Âú®ËßÜÈ¢ëÂ∏ßÊï∞Èáè‰∏äÂÖ∑ÊúâÁ∫øÊÄßÂ§çÊùÇÂ∫¶„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨ÈááÁî®Â§öÊ≠•‰∏ÄËá¥ÊÄßËí∏È¶èÊäÄÊúØÔºåÁº©Áü≠‰∫ÜÈááÊ†∑ËøáÁ®ãÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÊõ¥Âø´ÈÄüÁöÑËßÜÈ¢ëÁîüÊàê„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®‰ΩøÁî®ÊûÅÂ∞ëÁöÑÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÊó∂ÔºåÁîüÊàêÈÄüÂ∫¶ÊèêÈ´ò‰∫Ü7.4Âà∞7.8ÂÄçÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËâØÂ•ΩÁöÑÊÄßËÉΩ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06023",
            "title": "Dual Caption Preference Optimization for Diffusion Models",
            "url": "https://huggingface.co/papers/2502.06023",
            "abstract": "Recent advancements in human preference optimization, originally developed for Large Language Models (LLMs), have shown significant potential in improving text-to-image diffusion models. These methods aim to learn the distribution of preferred samples while distinguishing them from less preferred ones. However, existing preference datasets often exhibit overlap between these distributions, leading to a conflict distribution. Additionally, we identified that input prompts contain irrelevant information for less preferred images, limiting the denoising network's ability to accurately predict noise in preference optimization methods, known as the irrelevant prompt issue. To address these challenges, we propose Dual Caption Preference Optimization (DCPO), a novel approach that utilizes two distinct captions to mitigate irrelevant prompts. To tackle conflict distribution, we introduce the Pick-Double Caption dataset, a modified version of Pick-a-Pic v2 with separate captions for preferred and less preferred images. We further propose three different strategies for generating distinct captions: captioning, perturbation, and hybrid methods. Our experiments show that DCPO significantly improves image quality and relevance to prompts, outperforming Stable Diffusion (SD) 2.1, SFT_Chosen, Diffusion-DPO, and MaPO across multiple metrics, including Pickscore, HPSv2.1, GenEval, CLIPscore, and ImageReward, fine-tuned on SD 2.1 as the backbone.",
            "score": 1,
            "issue_id": 2141,
            "pub_date": "2025-02-09",
            "pub_date_card": {
                "ru": "9 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 9",
                "zh": "2Êúà9Êó•"
            },
            "hash": "07782353b3b8b697",
            "authors": [
                "Amir Saeidi",
                "Yiran Luo",
                "Agneet Chatterjee",
                "Shamanthak Hegde",
                "Bimsara Pathiraja",
                "Yezhou Yang",
                "Chitta Baral"
            ],
            "affiliations": [
                "Arizona State University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06023.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#rlhf",
                    "#dataset",
                    "#diffusion",
                    "#cv"
                ],
                "emoji": "üñºÔ∏è",
                "ru": {
                    "title": "–î–≤–æ–π–Ω—ã–µ –ø–æ–¥–ø–∏—Å–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –æ–ø–∏—Å–∞–Ω–∏—è–º",
                    "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Dual Caption Preference Optimization (DCPO) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏ —Ç–µ–∫—Å—Ç-–≤-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. DCPO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –≤ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç Pick-Double Caption —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ –ø–æ–¥–ø–∏—Å—è–º–∏ –¥–ª—è –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –º–µ–Ω–µ–µ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DCPO –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∏—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–º–ø—Ç–∞–º –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏."
                },
                "en": {
                    "title": "Enhancing Image Generation with Dual Captions!",
                    "desc": "This paper presents Dual Caption Preference Optimization (DCPO), a new method to enhance text-to-image diffusion models by addressing issues in human preference optimization. It identifies problems with existing preference datasets, such as overlapping distributions and irrelevant prompts that hinder the denoising process. To overcome these challenges, DCPO employs two distinct captions for preferred and less preferred images, utilizing a modified dataset called Pick-Double Caption. The results demonstrate that DCPO significantly improves image quality and relevance, outperforming several existing models across various evaluation metrics."
                },
                "zh": {
                    "title": "ÂèåÈáçÊ†áÈ¢ò‰ºòÂåñÔºåÊèêÂçáÂõæÂÉèË¥®ÈáèÔºÅ",
                    "desc": "ÊúÄËøëÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÂèëÂ±ïÁöÑ‰∫∫Á±ªÂÅèÂ•Ω‰ºòÂåñÊäÄÊúØÔºåÊòæÁ§∫Âá∫Âú®ÊîπËøõÊñáÊú¨Âà∞ÂõæÂÉèÊâ©Êï£Ê®°ÂûãÊñπÈù¢ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇËøô‰∫õÊñπÊ≥ïÊó®Âú®Â≠¶‰π†ÂÅèÂ•ΩÊ†∑Êú¨ÁöÑÂàÜÂ∏ÉÔºåÂπ∂Â∞ÜÂÖ∂‰∏é‰∏çÂ§™ÂÅèÂ•ΩÁöÑÊ†∑Êú¨Âå∫ÂàÜÂºÄÊù•„ÄÇÁÑ∂ËÄåÔºåÁé∞ÊúâÁöÑÂÅèÂ•ΩÊï∞ÊçÆÈõÜÈÄöÂ∏∏Â≠òÂú®ÂàÜÂ∏ÉÈáçÂè†ÁöÑÈóÆÈ¢òÔºåÂØºËá¥ÂÜ≤Á™ÅÂàÜÂ∏É„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂèëÁé∞ËæìÂÖ•ÊèêÁ§∫‰∏≠ÂåÖÂê´‰∏é‰∏çÂ§™ÂÅèÂ•ΩÁöÑÂõæÂÉèÊó†ÂÖ≥ÁöÑ‰ø°ÊÅØÔºåËøôÈôêÂà∂‰∫ÜÂéªÂô™ÁΩëÁªúÂú®ÂÅèÂ•Ω‰ºòÂåñÊñπÊ≥ï‰∏≠ÁöÑÂáÜÁ°ÆÈ¢ÑÊµãËÉΩÂäõ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÊåëÊàòÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜÂèåÈáçÊ†áÈ¢òÂÅèÂ•Ω‰ºòÂåñÔºàDCPOÔºâÔºåÂà©Áî®‰∏§‰∏™‰∏çÂêåÁöÑÊ†áÈ¢òÊù•ÂáèËΩªÊó†ÂÖ≥ÊèêÁ§∫ÁöÑÈóÆÈ¢ò„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.05431",
            "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding",
            "url": "https://huggingface.co/papers/2502.05431",
            "abstract": "Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as a sequence introduces a considerable computational burden by re-encoding the combined selection of contexts for every request. To address this, we explore the promising potential of parallel encoding to independently pre-compute and cache each context's KV states. This approach enables the direct loading of cached states during inference while accommodating more contexts through position reuse across contexts. However, due to misalignments in attention distribution, directly applying parallel encoding results in a significant performance drop. To enable effective and efficient CAG, we propose Adaptive Parallel Encoding (APE), which brings shared prefix, attention temperature, and scaling factor to align the distribution of parallel encoding with sequential encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98% and 93% sequential encoding performance using the same inputs while outperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales to many-shot CAG, effectively encoding hundreds of contexts in parallel. Efficiency evaluation shows that APE can achieve an end-to-end 4.5times speedup by reducing 28times prefilling time for a 128K-length context.",
            "score": 1,
            "issue_id": 2141,
            "pub_date": "2025-02-08",
            "pub_date_card": {
                "ru": "8 —Ñ–µ–≤—Ä–∞–ª—è",
                "en": "February 8",
                "zh": "2Êúà8Êó•"
            },
            "hash": "7bc5b7aeb3716893",
            "authors": [
                "Xinyu Yang",
                "Tianqi Chen",
                "Beidi Chen"
            ],
            "affiliations": [
                "Carnegie Mellon University",
                "Nvidia"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.05431.jpg",
            "data": {
                "categories": [
                    "#rag",
                    "#optimization",
                    "#inference",
                    "#long_context"
                ],
                "emoji": "‚ö°",
                "ru": {
                    "title": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º: –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (APE) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. APE –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è—Ç—å –∏ –∫—ç—à–∏—Ä–æ–≤–∞—Ç—å KV-—Å–æ—Å—Ç–æ—è–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤. –ú–µ—Ç–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—â–∏–π –ø—Ä–µ—Ñ–∏–∫—Å, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –≤–Ω–∏–º–∞–Ω–∏—è –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—â–∏–π —Ñ–∞–∫—Ç–æ—Ä. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ APE —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–æ 98% –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –æ–±—ã—á–Ω–æ–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ 3.6-7.9% –≤ –∑–∞–¥–∞—á–∞—Ö RAG –∏ ICL."
                },
                "en": {
                    "title": "Boosting Efficiency in Context-Augmented Generation with APE",
                    "desc": "This paper introduces Adaptive Parallel Encoding (APE) as a solution to improve the efficiency of context-augmented generation (CAG) techniques like RAG and ICL. Traditional methods face high computational costs when combining multiple contexts for generating responses, as they require re-encoding for each request. APE allows for the pre-computation and caching of key-value (KV) states for each context, which can then be loaded during inference, significantly speeding up the process. The proposed method aligns the attention distribution of parallel encoding with that of sequential encoding, achieving high performance while handling many contexts efficiently."
                },
                "zh": {
                    "title": "Ëá™ÈÄÇÂ∫îÂπ∂Ë°åÁºñÁ†ÅÔºöÊèêÂçá‰∏ä‰∏ãÊñáÁîüÊàêÊïàÁéáÁöÑÂÖ≥ÈîÆ",
                    "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰∏ä‰∏ãÊñáÂ¢ûÂº∫ÁîüÊàêÔºàCAGÔºâÊäÄÊúØ‰∏≠ÁöÑÂπ∂Ë°åÁºñÁ†ÅÊñπÊ≥ïÔºå‰ª•ÊèêÈ´òÁîüÊàêÁî®Êà∑Êü•ËØ¢ÂìçÂ∫îÁöÑÊïàÁéá„ÄÇ‰º†ÁªüÊñπÊ≥ïÂú®ÊØèÊ¨°ËØ∑Ê±ÇÊó∂ÈÉΩÈúÄË¶ÅÈáçÊñ∞ÁºñÁ†ÅÂ§ö‰∏™‰∏ä‰∏ãÊñáÔºåÂØºËá¥ËÆ°ÁÆóË¥üÊãÖËøáÈáç„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜËá™ÈÄÇÂ∫îÂπ∂Ë°åÁºñÁ†ÅÔºàAPEÔºâÔºåÈÄöËøáÂÖ±‰∫´ÂâçÁºÄ„ÄÅÊ≥®ÊÑèÂäõÊ∏©Â∫¶ÂíåÁº©ÊîæÂõ†Â≠êÊù•Ë∞ÉÊï¥Âπ∂Ë°åÁºñÁ†Å‰∏éÈ°∫Â∫èÁºñÁ†ÅÁöÑÊ≥®ÊÑèÂäõÂàÜÂ∏ÉÔºå‰ªéËÄåÊèêÈ´òÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAPEÂú®‰øùÊåÅÈ´òÊÄßËÉΩÁöÑÂêåÊó∂ÔºåËÉΩÂ§üÊòæËëóÂä†Âø´Â§ÑÁêÜÈÄüÂ∫¶ÔºåÈÄÇÁî®‰∫éÂ§ÑÁêÜÂ§ßÈáè‰∏ä‰∏ãÊñá„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2025-02-10.html",
    "link_next": "2025-02-12.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "10.02",
        "en": "02/10",
        "zh": "2Êúà10Êó•"
    },
    "short_date_next": {
        "ru": "12.02",
        "en": "02/12",
        "zh": "2Êúà12Êó•"
    },
    "categories": {
        "#dataset": 3,
        "#data": 1,
        "#benchmark": 4,
        "#agents": 0,
        "#cv": 2,
        "#rl": 2,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 5,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 2,
        "#multilingual": 1,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 8,
        "#robotics": 0,
        "#agi": 1,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 7,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 2,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 2,
        "#small_models": 1,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁß∞‰∏∫ÊªëÂä®Á£ÅË¥¥Ê≥®ÊÑèÂäõÔºàSTAÔºâÔºå‰ª•Ëß£ÂÜ≥Êâ©Êï£ÂèòÂéãÂô®ÔºàDiTsÔºâÂú®ËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑËÆ°ÁÆóÊàêÊú¨ÈóÆÈ¢ò„ÄÇ‰º†ÁªüÁöÑÂÖ®Ê≥®ÊÑèÂäõÊú∫Âà∂Âú®ÁîüÊàêÁü≠ËßÜÈ¢ëÊó∂ËÄóÊó∂ÊûÅÈïø„ÄÇSTAÈÄöËøáÂú®Â±ÄÈÉ®3DÁ™óÂè£ÂÜÖÊªëÂä®ÂíåÂÖ≥Ê≥®ÔºåÊ∂àÈô§‰∫ÜÂÖ®Ê≥®ÊÑèÂäõ‰∏≠ÁöÑÂÜó‰Ωô„ÄÇ‰∏é‰º†ÁªüÁöÑÊªëÂä®Á™óÂè£Ê≥®ÊÑèÂäõ‰∏çÂêåÔºåSTAÈááÁî®Á°¨‰ª∂ÊÑüÁü•ÁöÑËÆæËÆ°ÔºåÊèêÈ´ò‰∫ÜÊïàÁéá„ÄÇÂÆûÈ™åÊòæÁ§∫ÔºåSTAÂú®‰∏çÈôç‰ΩéË¥®ÈáèÁöÑÊÉÖÂÜµ‰∏ãÔºåÊòæËëóÂáèÂ∞ë‰∫ÜËßÜÈ¢ëÁîüÊàêÁöÑÂª∂Ëøü„ÄÇ",
        "title": "Fast Video Generation with Sliding Tile Attention",
        "pinyin": "ËøôÁØáÊñáÁ´†‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåÁß∞‰∏∫ÊªëÂä®Á£ÅË¥¥Ê≥®ÊÑèÂäõÔºàSTAÔºâÔºå‰ª•Ëß£ÂÜ≥Êâ©Êï£ÂèòÂéãÂô®ÔºàDiTsÔºâÂú®ËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑËÆ°ÁÆóÊàêÊú¨ÈóÆÈ¢ò„ÄÇ‰º†ÁªüÁöÑÂÖ®Ê≥®ÊÑèÂäõÊú∫Âà∂Âú®ÁîüÊàêÁü≠ËßÜÈ¢ëÊó∂ËÄóÊó∂ÊûÅÈïø„ÄÇSTAÈÄöËøáÂú®Â±ÄÈÉ®3DÁ™óÂè£ÂÜÖÊªëÂä®ÂíåÂÖ≥Ê≥®ÔºåÊ∂àÈô§‰∫ÜÂÖ®Ê≥®ÊÑèÂäõ‰∏≠ÁöÑÂÜó‰Ωô„ÄÇ‰∏é‰º†ÁªüÁöÑÊªëÂä®Á™óÂè£Ê≥®ÊÑèÂäõ‰∏çÂêåÔºåSTAÈááÁî®Á°¨‰ª∂ÊÑüÁü•ÁöÑËÆæËÆ°ÔºåÊèêÈ´ò‰∫ÜÊïàÁéá„ÄÇÂÆûÈ™åÊòæÁ§∫ÔºåSTAÂú®‰∏çÈôç‰ΩéË¥®ÈáèÁöÑÊÉÖÂÜµ‰∏ãÔºåÊòæËëóÂáèÂ∞ë‰∫ÜËßÜÈ¢ëÁîüÊàêÁöÑÂª∂Ëøü„ÄÇ\n\nzh√® piƒÅn w√©n zhƒÅng ji√® sh√†o le yƒ´ zh«íng xƒ´n de zh√π y√¨ l√¨ jƒ´ zh√¨, chƒìng w√©i hu√° d√≤ng c√≠ tiƒì zh√π y√¨ l√¨ (STA), y«ê jiƒõ ju√© ku√≤ s√†n bi√†n sh≈´ q√¨ (DiTs) z√†i sh√¨ p√≠n shƒìng ch√©ng zh≈çng de j√¨ su√†n ch√©ng bƒõn w√®n t√≠. chu√°n t«íng de qu√°n zh√π y√¨ l√¨ jƒ´ zh√¨ z√†i shƒìng ch√©ng du«én sh√¨ p√≠n sh√≠ h√†o sh√≠ j√≠ ch√°ng. STA t≈çng gu√≤ z√†i j√∫ b√π 3D chuƒÅng k«íu n√®i hu√° d√≤ng h√© guƒÅn zh√π, xiƒÅo ch√∫ le qu√°n zh√π y√¨ l√¨ zh≈çng de r√≥ng y√π. y«î chu√°n t«íng de hu√° d√≤ng chuƒÅng k«íu zh√π y√¨ l√¨ b√π t√≥ng, STA c«éi y√≤ng y√¨ng ji√†n g«én zhƒ´ de sh√® j√¨, t√≠ gƒÅo le xi√†o l«ú. sh√≠ y√†n xi«én sh√¨, STA z√†i b√π ji√†ng dƒ´ zh√¨ li√†ng de q√≠ng ku√†ng xi√†, xi«én zh√π ji«én sh«éo le sh√¨ p√≠n shƒìng ch√©ng de y√°n ch√≠.",
        "vocab": "[{'word': 'Ê≥®ÊÑèÂäõ', 'pinyin': 'zh√πy√¨l√¨', 'trans': 'attention'},\n{'word': 'Êú∫Âà∂', 'pinyin': 'jƒ´zh√¨', 'trans': 'mechanism'},\n{'word': 'ÊªëÂä®', 'pinyin': 'hu√°d√≤ng', 'trans': 'sliding'},\n{'word': 'Á£ÅË¥¥', 'pinyin': 'c√≠tiƒì', 'trans': 'magnetic tile'},\n{'word': 'Êâ©Êï£', 'pinyin': 'ku√≤s√†n', 'trans': 'diffusion'},\n{'word': 'ÂèòÂéãÂô®', 'pinyin': 'bi√†nyƒÅq√¨', 'trans': 'transformer'},\n{'word': 'ËÆ°ÁÆó', 'pinyin': 'j√¨su√†n', 'trans': 'computation'},\n{'word': 'ÊàêÊú¨', 'pinyin': 'ch√©ngbƒõn', 'trans': 'cost'},\n{'word': '‰º†Áªü', 'pinyin': 'chu√°nt«íng', 'trans': 'traditional'},\n{'word': 'ÂÖ®', 'pinyin': 'qu√°n', 'trans': 'full'},\n{'word': 'ËÄóÊó∂', 'pinyin': 'h√†osh√≠', 'trans': 'time-consuming'},\n{'word': 'ÊûÅÈïø', 'pinyin': 'j√≠ch√°ng', 'trans': 'extremely long'},\n{'word': 'Â±ÄÈÉ®', 'pinyin': 'j√∫b√π', 'trans': 'local'},\n{'word': '3D', 'pinyin': '', 'trans': '3D'},\n{'word': 'Á™óÂè£', 'pinyin': 'chuƒÅngk«íu', 'trans': 'window'},\n{'word': 'ÂÜó‰Ωô', 'pinyin': 'r«íngy√∫', 'trans': 'redundancy'},\n{'word': 'Á°¨‰ª∂', 'pinyin': 'y√¨ngji√†n', 'trans': 'hardware'},\n{'word': 'ÊÑüÁü•', 'pinyin': 'g«énzhƒ´', 'trans': 'perception'},\n{'word': 'ËÆæËÆ°', 'pinyin': 'sh√®j√¨', 'trans': 'design'},\n{'word': 'ÊïàÁéá', 'pinyin': 'xi√†ol«ú', 'trans': 'efficiency'},\n{'word': 'ÂÆûÈ™å', 'pinyin': 'sh√≠y√†n', 'trans': 'experiment'},\n{'word': 'ÊòæÁ§∫', 'pinyin': 'xi«énsh√¨', 'trans': 'display'},\n{'word': 'ÊòæËëó', 'pinyin': 'xi«énzh√π', 'trans': 'significant'},\n{'word': 'ÂáèÂ∞ë', 'pinyin': 'ji«énsh«éo', 'trans': 'reduce'},\n{'word': 'Âª∂Ëøü', 'pinyin': 'y√°nch√≠', 'trans': 'latency'}]",
        "trans": "This article introduces a new attention mechanism called Sliding Tile Attention (STA) to address the computational cost issues of Diffusion Transformers (DiTs) in video generation. Traditional full attention mechanisms take an extremely long time to generate short videos. STA eliminates redundancy in full attention by sliding and focusing within local 3D windows. Unlike traditional sliding window attention, STA employs a hardware-aware design to enhance efficiency. Experiments show that STA significantly reduces the latency of video generation without compromising quality.",
        "update_ts": "2025-02-10 09:11"
    }
}