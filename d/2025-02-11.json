{
    "date": {
        "ru": "11 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        "en": "February 11",
        "zh": "2æœˆ11æ—¥"
    },
    "time_utc": "2025-02-11 03:15",
    "weekday": 1,
    "issue_id": 2140,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2502.03628",
            "title": "The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering",
            "url": "https://huggingface.co/papers/2502.03628",
            "abstract": "Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits rankings throughout the generation process, revealing three key patterns in how LVLMs process information: (1) gradual visual information loss -- visually grounded tokens gradually become less favored throughout generation, and (2) early excitation -- semantically meaningful tokens achieve peak activation in the layers earlier than the final layer. (3) hidden genuine information -- visually grounded tokens though not being eventually decided still retain relatively high rankings at inference. Based on these insights, we propose VISTA (Visual Information Steering with Token-logit Augmentation), a training-free inference-time intervention framework that reduces hallucination while promoting genuine information. VISTA works by combining two complementary approaches: reinforcing visual information in activation space and leveraging early layer activations to promote semantically meaningful decoding. Compared to existing methods, VISTA requires no external supervision and is applicable to various decoding strategies. Extensive experiments show that VISTA on average reduces hallucination by abount 40% on evaluated open-ended generation task, and it consistently outperforms existing methods on four benchmarks across four architectures under three decoding strategies.",
            "score": 4,
            "issue_id": 2140,
            "pub_date": "2025-02-05",
            "pub_date_card": {
                "ru": "5 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 5",
                "zh": "2æœˆ5æ—¥"
            },
            "hash": "04b182d80abf9219",
            "authors": [
                "Zhuowei Li",
                "Haizhou Shi",
                "Yunhe Gao",
                "Di Liu",
                "Zhenting Wang",
                "Yuxiao Chen",
                "Ting Liu",
                "Long Zhao",
                "Hao Wang",
                "Dimitris N. Metaxas"
            ],
            "affiliations": [
                "Google DeepMind",
                "Rutgers University",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.03628.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#hallucinations",
                    "#benchmark",
                    "#inference",
                    "#interpretability",
                    "#multimodal"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ¼ĞµÑ‚Ğ¾Ğ´ VISTA",
                    "desc": "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LVLM) Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹, Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ VISTA Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VISTA Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° 40% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…."
                },
                "en": {
                    "title": "VISTA: Reducing Hallucination in Vision-Language Models",
                    "desc": "This paper explores the issue of hallucination in Large Vision-Language Models (LVLMs), where the models generate plausible text that does not correspond to visual inputs. The authors identify three patterns in the generation process: a gradual loss of visual information, early activation of semantically meaningful tokens, and the presence of high-ranking visually grounded tokens that are not ultimately chosen. To address these issues, they introduce VISTA, a framework that enhances visual information during inference without requiring additional training. VISTA effectively reduces hallucination by about 40% and outperforms existing methods across multiple benchmarks and architectures."
                },
                "zh": {
                    "title": "å‡å°‘å¹»è§‰ï¼Œæå‡çœŸå®ä¿¡æ¯çš„VISTAæ¡†æ¶",
                    "desc": "å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†æ–‡æœ¬å’Œè§†è§‰è¾“å…¥ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šäº§ç”Ÿè¯­æ³•ä¸Šè¿è´¯ä½†è§†è§‰ä¸Šä¸çœŸå®çš„å†…å®¹ã€‚æœ¬æ–‡ç ”ç©¶äº†å¹»è§‰çš„å†…éƒ¨åŠ¨æ€ï¼Œå‘ç°LVLMsåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¤„ç†ä¿¡æ¯çš„ä¸‰ç§å…³é”®æ¨¡å¼ï¼šé€æ¸ä¸§å¤±è§†è§‰ä¿¡æ¯ã€æ—©æœŸæ¿€æ´»å’Œéšè—çš„çœŸå®ä¿¡æ¯ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†VISTAï¼ˆè§†è§‰ä¿¡æ¯å¼•å¯¼ä¸æ ‡è®°é€»è¾‘å¢å¼ºï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ¨ç†æ—¶å¹²é¢„æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘å¹»è§‰å¹¶ä¿ƒè¿›çœŸå®ä¿¡æ¯çš„ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒVISTAåœ¨å¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡ä¸­å¹³å‡å‡å°‘äº†çº¦40%çš„å¹»è§‰ï¼Œå¹¶åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­åœ¨ä¸‰ç§è§£ç ç­–ç•¥ä¸‹å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06155",
            "title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile",
            "url": "https://huggingface.co/papers/2502.06155",
            "abstract": "Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify a prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 2) Shorten the sampling process by adopting existing multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities. We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities. Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video generation with a marginal performance trade-off in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91x speedup when running on 4 GPUs with sequence parallelism.",
            "score": 1,
            "issue_id": 2140,
            "pub_date": "2025-02-10",
            "pub_date_card": {
                "ru": "10 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 10",
                "zh": "2æœˆ10æ—¥"
            },
            "hash": "2f8d5e54db328d39",
            "authors": [
                "Hangliang Ding",
                "Dacheng Li",
                "Runlong Su",
                "Peiyuan Zhang",
                "Zhijie Deng",
                "Ion Stoica",
                "Hao Zhang"
            ],
            "affiliations": [
                "Shanghai Jiao Tong University",
                "Tsinghua University",
                "University of California, Berkeley",
                "University of California, San Diego"
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06155.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#diffusion",
                    "#optimization",
                    "#inference",
                    "#video"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (DiTs). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ 3D-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑˆĞ°Ğ³Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 7.4-7.8 Ñ€Ğ°Ğ· Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ 720p Ñ 29 Ğ¸ 93 ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 0.1% Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Speeding Up Video Generation with Efficient Attention Mechanisms",
                    "desc": "This paper presents a solution to the inefficiency of Diffusion Transformers (DiTs) in generating high-fidelity videos. It introduces a method to prune 3D full attention by recognizing repetitive patterns in video data, leading to a sparse attention mechanism that reduces computational complexity. Additionally, the authors propose a multi-step consistency distillation approach to shorten the sampling process, allowing for faster video generation. The resulting model, Open-Sora-Plan-1.2, achieves significant speed improvements while maintaining performance, especially when utilizing distributed inference across multiple GPUs."
                },
                "zh": {
                    "title": "é«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„Diffusion Transformersï¼ˆDiTsï¼‰æ¨¡å‹ï¼Œä»¥è§£å†³ç”Ÿæˆé«˜ä¿çœŸè§†é¢‘æ—¶çš„æ•ˆç‡é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡è¯†åˆ«è§†é¢‘æ•°æ®ä¸­çš„å†—ä½™ï¼Œæå‡ºäº†ä¸€ç§ç¨€ç–çš„3Dæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿å…¶åœ¨è§†é¢‘å¸§æ•°é‡ä¸Šå…·æœ‰çº¿æ€§å¤æ‚åº¦ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šæ­¥ä¸€è‡´æ€§è’¸é¦æŠ€æœ¯ï¼Œç¼©çŸ­äº†é‡‡æ ·è¿‡ç¨‹ï¼Œä»è€Œå®ç°äº†æ›´å¿«é€Ÿçš„è§†é¢‘ç”Ÿæˆã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä½¿ç”¨æå°‘çš„é¢„è®­ç»ƒæ•°æ®æ—¶ï¼Œç”Ÿæˆé€Ÿåº¦æé«˜äº†7.4åˆ°7.8å€ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ€§èƒ½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2502.06049",
            "title": "LM2: Large Memory Models",
            "url": "https://huggingface.co/papers/2502.06049",
            "abstract": "This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates a memory module that acts as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformers general-purpose capabilities, LM2 maintains the original information flow while integrating a complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures.",
            "score": 0,
            "issue_id": 2140,
            "pub_date": "2025-02-09",
            "pub_date_card": {
                "ru": "9 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
                "en": "February 9",
                "zh": "2æœˆ9æ—¥"
            },
            "hash": "5f62d7e814a6918f",
            "authors": [
                "Jikun Kang",
                "Wenqi Wu",
                "Filippos Christianos",
                "Alex J. Chan",
                "Fraser Greenlee",
                "George Thomas",
                "Marvin Purtorab",
                "Andy Toulis"
            ],
            "affiliations": [
                "Convergence Labs Ltd."
            ],
            "pdf_title_img": "assets/pdf/title_img/2502.06049.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#architecture",
                    "#benchmark",
                    "#interpretability",
                    "#long_context",
                    "#reasoning"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "LM2: Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Large Memory Model (LM2), Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€-Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. LM2 Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ BABILong Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. LM2 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ñ…, Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼."
                },
                "en": {
                    "title": "Enhancing Transformers with Memory for Superior Reasoning",
                    "desc": "The paper presents the Large Memory Model (LM2), a new type of Transformer designed to improve multi-step reasoning and information synthesis over long contexts. LM2 features an auxiliary memory module that stores contextual information and interacts with input data through cross attention, allowing it to update its memory dynamically. This model retains the original capabilities of Transformers while adding a memory pathway that enhances performance on complex tasks. Experimental results show that LM2 significantly outperforms existing models in reasoning tasks and maintains strong performance on general tasks, highlighting the value of integrating explicit memory into Transformer architectures."
                },
                "zh": {
                    "title": "å¤§å‹è®°å¿†æ¨¡å‹ï¼šæå‡Transformeræ¨ç†èƒ½åŠ›çš„å…³é”®",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºå¤§å‹è®°å¿†æ¨¡å‹ï¼ˆLM2ï¼‰çš„è§£ç å™¨ä»…Transformeræ¶æ„ï¼Œæ—¨åœ¨è§£å†³æ ‡å‡†Transformeråœ¨å¤šæ­¥æ¨ç†ã€å…³ç³»è®ºè¯å’Œé•¿ä¸Šä¸‹æ–‡ä¿¡æ¯ç»¼åˆæ–¹é¢çš„å±€é™æ€§ã€‚LM2å¼•å…¥äº†ä¸€ä¸ªè¾…åŠ©è®°å¿†æ¨¡å—ï¼Œä½œä¸ºä¸Šä¸‹æ–‡è¡¨ç¤ºçš„å­˜å‚¨åº“ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›ä¸è¾“å…¥æ ‡è®°äº¤äº’ï¼Œå¹¶é€šè¿‡é—¨æ§æœºåˆ¶è¿›è¡Œæ›´æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLM2åœ¨BABILongåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¹³å‡æ€§èƒ½æ¯”è®°å¿†å¢å¼ºçš„RMTæ¨¡å‹æé«˜äº†37.1%ï¼Œæ¯”åŸºçº¿Llama-3.2æ¨¡å‹æé«˜äº†86.3%ã€‚LM2åœ¨å¤šè·³æ¨ç†ã€æ•°å€¼æ¨ç†å’Œå¤§ä¸Šä¸‹æ–‡é—®ç­”æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†æ˜¾å¼è®°å¿†åœ¨å¢å¼ºTransformeræ¶æ„ä¸­çš„é‡è¦æ€§ã€‚"
                }
            }
        }
    ],
    "link_prev": "2025-02-10.html",
    "link_next": "2025-02-12.html",
    "link_month": "2025-02.html",
    "short_date_prev": {
        "ru": "10.02",
        "en": "02/10",
        "zh": "2æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "12.02",
        "en": "02/12",
        "zh": "2æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 1,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 0,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 2,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 1,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#healthcare": 0,
        "#training": 2,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 2,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 0,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºæ»‘åŠ¨ç£è´´æ³¨æ„åŠ›ï¼ˆSTAï¼‰ï¼Œä»¥è§£å†³æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ä¼ ç»Ÿçš„å…¨æ³¨æ„åŠ›æœºåˆ¶åœ¨ç”ŸæˆçŸ­è§†é¢‘æ—¶è€—æ—¶æé•¿ã€‚STAé€šè¿‡åœ¨å±€éƒ¨3Dçª—å£å†…æ»‘åŠ¨å’Œå…³æ³¨ï¼Œæ¶ˆé™¤äº†å…¨æ³¨æ„åŠ›ä¸­çš„å†—ä½™ã€‚ä¸ä¼ ç»Ÿçš„æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ä¸åŒï¼ŒSTAé‡‡ç”¨ç¡¬ä»¶æ„ŸçŸ¥çš„è®¾è®¡ï¼Œæé«˜äº†æ•ˆç‡ã€‚å®éªŒæ˜¾ç¤ºï¼ŒSTAåœ¨ä¸é™ä½è´¨é‡çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—å‡å°‘äº†è§†é¢‘ç”Ÿæˆçš„å»¶è¿Ÿã€‚",
        "title": "Fast Video Generation with Sliding Tile Attention",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºæ»‘åŠ¨ç£è´´æ³¨æ„åŠ›ï¼ˆSTAï¼‰ï¼Œä»¥è§£å†³æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ä¼ ç»Ÿçš„å…¨æ³¨æ„åŠ›æœºåˆ¶åœ¨ç”ŸæˆçŸ­è§†é¢‘æ—¶è€—æ—¶æé•¿ã€‚STAé€šè¿‡åœ¨å±€éƒ¨3Dçª—å£å†…æ»‘åŠ¨å’Œå…³æ³¨ï¼Œæ¶ˆé™¤äº†å…¨æ³¨æ„åŠ›ä¸­çš„å†—ä½™ã€‚ä¸ä¼ ç»Ÿçš„æ»‘åŠ¨çª—å£æ³¨æ„åŠ›ä¸åŒï¼ŒSTAé‡‡ç”¨ç¡¬ä»¶æ„ŸçŸ¥çš„è®¾è®¡ï¼Œæé«˜äº†æ•ˆç‡ã€‚å®éªŒæ˜¾ç¤ºï¼ŒSTAåœ¨ä¸é™ä½è´¨é‡çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—å‡å°‘äº†è§†é¢‘ç”Ÿæˆçš„å»¶è¿Ÿã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le yÄ« zhÇ’ng xÄ«n de zhÃ¹ yÃ¬ lÃ¬ jÄ« zhÃ¬, chÄ“ng wÃ©i huÃ¡ dÃ²ng cÃ­ tiÄ“ zhÃ¹ yÃ¬ lÃ¬ (STA), yÇ jiÄ› juÃ© kuÃ² sÃ n biÃ n shÅ« qÃ¬ (DiTs) zÃ i shÃ¬ pÃ­n shÄ“ng chÃ©ng zhÅng de jÃ¬ suÃ n chÃ©ng bÄ›n wÃ¨n tÃ­. chuÃ¡n tÇ’ng de quÃ¡n zhÃ¹ yÃ¬ lÃ¬ jÄ« zhÃ¬ zÃ i shÄ“ng chÃ©ng duÇn shÃ¬ pÃ­n shÃ­ hÃ o shÃ­ jÃ­ chÃ¡ng. STA tÅng guÃ² zÃ i jÃº bÃ¹ 3D chuÄng kÇ’u nÃ¨i huÃ¡ dÃ²ng hÃ© guÄn zhÃ¹, xiÄo chÃº le quÃ¡n zhÃ¹ yÃ¬ lÃ¬ zhÅng de rÃ³ng yÃ¹. yÇ” chuÃ¡n tÇ’ng de huÃ¡ dÃ²ng chuÄng kÇ’u zhÃ¹ yÃ¬ lÃ¬ bÃ¹ tÃ³ng, STA cÇi yÃ²ng yÃ¬ng jiÃ n gÇn zhÄ« de shÃ¨ jÃ¬, tÃ­ gÄo le xiÃ o lÇœ. shÃ­ yÃ n xiÇn shÃ¬, STA zÃ i bÃ¹ jiÃ ng dÄ« zhÃ¬ liÃ ng de qÃ­ng kuÃ ng xiÃ , xiÇn zhÃ¹ jiÇn shÇo le shÃ¬ pÃ­n shÄ“ng chÃ©ng de yÃ¡n chÃ­.",
        "vocab": "[{'word': 'æ³¨æ„åŠ›', 'pinyin': 'zhÃ¹yÃ¬lÃ¬', 'trans': 'attention'},\n{'word': 'æœºåˆ¶', 'pinyin': 'jÄ«zhÃ¬', 'trans': 'mechanism'},\n{'word': 'æ»‘åŠ¨', 'pinyin': 'huÃ¡dÃ²ng', 'trans': 'sliding'},\n{'word': 'ç£è´´', 'pinyin': 'cÃ­tiÄ“', 'trans': 'magnetic tile'},\n{'word': 'æ‰©æ•£', 'pinyin': 'kuÃ²sÃ n', 'trans': 'diffusion'},\n{'word': 'å˜å‹å™¨', 'pinyin': 'biÃ nyÄqÃ¬', 'trans': 'transformer'},\n{'word': 'è®¡ç®—', 'pinyin': 'jÃ¬suÃ n', 'trans': 'computation'},\n{'word': 'æˆæœ¬', 'pinyin': 'chÃ©ngbÄ›n', 'trans': 'cost'},\n{'word': 'ä¼ ç»Ÿ', 'pinyin': 'chuÃ¡ntÇ’ng', 'trans': 'traditional'},\n{'word': 'å…¨', 'pinyin': 'quÃ¡n', 'trans': 'full'},\n{'word': 'è€—æ—¶', 'pinyin': 'hÃ oshÃ­', 'trans': 'time-consuming'},\n{'word': 'æé•¿', 'pinyin': 'jÃ­chÃ¡ng', 'trans': 'extremely long'},\n{'word': 'å±€éƒ¨', 'pinyin': 'jÃºbÃ¹', 'trans': 'local'},\n{'word': '3D', 'pinyin': '', 'trans': '3D'},\n{'word': 'çª—å£', 'pinyin': 'chuÄngkÇ’u', 'trans': 'window'},\n{'word': 'å†—ä½™', 'pinyin': 'rÇ’ngyÃº', 'trans': 'redundancy'},\n{'word': 'ç¡¬ä»¶', 'pinyin': 'yÃ¬ngjiÃ n', 'trans': 'hardware'},\n{'word': 'æ„ŸçŸ¥', 'pinyin': 'gÇnzhÄ«', 'trans': 'perception'},\n{'word': 'è®¾è®¡', 'pinyin': 'shÃ¨jÃ¬', 'trans': 'design'},\n{'word': 'æ•ˆç‡', 'pinyin': 'xiÃ olÇœ', 'trans': 'efficiency'},\n{'word': 'å®éªŒ', 'pinyin': 'shÃ­yÃ n', 'trans': 'experiment'},\n{'word': 'æ˜¾ç¤º', 'pinyin': 'xiÇnshÃ¬', 'trans': 'display'},\n{'word': 'æ˜¾è‘—', 'pinyin': 'xiÇnzhÃ¹', 'trans': 'significant'},\n{'word': 'å‡å°‘', 'pinyin': 'jiÇnshÇo', 'trans': 'reduce'},\n{'word': 'å»¶è¿Ÿ', 'pinyin': 'yÃ¡nchÃ­', 'trans': 'latency'}]",
        "trans": "This article introduces a new attention mechanism called Sliding Tile Attention (STA) to address the computational cost issues of Diffusion Transformers (DiTs) in video generation. Traditional full attention mechanisms take an extremely long time to generate short videos. STA eliminates redundancy in full attention by sliding and focusing within local 3D windows. Unlike traditional sliding window attention, STA employs a hardware-aware design to enhance efficiency. Experiments show that STA significantly reduces the latency of video generation without compromising quality.",
        "update_ts": "2025-02-10 09:11"
    }
}