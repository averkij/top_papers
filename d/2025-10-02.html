
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 26 papers. October 2.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">26 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-01.html">â¬…ï¸ <span id="prev-date">01.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-03.html">â¡ï¸ <span id="next-date">03.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'};
        let feedDateNext = {'ru': '03.10', 'en': '10/03', 'zh': '10æœˆ3æ—¥'};
        let feedDatePrev = {'ru': '01.10', 'en': '10/01', 'zh': '10æœˆ1æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.25454', 'title': 'DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search', 'url': 'https://huggingface.co/papers/2509.25454', 'abstract': 'DeepSearch integrates Monte Carlo Tree Search into RLVR training to enhance exploration and credit assignment, achieving state-of-the-art performance with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models - using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.', 'score': 65, 'issue_id': 6201, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': 'c81bb4fe47e669e8', 'authors': ['Fang Wu', 'Weihao Xuan', 'Heli Qi', 'Ximing Lu', 'Aaron Tu', 'Li Erran Li', 'Yejin Choi'], 'affiliations': ['Amazon AWS', 'RIKEN AIP', 'Stanford University', 'UC Berkeley', 'University of Tokyo', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2509.25454.jpg', 'data': {'categories': ['#math', '#reasoning', '#optimization', '#rl', '#training'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ¹ ÑĞ¸Ğ»Ñ‹: Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM', 'desc': 'DeepSearch Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Monte Carlo Tree Search Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RLVR) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ»Ğ°Ñ‚Ğ¾ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰ÑƒÑ Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ñ… RLVR. DeepSearch Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑƒĞ·Ğ»Ğ¾Ğ², ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ±ÑƒÑ„ĞµÑ€ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 62.95% Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 1.5B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ² 5.7 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Revolutionizing RLVR: Strategic Exploration with DeepSearch', 'desc': 'DeepSearch is a novel framework that enhances Reinforcement Learning with Value Regression (RLVR) by incorporating Monte Carlo Tree Search (MCTS) into the training process. This integration allows for better exploration of the solution space and improves credit assignment, addressing the common issue of training plateaus in existing RLVR methods. By employing a global frontier selection strategy and entropy-based guidance, DeepSearch systematically identifies and prioritizes promising reasoning paths. The framework achieves state-of-the-art performance on mathematical reasoning tasks while significantly reducing computational costs, demonstrating the effectiveness of strategic exploration in machine learning.'}, 'zh': {'title': 'æ·±åº¦æœç´¢ï¼šé€šè¿‡ç³»ç»Ÿæ¢ç´¢æå‡æ¨ç†èƒ½åŠ›', 'desc': 'DeepSearch æ˜¯ä¸€ç§å°†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰é›†æˆåˆ°å¼ºåŒ–å­¦ä¹ ä»·å€¼å›å½’ï¼ˆRLVRï¼‰è®­ç»ƒä¸­çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºæ¢ç´¢èƒ½åŠ›å’Œä¿¡ç”¨åˆ†é…ã€‚å½“å‰çš„ RLVR æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­˜åœ¨æ¢ç´¢ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½æå‡å‡ç¼“ã€‚DeepSearch é€šè¿‡åœ¨è®­ç»ƒå¾ªç¯ä¸­åµŒå…¥ç»“æ„åŒ–æœç´¢ï¼Œç³»ç»Ÿæ€§åœ°æ¢ç´¢è§£å†³æ–¹æ¡ˆç©ºé—´ï¼Œä»è€Œè§£å†³äº†è¿™ä¸€ç“¶é¢ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepSearch åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† 62.95% çš„å¹³å‡å‡†ç¡®ç‡ï¼Œå¹¶ä¸”ä½¿ç”¨çš„ GPU è®¡ç®—æ—¶é—´æ¯”ä¼ ç»Ÿæ–¹æ³•å°‘äº† 5.7 å€ï¼Œå±•ç¤ºäº†ç®—æ³•åˆ›æ–°åœ¨æå‡ RLVR æ–¹æ³•ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01051', 'title': 'GEM: A Gym for Agentic LLMs', 'url': 'https://huggingface.co/papers/2510.01051', 'abstract': 'GEM, an open-source environment simulator, facilitates experience-based learning for large language models by providing a standardized framework and diverse environments for training and benchmarking reinforcement learning algorithms.  \t\t\t\t\tAI-generated summary \t\t\t\t The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which -- unlike GRPO -- is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research.', 'score': 40, 'issue_id': 6198, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '6d69bb75ee0b2258', 'authors': ['Zichen Liu', 'Anya Sims', 'Keyu Duan', 'Changyu Chen', 'Simon Yu', 'Xiangxin Zhou', 'Haotian Xu', 'Shaopan Xiong', 'Bo Liu', 'Chenmien Tan', 'Chuen Yang Beh', 'Weixun Wang', 'Hao Zhu', 'Weiyan Shi', 'Diyi Yang', 'Michael Shieh', 'Yee Whye Teh', 'Wee Sun Lee', 'Min Lin'], 'affiliations': ['NUS', 'Northeastern', 'OpenRLHF', 'Oxford', 'RL2', 'ROLL', 'SMU', 'Sea AI Lab', 'Stanford'], 'pdf_title_img': 'assets/pdf/title_img/2510.01051.jpg', 'data': {'categories': ['#rl', '#open_source', '#benchmark', '#training', '#games', '#agents'], 'emoji': 'ğŸ®', 'ru': {'title': 'GEM: ÑĞ¿Ğ¾Ñ€Ñ‚Ğ·Ğ°Ğ» Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· reinforcement learning', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ GEM (General Experience Maker) â€” Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑÑ€ĞµĞ´Ğ°-ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼. Ğ­Ñ‚Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³ OpenAI Gym, Ğ½Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ ÑĞ¿Ğ¾Ñ…Ğ¸ LLM, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸ ÑÑ€ĞµĞ´Ğ¾Ğ¹ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. GEM Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 24 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° REINFORCE Ñ Return Batch Normalization (ReBN), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ credit assignment Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ GRPO. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… RL-Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² (PPO, GRPO, REINFORCE) Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‚ GEM ĞºĞ°Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… LLM.'}, 'en': {'title': 'GEM: Empowering LLMs with Experience-Based Learning', 'desc': 'GEM (General Experience Maker) is an open-source simulator designed to enhance experience-based learning for large language models (LLMs) by providing a standardized framework for training and benchmarking reinforcement learning (RL) algorithms. It allows agents to learn by interacting with various complex environments, moving away from static datasets. GEM includes features like asynchronous vectorized execution for efficient processing and flexible wrappers for easy customization. Additionally, it offers a suite of environments and tools for evaluating different RL algorithms, aiming to accelerate research in agentic LLMs.'}, 'zh': {'title': 'GEMï¼šåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹çš„ç»éªŒå­¦ä¹ ', 'desc': 'GEMï¼ˆé€šç”¨ç»éªŒç”Ÿæˆå™¨ï¼‰æ˜¯ä¸€ä¸ªå¼€æºç¯å¢ƒæ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›åŸºäºç»éªŒçš„å­¦ä¹ ä½“éªŒã€‚å®ƒä¸ºå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„è®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•æä¾›äº†æ ‡å‡†åŒ–æ¡†æ¶å’Œå¤šæ ·åŒ–ç¯å¢ƒï¼Œç±»ä¼¼äºä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ä¸­çš„OpenAI-Gymã€‚GEMæ”¯æŒå¼‚æ­¥å‘é‡åŒ–æ‰§è¡Œï¼Œå…·æœ‰é«˜ååé‡ï¼Œå¹¶æä¾›çµæ´»çš„åŒ…è£…å™¨ä»¥ä¾¿äºæ‰©å±•ã€‚æ­¤å¤–ï¼ŒGEMè¿˜åŒ…å«å¤šç§ç¯å¢ƒã€å¼ºå¤§çš„é›†æˆå·¥å…·å’Œç¤ºä¾‹è„šæœ¬ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜åŠ é€Ÿæœªæ¥çš„æ™ºèƒ½ä½“è¯­è¨€æ¨¡å‹ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00406', 'title': 'VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified\n  Rewards in World Simulators', 'url': 'https://huggingface.co/papers/2510.00406', 'abstract': 'VLA-RFT uses a data-driven world model to fine-tune VLA models efficiently, reducing sample requirements and improving robustness under perturbations.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to https://vla-rft.github.io/.', 'score': 30, 'issue_id': 6198, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '8dead5a43b0cdc61', 'authors': ['Hengtao Li', 'Pengxiang Ding', 'Runze Suo', 'Yihao Wang', 'Zirui Ge', 'Dongyuan Zang', 'Kexian Yu', 'Mingyang Sun', 'Hongyin Zhang', 'Donglin Wang', 'Weihua Su'], 'affiliations': ['BUPT', 'Fudan University', 'Hebei University of Technology', 'OpenHelix Team', 'Westlake University', 'Zhejiang University', 'Zhengzhou University'], 'pdf_title_img': 'assets/pdf/title_img/2510.00406.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#optimization', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¸Ñ€-ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VLA-RFT â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¸Ñ€Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ data-driven world model, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼ĞµĞ½ĞµĞµ 400 ÑˆĞ°Ğ³Ğ¾Ğ² fine-tuning Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ supervised baseline, Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑĞ¼. VLA-RFT Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² imitation learning Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº.'}, 'en': {'title': 'Enhancing VLA Models with Efficient Reinforcement Fine-Tuning', 'desc': "VLA-RFT is a framework that improves Vision-Language-Action (VLA) models by using a data-driven world model for reinforcement fine-tuning. This approach reduces the number of samples needed for training and enhances the model's ability to handle unexpected changes in the environment. By simulating future visual observations based on actions, it provides a more effective learning signal that aligns with the desired outcomes. The results show that VLA-RFT not only outperforms traditional supervised methods but also maintains strong performance even when conditions are altered."}, 'zh': {'title': 'åˆ©ç”¨ä¸–ç•Œæ¨¡å‹æå‡VLAæ¨¡å‹çš„é²æ£’æ€§ä¸æ•ˆç‡', 'desc': 'VLA-RFTæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ¡†æ¶ï¼Œåˆ©ç”¨æ•°æ®é©±åŠ¨çš„ä¸–ç•Œæ¨¡å‹ä½œä¸ºå¯æ§æ¨¡æ‹Ÿå™¨ï¼Œä»è€Œæé«˜VLAæ¨¡å‹çš„æ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡çœŸå®äº¤äº’æ•°æ®è®­ç»ƒï¼Œèƒ½å¤Ÿé¢„æµ‹åŸºäºåŠ¨ä½œçš„æœªæ¥è§†è§‰è§‚å¯Ÿï¼Œæä¾›å¯†é›†çš„è½¨è¿¹çº§å¥–åŠ±ä¿¡å·ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒVLA-RFTåœ¨æ ·æœ¬éœ€æ±‚ä¸Šå¤§å¹…é™ä½ï¼Œä¸”åœ¨å°‘äº400æ­¥çš„å¾®è°ƒåè¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ã€‚å®ƒåœ¨æ‰°åŠ¨æ¡ä»¶ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ï¼Œç¡®ä¿ä»»åŠ¡æ‰§è¡Œçš„ç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25849', 'title': 'Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget\n  Allocation', 'url': 'https://huggingface.co/papers/2509.25849', 'abstract': 'An adaptive exploration budget allocation method for reinforcement learning in Large Language Models improves training efficiency and performance on mathematical reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can self-improve through reinforcement learning, where they generate trajectories to explore and discover better solutions. However, this exploration process is computationally expensive, often forcing current methods to assign limited exploration budgets to each task. This uniform allocation creates problematic edge cases: easy tasks consistently succeed while difficult tasks consistently fail, both producing zero gradients during training updates for the widely used Group Relative Policy Optimization (GRPO). We address this problem from the lens of exploration budget allocation. Viewing each task\'s exploration as an "item" with a distinct "value" and "cost", we establish a connection to the classical knapsack problem. This formulation allows us to derive an optimal assignment rule that adaptively distributes resources based on the model\'s current learning status. When applied to GRPO, our method increases the effective ratio of non-zero policy gradients by 20-40% during training. Acting as a computational "free lunch", our approach could reallocate exploration budgets from tasks where learning is saturated to those where it is most impactful. This enables significantly larger budgets (e.g., 93 rollouts) for especially challenging problems, which would be computationally prohibitive under a uniform allocation. These improvements translate to meaningful gains on mathematical reasoning benchmarks, with average improvements of 2-4 points and peak gains of 9 points on specific tasks. Notably, achieving comparable performance with traditional homogeneous allocation would require about 2x the computational resources.', 'score': 23, 'issue_id': 6201, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '7897a4d3a9007e61', 'authors': ['Ziniu Li', 'Congliang Chen', 'Tianyun Yang', 'Tian Ding', 'Ruoyu Sun', 'Ge Zhang', 'Wenhao Huang', 'Zhi-Quan Luo'], 'affiliations': ['ByteDance Seed', 'Shenzhen Research Institute of Big Data', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2509.25849.jpg', 'data': {'categories': ['#math', '#reasoning', '#optimization', '#rl', '#training'], 'emoji': 'ğŸ’', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹: Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ½Ğ° Ğ²ÑĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¸Ñ… Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº, ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞµ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ ĞºĞ°Ğº ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¾ Ñ€ÑĞºĞ·Ğ°ĞºĞµ, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¸Ğ¼ĞµĞµÑ‚ ÑĞ²Ğ¾Ñ Â«Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÂ» Ğ¸ Â«ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÂ». ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ñ Ğ½ĞµĞ½ÑƒĞ»ĞµĞ²Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° 20-40% Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ½Ğ° 2-9 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ‚ĞµÑ… Ğ¶Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Smart Budgeting for Smarter Learning', 'desc': 'This paper presents a new method for allocating exploration budgets in reinforcement learning, specifically for Large Language Models (LLMs). The authors identify that traditional uniform budget allocation leads to inefficiencies, where easy tasks succeed while difficult ones fail, resulting in zero gradients during training. By framing the exploration budget allocation as a knapsack problem, they develop an adaptive strategy that optimally distributes resources based on the learning status of each task. This approach significantly enhances training efficiency, increasing non-zero policy gradients by 20-40% and improving performance on mathematical reasoning tasks without requiring additional computational resources.'}, 'zh': {'title': 'è‡ªé€‚åº”æ¢ç´¢é¢„ç®—ï¼Œæå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„æ¢ç´¢é¢„ç®—åˆ†é…æ–¹æ³•ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œæ•°å­¦æ¨ç†åŸºå‡†çš„æ€§èƒ½ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨æ¯ä¸ªä»»åŠ¡ä¸Šå‡åŒ€åˆ†é…æœ‰é™çš„æ¢ç´¢é¢„ç®—ï¼Œå¯¼è‡´ç®€å•ä»»åŠ¡æ€»æ˜¯æˆåŠŸè€Œå›°éš¾ä»»åŠ¡æ€»æ˜¯å¤±è´¥ï¼Œé€ æˆè®­ç»ƒæ›´æ–°æ—¶æ¢¯åº¦ä¸ºé›¶çš„é—®é¢˜ã€‚æˆ‘ä»¬å°†æ¯ä¸ªä»»åŠ¡çš„æ¢ç´¢è§†ä¸ºå…·æœ‰ä¸åŒâ€œä»·å€¼â€å’Œâ€œæˆæœ¬â€çš„â€œç‰©å“â€ï¼Œå¹¶ä¸ç»å…¸çš„èƒŒåŒ…é—®é¢˜å»ºç«‹è”ç³»ï¼Œä»è€Œæ¨å¯¼å‡ºä¸€ç§æœ€ä½³åˆ†é…è§„åˆ™ã€‚é€šè¿‡å°†èµ„æºåŠ¨æ€åˆ†é…åˆ°å­¦ä¹ æ•ˆæœæœ€æ˜¾è‘—çš„ä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®­ç»ƒä¸­æœ‰æ•ˆåœ°æé«˜äº†éé›¶ç­–ç•¥æ¢¯åº¦çš„æ¯”ä¾‹ï¼Œå¹¶åœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01174', 'title': 'Code2Video: A Code-centric Paradigm for Educational Video Generation', 'url': 'https://huggingface.co/papers/2510.01174', 'abstract': 'Code2Video generates educational videos using a code-centric agent framework, improving coherence and interpretability compared to direct code generation.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at https://github.com/showlab/Code2Video.', 'score': 18, 'issue_id': 6198, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '26c2c9dd6c370251', 'authors': ['Yanzhe Chen', 'Kevin Qinghong Lin', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2510.01174.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#video', '#games', '#optimization', '#agents', '#dataset'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´', 'desc': 'Code2Video â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ Python-ĞºĞ¾Ğ´. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ… ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ»ĞµĞºÑ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ´ Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºÑƒ. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° TeachQuiz, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ LLM Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 40% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Code2Video: Crafting Coherent Educational Videos with Code', 'desc': 'Code2Video is a framework designed to create educational videos using a code-centric approach, which enhances coherence and interpretability compared to traditional methods. It consists of three main agents: the Planner organizes content into logical sequences, the Coder translates these sequences into executable Python code, and the Critic refines the visual layout using vision-language models. This method addresses the challenges of generating professional educational videos by allowing precise control over visual elements and transitions. The framework has shown a significant improvement in video quality and coherence, outperforming direct code generation by 40%.'}, 'zh': {'title': 'Code2Videoï¼šæ•™è‚²è§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'Code2Video æ˜¯ä¸€ä¸ªåŸºäºä»£ç çš„ä»£ç†æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆæ•™è‚²è§†é¢‘ï¼Œæå‡äº†è§†é¢‘çš„ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªåä½œä»£ç†ï¼šè§„åˆ’è€…è´Ÿè´£å°†è®²åº§å†…å®¹ç»“æ„åŒ–å¹¶å‡†å¤‡è§†è§‰èµ„äº§ï¼›ç¼–ç å™¨å°†ç»“æ„åŒ–æŒ‡ä»¤è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„ Python ä»£ç ï¼Œå¹¶é€šè¿‡èŒƒå›´å¼•å¯¼è‡ªåŠ¨ä¿®å¤æé«˜æ•ˆç‡ï¼›è¯„è®ºè€…åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä¼˜åŒ–ç©ºé—´å¸ƒå±€ï¼Œç¡®ä¿æ¸…æ™°åº¦ã€‚é€šè¿‡å»ºç«‹ä¸“ä¸šåˆ¶ä½œçš„æ•™è‚²è§†é¢‘åŸºå‡†MMMCï¼Œæˆ‘ä»¬è¯„ä¼°äº†Code2Videoåœ¨ç¾å­¦è¯„åˆ†ã€ä»£ç æ•ˆç‡å’ŒçŸ¥è¯†æ¢å¤ç­‰å¤šä¸ªç»´åº¦çš„è¡¨ç°ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨è§†é¢‘ç”Ÿæˆä¸Šä¼˜äºç›´æ¥ä»£ç ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00615', 'title': 'ACON: Optimizing Context Compression for Long-horizon LLM Agents', 'url': 'https://huggingface.co/papers/2510.00615', 'abstract': 'Agent Context Optimization (ACON) compresses context in large language models for efficient long-horizon tasks by analyzing failure cases and distilling the compressor into smaller models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement.', 'score': 14, 'issue_id': 6203, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'a32bcdd471ec0b1c', 'authors': ['Minki Kang', 'Wei-Ning Chen', 'Dongge Han', 'Huseyin A. Inan', 'Lukas Wutschitz', 'Yanzhi Chen', 'Robert Sim', 'Saravan Rajmohan'], 'affiliations': ['KAIST', 'Microsoft', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2510.00615.jpg', 'data': {'categories': ['#agents', '#small_models', '#optimization', '#long_context', '#inference', '#dataset'], 'emoji': 'ğŸ—œï¸', 'ru': {'title': 'ACON: Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Agent Context Optimization (ACON) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… ĞºĞ°Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¸, ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ¶Ğ°Ñ‚Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼, Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ğ¸ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… LLM. ACON Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° 26-54% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ 46%.'}, 'en': {'title': 'Efficient Context Compression for Long-Horizon Tasks', 'desc': 'Agent Context Optimization (ACON) is a method designed to improve the efficiency of large language models (LLMs) when handling long-term tasks by compressing the context they use. It identifies and analyzes failure cases where compressed context leads to poor performance, allowing the model to learn and refine its compression strategies. ACON not only optimizes the way observations and action histories are condensed but also distills this knowledge into smaller models, making them more efficient. Experiments demonstrate that ACON significantly reduces memory usage while maintaining high accuracy, thus enhancing the performance of smaller models in long-horizon tasks.'}, 'zh': {'title': 'é«˜æ•ˆå‹ç¼©ï¼Œæå‡é•¿æ—¶é—´ä»»åŠ¡è¡¨ç°çš„ä»£ç†ä¸Šä¸‹æ–‡ä¼˜åŒ–', 'desc': 'ä»£ç†ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼ˆACONï¼‰æ˜¯ä¸€ç§å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä¸Šä¸‹æ–‡çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜é•¿æ—¶é—´ä»»åŠ¡çš„æ•ˆç‡ã€‚é€šè¿‡åˆ†æå¤±è´¥æ¡ˆä¾‹ï¼ŒACONèƒ½å¤Ÿæç‚¼å‡ºæ›´å°çš„æ¨¡å‹ï¼Œä»è€Œä¼˜åŒ–ç¯å¢ƒè§‚å¯Ÿå’Œäº¤äº’å†å²çš„å‹ç¼©ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è‡ªç„¶è¯­è¨€ç©ºé—´ä¸­çš„å‹ç¼©æŒ‡å¯¼ä¼˜åŒ–ï¼Œç¡®ä¿åœ¨æˆåŠŸçš„å®Œæ•´ä¸Šä¸‹æ–‡å’Œå¤±è´¥çš„å‹ç¼©ä¸Šä¸‹æ–‡ä¹‹é—´è¿›è¡Œæœ‰æ•ˆåˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒACONåœ¨å‡å°‘å†…å­˜ä½¿ç”¨çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒä»»åŠ¡æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æå‡å°å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25455', 'title': 'PIPer: On-Device Environment Setup via Online Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.25455', 'abstract': 'A specialized model combining supervised fine-tuning and Reinforcement Learning with Verifiable Rewards achieves competitive performance in automated environment setup tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Environment setup-the process of configuring the system to work with a specific software project-represents a persistent challenge in Software Engineering (SE). Automated environment setup methods could assist developers by providing fully configured environments for arbitrary repositories without manual effort. This also helps SE researchers to scale execution-based benchmarks. However, recent studies reveal that even state-of-the-art Large Language Models (LLMs) achieve limited success in automating this task. To address this limitation, we tune a specialized model for environment setup. We combine supervised fine-tuning for generating correct Bash scripts and Reinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task of environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model runnable on consumer hardware) to perform on par with larger models-Qwen3-32B and GPT-4o. The training code and model checkpoints are available online: https://github.com/JetBrains-Research/PIPer.', 'score': 12, 'issue_id': 6205, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '08f51f097cf06715', 'authors': ['Alexander Kovrigin', 'Aleksandra Eliseeva', 'Konstantin Grotov', 'Egor Bogomolov', 'Yaroslav Zharov'], 'affiliations': ['Constructor University', 'Delft University of Technology', 'JetBrains Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.25455.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#open_source', '#small_models'], 'emoji': 'âš™ï¸', 'ru': {'title': 'Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ - ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ supervised fine-tuning Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Bash-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Reinforcement Learning with Verifiable Rewards (RLVR) Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ setup Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen3-8B, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ°Ñ Ğ½Ğ° Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ hardware, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¸Ğ¿Ğ° GPT-4o. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ EnvBench-Python Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Automating Environment Setup with Specialized ML Models', 'desc': 'This paper presents a specialized model that enhances automated environment setup tasks in software engineering by combining supervised fine-tuning and Reinforcement Learning with Verifiable Rewards (RLVR). The model is designed to generate accurate Bash scripts, addressing the limitations of existing Large Language Models (LLMs) in this domain. By utilizing RLVR, the model adapts effectively to the specific requirements of environment setup, achieving competitive performance on the EnvBench-Python benchmark. Notably, the Qwen3-8B model, which can run on consumer hardware, matches the performance of larger models like Qwen3-32B and GPT-4o.'}, 'zh': {'title': 'ä¸“é—¨æ¨¡å‹åŠ©åŠ›è‡ªåŠ¨åŒ–ç¯å¢ƒé…ç½®', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸“é—¨çš„æ¨¡å‹ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼ŒæˆåŠŸè§£å†³äº†è‡ªåŠ¨åŒ–ç¯å¢ƒé…ç½®ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚ç¯å¢ƒé…ç½®æ˜¯è½¯ä»¶å·¥ç¨‹ä¸­çš„ä¸€ä¸ªé‡è¦ç¯èŠ‚ï¼Œè‡ªåŠ¨åŒ–æ–¹æ³•å¯ä»¥å¸®åŠ©å¼€å‘è€…å¿«é€Ÿé…ç½®ç¯å¢ƒï¼Œå‡å°‘æ‰‹åŠ¨æ“ä½œã€‚å°½ç®¡ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šè¡¨ç°æœ‰é™ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•ä½¿å¾—Qwen3-8Bæ¨¡å‹åœ¨EnvBench-Pythonä¸Šä¸æ›´å¤§æ¨¡å‹çš„è¡¨ç°ç›¸å½“ã€‚è¯¥ç ”ç©¶çš„ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å·²åœ¨çº¿å‘å¸ƒï¼Œä¾›ç ”ç©¶è€…ä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00184', 'title': "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls", 'url': 'https://huggingface.co/papers/2510.00184', 'abstract': "Reverse-engineering a model that learns multi-digit multiplication via implicit chain-of-thought reveals that it uses attention to encode long-range dependencies and represents partial products efficiently, insights that help address limitations in standard fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models are increasingly capable, yet still fail at a seemingly simple task of multi-digit multiplication. In this work, we study why, by reverse-engineering a model that successfully learns multiplication via implicit chain-of-thought, and report three findings: (1) Evidence of long-range structure: Logit attributions and linear probes indicate that the model encodes the necessary long-range dependencies for multi-digit multiplication. (2) Mechanism: the model encodes long-range dependencies using attention to construct a directed acyclic graph to ``cache'' and ``retrieve'' pairwise partial products. (3) Geometry: the model implements partial products in attention heads by forming Minkowski sums between pairs of digits, and digits are represented using a Fourier basis, both of which are intuitive and efficient representations that the standard fine-tuning model lacks. With these insights, we revisit the learning dynamics of standard fine-tuning and find that the model converges to a local optimum that lacks the required long-range dependencies. We further validate this understanding by introducing an auxiliary loss that predicts the ``running sum'' via a linear regression probe, which provides an inductive bias that enables the model to successfully learn multi-digit multiplication. In summary, by reverse-engineering the mechanisms of an implicit chain-of-thought model we uncover a pitfall for learning long-range dependencies in Transformers and provide an example of how the correct inductive bias can address this issue.", 'score': 11, 'issue_id': 6198, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '27d7ce536d31aa04', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#training', '#architecture', '#long_context', '#reasoning'], 'emoji': 'ğŸ”¢', 'ru': {'title': 'ĞšĞ°Ğº Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ ÑƒĞ¼Ğ½Ğ¾Ğ¶Ğ°Ñ‚ÑŒ: Ñ€Ğ°Ğ·Ğ³Ğ°Ğ´ĞºĞ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°ÑƒÑ‡Ğ¸Ğ»Ğ°ÑÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾Ğ¼Ñƒ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµÑĞ²Ğ½ÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (chain-of-thought). ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ attention Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ°, ĞºÑÑˆĞ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‡Ğ¸ÑĞ»Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ°Ğ·Ğ¸Ñ Ğ¤ÑƒÑ€ÑŒĞµ Ğ¸ ÑÑƒĞ¼Ğ¼Ñ‹ ĞœĞ¸Ğ½ĞºĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾. Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ fine-tuning Ğ·Ğ°ÑÑ‚Ñ€ĞµĞ²Ğ°ĞµÑ‚ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼ÑƒĞ¼Ğµ Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ»Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ñ€ÑĞ´Ğ°Ğ¼Ğ¸ Ñ‡Ğ¸ÑĞµĞ». Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑƒĞ¼Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾ÑĞ²Ğ¾Ğ¸Ñ‚ÑŒ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Unlocking Multi-Digit Multiplication with Attention and Inductive Bias', 'desc': "This paper investigates how a model learns to perform multi-digit multiplication using an implicit chain-of-thought approach. It reveals that the model effectively encodes long-range dependencies through attention mechanisms, allowing it to manage partial products efficiently. The authors demonstrate that standard fine-tuning methods often fail to capture these dependencies, leading to suboptimal performance. By introducing an auxiliary loss that predicts running sums, they provide a solution to enhance learning dynamics and improve the model's ability to handle complex multiplication tasks."}, 'zh': {'title': 'æ­ç¤ºå¤šä½æ•°ä¹˜æ³•å­¦ä¹ çš„å…³é”®æœºåˆ¶', 'desc': 'æœ¬ç ”ç©¶é€šè¿‡é€†å‘å·¥ç¨‹ä¸€ä¸ªæˆåŠŸå­¦ä¹ å¤šä½æ•°ä¹˜æ³•çš„æ¨¡å‹ï¼Œæ­ç¤ºäº†å…¶ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ç¼–ç é•¿è·ç¦»ä¾èµ–å…³ç³»çš„æ–¹å¼ã€‚ç ”ç©¶å‘ç°ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ„å»ºæœ‰å‘æ— ç¯å›¾æ¥ç¼“å­˜å’Œæ£€ç´¢æˆå¯¹çš„éƒ¨åˆ†ç§¯ï¼Œä»è€Œæœ‰æ•ˆåœ°è¡¨ç¤ºéƒ¨åˆ†ç§¯ã€‚æ¨¡å‹åœ¨æ³¨æ„åŠ›å¤´ä¸­é€šè¿‡å½¢æˆé—µå¯å¤«æ–¯åŸºå’Œæ¥å®ç°éƒ¨åˆ†ç§¯ï¼Œå¹¶ä½¿ç”¨å‚…é‡Œå¶åŸºè¡¨ç¤ºæ•°å­—ï¼Œè¿™äº›éƒ½æ˜¯æ ‡å‡†å¾®è°ƒæ¨¡å‹æ‰€ç¼ºä¹çš„ç›´è§‚ä¸”é«˜æ•ˆçš„è¡¨ç¤ºæ–¹å¼ã€‚é€šè¿‡å¼•å…¥è¾…åŠ©æŸå¤±æ¥é¢„æµ‹â€œè¿è¡Œå’Œâ€ï¼Œæˆ‘ä»¬ä¸ºæ¨¡å‹æä¾›äº†ä¸€ä¸ªå½’çº³åç½®ï¼Œä½¿å…¶èƒ½å¤ŸæˆåŠŸå­¦ä¹ å¤šä½æ•°ä¹˜æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00232', 'title': 'BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model\n  Responses', 'url': 'https://huggingface.co/papers/2510.00232', 'abstract': "BiasFreeBench evaluates bias mitigation techniques in large language models using a unified benchmark and response-level metric to ensure fair and safe outputs in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing studies on bias mitigation methods for large language models (LLMs) use diverse baselines and metrics to evaluate debiasing performance, leading to inconsistent comparisons among them. Moreover, their evaluations are mostly based on the comparison between LLMs' probabilities of biased and unbiased contexts, which ignores the gap between such evaluations and real-world use cases where users interact with LLMs by reading model responses and expect fair and safe outputs rather than LLMs' probabilities. To enable consistent evaluation across debiasing methods and bridge this gap, we introduce BiasFreeBench, an empirical benchmark that comprehensively compares eight mainstream bias mitigation techniques (covering four prompting-based and four training-based methods) on two test scenarios (multi-choice QA and open-ended multi-turn QA) by reorganizing existing datasets into a unified query-response setting. We further introduce a response-level metric, Bias-Free Score, to measure the extent to which LLM responses are fair, safe, and anti-stereotypical. Debiasing performances are systematically compared and analyzed across key dimensions: the prompting vs. training paradigm, model size, and generalization of different training strategies to unseen bias types. We will publicly release our benchmark, aiming to establish a unified testbed for bias mitigation research.", 'score': 10, 'issue_id': 6198, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '7a6ed8974cc83369', 'authors': ['Xin Xu', 'Xunzhi He', 'Churan Zhi', 'Ruizhe Chen', 'Julian McAuley', 'Zexue He'], 'affiliations': ['Columbia University', 'MIT-IBM Watson Lab', 'UC San Diego', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.00232.jpg', 'data': {'categories': ['#ethics', '#dataset', '#benchmark'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ñ‡ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒÑ Ğ² LLM', 'desc': 'Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑĞµÑ‚ Ğ¸Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ BiasFreeBench â€” ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ bias (Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ) Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Bias-Free Score, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ°Ğ½Ñ‚Ğ¸ÑÑ‚ĞµÑ€ĞµĞ¾Ñ‚Ğ¸Ğ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² LLM. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ (Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ vs Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ), Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unifying Bias Mitigation Evaluation for Safer AI Outputs', 'desc': 'BiasFreeBench is a new benchmark designed to evaluate bias mitigation techniques in large language models (LLMs). It addresses the inconsistency in previous studies by providing a unified framework for comparing various debiasing methods. The benchmark includes a novel response-level metric called Bias-Free Score, which assesses the fairness and safety of model outputs in real-world scenarios. By systematically analyzing different debiasing strategies, BiasFreeBench aims to enhance the reliability of LLMs in producing equitable and safe responses.'}, 'zh': {'title': 'ç»Ÿä¸€è¯„ä¼°åè§ç¼“è§£æŠ€æœ¯çš„åŸºå‡†å·¥å…·', 'desc': 'BiasFreeBench æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åè§ç¼“è§£æŠ€æœ¯çš„åŸºå‡†å·¥å…·ï¼Œæ—¨åœ¨ç¡®ä¿æ¨¡å‹è¾“å‡ºåœ¨ç°å®åœºæ™¯ä¸­å…¬å¹³å’Œå®‰å…¨ã€‚è¯¥ç ”ç©¶é€šè¿‡ç»Ÿä¸€çš„æŸ¥è¯¢-å“åº”è®¾ç½®ï¼Œæ¯”è¾ƒäº†å…«ç§ä¸»æµçš„åè§ç¼“è§£æ–¹æ³•ï¼ŒåŒ…æ‹¬å››ç§åŸºäºæç¤ºå’Œå››ç§åŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å“åº”çº§åˆ«æŒ‡æ ‡â€”â€”æ— ååˆ†æ•°ï¼Œæ¥è¡¡é‡æ¨¡å‹å“åº”çš„å…¬å¹³æ€§ã€å®‰å…¨æ€§å’Œååˆ»æ¿å°è±¡ç¨‹åº¦ã€‚è¯¥åŸºå‡†çš„å‘å¸ƒå°†ä¸ºåè§ç¼“è§£ç ”ç©¶æä¾›ä¸€ä¸ªç»Ÿä¸€çš„æµ‹è¯•å¹³å°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25301', 'title': 'Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel\n  Execution', 'url': 'https://huggingface.co/papers/2509.25301', 'abstract': 'Flash-Searcher, a parallel agent reasoning framework using directed acyclic graphs, enhances efficiency and performance in complex reasoning tasks by enabling concurrent execution and dynamic workflow optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks when equipped with external tools. However, current frameworks predominantly rely on sequential processing, leading to inefficient execution particularly for tasks requiring extensive tool interaction. This paper introduces Flash-Searcher, a novel parallel agent reasoning framework that fundamentally reimagines the execution paradigm from sequential chains to directed acyclic graphs (DAGs). Flash-Searcher decomposes complex tasks into subtasks with explicit dependencies, enabling concurrent execution of independent reasoning paths while maintaining logical constraints. Through dynamic workflow optimization, our framework continuously refines the execution graph based on intermediate results, effectively integrating summary module. Comprehensive evaluations across multiple benchmarks demonstrate that Flash-Searcher consistently outperforms existing approaches. Specifically, it achieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, while reducing agent execution steps by up to 35% compared to current frameworks. Furthermore, when distilling this parallel reasoning pipeline into single models, we observe substantial performance gains across diverse backbone architectures, underscoring the generalizability of our methodology. Our work thus represents a significant advance in agent architecture design, offering a more scalable and efficient paradigm for complex reasoning tasks.', 'score': 8, 'issue_id': 6200, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '6daa8de407c2b924', 'authors': ['Tianrui Qin', 'Qianben Chen', 'Sinuo Wang', 'He Xing', 'King Zhu', 'He Zhu', 'Dingfeng Shi', 'Xinxin Liu', 'Ge Zhang', 'Jiaheng Liu', 'Yuchen Eleanor Jiang', 'Xitong Gao', 'Wangchunshu Zhou'], 'affiliations': ['OPPO', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2509.25301.jpg', 'data': {'categories': ['#architecture', '#optimization', '#reasoning', '#benchmark', '#agents'], 'emoji': 'âš¡', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Flash-Searcher â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ñ€Ğ°Ñ„Ğ°Ñ… (DAG) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²ÑĞ·ĞµĞ¹. Flash-Searcher Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ 67.7% Ğ½Ğ° BrowseComp Ğ¸ 83% Ğ½Ğ° xbench-DeepSearch, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ° 35% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ….'}, 'en': {'title': 'Revolutionizing Reasoning with Parallel Execution', 'desc': 'Flash-Searcher is a new framework designed to improve the efficiency of reasoning tasks in artificial intelligence. It uses directed acyclic graphs (DAGs) to allow multiple reasoning paths to be executed at the same time, rather than one after the other. This approach not only speeds up the process but also optimizes the workflow dynamically based on the results obtained during execution. The framework has shown significant performance improvements in various benchmarks, demonstrating its effectiveness over traditional sequential processing methods.'}, 'zh': {'title': 'Flash-Searcherï¼šé«˜æ•ˆçš„å¹¶è¡Œæ¨ç†æ¡†æ¶', 'desc': 'Flash-Searcher æ˜¯ä¸€ç§æ–°çš„å¹¶è¡Œæ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ï¼Œä½¿ç”¨æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰æ¥æé«˜å¤æ‚æ¨ç†ä»»åŠ¡çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚å®ƒé€šè¿‡å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå…·æœ‰æ˜ç¡®ä¾èµ–å…³ç³»çš„å­ä»»åŠ¡ï¼Œå…è®¸ç‹¬ç«‹æ¨ç†è·¯å¾„çš„å¹¶å‘æ‰§è¡Œï¼ŒåŒæ—¶ä¿æŒé€»è¾‘çº¦æŸã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€å·¥ä½œæµä¼˜åŒ–ï¼ŒåŸºäºä¸­é—´ç»“æœä¸æ–­æ”¹è¿›æ‰§è¡Œå›¾ï¼Œæœ‰æ•ˆæ•´åˆäº†æ‘˜è¦æ¨¡å—ã€‚ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼ŒFlash-Searcher åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01180', 'title': 'BroRL: Scaling Reinforcement Learning via Broadened Exploration', 'url': 'https://huggingface.co/papers/2510.01180', 'abstract': 'BroRL enhances reinforcement learning by increasing rollouts per example, overcoming performance plateaus and achieving state-of-the-art results in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key ingredient for unlocking complex reasoning capabilities in large language models. Recent work ProRL has shown promise in scaling RL by increasing the number of training steps. However, performance plateaus after thousands of steps, with clear diminishing returns from allocating more computation to additional training. In this work, we investigate a complementary paradigm for scaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to exhaustively Broaden exploration, which yields continuous performance gains beyond the saturation point observed in ProRL when scaling the number of training steps. Our approach is motivated by a mass balance equation analysis allowing us to characterize the rate of change in probability mass for correct and incorrect tokens during the reinforcement process. We show that under a one-step RL assumption, sampled rollout tokens always contribute to correct-mass expansion, while unsampled tokens outside rollouts may lead to gains or losses depending on their distribution and the net reward balance. Importantly, as the number of rollouts per example N increases, the effect of unsampled terms diminishes, ensuring overall correct-mass expansion. To validate our theoretical analysis, we conduct simulations under more relaxed conditions and find that a sufficiently large rollout size N-corresponding to ample exploration-guarantees an increase in the probability mass of all correct tokens. Empirically, BroRL revives models saturated after 3K ProRL training steps and demonstrates robust, continuous improvement, achieving state-of-the-art results for the 1.5B model across diverse benchmarks.', 'score': 7, 'issue_id': 6202, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '2491de2a9eaf3c36', 'authors': ['Jian Hu', 'Mingjie Liu', 'Ximing Lu', 'Fang Wu', 'Zaid Harchaoui', 'Shizhe Diao', 'Yejin Choi', 'Pavlo Molchanov', 'Jun Yang', 'Jan Kautz', 'Yi Dong'], 'affiliations': ['NVIDIA', 'Stanford University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2510.01180.jpg', 'data': {'categories': ['#rl', '#benchmark', '#reasoning', '#optimization', '#training'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ñ€Ğ¾Ğ»Ğ»Ğ°ÑƒÑ‚Ğ¾Ğ² â€” Ğ»ÑƒÑ‡ÑˆĞµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ RL Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BroRL â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ reinforcement learning Ğ´Ğ»Ñ LLM Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ‡Ğ¸ÑĞ»Ğ° Ñ€Ğ¾Ğ»Ğ»Ğ°ÑƒÑ‚Ğ¾Ğ² (Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹) Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ°ÑÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ Ñ€Ğ¾Ğ»Ğ»Ğ°ÑƒÑ‚Ğ¾Ğ² Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ€Ğ¾ÑÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑÑÑ‹ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¿Ğ»Ğ°Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ ProRL Ğ¿Ğ¾ÑĞ»Ğµ Ñ‚Ñ‹ÑÑÑ‡ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. BroRL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 1.5B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ°Ğ¼, Ğ³Ğ´Ğµ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ°ÑÑ‹Ñ‰Ğ°ÑÑ‚ÑÑ.'}, 'en': {'title': 'Broaden Exploration for Continuous Gains in RL', 'desc': 'BroRL is a novel approach in reinforcement learning that enhances the training process by increasing the number of rollouts per example, which allows for broader exploration of the solution space. This method addresses the issue of performance plateaus that occur when traditional training steps reach diminishing returns. By analyzing the probability mass of correct and incorrect tokens, BroRL ensures that as the number of rollouts increases, the overall performance improves continuously. The empirical results show that BroRL outperforms previous methods, achieving state-of-the-art results in large language models after extensive training.'}, 'zh': {'title': 'BroRLï¼šçªç ´å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ç“¶é¢ˆ', 'desc': 'BroRLæ˜¯ä¸€ç§å¢å¼ºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡å¢åŠ æ¯ä¸ªç¤ºä¾‹çš„å›åˆæ•°æ¥å…‹æœæ€§èƒ½å¹³å°æœŸã€‚å®ƒèƒ½å¤Ÿåœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­å®ç°æŒç»­çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ProRLçš„é™åˆ¶ã€‚é€šè¿‡å¯¹æ¦‚ç‡è´¨é‡å˜åŒ–çš„åˆ†æï¼ŒBroRLç¡®ä¿äº†æ­£ç¡®æ ‡è®°çš„æ¦‚ç‡è´¨é‡ä¸æ–­æ‰©å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBroRLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œå°¤å…¶æ˜¯åœ¨ç»è¿‡3000æ­¥ProRLè®­ç»ƒåï¼Œæ¨¡å‹çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æ¢å¤ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00526', 'title': 'Beyond Log Likelihood: Probability-Based Objectives for Supervised\n  Fine-Tuning across the Model Capability Continuum', 'url': 'https://huggingface.co/papers/2510.00526', 'abstract': 'Research identifies probability-based objectives that outperform negative log likelihood for fine-tuning large language models, depending on model capability.  \t\t\t\t\tAI-generated summary \t\t\t\t Supervised fine-tuning (SFT) is the standard approach for post-training large language models (LLMs), yet it often shows limited generalization. We trace this limitation to its default training objective: negative log likelihood (NLL). While NLL is classically optimal when training from scratch, post-training operates in a different paradigm and could violate its optimality assumptions, where models already encode task-relevant priors and supervision can be long and noisy. To this end, we study a general family of probability-based objectives and characterize their effectiveness under different conditions. Through comprehensive experiments and extensive ablation studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a critical dimension that governs objective behavior: the model-capability continuum. Near the model-strong end, prior-leaning objectives that downweight low-probability tokens (e.g., -p, -p^{10}, thresholded variants) consistently outperform NLL; toward the model-weak end, NLL dominates; in between, no single objective prevails. Our theoretical analysis further elucidates how objectives trade places across the continuum, providing a principled foundation for adapting objectives to model capability. Our code is available at https://github.com/GaotangLi/Beyond-Log-Likelihood.', 'score': 7, 'issue_id': 6198, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '9f43fe314cbe69af', 'authors': ['Gaotang Li', 'Ruizhong Qiu', 'Xiusi Chen', 'Heng Ji', 'Hanghang Tong'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2510.00526.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ÑĞ¸Ğ»Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ negative log likelihood (NLL) Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ° Ğ´Ğ»Ñ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ capability Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ”Ğ»Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ²ĞµÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, -p Ğ¸Ğ»Ğ¸ -p^10), Ğ´Ğ»Ñ ÑĞ»Ğ°Ğ±Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ NLL. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 7 Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ…, 14 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ 3 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚Ğ¸Ğ½ÑƒÑƒĞ¼Ğ° model-capability, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ.'}, 'en': {'title': 'Optimizing Fine-Tuning: Beyond Negative Log Likelihood', 'desc': "This paper explores how different training objectives can improve the fine-tuning of large language models (LLMs) beyond the traditional negative log likelihood (NLL). It identifies that NLL may not be optimal for models that have already been pre-trained, as they possess inherent task-relevant knowledge. The authors propose a range of probability-based objectives that adapt to the model's capability, showing that stronger models benefit from objectives that prioritize high-probability tokens. Through extensive experiments, they demonstrate that the effectiveness of these objectives varies along a continuum of model strength, providing insights into how to select the best training objective based on model performance."}, 'zh': {'title': 'è¶…è¶Šè´Ÿå¯¹æ•°ä¼¼ç„¶çš„å¾®è°ƒç›®æ ‡', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºæ¦‚ç‡çš„ç›®æ ‡å‡½æ•°åœ¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶çš„è¡¨ç°ï¼Œå‘ç°å…¶åœ¨ä¸åŒæ¨¡å‹èƒ½åŠ›ä¸‹ä¼˜äºè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰ã€‚ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•å¸¸å¸¸å—é™äºNLLè¿™ä¸€è®­ç»ƒç›®æ ‡ï¼Œè€Œåœ¨åè®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹å·²ç»å…·å¤‡äº†ä»»åŠ¡ç›¸å…³çš„å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼Œæ­ç¤ºäº†æ¨¡å‹èƒ½åŠ›çš„è¿ç»­æ€§å¯¹ç›®æ ‡å‡½æ•°è¡¨ç°çš„å½±å“ã€‚åœ¨å¼ºæ¨¡å‹ç«¯ï¼Œå€¾å‘äºå…ˆéªŒçš„ç›®æ ‡å‡½æ•°è¡¨ç°ä¼˜äºNLLï¼Œè€Œåœ¨å¼±æ¨¡å‹ç«¯åˆ™æ˜¯NLLå ä¼˜ï¼Œæä¾›äº†æ ¹æ®æ¨¡å‹èƒ½åŠ›è°ƒæ•´ç›®æ ‡å‡½æ•°çš„ç†è®ºåŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00553', 'title': 'On Predictability of Reinforcement Learning Dynamics for Large Language\n  Models', 'url': 'https://huggingface.co/papers/2510.00553', 'abstract': 'Two fundamental properties of reinforcement learning-induced parameter updates in large language models are identified, leading to a plug-in acceleration framework that significantly speeds up training without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reasoning capabilities of large language models (LLMs) are largely driven by reinforcement learning (RL), yet the underlying parameter dynamics during RL training remain poorly understood. This work identifies two fundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1 Dominance, where the top singular subspace of the parameter update matrix nearly fully determines reasoning improvements, recovering over 99\\% of performance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace evolves linearly throughout training, enabling accurate prediction from early checkpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the generalizability of these properties. More importantly, based on these findings, we propose AlphaRL, a plug-in acceleration framework that extrapolates the final parameter update using a short early training window, achieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning performance without extra modules or hyperparameter tuning. This positions our finding as a versatile and practical tool for large-scale RL, opening a path toward principled, interpretable, and efficient training paradigm for LLMs.', 'score': 4, 'issue_id': 6198, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '75c2581875112809', 'authors': ['Yuchen Cai', 'Ding Cao', 'Xin Xu', 'Zijun Yao', 'Yuqing Huang', 'Zhenyu Tan', 'Benyi Zhang', 'Guiquan Liu', 'Junfeng Fang'], 'affiliations': ['HKUST', 'NUS', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2510.00553.jpg', 'data': {'categories': ['#training', '#rl', '#optimization', '#reasoning'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ´Ğ²Ğ° Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ reinforcement learning: Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ½Ğ³Ğ°-1 Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ³Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ²ĞµÑ€Ñ…Ğ½ĞµĞµ ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 99% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¾Ğº Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº AlphaRL, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾Ğµ Ğ¾ĞºĞ½Ğ¾ Ñ€Ğ°Ğ½Ğ½ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 2.5x Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ 96% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ğ¸Ğ»Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Accelerating RL Training in LLMs with AlphaRL', 'desc': 'This paper explores how reinforcement learning (RL) affects the training of large language models (LLMs) by identifying two key properties of parameter updates. The first property, Rank-1 Dominance, shows that a specific part of the parameter update matrix is crucial for improving reasoning capabilities, capturing over 99% of performance gains. The second property, Rank-1 Linear Dynamics, indicates that this important part changes in a predictable way during training, allowing for accurate predictions from early training stages. Based on these insights, the authors introduce AlphaRL, a framework that accelerates training by predicting final updates from early data, achieving significant speed improvements while maintaining high performance.'}, 'zh': {'title': 'åŠ é€Ÿå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æœ‰æ•ˆå·¥å…·', 'desc': 'æœ¬æ–‡è¯†åˆ«äº†å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å¼•èµ·çš„å‚æ•°æ›´æ–°çš„ä¸¤ä¸ªåŸºæœ¬ç‰¹æ€§ã€‚è¿™äº›ç‰¹æ€§åŒ…æ‹¬ï¼š1ï¼‰ç§©-1ä¸»å¯¼æ€§ï¼Œæ„å‘³ç€å‚æ•°æ›´æ–°çŸ©é˜µçš„ä¸»å¯¼å­ç©ºé—´å‡ ä¹å®Œå…¨å†³å®šäº†æ¨ç†çš„æ”¹è¿›ï¼›2ï¼‰ç§©-1çº¿æ€§åŠ¨æ€ï¼Œè¡¨æ˜è¿™ä¸ªä¸»å¯¼å­ç©ºé—´åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çº¿æ€§æ¼”å˜ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæå‡ºäº†AlphaRLåŠ é€Ÿæ¡†æ¶ï¼Œå¯ä»¥åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹æ˜¾è‘—åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’Œç®—æ³•ä¸­å…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00536', 'title': 'GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness', 'url': 'https://huggingface.co/papers/2510.00536', 'abstract': "GUI-KV, a KV cache compression method for GUI agents, improves efficiency by exploiting spatial and temporal redundancies, reducing computational cost while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical user interface (GUI) agents built on vision-language models have emerged as a promising approach to automate human-computer workflows. However, they also face the inefficiency challenge as they process long sequences of high-resolution screenshots and solving long-horizon tasks, making inference slow, costly and memory-bound. While key-value (KV) caching can mitigate this, storing the full cache is prohibitive for image-heavy contexts. Existing cache-compression methods are sub-optimal as they do not account for the spatial and temporal redundancy of GUIs. In this work, we first analyze attention patterns in GUI agent workloads and find that, unlike in natural images, attention sparsity is uniformly high across all transformer layers. This insight motivates a simple uniform budget allocation strategy, which we show empirically outperforms more complex layer-varying schemes. Building on this, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI agents that requires no retraining. GUI-KV combines two novel techniques: (i) spatial saliency guidance, which augments attention scores with the L2 norm of hidden states to better preserve semantically important visual tokens, and (ii) temporal redundancy scoring, which projects previous frames' keys onto the current frame's key subspace to preferentially prune redundant history. Across standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV compression baselines, closely matching full-cache accuracy at modest budgets. Notably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the full-cache baseline. These results demonstrate that exploiting GUI-specific redundancies enables efficient and reliable agent performance.", 'score': 4, 'issue_id': 6198, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '6687d7079b2d54e2', 'authors': ['Kung-Hsiang Huang', 'Haoyi Qiu', 'Yutong Dai', 'Caiming Xiong', 'Chien-Sheng Wu'], 'affiliations': ['Salesforce AI Research', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2510.00536.jpg', 'data': {'categories': ['#optimization', '#inference', '#agents', '#benchmark'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ GUI-KV â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ KV-ĞºÑÑˆĞ° Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· L2-Ğ½Ğ¾Ñ€Ğ¼Ñƒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ğ°Ğ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² GUI Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ¾ Ğ²Ğ¾ Ğ²ÑĞµÑ… ÑĞ»Ğ¾ÑÑ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ AgentNetBench Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° 38.9% Ğ¿Ñ€Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 4.1%, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Efficient GUI Agents with GUI-KV Cache Compression', 'desc': 'The paper presents GUI-KV, a method for compressing key-value (KV) caches specifically designed for graphical user interface (GUI) agents. It addresses the inefficiencies in processing high-resolution screenshots by leveraging spatial and temporal redundancies, which reduces computational costs while maintaining accuracy. The authors analyze attention patterns in GUI workloads and propose a uniform budget allocation strategy that outperforms more complex methods. By introducing techniques like spatial saliency guidance and temporal redundancy scoring, GUI-KV achieves significant improvements in efficiency and accuracy across various benchmarks.'}, 'zh': {'title': 'é«˜æ•ˆçš„GUIä»£ç†ç¼“å­˜å‹ç¼©æ–¹æ³•', 'desc': 'GUI-KVæ˜¯ä¸€ç§é’ˆå¯¹å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å‹ç¼©æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨ç©ºé—´å’Œæ—¶é—´å†—ä½™æ¥æé«˜æ•ˆç‡ã€‚è¯¥æ–¹æ³•åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡æˆªå›¾å’Œé•¿æ—¶é—´ä»»åŠ¡æ—¶ï¼Œèƒ½å¤Ÿé™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGUIä»£ç†çš„æ³¨æ„åŠ›æ¨¡å¼ä¸è‡ªç„¶å›¾åƒä¸åŒï¼Œæ‰€æœ‰å˜æ¢å™¨å±‚çš„æ³¨æ„åŠ›ç¨€ç–æ€§å‡è¾ƒé«˜ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„å‡åŒ€é¢„ç®—åˆ†é…ç­–ç•¥ã€‚GUI-KVç»“åˆäº†ç©ºé—´æ˜¾è‘—æ€§å¼•å¯¼å’Œæ—¶é—´å†—ä½™è¯„åˆ†ä¸¤ç§æ–°æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†ç¼“å­˜å‹ç¼©çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.23250', 'title': 'Training Vision-Language Process Reward Models for Test-Time Scaling in\n  Multimodal Reasoning: Key Insights and Lessons Learned', 'url': 'https://huggingface.co/papers/2509.23250', 'abstract': 'Hybrid data synthesis and perception-focused supervision improve the reliability of Vision-Language Process Reward Models (VL-PRMs) in guiding VLMs across diverse multimodal benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) provide step-level supervision that improves the reliability of reasoning in large language models. While PRMs have been extensively studied in text-based domains, their extension to Vision Language Models (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on Monte Carlo Tree Search (MCTS) for data construction, which can often produce noisy supervision signals and limit generalization across tasks. In this work, we aim to elucidate the design space of VL-PRMs by exploring diverse strategies for dataset construction, training, and test-time scaling. First, we introduce a hybrid data synthesis framework that combines MCTS with judgments from a strong VLM, producing more accurate step-level labels. Second, we propose perception-focused supervision, enabling our PRM to explicitly detect errors at the visual grounding stage of reasoning. Third, we systematically evaluate multiple test-time scaling strategies, showing that our PRMs can reliably guide VLMs toward more accurate solutions. Our experiments covering five diverse multimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and MathVision) reveal several key insights: (i) VL-PRMs when used as Outcome Reward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM guided process step selection, (ii) smaller VL-PRMs can match or even surpass larger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning abilities in stronger VLM backbones, (iv) perception-level supervision leads to significant gains in test-time scaling, and (v) TTS performance of different policies improve on advanced math reasoning datasets despite not training VL-PRMs on such datasets. We hope our work will motivate further research and support the advancement of VLMs.', 'score': 4, 'issue_id': 6198, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 27', 'zh': '9æœˆ27æ—¥'}, 'hash': 'c59cc3e092f9a705', 'authors': ['Brandon Ong', 'Tej Deep Pala', 'Vernon Toh', 'William Chandra Tjhi', 'Soujanya Poria'], 'affiliations': ['AI Singapore', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2509.23250.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#training', '#games', '#multimodal', '#data', '#dataset'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'Ğ£Ñ‡Ğ¸Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Vision-Language Process Reward Models (VL-PRMs), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ MCTS Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ VLM, Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ supervision Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ VL-PRMs Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ: VL-PRMs ĞºĞ°Ğº Outcome Reward Models Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²ÑƒÑ ÑĞµĞ»ĞµĞºÑ†Ğ¸Ñ, Ğ° supervision Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ test-time scaling.'}, 'en': {'title': 'Enhancing Vision-Language Models with Hybrid Supervision and Data Synthesis', 'desc': 'This paper discusses improvements in Vision-Language Process Reward Models (VL-PRMs) to enhance their effectiveness in guiding Vision Language Models (VLMs). The authors introduce a hybrid data synthesis method that merges Monte Carlo Tree Search with insights from a robust VLM, resulting in more precise step-level supervision. They also propose a perception-focused supervision approach that helps the model identify errors during visual reasoning. Through extensive testing on various multimodal benchmarks, the study demonstrates that these enhancements lead to better performance and reliability in VLMs, even in complex reasoning tasks.'}, 'zh': {'title': 'æ··åˆæ•°æ®åˆæˆä¸æ„ŸçŸ¥ç›‘ç£æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é æ€§', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆæ•°æ®åˆæˆæ¡†æ¶ï¼Œç»“åˆäº†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰å’Œå¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„åˆ¤æ–­ï¼Œä»¥ç”Ÿæˆæ›´å‡†ç¡®çš„æ­¥éª¤çº§æ ‡ç­¾ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä»¥æ„ŸçŸ¥ä¸ºä¸­å¿ƒçš„ç›‘ç£ï¼Œå¸®åŠ©è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨æ¨ç†çš„è§†è§‰åŸºç¡€é˜¶æ®µæ˜ç¡®æ£€æµ‹é”™è¯¯ã€‚é€šè¿‡ç³»ç»Ÿè¯„ä¼°å¤šç§æµ‹è¯•æ—¶æ‰©å±•ç­–ç•¥ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè§†è§‰è¯­è¨€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆVL-PRMï¼‰èƒ½å¤Ÿå¯é åœ°å¼•å¯¼VLMæœå‘æ›´å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœä¸ºè¿›ä¸€æ­¥ç ”ç©¶å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æä¾›äº†é‡è¦çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.22887', 'title': 'Infusing Theory of Mind into Socially Intelligent LLM Agents', 'url': 'https://huggingface.co/papers/2509.22887', 'abstract': 'Integrating Theory of Mind into LLMs improves dialogue effectiveness and goal achievement by enabling strategic reasoning and better partner relationships.  \t\t\t\t\tAI-generated summary \t\t\t\t Theory of Mind (ToM)-an understanding of the mental states of others-is a key aspect of human social intelligence, yet, chatbots and LLM-based social agents do not typically integrate it. In this work, we demonstrate that LLMs that explicitly use ToM get better at dialogue, achieving goals more effectively. After showing that simply prompting models to generate mental states between dialogue turns already provides significant benefit, we further introduce ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Experiments on the Sotopia interactive social evaluation benchmark demonstrate the effectiveness of our method over a range of baselines. Comprehensive analysis shows that ToMA exhibits more strategic, goal-oriented reasoning behaviors, which enable long-horizon adaptation, while maintaining better relationships with their partners. Our results suggest a step forward in integrating ToM for building socially intelligent LLM agents.', 'score': 4, 'issue_id': 6200, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 26', 'zh': '9æœˆ26æ—¥'}, 'hash': 'de0a43468eb08889', 'authors': ['EunJeong Hwang', 'Yuwei Yin', 'Giuseppe Carenini', 'Peter West', 'Vered Shwartz'], 'affiliations': ['University of British Columbia', 'Vector Institute for AI'], 'pdf_title_img': 'assets/pdf/title_img/2509.22887.jpg', 'data': {'categories': ['#alignment', '#rl', '#reasoning', '#benchmark', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ñ Ñ‚ĞµĞ¾Ñ€Ğ¸ĞµĞ¹ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ° (ToM) â€” ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ»ÑĞ´ĞµĞ¹ â€” Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ToMAgent (ToMA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾ Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸ĞºĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ñ†ĞµĞ»ĞµĞ¹ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğµ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ToM Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Empowering LLMs with Theory of Mind for Smarter Conversations', 'desc': 'This paper explores how integrating Theory of Mind (ToM) into large language models (LLMs) enhances their ability to engage in dialogue and achieve specific goals. By understanding the mental states of conversation partners, LLMs can respond more strategically and maintain better relationships. The authors introduce ToMA, a dialogue agent that combines ToM with dialogue lookahead to optimize its responses for goal achievement. Experiments show that ToMA outperforms traditional models, demonstrating improved reasoning and adaptability in social interactions.'}, 'zh': {'title': 'å¿ƒæ™ºç†è®ºæå‡å¯¹è¯æ™ºèƒ½', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å°†å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ–¹æ³•ï¼Œä»¥æé«˜å¯¹è¯çš„æœ‰æ•ˆæ€§å’Œç›®æ ‡è¾¾æˆç‡ã€‚å¿ƒæ™ºç†è®ºæ˜¯ç†è§£ä»–äººå¿ƒç†çŠ¶æ€çš„èƒ½åŠ›ï¼Œæ˜¯äººç±»ç¤¾ä¼šæ™ºèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡æ˜ç¡®ä½¿ç”¨å¿ƒæ™ºç†è®ºçš„LLMåœ¨å¯¹è¯ä¸­è¡¨ç°æ›´å¥½ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å®ç°ç›®æ ‡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ToMAï¼ˆå¿ƒæ™ºç†è®ºå¯¹è¯ä»£ç†ï¼‰ï¼Œé€šè¿‡å°†å¿ƒæ™ºç†è®ºä¸å¯¹è¯å‰ç»ç»“åˆï¼Œè®­ç»ƒå‡ºèƒ½å¤Ÿäº§ç”Ÿæœ‰åŠ©äºå®ç°å¯¹è¯ç›®æ ‡çš„å¿ƒç†çŠ¶æ€çš„ä»£ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00931', 'title': 'Making, not Taking, the Best of N', 'url': 'https://huggingface.co/papers/2510.00931', 'abstract': 'Fusion-of-N (FusioN) method improves LLM generation quality by synthesizing elements from multiple samples, outperforming Best-of-N in various settings and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Obtaining high-quality generations in modern LLMs has largely been framed as a selection problem: identifying a single winning generation from a diverse pool of N samples, the Best-of-N (BoN). Yet, this approach is inherently zero-sum, discarding diverse and potentially useful information from the pool. Instead, we explore a collaborative setup, where all candidates can potentially contribute to the final winning generation. To this end, we propose Fusion-of-N (FusioN): a method that uses a general LLM judge to synthesize the most informative elements of each sample into a single final answer. We compare FusioN to BoN in two settings, (i) test-time scaling, where we sample and aggregate from a single model at test-time (ii) synthetic data generation, where we fuse samples from a pool of diverse teachers to improve a student model. We extensively benchmark both setups across 11 languages, 3 diverse tasks and varying model scales. Across the bench, FusioN consistently outperforms BoN showing versatility and robustness both in test-time scaling and in downstream gains from synthetic data generation. We also perform extensive analysis on FusioN, where it shows surprising strengths and robustness under challenging settings. These results show that we should shift how we think about evaluating and utilizing LLM generations from a monolithic measure of quality, to embracing their polylithic nature. This shift allows us to integrate diverse strengths, unlock latent potential, and achieve improvements that were previously inaccessible through selection alone.', 'score': 3, 'issue_id': 6201, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'f203d81f3a65b787', 'authors': ['Ammar Khairi', "Daniel D'souza", 'Marzieh Fadaee', 'Julia Kreutzer'], 'affiliations': ['Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2510.00931.jpg', 'data': {'categories': ['#rag', '#benchmark', '#optimization', '#synthetic', '#multilingual'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ¡Ğ¸Ğ»Ğ° ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Fusion-of-N (FusioN), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ LLM Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ², Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ° ĞºĞ°Ğº Ğ² Best-of-N. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ÑŒĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ° Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚. FusioN Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»ÑÑ Ğ² Ğ´Ğ²ÑƒÑ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ inference Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ student-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 11 ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ 3 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ FusioN Ğ½Ğ°Ğ´ Best-of-N, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ¾Ğ¼.'}, 'en': {'title': 'Unlocking the Power of Collaboration in LLM Generation', 'desc': 'The Fusion-of-N (FusioN) method enhances the quality of large language model (LLM) outputs by integrating elements from multiple generated samples rather than selecting just one. This approach contrasts with the traditional Best-of-N (BoN) method, which often overlooks valuable information by focusing on a single best output. FusioN employs a general LLM judge to synthesize the most informative parts of each candidate, leading to a more comprehensive final generation. Extensive benchmarking across various languages and tasks demonstrates that FusioN consistently outperforms BoN, highlighting the benefits of leveraging diverse contributions in LLM generation.'}, 'zh': {'title': 'èåˆå¤šæ ·æ€§ï¼Œæå‡ç”Ÿæˆè´¨é‡', 'desc': 'Fusion-of-Nï¼ˆFusioNï¼‰æ–¹æ³•é€šè¿‡ç»¼åˆå¤šä¸ªæ ·æœ¬çš„å…ƒç´ ï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆè´¨é‡ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„æœ€ä½³é€‰æ‹©æ–¹æ³•ï¼ˆBest-of-Nï¼‰ã€‚è¯¥æ–¹æ³•ä¸å†ä»…ä»…é€‰æ‹©ä¸€ä¸ªæœ€ä½³ç”Ÿæˆï¼Œè€Œæ˜¯å…è®¸æ‰€æœ‰å€™é€‰æ ·æœ¬å…±åŒè´¡çŒ®ä¿¡æ¯ï¼Œå½¢æˆæœ€ç»ˆçš„ç­”æ¡ˆã€‚FusioNåœ¨æµ‹è¯•æ—¶æ‰©å±•å’Œåˆæˆæ•°æ®ç”Ÿæˆçš„å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤šè¯­è¨€å’Œä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹çš„çµæ´»æ€§å’Œç¨³å¥æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬åº”å½“æ”¹å˜å¯¹LLMç”Ÿæˆç»“æœçš„è¯„ä¼°æ–¹å¼ï¼Œä»å•ä¸€çš„è´¨é‡è¡¡é‡è½¬å‘æ¥å—å…¶å¤šæ ·æ€§ï¼Œä»¥å®ç°æ›´å¤§çš„æ½œåŠ›å’Œæ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01070', 'title': 'Eliciting Secret Knowledge from Language Models', 'url': 'https://huggingface.co/papers/2510.01070', 'abstract': 'Researchers develop and evaluate techniques to uncover hidden knowledge in large language models through black-box and white-box methods, with prefill attacks and logit lens being particularly effective.  \t\t\t\t\tAI-generated summary \t\t\t\t We study secret elicitation: discovering knowledge that an AI possesses but does not explicitly verbalize. As a testbed, we train three families of large language models (LLMs) to possess specific knowledge that they apply downstream but deny knowing when asked directly. For example, in one setting, we train an LLM to generate replies that are consistent with knowing the user is female, while denying this knowledge when asked directly. We then design various black-box and white-box secret elicitation techniques and evaluate them based on whether they can help an LLM auditor successfully guess the secret knowledge. Many of our techniques improve on simple baselines. Our most effective techniques (performing best in 2/3 settings) are based on prefill attacks, a black-box technique where the LLM reveals secret knowledge when generating a completion from a predefined prefix. In our remaining setting, white-box techniques based on logit lens and sparse autoencoders (SAEs) are most effective. We release our models and code, establishing a public benchmark for evaluating secret elicitation methods.', 'score': 2, 'issue_id': 6203, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'f0f210658ad43fde', 'authors': ['Bartosz CywiÅ„ski', 'Emil Ryd', 'Rowan Wang', 'Senthooran Rajamanoharan', 'Neel Nanda', 'Arthur Conmy', 'Samuel Marks'], 'affiliations': ['Anthropic', 'IDEAS Research Institute', 'University of Oxford', 'Warsaw University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2510.01070.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#interpretability', '#multimodal', '#hallucinations', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‚Ğ°Ğ¹Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹: ĞºĞ°Ğº Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ LLM Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ñ‚ÑŒÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ° ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğµ, Ğ½Ğ¾ ÑĞ²Ğ½Ğ¾ Ğ½Ğµ Ğ¾Ğ·Ğ²ÑƒÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚. Ğ”Ğ»Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°Ñ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¾ Ğ¿Ğ¾Ğ»Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ½Ğ¾ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ…. ĞĞ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ prefill-Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ² black-box Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞµĞºÑ€ĞµÑ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ white-box Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ logit lens Ğ¸ sparse autoencoders. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºĞ¾Ğ´, ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· AI-ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Unveiling Secrets: Extracting Hidden Knowledge from Language Models', 'desc': 'This paper explores methods to extract hidden knowledge from large language models (LLMs) that they do not openly disclose. The researchers train LLMs to possess specific knowledge while denying it when questioned directly. They introduce both black-box and white-box techniques for secret elicitation, with prefill attacks showing significant effectiveness. The study also provides a public benchmark for evaluating these secret elicitation methods, contributing to the understanding of LLM behavior.'}, 'zh': {'title': 'æ­ç¤ºAIéšè—çŸ¥è¯†çš„åˆ›æ–°æŠ€æœ¯', 'desc': 'ç ”ç©¶äººå‘˜å¼€å‘å¹¶è¯„ä¼°äº†é€šè¿‡é»‘ç®±å’Œç™½ç®±æ–¹æ³•æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­éšè—çŸ¥è¯†çš„æŠ€æœ¯ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸‰ç±»å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨åº”ç”¨ç‰¹å®šçŸ¥è¯†æ—¶æ‹’ç»æ‰¿è®¤è¿™ä¸€çŸ¥è¯†ã€‚æˆ‘ä»¬è®¾è®¡äº†å¤šç§é»‘ç®±å’Œç™½ç®±çš„ç§˜å¯†å¼•å‡ºæŠ€æœ¯ï¼Œå¹¶è¯„ä¼°å®ƒä»¬åœ¨å¸®åŠ©å®¡è®¡è€…æˆåŠŸçŒœæµ‹ç§˜å¯†çŸ¥è¯†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æŠ€æœ¯åœ¨å¤šä¸ªè®¾ç½®ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åŸºäºé¢„å¡«æ”»å‡»çš„é»‘ç®±æŠ€æœ¯å’ŒåŸºäºlogit lensçš„ç™½ç®±æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01037', 'title': 'CurES: From Gradient Analysis to Efficient Curriculum Learning for\n  Reasoning LLMs', 'url': 'https://huggingface.co/papers/2510.01037', 'abstract': 'CurES, a reinforcement learning-based method, improves the training efficiency of large language models by optimizing prompt selection and rollout allocation, leading to faster convergence and reduced computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Curriculum learning plays a crucial role in enhancing the training efficiency of large language models (LLMs) on reasoning tasks. However, existing methods often fail to adequately account for variations in prompt difficulty or rely on simplistic filtering mechanisms to select prompt datasets within a narrow criterion range, resulting in significant computational waste. In this work, we approach the problem from the perspective of reinforcement learning gradient optimization, offering a systematic and theoretical investigation into how to improve the training efficiency of LLMs. We identify two key factors influencing training efficiency: the selection of training prompts and the allocation of rollout quantities across different prompts. Our theoretical analysis reveals that the sampling distribution of prompts dictates the convergence rate of gradient descent, while the allocation of the rollout quantity influences the consistency and stability of overall gradient updates. Based on these insights, we propose CurES, an efficient training method that accelerates convergence and employs Bayesian posterior estimation to minimize computational overhead. Experiments demonstrate that our CurES outperforms Group Relative Policy Optimization (GRPO) by +3.30 points and +4.82 points with 1.5B and 7B models, respectively. Additionally, CurES exhibits faster convergence compared to baselines, including GRPO.', 'score': 2, 'issue_id': 6204, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'a8919e29862921e6', 'authors': ['Yongcheng Zeng', 'Zexu Sun', 'Bokai Ji', 'Erxue Min', 'Hengyi Cai', 'Shuaiqiang Wang', 'Dawei Yin', 'Haifeng Zhang', 'Xu Chen', 'Jun Wang'], 'affiliations': ['Baidu Inc.', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2510.01037.jpg', 'data': {'categories': ['#training', '#optimization', '#reasoning', '#rl'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'CurES â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ reinforcement learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ gradient descent, Ğ° ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ rollouts Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ baseline Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ GRPO Ğ½Ğ° 3-5 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ². CurES Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… reasoning.'}, 'en': {'title': 'Optimizing Prompt Selection for Efficient LLM Training with CurES', 'desc': 'CurES is a novel method that enhances the training efficiency of large language models (LLMs) by using reinforcement learning to optimize how prompts are selected and how rollout allocations are managed. It addresses the shortcomings of traditional curriculum learning approaches, which often overlook the complexity of prompt difficulty and lead to inefficient training processes. By analyzing the impact of prompt sampling distributions and rollout allocations, CurES improves the convergence rate of gradient descent and stabilizes gradient updates. Experimental results show that CurES significantly outperforms existing methods, achieving faster convergence and reduced computational costs.'}, 'zh': {'title': 'CurESï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ•ˆç‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'CurESæ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–æç¤ºé€‰æ‹©å’Œå›æ»šåˆ†é…æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚è¿™ç§æ–¹æ³•è§£å†³äº†ç°æœ‰æŠ€æœ¯åœ¨å¤„ç†æç¤ºéš¾åº¦å˜åŒ–æ—¶çš„ä¸è¶³ï¼Œé¿å…äº†ä¸å¿…è¦çš„è®¡ç®—æµªè´¹ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œæˆ‘ä»¬å‘ç°æç¤ºçš„é‡‡æ ·åˆ†å¸ƒå’Œå›æ»šæ•°é‡çš„åˆ†é…æ˜¯å½±å“è®­ç»ƒæ•ˆç‡çš„ä¸¤ä¸ªå…³é”®å› ç´ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCurESåœ¨åŠ é€Ÿæ”¶æ•›å’Œå‡å°‘è®¡ç®—å¼€é”€æ–¹é¢ä¼˜äºç°æœ‰çš„ä¼˜åŒ–æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00777', 'title': 'In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn\n  Reasoning', 'url': 'https://huggingface.co/papers/2510.00777', 'abstract': "In-place feedback allows users to directly edit LLM responses, improving performance and reducing token usage in multi-turn reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly studied in the context of multi-turn reasoning, where models iteratively refine their outputs based on user-provided feedback. Such settings are crucial for tasks that require complex reasoning, yet existing feedback paradigms often rely on issuing new messages. LLMs struggle to integrate these reliably, leading to inconsistent improvements. In this work, we introduce in-place feedback, a novel interaction paradigm in which users directly edit an LLM's previous response, and the model conditions on this modified response to generate its revision. Empirical evaluations on diverse reasoning-intensive benchmarks reveal that in-place feedback achieves better performance than conventional multi-turn feedback while using 79.1% fewer tokens. Complementary analyses on controlled environments further demonstrate that in-place feedback resolves a core limitation of multi-turn feedback: models often fail to apply feedback precisely to erroneous parts of the response, leaving errors uncorrected and sometimes introducing new mistakes into previously correct content. These findings suggest that in-place feedback offers a more natural and effective mechanism for guiding LLMs in reasoning-intensive tasks.", 'score': 2, 'issue_id': 6198, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'be36c5f4f29f17e4', 'authors': ['Youngbin Choi', 'Minjong Lee', 'Saemi Moon', 'Seunghyuk Cho', 'Chaehyeon Chung', 'MoonJeong Park', 'Dongwoo Kim'], 'affiliations': ['Computer Science and Engineering, POSTECH', 'Graduate School of Artificial Intelligence, POSTECH'], 'pdf_title_img': 'assets/pdf/title_img/2510.00777.jpg', 'data': {'categories': ['#training', '#rlhf', '#interpretability', '#reasoning'], 'emoji': 'âœï¸', 'ru': {'title': 'Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞ¹ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ·Ğ´ĞµÑÑŒ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ Ğ´Ğ»Ñ LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ in-place feedback, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ LLM Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 79,1% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ Ğº Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ‡Ğ°ÑÑ‚ÑĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ñ€Ğ°Ğ½ĞµĞµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'In-Place Feedback: Direct Edits for Smarter LLMs', 'desc': "This paper presents a new method called in-place feedback for improving large language models (LLMs) during multi-turn reasoning tasks. Instead of sending new messages for feedback, users can directly edit the model's previous responses, allowing the model to learn from these modifications. The results show that this approach not only enhances the model's performance but also significantly reduces the number of tokens used by 79.1%. Overall, in-place feedback addresses the limitations of traditional feedback methods by enabling more precise corrections and reducing the introduction of new errors."}, 'zh': {'title': 'å°±åœ°åé¦ˆï¼šæå‡LLMæ¨ç†çš„æœ‰æ•ˆæ–°æ–¹å¼', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„äº¤äº’æ¨¡å¼â€”â€”å°±åœ°åé¦ˆï¼Œå…è®¸ç”¨æˆ·ç›´æ¥ç¼–è¾‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å“åº”ã€‚è¿™ç§æ–¹æ³•åœ¨å¤šè½®æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘79.1%çš„ä»¤ç‰Œä½¿ç”¨ã€‚é€šè¿‡å®è¯è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å°±åœ°åé¦ˆæ¯”ä¼ ç»Ÿçš„å¤šè½®åé¦ˆæ›´æœ‰æ•ˆï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°åº”ç”¨ç”¨æˆ·çš„åé¦ˆï¼Œé¿å…äº†æ¨¡å‹åœ¨ä¿®æ­£é”™è¯¯æ—¶å¼•å…¥æ–°çš„é”™è¯¯ã€‚æ€»çš„æ¥è¯´ï¼Œå°±åœ°åé¦ˆä¸ºæŒ‡å¯¼LLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æä¾›äº†ä¸€ç§æ›´è‡ªç„¶å’Œæœ‰æ•ˆçš„æœºåˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00510', 'title': 'JoyAgent-JDGenie: Technical Report on the GAIA', 'url': 'https://huggingface.co/papers/2510.00510', 'abstract': 'A generalist agent architecture combining multi-agent planning, hierarchical memory, and a refined tool suite outperforms existing systems in diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models are increasingly deployed as autonomous agents for complex real-world tasks, yet existing systems often focus on isolated improvements without a unifying design for robustness and adaptability. We propose a generalist agent architecture that integrates three core components: a collective multi-agent framework combining planning and execution agents with critic model voting, a hierarchical memory system spanning working, semantic, and procedural layers, and a refined tool suite for search, code execution, and multimodal parsing. Evaluated on a comprehensive benchmark, our framework consistently outperforms open-source baselines and approaches the performance of proprietary systems. These results demonstrate the importance of system-level integration and highlight a path toward scalable, resilient, and adaptive AI assistants capable of operating across diverse domains and tasks.', 'score': 2, 'issue_id': 6198, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '4a1605179598a812', 'authors': ['Jiarun Liu', 'Shiyue Xu', 'Shangkun Liu', 'Yang Li', 'Wen Liu', 'Min Liu', 'Xiaoqing Zhou', 'Hanmin Wang', 'Shilin Jia', 'zhen Wang', 'Shaohua Tian', 'Hanhao Li', 'Junbo Zhang', 'Yongli Yu', 'Peng Cao', 'Haofen Wang'], 'affiliations': ['GAIA JINGDONG CHO-EI Team'], 'pdf_title_img': 'assets/pdf/title_img/2510.00510.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#optimization', '#architecture', '#multimodal', '#agents', '#agi'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ â€” Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸. Ğ¢Ñ€ĞµÑ‚Ğ¸Ğ¹ â€” Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ open-source Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Empowering AI with Integrated Generalist Agent Architecture', 'desc': 'This paper presents a new architecture for generalist agents that enhances their performance in various tasks. It combines multi-agent planning, where different agents work together to make decisions, with a hierarchical memory system that organizes information at different levels. Additionally, it includes a refined tool suite that allows the agent to perform tasks like searching and executing code. The proposed system shows significant improvements over existing models, indicating that integrating these components leads to more robust and adaptable AI assistants.'}, 'zh': {'title': 'é€šç”¨æ™ºèƒ½ä½“æ¶æ„ï¼šæå‡AIåŠ©æ‰‹çš„é€‚åº”æ€§ä¸é²æ£’æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨æ™ºèƒ½ä½“æ¶æ„ï¼Œç»“åˆäº†å¤šæ™ºèƒ½ä½“è§„åˆ’ã€åˆ†å±‚è®°å¿†å’Œç²¾ç»†åŒ–å·¥å…·å¥—ä»¶ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡ä¸­è¶…è¶Šç°æœ‰ç³»ç»Ÿã€‚è¯¥æ¶æ„æ•´åˆäº†é›†ä½“å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€åˆ†å±‚è®°å¿†ç³»ç»Ÿä»¥åŠç”¨äºæœç´¢ã€ä»£ç æ‰§è¡Œå’Œå¤šæ¨¡æ€è§£æçš„å·¥å…·ã€‚é€šè¿‡å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºå¼€æºåŸºçº¿ï¼Œå¹¶æ¥è¿‘ä¸“æœ‰ç³»ç»Ÿçš„è¡¨ç°ã€‚è¿™äº›ç»“æœè¡¨æ˜ç³»ç»Ÿçº§é›†æˆçš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¯æ‰©å±•ã€å¼¹æ€§å’Œé€‚åº”æ€§å¼ºçš„äººå·¥æ™ºèƒ½åŠ©æ‰‹æŒ‡æ˜äº†æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.19185', 'title': 'An Empirical Study of Testing Practices in Open Source AI Agent\n  Frameworks and Agentic Applications', 'url': 'https://huggingface.co/papers/2509.19185', 'abstract': 'The study identifies testing practices in AI agent frameworks and applications, highlighting a focus on deterministic components and a neglect of the Trigger component, suggesting improvements for robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation model (FM)-based AI agents are rapidly gaining adoption across diverse domains, but their inherent non-determinism and non-reproducibility pose testing and quality assurance challenges. While recent benchmarks provide task-level evaluations, there is limited understanding of how developers verify the internal correctness of these agents during development.   To address this gap, we conduct the first large-scale empirical study of testing practices in the AI agent ecosystem, analyzing 39 open-source agent frameworks and 439 agentic applications. We identify ten distinct testing patterns and find that novel, agent-specific methods like DeepEval are seldom used (around 1%), while traditional patterns like negative and membership testing are widely adapted to manage FM uncertainty. By mapping these patterns to canonical architectural components of agent frameworks and agentic applications, we uncover a fundamental inversion of testing effort: deterministic components like Resource Artifacts (tools) and Coordination Artifacts (workflows) consume over 70% of testing effort, while the FM-based Plan Body receives less than 5%. Crucially, this reveals a critical blind spot, as the Trigger component (prompts) remains neglected, appearing in around 1% of all tests.   Our findings offer the first empirical testing baseline in FM-based agent frameworks and agentic applications, revealing a rational but incomplete adaptation to non-determinism. To address it, framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore barriers to adoption. Strengthening these practices is vital for building more robust and dependable AI agents.', 'score': 2, 'issue_id': 6199, 'pub_date': '2025-09-23', 'pub_date_card': {'ru': '23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 23', 'zh': '9æœˆ23æ—¥'}, 'hash': 'e00a1db1e43fa4c8', 'authors': ['Mohammed Mehedi Hasan', 'Hao Li', 'Emad Fallahzadeh', 'Gopi Krishnan Rajbahadur', 'Bram Adams', 'Ahmed E. Hassan'], 'affiliations': ['School of Computing, Queens University, Kingston, ON, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2509.19185.jpg', 'data': {'categories': ['#benchmark', '#agents', '#security', '#open_source', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğµ Ñ‚Ğ°Ğ¼, Ğ³Ğ´Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² 39 Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ 439 Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ´ĞµÑÑÑ‚ÑŒ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°: Ğ±Ğ¾Ğ»ĞµĞµ 70% ÑƒÑĞ¸Ğ»Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ (Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ²Ğ¾Ñ€ĞºÑ„Ğ»Ğ¾Ñƒ), Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ foundation models Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 5% Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ (Trigger component) Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ, Ğ¿Ğ¾ÑĞ²Ğ»ÑÑÑÑŒ Ğ»Ğ¸ÑˆÑŒ Ğ² 1% Ñ‚ĞµÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ â€” Ğ²Ğ½ĞµĞ´Ñ€Ğ¸Ñ‚ÑŒ regression-Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing AI Agent Testing: Bridging the Gap in Robustness', 'desc': 'This paper investigates the testing practices used in AI agent frameworks and applications, revealing a significant focus on deterministic components while largely neglecting the Trigger component. The study analyzes 39 open-source frameworks and 439 applications, identifying ten distinct testing patterns, with traditional methods dominating the landscape. It highlights that over 70% of testing effort is spent on deterministic components, while less than 5% is allocated to the FM-based Plan Body, indicating a critical oversight. The authors suggest that improving testing methods and incorporating prompt regression testing are essential for enhancing the robustness of AI agents.'}, 'zh': {'title': 'æå‡AIä»£ç†æµ‹è¯•çš„é²æ£’æ€§', 'desc': 'æœ¬ç ”ç©¶åˆ†æäº†äººå·¥æ™ºèƒ½ä»£ç†æ¡†æ¶å’Œåº”ç”¨ä¸­çš„æµ‹è¯•å®è·µï¼Œå‘ç°ç›®å‰çš„æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨ç¡®å®šæ€§ç»„ä»¶ä¸Šï¼Œè€Œè§¦å‘ç»„ä»¶å´è¢«å¿½è§†ã€‚æˆ‘ä»¬å¯¹39ä¸ªå¼€æºä»£ç†æ¡†æ¶å’Œ439ä¸ªä»£ç†åº”ç”¨è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œè¯†åˆ«å‡ºåç§ä¸åŒçš„æµ‹è¯•æ¨¡å¼ã€‚ç»“æœæ˜¾ç¤ºï¼ŒåƒDeepEvalè¿™æ ·çš„æ–°å‹ä»£ç†ç‰¹å®šæ–¹æ³•ä½¿ç”¨ç‡æä½ï¼Œè€Œä¼ ç»Ÿçš„è´Ÿé¢æµ‹è¯•å’Œæˆå‘˜æµ‹è¯•è¢«å¹¿æ³›åº”ç”¨ä»¥åº”å¯¹åŸºç¡€æ¨¡å‹çš„ä¸ç¡®å®šæ€§ã€‚ä¸ºäº†æé«˜AIä»£ç†çš„é²æ£’æ€§ï¼Œå¼€å‘è€…éœ€è¦æ”¹è¿›å¯¹æ–°æµ‹è¯•æ–¹æ³•çš„æ”¯æŒï¼Œå¹¶åœ¨åº”ç”¨ä¸­é‡‡ç”¨æç¤ºå›å½’æµ‹è¯•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01061', 'title': "ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced\n  Wasserstein Distance for Variance Reduction", 'url': 'https://huggingface.co/papers/2510.01061', 'abstract': 'Reservoir SWD reduces variance in Sliced Wasserstein Distance, improving gradient stability and performance in vision and graphics tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Distribution matching is central to many vision and graphics tasks, where the widely used Wasserstein distance is too costly to compute for high dimensional distributions. The Sliced Wasserstein Distance (SWD) offers a scalable alternative, yet its Monte Carlo estimator suffers from high variance, resulting in noisy gradients and slow convergence. We introduce Reservoir SWD (ReSWD), which integrates Weighted Reservoir Sampling into SWD to adaptively retain informative projection directions in optimization steps, resulting in stable gradients while remaining unbiased. Experiments on synthetic benchmarks and real-world tasks such as color correction and diffusion guidance show that ReSWD consistently outperforms standard SWD and other variance reduction baselines. Project page: https://reservoirswd.github.io/', 'score': 1, 'issue_id': 6203, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'a1e6afd30da770db', 'authors': ['Mark Boss', 'Andreas Engelhardt', 'Simon DonnÃ©', 'Varun Jampani'], 'affiliations': ['Stability AI', 'University of TÃ¼bingen'], 'pdf_title_img': 'assets/pdf/title_img/2510.01061.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#synthetic', '#diffusion', '#training', '#cv'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞ·ĞµÑ€Ğ²ÑƒĞ°Ñ€Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Reservoir SWD Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸. ĞšĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Wasserstein distance ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ´Ğ¾Ñ€Ğ¾Ğ³ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ° ĞµĞ³Ğ¾ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Sliced Wasserstein Distance ÑÑ‚Ñ€Ğ°Ğ´Ğ°ĞµÑ‚ Ğ¾Ñ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Weighted Reservoir Sampling Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ†Ğ²ĞµÑ‚Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ReSWD Ğ½Ğ°Ğ´ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ SWD.'}, 'en': {'title': 'Stabilizing Gradients with Reservoir SWD for Better Performance', 'desc': 'This paper presents Reservoir SWD (ReSWD), a novel approach that enhances the Sliced Wasserstein Distance (SWD) by reducing its variance. The traditional SWD is effective for matching distributions in vision and graphics but suffers from high variance in its Monte Carlo estimations, leading to unstable gradients. By incorporating Weighted Reservoir Sampling, ReSWD retains the most informative projection directions, which stabilizes the gradient and improves convergence rates. Experimental results demonstrate that ReSWD outperforms both standard SWD and other methods aimed at variance reduction in various tasks.'}, 'zh': {'title': 'Reservoir SWDï¼šæå‡åˆ‡ç‰‡Wassersteinè·ç¦»çš„ç¨³å®šæ€§ä¸æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºReservoir SWDï¼ˆReSWDï¼‰ï¼Œæ—¨åœ¨å‡å°‘åˆ‡ç‰‡Wassersteinè·ç¦»ï¼ˆSWDï¼‰ä¸­çš„æ–¹å·®ï¼Œä»è€Œæé«˜æ¢¯åº¦çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚ä¼ ç»Ÿçš„SWDåœ¨é«˜ç»´åˆ†å¸ƒä¸­è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œè€ŒReSWDé€šè¿‡å°†åŠ æƒæ°´åº“æŠ½æ ·æŠ€æœ¯æ•´åˆåˆ°SWDä¸­ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°ä¿ç•™æœ‰ç”¨çš„æŠ•å½±æ–¹å‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReSWDåœ¨åˆæˆåŸºå‡†å’Œå®é™…ä»»åŠ¡ï¼ˆå¦‚é¢œè‰²æ ¡æ­£å’Œæ‰©æ•£å¼•å¯¼ï¼‰ä¸­ï¼Œå‡ä¼˜äºæ ‡å‡†SWDå’Œå…¶ä»–æ–¹å·®å‡å°‘åŸºçº¿ã€‚è¯¥æ–¹æ³•ä¸ºè§†è§‰å’Œå›¾å½¢ä»»åŠ¡ä¸­çš„åˆ†å¸ƒåŒ¹é…æä¾›äº†æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25916', 'title': 'VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained\n  Perception in VLMs', 'url': 'https://huggingface.co/papers/2509.25916', 'abstract': "VLM-FO1 enhances vision-language models with a hybrid fine-grained region encoder to improve object localization and region understanding without sacrificing general visual capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) excel at high-level scene understanding but falter on fine-grained perception tasks requiring precise localization. This failure stems from a fundamental mismatch, as generating exact numerical coordinates is a challenging task for language-centric architectures. In this paper, we introduce VLM-FO1, a novel framework that overcomes this limitation by reframing object-centric perception from a brittle coordinate generation problem into a robust feature retrieval task. Our method operates as a plug-and-play module that integrates with any pre-trained VLM. It leverages a Hybrid Fine-grained Region Encoder (HFRE), featuring a dual vision encoder, to generate powerful region tokens rich in both semantic and spatial detail. A token-based referencing system then enables the LLM to seamlessly reason about and ground language in these specific visual regions. Experiments show that VLM-FO1 achieves state-of-the-art performance across a diverse suite of benchmarks, demonstrating exceptional capabilities in object grounding, region generational understanding, and visual region reasoning. Crucially, our two-stage training strategy ensures that these perception gains are achieved without compromising the base model's general visual understanding capabilities. VLM-FO1 establishes an effective and flexible paradigm for building perception-aware VLMs, bridging the gap between high-level reasoning and fine-grained visual grounding.", 'score': 1, 'issue_id': 6204, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '93fe431b882d5210', 'authors': ['Peng Liu', 'Haozhan Shen', 'Chunxin Fang', 'Zhicheng Sun', 'Jiajia Liao', 'Tiancheng Zhao'], 'affiliations': ['Binjiang Institute of Zhejiang University', 'College of Computer Science and Technology, Zhejiang University', 'Om AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.25916.jpg', 'data': {'categories': ['#training', '#architecture', '#multimodal', '#reasoning', '#agi', '#cv', '#interpretability', '#benchmark'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞÑ‚ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğº Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼: Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'VLM-FO1 Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¸Ğ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¼ĞµĞ»ĞºĞ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² (HFRE) Ñ Ğ´Ğ²ÑƒĞ¼Ñ vision-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ², Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² grounding Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ². Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¾Ğ±Ñ‰ĞµĞ¼Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ´ĞµĞ»Ğ°Ñ VLM-FO1 ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ plug-and-play Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼.'}, 'en': {'title': 'Bridging High-Level Reasoning and Fine-Grained Visual Grounding', 'desc': 'VLM-FO1 is a new framework designed to enhance vision-language models (VLMs) by improving their ability to locate and understand specific regions in images. Traditional VLMs struggle with precise localization due to their focus on language, which makes generating exact coordinates difficult. This paper introduces a Hybrid Fine-grained Region Encoder (HFRE) that transforms the localization challenge into a feature retrieval task, allowing for better integration of visual and semantic information. The results show that VLM-FO1 not only excels in object grounding and region understanding but also maintains the general visual capabilities of the original model, making it a versatile tool for perception-aware VLMs.'}, 'zh': {'title': 'VLM-FO1ï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç‰©ä½“å®šä½èƒ½åŠ›', 'desc': 'VLM-FO1 æ˜¯ä¸€ç§å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„ç‰©ä½“å®šä½å’ŒåŒºåŸŸç†è§£èƒ½åŠ›ã€‚å®ƒé€šè¿‡æ··åˆç»†ç²’åº¦åŒºåŸŸç¼–ç å™¨ï¼Œå°†ç‰©ä½“ä¸­å¿ƒæ„ŸçŸ¥é—®é¢˜è½¬å˜ä¸ºå¼ºå¤§çš„ç‰¹å¾æ£€ç´¢ä»»åŠ¡ï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿè¯­è¨€æ¨¡å‹åœ¨ç²¾ç¡®åæ ‡ç”Ÿæˆä¸Šçš„å±€é™ã€‚è¯¥æ–¹æ³•å¯ä»¥ä½œä¸ºæ’ä»¶ä¸ä»»ä½•é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹é›†æˆï¼Œåˆ©ç”¨åŒè§†è§‰ç¼–ç å™¨ç”Ÿæˆä¸°å¯Œçš„åŒºåŸŸæ ‡è®°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVLM-FO1 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒæˆåŠŸå®ç°äº†ç‰©ä½“å®šä½å’Œè§†è§‰åŒºåŸŸæ¨ç†çš„æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25411', 'title': 'Boolean Satisfiability via Imitation Learning', 'url': 'https://huggingface.co/papers/2509.25411', 'abstract': 'ImitSAT, a branching policy for CDCL solvers using imitation learning from expert traces, reduces propagation counts and runtime by providing dense decision-level supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose ImitSAT, a branching policy for conflict-driven clause learning (CDCL) solvers based on imitation learning for the Boolean satisfiability problem (SAT). Unlike previous methods that predict instance-level signals to improve CDCL branching indirectly, or rely on reinforcement learning and insufficient CDCL information to enhance branching, ImitSAT learns from expert KeyTrace that collapses a full run into the sequence of surviving decisions. Replaying a KeyTrace on the same instance is nearly conflict-free, providing dense decision-level supervision and directly reducing propagations -- the dominant contributor to wall-clock time. This prefix-conditioned supervision enables ImitSAT to reproduce high-quality branches without exploration, yielding faster convergence, stable training, and seamless integration into CDCL. Extensive experiments demonstrate that ImitSAT reduces propagation counts and runtime, outperforming state-of-the-art learned approaches. We released the source code and trained model at https://github.com/zewei-Zhang/ImitSAT', 'score': 1, 'issue_id': 6199, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '8518038ce5781379', 'authors': ['Zewei Zhang', 'Huan Liu', 'Yuanhao Yu', 'Jun Chen', 'Xiangyu Xu'], 'affiliations': ['McMaster University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2509.25411.jpg', 'data': {'categories': ['#rl', '#open_source', '#training', '#optimization', '#math'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ÑÑĞ°Ñ… Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ SAT-ÑĞ¾Ğ»Ğ²ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ImitSAT - Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ CDCL ÑĞ¾Ğ»Ğ²ĞµÑ€Ğ¾Ğ² SAT-Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° imitation learning. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ÑÑĞ°Ñ… KeyTrace, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¹ supervised ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ propagations - Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ImitSAT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ML, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ propagations.'}, 'en': {'title': 'ImitSAT: Learning from Experts for Faster SAT Solving', 'desc': 'ImitSAT is a new branching policy designed for conflict-driven clause learning (CDCL) solvers that uses imitation learning from expert traces to improve performance on the Boolean satisfiability problem (SAT). Unlike traditional methods that rely on indirect signals or reinforcement learning, ImitSAT directly learns from a sequence of expert decisions, known as KeyTrace, which simplifies the decision-making process. This approach minimizes conflicts during execution, leading to fewer propagation counts and reduced runtime. The results show that ImitSAT significantly outperforms existing learned methods, providing a more efficient and effective solution for SAT problems.'}, 'zh': {'title': 'ImitSATï¼šé«˜æ•ˆçš„CDCLæ±‚è§£å™¨åˆ†æ”¯ç­–ç•¥', 'desc': 'ImitSATæ˜¯ä¸€ç§åŸºäºæ¨¡ä»¿å­¦ä¹ çš„åˆ†æ”¯ç­–ç•¥ï¼Œä¸“ä¸ºå†²çªé©±åŠ¨å­å¥å­¦ä¹ ï¼ˆCDCLï¼‰æ±‚è§£å™¨è®¾è®¡ï¼Œæ—¨åœ¨è§£å†³å¸ƒå°”å¯æ»¡è¶³æ€§é—®é¢˜ï¼ˆSATï¼‰ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒImitSATé€šè¿‡å­¦ä¹ ä¸“å®¶çš„KeyTraceï¼Œç›´æ¥æä¾›å†³ç­–çº§çš„ç›‘ç£ï¼Œä»è€Œå‡å°‘ä¼ æ’­æ¬¡æ•°å’Œè¿è¡Œæ—¶é—´ã€‚é€šè¿‡åœ¨åŒä¸€å®ä¾‹ä¸Šé‡æ”¾KeyTraceï¼ŒImitSATå‡ ä¹æ²¡æœ‰å†²çªï¼Œæ˜¾è‘—æé«˜äº†åˆ†æ”¯çš„è´¨é‡å’Œæ”¶æ•›é€Ÿåº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒImitSATåœ¨ä¼ æ’­æ¬¡æ•°å’Œè¿è¡Œæ—¶é—´ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›å­¦ä¹ æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25045', 'title': 'Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic\n  Architectures', 'url': 'https://huggingface.co/papers/2509.25045', 'abstract': "A novel Hyperdimensional Probe method decodes information from LLM vector spaces using Vector Symbolic Architectures, providing interpretable insights into model states and failures.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite their capabilities, Large Language Models (LLMs) remain opaque with limited understanding of their internal representations. Current interpretability methods, such as direct logit attribution (DLA) and sparse autoencoders (SAEs), provide restricted insight due to limitations such as the model's output vocabulary or unclear feature names. This work introduces Hyperdimensional Probe, a novel paradigm for decoding information from the LLM vector space. It combines ideas from symbolic representations and neural probing to project the model's residual stream into interpretable concepts via Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs and conventional probes while overcoming their key limitations. We validate our decoding paradigm with controlled input-completion tasks, probing the model's final state before next-token prediction on inputs spanning syntactic pattern recognition, key-value associations, and abstract inference. We further assess it in a question-answering setting, examining the state of the model both before and after text generation. Our experiments show that our probe reliably extracts meaningful concepts across varied LLMs, embedding sizes, and input domains, also helping identify LLM failures. Our work advances information decoding in LLM vector space, enabling extracting more informative, interpretable, and structured features from neural representations.", 'score': 1, 'issue_id': 6204, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': 'b34a8aa388991a99', 'authors': ['Marco Bronzini', 'Carlo Nicolini', 'Bruno Lepri', 'Jacopo Staiano', 'Andrea Passerini'], 'affiliations': ['Fondazione Bruno Kessler (FBK), Trento, Italy', 'Ipazia S.p.A., Milan, Italy', 'University of Trento, Trento, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2509.25045.jpg', 'data': {'categories': ['#data', '#architecture', '#interpretability'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ“Ğ¸Ğ¿ĞµÑ€Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Hyperdimensional Probe Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Vector Symbolic Architectures (VSA) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ residual stream Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ñ‹, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ²Ñ€Ğ¾Ğ´Ğµ sparse autoencoders. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ¾Ğ½Ğ´ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ², Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ¾ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ¸Ñ… Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸.'}, 'en': {'title': 'Decoding LLMs: Unveiling Insights with Hyperdimensional Probes', 'desc': 'The paper presents a new method called Hyperdimensional Probe that helps decode information from the vector spaces of Large Language Models (LLMs) using Vector Symbolic Architectures (VSAs). This method aims to improve interpretability by providing clearer insights into the internal workings and potential failures of LLMs, which are often difficult to understand. Unlike existing methods like direct logit attribution and sparse autoencoders, Hyperdimensional Probe combines the strengths of these approaches while addressing their limitations. The authors validate their method through various tasks, demonstrating its ability to extract meaningful concepts and enhance our understanding of LLM behavior.'}, 'zh': {'title': 'è¶…ç»´æ¢æµ‹å™¨ï¼šè§£ç å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¶…ç»´æ¢æµ‹å™¨æ–¹æ³•ï¼Œé€šè¿‡å‘é‡ç¬¦å·æ¶æ„ä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘é‡ç©ºé—´ä¸­è§£ç ä¿¡æ¯ï¼Œæä¾›å¯¹æ¨¡å‹çŠ¶æ€å’Œå¤±è´¥çš„å¯è§£é‡Šæ€§æ´å¯Ÿã€‚ç°æœ‰çš„å¯è§£é‡Šæ€§æ–¹æ³•å¦‚ç›´æ¥é€»è¾‘å½’å› å’Œç¨€ç–è‡ªç¼–ç å™¨ç”±äºæ¨¡å‹è¾“å‡ºè¯æ±‡æˆ–ç‰¹å¾åç§°ä¸æ¸…æ™°ç­‰é™åˆ¶ï¼Œæä¾›çš„æ´å¯ŸåŠ›æœ‰é™ã€‚è¶…ç»´æ¢æµ‹å™¨ç»“åˆäº†ç¬¦å·è¡¨ç¤ºå’Œç¥ç»æ¢æµ‹çš„æ€æƒ³ï¼Œå°†æ¨¡å‹çš„æ®‹å·®æµæŠ•å½±åˆ°å¯è§£é‡Šçš„æ¦‚å¿µä¸­ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¢æµ‹å™¨èƒ½å¤Ÿå¯é åœ°æå–æœ‰æ„ä¹‰çš„æ¦‚å¿µï¼Œå¹¶å¸®åŠ©è¯†åˆ«LLMçš„å¤±è´¥ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (9)', '#agi (2)', '#alignment (1)', '#architecture (5)', '#audio', '#benchmark (14)', '#cv (2)', '#data (2)', '#dataset (5)', '#diffusion (1)', '#ethics (1)', '#games (3)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (2)', '#interpretability (5)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math (3)', '#multilingual (1)', '#multimodal (4)', '#open_source (6)', '#optimization (16)', '#plp', '#rag (1)', '#reasoning (12)', '#rl (10)', '#rlhf (1)', '#robotics', '#science', '#security (1)', '#small_models (2)', '#story_generation', '#survey', '#synthetic (2)', '#training (16)', '#transfer_learning', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-10-02 11:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-02 11:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-02 11:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    