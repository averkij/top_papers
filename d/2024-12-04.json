{
    "date": {
        "ru": "4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
        "en": "December 4",
        "zh": "12æœˆ4æ—¥"
    },
    "time_utc": "2024-12-04 08:29",
    "weekday": 2,
    "issue_id": 938,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2411.19943",
            "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability",
            "url": "https://huggingface.co/papers/2411.19943",
            "abstract": "Large Language Models (LLMs) have exhibited remarkable performance on reasoning tasks. They utilize autoregressive token generation to construct reasoning trajectories, enabling the development of a coherent chain of thought. In this work, we explore the impact of individual tokens on the final outcomes of reasoning tasks. We identify the existence of ``critical tokens'' that lead to incorrect reasoning trajectories in LLMs. Specifically, we find that LLMs tend to produce positive outcomes when forced to decode other tokens instead of critical tokens. Motivated by this observation, we propose a novel approach - cDPO - designed to automatically recognize and conduct token-level rewards for the critical tokens during the alignment process. Specifically, we develop a contrastive estimation approach to automatically identify critical tokens. It is achieved by comparing the generation likelihood of positive and negative models. To achieve this, we separately fine-tune the positive and negative models on various reasoning trajectories, consequently, they are capable of identifying identify critical tokens within incorrect trajectories that contribute to erroneous outcomes. Moreover, to further align the model with the critical token information during the alignment process, we extend the conventional DPO algorithms to token-level DPO and utilize the differential likelihood from the aforementioned positive and negative model as important weight for token-level DPO learning.Experimental results on GSM8K and MATH500 benchmarks with two-widely used models Llama-3 (8B and 70B) and deepseek-math (7B) demonstrate the effectiveness of the propsoed approach cDPO.",
            "score": 20,
            "issue_id": 933,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 29",
                "zh": "11æœˆ29æ—¥"
            },
            "hash": "aaf523f6bd9412e3",
            "authors": [
                "Zicheng Lin",
                "Tian Liang",
                "Jiahao Xu",
                "Xing Wang",
                "Ruilin Luo",
                "Chufan Shi",
                "Siheng Li",
                "Yujiu Yang",
                "Zhaopeng Tu"
            ],
            "affiliations": [
                "Tsinghua University",
                "Tencent AI Lab"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19943.jpg",
            "data": {
                "categories": [
                    "#math",
                    "#training",
                    "#rlhf",
                    "#reasoning",
                    "#benchmark",
                    "#alignment"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 'ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ cDPO Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… GSM8K Ğ¸ MATH500 Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Llama-3 Ğ¸ deepseek-math Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°."
                },
                "en": {
                    "title": "Enhancing Reasoning in LLMs by Identifying Critical Tokens",
                    "desc": "This paper investigates how individual tokens in Large Language Models (LLMs) affect reasoning outcomes. It identifies 'critical tokens' that can lead to incorrect reasoning paths, suggesting that LLMs perform better when they focus on non-critical tokens. The authors introduce a new method called cDPO, which uses contrastive estimation to automatically detect these critical tokens during the model's alignment process. Experimental results show that cDPO improves reasoning performance on benchmark datasets by effectively managing token-level rewards."
                },
                "zh": {
                    "title": "è¯†åˆ«å…³é”®tokenï¼Œæå‡æ¨ç†å‡†ç¡®æ€§",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿé€šè¿‡è‡ªå›å½’çš„æ–¹å¼ç”Ÿæˆæ¨ç†è¿‡ç¨‹ã€‚æœ¬æ–‡æ¢è®¨äº†å•ä¸ªtokenå¯¹æ¨ç†ä»»åŠ¡æœ€ç»ˆç»“æœçš„å½±å“ï¼Œå‘ç°å­˜åœ¨â€œå…³é”®tokenâ€ï¼Œè¿™äº›tokenä¼šå¯¼è‡´é”™è¯¯çš„æ¨ç†è½¨è¿¹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•cDPOï¼Œæ—¨åœ¨è‡ªåŠ¨è¯†åˆ«å…³é”®tokenå¹¶åœ¨å¯¹é½è¿‡ç¨‹ä¸­è¿›è¡Œtokençº§å¥–åŠ±ã€‚é€šè¿‡å¯¹æ¯”æ­£è´Ÿæ¨¡å‹çš„ç”Ÿæˆå¯èƒ½æ€§ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè¯†åˆ«å‡ºåœ¨é”™è¯¯è½¨è¿¹ä¸­å¯¼è‡´é”™è¯¯ç»“æœçš„å…³é”®tokenï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02259",
            "title": "VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation",
            "url": "https://huggingface.co/papers/2412.02259",
            "abstract": "Current video generation models excel at generating short clips but still struggle with creating multi-shot, movie-like videos. Existing models trained on large-scale data on the back of rich computational resources are unsurprisingly inadequate for maintaining a logical storyline and visual consistency across multiple shots of a cohesive script since they are often trained with a single-shot objective. To this end, we propose VideoGen-of-Thought (VGoT), a collaborative and training-free architecture designed specifically for multi-shot video generation. VGoT is designed with three goals in mind as follows. Multi-Shot Video Generation: We divide the video generation process into a structured, modular sequence, including (1) Script Generation, which translates a curt story into detailed prompts for each shot; (2) Keyframe Generation, responsible for creating visually consistent keyframes faithful to character portrayals; and (3) Shot-Level Video Generation, which transforms information from scripts and keyframes into shots; (4) Smoothing Mechanism that ensures a consistent multi-shot output. Reasonable Narrative Design: Inspired by cinematic scriptwriting, our prompt generation approach spans five key domains, ensuring logical consistency, character development, and narrative flow across the entire video. Cross-Shot Consistency: We ensure temporal and identity consistency by leveraging identity-preserving (IP) embeddings across shots, which are automatically created from the narrative. Additionally, we incorporate a cross-shot smoothing mechanism, which integrates a reset boundary that effectively combines latent features from adjacent shots, resulting in smooth transitions and maintaining visual coherence throughout the video. Our experiments demonstrate that VGoT surpasses existing video generation methods in producing high-quality, coherent, multi-shot videos.",
            "score": 19,
            "issue_id": 933,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "5a007f38be3e3ba7",
            "authors": [
                "Mingzhe Zheng",
                "Yongqi Xu",
                "Haojian Huang",
                "Xuran Ma",
                "Yexin Liu",
                "Wenjie Shu",
                "Yatian Pang",
                "Feilong Tang",
                "Qifeng Chen",
                "Harry Yang",
                "Ser-Nam Lim"
            ],
            "affiliations": [
                "Everlyn AI",
                "Hong Kong University of Science and Technology",
                "National University of Singapore",
                "Peking University",
                "University of Central Florida",
                "University of Hong Kong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02259.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#story_generation",
                    "#games"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "VGoT: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ²ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ VideoGen-of-Thought (VGoT). Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. VGoT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµĞ¶ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğµ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VGoT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾."
                },
                "en": {
                    "title": "Revolutionizing Multi-Shot Video Generation with VGoT",
                    "desc": "The paper introduces VideoGen-of-Thought (VGoT), a novel architecture aimed at improving multi-shot video generation. Unlike traditional models that focus on single-shot outputs, VGoT employs a structured approach that includes script generation, keyframe creation, and shot-level video generation, ensuring a cohesive narrative. It emphasizes reasonable narrative design by incorporating principles from cinematic scriptwriting, which enhances character development and logical flow. Additionally, VGoT utilizes identity-preserving embeddings and a cross-shot smoothing mechanism to maintain visual consistency and smooth transitions across multiple shots."
                },
                "zh": {
                    "title": "å¤šé•œå¤´è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´",
                    "desc": "å½“å‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç”ŸæˆçŸ­ç‰‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åˆ›å»ºå¤šé•œå¤´ã€ç”µå½±èˆ¬çš„è§†é¢‘æ—¶ä»ç„¶å­˜åœ¨å›°éš¾ã€‚ç°æœ‰æ¨¡å‹é€šå¸¸åªé’ˆå¯¹å•é•œå¤´ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œå› æ­¤åœ¨ä¿æŒé€»è¾‘æ•…äº‹çº¿å’Œè§†è§‰ä¸€è‡´æ€§æ–¹é¢æ˜¾å¾—ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†VideoGen-of-Thoughtï¼ˆVGoTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºå¤šé•œå¤´è§†é¢‘ç”Ÿæˆè®¾è®¡çš„åä½œå’Œæ— è®­ç»ƒæ¶æ„ã€‚VGoTé€šè¿‡è„šæœ¬ç”Ÿæˆã€å…³é”®å¸§ç”Ÿæˆå’Œé•œå¤´çº§è§†é¢‘ç”Ÿæˆç­‰æ¨¡å—åŒ–æ­¥éª¤ï¼Œç¡®ä¿äº†åˆç†çš„å™äº‹è®¾è®¡å’Œè·¨é•œå¤´çš„ä¸€è‡´æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01981",
            "title": "Free Process Rewards without Process Labels",
            "url": "https://huggingface.co/papers/2412.01981",
            "abstract": "Different from its counterpart outcome reward models (ORMs), which evaluate the entire responses, a process reward model (PRM) scores a reasoning trajectory step by step, providing denser and more fine grained rewards. However, training a PRM requires labels annotated at every intermediate step, presenting significant challenges for both manual and automatic data collection. This paper aims to address this challenge. Both theoretically and empirically, we show that an implicit PRM can be obtained at no additional cost, by simply training an ORM on the cheaper response-level labels. The only assumption is to parameterize the outcome reward as the log-likelihood ratios of the policy and reference models, which can be optimized regardless of the specific choice of loss objectives. In experiments, we instantiate our implicit PRMs with various objectives and evaluate their performance on MATH. We show that our implicit PRM outperforms a strong MCTS-based baseline \\'a la Math-Shepherd using less than 1/38 of the training data. Its performance can be further improved with majority voting. We further find that scaling up instructions and responses benefits our implicit PRM, and the latter brings a larger gain. Particularly, we find that our implicit PRM, when instantiated with the cross-entropy (CE) loss, is more data-efficient and can keep improving generation models even when trained with only one response per instruction, the setup that suffers from extreme data scarcity and imbalance. Further, instructions should be relevant to downstream tasks while the diversity of responses does not bring gains. Surprisingly, training on extra Math-Shepherd step labels brings no further improvements to our implicit PRM trained on only outcome data. We hope that our work will encourage a rethinking of PRM training approaches and contribute to making training PRMs more accessible.",
            "score": 14,
            "issue_id": 933,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "13434e4f301a0d88",
            "authors": [
                "Lifan Yuan",
                "Wendi Li",
                "Huayu Chen",
                "Ganqu Cui",
                "Ning Ding",
                "Kaiyan Zhang",
                "Bowen Zhou",
                "Zhiyuan Liu",
                "Hao Peng"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "Tsinghua University",
                "University of Illinois Urbana-Champaign"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01981.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#math",
                    "#training",
                    "#reasoning",
                    "#data",
                    "#low_resource"
                ],
                "emoji": "ğŸ§ ",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ PRM Ğ±ĞµĞ· Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ (PRM) Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµÑĞ²Ğ½ÑƒÑ PRM Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ (ORM) Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑˆĞµĞ²Ñ‹Ñ… Ğ¼ĞµÑ‚ĞºĞ°Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ MATH Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ¹Ğ·Ğ»Ğ°Ğ¹Ğ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MCTS, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ 1/38 Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ¹ PRM."
                },
                "en": {
                    "title": "Unlocking Efficient Training for Process Reward Models",
                    "desc": "This paper introduces a novel approach to training process reward models (PRMs) that score reasoning steps individually, as opposed to outcome reward models (ORMs) which evaluate entire responses. The authors propose that an implicit PRM can be derived from an ORM trained on simpler response-level labels, thus avoiding the need for detailed step-by-step annotations. Through theoretical and empirical analysis, they demonstrate that this implicit PRM can achieve superior performance on tasks like MATH, using significantly less training data than traditional methods. The findings suggest that optimizing the outcome reward as log-likelihood ratios enhances data efficiency and model performance, even in scenarios with limited training examples."
                },
                "zh": {
                    "title": "éšå¼è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼šé«˜æ•ˆè®­ç»ƒçš„æ–°æ€è·¯",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ï¼Œä¸ä¼ ç»Ÿçš„ç»“æœå¥–åŠ±æ¨¡å‹ï¼ˆORMï¼‰ä¸åŒï¼ŒPRMèƒ½å¤Ÿé€æ­¥è¯„ä¼°æ¨ç†è¿‡ç¨‹ï¼Œæä¾›æ›´ç»†è‡´çš„å¥–åŠ±ã€‚ç„¶è€Œï¼Œè®­ç»ƒPRMéœ€è¦åœ¨æ¯ä¸ªä¸­é—´æ­¥éª¤éƒ½æœ‰æ ‡æ³¨ï¼Œè¿™åœ¨æ•°æ®æ”¶é›†ä¸Šé¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡è®­ç»ƒORMå¹¶ä½¿ç”¨å“åº”çº§åˆ«çš„æ ‡ç­¾ï¼Œå¯ä»¥åœ¨æ²¡æœ‰é¢å¤–æˆæœ¬çš„æƒ…å†µä¸‹è·å¾—éšå¼PRMã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œéšå¼PRMåœ¨æ•°æ®æ•ˆç‡å’Œæ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02611",
            "title": "AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?",
            "url": "https://huggingface.co/papers/2412.02611",
            "abstract": "Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini 1.5 Pro, and Reka Core, have expanded their capabilities to include vision and audio modalities. While these models demonstrate impressive performance across a wide range of audio-visual applications, our proposed DeafTest reveals that MLLMs often struggle with simple tasks humans find trivial: 1) determining which of two sounds is louder, and 2) determining which of two sounds has a higher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a comprehensive audio-visual benchmark designed to assess whether those MLLMs can truly understand the audio-visual information. This benchmark encompasses 4,555 carefully crafted problems, each incorporating text, visual, and audio components. To successfully infer answers, models must effectively leverage clues from both visual and audio inputs. To ensure precise and objective evaluation of MLLM responses, we have structured the questions as multiple-choice, eliminating the need for human evaluation or LLM-assisted assessment. We benchmark a series of closed-source and open-source models and summarize the observations. By revealing the limitations of current models, we aim to provide useful insight for future dataset collection and model development.",
            "score": 11,
            "issue_id": 933,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "f63565048b4948b4",
            "authors": [
                "Kaixiong Gong",
                "Kaituo Feng",
                "Bohao Li",
                "Yibing Wang",
                "Mofan Cheng",
                "Shijia Yang",
                "Jiaming Han",
                "Benyou Wang",
                "Yutong Bai",
                "Zhuoran Yang",
                "Xiangyu Yue"
            ],
            "affiliations": [
                "CUHK (SZ)",
                "CUHK MMLab",
                "Stanford University",
                "UC Berkeley",
                "Yale University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02611.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#benchmark",
                    "#interpretability",
                    "#multimodal",
                    "#games"
                ],
                "emoji": "ğŸ§",
                "ru": {
                    "title": "Ğ¡Ğ»Ñ‹ÑˆĞ°Ñ‚ Ğ»Ğ¸ Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ´ÑÑ‚?",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ DeafTest Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AV-Odyssey Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ MLLM Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ¾Ğ¼ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾Ñ‚Ñ‹ Ğ·Ğ²ÑƒĞºĞ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 4,555 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ÑĞ´Ğ° Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."
                },
                "en": {
                    "title": "Unveiling the Limits of Multimodal Models with AV-Odyssey Bench",
                    "desc": "This paper introduces DeafTest, a benchmark that highlights the limitations of multimodal large language models (MLLMs) in understanding basic audio tasks that humans find easy. The authors present AV-Odyssey Bench, which consists of 4,555 problems that require models to analyze and integrate audio, visual, and text information. The benchmark is designed to objectively evaluate MLLM performance through multiple-choice questions, avoiding reliance on human judgment. By assessing various models, the study aims to shed light on the shortcomings of current MLLMs and guide future improvements in model training and dataset creation."
                },
                "zh": {
                    "title": "æ­ç¤ºå¤šæ¨¡æ€æ¨¡å‹çš„å±€é™æ€§",
                    "desc": "æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¦‚GPT-4oã€Gemini 1.5 Proå’ŒReka Coreï¼Œæ‰©å±•äº†å…¶åœ¨è§†è§‰å’ŒéŸ³é¢‘æ–¹é¢çš„èƒ½åŠ›ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨å¤šç§éŸ³é¢‘-è§†è§‰åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æˆ‘ä»¬çš„DeafTestæ˜¾ç¤ºï¼ŒMLLMsåœ¨ä¸€äº›äººç±»è®¤ä¸ºç®€å•çš„ä»»åŠ¡ä¸Šå¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œä¾‹å¦‚åˆ¤æ–­ä¸¤ä¸ªå£°éŸ³å“ªä¸ªæ›´å“å’Œå“ªä¸ªéŸ³è°ƒæ›´é«˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†AV-Odyssey Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„éŸ³é¢‘-è§†è§‰åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°è¿™äº›MLLMsæ˜¯å¦çœŸæ­£ç†è§£éŸ³é¢‘-è§†è§‰ä¿¡æ¯ã€‚è¯¥åŸºå‡†åŒ…å«4555ä¸ªç²¾å¿ƒè®¾è®¡çš„é—®é¢˜ï¼Œè¦æ±‚æ¨¡å‹æœ‰æ•ˆåˆ©ç”¨è§†è§‰å’ŒéŸ³é¢‘è¾“å…¥ä¸­çš„çº¿ç´¢ï¼Œä»¥å‡†ç¡®æ¨æ–­ç­”æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02632",
            "title": "Scaling Image Tokenizers with Grouped Spherical Quantization",
            "url": "https://huggingface.co/papers/2412.02632",
            "abstract": "Vision tokenizers have gained a lot of attraction due to their scalability and compactness; previous works depend on old-school GAN-based hyperparameters, biased comparisons, and a lack of comprehensive analysis of the scaling behaviours. To tackle those issues, we introduce Grouped Spherical Quantization (GSQ), featuring spherical codebook initialization and lookup regularization to constrain codebook latent to a spherical surface. Our empirical analysis of image tokenizer training strategies demonstrates that GSQ-GAN achieves superior reconstruction quality over state-of-the-art methods with fewer training iterations, providing a solid foundation for scaling studies. Building on this, we systematically examine the scaling behaviours of GSQ, specifically in latent dimensionality, codebook size, and compression ratios, and their impact on model performance. Our findings reveal distinct behaviours at high and low spatial compression levels, underscoring challenges in representing high-dimensional latent spaces. We show that GSQ can restructure high-dimensional latent into compact, low-dimensional spaces, thus enabling efficient scaling with improved quality. As a result, GSQ-GAN achieves a 16x down-sampling with a reconstruction FID (rFID) of 0.50.",
            "score": 5,
            "issue_id": 933,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "60eda94a31cded90",
            "authors": [
                "Jiangtao Wang",
                "Zhen Qin",
                "Yifan Zhang",
                "Vincent Tao Hu",
                "BjÃ¶rn Ommer",
                "Rania Briq",
                "Stefan Kesselheim"
            ],
            "affiliations": [
                "CompVis @ LMU Munich",
                "JÃ¼lich Supercomputing Centre",
                "TapTap",
                "Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02632.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#inference",
                    "#cv",
                    "#data"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "GSQ: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ - Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ°Ñ Ğ¡Ñ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (GSQ). GSQ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ½Ğ¸Ğ³Ğ¸ Ğ¸ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ½Ğ¸Ğ³Ğ¸ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ GSQ-GAN Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ GSQ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¸Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²."
                },
                "en": {
                    "title": "Efficient Image Processing with Grouped Spherical Quantization",
                    "desc": "This paper introduces a new method called Grouped Spherical Quantization (GSQ) for improving vision tokenizers, which are tools used to process images efficiently. GSQ uses a special technique to initialize and regularize a spherical codebook, helping to keep the data organized on a spherical surface. The authors demonstrate that GSQ-GAN, a model based on this method, can reconstruct images with high quality while requiring fewer training iterations compared to existing methods. Their analysis also reveals how different factors like latent dimensionality and codebook size affect the model's performance, particularly in handling high-dimensional data efficiently."
                },
                "zh": {
                    "title": "åˆ†ç»„çƒé¢é‡åŒ–ï¼šé«˜æ•ˆçš„è§†è§‰æ ‡è®°å™¨æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†è§‰æ ‡è®°å™¨æ–¹æ³•ï¼Œç§°ä¸ºåˆ†ç»„çƒé¢é‡åŒ–ï¼ˆGSQï¼‰ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ–¹æ³•ä¸­çš„ä¸€äº›é—®é¢˜ã€‚GSQé€šè¿‡çƒé¢ä»£ç æœ¬åˆå§‹åŒ–å’ŒæŸ¥æ‰¾æ­£åˆ™åŒ–ï¼Œé™åˆ¶äº†ä»£ç æœ¬æ½œåœ¨ç©ºé—´åœ¨çƒé¢ä¸Šçš„åˆ†å¸ƒã€‚æˆ‘ä»¬çš„å®è¯åˆ†æè¡¨æ˜ï¼ŒGSQ-GANåœ¨é‡å»ºè´¨é‡ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶ä¸”è®­ç»ƒè¿­ä»£æ¬¡æ•°æ›´å°‘ã€‚ç ”ç©¶è¿˜ç³»ç»Ÿåœ°è€ƒå¯Ÿäº†GSQåœ¨æ½œåœ¨ç»´åº¦ã€ä»£ç æœ¬å¤§å°å’Œå‹ç¼©æ¯”ç­‰æ–¹é¢çš„æ‰©å±•è¡Œä¸ºï¼Œæ­ç¤ºäº†é«˜ç»´æ½œåœ¨ç©ºé—´è¡¨ç¤ºçš„æŒ‘æˆ˜ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01292",
            "title": "LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences",
            "url": "https://huggingface.co/papers/2412.01292",
            "abstract": "Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing attention, which is crucial for developing embodied AI within 3D scenes, such as visual navigation and embodied question answering. Due to the high density of visual features, especially in large 3D scenes, accurately locating task-relevant visual information is challenging. Existing works attempt to segment all objects and consider their features as scene representations. However, these task-agnostic object features include much redundant information and missing details for the task-relevant area. To tackle these problems, we propose LSceneLLM, an adaptive framework that automatically identifies task-relevant areas by leveraging LLM's visual preference for different tasks, followed by a plug-and-play scene magnifier module to capture fine-grained details in focused areas. Specifically, a dense token selector examines the attention map of LLM to identify visual preferences for the instruction input. It then magnifies fine-grained details of the focusing area. An adaptive self-attention module is leveraged to fuse the coarse-grained and selected fine-grained visual information. To comprehensively evaluate the large scene understanding ability of 3D-VLMs, we further introduce a cross-room understanding benchmark, XR-Scene, which contains a series of large scene understanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption. Experiments show that our method surpasses existing methods on both large scene understanding and existing scene understanding benchmarks. Plunging our scene magnifier module into the existing 3D-VLMs also brings significant improvement.",
            "score": 4,
            "issue_id": 933,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "e8f8ddd05e13e9ef",
            "authors": [
                "Hongyan Zhi",
                "Peihao Chen",
                "Junyan Li",
                "Shuailei Ma",
                "Xinyu Sun",
                "Tianhang Xiang",
                "Yinjie Lei",
                "Mingkui Tan",
                "Chuang Gan"
            ],
            "affiliations": [
                "MIT-IBM Watson AI Lab",
                "Northeastern University",
                "Pazhou Laboratory",
                "Sichuan University",
                "South China University of Technology",
                "Tencent Robotics X",
                "UMass Amherst"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01292.jpg",
            "data": {
                "categories": [
                    "#architecture",
                    "#benchmark",
                    "#3d",
                    "#multimodal"
                ],
                "emoji": "ğŸ–¼ï¸",
                "ru": {
                    "title": "Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ 3D-ÑÑ†ĞµĞ½ Ñ LSceneLLM",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 3D Vision-Language Models (3D-VLMs) Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ AI Ğ² 3D-ÑÑ†ĞµĞ½Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. Ğ˜Ğ·-Ğ·Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… 3D-ÑÑ†ĞµĞ½Ğ°Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° LSceneLLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ LLM Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ñˆ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑÑ†ĞµĞ½ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ 3D-VLMs."
                },
                "en": {
                    "title": "Enhancing 3D Scene Understanding with LSceneLLM",
                    "desc": "This paper introduces LSceneLLM, a novel framework designed to enhance 3D Vision-Language Models (3D-VLMs) for better understanding of large 3D scenes. It addresses the challenge of identifying task-relevant visual information amidst the dense features present in these scenes. By utilizing a dense token selector and an adaptive self-attention module, the framework effectively magnifies important details while reducing redundant information. The authors also present a new benchmark, XR-Scene, to evaluate the performance of 3D-VLMs on various large scene understanding tasks, demonstrating that their approach significantly outperforms existing methods."
                },
                "zh": {
                    "title": "è‡ªé€‚åº”3Dè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæå‡åœºæ™¯ç†è§£èƒ½åŠ›",
                    "desc": "3Dè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆ3D-VLMsï¼‰çš„ç ”ç©¶è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œè¿™å¯¹åœ¨3Dåœºæ™¯ä¸­å‘å±•å…·èº«äººå·¥æ™ºèƒ½è‡³å…³é‡è¦ï¼Œå¦‚è§†è§‰å¯¼èˆªå’Œå…·èº«é—®ç­”ã€‚ç”±äº3Dåœºæ™¯ä¸­è§†è§‰ç‰¹å¾çš„é«˜å¯†åº¦ï¼Œå‡†ç¡®å®šä½ä¸ä»»åŠ¡ç›¸å…³çš„è§†è§‰ä¿¡æ¯å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•å°è¯•å¯¹æ‰€æœ‰å¯¹è±¡è¿›è¡Œåˆ†å‰²ï¼Œå¹¶å°†å…¶ç‰¹å¾è§†ä¸ºåœºæ™¯è¡¨ç¤ºï¼Œä½†è¿™äº›ä¸ä»»åŠ¡æ— å…³çš„å¯¹è±¡ç‰¹å¾åŒ…å«å¤§é‡å†—ä½™ä¿¡æ¯å’Œç¼ºå¤±çš„ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LSceneLLMï¼Œä¸€ä¸ªè‡ªé€‚åº”æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹ä¸åŒä»»åŠ¡çš„è§†è§‰åå¥½ï¼Œè‡ªåŠ¨è¯†åˆ«ä¸ä»»åŠ¡ç›¸å…³çš„åŒºåŸŸï¼Œå¹¶é€šè¿‡å¯æ’æ‹”çš„åœºæ™¯æ”¾å¤§æ¨¡å—æ•æ‰èšç„¦åŒºåŸŸçš„ç»†ç²’åº¦ç»†èŠ‚ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19067",
            "title": "MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation",
            "url": "https://huggingface.co/papers/2411.19067",
            "abstract": "Referring Image Segmentation (RIS) is an advanced vision-language task that involves identifying and segmenting objects within an image as described by free-form text descriptions. While previous studies focused on aligning visual and language features, exploring training techniques, such as data augmentation, remains underexplored. In this work, we explore effective data augmentation for RIS and propose a novel training framework called Masked Referring Image Segmentation (MaskRIS). We observe that the conventional image augmentations fall short of RIS, leading to performance degradation, while simple random masking significantly enhances the performance of RIS. MaskRIS uses both image and text masking, followed by Distortion-aware Contextual Learning (DCL) to fully exploit the benefits of the masking strategy. This approach can improve the model's robustness to occlusions, incomplete information, and various linguistic complexities, resulting in a significant performance improvement. Experiments demonstrate that MaskRIS can easily be applied to various RIS models, outperforming existing methods in both fully supervised and weakly supervised settings. Finally, MaskRIS achieves new state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg datasets. Code is available at https://github.com/naver-ai/maskris.",
            "score": 3,
            "issue_id": 934,
            "pub_date": "2024-11-28",
            "pub_date_card": {
                "ru": "28 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 28",
                "zh": "11æœˆ28æ—¥"
            },
            "hash": "74d4a17af3574a5d",
            "authors": [
                "Minhyun Lee",
                "Seungho Lee",
                "Song Park",
                "Dongyoon Han",
                "Byeongho Heo",
                "Hyunjung Shim"
            ],
            "affiliations": [
                "KAIST AI",
                "NAVER AI Lab",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19067.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#cv",
                    "#survey",
                    "#dataset",
                    "#training"
                ],
                "emoji": "ğŸ­",
                "ru": {
                    "title": "ĞœĞ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ (RIS). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MaskRIS, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸ÑĞ¼, Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MaskRIS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ»Ğ°Ğ±Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸."
                },
                "en": {
                    "title": "Enhancing Referring Image Segmentation with Masking Techniques",
                    "desc": "Referring Image Segmentation (RIS) is a task that combines visual and language understanding to identify and segment objects in images based on text descriptions. This paper introduces a new training framework called Masked Referring Image Segmentation (MaskRIS), which focuses on effective data augmentation techniques for RIS. The authors found that traditional image augmentations were inadequate, while their method of random masking significantly improved performance. MaskRIS enhances model robustness against occlusions and linguistic variations, achieving state-of-the-art results on several benchmark datasets."
                },
                "zh": {
                    "title": "Masked Referring Image Segmentationï¼šæå‡å›¾åƒåˆ†å‰²æ€§èƒ½çš„æ–°æ–¹æ³•",
                    "desc": "å¼•ç”¨å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰æ˜¯ä¸€ç§å…ˆè¿›çš„è§†è§‰-è¯­è¨€ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®è‡ªç”±å½¢å¼çš„æ–‡æœ¬æè¿°è¯†åˆ«å’Œåˆ†å‰²å›¾åƒä¸­çš„å¯¹è±¡ã€‚æœ¬æ–‡æ¢è®¨äº†æœ‰æ•ˆçš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œç§°ä¸ºMasked Referring Image Segmentationï¼ˆMaskRISï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„å›¾åƒå¢å¼ºæ–¹æ³•åœ¨RISä¸­æ•ˆæœä¸ä½³ï¼Œè€Œç®€å•çš„éšæœºé®ç½©æ˜¾è‘—æå‡äº†RISçš„æ€§èƒ½ã€‚MaskRISç»“åˆäº†å›¾åƒå’Œæ–‡æœ¬é®ç½©ï¼Œå¹¶é‡‡ç”¨äº†å¤±çœŸæ„ŸçŸ¥ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆDCLï¼‰ï¼Œä»è€Œæé«˜äº†æ¨¡å‹å¯¹é®æŒ¡ã€ä¸å®Œæ•´ä¿¡æ¯å’Œè¯­è¨€å¤æ‚æ€§çš„é²æ£’æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.02592",
            "title": "OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation",
            "url": "https://huggingface.co/papers/2412.02592",
            "abstract": "Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge to reduce hallucinations and incorporate up-to-date information without retraining. As an essential part of RAG, external knowledge bases are commonly built by extracting structured data from unstructured PDF documents using Optical Character Recognition (OCR). However, given the imperfect prediction of OCR and the inherent non-uniform representation of structured data, knowledge bases inevitably contain various OCR noises. In this paper, we introduce OHRBench, the first benchmark for understanding the cascading impact of OCR on RAG systems. OHRBench includes 350 carefully selected unstructured PDF documents from six real-world RAG application domains, along with Q&As derived from multimodal elements in documents, challenging existing OCR solutions used for RAG To better understand OCR's impact on RAG systems, we identify two primary types of OCR noise: Semantic Noise and Formatting Noise and apply perturbation to generate a set of structured data with varying degrees of each OCR noise. Using OHRBench, we first conduct a comprehensive evaluation of current OCR solutions and reveal that none is competent for constructing high-quality knowledge bases for RAG systems. We then systematically evaluate the impact of these two noise types and demonstrate the vulnerability of RAG systems. Furthermore, we discuss the potential of employing Vision-Language Models (VLMs) without OCR in RAG systems. Code: https://github.com/opendatalab/OHR-Bench",
            "score": 2,
            "issue_id": 937,
            "pub_date": "2024-12-03",
            "pub_date_card": {
                "ru": "3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 3",
                "zh": "12æœˆ3æ—¥"
            },
            "hash": "91dbac114744b1e9",
            "authors": [
                "Junyuan Zhang",
                "Qintong Zhang",
                "Bin Wang",
                "Linke Ouyang",
                "Zichen Wen",
                "Ying Li",
                "Ka-Ho Chow",
                "Conghui He",
                "Wentao Zhang"
            ],
            "affiliations": [
                "Beihang University",
                "Peking University",
                "Shanghai AI Laboratory",
                "Shanghai Jiaotong University",
                "The University of HongKong"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.02592.jpg",
            "data": {
                "categories": [
                    "#hallucinations",
                    "#dataset",
                    "#multimodal",
                    "#benchmark",
                    "#rag",
                    "#optimization",
                    "#survey"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "OHRBench: Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ OCR Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ RAG",
                    "desc": "OHRBench - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² (OCR) Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (RAG). ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 350 Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… PDF-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· ÑˆĞµÑÑ‚Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ RAG, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ğ´Ğ²Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ° ÑˆÑƒĞ¼Ğ° OCR: ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ½Ñ‹Ğ¹, Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° ÑˆÑƒĞ¼Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼ RAG Ğº Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼ OCR Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ±ĞµĞ· OCR Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… RAG."
                },
                "en": {
                    "title": "Enhancing RAG: Understanding OCR's Impact with OHRBench",
                    "desc": "This paper presents OHRBench, a benchmark designed to evaluate the effects of Optical Character Recognition (OCR) errors on Retrieval-Augmented Generation (RAG) systems. It identifies two main types of OCR noise: Semantic Noise, which affects the meaning of the extracted data, and Formatting Noise, which impacts the structure and presentation. The study reveals that current OCR solutions are inadequate for creating high-quality knowledge bases necessary for effective RAG applications. Additionally, it explores the potential of using Vision-Language Models (VLMs) as an alternative to traditional OCR methods in RAG systems."
                },
                "zh": {
                    "title": "æ­ç¤ºOCRå¯¹RAGç³»ç»Ÿçš„å½±å“",
                    "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†OHRBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºç†è§£å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿå½±å“çš„åŸºå‡†ã€‚ç ”ç©¶å‘ç°ï¼ŒOCRåœ¨å¤„ç†éç»“æ„åŒ–PDFæ–‡æ¡£æ—¶ä¼šå¼•å…¥è¯­ä¹‰å™ªå£°å’Œæ ¼å¼å™ªå£°ï¼Œå¯¼è‡´çŸ¥è¯†åº“è´¨é‡ä¸‹é™ã€‚é€šè¿‡å¯¹350ä¸ªçœŸå®ä¸–ç•Œåº”ç”¨é¢†åŸŸçš„æ–‡æ¡£è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºç°æœ‰çš„OCRè§£å†³æ–¹æ¡ˆæ— æ³•æœ‰æ•ˆæ„å»ºé«˜è´¨é‡çš„çŸ¥è¯†åº“ã€‚æœ€åï¼Œè®ºæ–‡æ¢è®¨äº†åœ¨RAGç³»ç»Ÿä¸­ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è€Œä¸ä¾èµ–OCRçš„æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2411.19542",
            "title": "A dynamic parallel method for performance optimization on hybrid CPUs",
            "url": "https://huggingface.co/papers/2411.19542",
            "abstract": "The AIPC concept is gaining popularity, and more and more hybrid CPUs will be running AI models on client devices. However, the current AI inference framework overlooks the imbalanced hardware capability of hybrid CPUs, leading to low inference performance. To address this issue, we have introduced a dynamic parallel method for hybrid CPUs, which significantly increases LLM inference performance by balancing the workload for each core of a hybrid CPU before the parallel work starts. This method has enabled Neural Speed to achieve more than 90% (on average) of memory bandwidth on two hybrid Intel CPUs.",
            "score": 2,
            "issue_id": 936,
            "pub_date": "2024-11-29",
            "pub_date_card": {
                "ru": "29 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
                "en": "November 29",
                "zh": "11æœˆ29æ—¥"
            },
            "hash": "27226211eddf71d4",
            "authors": [
                "Luo Yu",
                "Liu Yucheng",
                "Shen Haihao"
            ],
            "affiliations": [
                "Intel Corporation"
            ],
            "pdf_title_img": "assets/pdf/title_img/2411.19542.jpg",
            "data": {
                "categories": [
                    "#inference",
                    "#architecture",
                    "#optimization"
                ],
                "emoji": "ğŸš€",
                "ru": {
                    "title": "Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ˜Ğ˜ Ğ½Ğ° Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… CPU: Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ¾Ğ², Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ˜Ğ˜ Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµÑ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ´ĞµÑ€ Ğ² Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… CPU, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ´Ñ€Ğ°Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° LLM. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ, Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Neural Speed Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ğ±Ğ¾Ğ»ĞµĞµ 90% Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ°Ñ… Intel."
                },
                "en": {
                    "title": "Boosting AI Inference with Dynamic Workload Balancing on Hybrid CPUs",
                    "desc": "The paper discusses the growing trend of using hybrid CPUs for running AI models on client devices. It highlights a problem where existing AI inference frameworks do not effectively utilize the varying hardware capabilities of these hybrid CPUs, resulting in suboptimal performance. To solve this, the authors propose a dynamic parallel method that balances the workload across CPU cores before starting parallel processing. This approach has led to significant improvements in inference performance, achieving over 90% memory bandwidth utilization on hybrid Intel CPUs."
                },
                "zh": {
                    "title": "åŠ¨æ€å¹³è¡¡ï¼Œæå‡AIæ¨ç†æ€§èƒ½ï¼",
                    "desc": "AIPCæ¦‚å¿µè¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œè¶Šæ¥è¶Šå¤šçš„æ··åˆCPUå°†åœ¨å®¢æˆ·ç«¯è®¾å¤‡ä¸Šè¿è¡ŒAIæ¨¡å‹ã€‚ç„¶è€Œï¼Œç›®å‰çš„AIæ¨ç†æ¡†æ¶å¿½è§†äº†æ··åˆCPUçš„ä¸å¹³è¡¡ç¡¬ä»¶èƒ½åŠ›ï¼Œå¯¼è‡´æ¨ç†æ€§èƒ½ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€å¹¶è¡Œæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ··åˆCPUçš„LLMæ¨ç†æ€§èƒ½ï¼Œé€šè¿‡åœ¨å¹¶è¡Œå·¥ä½œå¼€å§‹ä¹‹å‰å¹³è¡¡æ¯ä¸ªæ ¸å¿ƒçš„å·¥ä½œè´Ÿè½½ã€‚è¯¥æ–¹æ³•ä½¿Neural Speedåœ¨ä¸¤æ¬¾æ··åˆIntel CPUä¸Šå®ç°äº†è¶…è¿‡90%çš„å†…å­˜å¸¦å®½ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2412.01558",
            "title": "VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval",
            "url": "https://huggingface.co/papers/2412.01558",
            "abstract": "Video Highlight Detection and Moment Retrieval (HD/MR) are essential in video analysis. Recent joint prediction transformer models often overlook their cross-task dynamics and video-text alignment and refinement. Moreover, most models typically use limited, uni-directional attention mechanisms, resulting in weakly integrated representations and suboptimal performance in capturing the interdependence between video and text modalities. Although large-language and vision-language models (LLM/LVLMs) have gained prominence across various domains, their application in this field remains relatively underexplored. Here we propose VideoLights, a novel HD/MR framework addressing these limitations through (i) Convolutional Projection and Feature Refinement modules with an alignment loss for better video-text feature alignment, (ii) Bi-Directional Cross-Modal Fusion network for strongly coupled query-aware clip representations, and (iii) Uni-directional joint-task feedback mechanism enhancing both tasks through correlation. In addition, (iv) we introduce hard positive/negative losses for adaptive error penalization and improved learning, and (v) leverage LVLMs like BLIP-2 for enhanced multimodal feature integration and intelligent pretraining using synthetic data generated from LVLMs. Comprehensive experiments on QVHighlights, TVSum, and Charades-STA benchmarks demonstrate state-of-the-art performance. Codes and models are available at https://github.com/dpaul06/VideoLights .",
            "score": 1,
            "issue_id": 935,
            "pub_date": "2024-12-02",
            "pub_date_card": {
                "ru": "2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
                "en": "December 2",
                "zh": "12æœˆ2æ—¥"
            },
            "hash": "12235c4ebf26fe4a",
            "authors": [
                "Dhiman Paul",
                "Md Rizwan Parvez",
                "Nabeel Mohammed",
                "Shafin Rahman"
            ],
            "affiliations": [
                "Department of Electrical and Computer Engineering, North South University, Dhaka, Bangladesh",
                "Qatar Computing Research Institute (QCRI), Qatar"
            ],
            "pdf_title_img": "assets/pdf/title_img/2412.01558.jpg",
            "data": {
                "categories": [
                    "#video",
                    "#games",
                    "#synthetic",
                    "#architecture",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "VideoLights: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoLights - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ Ğ½Ğ¸Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VideoLights Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."
                },
                "en": {
                    "title": "Enhancing Video-Text Integration with VideoLights",
                    "desc": "This paper presents VideoLights, a new framework for Video Highlight Detection and Moment Retrieval that improves the integration of video and text data. It introduces several innovative components, including Convolutional Projection and Feature Refinement modules to enhance video-text alignment, and a Bi-Directional Cross-Modal Fusion network for better representation of clips. The framework also employs a Uni-directional joint-task feedback mechanism to strengthen the relationship between the two tasks and introduces hard positive/negative losses for more effective learning. The results show that VideoLights achieves state-of-the-art performance on multiple benchmarks, demonstrating its effectiveness in video analysis."
                },
                "zh": {
                    "title": "VideoLightsï¼šæå‡è§†é¢‘ä¸æ–‡æœ¬åˆ†æçš„å…¨æ–°æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºVideoLightsçš„è§†é¢‘é«˜äº®æ£€æµ‹å’Œæ—¶åˆ»æ£€ç´¢æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨è§†é¢‘ä¸æ–‡æœ¬å¯¹é½å’Œè·¨ä»»åŠ¡åŠ¨æ€æ–¹é¢çš„ä¸è¶³ã€‚æˆ‘ä»¬å¼•å…¥äº†å·ç§¯æŠ•å½±å’Œç‰¹å¾ç²¾ç‚¼æ¨¡å—ï¼Œä»¥æé«˜è§†é¢‘å’Œæ–‡æœ¬ç‰¹å¾çš„å¯¹é½æ•ˆæœï¼Œå¹¶é‡‡ç”¨åŒå‘è·¨æ¨¡æ€èåˆç½‘ç»œæ¥å¢å¼ºæŸ¥è¯¢æ„ŸçŸ¥çš„ç‰‡æ®µè¡¨ç¤ºã€‚é€šè¿‡å•å‘è”åˆä»»åŠ¡åé¦ˆæœºåˆ¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæå‡ä¸¤ä¸ªä»»åŠ¡ä¹‹é—´çš„ç›¸å…³æ€§ï¼ŒåŒæ—¶å¼•å…¥ç¡¬æ­£è´ŸæŸå¤±ä»¥æ”¹å–„å­¦ä¹ æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoLightsåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-12-03.html",
    "link_next": "2024-12-05.html",
    "link_month": "2024-12.html",
    "short_date_prev": {
        "ru": "03.12",
        "en": "12/03",
        "zh": "12æœˆ3æ—¥"
    },
    "short_date_next": {
        "ru": "05.12",
        "en": "12/05",
        "zh": "12æœˆ5æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 2,
        "#benchmark": 5,
        "#agents": 0,
        "#cv": 2,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 1,
        "#plp": 0,
        "#inference": 2,
        "#3d": 1,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 0,
        "#training": 4,
        "#robotics": 0,
        "#agi": 0,
        "#games": 3,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 4,
        "#survey": 2,
        "#diffusion": 0,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 1,
        "#long_context": 0,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 1
    },
    "zh": {
        "text": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç”Ÿæˆäº¤é”™çš„å›¾åƒ-æ–‡æœ¬å†…å®¹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ç”±äºæ•°æ®é‡å’Œå¤šæ ·æ€§çš„é™åˆ¶ï¼Œæ— æ³•å……åˆ†è¯„ä¼°è¿™äº›æ–¹æ³•ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†GATE OpenINGï¼ˆOpenINGï¼‰ï¼Œä¸€ä¸ªåŒ…å«5,400ä¸ªé«˜è´¨é‡äººå·¥æ ‡æ³¨å®ä¾‹çš„å…¨é¢åŸºå‡†ï¼Œæ¶µç›–äº†56ä¸ªçœŸå®ä¸–ç•Œä»»åŠ¡ã€‚OpenINGæ¶µç›–äº†å„ç§æ—¥å¸¸åœºæ™¯ï¼Œå¦‚æ—…è¡ŒæŒ‡å—ã€è®¾è®¡å’Œå¤´è„‘é£æš´ï¼Œä¸ºæŒ‘æˆ˜æ€§çš„äº¤é”™ç”Ÿæˆæ–¹æ³•æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å¹³å°ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜ä»‹ç»äº†IntJudgeï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¼€æ”¾å¼å¤šæ¨¡æ€ç”Ÿæˆæ–¹æ³•çš„è¯„ä¼°æ¨¡å‹ã€‚",
        "title": "GATE OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation",
        "pinyin": "è¿™ç¯‡æ–‡ç« ä»‹ç»äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç”Ÿæˆäº¤é”™çš„å›¾åƒ-æ–‡æœ¬å†…å®¹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ç”±äºæ•°æ®é‡å’Œå¤šæ ·æ€§çš„é™åˆ¶ï¼Œæ— æ³•å……åˆ†è¯„ä¼°è¿™äº›æ–¹æ³•ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†GATE OpenINGï¼ˆOpenINGï¼‰ï¼Œä¸€ä¸ªåŒ…å«5,400ä¸ªé«˜è´¨é‡äººå·¥æ ‡æ³¨å®ä¾‹çš„å…¨é¢åŸºå‡†ï¼Œæ¶µç›–äº†56ä¸ªçœŸå®ä¸–ç•Œä»»åŠ¡ã€‚OpenINGæ¶µç›–äº†å„ç§æ—¥å¸¸åœºæ™¯ï¼Œå¦‚æ—…è¡ŒæŒ‡å—ã€è®¾è®¡å’Œå¤´è„‘é£æš´ï¼Œä¸ºæŒ‘æˆ˜æ€§çš„äº¤é”™ç”Ÿæˆæ–¹æ³•æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å¹³å°ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜ä»‹ç»äº†IntJudgeï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¼€æ”¾å¼å¤šæ¨¡æ€ç”Ÿæˆæ–¹æ³•çš„è¯„ä¼°æ¨¡å‹ã€‚\n\nzhÃ¨ piÄn wÃ©n zhÄng jiÃ¨ shÃ o le duÅ mÃ³ tÃ i dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng (MLLMs) zÃ i shÃ¬ juÃ© lÇ jiÄ› hÃ© shÄ“ng chÃ©ng rÃ¨n wÃ¹ zhÅng de jÃ¬n zhÃ n. rÃ¡n Ã©r, shÄ“ng chÃ©ng jiÄo cuÃ² de tÃº xiÃ ng-wÃ©n bÄ›n nÃ¨i rÃ³ng rÃ©ng shÃ¬ yÄ« gÃ¨ tiÇo zhÃ n. xiÃ n yÇ’u de jÄ« zhÇ”n cÃ¨ shÃ¬ yÃ³u yÃº shÃ¹ jÃ¹ liÃ ng hÃ© duÅ yÃ ng xÃ¬ng de xiÃ n zhÃ¬, wÃº fÇ chÅng fÄ“n pÃ­ng gÅ« zhÃ¨ xiÄ“ fÄng fÇ. wÃ¨i le jiÄ› juÃ© zhÃ¨ gÃ¨ wÃ¨n tÃ­, zuÃ² zhÄ› tÃ­ chÅ« le GATE OpenING (OpenING), yÄ« gÃ¨ bÄo hÃ¡n 5,400 gÃ¨ gÄo zhÃ¬ liÃ ng rÃ©n gÅng biÄo zhÃ¹ shÃ­ lÃ¬ de quÃ¡n miÃ n jÄ« zhÇ”n, hÃ¡n gÇi le 56 gÃ¨ zhÄ“n shÃ­ shÃ¬ jiÃ¨ rÃ¨n wÃ¹. OpenING hÃ¡n gÇi le gÃ¨ zhÇ’ng rÃ¬ chÃ¡ng chÇng jÄ«ng, rÃº lÇš xÃ­ng zhÇ nÃ¡n, shÃ¨ jÃ¬ hÃ© tÃ³u nÇo fÄ“ng bÃ o, wÃ¨i tiÇo zhÃ n xÃ¬ng de jiÄo cuÃ² shÄ“ng chÃ©ng fÄng fÇ tÃ­ gÅng le yÄ« gÃ¨ qiÃ¡ng dÃ  de pÃ­ng tÃ¡i. cÇ wÃ i, zuÃ² zhÄ› hÃ¡i jiÃ¨ shÃ o le IntJudge, yÄ« gÃ¨ yÃ²ng yÃº pÃ­ng gÅ« kÄi fÃ ng shÃ¬ duÅ mÃ³ tÃ i shÄ“ng chÃ©ng fÄng fÇ de pÃ­ng gÅ« mÃ³ xÃ­ng.",
        "vocab": "[\n    {\"word\": \"å¤šæ¨¡æ€\", \"pinyin\": \"duÅ mÃ³ tÃ i\", \"trans\": \"multimodal\"},\n    {\"word\": \"å¤§è¯­è¨€æ¨¡å‹\", \"pinyin\": \"dÃ  yÇ” yÃ¡n mÃ³ xÃ­ng\", \"trans\": \"large language model\"},\n    {\"word\": \"è§†è§‰ç†è§£\", \"pinyin\": \"shÃ¬ juÃ© lÇ jiÄ›\", \"trans\": \"visual understanding\"},\n    {\"word\": \"ç”Ÿæˆ\", \"pinyin\": \"shÄ“ng chÃ©ng\", \"trans\": \"generation\"},\n    {\"word\": \"äº¤é”™\", \"pinyin\": \"jiÄo cuÃ²\", \"trans\": \"interleaved\"},\n    {\"word\": \"å›¾åƒ-æ–‡æœ¬\", \"pinyin\": \"tÃº xiÃ ng wÃ©n bÄ›n\", \"trans\": \"image-text\"},\n    {\"word\": \"æŒ‘æˆ˜\", \"pinyin\": \"tiÇo zhÃ n\", \"trans\": \"challenge\"},\n    {\"word\": \"åŸºå‡†æµ‹è¯•\", \"pinyin\": \"jÄ« zhÇ”n cÃ¨ shÃ¬\", \"trans\": \"benchmark test\"},\n    {\"word\": \"æ•°æ®é‡\", \"pinyin\": \"shÃ¹ jÃ¹ liÃ ng\", \"trans\": \"data volume\"},\n    {\"word\": \"å¤šæ ·æ€§\", \"pinyin\": \"duÅ yÃ ng xÃ¬ng\", \"trans\": \"diversity\"},\n    {\"word\": \"è¯„ä¼°\", \"pinyin\": \"pÃ­ng gÅ«\", \"trans\": \"evaluation\"},\n    {\"word\": \"æ–¹æ³•\", \"pinyin\": \"fÄng fÇ\", \"trans\": \"method\"},\n    {\"word\": \"æå‡º\", \"pinyin\": \"tÃ­ chÅ«\", \"trans\": \"propose\"},\n    {\"word\": \"é«˜è´¨é‡\", \"pinyin\": \"gÄo zhÃ¬ liÃ ng\", \"trans\": \"high quality\"},\n    {\"word\": \"äººå·¥æ ‡æ³¨\", \"pinyin\": \"rÃ©ngÅng biÄo zhÃ¹\", \"trans\": \"manual annotation\"},\n    {\"word\": \"å®ä¾‹\", \"pinyin\": \"shÃ­ lÃ¬\", \"trans\": \"instance\"},\n    {\"word\": \"çœŸå®ä¸–ç•Œ\", \"pinyin\": \"zhÄ“n shÃ­ shÃ¬ jiÃ¨\", \"trans\": \"real world\"},\n    {\"word\": \"ä»»åŠ¡\", \"pinyin\": \"rÃ¨n wÃ¹\", \"trans\": \"task\"},\n    {\"word\": \"æ—¥å¸¸åœºæ™¯\", \"pinyin\": \"rÃ¬ chÃ¡ng chÇng jÇng\", \"trans\": \"daily scenarios\"},\n    {\"word\": \"æ—…è¡ŒæŒ‡å—\", \"pinyin\": \"lÇš xÃ­ng zhÇ nÃ¡n\", \"trans\": \"travel guide\"},\n    {\"word\": \"è®¾è®¡\", \"pinyin\": \"shÃ¨ jÃ¬\", \"trans\": \"design\"},\n    {\"word\": \"å¤´è„‘é£æš´\", \"pinyin\": \"tÃ³u nÇo fÄ“ng bÃ o\", \"trans\": \"brainstorming\"},\n    {\"word\": \"å¹³å°\", \"pinyin\": \"pÃ­ng tÃ i\", \"trans\": \"platform\"},\n    {\"word\": \"è¯„ä¼°æ¨¡å‹\", \"pinyin\": \"pÃ­ng gÅ« mÃ³ xÃ­ng\", \"trans\": \"evaluation model\"},\n    {\"word\": \"å¼€æ”¾å¼\", \"pinyin\": \"kÄi fÃ ng shÃ¬\", \"trans\": \"open-ended\"}\n]",
        "trans": "This article discusses the advancements of Multimodal Large Language Models (MLLMs) in visual understanding and generation tasks. However, generating interleaved image-text content remains a challenge. Existing benchmarks, due to limitations in data volume and diversity, are insufficient for evaluating these methods. To address this issue, the authors propose GATE OpenING (OpenING), a comprehensive benchmark containing 5,400 high-quality, manually annotated instances covering 56 real-world tasks. OpenING encompasses a variety of everyday scenarios, such as travel guides, design, and brainstorming, providing a robust platform for challenging interleaved generation methods. Additionally, the authors introduce IntJudge, a model for evaluating open-ended multimodal generation methods.",
        "update_ts": "2024-12-03 09:11"
    }
}