{
    "date": {
        "ru": "26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 26",
        "zh": "9æœˆ26æ—¥"
    },
    "time_utc": "2024-09-26 09:00",
    "weekday": 3,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-26",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.17146",
            "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models",
            "url": "https://huggingface.co/papers/2409.17146",
            "abstract": "Today's most advanced multimodal models remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed models into open ones. As a result, the community is still missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key innovation is a novel, highly detailed image caption dataset collected entirely from human annotators using speech-based descriptions. To enable a wide array of user interactions, we also introduce a diverse dataset mixture for fine-tuning that includes in-the-wild Q&A and innovative 2D pointing data. The success of our approach relies on careful choices for the model architecture details, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets, all of which will be released. The best-in-class 72B model within the Molmo family not only outperforms others in the class of open weight and data models but also compares favorably against proprietary systems like GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human evaluation.   We will be releasing all of our model weights, captioning and fine-tuning data, and source code in the near future. Select model weights, inference code, and demo are available at https://molmo.allenai.org.",
            "score": 99,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "3897ddd4f942abd3",
            "data": {
                "categories": [
                    "#audio",
                    "#dataset",
                    "#cv",
                    "#training",
                    "#data",
                    "#benchmark",
                    "#open_source",
                    "#architecture",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "ğŸ”“",
                "ru": {
                    "title": "Molmo: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Molmo, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ² ÑĞ²Ğ¾ĞµĞ¼ ĞºĞ»Ğ°ÑÑĞµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ½Ğ¾Ğ²Ğ¾Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹-Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ 2D-ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ. Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Molmo Ñ 72 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ° Ñ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-4 Ğ¸ Gemini 1.5 Ğ¿Ğ¾ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ»ÑĞ´ĞµĞ¹."
                },
                "en": {
                    "title": "Unlocking Open-Weight Vision-Language Models with Molmo",
                    "desc": "This paper introduces Molmo, a new family of open-weight vision-language models (VLMs) that achieve state-of-the-art performance. The key innovation is a detailed image caption dataset created by human annotators using speech-based descriptions, which enhances the model's understanding of visual content. Additionally, the authors present a diverse mixture of datasets for fine-tuning, including real-world question-and-answer data and 2D pointing interactions. The Molmo models, particularly the 72B variant, outperform existing open models and even compete well against proprietary systems, with plans to release all related resources to the community."
                },
                "zh": {
                    "title": "Molmoï¼šå¼€åˆ›å¼€æ”¾å¤šæ¨¡æ€æ¨¡å‹çš„æ–°çºªå…ƒ",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ¨¡å‹å®¶æ—Molmoï¼Œè¯¥æ¨¡å‹åœ¨å¼€æ”¾æ€§æ–¹é¢å¤„äºé¢†å…ˆåœ°ä½ã€‚Molmoçš„åˆ›æ–°ä¹‹å¤„åœ¨äºå…¶ä½¿ç”¨äººç±»æ³¨é‡Šè€…æ”¶é›†çš„è¯¦ç»†å›¾åƒæè¿°æ•°æ®é›†ã€‚ä¸ºäº†æ”¯æŒå¤šç§ç”¨æˆ·äº¤äº’ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼•å…¥äº†å¤šæ ·åŒ–çš„å¾®è°ƒæ•°æ®é›†ï¼ŒåŒ…æ‹¬é‡å¤–é—®ç­”å’Œåˆ›æ–°çš„2DæŒ‡å‘æ•°æ®ã€‚Molmoçš„æœ€ä½³æ¨¡å‹åœ¨å¼€æ”¾æƒé‡å’Œæ•°æ®æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨å­¦æœ¯åŸºå‡†å’Œäººç±»è¯„ä¼°ä¸­ä¸ä¸€äº›ä¸“æœ‰ç³»ç»Ÿç›¸åª²ç¾ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17115",
            "title": "Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale",
            "url": "https://huggingface.co/papers/2409.17115",
            "abstract": "Large language model pre-training has traditionally relied on human experts to craft heuristics for improving the corpora quality, resulting in numerous rules developed to date. However, these rules lack the flexibility to address the unique characteristics of individual example effectively. Meanwhile, applying tailored rules to every example is impractical for human experts. In this paper, we demonstrate that even small language models, with as few as 0.3B parameters, can exhibit substantial data refining capabilities comparable to those of human experts. We introduce Programming Every Example (ProX), a novel framework that treats data refinement as a programming task, enabling models to refine corpora by generating and executing fine-grained operations, such as string normalization, for each individual example at scale. Experimental results show that models pre-trained on ProX-curated data outperform either original data or data filtered by other selection methods by more than 2% across various downstream benchmarks. Its effectiveness spans various model sizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb. Furthermore, ProX exhibits significant potential in domain-specific continual pre-training: without domain specific design, models trained on OpenWebMath refined by ProX outperform human-crafted rule-based methods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B trained on 200B tokens. Further analysis highlights that ProX significantly saves training FLOPs, offering a promising path for efficient LLM pre-training.We are open-sourcing ProX with >100B corpus, models, and sharing all training and implementation details for reproducible research and future innovation. Code: https://github.com/GAIR-NLP/ProX",
            "score": 59,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "7949c35f04a3db9d",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#multilingual",
                    "#training",
                    "#data",
                    "#plp",
                    "#optimization",
                    "#benchmark",
                    "#open_source",
                    "#small_models",
                    "#synthetic"
                ],
                "emoji": "ğŸ§¹",
                "ru": {
                    "title": "ProX: ĞŸÑ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Programming Every Example (ProX). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ·Ğ°Ğ¼ĞµĞ½ÑÑ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸. ProX Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… ProX, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "ProX: Empowering Language Models with Tailored Data Refinement",
                    "desc": "This paper presents a new approach called Programming Every Example (ProX) for refining training data used in large language models. Instead of relying on rigid human-crafted rules, ProX allows models to generate and execute specific operations for each data example, enhancing flexibility and effectiveness. The results show that even smaller models can achieve data refinement capabilities similar to those of human experts, leading to improved performance on various tasks. ProX not only boosts accuracy but also reduces the computational resources needed for training, making it a promising method for efficient language model pre-training."
                },
                "zh": {
                    "title": "ProXï¼šä¸ªæ€§åŒ–æ•°æ®ç²¾ç‚¼çš„æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®ç²¾ç‚¼æ¡†æ¶ï¼Œç§°ä¸ºProgramming Every Exampleï¼ˆProXï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒæ•°æ®è´¨é‡ã€‚ProXé€šè¿‡å°†æ•°æ®ç²¾ç‚¼è§†ä¸ºç¼–ç¨‹ä»»åŠ¡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä¸ºæ¯ä¸ªç¤ºä¾‹ç”Ÿæˆå’Œæ‰§è¡Œç»†ç²’åº¦æ“ä½œï¼Œä»è€Œå®ç°æ•°æ®çš„ä¸ªæ€§åŒ–å¤„ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ProXç²¾ç‚¼çš„æ•°æ®åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºåŸå§‹æ•°æ®å’Œå…¶ä»–ç­›é€‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡å’Œé¢„è®­ç»ƒè¯­æ–™åº“ä¸­å‡æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ•ˆæœï¼Œå°¤å…¶åœ¨ç‰¹å®šé¢†åŸŸçš„æŒç»­é¢„è®­ç»ƒä¸­è¡¨ç°å‡ºè‰²ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.15127",
            "title": "Boosting Healthcare LLMs Through Retrieved Context",
            "url": "https://huggingface.co/papers/2409.15127",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing, and yet, their factual inaccuracies and hallucinations limits their application, particularly in critical domains like healthcare. Context retrieval methods, by introducing relevant information as input, have emerged as a crucial approach for enhancing LLM factuality and reliability. This study explores the boundaries of context retrieval methods within the healthcare domain, optimizing their components and benchmarking their performance against open and closed alternatives. Our findings reveal how open LLMs, when augmented with an optimized retrieval system, can achieve performance comparable to the biggest private solutions on established healthcare benchmarks (multiple-choice question answering). Recognizing the lack of realism of including the possible answers within the question (a setup only found in medical exams), and after assessing a strong LLM performance degradation in the absence of those options, we extend the context retrieval system in that direction. In particular, we propose OpenMedPrompt a pipeline that improves the generation of more reliable open-ended answers, moving this technology closer to practical application.",
            "score": 19,
            "issue_id": 1,
            "pub_date": "2024-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "3a7c5c8e7a8d8071",
            "data": {
                "categories": [
                    "#science",
                    "#hallucinations",
                    "#long_context",
                    "#rag",
                    "#healthcare",
                    "#benchmark",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°",
                    "desc": "Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ ĞµĞµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ LLM Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ñ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¼Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ OpenMedPrompt Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹."
                },
                "en": {
                    "title": "Enhancing Healthcare LLMs with Context Retrieval for Reliable Answers",
                    "desc": "This paper discusses how Large Language Models (LLMs) can struggle with providing accurate information, especially in sensitive areas like healthcare. To improve their reliability, the authors focus on context retrieval methods that supply relevant information to the LLMs. They benchmark these methods against existing solutions and find that optimized retrieval systems can enhance the performance of open LLMs to match that of private models on healthcare tasks. The study introduces OpenMedPrompt, a new pipeline designed to generate more accurate open-ended responses, making LLMs more applicable in real-world healthcare scenarios."
                },
                "zh": {
                    "title": "ä¼˜åŒ–æ£€ç´¢ç³»ç»Ÿï¼Œæå‡åŒ»ç–—é¢†åŸŸLLMçš„å¯é æ€§",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åŒ»ç–—ç­‰å…³é”®é¢†åŸŸçš„äº‹å®å‡†ç¡®æ€§å’Œå¹»è§‰é—®é¢˜é™åˆ¶äº†å®ƒä»¬çš„åº”ç”¨ã€‚é€šè¿‡å¼•å…¥ç›¸å…³ä¿¡æ¯ä½œä¸ºè¾“å…¥ï¼Œä¸Šä¸‹æ–‡æ£€ç´¢æ–¹æ³•æˆä¸ºæé«˜LLMäº‹å®æ€§å’Œå¯é æ€§çš„é‡è¦æ‰‹æ®µã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸Šä¸‹æ–‡æ£€ç´¢æ–¹æ³•åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ï¼Œä¼˜åŒ–å…¶ç»„ä»¶å¹¶ä¸å…¶ä»–æ–¹æ³•è¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡ä¼˜åŒ–çš„æ£€ç´¢ç³»ç»Ÿå¯ä»¥ä½¿å¼€æ”¾å¼LLMåœ¨åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°ä¸å¤§å‹ç§æœ‰è§£å†³æ–¹æ¡ˆç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶æå‡ºäº†OpenMedPromptä»¥ç”Ÿæˆæ›´å¯é çš„å¼€æ”¾å¼ç­”æ¡ˆã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17145",
            "title": "DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion",
            "url": "https://huggingface.co/papers/2409.17145",
            "abstract": "Leveraging pretrained 2D diffusion models and score distillation sampling (SDS), recent methods have shown promising results for text-to-3D avatar generation. However, generating high-quality 3D avatars capable of expressive animation remains challenging. In this work, we present DreamWaltz-G, a novel learning framework for animatable 3D avatar generation from text. The core of this framework lies in Skeleton-guided Score Distillation and Hybrid 3D Gaussian Avatar representation. Specifically, the proposed skeleton-guided score distillation integrates skeleton controls from 3D human templates into 2D diffusion models, enhancing the consistency of SDS supervision in terms of view and human pose. This facilitates the generation of high-quality avatars, mitigating issues such as multiple faces, extra limbs, and blurring. The proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D Gaussians, combining neural implicit fields and parameterized 3D meshes to enable real-time rendering, stable SDS optimization, and expressive animation. Extensive experiments demonstrate that DreamWaltz-G is highly effective in generating and animating 3D avatars, outperforming existing methods in both visual quality and animation expressiveness. Our framework further supports diverse applications, including human video reenactment and multi-subject scene composition.",
            "score": 13,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "629ce97635711d75",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#3d"
                ],
                "emoji": "ğŸ•º",
                "ru": {
                    "title": "Ğ¢Ğ°Ğ½Ñ†ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ñ‹: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼",
                    "desc": "DreamWaltz-G - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ÑƒÑ ÑĞºĞµĞ»ĞµÑ‚Ğ¾Ğ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D-Ğ°Ğ²Ğ°Ñ‚Ğ°Ñ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ÑĞºĞµĞ»ĞµÑ‚Ğ° Ğ¸Ğ· 3D-ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² 2D-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ»Ğ¸Ñ† Ğ¸Ğ»Ğ¸ Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾ÑÑ‚ĞµĞ¹. DreamWaltz-G Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸."
                },
                "en": {
                    "title": "DreamWaltz-G: Transforming Text to Lively 3D Avatars!",
                    "desc": "This paper introduces DreamWaltz-G, a new framework for creating 3D avatars from text that can be animated. It uses a technique called Skeleton-guided Score Distillation (SDS) to improve the quality of the generated avatars by incorporating 3D human skeletons into 2D diffusion models. The framework also employs a Hybrid 3D Gaussian representation, which combines different 3D modeling techniques for better rendering and animation. The results show that DreamWaltz-G produces high-quality, expressive avatars and outperforms previous methods in visual quality and animation capabilities."
                },
                "zh": {
                    "title": "DreamWaltz-Gï¼šæ–‡æœ¬ç”Ÿæˆå¯åŠ¨ç”»3Då¤´åƒçš„æ–°æ¡†æ¶",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDreamWaltz-Gçš„æ–°æ¡†æ¶ï¼Œç”¨äºä»æ–‡æœ¬ç”Ÿæˆå¯åŠ¨ç”»çš„3Då¤´åƒã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯éª¨æ¶å¼•å¯¼çš„å¾—åˆ†è’¸é¦å’Œæ··åˆ3Dé«˜æ–¯å¤´åƒè¡¨ç¤ºï¼Œèƒ½å¤Ÿæé«˜ç”Ÿæˆå¤´åƒçš„ä¸€è‡´æ€§å’Œè´¨é‡ã€‚é€šè¿‡å°†3Däººç±»æ¨¡æ¿çš„éª¨æ¶æ§åˆ¶æ•´åˆåˆ°2Dæ‰©æ•£æ¨¡å‹ä¸­ï¼Œè§£å†³äº†å¤šé¢å­”ã€é¢å¤–è‚¢ä½“å’Œæ¨¡ç³Šç­‰é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDreamWaltz-Gåœ¨ç”Ÿæˆå’ŒåŠ¨ç”»3Då¤´åƒæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.15041",
            "title": "AIM 2024 Sparse Neural Rendering Challenge: Dataset and Benchmark",
            "url": "https://huggingface.co/papers/2409.15041",
            "abstract": "Recent developments in differentiable and neural rendering have made impressive breakthroughs in a variety of 2D and 3D tasks, e.g. novel view synthesis, 3D reconstruction. Typically, differentiable rendering relies on a dense viewpoint coverage of the scene, such that the geometry can be disambiguated from appearance observations alone. Several challenges arise when only a few input views are available, often referred to as sparse or few-shot neural rendering. As this is an underconstrained problem, most existing approaches introduce the use of regularisation, together with a diversity of learnt and hand-crafted priors. A recurring problem in sparse rendering literature is the lack of an homogeneous, up-to-date, dataset and evaluation protocol. While high-resolution datasets are standard in dense reconstruction literature, sparse rendering methods often evaluate with low-resolution images. Additionally, data splits are inconsistent across different manuscripts, and testing ground-truth images are often publicly available, which may lead to over-fitting. In this work, we propose the Sparse Rendering (SpaRe) dataset and benchmark. We introduce a new dataset that follows the setup of the DTU MVS dataset. The dataset is composed of 97 new scenes based on synthetic, high-quality assets. Each scene has up to 64 camera views and 7 lighting configurations, rendered at 1600x1200 resolution. We release a training split of 82 scenes to foster generalizable approaches, and provide an online evaluation platform for the validation and test sets, whose ground-truth images remain hidden. We propose two different sparse configurations (3 and 9 input images respectively). This provides a powerful and convenient tool for reproducible evaluation, and enable researchers easy access to a public leaderboard with the state-of-the-art performance scores. Available at: https://sparebenchmark.github.io/",
            "score": 12,
            "issue_id": 1,
            "pub_date": "2024-09-23",
            "pub_date_card": {
                "ru": "23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 23",
                "zh": "9æœˆ23æ—¥"
            },
            "hash": "94750a64c54ff82d",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "ğŸ¥",
                "ru": {
                    "title": "SpaRe: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ñ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (sparse rendering). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SpaRe, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 97 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ†ĞµĞ½ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€ Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ 3 Ğ¸ 9 Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ»Ğ¸Ğ´ĞµÑ€Ğ±Ğ¾Ñ€Ğ´ Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ²."
                },
                "en": {
                    "title": "Advancing Sparse Rendering with the SpaRe Dataset",
                    "desc": "This paper presents the Sparse Rendering (SpaRe) dataset and benchmark, addressing the challenges in few-shot neural rendering. It highlights the need for a consistent and high-resolution dataset, as existing methods often rely on low-quality images and inconsistent data splits. The SpaRe dataset includes 97 synthetic scenes with multiple camera views and lighting conditions, designed to facilitate the evaluation of sparse rendering techniques. By providing a public leaderboard and an online evaluation platform, this work aims to promote reproducibility and advance research in the field of sparse rendering."
                },
                "zh": {
                    "title": "ç¨€ç–æ¸²æŸ“æ–°æ•°æ®é›†ï¼ŒåŠ©åŠ›ç¥ç»æ¸²æŸ“ç ”ç©¶",
                    "desc": "æœ€è¿‘åœ¨å¯å¾®æ¸²æŸ“å’Œç¥ç»æ¸²æŸ“æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨2Då’Œ3Dä»»åŠ¡ä¸­ï¼Œå¦‚æ–°è§†è§’åˆæˆå’Œ3Dé‡å»ºã€‚å¯å¾®æ¸²æŸ“é€šå¸¸ä¾èµ–äºåœºæ™¯çš„å¯†é›†è§†è§’è¦†ç›–ï¼Œä»¥ä¾¿ä»å¤–è§‚è§‚å¯Ÿä¸­åŒºåˆ†å‡ ä½•å½¢çŠ¶ã€‚ç„¶è€Œï¼Œå½“åªæœ‰å°‘é‡è¾“å…¥è§†å›¾å¯ç”¨æ—¶ï¼Œé€šå¸¸ä¼šé¢ä¸´ç¨€ç–æˆ–å°‘æ ·æœ¬ç¥ç»æ¸²æŸ“çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ç¨€ç–æ¸²æŸ“ï¼ˆSpaReï¼‰æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æä¾›ä¸€ä¸ªä¸€è‡´çš„è¯„ä¼°å¹³å°ï¼Œä¿ƒè¿›å¯é‡å¤çš„è¯„ä¼°å’Œç ”ç©¶ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17058",
            "title": "Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors",
            "url": "https://huggingface.co/papers/2409.17058",
            "abstract": "Diffusion-based image super-resolution (SR) methods have achieved remarkable success by leveraging large pre-trained text-to-image diffusion models as priors. However, these methods still face two challenges: the requirement for dozens of sampling steps to achieve satisfactory results, which limits efficiency in real scenarios, and the neglect of degradation models, which are critical auxiliary information in solving the SR problem. In this work, we introduced a novel one-step SR model, which significantly addresses the efficiency issue of diffusion-based SR methods. Unlike existing fine-tuning strategies, we designed a degradation-guided Low-Rank Adaptation (LoRA) module specifically for SR, which corrects the model parameters based on the pre-estimated degradation information from low-resolution images. This module not only facilitates a powerful data-dependent or degradation-dependent SR model but also preserves the generative prior of the pre-trained diffusion model as much as possible. Furthermore, we tailor a novel training pipeline by introducing an online negative sample generation strategy. Combined with the classifier-free guidance strategy during inference, it largely improves the perceptual quality of the super-resolution results. Extensive experiments have demonstrated the superior efficiency and effectiveness of the proposed model compared to recent state-of-the-art methods.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "c52ca2b156d80f27",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#inference",
                    "#optimization",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "ğŸ”",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ½Ğ¾Ğ²ÑˆĞµÑÑ‚Ğ²Ğ¾ - Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ (LoRA), ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸."
                },
                "en": {
                    "title": "Efficient Super-Resolution with Degradation-Guided Diffusion Models",
                    "desc": "This paper presents a new approach to image super-resolution (SR) using diffusion models, focusing on improving efficiency and incorporating degradation models. The authors introduce a one-step SR model that reduces the number of sampling steps needed, making it faster for real-world applications. They also propose a Low-Rank Adaptation (LoRA) module that adjusts model parameters based on degradation information from low-resolution images, enhancing the model's performance. The combination of this module with a novel training pipeline and classifier-free guidance leads to better perceptual quality in the generated images, outperforming existing methods."
                },
                "zh": {
                    "title": "é«˜æ•ˆè¶…åˆ†è¾¨ç‡ï¼šä¸€é”®è§£å†³å›¾åƒé€€åŒ–é—®é¢˜",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å›¾åƒè¶…åˆ†è¾¨ç‡(SR)æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ•ˆç‡å¹¶è§£å†³ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä½ç§©é€‚åº”(LoRA)æ¨¡å—ï¼Œåˆ©ç”¨ä½åˆ†è¾¨ç‡å›¾åƒçš„é€€åŒ–ä¿¡æ¯æ¥è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œä»è€Œå®ç°ä¸€é”®è¶…åˆ†è¾¨ç‡ã€‚è¯¥æ¨¡å—ä¸ä»…å¢å¼ºäº†æ¨¡å‹çš„é€‚åº”æ€§ï¼Œè¿˜å°½å¯èƒ½ä¿ç•™äº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åœ¨çº¿è´Ÿæ ·æœ¬ç”Ÿæˆç­–ç•¥ï¼Œç»“åˆæ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†è¶…åˆ†è¾¨ç‡ç»“æœçš„æ„ŸçŸ¥è´¨é‡ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.16299",
            "title": "HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks at Scale",
            "url": "https://huggingface.co/papers/2409.16299",
            "abstract": "Large Language Models (LLMs) have revolutionized software engineering (SE), demonstrating remarkable capabilities in various coding tasks. While recent efforts have produced autonomous software agents based on LLMs for end-to-end development tasks, these systems are typically designed for specific SE tasks. We introduce HyperAgent, a novel generalist multi-agent system designed to address a wide spectrum of SE tasks across different programming languages by mimicking human developers' workflows. Comprising four specialized agents - Planner, Navigator, Code Editor, and Executor. HyperAgent manages the full lifecycle of SE tasks, from initial conception to final verification. Through extensive evaluations, HyperAgent achieves state-of-the-art performance across diverse SE tasks: it attains a 25.01% success rate on SWE-Bench-Lite and 31.40% on SWE-Bench-Verified for GitHub issue resolution, surpassing existing methods. Furthermore, HyperAgent demonstrates SOTA performance in repository-level code generation (RepoExec), and in fault localization and program repair (Defects4J), often outperforming specialized systems. This work represents a significant advancement towards versatile, autonomous agents capable of handling complex, multi-step SE tasks across various domains and languages, potentially transforming AI-assisted software development practices.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-09",
            "pub_date_card": {
                "ru": "9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 9",
                "zh": "9æœˆ9æ—¥"
            },
            "hash": "a713e3f82d512439",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multilingual",
                    "#agi",
                    "#optimization",
                    "#plp",
                    "#agents",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "HyperAgent: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚Ğ¾Ğ²",
                    "desc": "HyperAgent - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: ĞŸĞ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°, ĞĞ°Ğ²Ğ¸Ğ³Ğ°Ñ‚Ğ¾Ñ€Ğ°, Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€Ğ° ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ˜ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°-Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°. HyperAgent Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ GitHub issues, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ­Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ²Ğ¿ĞµÑ€ĞµĞ´ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "HyperAgent: Revolutionizing Software Engineering with Multi-Agent Intelligence",
                    "desc": "This paper presents HyperAgent, a generalist multi-agent system that enhances software engineering (SE) by mimicking human workflows. It consists of four specialized agents: Planner, Navigator, Code Editor, and Executor, which together manage the entire lifecycle of SE tasks. HyperAgent has shown superior performance in various coding challenges, achieving notable success rates in GitHub issue resolution and repository-level code generation. This innovation marks a significant step towards creating versatile, autonomous agents that can efficiently tackle complex SE tasks across multiple programming languages."
                },
                "zh": {
                    "title": "HyperAgentï¼šé€šç”¨è½¯ä»¶å·¥ç¨‹çš„æ™ºèƒ½ä»£ç†",
                    "desc": "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ï¼ˆSEï¼‰é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œå±•ç°äº†åœ¨å„ç§ç¼–ç ä»»åŠ¡ä¸­çš„å“è¶Šèƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†HyperAgentï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„é€šç”¨å¤šä»£ç†ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿäººç±»å¼€å‘è€…çš„å·¥ä½œæµç¨‹ï¼Œè§£å†³ä¸åŒç¼–ç¨‹è¯­è¨€ä¸­çš„å¹¿æ³›SEä»»åŠ¡ã€‚HyperAgentç”±å››ä¸ªä¸“ä¸šä»£ç†ç»„æˆï¼šè§„åˆ’è€…ã€å¯¼èˆªè€…ã€ä»£ç ç¼–è¾‘å™¨å’Œæ‰§è¡Œè€…ï¼Œèƒ½å¤Ÿç®¡ç†SEä»»åŠ¡çš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸï¼Œä»åˆæ­¥æ„æƒ³åˆ°æœ€ç»ˆéªŒè¯ã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼ŒHyperAgentåœ¨å¤šç§SEä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæ ‡å¿—ç€å‘èƒ½å¤Ÿå¤„ç†å¤æ‚å¤šæ­¥éª¤SEä»»åŠ¡çš„è‡ªä¸»ä»£ç†çš„é‡è¦è¿›å±•ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.16629",
            "title": "Synchronize Dual Hands for Physics-Based Dexterous Guitar Playing",
            "url": "https://huggingface.co/papers/2409.16629",
            "abstract": "We present a novel approach to synthesize dexterous motions for physically simulated hands in tasks that require coordination between the control of two hands with high temporal precision. Instead of directly learning a joint policy to control two hands, our approach performs bimanual control through cooperative learning where each hand is treated as an individual agent. The individual policies for each hand are first trained separately, and then synchronized through latent space manipulation in a centralized environment to serve as a joint policy for two-hand control. By doing so, we avoid directly performing policy learning in the joint state-action space of two hands with higher dimensions, greatly improving the overall training efficiency. We demonstrate the effectiveness of our proposed approach in the challenging guitar-playing task. The virtual guitarist trained by our approach can synthesize motions from unstructured reference data of general guitar-playing practice motions, and accurately play diverse rhythms with complex chord pressing and string picking patterns based on the input guitar tabs that do not exist in the references. Along with this paper, we provide the motion capture data that we collected as the reference for policy training. Code is available at: https://pei-xu.github.io/guitar.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "319d5a32d76eb024",
            "data": {
                "categories": [
                    "#dataset",
                    "#rl",
                    "#agents",
                    "#games",
                    "#open_source",
                    "#robotics"
                ],
                "emoji": "ğŸ¸",
                "ru": {
                    "title": "ĞšĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ²ÑƒÑ€ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€ÑƒĞº Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ²ÑƒĞ¼Ñ Ñ€ÑƒĞºĞ°Ğ¼Ğ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ñ€ÑƒĞºĞ°Ğ¼Ğ¸, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ñ€ÑƒĞºĞ° Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚. Ğ˜Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ñ€ÑƒĞºĞ¸ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ² Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ñ€ÑƒĞºĞ°Ğ¼Ğ¸. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¸Ğ³Ñ€Ñ‹ Ğ½Ğ° Ğ³Ğ¸Ñ‚Ğ°Ñ€Ğµ."
                },
                "en": {
                    "title": "Efficient Bimanual Control through Cooperative Learning",
                    "desc": "This paper introduces a new method for controlling two simulated hands to perform tasks that require precise coordination, like playing the guitar. Instead of creating a single complex policy for both hands, the authors train each hand as a separate agent and then synchronize their movements using latent space manipulation. This approach simplifies the learning process by avoiding the high-dimensional joint state-action space, leading to more efficient training. The results show that their method allows a virtual guitarist to accurately play various rhythms and complex patterns based on guitar tabs, even when trained on unstructured data."
                },
                "zh": {
                    "title": "é«˜æ•ˆåŒæ‰‹æ§åˆ¶çš„åˆ›æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç”¨äºåˆæˆç‰©ç†æ¨¡æ‹Ÿæ‰‹çš„çµå·§åŠ¨ä½œï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é«˜æ—¶é—´ç²¾åº¦çš„åŒæ‰‹åè°ƒä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆä½œå­¦ä¹ å®ç°åŒæ‰‹æ§åˆ¶ï¼Œå°†æ¯åªæ‰‹è§†ä¸ºç‹¬ç«‹çš„æ™ºèƒ½ä½“ï¼Œè€Œä¸æ˜¯ç›´æ¥å­¦ä¹ æ§åˆ¶ä¸¤åªæ‰‹çš„è”åˆç­–ç•¥ã€‚æ¯åªæ‰‹çš„ä¸ªä½“ç­–ç•¥é¦–å…ˆå•ç‹¬è®­ç»ƒï¼Œç„¶åé€šè¿‡æ½œåœ¨ç©ºé—´æ“ä½œåœ¨é›†ä¸­ç¯å¢ƒä¸­åŒæ­¥ï¼Œä»¥å½¢æˆåŒæ‰‹æ§åˆ¶çš„è”åˆç­–ç•¥ã€‚æˆ‘ä»¬åœ¨æŒ‘æˆ˜æ€§çš„å‰ä»–æ¼”å¥ä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè®­ç»ƒå‡ºçš„è™šæ‹Ÿå‰ä»–æ‰‹èƒ½å¤Ÿä»æ— ç»“æ„çš„å‚è€ƒæ•°æ®ä¸­åˆæˆåŠ¨ä½œï¼Œå‡†ç¡®æ¼”å¥å¤æ‚çš„èŠ‚å¥å’Œå’Œå¼¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.16493",
            "title": "NoTeeline: Supporting Real-Time Notetaking from Keypoints with Large Language Models",
            "url": "https://huggingface.co/papers/2409.16493",
            "abstract": "Video has become a popular media form for information sharing and consumption. However, taking notes while watching a video requires significant time and effort. To address this, we propose a novel interactive system, NoTeeline, for taking real-time, personalized notes. NoTeeline lets users quickly jot down keypoints (micronotes), which are automatically expanded into full-fledged notes that capture the content of the user's micronotes and are consistent with the user's writing style. In a within-subjects study (N=12), we found that NoTeeline helps users create high-quality notes that capture the essence of their micronotes with a higher factual correctness (93.2%) while accurately reflecting their writing style. While using NoTeeline, participants experienced significantly reduced mental effort, captured satisfactory notes while writing 47% less text, and completed notetaking with 43.9% less time compared to a manual notetaking baseline.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2024-09-24",
            "pub_date_card": {
                "ru": "24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 24",
                "zh": "9æœˆ24æ—¥"
            },
            "hash": "83e7a802e2a7e4e8",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal"
                ],
                "emoji": "ğŸ“",
                "ru": {
                    "title": "NoTeeline: ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ÑĞ¿ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ NoTeeline Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¼ĞµÑ‚Ğ¾Ğº Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ÑÑ Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¼ĞµÑ‚ĞºĞ¸, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ñ Ğ¿Ğ¸ÑÑŒĞ¼Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ NoTeeline Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¼ĞµÑ‚ĞºĞ¸ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ ÑƒÑĞ¸Ğ»Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€ÑƒÑ‡Ğ½Ñ‹Ğ¼ Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ¾Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°."
                },
                "en": {
                    "title": "Revolutionizing Video Note-Taking with NoTeeline",
                    "desc": "The paper presents NoTeeline, an innovative interactive system designed to enhance the note-taking process while watching videos. It allows users to create quick, concise notes called micronotes, which are then transformed into comprehensive notes that align with the user's unique writing style. A study involving 12 participants demonstrated that NoTeeline significantly improves the quality and factual accuracy of notes, achieving a correctness rate of 93.2%. Additionally, users reported reduced mental effort, less text written (47% less), and faster completion times (43.9% less) compared to traditional note-taking methods."
                },
                "zh": {
                    "title": "å®æ—¶ä¸ªæ€§åŒ–ç¬”è®°ï¼Œè½»æ¾è®°å½•è§†é¢‘ç²¾å",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºNoTeelineçš„äº’åŠ¨ç³»ç»Ÿï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·åœ¨è§‚çœ‹è§†é¢‘æ—¶å®æ—¶è®°å½•ä¸ªæ€§åŒ–ç¬”è®°ã€‚ç”¨æˆ·å¯ä»¥å¿«é€Ÿè®°å½•å…³é”®ç‚¹ï¼ˆå¾®ç¬”è®°ï¼‰ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å°†å…¶æ‰©å±•ä¸ºå®Œæ•´çš„ç¬”è®°ï¼Œç¡®ä¿å†…å®¹ä¸ç”¨æˆ·çš„å†™ä½œé£æ ¼ä¸€è‡´ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨NoTeelineçš„ç”¨æˆ·èƒ½å¤Ÿä»¥æ›´é«˜çš„å‡†ç¡®æ€§ï¼ˆ93.2%ï¼‰åˆ›å»ºé«˜è´¨é‡çš„ç¬”è®°ï¼ŒåŒæ—¶å‡å°‘äº†47%çš„æ–‡æœ¬è¾“å…¥é‡å’Œ43.9%çš„æ—¶é—´æ¶ˆè€—ã€‚è¯¥ç³»ç»Ÿæ˜¾è‘—é™ä½äº†ç”¨æˆ·çš„å¿ƒç†è´Ÿæ‹…ï¼Œæé«˜äº†ç¬”è®°çš„æ»¡æ„åº¦ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.16925",
            "title": "Game4Loc: A UAV Geo-Localization Benchmark from Game Data",
            "url": "https://huggingface.co/papers/2409.16925",
            "abstract": "The vision-based geo-localization technology for UAV, serving as a secondary source of GPS information in addition to the global navigation satellite systems (GNSS), can still operate independently in the GPS-denied environment. Recent deep learning based methods attribute this as the task of image matching and retrieval. By retrieving drone-view images in geo-tagged satellite image database, approximate localization information can be obtained. However, due to high costs and privacy concerns, it is usually difficult to obtain large quantities of drone-view images from a continuous area. Existing drone-view datasets are mostly composed of small-scale aerial photography with a strong assumption that there exists a perfect one-to-one aligned reference image for any query, leaving a significant gap from the practical localization scenario. In this work, we construct a large-range contiguous area UAV geo-localization dataset named GTA-UAV, featuring multiple flight altitudes, attitudes, scenes, and targets using modern computer games. Based on this dataset, we introduce a more practical UAV geo-localization task including partial matches of cross-view paired data, and expand the image-level retrieval to the actual localization in terms of distance (meters). For the construction of drone-view and satellite-view pairs, we adopt a weight-based contrastive learning approach, which allows for effective learning while avoiding additional post-processing matching steps. Experiments demonstrate the effectiveness of our data and training method for UAV geo-localization, as well as the generalization capabilities to real-world scenarios.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "bc7c7309053e8db8",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#training",
                    "#graphs",
                    "#optimization",
                    "#games",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "ğŸ›°ï¸",
                "ru": {
                    "title": "Ğ“ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸Ñ Ğ‘ĞŸĞ›Ğ Ğ±ĞµĞ· GPS: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ñ Ğ²Ñ‹ÑĞ¾Ñ‚Ñ‹ Ğ¿Ñ‚Ğ¸Ñ‡ÑŒĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ»ĞµÑ‚Ğ°",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ»ĞµÑ‚Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ¾Ğ² (Ğ‘ĞŸĞ›Ğ) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹ GPS. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GTA-UAV, ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¿Ğ¾Ğ»ĞµÑ‚Ğ° Ğ‘ĞŸĞ›Ğ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ‘ĞŸĞ›Ğ Ğ¸ ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…."
                },
                "en": {
                    "title": "Revolutionizing UAV Localization with GTA-UAV Dataset",
                    "desc": "This paper presents a new dataset called GTA-UAV for vision-based geo-localization of UAVs, which can function without GPS in areas where satellite signals are unavailable. The authors address the limitations of existing datasets that assume perfect image alignment, which is often unrealistic in practical situations. They propose a novel task that includes partial matches between drone-view and satellite-view images, enhancing the localization process by measuring actual distances. The study employs a weight-based contrastive learning method to improve the learning process and demonstrates the effectiveness of their approach through experiments that show good performance in real-world applications."
                },
                "zh": {
                    "title": "æ— äººæœºåœ°ç†å®šä½çš„æ–°çªç ´",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºè§†è§‰çš„æ— äººæœºåœ°ç†å®šä½æŠ€æœ¯ï¼Œä½œä¸ºå…¨çƒå¯¼èˆªå«æ˜Ÿç³»ç»Ÿï¼ˆGNSSï¼‰çš„è¾…åŠ©ä¿¡æ¯æºï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰GPSä¿¡å·çš„ç¯å¢ƒä¸­ç‹¬ç«‹å·¥ä½œã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºGTA-UAVçš„å¤§èŒƒå›´è¿ç»­åŒºåŸŸæ— äººæœºåœ°ç†å®šä½æ•°æ®é›†ï¼ŒåŒ…å«å¤šç§é£è¡Œé«˜åº¦ã€å§¿æ€ã€åœºæ™¯å’Œç›®æ ‡ã€‚é€šè¿‡é‡‡ç”¨åŸºäºæƒé‡çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†æ— äººæœºè§†è§’ä¸å«æ˜Ÿè§†è§’å›¾åƒå¯¹çš„æœ‰æ•ˆåŒ¹é…ï¼Œé¿å…äº†é¢å¤–çš„åå¤„ç†æ­¥éª¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ— äººæœºåœ°ç†å®šä½ä»»åŠ¡ä¸­å…·æœ‰è‰¯å¥½çš„æ•ˆæœå’Œå®é™…åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.16288",
            "title": "Self-Supervised Any-Point Tracking by Contrastive Random Walks",
            "url": "https://huggingface.co/papers/2409.16288",
            "abstract": "We present a simple, self-supervised approach to the Tracking Any Point (TAP) problem. We train a global matching transformer to find cycle consistent tracks through video via contrastive random walks, using the transformer's attention-based global matching to define the transition matrices for a random walk on a space-time graph. The ability to perform \"all pairs\" comparisons between points allows the model to obtain high spatial precision and to obtain a strong contrastive learning signal, while avoiding many of the complexities of recent approaches (such as coarse-to-fine matching). To do this, we propose a number of design decisions that allow global matching architectures to be trained through self-supervision using cycle consistency. For example, we identify that transformer-based methods are sensitive to shortcut solutions, and propose a data augmentation scheme to address them. Our method achieves strong performance on the TapVid benchmarks, outperforming previous self-supervised tracking methods, such as DIFT, and is competitive with several supervised methods.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2024-09-24",
            "pub_date_card": {
                "ru": "24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 24",
                "zh": "9æœˆ24æ—¥"
            },
            "hash": "9d0502c19cafc49e",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#training",
                    "#graphs",
                    "#optimization",
                    "#benchmark",
                    "#games",
                    "#architecture"
                ],
                "emoji": "ğŸ¯",
                "ru": {
                    "title": "Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»ÑĞ±Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ (TAP). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞºĞ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… Ğ±Ğ»ÑƒĞ¶Ğ´Ğ°Ğ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¸ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¼ Ğ±Ğ»ÑƒĞ¶Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ³Ñ€Ğ°Ñ„Ñƒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… TapVid, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Video Tracking with Self-Supervised Learning",
                    "desc": "This paper introduces a self-supervised method for the Tracking Any Point (TAP) problem using a global matching transformer. The approach leverages contrastive random walks to establish cycle consistent tracks in video data, enhancing spatial precision through all pairs comparisons. By focusing on self-supervision and cycle consistency, the model simplifies the training process while effectively avoiding common pitfalls in tracking methods. The proposed design choices, including a data augmentation strategy, lead to superior performance on the TapVid benchmarks compared to existing self-supervised and some supervised tracking techniques."
                },
                "zh": {
                    "title": "è‡ªç›‘ç£ä»»æ„ç‚¹è·Ÿè¸ªçš„æ–°æ–¹æ³•",
                    "desc": "æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„è‡ªç›‘ç£æ–¹æ³•æ¥è§£å†³ä»»æ„ç‚¹è·Ÿè¸ªï¼ˆTAPï¼‰é—®é¢˜ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå…¨å±€åŒ¹é…å˜æ¢å™¨ï¼Œé€šè¿‡å¯¹æ¯”éšæœºæ¸¸èµ°åœ¨è§†é¢‘ä¸­æ‰¾åˆ°å¾ªç¯ä¸€è‡´çš„è½¨è¿¹ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å˜æ¢å™¨çš„æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå…¨å±€åŒ¹é…ï¼Œä»è€Œå®šä¹‰æ—¶ç©ºå›¾ä¸Šçš„éšæœºæ¸¸èµ°è½¬ç§»çŸ©é˜µã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨TapVidåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„è‡ªç›‘ç£è·Ÿè¸ªæ–¹æ³•ï¼Œå¹¶ä¸ä¸€äº›ç›‘ç£æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.16666",
            "title": "TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans",
            "url": "https://huggingface.co/papers/2409.16666",
            "abstract": "We introduce a novel framework that learns a dynamic neural radiance field (NeRF) for full-body talking humans from monocular videos. Prior work represents only the body pose or the face. However, humans communicate with their full body, combining body pose, hand gestures, as well as facial expressions. In this work, we propose TalkinNeRF, a unified NeRF-based network that represents the holistic 4D human motion. Given a monocular video of a subject, we learn corresponding modules for the body, face, and hands, that are combined together to generate the final result. To capture complex finger articulation, we learn an additional deformation field for the hands. Our multi-identity representation enables simultaneous training for multiple subjects, as well as robust animation under completely unseen poses. It can also generalize to novel identities, given only a short video as input. We demonstrate state-of-the-art performance for animating full-body talking humans, with fine-grained hand articulation and facial expressions.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 25",
                "zh": "9æœˆ25æ—¥"
            },
            "hash": "95c442e9c5d9f23c",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#cv",
                    "#3d"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "TalkinNeRF: Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ NeRF",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ¸Ğ·Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ (NeRF) Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ñ… Ğ»ÑĞ´ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. TalkinNeRF - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ NeRF-ÑĞµÑ‚ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğµ 4D-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ·Ñƒ Ñ‚ĞµĞ»Ğ°, Ğ¶ĞµÑÑ‚Ñ‹ Ñ€ÑƒĞº Ğ¸ Ğ¼Ğ¸Ğ¼Ğ¸ĞºÑƒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµĞ»Ğ°, Ğ»Ğ¸Ñ†Ğ° Ğ¸ Ñ€ÑƒĞº, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»Ğµ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ‚Ğ¸ĞºÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ»ÑŒÑ†ĞµĞ². ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ ĞµĞµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸."
                },
                "en": {
                    "title": "Animating Full-Body Talking Humans with TalkinNeRF",
                    "desc": "This paper presents TalkinNeRF, a new framework that learns to create dynamic neural radiance fields (NeRF) for animating full-body talking humans using just monocular videos. Unlike previous methods that focused only on body pose or facial expressions, TalkinNeRF integrates body motion, hand gestures, and facial expressions into a single model. It includes specialized modules for the body, face, and hands, and introduces a deformation field to accurately capture complex hand movements. The framework allows for training on multiple identities and can generate animations for unseen poses, demonstrating advanced capabilities in human motion representation and animation."
                },
                "zh": {
                    "title": "å…¨èº«è¯´è¯çš„åŠ¨æ€ç¥ç»è¾å°„åœº",
                    "desc": "æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»å•ç›®è§†é¢‘ä¸­å­¦ä¹ åŠ¨æ€ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ï¼Œç”¨äºå…¨èº«è¯´è¯çš„äººç±»ã€‚ä»¥å¾€çš„ç ”ç©¶ä»…è¡¨ç¤ºèº«ä½“å§¿åŠ¿æˆ–é¢éƒ¨è¡¨æƒ…ï¼Œè€Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†èº«ä½“å§¿åŠ¿ã€æ‰‹åŠ¿å’Œé¢éƒ¨è¡¨æƒ…ï¼Œå…¨é¢æ•æ‰äººç±»çš„äº¤æµæ–¹å¼ã€‚æˆ‘ä»¬æå‡ºçš„TalkinNeRFç½‘ç»œèƒ½å¤ŸåŒæ—¶å¤„ç†èº«ä½“ã€é¢éƒ¨å’Œæ‰‹éƒ¨çš„è¿åŠ¨ï¼Œå¹¶ç”Ÿæˆæœ€ç»ˆç»“æœã€‚è¯¥æ–¹æ³•æ”¯æŒå¤šèº«ä»½è¡¨ç¤ºï¼Œèƒ½å¤Ÿåœ¨æœªè§è¿‡çš„å§¿åŠ¿ä¸‹è¿›è¡Œé²æ£’åŠ¨ç”»ï¼Œå¹¶ä¸”å¯ä»¥æ ¹æ®çŸ­è§†é¢‘è¾“å…¥ç”Ÿæˆæ–°çš„èº«ä»½ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-09-25.html",
    "link_next": "2024-09-27.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "25.09",
        "en": "09/25",
        "zh": "9æœˆ25æ—¥"
    },
    "short_date_next": {
        "ru": "27.09",
        "en": "09/27",
        "zh": "9æœˆ27æ—¥"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 6,
        "#agents": 2,
        "#cv": 6,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 2,
        "#inference": 1,
        "#3d": 4,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 7,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 1,
        "#agi": 1,
        "#games": 3,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 2,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 4,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    }
}