{
    "date": {
        "ru": "26 —Å–µ–Ω—Ç—è–±—Ä—è",
        "en": "September 26",
        "zh": "9Êúà26Êó•"
    },
    "time_utc": "2024-09-26 09:00",
    "weekday": 3,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-26",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.17146",
            "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models",
            "url": "https://huggingface.co/papers/2409.17146",
            "abstract": "Today's most advanced multimodal models remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed models into open ones. As a result, the community is still missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key innovation is a novel, highly detailed image caption dataset collected entirely from human annotators using speech-based descriptions. To enable a wide array of user interactions, we also introduce a diverse dataset mixture for fine-tuning that includes in-the-wild Q&A and innovative 2D pointing data. The success of our approach relies on careful choices for the model architecture details, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets, all of which will be released. The best-in-class 72B model within the Molmo family not only outperforms others in the class of open weight and data models but also compares favorably against proprietary systems like GPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human evaluation.   We will be releasing all of our model weights, captioning and fine-tuning data, and source code in the near future. Select model weights, inference code, and demo are available at https://molmo.allenai.org.",
            "score": 99,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 —Å–µ–Ω—Ç—è–±—Ä—è",
                "en": "September 25",
                "zh": "9Êúà25Êó•"
            },
            "hash": "3897ddd4f942abd3",
            "data": {
                "categories": [
                    "#audio",
                    "#dataset",
                    "#cv",
                    "#training",
                    "#data",
                    "#benchmark",
                    "#open_source",
                    "#architecture",
                    "#synthetic",
                    "#multimodal"
                ],
                "emoji": "üîì",
                "ru": {
                    "title": "Molmo: –ø—Ä–æ—Ä—ã–≤ –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Molmo, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –ª—É—á—à–∏–º–∏ –≤ —Å–≤–æ–µ–º –∫–ª–∞—Å—Å–µ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π. –ö–ª—é—á–µ–≤—ã–º –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–µ–º —è–≤–ª—è–µ—Ç—Å—è –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Å–æ–±—Ä–∞–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é —Ä–µ—á–µ–≤—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π. –ú–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–µ–º –≤–æ–ø—Ä–æ—Å—ã-–æ—Ç–≤–µ—Ç—ã –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ 2D-—É–∫–∞–∑–∞–Ω–∏—è. –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å Molmo —Å 72 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –∏ —Å—Ä–∞–≤–Ω–∏–º–∞ —Å –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ –≤—Ä–æ–¥–µ GPT-4 –∏ Gemini 1.5 –ø–æ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–º –±–µ–Ω—á–º–∞—Ä–∫–∞–º –∏ –æ—Ü–µ–Ω–∫–∞–º –ª—é–¥–µ–π."
                },
                "en": {
                    "title": "Unlocking Open-Weight Vision-Language Models with Molmo",
                    "desc": "This paper introduces Molmo, a new family of open-weight vision-language models (VLMs) that achieve state-of-the-art performance. The key innovation is a detailed image caption dataset created by human annotators using speech-based descriptions, which enhances the model's understanding of visual content. Additionally, the authors present a diverse mixture of datasets for fine-tuning, including real-world question-and-answer data and 2D pointing interactions. The Molmo models, particularly the 72B variant, outperform existing open models and even compete well against proprietary systems, with plans to release all related resources to the community."
                },
                "zh": {
                    "title": "MolmoÔºöÂºÄÂàõÂºÄÊîæÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÊñ∞Á∫™ÂÖÉ",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ§öÊ®°ÊÄÅÊ®°ÂûãÂÆ∂ÊóèMolmoÔºåËØ•Ê®°ÂûãÂú®ÂºÄÊîæÊÄßÊñπÈù¢Â§Ñ‰∫éÈ¢ÜÂÖàÂú∞‰Ωç„ÄÇMolmoÁöÑÂàõÊñ∞‰πãÂ§ÑÂú®‰∫éÂÖ∂‰ΩøÁî®‰∫∫Á±ªÊ≥®ÈáäËÄÖÊî∂ÈõÜÁöÑËØ¶ÁªÜÂõæÂÉèÊèèËø∞Êï∞ÊçÆÈõÜ„ÄÇ‰∏∫‰∫ÜÊîØÊåÅÂ§öÁßçÁî®Êà∑‰∫§‰∫íÔºåÁ†îÁ©∂Âõ¢ÈòüËøòÂºïÂÖ•‰∫ÜÂ§öÊ†∑ÂåñÁöÑÂæÆË∞ÉÊï∞ÊçÆÈõÜÔºåÂåÖÊã¨ÈáéÂ§ñÈóÆÁ≠îÂíåÂàõÊñ∞ÁöÑ2DÊåáÂêëÊï∞ÊçÆ„ÄÇMolmoÁöÑÊúÄ‰Ω≥Ê®°ÂûãÂú®ÂºÄÊîæÊùÉÈáçÂíåÊï∞ÊçÆÊ®°Âûã‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂπ∂Âú®Â≠¶ÊúØÂü∫ÂáÜÂíå‰∫∫Á±ªËØÑ‰º∞‰∏≠‰∏é‰∏Ä‰∫õ‰∏ìÊúâÁ≥ªÁªüÁõ∏Â™≤Áæé„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17115",
            "title": "Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale",
            "url": "https://huggingface.co/papers/2409.17115",
            "abstract": "Large language model pre-training has traditionally relied on human experts to craft heuristics for improving the corpora quality, resulting in numerous rules developed to date. However, these rules lack the flexibility to address the unique characteristics of individual example effectively. Meanwhile, applying tailored rules to every example is impractical for human experts. In this paper, we demonstrate that even small language models, with as few as 0.3B parameters, can exhibit substantial data refining capabilities comparable to those of human experts. We introduce Programming Every Example (ProX), a novel framework that treats data refinement as a programming task, enabling models to refine corpora by generating and executing fine-grained operations, such as string normalization, for each individual example at scale. Experimental results show that models pre-trained on ProX-curated data outperform either original data or data filtered by other selection methods by more than 2% across various downstream benchmarks. Its effectiveness spans various model sizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb. Furthermore, ProX exhibits significant potential in domain-specific continual pre-training: without domain specific design, models trained on OpenWebMath refined by ProX outperform human-crafted rule-based methods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B trained on 200B tokens. Further analysis highlights that ProX significantly saves training FLOPs, offering a promising path for efficient LLM pre-training.We are open-sourcing ProX with >100B corpus, models, and sharing all training and implementation details for reproducible research and future innovation. Code: https://github.com/GAIR-NLP/ProX",
            "score": 59,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 —Å–µ–Ω—Ç—è–±—Ä—è",
                "en": "September 25",
                "zh": "9Êúà25Êó•"
            },
            "hash": "7949c35f04a3db9d",
            "data": {
                "categories": [
                    "#science",
                    "#dataset",
                    "#multilingual",
                    "#training",
                    "#data",
                    "#plp",
                    "#optimization",
                    "#benchmark",
                    "#open_source",
                    "#small_models",
                    "#synthetic"
                ],
                "emoji": "üßπ",
                "ru": {
                    "title": "ProX: –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π Programming Every Example (ProX). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∑–∞–º–µ–Ω—è—è —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏, —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏. ProX –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø–æ –æ—á–∏—Å—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤ –º–∞—Å—à—Ç–∞–±–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö ProX, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –¥–∞–Ω–Ω—ã—Ö, –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏."
                },
                "en": {
                    "title": "ProX: Empowering Language Models with Tailored Data Refinement",
                    "desc": "This paper presents a new approach called Programming Every Example (ProX) for refining training data used in large language models. Instead of relying on rigid human-crafted rules, ProX allows models to generate and execute specific operations for each data example, enhancing flexibility and effectiveness. The results show that even smaller models can achieve data refinement capabilities similar to those of human experts, leading to improved performance on various tasks. ProX not only boosts accuracy but also reduces the computational resources needed for training, making it a promising method for efficient language model pre-training."
                },
                "zh": {
                    "title": "ProXÔºö‰∏™ÊÄßÂåñÊï∞ÊçÆÁ≤æÁÇºÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊï∞ÊçÆÁ≤æÁÇºÊ°ÜÊû∂ÔºåÁß∞‰∏∫Programming Every ExampleÔºàProXÔºâÔºåÊó®Âú®ÊèêÈ´òÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆË¥®Èáè„ÄÇProXÈÄöËøáÂ∞ÜÊï∞ÊçÆÁ≤æÁÇºËßÜ‰∏∫ÁºñÁ®ã‰ªªÂä°Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§ü‰∏∫ÊØè‰∏™Á§∫‰æãÁîüÊàêÂíåÊâßË°åÁªÜÁ≤íÂ∫¶Êìç‰ΩúÔºå‰ªéËÄåÂÆûÁé∞Êï∞ÊçÆÁöÑ‰∏™ÊÄßÂåñÂ§ÑÁêÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®ProXÁ≤æÁÇºÁöÑÊï∞ÊçÆÂú®Â§ö‰∏™‰∏ãÊ∏∏‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫éÂéüÂßãÊï∞ÊçÆÂíåÂÖ∂‰ªñÁ≠õÈÄâÊñπÊ≥ï„ÄÇËØ•ÊñπÊ≥ïÂú®‰∏çÂêåÊ®°ÂûãËßÑÊ®°ÂíåÈ¢ÑËÆ≠ÁªÉËØ≠ÊñôÂ∫ì‰∏≠ÂùáÊòæÁ§∫Âá∫ÊòæËëóÁöÑÊïàÊûúÔºåÂ∞§ÂÖ∂Âú®ÁâπÂÆöÈ¢ÜÂüüÁöÑÊåÅÁª≠È¢ÑËÆ≠ÁªÉ‰∏≠Ë°®Áé∞Âá∫Ëâ≤„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.15127",
            "title": "Boosting Healthcare LLMs Through Retrieved Context",
            "url": "https://huggingface.co/papers/2409.15127",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing, and yet, their factual inaccuracies and hallucinations limits their application, particularly in critical domains like healthcare. Context retrieval methods, by introducing relevant information as input, have emerged as a crucial approach for enhancing LLM factuality and reliability. This study explores the boundaries of context retrieval methods within the healthcare domain, optimizing their components and benchmarking their performance against open and closed alternatives. Our findings reveal how open LLMs, when augmented with an optimized retrieval system, can achieve performance comparable to the biggest private solutions on established healthcare benchmarks (multiple-choice question answering). Recognizing the lack of realism of including the possible answers within the question (a setup only found in medical exams), and after assessing a strong LLM performance degradation in the absence of those options, we extend the context retrieval system in that direction. In particular, we propose OpenMedPrompt a pipeline that improves the generation of more reliable open-ended answers, moving this technology closer to practical application.",
            "score": 19,
            "issue_id": 1,
            "pub_date": "2024-09-23",
            "pub_date_card": {
                "ru": "23 —Å–µ–Ω—Ç—è–±—Ä—è",
                "en": "September 23",
                "zh": "9Êúà23Êó•"
            },
            "hash": "3a7c5c8e7a8d8071",
            "data": {
                "categories": [
                    "#science",
                    "#hallucinations",
                    "#long_context",
                    "#rag",
                    "#healthcare",
                    "#benchmark",
                    "#open_source",
                    "#architecture"
                ],
                "emoji": "üè•",
                "ru": {
                    "title": "–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –º–µ–¥–∏—Ü–∏–Ω–µ —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞",
                    "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –º–µ—Ç–æ–¥–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤ –æ–±–ª–∞—Å—Ç–∏ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç –µ–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –∏ –∑–∞–∫—Ä—ã—Ç—ã–º–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–µ LLM —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–≥—É—Ç –¥–æ—Å—Ç–∏—á—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–π —Å –∫—Ä—É–ø–Ω–µ–π—à–∏–º–∏ —á–∞—Å—Ç–Ω—ã–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏ –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø–∞–π–ø–ª–∞–π–Ω OpenMedPrompt –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã."
                },
                "en": {
                    "title": "Enhancing Healthcare LLMs with Context Retrieval for Reliable Answers",
                    "desc": "This paper discusses how Large Language Models (LLMs) can struggle with providing accurate information, especially in sensitive areas like healthcare. To improve their reliability, the authors focus on context retrieval methods that supply relevant information to the LLMs. They benchmark these methods against existing solutions and find that optimized retrieval systems can enhance the performance of open LLMs to match that of private models on healthcare tasks. The study introduces OpenMedPrompt, a new pipeline designed to generate more accurate open-ended responses, making LLMs more applicable in real-world healthcare scenarios."
                },
                "zh": {
                    "title": "‰ºòÂåñÊ£ÄÁ¥¢Á≥ªÁªüÔºåÊèêÂçáÂåªÁñóÈ¢ÜÂüüLLMÁöÑÂèØÈù†ÊÄß",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÂåªÁñóÁ≠âÂÖ≥ÈîÆÈ¢ÜÂüüÁöÑ‰∫ãÂÆûÂáÜÁ°ÆÊÄßÂíåÂπªËßâÈóÆÈ¢òÈôêÂà∂‰∫ÜÂÆÉ‰ª¨ÁöÑÂ∫îÁî®„ÄÇÈÄöËøáÂºïÂÖ•Áõ∏ÂÖ≥‰ø°ÊÅØ‰Ωú‰∏∫ËæìÂÖ•Ôºå‰∏ä‰∏ãÊñáÊ£ÄÁ¥¢ÊñπÊ≥ïÊàê‰∏∫ÊèêÈ´òLLM‰∫ãÂÆûÊÄßÂíåÂèØÈù†ÊÄßÁöÑÈáçË¶ÅÊâãÊÆµ„ÄÇÊú¨ÊñáÁ†îÁ©∂‰∫Ü‰∏ä‰∏ãÊñáÊ£ÄÁ¥¢ÊñπÊ≥ïÂú®ÂåªÁñóÈ¢ÜÂüüÁöÑÂ∫îÁî®Ôºå‰ºòÂåñÂÖ∂ÁªÑ‰ª∂Âπ∂‰∏éÂÖ∂‰ªñÊñπÊ≥ïËøõË°åÊÄßËÉΩÂü∫ÂáÜÊµãËØï„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÁªèËøá‰ºòÂåñÁöÑÊ£ÄÁ¥¢Á≥ªÁªüÂèØ‰ª•‰ΩøÂºÄÊîæÂºèLLMÂú®ÂåªÁñóÂü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞‰∏éÂ§ßÂûãÁßÅÊúâËß£ÂÜ≥ÊñπÊ°àÁõ∏ÂΩìÁöÑÊÄßËÉΩÔºåÂπ∂ÊèêÂá∫‰∫ÜOpenMedPrompt‰ª•ÁîüÊàêÊõ¥ÂèØÈù†ÁöÑÂºÄÊîæÂºèÁ≠îÊ°à„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17145",
            "title": "DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion",
            "url": "https://huggingface.co/papers/2409.17145",
            "abstract": "Leveraging pretrained 2D diffusion models and score distillation sampling (SDS), recent methods have shown promising results for text-to-3D avatar generation. However, generating high-quality 3D avatars capable of expressive animation remains challenging. In this work, we present DreamWaltz-G, a novel learning framework for animatable 3D avatar generation from text. The core of this framework lies in Skeleton-guided Score Distillation and Hybrid 3D Gaussian Avatar representation. Specifically, the proposed skeleton-guided score distillation integrates skeleton controls from 3D human templates into 2D diffusion models, enhancing the consistency of SDS supervision in terms of view and human pose. This facilitates the generation of high-quality avatars, mitigating issues such as multiple faces, extra limbs, and blurring. The proposed hybrid 3D Gaussian avatar representation builds on the efficient 3D Gaussians, combining neural implicit fields and parameterized 3D meshes to enable real-time rendering, stable SDS optimization, and expressive animation. Extensive experiments demonstrate that DreamWaltz-G is highly effective in generating and animating 3D avatars, outperforming existing methods in both visual quality and animation expressiveness. Our framework further supports diverse applications, including human video reenactment and multi-subject scene composition.",
            "score": 13,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 —Å–µ–Ω—Ç—è–±—Ä—è",
                "en": "September 25",
                "zh": "9Êúà25Êó•"
            },
            "hash": "629ce97635711d75",
            "data": {
                "categories": [
                    "#cv",
                    "#optimization",
                    "#diffusion",
                    "#architecture",
                    "#3d"
                ],
                "emoji": "üï∫",
                "ru": {
                    "title": "–¢–∞–Ω—Ü—É—é—â–∏–µ –∞–≤–∞—Ç–∞—Ä—ã: –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∫ –∞–Ω–∏–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–º 3D-–º–æ–¥–µ–ª—è–º",
                    "desc": "DreamWaltz-G - —ç—Ç–æ –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∞–Ω–∏–º–∏—Ä—É–µ–º—ã—Ö 3D-–∞–≤–∞—Ç–∞—Ä–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ø—Ä–∞–≤–ª—è–µ–º—É—é —Å–∫–µ–ª–µ—Ç–æ–º –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –æ—Ü–µ–Ω–æ–∫ –∏ –≥–∏–±—Ä–∏–¥–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ 3D-–∞–≤–∞—Ç–∞—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–∞—É—Å—Å–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π. –ú–µ—Ç–æ–¥ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª—å —Å–∫–µ–ª–µ—Ç–∞ –∏–∑ 3D-—à–∞–±–ª–æ–Ω–æ–≤ —á–µ–ª–æ–≤–µ–∫–∞ –≤ 2D-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏, —á—Ç–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤—Ä–æ–¥–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ª–∏—Ü –∏–ª–∏ –ª–∏—à–Ω–∏—Ö –∫–æ–Ω–µ—á–Ω–æ—Å—Ç–µ–π. DreamWaltz-G –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –∫–∞—á–µ—Å—Ç–≤—É –∏ –≤—ã—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω–∏–º–∞—Ü–∏–∏."
                },
                "en": {
                    "title": "DreamWaltz-G: Transforming Text to Lively 3D Avatars!",
                    "desc": "This paper introduces DreamWaltz-G, a new framework for creating 3D avatars from text that can be animated. It uses a technique called Skeleton-guided Score Distillation (SDS) to improve the quality of the generated avatars by incorporating 3D human skeletons into 2D diffusion models. The framework also employs a Hybrid 3D Gaussian representation, which combines different 3D modeling techniques for better rendering and animation. The results show that DreamWaltz-G produces high-quality, expressive avatars and outperforms previous methods in visual quality and animation capabilities."
                },
                "zh": {
                    "title": "DreamWaltz-GÔºöÊñáÊú¨ÁîüÊàêÂèØÂä®Áîª3DÂ§¥ÂÉèÁöÑÊñ∞Ê°ÜÊû∂",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫DreamWaltz-GÁöÑÊñ∞Ê°ÜÊû∂ÔºåÁî®‰∫é‰ªéÊñáÊú¨ÁîüÊàêÂèØÂä®ÁîªÁöÑ3DÂ§¥ÂÉè„ÄÇËØ•Ê°ÜÊû∂ÁöÑÊ†∏ÂøÉÊòØÈ™®Êû∂ÂºïÂØºÁöÑÂæóÂàÜËí∏È¶èÂíåÊ∑∑Âêà3DÈ´òÊñØÂ§¥ÂÉèË°®Á§∫ÔºåËÉΩÂ§üÊèêÈ´òÁîüÊàêÂ§¥ÂÉèÁöÑ‰∏ÄËá¥ÊÄßÂíåË¥®Èáè„ÄÇÈÄöËøáÂ∞Ü3D‰∫∫Á±ªÊ®°ÊùøÁöÑÈ™®Êû∂ÊéßÂà∂Êï¥ÂêàÂà∞2DÊâ©Êï£Ê®°Âûã‰∏≠ÔºåËß£ÂÜ≥‰∫ÜÂ§öÈù¢Â≠î„ÄÅÈ¢ùÂ§ñËÇ¢‰ΩìÂíåÊ®°Á≥äÁ≠âÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDreamWaltz-GÂú®ÁîüÊàêÂíåÂä®Áîª3DÂ§¥ÂÉèÊñπÈù¢Ë°®Áé∞‰ºòÂºÇÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.15041",
            "title": "AIM 2024 Sparse Neural Rendering Challenge: Dataset and Benchmark",
            "url": "https://huggingface.co/papers/2409.15041",
            "abstract": "Recent developments in differentiable and neural rendering have made impressive breakthroughs in a variety of 2D and 3D tasks, e.g. novel view synthesis, 3D reconstruction. Typically, differentiable rendering relies on a dense viewpoint coverage of the scene, such that the geometry can be disambiguated from appearance observations alone. Several challenges arise when only a few input views are available, often referred to as sparse or few-shot neural rendering. As this is an underconstrained problem, most existing approaches introduce the use of regularisation, together with a diversity of learnt and hand-crafted priors. A recurring problem in sparse rendering literature is the lack of an homogeneous, up-to-date, dataset and evaluation protocol. While high-resolution datasets are standard in dense reconstruction literature, sparse rendering methods often evaluate with low-resolution images. Additionally, data splits are inconsistent across different manuscripts, and testing ground-truth images are often publicly available, which may lead to over-fitting. In this work, we propose the Sparse Rendering (SpaRe) dataset and benchmark. We introduce a new dataset that follows the setup of the DTU MVS dataset. The dataset is composed of 97 new scenes based on synthetic, high-quality assets. Each scene has up to 64 camera views and 7 lighting configurations, rendered at 1600x1200 resolution. We release a training split of 82 scenes to foster generalizable approaches, and provide an online evaluation platform for the validation and test sets, whose ground-truth images remain hidden. We propose two different sparse configurations (3 and 9 input images respectively). This provides a powerful and convenient tool for reproducible evaluation, and enable researchers easy access to a public leaderboard with the state-of-the-art performance scores. Available at: https://sparebenchmark.github.io/",
            "score": 12,
            "issue_id": 1,
            "pub_date": "2024-09-23",
            "pub_date_card": {
                "ru": "23 —Å–µ–Ω—Ç—è–±—Ä—è",
                "en": "September 23",
                "zh": "9Êúà23Êó•"
            },
            "hash": "94750a64c54ff82d",
            "data": {
                "categories": [
                    "#dataset",
                    "#benchmark",
                    "#open_source",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "üé•",
                "ru": {
                    "title": "SpaRe: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –∑–∞–¥–∞—á–∏ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –≤—Ö–æ–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (sparse rendering). –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç SpaRe, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 97 –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ä–∞–∫—É—Ä—Å–∞–º–∏ –∫–∞–º–µ—Ä –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏ –æ—Å–≤–µ—â–µ–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω—ã –¥–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å 3 –∏ 9 –≤—Ö–æ–¥–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤. –¢–∞–∫–∂–µ –∞–≤—Ç–æ—Ä—ã –∑–∞–ø—É—Å—Ç–∏–ª–∏ –æ–Ω–ª–∞–π–Ω-–ø–ª–∞—Ç—Ñ–æ—Ä–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Å–∫—Ä—ã—Ç–æ–º —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –∏ –ø—É–±–ª–∏—á–Ω—ã–π –ª–∏–¥–µ—Ä–±–æ—Ä–¥ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤."
                },
                "en": {
                    "title": "Advancing Sparse Rendering with the SpaRe Dataset",
                    "desc": "This paper presents the Sparse Rendering (SpaRe) dataset and benchmark, addressing the challenges in few-shot neural rendering. It highlights the need for a consistent and high-resolution dataset, as existing methods often rely on low-quality images and inconsistent data splits. The SpaRe dataset includes 97 synthetic scenes with multiple camera views and lighting conditions, designed to facilitate the evaluation of sparse rendering techniques. By providing a public leaderboard and an online evaluation platform, this work aims to promote reproducibility and advance research in the field of sparse rendering."
                },
                "zh": {
                    "title": "Á®ÄÁñèÊ∏≤ÊüìÊñ∞Êï∞ÊçÆÈõÜÔºåÂä©ÂäõÁ•ûÁªèÊ∏≤ÊüìÁ†îÁ©∂",
                    "desc": "ÊúÄËøëÂú®ÂèØÂæÆÊ∏≤ÊüìÂíåÁ•ûÁªèÊ∏≤ÊüìÊñπÈù¢ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºåÂ∞§ÂÖ∂ÊòØÂú®2DÂíå3D‰ªªÂä°‰∏≠ÔºåÂ¶ÇÊñ∞ËßÜËßíÂêàÊàêÂíå3DÈáçÂª∫„ÄÇÂèØÂæÆÊ∏≤ÊüìÈÄöÂ∏∏‰æùËµñ‰∫éÂú∫ÊôØÁöÑÂØÜÈõÜËßÜËßíË¶ÜÁõñÔºå‰ª•‰æø‰ªéÂ§ñËßÇËßÇÂØü‰∏≠Âå∫ÂàÜÂá†‰ΩïÂΩ¢Áä∂„ÄÇÁÑ∂ËÄåÔºåÂΩìÂè™ÊúâÂ∞ëÈáèËæìÂÖ•ËßÜÂõæÂèØÁî®Êó∂ÔºåÈÄöÂ∏∏‰ºöÈù¢‰∏¥Á®ÄÁñèÊàñÂ∞ëÊ†∑Êú¨Á•ûÁªèÊ∏≤ÊüìÁöÑÊåëÊàò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜÁ®ÄÁñèÊ∏≤ÊüìÔºàSpaReÔºâÊï∞ÊçÆÈõÜÂíåÂü∫ÂáÜÊµãËØïÔºåÊó®Âú®Êèê‰æõ‰∏Ä‰∏™‰∏ÄËá¥ÁöÑËØÑ‰º∞Âπ≥Âè∞Ôºå‰øÉËøõÂèØÈáçÂ§çÁöÑËØÑ‰º∞ÂíåÁ†îÁ©∂„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.17058",
            "title": "Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors",
            "url": "https://huggingface.co/papers/2409.17058",
            "abstract": "Diffusion-based image super-resolution (SR) methods have achieved remarkable success by leveraging large pre-trained text-to-image diffusion models as priors. However, these methods still face two challenges: the requirement for dozens of sampling steps to achieve satisfactory results, which limits efficiency in real scenarios, and the neglect of degradation models, which are critical auxiliary information in solving the SR problem. In this work, we introduced a novel one-step SR model, which significantly addresses the efficiency issue of diffusion-based SR methods. Unlike existing fine-tuning strategies, we designed a degradation-guided Low-Rank Adaptation (LoRA) module specifically for SR, which corrects the model parameters based on the pre-estimated degradation information from low-resolution images. This module not only facilitates a powerful data-dependent or degradation-dependent SR model but also preserves the generative prior of the pre-trained diffusion model as much as possible. Furthermore, we tailor a novel training pipeline by introducing an online negative sample generation strategy. Combined with the classifier-free guidance strategy during inference, it largely improves the perceptual quality of the super-resolution results. Extensive experiments have demonstrated the superior efficiency and effectiveness of the proposed model compared to recent state-of-the-art methods.",
            "score": 11,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 —Å–µ–Ω—Ç—è–±—Ä—è",
                "en": "September 25",
                "zh": "9Êúà25Êó•"
            },
            "hash": "c52ca2b156d80f27",
            "data": {
                "categories": [
                    "#cv",
                    "#training",
                    "#inference",
                    "#optimization",
                    "#diffusion",
                    "#architecture"
                ],
                "emoji": "üîç",
                "ru": {
                    "title": "–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å–≤–µ—Ä—Ö—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞ –æ–¥–∏–Ω —à–∞–≥ —Å –ø–æ–º–æ—â—å—é –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π",
                    "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Å–≤–µ—Ä—Ö—Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ö–ª—é—á–µ–≤–æ–µ –Ω–æ–≤—à–µ—Å—Ç–≤–æ - –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∑–∞ –æ–¥–∏–Ω —à–∞–≥, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞. –í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–æ–¥—É–ª—å –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ (LoRA), —É—á–∏—Ç—ã–≤–∞—é—â–∏–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –¢–∞–∫–∂–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –Ω–æ–≤—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –æ–±—É—á–µ–Ω–∏—è —Å –æ–Ω–ª–∞–π–Ω-–≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –Ω–∞–¥ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∞–Ω–∞–ª–æ–≥–∞–º–∏."
                },
                "en": {
                    "title": "Efficient Super-Resolution with Degradation-Guided Diffusion Models",
                    "desc": "This paper presents a new approach to image super-resolution (SR) using diffusion models, focusing on improving efficiency and incorporating degradation models. The authors introduce a one-step SR model that reduces the number of sampling steps needed, making it faster for real-world applications. They also propose a Low-Rank Adaptation (LoRA) module that adjusts model parameters based on degradation information from low-resolution images, enhancing the model's performance. The combination of this module with a novel training pipeline and classifier-free guidance leads to better perceptual quality in the generated images, outperforming existing methods."
                },
                "zh": {
                    "title": "È´òÊïàË∂ÖÂàÜËæ®ÁéáÔºö‰∏ÄÈîÆËß£ÂÜ≥ÂõæÂÉèÈÄÄÂåñÈóÆÈ¢ò",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÊâ©Êï£Ê®°ÂûãÁöÑÂõæÂÉèË∂ÖÂàÜËæ®Áéá(SR)Êñ∞ÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÊïàÁéáÂπ∂Ëß£ÂÜ≥Áé∞ÊúâÊñπÊ≥ïÁöÑ‰∏çË∂≥„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™‰ΩéÁß©ÈÄÇÂ∫î(LoRA)Ê®°ÂùóÔºåÂà©Áî®‰ΩéÂàÜËæ®ÁéáÂõæÂÉèÁöÑÈÄÄÂåñ‰ø°ÊÅØÊù•Ë∞ÉÊï¥Ê®°ÂûãÂèÇÊï∞Ôºå‰ªéËÄåÂÆûÁé∞‰∏ÄÈîÆË∂ÖÂàÜËæ®Áéá„ÄÇËØ•Ê®°Âùó‰∏ç‰ªÖÂ¢ûÂº∫‰∫ÜÊ®°ÂûãÁöÑÈÄÇÂ∫îÊÄßÔºåËøòÂ∞ΩÂèØËÉΩ‰øùÁïô‰∫ÜÈ¢ÑËÆ≠ÁªÉÊâ©Êï£Ê®°ÂûãÁöÑÁîüÊàêÂÖàÈ™å„ÄÇÊ≠§Â§ñÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂú®Á∫øË¥üÊ†∑Êú¨ÁîüÊàêÁ≠ñÁï•ÔºåÁªìÂêàÊó†ÂàÜÁ±ªÂô®ÂºïÂØºÁ≠ñÁï•ÔºåÊòæËëóÊèêÂçá‰∫ÜË∂ÖÂàÜËæ®ÁéáÁªìÊûúÁöÑÊÑüÁü•Ë¥®Èáè„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.16299",
            "title": "HyperAgent: Generalist Software Engineering Agents to Solve Coding Tasks at Scale",
            "url": "https://huggingface.co/papers/2409.16299",
            "abstract": "Large Language Models (LLMs) have revolutionized software engineering (SE), demonstrating remarkable capabilities in various coding tasks. While recent efforts have produced autonomous software agents based on LLMs for end-to-end development tasks, these systems are typically designed for specific SE tasks. We introduce HyperAgent, a novel generalist multi-agent system designed to address a wide spectrum of SE tasks across different programming languages by mimicking human developers' workflows. Comprising four specialized agents - Planner, Navigator, Code Editor, and Executor. HyperAgent manages the full lifecycle of SE tasks, from initial conception to final verification. Through extensive evaluations, HyperAgent achieves state-of-the-art performance across diverse SE tasks: it attains a 25.01% success rate on SWE-Bench-Lite and 31.40% on SWE-Bench-Verified for GitHub issue resolution, surpassing existing methods. Furthermore, HyperAgent demonstrates SOTA performance in repository-level code generation (RepoExec), and in fault localization and program repair (Defects4J), often outperforming specialized systems. This work represents a significant advancement towards versatile, autonomous agents capable of handling complex, multi-step SE tasks across various domains and languages, potentially transforming AI-assisted software development practices.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-09",
            "pub_date_card": {
                "ru": "9 —Å–µ–Ω—Ç—è–±—Ä—è",
                "en": "September 9",
                "zh": "9Êúà9Êó•"
            },
            "hash": "a713e3f82d512439",
            "data": {
                "categories": [
                    "#reasoning",
                    "#multilingual",
                    "#agi",
                    "#optimization",
                    "#plp",
                    "#agents",
                    "#benchmark",
                    "#architecture"
                ],
                "emoji": "ü§ñ",
                "ru": {
                    "title": "HyperAgent: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–≤",
                    "desc": "HyperAgent - —ç—Ç–æ –Ω–æ–≤–∞—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ –∑–∞–¥–∞—á –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —á–µ—Ç—ã—Ä–µ—Ö —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤: –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞, –ù–∞–≤–∏–≥–∞—Ç–æ—Ä–∞, –†–µ–¥–∞–∫—Ç–æ—Ä–∞ –∫–æ–¥–∞ –∏ –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–∏—Ç–∏—Ä—É—é—Ç —Ä–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å —á–µ–ª–æ–≤–µ–∫–∞-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞. HyperAgent –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ GitHub issues, –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é –æ—à–∏–±–æ–∫. –≠—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —à–∞–≥ –≤–ø–µ—Ä–µ–¥ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è."
                },
                "en": {
                    "title": "HyperAgent: Revolutionizing Software Engineering with Multi-Agent Intelligence",
                    "desc": "This paper presents HyperAgent, a generalist multi-agent system that enhances software engineering (SE) by mimicking human workflows. It consists of four specialized agents: Planner, Navigator, Code Editor, and Executor, which together manage the entire lifecycle of SE tasks. HyperAgent has shown superior performance in various coding challenges, achieving notable success rates in GitHub issue resolution and repository-level code generation. This innovation marks a significant step towards creating versatile, autonomous agents that can efficiently tackle complex SE tasks across multiple programming languages."
                },
                "zh": {
                    "title": "HyperAgentÔºöÈÄöÁî®ËΩØ‰ª∂Â∑•Á®ãÁöÑÊô∫ËÉΩ‰ª£ÁêÜ",
                    "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ËΩØ‰ª∂Â∑•Á®ãÔºàSEÔºâÈ¢ÜÂüüÂ∏¶Êù•‰∫ÜÈù©ÂëΩÊÄßÁöÑÂèòÂåñÔºåÂ±ïÁé∞‰∫ÜÂú®ÂêÑÁßçÁºñÁ†Å‰ªªÂä°‰∏≠ÁöÑÂçìË∂äËÉΩÂäõ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜHyperAgentÔºåËøôÊòØ‰∏ÄÁßçÊñ∞ÂûãÁöÑÈÄöÁî®Â§ö‰ª£ÁêÜÁ≥ªÁªüÔºåÊó®Âú®ÈÄöËøáÊ®°Êãü‰∫∫Á±ªÂºÄÂèëËÄÖÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåËß£ÂÜ≥‰∏çÂêåÁºñÁ®ãËØ≠Ë®Ä‰∏≠ÁöÑÂπøÊ≥õSE‰ªªÂä°„ÄÇHyperAgentÁî±Âõõ‰∏™‰∏ì‰∏ö‰ª£ÁêÜÁªÑÊàêÔºöËßÑÂàíËÄÖ„ÄÅÂØºËà™ËÄÖ„ÄÅ‰ª£Á†ÅÁºñËæëÂô®ÂíåÊâßË°åËÄÖÔºåËÉΩÂ§üÁÆ°ÁêÜSE‰ªªÂä°ÁöÑÊï¥‰∏™ÁîüÂëΩÂë®ÊúüÔºå‰ªéÂàùÊ≠•ÊûÑÊÉ≥Âà∞ÊúÄÁªàÈ™åËØÅ„ÄÇÁªèËøáÂπøÊ≥õËØÑ‰º∞ÔºåHyperAgentÂú®Â§öÁßçSE‰ªªÂä°‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåË∂ÖË∂ä‰∫ÜÁé∞ÊúâÊñπÊ≥ïÔºåÊ†áÂøóÁùÄÂêëËÉΩÂ§üÂ§ÑÁêÜÂ§çÊùÇÂ§öÊ≠•È™§SE‰ªªÂä°ÁöÑËá™‰∏ª‰ª£ÁêÜÁöÑÈáçË¶ÅËøõÂ±ï„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.16629",
            "title": "Synchronize Dual Hands for Physics-Based Dexterous Guitar Playing",
            "url": "https://huggingface.co/papers/2409.16629",
            "abstract": "We present a novel approach to synthesize dexterous motions for physically simulated hands in tasks that require coordination between the control of two hands with high temporal precision. Instead of directly learning a joint policy to control two hands, our approach performs bimanual control through cooperative learning where each hand is treated as an individual agent. The individual policies for each hand are first trained separately, and then synchronized through latent space manipulation in a centralized environment to serve as a joint policy for two-hand control. By doing so, we avoid directly performing policy learning in the joint state-action space of two hands with higher dimensions, greatly improving the overall training efficiency. We demonstrate the effectiveness of our proposed approach in the challenging guitar-playing task. The virtual guitarist trained by our approach can synthesize motions from unstructured reference data of general guitar-playing practice motions, and accurately play diverse rhythms with complex chord pressing and string picking patterns based on the input guitar tabs that do not exist in the references. Along with this paper, we provide the motion capture data that we collected as the reference for policy training. Code is available at: https://pei-xu.github.io/guitar.",
            "score": 9,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 —Å–µ–Ω—Ç—è–±—Ä—è",
                "en": "September 25",
                "zh": "9Êúà25Êó•"
            },
            "hash": "319d5a32d76eb024",
            "data": {
                "categories": [
                    "#dataset",
                    "#rl",
                    "#agents",
                    "#games",
                    "#open_source",
                    "#robotics"
                ],
                "emoji": "üé∏",
                "ru": {
                    "title": "–ö–æ–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–≤—É—Ä—É—á–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π",
                    "desc": "–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∏–Ω—Ç–µ–∑—É —Å–ª–æ–∂–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π –¥–ª—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏ —Å–∏–º—É–ª–∏—Ä—É–µ–º—ã—Ö —Ä—É–∫ –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –º–µ–∂–¥—É –¥–≤—É–º—è —Ä—É–∫–∞–º–∏ —Å –≤—ã—Å–æ–∫–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é. –í–º–µ—Å—Ç–æ –ø—Ä—è–º–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤—É–º—è —Ä—É–∫–∞–º–∏, –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –≥–¥–µ –∫–∞–∂–¥–∞—è —Ä—É–∫–∞ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç. –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —Ä—É–∫–∏ —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∞—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ, –∞ –∑–∞—Ç–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É—é—Ç—Å—è —á–µ—Ä–µ–∑ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –≤ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ, —á—Ç–æ–±—ã —Å–ª—É–∂–∏—Ç—å —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–æ–π –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤—É–º—è —Ä—É–∫–∞–º–∏. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–µ –∏–≥—Ä—ã –Ω–∞ –≥–∏—Ç–∞—Ä–µ."
                },
                "en": {
                    "title": "Efficient Bimanual Control through Cooperative Learning",
                    "desc": "This paper introduces a new method for controlling two simulated hands to perform tasks that require precise coordination, like playing the guitar. Instead of creating a single complex policy for both hands, the authors train each hand as a separate agent and then synchronize their movements using latent space manipulation. This approach simplifies the learning process by avoiding the high-dimensional joint state-action space, leading to more efficient training. The results show that their method allows a virtual guitarist to accurately play various rhythms and complex patterns based on guitar tabs, even when trained on unstructured data."
                },
                "zh": {
                    "title": "È´òÊïàÂèåÊâãÊéßÂà∂ÁöÑÂàõÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÂêàÊàêÁâ©ÁêÜÊ®°ÊãüÊâãÁöÑÁÅµÂ∑ßÂä®‰ΩúÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÈ´òÊó∂Èó¥Á≤æÂ∫¶ÁöÑÂèåÊâãÂçèË∞É‰ªªÂä°‰∏≠„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÈÄöËøáÂêà‰ΩúÂ≠¶‰π†ÂÆûÁé∞ÂèåÊâãÊéßÂà∂ÔºåÂ∞ÜÊØèÂè™ÊâãËßÜ‰∏∫Áã¨Á´ãÁöÑÊô∫ËÉΩ‰ΩìÔºåËÄå‰∏çÊòØÁõ¥Êé•Â≠¶‰π†ÊéßÂà∂‰∏§Âè™ÊâãÁöÑËÅîÂêàÁ≠ñÁï•„ÄÇÊØèÂè™ÊâãÁöÑ‰∏™‰ΩìÁ≠ñÁï•È¶ñÂÖàÂçïÁã¨ËÆ≠ÁªÉÔºåÁÑ∂ÂêéÈÄöËøáÊΩúÂú®Á©∫Èó¥Êìç‰ΩúÂú®ÈõÜ‰∏≠ÁéØÂ¢É‰∏≠ÂêåÊ≠•Ôºå‰ª•ÂΩ¢ÊàêÂèåÊâãÊéßÂà∂ÁöÑËÅîÂêàÁ≠ñÁï•„ÄÇÊàë‰ª¨Âú®ÊåëÊàòÊÄßÁöÑÂêâ‰ªñÊºîÂ•è‰ªªÂä°‰∏≠È™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåËÆ≠ÁªÉÂá∫ÁöÑËôöÊãüÂêâ‰ªñÊâãËÉΩÂ§ü‰ªéÊó†ÁªìÊûÑÁöÑÂèÇËÄÉÊï∞ÊçÆ‰∏≠ÂêàÊàêÂä®‰ΩúÔºåÂáÜÁ°ÆÊºîÂ•èÂ§çÊùÇÁöÑËäÇÂ•èÂíåÂíåÂº¶„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.16493",
            "title": "NoTeeline: Supporting Real-Time Notetaking from Keypoints with Large Language Models",
            "url": "https://huggingface.co/papers/2409.16493",
            "abstract": "Video has become a popular media form for information sharing and consumption. However, taking notes while watching a video requires significant time and effort. To address this, we propose a novel interactive system, NoTeeline, for taking real-time, personalized notes. NoTeeline lets users quickly jot down keypoints (micronotes), which are automatically expanded into full-fledged notes that capture the content of the user's micronotes and are consistent with the user's writing style. In a within-subjects study (N=12), we found that NoTeeline helps users create high-quality notes that capture the essence of their micronotes with a higher factual correctness (93.2%) while accurately reflecting their writing style. While using NoTeeline, participants experienced significantly reduced mental effort, captured satisfactory notes while writing 47% less text, and completed notetaking with 43.9% less time compared to a manual notetaking baseline.",
            "score": 8,
            "issue_id": 1,
            "pub_date": "2024-09-24",
            "pub_date_card": {
                "ru": "24 —Å–µ–Ω—Ç—è–±—Ä—è",
                "en": "September 24",
                "zh": "9Êúà24Êó•"
            },
            "hash": "83e7a802e2a7e4e8",
            "data": {
                "categories": [
                    "#video",
                    "#multimodal"
                ],
                "emoji": "üìù",
                "ru": {
                    "title": "NoTeeline: —É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Å–ø–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ",
                    "desc": "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ —Å–∏—Å—Ç–µ–º—É NoTeeline –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–º–µ—Ç–æ–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –ø—Ä–∏ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ –≤–∏–¥–µ–æ. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –±—ã—Å—Ç—Ä–æ –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞—Å—à–∏—Ä—è—é—Ç—Å—è –≤ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–∏–ª—é –ø–∏—Å—å–º–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ NoTeeline –ø–æ–º–æ–≥–∞–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏–∏ –∑–∞—Ç—Ä–∞—á–∏–≤–∞–µ–º–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ —É—Å–∏–ª–∏–π. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ä—É—á–Ω—ã–º –≤–µ–¥–µ–Ω–∏–µ–º –∑–∞–º–µ—Ç–æ–∫, –≤–∫–ª—é—á–∞—è —Å–Ω–∏–∂–µ–Ω–∏–µ –º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—ä–µ–º–∞ –Ω–∞–ø–∏—Å–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."
                },
                "en": {
                    "title": "Revolutionizing Video Note-Taking with NoTeeline",
                    "desc": "The paper presents NoTeeline, an innovative interactive system designed to enhance the note-taking process while watching videos. It allows users to create quick, concise notes called micronotes, which are then transformed into comprehensive notes that align with the user's unique writing style. A study involving 12 participants demonstrated that NoTeeline significantly improves the quality and factual accuracy of notes, achieving a correctness rate of 93.2%. Additionally, users reported reduced mental effort, less text written (47% less), and faster completion times (43.9% less) compared to traditional note-taking methods."
                },
                "zh": {
                    "title": "ÂÆûÊó∂‰∏™ÊÄßÂåñÁ¨îËÆ∞ÔºåËΩªÊùæËÆ∞ÂΩïËßÜÈ¢ëÁ≤æÂçé",
                    "desc": "Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫NoTeelineÁöÑ‰∫íÂä®Á≥ªÁªüÔºåÊó®Âú®Â∏ÆÂä©Áî®Êà∑Âú®ËßÇÁúãËßÜÈ¢ëÊó∂ÂÆûÊó∂ËÆ∞ÂΩï‰∏™ÊÄßÂåñÁ¨îËÆ∞„ÄÇÁî®Êà∑ÂèØ‰ª•Âø´ÈÄüËÆ∞ÂΩïÂÖ≥ÈîÆÁÇπÔºàÂæÆÁ¨îËÆ∞ÔºâÔºåÁ≥ªÁªü‰ºöËá™Âä®Â∞ÜÂÖ∂Êâ©Â±ï‰∏∫ÂÆåÊï¥ÁöÑÁ¨îËÆ∞ÔºåÁ°Æ‰øùÂÜÖÂÆπ‰∏éÁî®Êà∑ÁöÑÂÜô‰ΩúÈ£éÊ†º‰∏ÄËá¥„ÄÇÁ†îÁ©∂ÁªìÊûúÊòæÁ§∫Ôºå‰ΩøÁî®NoTeelineÁöÑÁî®Êà∑ËÉΩÂ§ü‰ª•Êõ¥È´òÁöÑÂáÜÁ°ÆÊÄßÔºà93.2%ÔºâÂàõÂª∫È´òË¥®ÈáèÁöÑÁ¨îËÆ∞ÔºåÂêåÊó∂ÂáèÂ∞ë‰∫Ü47%ÁöÑÊñáÊú¨ËæìÂÖ•ÈáèÂíå43.9%ÁöÑÊó∂Èó¥Ê∂àËÄó„ÄÇËØ•Á≥ªÁªüÊòæËëóÈôç‰Ωé‰∫ÜÁî®Êà∑ÁöÑÂøÉÁêÜË¥üÊãÖÔºåÊèêÈ´ò‰∫ÜÁ¨îËÆ∞ÁöÑÊª°ÊÑèÂ∫¶„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.16925",
            "title": "Game4Loc: A UAV Geo-Localization Benchmark from Game Data",
            "url": "https://huggingface.co/papers/2409.16925",
            "abstract": "The vision-based geo-localization technology for UAV, serving as a secondary source of GPS information in addition to the global navigation satellite systems (GNSS), can still operate independently in the GPS-denied environment. Recent deep learning based methods attribute this as the task of image matching and retrieval. By retrieving drone-view images in geo-tagged satellite image database, approximate localization information can be obtained. However, due to high costs and privacy concerns, it is usually difficult to obtain large quantities of drone-view images from a continuous area. Existing drone-view datasets are mostly composed of small-scale aerial photography with a strong assumption that there exists a perfect one-to-one aligned reference image for any query, leaving a significant gap from the practical localization scenario. In this work, we construct a large-range contiguous area UAV geo-localization dataset named GTA-UAV, featuring multiple flight altitudes, attitudes, scenes, and targets using modern computer games. Based on this dataset, we introduce a more practical UAV geo-localization task including partial matches of cross-view paired data, and expand the image-level retrieval to the actual localization in terms of distance (meters). For the construction of drone-view and satellite-view pairs, we adopt a weight-based contrastive learning approach, which allows for effective learning while avoiding additional post-processing matching steps. Experiments demonstrate the effectiveness of our data and training method for UAV geo-localization, as well as the generalization capabilities to real-world scenarios.",
            "score": 6,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 —Å–µ–Ω—Ç—è–±—Ä—è",
                "en": "September 25",
                "zh": "9Êúà25Êó•"
            },
            "hash": "bc7c7309053e8db8",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#training",
                    "#graphs",
                    "#optimization",
                    "#games",
                    "#synthetic",
                    "#3d"
                ],
                "emoji": "üõ∞Ô∏è",
                "ru": {
                    "title": "–ì–µ–æ–ª–æ–∫–∞—Ü–∏—è –ë–ü–õ–ê –±–µ–∑ GPS: –Ω–æ–≤—ã–π –≤–∑–≥–ª—è–¥ —Å –≤—ã—Å–æ—Ç—ã –ø—Ç–∏—á—å–µ–≥–æ –ø–æ–ª–µ—Ç–∞",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–æ–ª–æ–∫–∞—Ü–∏–∏ –±–µ—Å–ø–∏–ª–æ—Ç–Ω—ã—Ö –ª–µ—Ç–∞—Ç–µ–ª—å–Ω—ã—Ö –∞–ø–ø–∞—Ä–∞—Ç–æ–≤ (–ë–ü–õ–ê) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã GPS. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –º–∞—Å—à—Ç–∞–±–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö GTA-UAV, —Å–∏–º—É–ª–∏—Ä—É—é—â–∏–π —Ä–∞–∑–ª–∏—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –ø–æ–ª–µ—Ç–∞ –ë–ü–õ–ê. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ë–ü–õ–ê –∏ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã—Ö —Å–Ω–∏–º–∫–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ –∏ –µ–≥–æ –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö."
                },
                "en": {
                    "title": "Revolutionizing UAV Localization with GTA-UAV Dataset",
                    "desc": "This paper presents a new dataset called GTA-UAV for vision-based geo-localization of UAVs, which can function without GPS in areas where satellite signals are unavailable. The authors address the limitations of existing datasets that assume perfect image alignment, which is often unrealistic in practical situations. They propose a novel task that includes partial matches between drone-view and satellite-view images, enhancing the localization process by measuring actual distances. The study employs a weight-based contrastive learning method to improve the learning process and demonstrates the effectiveness of their approach through experiments that show good performance in real-world applications."
                },
                "zh": {
                    "title": "Êó†‰∫∫Êú∫Âú∞ÁêÜÂÆö‰ΩçÁöÑÊñ∞Á™ÅÁ†¥",
                    "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂü∫‰∫éËßÜËßâÁöÑÊó†‰∫∫Êú∫Âú∞ÁêÜÂÆö‰ΩçÊäÄÊúØÔºå‰Ωú‰∏∫ÂÖ®ÁêÉÂØºËà™Âç´ÊòüÁ≥ªÁªüÔºàGNSSÔºâÁöÑËæÖÂä©‰ø°ÊÅØÊ∫êÔºåËÉΩÂ§üÂú®Ê≤°ÊúâGPS‰ø°Âè∑ÁöÑÁéØÂ¢É‰∏≠Áã¨Á´ãÂ∑•‰Ωú„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫GTA-UAVÁöÑÂ§ßËåÉÂõ¥ËøûÁª≠Âå∫ÂüüÊó†‰∫∫Êú∫Âú∞ÁêÜÂÆö‰ΩçÊï∞ÊçÆÈõÜÔºåÂåÖÂê´Â§öÁßçÈ£ûË°åÈ´òÂ∫¶„ÄÅÂßøÊÄÅ„ÄÅÂú∫ÊôØÂíåÁõÆÊ†á„ÄÇÈÄöËøáÈááÁî®Âü∫‰∫éÊùÉÈáçÁöÑÂØπÊØîÂ≠¶‰π†ÊñπÊ≥ïÔºåÊàë‰ª¨ÂÆûÁé∞‰∫ÜÊó†‰∫∫Êú∫ËßÜËßí‰∏éÂç´ÊòüËßÜËßíÂõæÂÉèÂØπÁöÑÊúâÊïàÂåπÈÖçÔºåÈÅøÂÖç‰∫ÜÈ¢ùÂ§ñÁöÑÂêéÂ§ÑÁêÜÊ≠•È™§„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Êó†‰∫∫Êú∫Âú∞ÁêÜÂÆö‰Ωç‰ªªÂä°‰∏≠ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊïàÊûúÂíåÂÆûÈôÖÂú∫ÊôØÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.16288",
            "title": "Self-Supervised Any-Point Tracking by Contrastive Random Walks",
            "url": "https://huggingface.co/papers/2409.16288",
            "abstract": "We present a simple, self-supervised approach to the Tracking Any Point (TAP) problem. We train a global matching transformer to find cycle consistent tracks through video via contrastive random walks, using the transformer's attention-based global matching to define the transition matrices for a random walk on a space-time graph. The ability to perform \"all pairs\" comparisons between points allows the model to obtain high spatial precision and to obtain a strong contrastive learning signal, while avoiding many of the complexities of recent approaches (such as coarse-to-fine matching). To do this, we propose a number of design decisions that allow global matching architectures to be trained through self-supervision using cycle consistency. For example, we identify that transformer-based methods are sensitive to shortcut solutions, and propose a data augmentation scheme to address them. Our method achieves strong performance on the TapVid benchmarks, outperforming previous self-supervised tracking methods, such as DIFT, and is competitive with several supervised methods.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2024-09-24",
            "pub_date_card": {
                "ru": "24 —Å–µ–Ω—Ç—è–±—Ä—è",
                "en": "September 24",
                "zh": "9Êúà24Êó•"
            },
            "hash": "9d0502c19cafc49e",
            "data": {
                "categories": [
                    "#video",
                    "#cv",
                    "#training",
                    "#graphs",
                    "#optimization",
                    "#benchmark",
                    "#games",
                    "#architecture"
                ],
                "emoji": "üéØ",
                "ru": {
                    "title": "–ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –≤–∏–¥–µ–æ",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Å—Ç–æ–π —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∑–∞–¥–∞—á–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ª—é–±–æ–π —Ç–æ—á–∫–∏ (TAP). –ê–≤—Ç–æ—Ä—ã –æ–±—É—á–∞—é—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤ –≤ –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω—ã—Ö —Å–ª—É—á–∞–π–Ω—ã—Ö –±–ª—É–∂–¥–∞–Ω–∏–π. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–Ω–∏–º–∞–Ω–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü –ø–µ—Ä–µ—Ö–æ–¥–∞ –ø—Ä–∏ —Å–ª—É—á–∞–π–Ω–æ–º –±–ª—É–∂–¥–∞–Ω–∏–∏ –ø–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–º—É –≥—Ä–∞—Ñ—É. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö TapVid, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–µ –º–µ—Ç–æ–¥—ã –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è."
                },
                "en": {
                    "title": "Revolutionizing Video Tracking with Self-Supervised Learning",
                    "desc": "This paper introduces a self-supervised method for the Tracking Any Point (TAP) problem using a global matching transformer. The approach leverages contrastive random walks to establish cycle consistent tracks in video data, enhancing spatial precision through all pairs comparisons. By focusing on self-supervision and cycle consistency, the model simplifies the training process while effectively avoiding common pitfalls in tracking methods. The proposed design choices, including a data augmentation strategy, lead to superior performance on the TapVid benchmarks compared to existing self-supervised and some supervised tracking techniques."
                },
                "zh": {
                    "title": "Ëá™ÁõëÁù£‰ªªÊÑèÁÇπË∑üË∏™ÁöÑÊñ∞ÊñπÊ≥ï",
                    "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁÆÄÂçïÁöÑËá™ÁõëÁù£ÊñπÊ≥ïÊù•Ëß£ÂÜ≥‰ªªÊÑèÁÇπË∑üË∏™ÔºàTAPÔºâÈóÆÈ¢ò„ÄÇÊàë‰ª¨ËÆ≠ÁªÉ‰∫Ü‰∏Ä‰∏™ÂÖ®Â±ÄÂåπÈÖçÂèòÊç¢Âô®ÔºåÈÄöËøáÂØπÊØîÈöèÊú∫Ê∏∏Ëµ∞Âú®ËßÜÈ¢ë‰∏≠ÊâæÂà∞Âæ™ÁéØ‰∏ÄËá¥ÁöÑËΩ®Ëøπ„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®ÂèòÊç¢Âô®ÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ËøõË°åÂÖ®Â±ÄÂåπÈÖçÔºå‰ªéËÄåÂÆö‰πâÊó∂Á©∫Âõæ‰∏äÁöÑÈöèÊú∫Ê∏∏Ëµ∞ËΩ¨ÁßªÁü©Èòµ„ÄÇÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®TapVidÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåË∂ÖË∂ä‰∫Ü‰πãÂâçÁöÑËá™ÁõëÁù£Ë∑üË∏™ÊñπÊ≥ïÔºåÂπ∂‰∏é‰∏Ä‰∫õÁõëÁù£ÊñπÊ≥ïÂÖ∑ÊúâÁ´û‰∫âÂäõ„ÄÇ"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.16666",
            "title": "TalkinNeRF: Animatable Neural Fields for Full-Body Talking Humans",
            "url": "https://huggingface.co/papers/2409.16666",
            "abstract": "We introduce a novel framework that learns a dynamic neural radiance field (NeRF) for full-body talking humans from monocular videos. Prior work represents only the body pose or the face. However, humans communicate with their full body, combining body pose, hand gestures, as well as facial expressions. In this work, we propose TalkinNeRF, a unified NeRF-based network that represents the holistic 4D human motion. Given a monocular video of a subject, we learn corresponding modules for the body, face, and hands, that are combined together to generate the final result. To capture complex finger articulation, we learn an additional deformation field for the hands. Our multi-identity representation enables simultaneous training for multiple subjects, as well as robust animation under completely unseen poses. It can also generalize to novel identities, given only a short video as input. We demonstrate state-of-the-art performance for animating full-body talking humans, with fine-grained hand articulation and facial expressions.",
            "score": 5,
            "issue_id": 1,
            "pub_date": "2024-09-25",
            "pub_date_card": {
                "ru": "25 —Å–µ–Ω—Ç—è–±—Ä—è",
                "en": "September 25",
                "zh": "9Êúà25Êó•"
            },
            "hash": "95c442e9c5d9f23c",
            "data": {
                "categories": [
                    "#video",
                    "#architecture",
                    "#cv",
                    "#3d"
                ],
                "emoji": "üó£Ô∏è",
                "ru": {
                    "title": "TalkinNeRF: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –≥–æ–≤–æ—Ä—è—â–∏—Ö –ª—é–¥–µ–π —Å –ø–æ–º–æ—â—å—é NeRF",
                    "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π –∏–∑–ª—É—á–µ–Ω–∏—è (NeRF) –ø–æ–ª–Ω–æ—Ä–∞–∑–º–µ—Ä–Ω—ã—Ö –≥–æ–≤–æ—Ä—è—â–∏—Ö –ª—é–¥–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–Ω–æ–∫—É–ª—è—Ä–Ω—ã—Ö –≤–∏–¥–µ–æ. TalkinNeRF - —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è NeRF-—Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ–µ 4D-–¥–≤–∏–∂–µ–Ω–∏–µ —á–µ–ª–æ–≤–µ–∫–∞, –≤–∫–ª—é—á–∞—è –ø–æ–∑—É —Ç–µ–ª–∞, –∂–µ—Å—Ç—ã —Ä—É–∫ –∏ –º–∏–º–∏–∫—É. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏ –¥–ª—è —Ç–µ–ª–∞, –ª–∏—Ü–∞ –∏ —Ä—É–∫, –∞ —Ç–∞–∫–∂–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ –¥–µ—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è —Å–ª–æ–∂–Ω–æ–π –∞—Ä—Ç–∏–∫—É–ª—è—Ü–∏–∏ –ø–∞–ª—å—Ü–µ–≤. –ú–Ω–æ–≥–æ–∏–¥–µ–Ω—Ç–∏—á–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—É–±—ä–µ–∫—Ç–∞—Ö –∏ –æ–±–æ–±—â–∞—Ç—å –µ–µ –Ω–∞ –Ω–æ–≤—ã–µ –ª–∏—á–Ω–æ—Å—Ç–∏."
                },
                "en": {
                    "title": "Animating Full-Body Talking Humans with TalkinNeRF",
                    "desc": "This paper presents TalkinNeRF, a new framework that learns to create dynamic neural radiance fields (NeRF) for animating full-body talking humans using just monocular videos. Unlike previous methods that focused only on body pose or facial expressions, TalkinNeRF integrates body motion, hand gestures, and facial expressions into a single model. It includes specialized modules for the body, face, and hands, and introduces a deformation field to accurately capture complex hand movements. The framework allows for training on multiple identities and can generate animations for unseen poses, demonstrating advanced capabilities in human motion representation and animation."
                },
                "zh": {
                    "title": "ÂÖ®Ë∫´ËØ¥ËØùÁöÑÂä®ÊÄÅÁ•ûÁªèËæêÂ∞ÑÂú∫",
                    "desc": "Êàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåËÉΩÂ§ü‰ªéÂçïÁõÆËßÜÈ¢ë‰∏≠Â≠¶‰π†Âä®ÊÄÅÁ•ûÁªèËæêÂ∞ÑÂú∫ÔºàNeRFÔºâÔºåÁî®‰∫éÂÖ®Ë∫´ËØ¥ËØùÁöÑ‰∫∫Á±ª„ÄÇ‰ª•ÂæÄÁöÑÁ†îÁ©∂‰ªÖË°®Á§∫Ë∫´‰ΩìÂßøÂäøÊàñÈù¢ÈÉ®Ë°®ÊÉÖÔºåËÄåÊàë‰ª¨ÁöÑÊñπÊ≥ïÁªìÂêà‰∫ÜË∫´‰ΩìÂßøÂäø„ÄÅÊâãÂäøÂíåÈù¢ÈÉ®Ë°®ÊÉÖÔºåÂÖ®Èù¢ÊçïÊçâ‰∫∫Á±ªÁöÑ‰∫§ÊµÅÊñπÂºè„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑTalkinNeRFÁΩëÁªúËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜË∫´‰Ωì„ÄÅÈù¢ÈÉ®ÂíåÊâãÈÉ®ÁöÑËøêÂä®ÔºåÂπ∂ÁîüÊàêÊúÄÁªàÁªìÊûú„ÄÇËØ•ÊñπÊ≥ïÊîØÊåÅÂ§öË∫´‰ªΩË°®Á§∫ÔºåËÉΩÂ§üÂú®Êú™ËßÅËøáÁöÑÂßøÂäø‰∏ãËøõË°åÈ≤ÅÊ£íÂä®ÁîªÔºåÂπ∂‰∏îÂèØ‰ª•Ê†πÊçÆÁü≠ËßÜÈ¢ëËæìÂÖ•ÁîüÊàêÊñ∞ÁöÑË∫´‰ªΩ„ÄÇ"
                }
            }
        }
    ],
    "link_prev": "2024-09-25.html",
    "link_next": "2024-09-27.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "25.09",
        "en": "09/25",
        "zh": "9Êúà25Êó•"
    },
    "short_date_next": {
        "ru": "27.09",
        "en": "09/27",
        "zh": "9Êúà27Êó•"
    },
    "categories": {
        "#dataset": 5,
        "#data": 2,
        "#benchmark": 6,
        "#agents": 2,
        "#cv": 6,
        "#rl": 1,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 2,
        "#inference": 1,
        "#3d": 4,
        "#audio": 1,
        "#video": 3,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 2,
        "#architecture": 7,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 1,
        "#agi": 1,
        "#games": 3,
        "#interpretability": 0,
        "#reasoning": 1,
        "#transfer_learning": 0,
        "#graphs": 2,
        "#ethics": 0,
        "#security": 0,
        "#optimization": 6,
        "#survey": 0,
        "#diffusion": 2,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 1,
        "#long_context": 1,
        "#synthetic": 4,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 5,
        "#small_models": 1,
        "#science": 2,
        "#low_resource": 0
    }
}