{
    "date": {
        "ru": "11 сентября",
        "en": "September 11",
        "zh": "9月11日"
    },
    "time_utc": "2024-09-11 09:00",
    "weekday": 2,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-11",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.06666",
            "title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models",
            "url": "https://huggingface.co/papers/2409.06666",
            "abstract": "Models like GPT-4o enable real-time interaction with large language models (LLMs) through speech, significantly enhancing user experience compared to traditional text-based interaction. However, there is still a lack of exploration on how to build speech interaction models based on open-source LLMs. To address this, we propose LLaMA-Omni, a novel model architecture designed for low-latency and high-quality speech interaction with LLMs. LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder. It eliminates the need for speech transcription, and can simultaneously generate text and speech responses directly from speech instructions with extremely low latency. We build our model based on the latest Llama-3.1-8B-Instruct model. To align the model with speech interaction scenarios, we construct a dataset named InstructS2S-200K, which includes 200K speech instructions and corresponding speech responses. Experimental results show that compared to previous speech-language models, LLaMA-Omni provides better responses in both content and style, with a response latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3 days on just 4 GPUs, paving the way for the efficient development of speech-language models in the future.",
            "score": 55,
            "issue_id": 1,
            "pub_date": "2024-09-10",
            "pub_date_card": {
                "ru": "10 сентября",
                "en": "September 10",
                "zh": "9月10日"
            },
            "hash": "4631591a898a5ac2",
            "data": {
                "categories": [
                    "#architecture",
                    "#audio",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "🗣️",
                "ru": {
                    "title": "LLaMA-Omni: Революция в речевом взаимодействии с ИИ",
                    "desc": "LLaMA-Omni - это новая архитектура модели для низколатентного и качественного речевого взаимодействия с большими языковыми моделями (LLM). Модель интегрирует предобученный речевой энкодер, речевой адаптер, LLM и потоковый речевой декодер, устраняя необходимость в транскрипции речи. LLaMA-Omni обучена на специально созданном наборе данных InstructS2S-200K, содержащем 200 тысяч речевых инструкций и ответов. Экспериментальные результаты показывают, что модель обеспечивает лучшие ответы по содержанию и стилю с задержкой всего 226 мс."
                },
                "en": {
                    "title": "LLaMA-Omni: Revolutionizing Speech Interaction with LLMs",
                    "desc": "The paper introduces LLaMA-Omni, a new model architecture that enhances real-time speech interaction with large language models (LLMs). It combines a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder to generate text and speech responses directly from speech instructions without needing transcription. The model is built on the Llama-3.1-8B-Instruct and is trained on a dataset called InstructS2S-200K, which contains 200,000 speech instructions and responses. Experimental results demonstrate that LLaMA-Omni outperforms previous models in response quality and speed, achieving a low latency of 226ms and efficient training on limited hardware."
                },
                "zh": {
                    "title": "实时语音交互的新突破：LLaMA-Omni",
                    "desc": "本文介绍了一种新模型LLaMA-Omni，旨在通过语音与大型语言模型（LLMs）进行实时交互。该模型集成了预训练的语音编码器、语音适配器、LLM和流式语音解码器，能够直接从语音指令生成文本和语音响应，且延迟极低。我们构建了一个名为InstructS2S-200K的数据集，包含20万个语音指令及其对应的响应，以优化模型在语音交互场景中的表现。实验结果表明，LLaMA-Omni在内容和风格上优于以往的语音语言模型，响应延迟低至226毫秒，训练时间也大大缩短。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.06595",
            "title": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question Answering",
            "url": "https://huggingface.co/papers/2409.06595",
            "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use Large Language Models (LLMs) alongside private and up-to-date knowledge bases. In this work, we address the challenges of using LLM-as-a-Judge when evaluating grounded answers generated by RAG systems. To assess the calibration and discrimination capabilities of judge models, we identify 7 generator failure modes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a meta-evaluation benchmark of 144 unit tests. This benchmark reveals that existing automated RAG evaluation frameworks often overlook important failure modes, even when using GPT-4 as a judge.   To improve on the current design of automated RAG evaluation frameworks, we propose a novel pipeline and find that while closed models perform well on GroUSE, state-of-the-art open-source judges do not generalize to our proposed criteria, despite strong correlation with GPT-4's judgement. Our findings suggest that correlation with GPT-4 is an incomplete proxy for the practical performance of judge models and should be supplemented with evaluations on unit tests for precise failure mode detection.   We further show that finetuning Llama-3 on GPT-4's reasoning traces significantly boosts its evaluation capabilities, improving upon both correlation with GPT-4's evaluations and calibration on reference situations.",
            "score": 37,
            "issue_id": 1,
            "pub_date": "2024-09-10",
            "pub_date_card": {
                "ru": "10 сентября",
                "en": "September 10",
                "zh": "9月10日"
            },
            "hash": "0ccedfeb3699017a",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "⚖️",
                "ru": {
                    "title": "Совершенствование оценки ответов RAG-систем с помощью LLM-судей",
                    "desc": "В этой статье рассматриваются проблемы использования больших языковых моделей (LLM) в качестве судей при оценке ответов, сгенерированных системами RAG (Retrieval-Augmented Generation). Авторы представляют GroUSE - набор тестов для мета-оценки судейских моделей, выявляющий их недостатки в обнаружении важных ошибок. Исследование показывает, что корреляция с оценками GPT-4 не является достаточным показателем практической эффективности судейских моделей. Авторы предлагают новый подход, включающий дообучение модели Llama-3 на рассуждениях GPT-4, что значительно улучшает её способности к оценке."
                },
                "en": {
                    "title": "Enhancing RAG Evaluation with GroUSE: Beyond GPT-4 Correlation",
                    "desc": "This paper discusses the use of Retrieval-Augmented Generation (RAG) systems that combine Large Language Models (LLMs) with knowledge bases for generating answers. It highlights the challenges of evaluating these generated answers using LLMs as judges, particularly focusing on the calibration and discrimination abilities of these judge models. The authors introduce GroUSE, a benchmark designed to identify generator failure modes and improve the evaluation process of RAG systems. Their findings indicate that while closed models perform well, open-source judges struggle to meet the proposed evaluation criteria, suggesting that relying solely on correlation with GPT-4 is insufficient for assessing judge model performance."
                },
                "zh": {
                    "title": "提升RAG系统评估的准确性与可靠性",
                    "desc": "本研究探讨了在评估基于检索增强生成（RAG）系统生成的答案时，使用大型语言模型（LLM）作为评判者所面临的挑战。我们识别了七种生成器失败模式，并引入了GroUSE（基于基础问答的评估单元评分），这是一个包含144个单元测试的元评估基准。研究表明，现有的自动化RAG评估框架常常忽视重要的失败模式，即使使用GPT-4作为评判者也不例外。我们提出了一种新的自动化RAG评估框架，并发现尽管封闭模型在GroUSE上表现良好，但最先进的开源评判者未能适应我们的标准，尽管与GPT-4的判断有很强的相关性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.06210",
            "title": "INTRA: Interaction Relationship-aware Weakly Supervised Affordance Grounding",
            "url": "https://huggingface.co/papers/2409.06210",
            "abstract": "Affordance denotes the potential interactions inherent in objects. The perception of affordance can enable intelligent agents to navigate and interact with new environments efficiently. Weakly supervised affordance grounding teaches agents the concept of affordance without costly pixel-level annotations, but with exocentric images. Although recent advances in weakly supervised affordance grounding yielded promising results, there remain challenges including the requirement for paired exocentric and egocentric image dataset, and the complexity in grounding diverse affordances for a single object. To address them, we propose INTeraction Relationship-aware weakly supervised Affordance grounding (INTRA). Unlike prior arts, INTRA recasts this problem as representation learning to identify unique features of interactions through contrastive learning with exocentric images only, eliminating the need for paired datasets. Moreover, we leverage vision-language model embeddings for performing affordance grounding flexibly with any text, designing text-conditioned affordance map generation to reflect interaction relationship for contrastive learning and enhancing robustness with our text synonym augmentation. Our method outperformed prior arts on diverse datasets such as AGD20K, IIT-AFF, CAD and UMD. Additionally, experimental results demonstrate that our method has remarkable domain scalability for synthesized images / illustrations and is capable of performing affordance grounding for novel interactions and objects.",
            "score": 24,
            "issue_id": 1,
            "pub_date": "2024-09-10",
            "pub_date_card": {
                "ru": "10 сентября",
                "en": "September 10",
                "zh": "9月10日"
            },
            "hash": "d163ba5e839ca44b",
            "data": {
                "categories": [
                    "#agents",
                    "#cv",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Гибкое обучение восприятию аффордансов без парных данных",
                    "desc": "Статья представляет новый метод INTRA для слабоконтролируемого обучения восприятию аффордансов объектов. В отличие от предыдущих подходов, INTRA использует только экзоцентрические изображения и контрастивное обучение для выделения уникальных признаков взаимодействий. Метод применяет мультимодальные языковые модели для гибкой генерации карт аффордансов на основе текстовых описаний. INTRA превзошел существующие методы на различных наборах данных и продемонстрировал способность к обобщению на новые объекты и взаимодействия."
                },
                "en": {
                    "title": "Revolutionizing Affordance Grounding with INTRA: No Paired Images Needed!",
                    "desc": "This paper introduces a new method called INTRA for weakly supervised affordance grounding, which helps intelligent agents understand how to interact with objects in their environment. INTRA uses contrastive learning to identify unique interaction features from exocentric images, eliminating the need for paired egocentric images. The approach also incorporates vision-language model embeddings to create flexible affordance maps based on text descriptions, enhancing the model's ability to generalize across different contexts. The results show that INTRA outperforms previous methods on various datasets and can adapt to new interactions and objects effectively."
                },
                "zh": {
                    "title": "智能代理的可供性基础新方法",
                    "desc": "本论文提出了一种新的弱监督可供性基础方法，称为INTRA，旨在通过对比学习从外部图像中识别交互特征，而无需配对的图像数据集。INTRA利用视觉-语言模型嵌入，灵活地进行可供性基础，并设计了文本条件的可供性图生成，以反映交互关系。该方法在多个数据集上表现优异，超越了之前的研究成果，并展示了在合成图像和新对象上的显著领域可扩展性。通过这种方式，智能代理能够更有效地理解和与新环境进行交互。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.06029",
            "title": "SongCreator: Lyrics-based Universal Song Generation",
            "url": "https://huggingface.co/papers/2409.06029",
            "abstract": "Music is an integral part of human culture, embodying human intelligence and creativity, of which songs compose an essential part. While various aspects of song generation have been explored by previous works, such as singing voice, vocal composition and instrumental arrangement, etc., generating songs with both vocals and accompaniment given lyrics remains a significant challenge, hindering the application of music generation models in the real world. In this light, we propose SongCreator, a song-generation system designed to tackle this challenge. The model features two novel designs: a meticulously designed dual-sequence language model (DSLM) to capture the information of vocals and accompaniment for song generation, and an additional attention mask strategy for DSLM, which allows our model to understand, generate and edit songs, making it suitable for various song-related generation tasks. Extensive experiments demonstrate the effectiveness of SongCreator by achieving state-of-the-art or competitive performances on all eight tasks. Notably, it surpasses previous works by a large margin in lyrics-to-song and lyrics-to-vocals. Additionally, it is able to independently control the acoustic conditions of the vocals and accompaniment in the generated song through different prompts, exhibiting its potential applicability. Our samples are available at https://songcreator.github.io/.",
            "score": 20,
            "issue_id": 1,
            "pub_date": "2024-09-09",
            "pub_date_card": {
                "ru": "9 сентября",
                "en": "September 9",
                "zh": "9月9日"
            },
            "hash": "a99ff7eb8a35685f",
            "data": {
                "categories": [
                    "#audio",
                    "#music",
                    "#generative"
                ],
                "emoji": "🎵",
                "ru": {
                    "title": "SongCreator: ИИ-композитор для создания полноценных песен",
                    "desc": "SongCreator - это система генерации песен, использующая двухпоследовательную языковую модель (DSLM) для создания вокала и аккомпанемента. Модель применяет дополнительную стратегию маскирования внимания, что позволяет ей понимать, генерировать и редактировать песни. SongCreator демонстрирует высокую эффективность в различных задачах, связанных с генерацией песен, особенно в преобразовании текста в песню и вокал. Система также способна независимо контролировать акустические условия вокала и аккомпанемента в сгенерированной песне."
                },
                "en": {
                    "title": "Revolutionizing Song Generation with SongCreator!",
                    "desc": "The paper introduces SongCreator, a novel system for generating songs that includes both vocals and instrumental accompaniment based on given lyrics. It utilizes a dual-sequence language model (DSLM) that effectively captures the relationships between vocals and accompaniment, enhancing the song generation process. Additionally, an attention mask strategy is implemented to improve the model's ability to understand, generate, and edit songs, making it versatile for various music generation tasks. The results show that SongCreator outperforms existing models, particularly in converting lyrics to complete songs and controlling the acoustic properties of the generated music."
                },
                "zh": {
                    "title": "SongCreator：创新的歌曲生成系统",
                    "desc": "本论文提出了一种名为SongCreator的歌曲生成系统，旨在解决歌词生成伴奏和人声的挑战。该系统采用了双序列语言模型（DSLM），能够有效捕捉人声和伴奏的信息，并通过注意力掩码策略增强模型的理解和生成能力。通过大量实验，SongCreator在八个任务上表现出色，尤其在歌词转歌曲和歌词转人声的任务中显著超越了之前的研究。该模型还可以通过不同的提示独立控制生成歌曲的人声和伴奏的音响条件，展示了其广泛的应用潜力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.06135",
            "title": "Draw an Audio: Leveraging Multi-Instruction for Video-to-Audio Synthesis",
            "url": "https://huggingface.co/papers/2409.06135",
            "abstract": "Foley is a term commonly used in filmmaking, referring to the addition of daily sound effects to silent films or videos to enhance the auditory experience. Video-to-Audio (V2A), as a particular type of automatic foley task, presents inherent challenges related to audio-visual synchronization. These challenges encompass maintaining the content consistency between the input video and the generated audio, as well as the alignment of temporal and loudness properties within the video. To address these issues, we construct a controllable video-to-audio synthesis model, termed Draw an Audio, which supports multiple input instructions through drawn masks and loudness signals. To ensure content consistency between the synthesized audio and target video, we introduce the Mask-Attention Module (MAM), which employs masked video instruction to enable the model to focus on regions of interest. Additionally, we implement the Time-Loudness Module (TLM), which uses an auxiliary loudness signal to ensure the synthesis of sound that aligns with the video in both loudness and temporal dimensions. Furthermore, we have extended a large-scale V2A dataset, named VGGSound-Caption, by annotating caption prompts. Extensive experiments on challenging benchmarks across two large-scale V2A datasets verify Draw an Audio achieves the state-of-the-art. Project page: https://yannqi.github.io/Draw-an-Audio/.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2024-09-10",
            "pub_date_card": {
                "ru": "10 сентября",
                "en": "September 10",
                "zh": "9月10日"
            },
            "hash": "7a9dd7cd522ce7ab",
            "data": {
                "categories": [
                    "#audio",
                    "#video",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "Рисуем звук: управляемый синтез аудио из видео",
                    "desc": "Статья представляет модель Draw an Audio для синтеза аудио из видео (V2A) с поддержкой множественных инструкций через нарисованные маски и сигналы громкости. Авторы вводят модуль Mask-Attention (MAM) для обеспечения согласованности содержания между синтезированным аудио и целевым видео. Также применяется модуль Time-Loudness (TLM) для выравнивания синтезированного звука с видео по громкости и временным параметрам. Модель достигает наилучших результатов на сложных тестах по двум крупномасштабным наборам данных V2A."
                },
                "en": {
                    "title": "Transforming Silence into Sound: Draw an Audio",
                    "desc": "This paper presents a novel approach to the Video-to-Audio (V2A) synthesis task, which involves generating sound effects that match silent videos. The proposed model, Draw an Audio, utilizes a Mask-Attention Module (MAM) to ensure that the generated audio is consistent with the visual content by focusing on specific areas of the video. Additionally, the Time-Loudness Module (TLM) is introduced to align the audio's loudness and timing with the video, enhancing the overall auditory experience. The authors also expand a large-scale dataset, VGGSound-Caption, to support their experiments, demonstrating that their model achieves state-of-the-art performance on various benchmarks."
                },
                "zh": {
                    "title": "视频到音频合成的新突破",
                    "desc": "Foley是电影制作中常用的术语，指的是为无声电影或视频添加日常音效，以增强听觉体验。视频到音频（V2A）是一种自动化的Foley任务，面临音视频同步的挑战，包括保持输入视频与生成音频之间的内容一致性，以及视频中的时间和响度属性的对齐。为了解决这些问题，我们构建了一个可控的视频到音频合成模型，称为Draw an Audio，支持通过绘制的掩码和响度信号进行多种输入指令。我们引入了掩码注意力模块（MAM）和时间响度模块（TLM），确保合成音频与目标视频在内容和响度上保持一致，实验结果表明该模型在多个大型V2A数据集上达到了最先进的水平。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.06633",
            "title": "SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation",
            "url": "https://huggingface.co/papers/2409.06633",
            "abstract": "In recent years, the development of diffusion models has led to significant progress in image and video generation tasks, with pre-trained models like the Stable Diffusion series playing a crucial role. Inspired by model pruning which lightens large pre-trained models by removing unimportant parameters, we propose a novel model fine-tuning method to make full use of these ineffective parameters and enable the pre-trained model with new task-specified capabilities. In this work, we first investigate the importance of parameters in pre-trained diffusion models, and discover that the smallest 10% to 20% of parameters by absolute values do not contribute to the generation process. Based on this observation, we propose a method termed SaRA that re-utilizes these temporarily ineffective parameters, equating to optimizing a sparse weight matrix to learn the task-specific knowledge. To mitigate overfitting, we propose a nuclear-norm-based low-rank sparse training scheme for efficient fine-tuning. Furthermore, we design a new progressive parameter adjustment strategy to make full use of the re-trained/finetuned parameters. Finally, we propose a novel unstructural backpropagation strategy, which significantly reduces memory costs during fine-tuning. Our method enhances the generative capabilities of pre-trained models in downstream applications and outperforms traditional fine-tuning methods like LoRA in maintaining model's generalization ability. We validate our approach through fine-tuning experiments on SD models, demonstrating significant improvements. SaRA also offers a practical advantage that requires only a single line of code modification for efficient implementation and is seamlessly compatible with existing methods.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2024-09-10",
            "pub_date_card": {
                "ru": "10 сентября",
                "en": "September 10",
                "zh": "9月10日"
            },
            "hash": "98510c6f45c81c15",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#optimization"
                ],
                "emoji": "🔬",
                "ru": {
                    "title": "Эффективное дообучение диффузионных моделей путем переиспользования неэффективных параметров",
                    "desc": "Авторы предлагают новый метод дообучения предобученных диффузионных моделей, называемый SaRA. Метод основан на переиспользовании неэффективных параметров модели для обучения специфичным для задачи знаниям. SaRA использует разреженное обучение с низким рангом и прогрессивную стратегию настройки параметров для улучшения генеративных возможностей. Эксперименты показывают, что SaRA превосходит традиционные методы дообучения, такие как LoRA, в сохранении способности модели к обобщению."
                },
                "en": {
                    "title": "Unlocking Hidden Potential in Diffusion Models with SaRA",
                    "desc": "This paper introduces a new fine-tuning method called SaRA for pre-trained diffusion models, which enhances their performance in image and video generation tasks. The authors identify that a significant portion of parameters in these models, specifically the smallest 10% to 20%, are ineffective during generation and propose to re-utilize them for task-specific learning. To prevent overfitting, they implement a low-rank sparse training scheme and a progressive parameter adjustment strategy. The results show that SaRA not only improves generative capabilities but also reduces memory costs during fine-tuning, making it a practical and efficient solution compared to traditional methods."
                },
                "zh": {
                    "title": "充分利用无效参数，提升生成能力",
                    "desc": "近年来，扩散模型在图像和视频生成任务中取得了显著进展，预训练模型如稳定扩散系列发挥了重要作用。我们提出了一种新颖的模型微调方法，充分利用无效参数，使预训练模型具备新的任务特定能力。通过研究预训练扩散模型中参数的重要性，我们发现绝对值最小的10%到20%的参数对生成过程没有贡献。我们的方法SaRA通过优化稀疏权重矩阵来重新利用这些无效参数，从而提高了预训练模型在下游应用中的生成能力。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.06703",
            "title": "LEIA: Latent View-invariant Embeddings for Implicit 3D Articulation",
            "url": "https://huggingface.co/papers/2409.06703",
            "abstract": "Neural Radiance Fields (NeRFs) have revolutionized the reconstruction of static scenes and objects in 3D, offering unprecedented quality. However, extending NeRFs to model dynamic objects or object articulations remains a challenging problem. Previous works have tackled this issue by focusing on part-level reconstruction and motion estimation for objects, but they often rely on heuristics regarding the number of moving parts or object categories, which can limit their practical use. In this work, we introduce LEIA, a novel approach for representing dynamic 3D objects. Our method involves observing the object at distinct time steps or \"states\" and conditioning a hypernetwork on the current state, using this to parameterize our NeRF. This approach allows us to learn a view-invariant latent representation for each state. We further demonstrate that by interpolating between these states, we can generate novel articulation configurations in 3D space that were previously unseen. Our experimental results highlight the effectiveness of our method in articulating objects in a manner that is independent of the viewing angle and joint configuration. Notably, our approach outperforms previous methods that rely on motion information for articulation registration.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2024-09-10",
            "pub_date_card": {
                "ru": "10 сентября",
                "en": "September 10",
                "zh": "9月10日"
            },
            "hash": "a65e33101dcdfaf4",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "LEIA: Динамичные 3D объекты без ограничений",
                    "desc": "Статья представляет LEIA - новый подход к представлению динамичных 3D объектов с использованием нейронных радиационных полей (NeRF). Метод наблюдает объект в разных состояниях и использует гиперсеть для параметризации NeRF, обучая инвариантное к ракурсу латентное представление каждого состояния. LEIA позволяет интерполировать между состояниями, генерируя новые конфигурации артикуляции в 3D пространстве. Экспериментальные результаты показывают эффективность метода в артикуляции объектов независимо от угла обзора и конфигурации суставов."
                },
                "en": {
                    "title": "LEIA: Revolutionizing Dynamic 3D Object Representation with NeRFs",
                    "desc": "This paper presents LEIA, a new method for modeling dynamic 3D objects using Neural Radiance Fields (NeRFs). Unlike previous approaches that depend on heuristics for motion estimation, LEIA conditions a hypernetwork on distinct time states of the object to create a more flexible representation. By learning a view-invariant latent representation, the method can interpolate between states to generate new articulations that were not previously captured. Experimental results show that LEIA outperforms existing techniques in articulating objects regardless of viewing angles or joint configurations."
                },
                "zh": {
                    "title": "LEIA：动态三维物体的新视角",
                    "desc": "神经辐射场（NeRF）在静态场景和物体的三维重建中取得了革命性的进展，但将其扩展到动态物体或物体关节的建模仍然是一个挑战。以往的研究主要集中在部分重建和运动估计上，通常依赖于关于移动部件数量或物体类别的启发式方法，这限制了它们的实际应用。我们提出了一种新方法LEIA，通过在不同时间步观察物体，并根据当前状态对超网络进行条件化，从而参数化我们的NeRF。我们的实验结果表明，该方法在物体关节的表现上超越了依赖运动信息的先前方法。"
                }
            }
        }
    ],
    "link_prev": "2024-09-10.html",
    "link_next": "2024-09-12.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "10.09",
        "en": "09/10",
        "zh": "9月10日"
    },
    "short_date_next": {
        "ru": "12.09",
        "en": "09/12",
        "zh": "9月12日"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 3,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#medicine": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 0,
        "#music": 0,
        "#generative": 0
    }
}