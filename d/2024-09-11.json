{
    "date": {
        "ru": "11 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        "en": "September 11",
        "zh": "9æœˆ11æ—¥"
    },
    "time_utc": "2024-09-11 09:00",
    "weekday": 2,
    "issue_id": 1,
    "home_page_url": "https://huggingface.co/papers?date=2024-09-11",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2409.06666",
            "title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models",
            "url": "https://huggingface.co/papers/2409.06666",
            "abstract": "Models like GPT-4o enable real-time interaction with large language models (LLMs) through speech, significantly enhancing user experience compared to traditional text-based interaction. However, there is still a lack of exploration on how to build speech interaction models based on open-source LLMs. To address this, we propose LLaMA-Omni, a novel model architecture designed for low-latency and high-quality speech interaction with LLMs. LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder. It eliminates the need for speech transcription, and can simultaneously generate text and speech responses directly from speech instructions with extremely low latency. We build our model based on the latest Llama-3.1-8B-Instruct model. To align the model with speech interaction scenarios, we construct a dataset named InstructS2S-200K, which includes 200K speech instructions and corresponding speech responses. Experimental results show that compared to previous speech-language models, LLaMA-Omni provides better responses in both content and style, with a response latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3 days on just 4 GPUs, paving the way for the efficient development of speech-language models in the future.",
            "score": 55,
            "issue_id": 1,
            "pub_date": "2024-09-10",
            "pub_date_card": {
                "ru": "10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 10",
                "zh": "9æœˆ10æ—¥"
            },
            "hash": "4631591a898a5ac2",
            "data": {
                "categories": [
                    "#architecture",
                    "#audio",
                    "#dataset",
                    "#multimodal"
                ],
                "emoji": "ğŸ—£ï¸",
                "ru": {
                    "title": "LLaMA-Omni: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ˜Ğ˜",
                    "desc": "LLaMA-Omni - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€, Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€, LLM Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€, ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸. LLaMA-Omni Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… InstructS2S-200K, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¼ 200 Ñ‚Ñ‹ÑÑÑ‡ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¿Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ²ÑĞµĞ³Ğ¾ 226 Ğ¼Ñ."
                },
                "en": {
                    "title": "LLaMA-Omni: Revolutionizing Speech Interaction with LLMs",
                    "desc": "The paper introduces LLaMA-Omni, a new model architecture that enhances real-time speech interaction with large language models (LLMs). It combines a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder to generate text and speech responses directly from speech instructions without needing transcription. The model is built on the Llama-3.1-8B-Instruct and is trained on a dataset called InstructS2S-200K, which contains 200,000 speech instructions and responses. Experimental results demonstrate that LLaMA-Omni outperforms previous models in response quality and speed, achieving a low latency of 226ms and efficient training on limited hardware."
                },
                "zh": {
                    "title": "å®æ—¶è¯­éŸ³äº¤äº’çš„æ–°çªç ´ï¼šLLaMA-Omni",
                    "desc": "æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°æ¨¡å‹LLaMA-Omniï¼Œæ—¨åœ¨é€šè¿‡è¯­éŸ³ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå®æ—¶äº¤äº’ã€‚è¯¥æ¨¡å‹é›†æˆäº†é¢„è®­ç»ƒçš„è¯­éŸ³ç¼–ç å™¨ã€è¯­éŸ³é€‚é…å™¨ã€LLMå’Œæµå¼è¯­éŸ³è§£ç å™¨ï¼Œèƒ½å¤Ÿç›´æ¥ä»è¯­éŸ³æŒ‡ä»¤ç”Ÿæˆæ–‡æœ¬å’Œè¯­éŸ³å“åº”ï¼Œä¸”å»¶è¿Ÿæä½ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºInstructS2S-200Kçš„æ•°æ®é›†ï¼ŒåŒ…å«20ä¸‡ä¸ªè¯­éŸ³æŒ‡ä»¤åŠå…¶å¯¹åº”çš„å“åº”ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹åœ¨è¯­éŸ³äº¤äº’åœºæ™¯ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLaMA-Omniåœ¨å†…å®¹å’Œé£æ ¼ä¸Šä¼˜äºä»¥å¾€çš„è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼Œå“åº”å»¶è¿Ÿä½è‡³226æ¯«ç§’ï¼Œè®­ç»ƒæ—¶é—´ä¹Ÿå¤§å¤§ç¼©çŸ­ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.06595",
            "title": "GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question Answering",
            "url": "https://huggingface.co/papers/2409.06595",
            "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use Large Language Models (LLMs) alongside private and up-to-date knowledge bases. In this work, we address the challenges of using LLM-as-a-Judge when evaluating grounded answers generated by RAG systems. To assess the calibration and discrimination capabilities of judge models, we identify 7 generator failure modes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a meta-evaluation benchmark of 144 unit tests. This benchmark reveals that existing automated RAG evaluation frameworks often overlook important failure modes, even when using GPT-4 as a judge.   To improve on the current design of automated RAG evaluation frameworks, we propose a novel pipeline and find that while closed models perform well on GroUSE, state-of-the-art open-source judges do not generalize to our proposed criteria, despite strong correlation with GPT-4's judgement. Our findings suggest that correlation with GPT-4 is an incomplete proxy for the practical performance of judge models and should be supplemented with evaluations on unit tests for precise failure mode detection.   We further show that finetuning Llama-3 on GPT-4's reasoning traces significantly boosts its evaluation capabilities, improving upon both correlation with GPT-4's evaluations and calibration on reference situations.",
            "score": 37,
            "issue_id": 1,
            "pub_date": "2024-09-10",
            "pub_date_card": {
                "ru": "10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 10",
                "zh": "9æœˆ10æ—¥"
            },
            "hash": "0ccedfeb3699017a",
            "data": {
                "categories": [
                    "#rag",
                    "#benchmark",
                    "#training"
                ],
                "emoji": "âš–ï¸",
                "ru": {
                    "title": "Ğ¡Ğ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² RAG-ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM-ÑÑƒĞ´ĞµĞ¹",
                    "desc": "Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ RAG (Retrieval-Augmented Generation). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ GroUSE - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼ĞµÑ‚Ğ°-Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑƒĞ´ĞµĞ¹ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¸Ñ… Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ GPT-4 Ğ½Ğµ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒĞ´ĞµĞ¹ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama-3 Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… GPT-4, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞµÑ‘ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ."
                },
                "en": {
                    "title": "Enhancing RAG Evaluation with GroUSE: Beyond GPT-4 Correlation",
                    "desc": "This paper discusses the use of Retrieval-Augmented Generation (RAG) systems that combine Large Language Models (LLMs) with knowledge bases for generating answers. It highlights the challenges of evaluating these generated answers using LLMs as judges, particularly focusing on the calibration and discrimination abilities of these judge models. The authors introduce GroUSE, a benchmark designed to identify generator failure modes and improve the evaluation process of RAG systems. Their findings indicate that while closed models perform well, open-source judges struggle to meet the proposed evaluation criteria, suggesting that relying solely on correlation with GPT-4 is insufficient for assessing judge model performance."
                },
                "zh": {
                    "title": "æå‡RAGç³»ç»Ÿè¯„ä¼°çš„å‡†ç¡®æ€§ä¸å¯é æ€§",
                    "desc": "æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨è¯„ä¼°åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿç”Ÿæˆçš„ç­”æ¡ˆæ—¶ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„åˆ¤è€…æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¯†åˆ«äº†ä¸ƒç§ç”Ÿæˆå™¨å¤±è´¥æ¨¡å¼ï¼Œå¹¶å¼•å…¥äº†GroUSEï¼ˆåŸºäºåŸºç¡€é—®ç­”çš„è¯„ä¼°å•å…ƒè¯„åˆ†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«144ä¸ªå•å…ƒæµ‹è¯•çš„å…ƒè¯„ä¼°åŸºå‡†ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„è‡ªåŠ¨åŒ–RAGè¯„ä¼°æ¡†æ¶å¸¸å¸¸å¿½è§†é‡è¦çš„å¤±è´¥æ¨¡å¼ï¼Œå³ä½¿ä½¿ç”¨GPT-4ä½œä¸ºè¯„åˆ¤è€…ä¹Ÿä¸ä¾‹å¤–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è‡ªåŠ¨åŒ–RAGè¯„ä¼°æ¡†æ¶ï¼Œå¹¶å‘ç°å°½ç®¡å°é—­æ¨¡å‹åœ¨GroUSEä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†æœ€å…ˆè¿›çš„å¼€æºè¯„åˆ¤è€…æœªèƒ½é€‚åº”æˆ‘ä»¬çš„æ ‡å‡†ï¼Œå°½ç®¡ä¸GPT-4çš„åˆ¤æ–­æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.06210",
            "title": "INTRA: Interaction Relationship-aware Weakly Supervised Affordance Grounding",
            "url": "https://huggingface.co/papers/2409.06210",
            "abstract": "Affordance denotes the potential interactions inherent in objects. The perception of affordance can enable intelligent agents to navigate and interact with new environments efficiently. Weakly supervised affordance grounding teaches agents the concept of affordance without costly pixel-level annotations, but with exocentric images. Although recent advances in weakly supervised affordance grounding yielded promising results, there remain challenges including the requirement for paired exocentric and egocentric image dataset, and the complexity in grounding diverse affordances for a single object. To address them, we propose INTeraction Relationship-aware weakly supervised Affordance grounding (INTRA). Unlike prior arts, INTRA recasts this problem as representation learning to identify unique features of interactions through contrastive learning with exocentric images only, eliminating the need for paired datasets. Moreover, we leverage vision-language model embeddings for performing affordance grounding flexibly with any text, designing text-conditioned affordance map generation to reflect interaction relationship for contrastive learning and enhancing robustness with our text synonym augmentation. Our method outperformed prior arts on diverse datasets such as AGD20K, IIT-AFF, CAD and UMD. Additionally, experimental results demonstrate that our method has remarkable domain scalability for synthesized images / illustrations and is capable of performing affordance grounding for novel interactions and objects.",
            "score": 24,
            "issue_id": 1,
            "pub_date": "2024-09-10",
            "pub_date_card": {
                "ru": "10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 10",
                "zh": "9æœˆ10æ—¥"
            },
            "hash": "d163ba5e839ca44b",
            "data": {
                "categories": [
                    "#agents",
                    "#cv",
                    "#training",
                    "#multimodal"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "Ğ“Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ°Ñ„Ñ„Ğ¾Ñ€Ğ´Ğ°Ğ½ÑĞ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ INTRA Ğ´Ğ»Ñ ÑĞ»Ğ°Ğ±Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ°Ñ„Ñ„Ğ¾Ñ€Ğ´Ğ°Ğ½ÑĞ¾Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², INTRA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞºĞ·Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ€Ñ‚ Ğ°Ñ„Ñ„Ğ¾Ñ€Ğ´Ğ°Ğ½ÑĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. INTRA Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ."
                },
                "en": {
                    "title": "Revolutionizing Affordance Grounding with INTRA: No Paired Images Needed!",
                    "desc": "This paper introduces a new method called INTRA for weakly supervised affordance grounding, which helps intelligent agents understand how to interact with objects in their environment. INTRA uses contrastive learning to identify unique interaction features from exocentric images, eliminating the need for paired egocentric images. The approach also incorporates vision-language model embeddings to create flexible affordance maps based on text descriptions, enhancing the model's ability to generalize across different contexts. The results show that INTRA outperforms previous methods on various datasets and can adapt to new interactions and objects effectively."
                },
                "zh": {
                    "title": "æ™ºèƒ½ä»£ç†çš„å¯ä¾›æ€§åŸºç¡€æ–°æ–¹æ³•",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼±ç›‘ç£å¯ä¾›æ€§åŸºç¡€æ–¹æ³•ï¼Œç§°ä¸ºINTRAï¼Œæ—¨åœ¨é€šè¿‡å¯¹æ¯”å­¦ä¹ ä»å¤–éƒ¨å›¾åƒä¸­è¯†åˆ«äº¤äº’ç‰¹å¾ï¼Œè€Œæ— éœ€é…å¯¹çš„å›¾åƒæ•°æ®é›†ã€‚INTRAåˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹åµŒå…¥ï¼Œçµæ´»åœ°è¿›è¡Œå¯ä¾›æ€§åŸºç¡€ï¼Œå¹¶è®¾è®¡äº†æ–‡æœ¬æ¡ä»¶çš„å¯ä¾›æ€§å›¾ç”Ÿæˆï¼Œä»¥åæ˜ äº¤äº’å…³ç³»ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„ç ”ç©¶æˆæœï¼Œå¹¶å±•ç¤ºäº†åœ¨åˆæˆå›¾åƒå’Œæ–°å¯¹è±¡ä¸Šçš„æ˜¾è‘—é¢†åŸŸå¯æ‰©å±•æ€§ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ™ºèƒ½ä»£ç†èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ç†è§£å’Œä¸æ–°ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.06029",
            "title": "SongCreator: Lyrics-based Universal Song Generation",
            "url": "https://huggingface.co/papers/2409.06029",
            "abstract": "Music is an integral part of human culture, embodying human intelligence and creativity, of which songs compose an essential part. While various aspects of song generation have been explored by previous works, such as singing voice, vocal composition and instrumental arrangement, etc., generating songs with both vocals and accompaniment given lyrics remains a significant challenge, hindering the application of music generation models in the real world. In this light, we propose SongCreator, a song-generation system designed to tackle this challenge. The model features two novel designs: a meticulously designed dual-sequence language model (DSLM) to capture the information of vocals and accompaniment for song generation, and an additional attention mask strategy for DSLM, which allows our model to understand, generate and edit songs, making it suitable for various song-related generation tasks. Extensive experiments demonstrate the effectiveness of SongCreator by achieving state-of-the-art or competitive performances on all eight tasks. Notably, it surpasses previous works by a large margin in lyrics-to-song and lyrics-to-vocals. Additionally, it is able to independently control the acoustic conditions of the vocals and accompaniment in the generated song through different prompts, exhibiting its potential applicability. Our samples are available at https://songcreator.github.io/.",
            "score": 20,
            "issue_id": 1,
            "pub_date": "2024-09-09",
            "pub_date_card": {
                "ru": "9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 9",
                "zh": "9æœˆ9æ—¥"
            },
            "hash": "a99ff7eb8a35685f",
            "data": {
                "categories": [
                    "#audio",
                    "#music",
                    "#generative"
                ],
                "emoji": "ğŸµ",
                "ru": {
                    "title": "SongCreator: Ğ˜Ğ˜-ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑĞµĞ½",
                    "desc": "SongCreator - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑĞµĞ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (DSLM) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ĞºĞ°Ğ»Ğ° Ğ¸ Ğ°ĞºĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½ĞµĞ¼ĞµĞ½Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿ĞµÑĞ½Ğ¸. SongCreator Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿ĞµÑĞµĞ½, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¿ĞµÑĞ½Ñ Ğ¸ Ğ²Ğ¾ĞºĞ°Ğ». Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ²Ğ¾ĞºĞ°Ğ»Ğ° Ğ¸ Ğ°ĞºĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½ĞµĞ¼ĞµĞ½Ñ‚Ğ° Ğ² ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑĞ½Ğµ."
                },
                "en": {
                    "title": "Revolutionizing Song Generation with SongCreator!",
                    "desc": "The paper introduces SongCreator, a novel system for generating songs that includes both vocals and instrumental accompaniment based on given lyrics. It utilizes a dual-sequence language model (DSLM) that effectively captures the relationships between vocals and accompaniment, enhancing the song generation process. Additionally, an attention mask strategy is implemented to improve the model's ability to understand, generate, and edit songs, making it versatile for various music generation tasks. The results show that SongCreator outperforms existing models, particularly in converting lyrics to complete songs and controlling the acoustic properties of the generated music."
                },
                "zh": {
                    "title": "SongCreatorï¼šåˆ›æ–°çš„æ­Œæ›²ç”Ÿæˆç³»ç»Ÿ",
                    "desc": "æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSongCreatorçš„æ­Œæ›²ç”Ÿæˆç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³æ­Œè¯ç”Ÿæˆä¼´å¥å’Œäººå£°çš„æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨äº†åŒåºåˆ—è¯­è¨€æ¨¡å‹ï¼ˆDSLMï¼‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰äººå£°å’Œä¼´å¥çš„ä¿¡æ¯ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›æ©ç ç­–ç•¥å¢å¼ºæ¨¡å‹çš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒSongCreatoråœ¨å…«ä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨æ­Œè¯è½¬æ­Œæ›²å’Œæ­Œè¯è½¬äººå£°çš„ä»»åŠ¡ä¸­æ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„ç ”ç©¶ã€‚è¯¥æ¨¡å‹è¿˜å¯ä»¥é€šè¿‡ä¸åŒçš„æç¤ºç‹¬ç«‹æ§åˆ¶ç”Ÿæˆæ­Œæ›²çš„äººå£°å’Œä¼´å¥çš„éŸ³å“æ¡ä»¶ï¼Œå±•ç¤ºäº†å…¶å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.06135",
            "title": "Draw an Audio: Leveraging Multi-Instruction for Video-to-Audio Synthesis",
            "url": "https://huggingface.co/papers/2409.06135",
            "abstract": "Foley is a term commonly used in filmmaking, referring to the addition of daily sound effects to silent films or videos to enhance the auditory experience. Video-to-Audio (V2A), as a particular type of automatic foley task, presents inherent challenges related to audio-visual synchronization. These challenges encompass maintaining the content consistency between the input video and the generated audio, as well as the alignment of temporal and loudness properties within the video. To address these issues, we construct a controllable video-to-audio synthesis model, termed Draw an Audio, which supports multiple input instructions through drawn masks and loudness signals. To ensure content consistency between the synthesized audio and target video, we introduce the Mask-Attention Module (MAM), which employs masked video instruction to enable the model to focus on regions of interest. Additionally, we implement the Time-Loudness Module (TLM), which uses an auxiliary loudness signal to ensure the synthesis of sound that aligns with the video in both loudness and temporal dimensions. Furthermore, we have extended a large-scale V2A dataset, named VGGSound-Caption, by annotating caption prompts. Extensive experiments on challenging benchmarks across two large-scale V2A datasets verify Draw an Audio achieves the state-of-the-art. Project page: https://yannqi.github.io/Draw-an-Audio/.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2024-09-10",
            "pub_date_card": {
                "ru": "10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 10",
                "zh": "9æœˆ10æ—¥"
            },
            "hash": "7a9dd7cd522ce7ab",
            "data": {
                "categories": [
                    "#audio",
                    "#video",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "ğŸ¬",
                "ru": {
                    "title": "Ğ Ğ¸ÑÑƒĞµĞ¼ Ğ·Ğ²ÑƒĞº: ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Draw an Audio Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ (V2A) Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ°ÑĞºĞ¸ Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ³Ñ€Ğ¾Ğ¼ĞºĞ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Mask-Attention (MAM) Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Time-Loudness (TLM) Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ²ÑƒĞºĞ° Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ³Ñ€Ğ¾Ğ¼ĞºĞ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ğ´Ğ²ÑƒĞ¼ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… V2A."
                },
                "en": {
                    "title": "Transforming Silence into Sound: Draw an Audio",
                    "desc": "This paper presents a novel approach to the Video-to-Audio (V2A) synthesis task, which involves generating sound effects that match silent videos. The proposed model, Draw an Audio, utilizes a Mask-Attention Module (MAM) to ensure that the generated audio is consistent with the visual content by focusing on specific areas of the video. Additionally, the Time-Loudness Module (TLM) is introduced to align the audio's loudness and timing with the video, enhancing the overall auditory experience. The authors also expand a large-scale dataset, VGGSound-Caption, to support their experiments, demonstrating that their model achieves state-of-the-art performance on various benchmarks."
                },
                "zh": {
                    "title": "è§†é¢‘åˆ°éŸ³é¢‘åˆæˆçš„æ–°çªç ´",
                    "desc": "Foleyæ˜¯ç”µå½±åˆ¶ä½œä¸­å¸¸ç”¨çš„æœ¯è¯­ï¼ŒæŒ‡çš„æ˜¯ä¸ºæ— å£°ç”µå½±æˆ–è§†é¢‘æ·»åŠ æ—¥å¸¸éŸ³æ•ˆï¼Œä»¥å¢å¼ºå¬è§‰ä½“éªŒã€‚è§†é¢‘åˆ°éŸ³é¢‘ï¼ˆV2Aï¼‰æ˜¯ä¸€ç§è‡ªåŠ¨åŒ–çš„Foleyä»»åŠ¡ï¼Œé¢ä¸´éŸ³è§†é¢‘åŒæ­¥çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¿æŒè¾“å…¥è§†é¢‘ä¸ç”ŸæˆéŸ³é¢‘ä¹‹é—´çš„å†…å®¹ä¸€è‡´æ€§ï¼Œä»¥åŠè§†é¢‘ä¸­çš„æ—¶é—´å’Œå“åº¦å±æ€§çš„å¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¯æ§çš„è§†é¢‘åˆ°éŸ³é¢‘åˆæˆæ¨¡å‹ï¼Œç§°ä¸ºDraw an Audioï¼Œæ”¯æŒé€šè¿‡ç»˜åˆ¶çš„æ©ç å’Œå“åº¦ä¿¡å·è¿›è¡Œå¤šç§è¾“å…¥æŒ‡ä»¤ã€‚æˆ‘ä»¬å¼•å…¥äº†æ©ç æ³¨æ„åŠ›æ¨¡å—ï¼ˆMAMï¼‰å’Œæ—¶é—´å“åº¦æ¨¡å—ï¼ˆTLMï¼‰ï¼Œç¡®ä¿åˆæˆéŸ³é¢‘ä¸ç›®æ ‡è§†é¢‘åœ¨å†…å®¹å’Œå“åº¦ä¸Šä¿æŒä¸€è‡´ï¼Œå®éªŒç»“æœè¡¨æ˜è¯¥æ¨¡å‹åœ¨å¤šä¸ªå¤§å‹V2Aæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.06633",
            "title": "SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation",
            "url": "https://huggingface.co/papers/2409.06633",
            "abstract": "In recent years, the development of diffusion models has led to significant progress in image and video generation tasks, with pre-trained models like the Stable Diffusion series playing a crucial role. Inspired by model pruning which lightens large pre-trained models by removing unimportant parameters, we propose a novel model fine-tuning method to make full use of these ineffective parameters and enable the pre-trained model with new task-specified capabilities. In this work, we first investigate the importance of parameters in pre-trained diffusion models, and discover that the smallest 10% to 20% of parameters by absolute values do not contribute to the generation process. Based on this observation, we propose a method termed SaRA that re-utilizes these temporarily ineffective parameters, equating to optimizing a sparse weight matrix to learn the task-specific knowledge. To mitigate overfitting, we propose a nuclear-norm-based low-rank sparse training scheme for efficient fine-tuning. Furthermore, we design a new progressive parameter adjustment strategy to make full use of the re-trained/finetuned parameters. Finally, we propose a novel unstructural backpropagation strategy, which significantly reduces memory costs during fine-tuning. Our method enhances the generative capabilities of pre-trained models in downstream applications and outperforms traditional fine-tuning methods like LoRA in maintaining model's generalization ability. We validate our approach through fine-tuning experiments on SD models, demonstrating significant improvements. SaRA also offers a practical advantage that requires only a single line of code modification for efficient implementation and is seamlessly compatible with existing methods.",
            "score": 14,
            "issue_id": 1,
            "pub_date": "2024-09-10",
            "pub_date_card": {
                "ru": "10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 10",
                "zh": "9æœˆ10æ—¥"
            },
            "hash": "98510c6f45c81c15",
            "data": {
                "categories": [
                    "#diffusion",
                    "#training",
                    "#optimization"
                ],
                "emoji": "ğŸ”¬",
                "ru": {
                    "title": "Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²",
                    "desc": "ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ SaRA. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼. SaRA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SaRA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº LoRA, Ğ² ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ."
                },
                "en": {
                    "title": "Unlocking Hidden Potential in Diffusion Models with SaRA",
                    "desc": "This paper introduces a new fine-tuning method called SaRA for pre-trained diffusion models, which enhances their performance in image and video generation tasks. The authors identify that a significant portion of parameters in these models, specifically the smallest 10% to 20%, are ineffective during generation and propose to re-utilize them for task-specific learning. To prevent overfitting, they implement a low-rank sparse training scheme and a progressive parameter adjustment strategy. The results show that SaRA not only improves generative capabilities but also reduces memory costs during fine-tuning, making it a practical and efficient solution compared to traditional methods."
                },
                "zh": {
                    "title": "å……åˆ†åˆ©ç”¨æ— æ•ˆå‚æ•°ï¼Œæå‡ç”Ÿæˆèƒ½åŠ›",
                    "desc": "è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œé¢„è®­ç»ƒæ¨¡å‹å¦‚ç¨³å®šæ‰©æ•£ç³»åˆ—å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¨¡å‹å¾®è°ƒæ–¹æ³•ï¼Œå……åˆ†åˆ©ç”¨æ— æ•ˆå‚æ•°ï¼Œä½¿é¢„è®­ç»ƒæ¨¡å‹å…·å¤‡æ–°çš„ä»»åŠ¡ç‰¹å®šèƒ½åŠ›ã€‚é€šè¿‡ç ”ç©¶é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ä¸­å‚æ•°çš„é‡è¦æ€§ï¼Œæˆ‘ä»¬å‘ç°ç»å¯¹å€¼æœ€å°çš„10%åˆ°20%çš„å‚æ•°å¯¹ç”Ÿæˆè¿‡ç¨‹æ²¡æœ‰è´¡çŒ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•SaRAé€šè¿‡ä¼˜åŒ–ç¨€ç–æƒé‡çŸ©é˜µæ¥é‡æ–°åˆ©ç”¨è¿™äº›æ— æ•ˆå‚æ•°ï¼Œä»è€Œæé«˜äº†é¢„è®­ç»ƒæ¨¡å‹åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„ç”Ÿæˆèƒ½åŠ›ã€‚"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2409.06703",
            "title": "LEIA: Latent View-invariant Embeddings for Implicit 3D Articulation",
            "url": "https://huggingface.co/papers/2409.06703",
            "abstract": "Neural Radiance Fields (NeRFs) have revolutionized the reconstruction of static scenes and objects in 3D, offering unprecedented quality. However, extending NeRFs to model dynamic objects or object articulations remains a challenging problem. Previous works have tackled this issue by focusing on part-level reconstruction and motion estimation for objects, but they often rely on heuristics regarding the number of moving parts or object categories, which can limit their practical use. In this work, we introduce LEIA, a novel approach for representing dynamic 3D objects. Our method involves observing the object at distinct time steps or \"states\" and conditioning a hypernetwork on the current state, using this to parameterize our NeRF. This approach allows us to learn a view-invariant latent representation for each state. We further demonstrate that by interpolating between these states, we can generate novel articulation configurations in 3D space that were previously unseen. Our experimental results highlight the effectiveness of our method in articulating objects in a manner that is independent of the viewing angle and joint configuration. Notably, our approach outperforms previous methods that rely on motion information for articulation registration.",
            "score": 2,
            "issue_id": 1,
            "pub_date": "2024-09-10",
            "pub_date_card": {
                "ru": "10 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
                "en": "September 10",
                "zh": "9æœˆ10æ—¥"
            },
            "hash": "a65e33101dcdfaf4",
            "data": {
                "categories": [
                    "#3d"
                ],
                "emoji": "ğŸ¤–",
                "ru": {
                    "title": "LEIA: Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ±ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹",
                    "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LEIA - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ´Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹ (NeRF). ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ¿ĞµÑ€ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ NeRF, Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğµ Ğº Ñ€Ğ°ĞºÑƒÑ€ÑÑƒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. LEIA Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ñ€Ñ‚Ğ¸ĞºÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ°Ñ€Ñ‚Ğ¸ĞºÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ ÑƒĞ³Ğ»Ğ° Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑƒÑÑ‚Ğ°Ğ²Ğ¾Ğ²."
                },
                "en": {
                    "title": "LEIA: Revolutionizing Dynamic 3D Object Representation with NeRFs",
                    "desc": "This paper presents LEIA, a new method for modeling dynamic 3D objects using Neural Radiance Fields (NeRFs). Unlike previous approaches that depend on heuristics for motion estimation, LEIA conditions a hypernetwork on distinct time states of the object to create a more flexible representation. By learning a view-invariant latent representation, the method can interpolate between states to generate new articulations that were not previously captured. Experimental results show that LEIA outperforms existing techniques in articulating objects regardless of viewing angles or joint configurations."
                },
                "zh": {
                    "title": "LEIAï¼šåŠ¨æ€ä¸‰ç»´ç‰©ä½“çš„æ–°è§†è§’",
                    "desc": "ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨é™æ€åœºæ™¯å’Œç‰©ä½“çš„ä¸‰ç»´é‡å»ºä¸­å–å¾—äº†é©å‘½æ€§çš„è¿›å±•ï¼Œä½†å°†å…¶æ‰©å±•åˆ°åŠ¨æ€ç‰©ä½“æˆ–ç‰©ä½“å…³èŠ‚çš„å»ºæ¨¡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨éƒ¨åˆ†é‡å»ºå’Œè¿åŠ¨ä¼°è®¡ä¸Šï¼Œé€šå¸¸ä¾èµ–äºå…³äºç§»åŠ¨éƒ¨ä»¶æ•°é‡æˆ–ç‰©ä½“ç±»åˆ«çš„å¯å‘å¼æ–¹æ³•ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å®é™…åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•LEIAï¼Œé€šè¿‡åœ¨ä¸åŒæ—¶é—´æ­¥è§‚å¯Ÿç‰©ä½“ï¼Œå¹¶æ ¹æ®å½“å‰çŠ¶æ€å¯¹è¶…ç½‘ç»œè¿›è¡Œæ¡ä»¶åŒ–ï¼Œä»è€Œå‚æ•°åŒ–æˆ‘ä»¬çš„NeRFã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç‰©ä½“å…³èŠ‚çš„è¡¨ç°ä¸Šè¶…è¶Šäº†ä¾èµ–è¿åŠ¨ä¿¡æ¯çš„å…ˆå‰æ–¹æ³•ã€‚"
                }
            }
        }
    ],
    "link_prev": "2024-09-10.html",
    "link_next": "2024-09-12.html",
    "link_month": "2024-09.html",
    "short_date_prev": {
        "ru": "10.09",
        "en": "09/10",
        "zh": "9æœˆ10æ—¥"
    },
    "short_date_next": {
        "ru": "12.09",
        "en": "09/12",
        "zh": "9æœˆ12æ—¥"
    },
    "categories": {
        "#dataset": 2,
        "#data": 0,
        "#benchmark": 2,
        "#agents": 1,
        "#cv": 1,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 1,
        "#plp": 0,
        "#inference": 0,
        "#3d": 1,
        "#audio": 3,
        "#video": 1,
        "#multimodal": 2,
        "#math": 0,
        "#multilingual": 0,
        "#architecture": 1,
        "#medicine": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 0,
        "#interpretability": 0,
        "#reasoning": 0,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 0,
        "#security": 0,
        "#edge_computing": 0,
        "#optimization": 1,
        "#survey": 0,
        "#diffusion": 1,
        "#alignment": 0,
        "#story_generation": 0,
        "#hallucinations": 0,
        "#long_context": 0,
        "#synthetic": 0,
        "#translation": 0,
        "#music": 0,
        "#generative": 0
    }
}