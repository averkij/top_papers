{
    "date": {
        "ru": "8 апреля",
        "en": "April 8",
        "zh": "4月8日"
    },
    "time_utc": "2025-04-08 06:15",
    "weekday": 1,
    "issue_id": 3118,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.05298",
            "title": "One-Minute Video Generation with Test-Time Training",
            "url": "https://huggingface.co/papers/2504.05298",
            "abstract": "Transformers today still struggle to generate one-minute videos because self-attention layers are inefficient for long context. Alternatives such as Mamba layers struggle with complex multi-scene stories because their hidden states are less expressive. We experiment with Test-Time Training (TTT) layers, whose hidden states themselves can be neural networks, therefore more expressive. Adding TTT layers into a pre-trained Transformer enables it to generate one-minute videos from text storyboards. For proof of concept, we curate a dataset based on Tom and Jerry cartoons. Compared to baselines such as Mamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers generate much more coherent videos that tell complex stories, leading by 34 Elo points in a human evaluation of 100 videos per method. Although promising, results still contain artifacts, likely due to the limited capability of the pre-trained 5B model. The efficiency of our implementation can also be improved. We have only experimented with one-minute videos due to resource constraints, but the approach can be extended to longer videos and more complex stories. Sample videos, code and annotations are available at: https://test-time-training.github.io/video-dit",
            "score": 14,
            "issue_id": 3116,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "c8edb1a98923d77d",
            "authors": [
                "Karan Dalal",
                "Daniel Koceja",
                "Gashon Hussein",
                "Jiarui Xu",
                "Yue Zhao",
                "Youjin Song",
                "Shihao Han",
                "Ka Chun Cheung",
                "Jan Kautz",
                "Carlos Guestrin",
                "Tatsunori Hashimoto",
                "Sanmi Koyejo",
                "Yejin Choi",
                "Yu Sun",
                "Xiaolong Wang"
            ],
            "affiliations": [
                "NVIDIA",
                "Stanford University",
                "UC Berkeley",
                "UCSD",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05298.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#story_generation",
                    "#long_context",
                    "#dataset"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "TTT слои: прорыв в генерации длинных видео трансформерами",
                    "desc": "Эта статья представляет новый подход к генерации длинных видео с использованием слоев Test-Time Training (TTT) в трансформерах. TTT слои позволяют создавать более выразительные скрытые состояния, что улучшает генерацию сложных многосценных историй по сравнению с альтернативами вроде Mamba. Авторы провели эксперименты на наборе данных мультфильмов 'Том и Джерри', показав преимущество TTT слоев в создании связных минутных видео по текстовым раскадровкам. Хотя результаты многообещающие, все еще присутствуют артефакты, вероятно из-за ограничений предобученной модели."
                },
                "en": {
                    "title": "Enhancing Video Generation with Test-Time Training Layers",
                    "desc": "This paper addresses the challenge of generating one-minute videos from text using Transformers, which struggle with long contexts due to inefficient self-attention layers. The authors introduce Test-Time Training (TTT) layers, which enhance the expressiveness of hidden states by allowing them to be neural networks. By integrating TTT layers into a pre-trained Transformer, the model significantly improves video coherence and storytelling ability compared to existing methods like Mamba and Gated DeltaNet. The results show a notable increase in human evaluation scores, although the authors acknowledge the presence of artifacts and the need for further efficiency improvements."
                },
                "zh": {
                    "title": "提升视频生成的表达能力",
                    "desc": "本文探讨了在生成一分钟视频时，变换器模型面临的挑战，尤其是自注意力层在处理长上下文时的低效。我们提出了测试时训练（TTT）层，这些层的隐藏状态可以是神经网络，从而提高了表达能力。通过将TTT层添加到预训练的变换器中，我们能够从文本故事板生成更连贯的一分钟视频。尽管结果显示出良好的潜力，但仍存在一些伪影，表明预训练的5B模型能力有限。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05305",
            "title": "URECA: Unique Region Caption Anything",
            "url": "https://huggingface.co/papers/2504.05305",
            "abstract": "Region-level captioning aims to generate natural language descriptions for specific image regions while highlighting their distinguishing features. However, existing methods struggle to produce unique captions across multi-granularity, limiting their real-world applicability. To address the need for detailed region-level understanding, we introduce URECA dataset, a large-scale dataset tailored for multi-granularity region captioning. Unlike prior datasets that focus primarily on salient objects, URECA dataset ensures a unique and consistent mapping between regions and captions by incorporating a diverse set of objects, parts, and background elements. Central to this is a stage-wise data curation pipeline, where each stage incrementally refines region selection and caption generation. By leveraging Multimodal Large Language Models (MLLMs) at each stage, our pipeline produces distinctive and contextually grounded captions with improved accuracy and semantic diversity. Building upon this dataset, we present URECA, a novel captioning model designed to effectively encode multi-granularity regions. URECA maintains essential spatial properties such as position and shape through simple yet impactful modifications to existing MLLMs, enabling fine-grained and semantically rich region descriptions. Our approach introduces dynamic mask modeling and a high-resolution mask encoder to enhance caption uniqueness. Experiments show that URECA achieves state-of-the-art performance on URECA dataset and generalizes well to existing region-level captioning benchmarks.",
            "score": 9,
            "issue_id": 3115,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "6eec948e6319fc99",
            "authors": [
                "Sangbeom Lim",
                "Junwan Kim",
                "Heeji Yoon",
                "Jaewoo Jung",
                "Seungryong Kim"
            ],
            "affiliations": [
                "KAIST AI",
                "Korea University",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05305.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#games",
                    "#cv",
                    "#interpretability",
                    "#dataset",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Точное описание регионов изображений с помощью многоуровневого подхода",
                    "desc": "Статья представляет новый набор данных URECA для многоуровневого описания регионов изображений. Авторы разработали поэтапный процесс создания данных с использованием мультимодальных больших языковых моделей для генерации уникальных и контекстуально обоснованных описаний. На основе этого набора данных предложена модель URECA, которая эффективно кодирует регионы разной детализации, сохраняя их пространственные свойства. Эксперименты показывают, что URECA достигает наилучших результатов на созданном наборе данных и хорошо обобщается на существующие эталонные тесты."
                },
                "en": {
                    "title": "Enhancing Region-Level Captioning with URECA Dataset and Model",
                    "desc": "This paper presents a new approach to region-level captioning, which generates detailed descriptions for specific parts of images. The authors introduce the URECA dataset, designed to improve the uniqueness of captions by including a variety of objects and backgrounds. They propose a novel captioning model, URECA, that uses advanced techniques like dynamic mask modeling to maintain spatial properties and enhance the quality of generated captions. The results demonstrate that URECA outperforms existing methods, providing more accurate and diverse descriptions across different image regions."
                },
                "zh": {
                    "title": "多粒度区域描述的新突破",
                    "desc": "区域级描述旨在为特定图像区域生成自然语言描述，并突出其独特特征。然而，现有方法在多粒度生成独特描述方面存在困难，限制了其在实际应用中的有效性。为了解决这一问题，我们引入了URECA数据集，这是一个针对多粒度区域描述的大规模数据集，确保区域与描述之间的独特和一致的映射。基于此数据集，我们提出了URECA模型，能够有效编码多粒度区域，生成细致且语义丰富的描述。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02828",
            "title": "Concept Lancet: Image Editing with Compositional Representation\n  Transplant",
            "url": "https://huggingface.co/papers/2504.02828",
            "abstract": "Diffusion models are widely used for image editing tasks. Existing editing methods often design a representation manipulation procedure by curating an edit direction in the text embedding or score space. However, such a procedure faces a key challenge: overestimating the edit strength harms visual consistency while underestimating it fails the editing task. Notably, each source image may require a different editing strength, and it is costly to search for an appropriate strength via trial-and-error. To address this challenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play framework for principled representation manipulation in diffusion-based image editing. At inference time, we decompose the source input in the latent (text embedding or diffusion score) space as a sparse linear combination of the representations of the collected visual concepts. This allows us to accurately estimate the presence of concepts in each image, which informs the edit. Based on the editing task (replace/add/remove), we perform a customized concept transplant process to impose the corresponding editing direction. To sufficiently model the concept space, we curate a conceptual representation dataset, CoLan-150K, which contains diverse descriptions and scenarios of visual terms and phrases for the latent dictionary. Experiments on multiple diffusion-based image editing baselines show that methods equipped with CoLan achieve state-of-the-art performance in editing effectiveness and consistency preservation.",
            "score": 9,
            "issue_id": 3117,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "1c289ffc8ceda51e",
            "authors": [
                "Jinqi Luo",
                "Tianjiao Ding",
                "Kwan Ho Ryan Chan",
                "Hancheng Min",
                "Chris Callison-Burch",
                "René Vidal"
            ],
            "affiliations": [
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02828.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#dataset",
                    "#cv",
                    "#inference"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "Точное редактирование изображений с помощью концептуального скальпеля",
                    "desc": "Статья представляет новый подход к редактированию изображений с помощью диффузионных моделей, называемый Concept Lancet (CoLan). CoLan решает проблему определения оптимальной силы редактирования для каждого изображения, разлагая входное изображение в латентном пространстве как линейную комбинацию визуальных концептов. Этот метод позволяет точно оценить присутствие концептов в изображении и выполнить соответствующее редактирование. Авторы также создали датасет CoLan-150K с разнообразными описаниями визуальных терминов для латентного словаря."
                },
                "en": {
                    "title": "Precision Editing with Concept Lancet",
                    "desc": "This paper introduces Concept Lancet (CoLan), a novel framework for improving image editing using diffusion models. It addresses the challenge of determining the right strength of edits needed for different images, which can vary significantly. CoLan utilizes a sparse linear combination of visual concept representations to accurately assess and manipulate the presence of these concepts in images. The framework is supported by a comprehensive dataset, CoLan-150K, which enhances the editing process by providing diverse visual descriptions and scenarios."
                },
                "zh": {
                    "title": "精准编辑，概念移植！",
                    "desc": "扩散模型在图像编辑任务中被广泛应用。现有的编辑方法通常通过在文本嵌入或评分空间中设计编辑方向来操控表示。然而，这种方法面临一个关键挑战：过高的编辑强度会损害视觉一致性，而过低的编辑强度则无法完成编辑任务。为了解决这个问题，我们提出了Concept Lancet（CoLan），这是一个零-shot的即插即用框架，能够在扩散基础的图像编辑中进行原则性的表示操控。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05288",
            "title": "LiveVQA: Live Visual Knowledge Seeking",
            "url": "https://huggingface.co/papers/2504.05288",
            "abstract": "We introduce LiveVQA, an automatically collected dataset of latest visual knowledge from the Internet with synthesized VQA problems. LiveVQA consists of 3,602 single- and multi-hop visual questions from 6 news websites across 14 news categories, featuring high-quality image-text coherence and authentic information. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and Qwen-2.5-VL family) demonstrates that stronger models perform better overall, with advanced visual reasoning capabilities proving crucial for complex multi-hop questions. Despite excellent performance on textual problems, models with tools like search engines still show significant gaps when addressing visual questions requiring latest visual knowledge, highlighting important areas for future research.",
            "score": 6,
            "issue_id": 3117,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "8302679426ac3ec4",
            "authors": [
                "Mingyang Fu",
                "Yuyang Peng",
                "Benlin Liu",
                "Yao Wan",
                "Dongping Chen"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05288.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#reasoning",
                    "#survey",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "LiveVQA: Новый рубеж в визуальном вопросно-ответном анализе",
                    "desc": "LiveVQA - это автоматически собранный датасет с последними визуальными знаниями из интернета и синтезированными задачами визуальных вопросов и ответов (VQA). Он содержит 3602 одношаговых и многошаговых визуальных вопроса с 6 новостных сайтов по 14 категориям новостей, отличаясь высоким качеством согласованности изображений и текста. Оценка 15 мультимодальных языковых моделей (MLLM) показала, что более мощные модели работают лучше в целом, а продвинутые возможности визуального рассуждения критически важны для сложных многошаговых вопросов. Несмотря на отличные результаты в текстовых задачах, модели с инструментами вроде поисковых систем все еще демонстрируют значительные пробелы при ответах на визуальные вопросы, требующие актуальных визуальных знаний."
                },
                "en": {
                    "title": "Empowering Visual Question Answering with LiveVQA",
                    "desc": "LiveVQA is a new dataset designed to enhance visual question answering (VQA) by providing up-to-date visual knowledge sourced from the Internet. It includes 3,602 questions that require reasoning over images and text, covering various news topics. Our tests on 15 advanced machine learning language models (MLLMs) show that models with better visual reasoning skills excel at answering complex questions. However, even the best models struggle with visual questions that need the latest information, indicating a need for further research in this area."
                },
                "zh": {
                    "title": "最新视觉知识的问答挑战",
                    "desc": "我们介绍了LiveVQA，这是一个自动收集的最新视觉知识数据集，包含合成的视觉问答（VQA）问题。LiveVQA包含来自6个新闻网站的3,602个单跳和多跳视觉问题，涵盖14个新闻类别，具有高质量的图像-文本一致性和真实信息。我们的评估显示，15个大型语言模型（如GPT-4o、Gemma-3和Qwen-2.5-VL系列）中，性能更强的模型在整体表现上更好，尤其在复杂的多跳问题上，先进的视觉推理能力至关重要。尽管在文本问题上表现出色，但使用搜索引擎等工具的模型在处理需要最新视觉知识的视觉问题时仍存在显著差距，这突显了未来研究的重要领域。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.04823",
            "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
            "url": "https://huggingface.co/papers/2504.04823",
            "abstract": "Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this study, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, and QwQ-32B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes will be open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.",
            "score": 5,
            "issue_id": 3117,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "92bd3deed21195f2",
            "authors": [
                "Ruikang Liu",
                "Yuxuan Sun",
                "Manyi Zhang",
                "Haoli Bai",
                "Xianzhi Yu",
                "Tiezheng Yu",
                "Chun Yuan",
                "Lu Hou"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.04823.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#inference",
                    "#reasoning",
                    "#math",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Квантование моделей рассуждений: баланс между эффективностью и точностью",
                    "desc": "Это исследование посвящено изучению влияния квантования на языковые модели, специализирующиеся на рассуждениях. Авторы провели систематический анализ квантованных моделей рассуждений, оценивая различные семейства моделей с параметрами от 1,5B до 70B. Исследование охватывает квантование весов, KV-кэша и активаций с использованием современных алгоритмов при различных битовых ширинах. Результаты показывают, что хотя безлосстное квантование возможно при W8A8 или W4A16, более низкие битовые ширины значительно снижают точность."
                },
                "en": {
                    "title": "Optimizing Reasoning Models with Quantization",
                    "desc": "This paper investigates the effects of quantization on reasoning language models, which are known for their complex task performance but high inference costs. The authors systematically evaluate various quantization techniques on models like DeepSeek-R1-Distilled Qwen and LLaMA, focusing on different parameter sizes and quantization methods. They find that while lossless quantization is possible with certain configurations, lower bit-widths can lead to significant accuracy drops. Additionally, the study highlights that model size, origin, and task difficulty are crucial factors influencing performance, and suggests that adjusting model sizes or reasoning steps can improve outcomes."
                },
                "zh": {
                    "title": "量化推理模型的系统研究",
                    "desc": "最近，推理语言模型在复杂任务中表现出色，但其链式推理过程增加了推理开销。虽然量化技术已被广泛应用于降低大型语言模型的推理成本，但其对推理模型的影响仍未得到充分研究。我们首次系统性地研究了量化推理模型，评估了多个开源模型，并在不同的位宽下进行权重、KV缓存和激活量化的实验。研究发现，尽管可以实现无损量化，但较低的位宽会显著影响准确性，同时模型大小、来源和任务难度是性能的关键因素。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05304",
            "title": "Gaussian Mixture Flow Matching Models",
            "url": "https://huggingface.co/papers/2504.05304",
            "abstract": "Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an L_2 denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256times256.",
            "score": 2,
            "issue_id": 3115,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "b0223808c61a3545",
            "authors": [
                "Hansheng Chen",
                "Kai Zhang",
                "Hao Tan",
                "Zexiang Xu",
                "Fujun Luan",
                "Leonidas Guibas",
                "Gordon Wetzstein",
                "Sai Bi"
            ],
            "affiliations": [
                "Adobe Research, CA 95110, USA",
                "Hillbot",
                "Stanford University, CA 94305, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05304.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "GMFlow: мощная генерация изображений с гауссовыми смесями",
                    "desc": "Статья представляет новую модель Gaussian mixture flow matching (GMFlow) для генерации изображений. GMFlow предсказывает параметры динамической гауссовой смеси для захвата мультимодального распределения скорости потока, что можно обучить с помощью потери KL-дивергенции. Авторы разработали специальные решатели GM-SDE/ODE для точного сэмплирования за небольшое число шагов. Также предложена новая схема вероятностного управления, улучшающая качество генерации изображений и решающая проблему пересыщенности цветов при классификационно-свободном управлении."
                },
                "en": {
                    "title": "GMFlow: Enhancing Image Generation with Dynamic Gaussian Mixtures",
                    "desc": "This paper introduces a new model called Gaussian Mixture Flow Matching (GMFlow) that improves upon traditional diffusion models and flow matching models. Instead of just predicting a single Gaussian mean, GMFlow predicts parameters for a dynamic Gaussian mixture, allowing it to better capture complex distributions in the data. The model addresses issues like discretization error and color saturation in generated images by using a novel probabilistic guidance scheme. Experimental results show that GMFlow achieves higher image generation quality with fewer sampling steps compared to existing methods."
                },
                "zh": {
                    "title": "高斯混合流匹配：提升图像生成质量的新方法",
                    "desc": "扩散模型通过高斯分布来近似去噪分布并预测其均值，而流匹配模型则将高斯均值重新参数化为流速。然而，它们在少步采样时表现不佳，主要是由于离散化误差，并且在无分类器引导下容易产生过饱和的颜色。为了解决这些问题，我们提出了一种新颖的高斯混合流匹配（GMFlow）模型：GMFlow预测动态高斯混合参数，以捕捉多模态流速分布，并通过KL散度损失进行学习。实验表明，GMFlow在生成质量上始终优于流匹配基线，在ImageNet 256x256上仅用6个采样步骤就达到了0.942的精度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.04715",
            "title": "Are You Getting What You Pay For? Auditing Model Substitution in LLM\n  APIs",
            "url": "https://huggingface.co/papers/2504.04715",
            "abstract": "The proliferation of Large Language Models (LLMs) accessed via black-box APIs introduces a significant trust challenge: users pay for services based on advertised model capabilities (e.g., size, performance), but providers may covertly substitute the specified model with a cheaper, lower-quality alternative to reduce operational costs. This lack of transparency undermines fairness, erodes trust, and complicates reliable benchmarking. Detecting such substitutions is difficult due to the black-box nature, typically limiting interaction to input-output queries. This paper formalizes the problem of model substitution detection in LLM APIs. We systematically evaluate existing verification techniques, including output-based statistical tests, benchmark evaluations, and log probability analysis, under various realistic attack scenarios like model quantization, randomized substitution, and benchmark evasion. Our findings reveal the limitations of methods relying solely on text outputs, especially against subtle or adaptive attacks. While log probability analysis offers stronger guarantees when available, its accessibility is often limited. We conclude by discussing the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as a pathway towards provable model integrity, highlighting the trade-offs between security, performance, and provider adoption. Code is available at https://github.com/sunblaze-ucb/llm-api-audit",
            "score": 2,
            "issue_id": 3117,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "28e9dfa4b4a0421a",
            "authors": [
                "Will Cai",
                "Tianneng Shi",
                "Xuandong Zhao",
                "Dawn Song"
            ],
            "affiliations": [
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.04715.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#inference",
                    "#benchmark",
                    "#ethics"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Защита от подмены: обеспечение честности в API языковых моделей",
                    "desc": "Статья рассматривает проблему доверия к API больших языковых моделей (LLM), когда провайдеры могут тайно подменять заявленные модели на более дешевые аналоги. Авторы формализуют задачу обнаружения таких подмен и оценивают эффективность существующих методов верификации. Исследование показывает ограниченность методов, основанных только на анализе текстовых выходов, особенно против адаптивных атак. В качестве потенциального решения предлагается использование доверенных сред исполнения (TEE) для обеспечения целостности модели."
                },
                "en": {
                    "title": "Ensuring Trust in Large Language Models: Detecting Substitutions in Black-Box APIs",
                    "desc": "This paper addresses the issue of trust in Large Language Models (LLMs) accessed through APIs, where users may unknowingly receive lower-quality models instead of the advertised ones. It formalizes the challenge of detecting these model substitutions, which is complicated by the black-box nature of LLMs that limits user interactions to simple input-output queries. The authors evaluate various existing verification techniques, revealing their limitations, particularly against sophisticated attacks that can evade detection. They propose hardware-based solutions like Trusted Execution Environments (TEEs) as a potential way to ensure model integrity, while also considering the trade-offs involved."
                },
                "zh": {
                    "title": "确保大型语言模型的透明性与信任",
                    "desc": "本文探讨了大型语言模型（LLMs）在黑箱API中使用所带来的信任挑战。用户支付服务费用时，依赖于模型的能力（如规模和性能），但提供者可能会偷偷用更便宜、质量更低的替代模型来降低成本。这种缺乏透明度的问题影响了公平性和信任度，并使得可靠的基准测试变得复杂。我们系统评估了现有的验证技术，并提出了基于硬件的解决方案，以提高模型的完整性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02812",
            "title": "BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose\n  Estimation",
            "url": "https://huggingface.co/papers/2504.02812",
            "abstract": "We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the sixth in a series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world scenarios. First, we introduced new model-free tasks, where no 3D object models are available and methods need to onboard objects just from provided reference videos. Second, we defined a new, more practical 6D object detection task where identities of objects visible in a test image are not provided as input. Third, we introduced new BOP-H3 datasets recorded with high-resolution sensors and AR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D models and onboarding videos to support both model-based and model-free tasks. Participants competed on seven challenge tracks, each defined by a task, object onboarding setup, and dataset group. Notably, the best 2024 method for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only 4% behind the best 2023 method for seen objects (GPose2023) although being significantly slower (24.9 vs 2.7s per image). A more practical 2024 method for this task is Co-op which takes only 0.8s per image and is 25X faster and 13% more accurate than GenFlow. Methods have a similar ranking on 6D detection as on 6D localization but higher run time. On model-based 2D detection of unseen objects, the best 2024 method (MUSE) achieves 21% relative improvement compared to the best 2023 method (CNOS). However, the 2D detection accuracy for unseen objects is still noticealy (-53%) behind the accuracy for seen objects (GDet2023). The online evaluation system stays open and is available at http://bop.felk.cvut.cz/",
            "score": 1,
            "issue_id": 3118,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "692278d7307c7950",
            "authors": [
                "Van Nguyen Nguyen",
                "Stephen Tyree",
                "Andrew Guo",
                "Mederic Fourmy",
                "Anas Gouda",
                "Taeyeop Lee",
                "Sungphill Moon",
                "Hyeontae Son",
                "Lukas Ranftl",
                "Jonathan Tremblay",
                "Eric Brachmann",
                "Bertram Drost",
                "Vincent Lepetit",
                "Carsten Rother",
                "Stan Birchfield",
                "Jiri Matas",
                "Yann Labbe",
                "Martin Sundermeyer",
                "Tomas Hodan"
            ],
            "affiliations": [
                "CTU Prague",
                "ENPC",
                "Google",
                "Heidelberg University",
                "KAIST",
                "MVTec",
                "Meta",
                "NAVER LABS",
                "NVIDIA",
                "Niantic",
                "TU Dortmund",
                "TU Munich",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02812.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Прорыв в компьютерном зрении: новые горизонты оценки 6D позы объектов",
                    "desc": "Статья представляет результаты соревнования BOP Challenge 2024 по оценке 6D позы объектов. Введены новые задачи без использования 3D моделей и более практичные сценарии обнаружения объектов. Представлены новые наборы данных BOP-H3, записанные с помощью высокоточных сенсоров и AR/VR гарнитур. Лучшие методы 2024 года показали значительное улучшение точности по сравнению с методами 2023 года для различных задач, включая локализацию и обнаружение объектов."
                },
                "en": {
                    "title": "Advancing 6D Object Pose Estimation in Real-World Scenarios",
                    "desc": "The BOP Challenge 2024 focuses on advancing 6D object pose estimation by transitioning from controlled lab environments to real-world applications. This year, new model-free tasks were introduced, requiring methods to learn from reference videos without 3D models. The challenge also featured a practical 6D object detection task where object identities were not provided, alongside the release of the BOP-H3 datasets that simulate real-world conditions. Notably, the top-performing methods demonstrated significant improvements in accuracy and speed compared to previous years, highlighting the ongoing evolution in this field of machine learning."
                },
                "zh": {
                    "title": "BOP挑战赛：从实验室到真实世界的6D物体姿态估计",
                    "desc": "本文介绍了2024年BOP挑战赛的评估方法、数据集和结果，这是一个旨在捕捉6D物体姿态估计最新技术的公开竞赛。2024年的目标是将BOP从实验室环境转向真实世界场景，推出了新的无模型任务，要求方法仅通过参考视频进行物体识别。我们还定义了一个更实用的6D物体检测任务，测试图像中物体的身份不再作为输入提供。此外，BOP-H3数据集使用高分辨率传感器和AR/VR头显录制，支持模型基础和无模型任务。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03770",
            "title": "JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language\n  Model",
            "url": "https://huggingface.co/papers/2504.03770",
            "abstract": "Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety mechanisms in models, leading to the generation of inappropriate or unsafe content. Detecting such attacks is critical to ensuring the responsible deployment of MLLMs. Existing jailbreak detection methods face three primary challenges: (1) Many rely on model hidden states or gradients, limiting their applicability to white-box models, where the internal workings of the model are accessible; (2) They involve high computational overhead from uncertainty-based analysis, which limits real-time detection, and (3) They require fully labeled harmful datasets, which are often scarce in real-world settings. To address these issues, we introduce a test-time adaptive framework called JAILDAM. Our method leverages a memory-based approach guided by policy-driven unsafe knowledge representations, eliminating the need for explicit exposure to harmful data. By dynamically updating unsafe knowledge during test-time, our framework improves generalization to unseen jailbreak strategies while maintaining efficiency. Experiments on multiple VLM jailbreak benchmarks demonstrate that JAILDAM delivers state-of-the-art performance in harmful content detection, improving both accuracy and speed.",
            "score": 1,
            "issue_id": 3117,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "01c2f4c752f3e71d",
            "authors": [
                "Yi Nian",
                "Shenzhe Zhu",
                "Yuehan Qin",
                "Li Li",
                "Ziyi Wang",
                "Chaowei Xiao",
                "Yue Zhao"
            ],
            "affiliations": [
                "University of Maryland",
                "University of Southern California",
                "University of Toronto",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03770.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#multimodal",
                    "#security",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Адаптивная защита MLLM от атак 'jailbreak' без доступа к вредоносным данным",
                    "desc": "В статье представлен метод JAILDAM для обнаружения атак типа 'jailbreak' на мультимодальные большие языковые модели (MLLM). JAILDAM использует подход на основе памяти, управляемый представлениями небезопасных знаний, что устраняет необходимость в явном воздействии на вредоносные данные. Метод динамически обновляет небезопасные знания во время тестирования, улучшая обобщение на новые стратегии атак при сохранении эффективности. Эксперименты показывают, что JAILDAM обеспечивает современную производительность в обнаружении вредоносного контента, улучшая как точность, так и скорость."
                },
                "en": {
                    "title": "JAILDAM: Enhancing Safety in MLLMs Against Jailbreak Attacks",
                    "desc": "This paper discusses the challenges of detecting jailbreak attacks in multimodal large language models (MLLMs), which can generate harmful content. Jailbreak attacks manipulate models to bypass safety features, making detection crucial for responsible use. The authors present JAILDAM, a novel framework that adapts during testing to identify these attacks without needing extensive harmful datasets. By using a memory-based approach, JAILDAM enhances detection efficiency and accuracy against various jailbreak strategies."
                },
                "zh": {
                    "title": "提升多模态模型安全性的关键",
                    "desc": "多模态大型语言模型（MLLMs）在视觉语言任务中表现出色，但也存在生成有害内容的重大风险，尤其是通过越狱攻击。越狱攻击是指故意操控以绕过模型的安全机制，导致生成不当或不安全的内容。检测此类攻击对于确保MLLMs的负责任部署至关重要。我们提出了一种名为JAILDAM的测试时自适应框架，通过动态更新不安全知识来提高对未见越狱策略的泛化能力，同时保持高效性。"
                }
            }
        }
    ],
    "link_prev": "2025-04-07.html",
    "link_next": "2025-04-09.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "07.04",
        "en": "04/07",
        "zh": "4月7日"
    },
    "short_date_next": {
        "ru": "09.04",
        "en": "04/09",
        "zh": "4月9日"
    },
    "categories": {
        "#dataset": 6,
        "#data": 1,
        "#benchmark": 6,
        "#agents": 0,
        "#cv": 5,
        "#rl": 0,
        "#rlhf": 0,
        "#rag": 0,
        "#plp": 0,
        "#inference": 4,
        "#3d": 0,
        "#audio": 0,
        "#video": 1,
        "#multimodal": 2,
        "#math": 1,
        "#multilingual": 0,
        "#architecture": 0,
        "#healthcare": 0,
        "#training": 3,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 2,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 2,
        "#optimization": 1,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 1,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 1,
        "#synthetic": 0,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 0,
        "#science": 0,
        "#low_resource": 0
    },
    "zh": {
        "text": "这篇文章介绍了一个新的多语言问题解决基准，称为Multi-SWE-bench。它涵盖了Java、TypeScript、JavaScript、Go、Rust、C和C++等语言。该基准包含1,632个高质量实例，由68位专家注释。基于这个基准，作者评估了一系列先进的模型，并提供了详细的分析。此外，文章还宣布成立了一个开源社区Multi-SWE-RL，旨在为问题解决任务构建大规模强化学习训练数据集。",
        "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
        "pinyin": "这篇文章介绍了一个新的多语言问题解决基准，称为Multi-SWE-bench。它涵盖了Java、TypeScript、JavaScript、Go、Rust、C和C++等语言。该基准包含1,632个高质量实例，由68位专家注释。基于这个基准，作者评估了一系列先进的模型，并提供了详细的分析。此外，文章还宣布成立了一个开源社区Multi-SWE-RL，旨在为问题解决任务构建大规模强化学习训练数据集。\n\nzhè piān wén zhāng jiè shào le yī gè xīn de duō yǔ yán wèn tí jiě jué jī zhǔn, chēng wéi Multi-SWE-bench. tā hán gǎi le Java, TypeScript, JavaScript, Go, Rust, C hé C++ děng yǔ yán. gǎi jī zhǔn bāo hán 1,632 gè gāo zhì liàng shí lì, yǒu 68 wèi zhuān jiā zhù shì. jī yú zhè gè jī zhǔn, zuò zhě píng gū le yī xì liè xiān jìn de mó xíng, bìng tí gōng le xiáng xì de fēn xī. cǐ wài, wén zhāng hái xuān bù chéng lì le yī gè kāi yuán shè qū Multi-SWE-RL, zhǐ yú wèi wèn tí jiě jué rèn wù gòu jiàn dà guī mó qiáng huà xué xùn liàn shù jù jí.",
        "vocab": "[\n    {\"word\": \"涵盖\", \"pinyin\": \"hángài\", \"trans\": \"cover\"},\n    {\"word\": \"基准\", \"pinyin\": \"jīzhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"注释\", \"pinyin\": \"zhùshì\", \"trans\": \"annotate\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"evaluate\"},\n    {\"word\": \"先进\", \"pinyin\": \"xiānjìn\", \"trans\": \"advanced\"},\n    {\"word\": \"详细\", \"pinyin\": \"xiángxì\", \"trans\": \"detailed\"},\n    {\"word\": \"分析\", \"pinyin\": \"fēnxī\", \"trans\": \"analysis\"},\n    {\"word\": \"宣布\", \"pinyin\": \"xuānbù\", \"trans\": \"announce\"},\n    {\"word\": \"成立\", \"pinyin\": \"chénglì\", \"trans\": \"establish\"},\n    {\"word\": \"开源\", \"pinyin\": \"kāiyuán\", \"trans\": \"open-source\"},\n    {\"word\": \"社区\", \"pinyin\": \"shèqū\", \"trans\": \"community\"},\n    {\"word\": \"旨在\", \"pinyin\": \"zhǐzài\", \"trans\": \"aim to\"},\n    {\"word\": \"构建\", \"pinyin\": \"gòujiàn\", \"trans\": \"build\"},\n    {\"word\": \"任务\", \"pinyin\": \"rènwù\", \"trans\": \"task\"},\n    {\"word\": \"强化学习\", \"pinyin\": \"qiáng huà xuéxí\", \"trans\": \"reinforcement learning\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùnliàn\", \"trans\": \"training\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shùjùjí\", \"trans\": \"dataset\"}\n]",
        "trans": "This article introduces a new multilingual problem-solving benchmark called Multi-SWE-bench. It covers languages such as Java, TypeScript, JavaScript, Go, Rust, C, and C++. The benchmark contains 1,632 high-quality instances annotated by 68 experts. Based on this benchmark, the authors evaluated a series of advanced models and provided detailed analyses. Additionally, the article announces the establishment of an open-source community, Multi-SWE-RL, aimed at building large-scale reinforcement learning training datasets for problem-solving tasks.",
        "update_ts": "2025-04-07 09:12"
    }
}