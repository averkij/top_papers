{
    "date": {
        "ru": "8 апреля",
        "en": "April 8",
        "zh": "4月8日"
    },
    "time_utc": "2025-04-08 08:15",
    "weekday": 1,
    "issue_id": 3120,
    "home_page_url": "https://huggingface.co/papers",
    "papers": [
        {
            "id": "https://huggingface.co/papers/2504.05298",
            "title": "One-Minute Video Generation with Test-Time Training",
            "url": "https://huggingface.co/papers/2504.05298",
            "abstract": "Transformers today still struggle to generate one-minute videos because self-attention layers are inefficient for long context. Alternatives such as Mamba layers struggle with complex multi-scene stories because their hidden states are less expressive. We experiment with Test-Time Training (TTT) layers, whose hidden states themselves can be neural networks, therefore more expressive. Adding TTT layers into a pre-trained Transformer enables it to generate one-minute videos from text storyboards. For proof of concept, we curate a dataset based on Tom and Jerry cartoons. Compared to baselines such as Mamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers generate much more coherent videos that tell complex stories, leading by 34 Elo points in a human evaluation of 100 videos per method. Although promising, results still contain artifacts, likely due to the limited capability of the pre-trained 5B model. The efficiency of our implementation can also be improved. We have only experimented with one-minute videos due to resource constraints, but the approach can be extended to longer videos and more complex stories. Sample videos, code and annotations are available at: https://test-time-training.github.io/video-dit",
            "score": 25,
            "issue_id": 3116,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "c8edb1a98923d77d",
            "authors": [
                "Karan Dalal",
                "Daniel Koceja",
                "Gashon Hussein",
                "Jiarui Xu",
                "Yue Zhao",
                "Youjin Song",
                "Shihao Han",
                "Ka Chun Cheung",
                "Jan Kautz",
                "Carlos Guestrin",
                "Tatsunori Hashimoto",
                "Sanmi Koyejo",
                "Yejin Choi",
                "Yu Sun",
                "Xiaolong Wang"
            ],
            "affiliations": [
                "NVIDIA",
                "Stanford University",
                "UC Berkeley",
                "UCSD",
                "UT Austin"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05298.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#video",
                    "#story_generation",
                    "#long_context",
                    "#dataset"
                ],
                "emoji": "🎬",
                "ru": {
                    "title": "TTT слои: прорыв в генерации длинных видео трансформерами",
                    "desc": "Эта статья представляет новый подход к генерации длинных видео с использованием слоев Test-Time Training (TTT) в трансформерах. TTT слои позволяют создавать более выразительные скрытые состояния, что улучшает генерацию сложных многосценных историй по сравнению с альтернативами вроде Mamba. Авторы провели эксперименты на наборе данных мультфильмов 'Том и Джерри', показав преимущество TTT слоев в создании связных минутных видео по текстовым раскадровкам. Хотя результаты многообещающие, все еще присутствуют артефакты, вероятно из-за ограничений предобученной модели."
                },
                "en": {
                    "title": "Enhancing Video Generation with Test-Time Training Layers",
                    "desc": "This paper addresses the challenge of generating one-minute videos from text using Transformers, which struggle with long contexts due to inefficient self-attention layers. The authors introduce Test-Time Training (TTT) layers, which enhance the expressiveness of hidden states by allowing them to be neural networks. By integrating TTT layers into a pre-trained Transformer, the model significantly improves video coherence and storytelling ability compared to existing methods like Mamba and Gated DeltaNet. The results show a notable increase in human evaluation scores, although the authors acknowledge the presence of artifacts and the need for further efficiency improvements."
                },
                "zh": {
                    "title": "提升视频生成的表达能力",
                    "desc": "本文探讨了在生成一分钟视频时，变换器模型面临的挑战，尤其是自注意力层在处理长上下文时的低效。我们提出了测试时训练（TTT）层，这些层的隐藏状态可以是神经网络，从而提高了表达能力。通过将TTT层添加到预训练的变换器中，我们能够从文本故事板生成更连贯的一分钟视频。尽管结果显示出良好的潜力，但仍存在一些伪影，表明预训练的5B模型能力有限。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.04718",
            "title": "T1: Tool-integrated Self-verification for Test-time Compute Scaling in\n  Small Language Models",
            "url": "https://huggingface.co/papers/2504.04718",
            "abstract": "Recent studies have demonstrated that test-time compute scaling effectively improves the performance of small language models (sLMs). However, prior research has mainly examined test-time compute scaling with an additional larger model as a verifier, leaving self-verification by sLMs underexplored. In this work, we investigate whether sLMs can reliably self-verify their outputs under test-time scaling. We find that even with knowledge distillation from larger verifiers, sLMs struggle with verification tasks requiring memorization, such as numerical calculations and fact-checking. To address this limitation, we propose Tool-integrated self-verification (T1), which delegates memorization-heavy verification steps to external tools, such as a code interpreter. Our theoretical analysis shows that tool integration reduces memorization demands and improves test-time scaling performance. Experiments on the MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under test-time scaling outperforms the significantly larger Llama-3.1 8B model. Moreover, T1 generalizes effectively to both mathematical (MATH500) and multi-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the potential of tool integration to substantially improve the self-verification abilities of sLMs.",
            "score": 16,
            "issue_id": 3120,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "51f832049b5599a6",
            "authors": [
                "Minki Kang",
                "Jongwon Jeong",
                "Jaewoong Cho"
            ],
            "affiliations": [
                "KAIST",
                "KRAFTON"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.04718.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#training",
                    "#math",
                    "#small_models",
                    "#reasoning"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "Малые языковые модели становятся умнее с помощью инструментов",
                    "desc": "Исследователи изучили возможность самопроверки малых языковых моделей (sLMs) при масштабировании во время тестирования. Обнаружено, что sLMs испытывают трудности с задачами, требующими запоминания, даже после дистилляции знаний от больших моделей. Предложен метод Tool-integrated self-verification (T1), который делегирует задачи, требующие запоминания, внешним инструментам. Эксперименты показали, что T1 позволяет небольшой модели Llama-3.2 1B превзойти значительно большую Llama-3.1 8B на различных задачах."
                },
                "en": {
                    "title": "Empowering Small Models with Tool Integration for Better Self-Verification",
                    "desc": "This paper explores how small language models (sLMs) can improve their performance through test-time compute scaling, particularly focusing on their ability to self-verify outputs. The authors find that sLMs face challenges in verification tasks that require memorization, such as numerical calculations and fact-checking, even when they learn from larger models. To overcome this, they introduce Tool-integrated self-verification (T1), which allows sLMs to use external tools for tasks that require heavy memorization. Their experiments show that T1 significantly enhances the performance of sLMs, enabling them to outperform larger models in various knowledge-intensive tasks."
                },
                "zh": {
                    "title": "工具集成提升小型语言模型自我验证能力",
                    "desc": "最近的研究表明，测试时计算扩展可以有效提高小型语言模型（sLMs）的性能。然而，之前的研究主要关注于使用更大模型作为验证者的测试时计算扩展，而对sLMs的自我验证研究较少。我们发现，即使通过知识蒸馏从更大的验证者那里获得知识，sLMs在需要记忆的验证任务（如数字计算和事实核查）中仍然面临困难。为了解决这个问题，我们提出了工具集成自我验证（T1），将重记忆的验证步骤委托给外部工具，如代码解释器。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05305",
            "title": "URECA: Unique Region Caption Anything",
            "url": "https://huggingface.co/papers/2504.05305",
            "abstract": "Region-level captioning aims to generate natural language descriptions for specific image regions while highlighting their distinguishing features. However, existing methods struggle to produce unique captions across multi-granularity, limiting their real-world applicability. To address the need for detailed region-level understanding, we introduce URECA dataset, a large-scale dataset tailored for multi-granularity region captioning. Unlike prior datasets that focus primarily on salient objects, URECA dataset ensures a unique and consistent mapping between regions and captions by incorporating a diverse set of objects, parts, and background elements. Central to this is a stage-wise data curation pipeline, where each stage incrementally refines region selection and caption generation. By leveraging Multimodal Large Language Models (MLLMs) at each stage, our pipeline produces distinctive and contextually grounded captions with improved accuracy and semantic diversity. Building upon this dataset, we present URECA, a novel captioning model designed to effectively encode multi-granularity regions. URECA maintains essential spatial properties such as position and shape through simple yet impactful modifications to existing MLLMs, enabling fine-grained and semantically rich region descriptions. Our approach introduces dynamic mask modeling and a high-resolution mask encoder to enhance caption uniqueness. Experiments show that URECA achieves state-of-the-art performance on URECA dataset and generalizes well to existing region-level captioning benchmarks.",
            "score": 11,
            "issue_id": 3115,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "6eec948e6319fc99",
            "authors": [
                "Sangbeom Lim",
                "Junwan Kim",
                "Heeji Yoon",
                "Jaewoo Jung",
                "Seungryong Kim"
            ],
            "affiliations": [
                "KAIST AI",
                "Korea University",
                "Yonsei University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05305.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#games",
                    "#cv",
                    "#interpretability",
                    "#dataset",
                    "#benchmark",
                    "#multimodal"
                ],
                "emoji": "🖼️",
                "ru": {
                    "title": "Точное описание регионов изображений с помощью многоуровневого подхода",
                    "desc": "Статья представляет новый набор данных URECA для многоуровневого описания регионов изображений. Авторы разработали поэтапный процесс создания данных с использованием мультимодальных больших языковых моделей для генерации уникальных и контекстуально обоснованных описаний. На основе этого набора данных предложена модель URECA, которая эффективно кодирует регионы разной детализации, сохраняя их пространственные свойства. Эксперименты показывают, что URECA достигает наилучших результатов на созданном наборе данных и хорошо обобщается на существующие эталонные тесты."
                },
                "en": {
                    "title": "Enhancing Region-Level Captioning with URECA Dataset and Model",
                    "desc": "This paper presents a new approach to region-level captioning, which generates detailed descriptions for specific parts of images. The authors introduce the URECA dataset, designed to improve the uniqueness of captions by including a variety of objects and backgrounds. They propose a novel captioning model, URECA, that uses advanced techniques like dynamic mask modeling to maintain spatial properties and enhance the quality of generated captions. The results demonstrate that URECA outperforms existing methods, providing more accurate and diverse descriptions across different image regions."
                },
                "zh": {
                    "title": "多粒度区域描述的新突破",
                    "desc": "区域级描述旨在为特定图像区域生成自然语言描述，并突出其独特特征。然而，现有方法在多粒度生成独特描述方面存在困难，限制了其在实际应用中的有效性。为了解决这一问题，我们引入了URECA数据集，这是一个针对多粒度区域描述的大规模数据集，确保区域与描述之间的独特和一致的映射。基于此数据集，我们提出了URECA模型，能够有效编码多粒度区域，生成细致且语义丰富的描述。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02828",
            "title": "Concept Lancet: Image Editing with Compositional Representation\n  Transplant",
            "url": "https://huggingface.co/papers/2504.02828",
            "abstract": "Diffusion models are widely used for image editing tasks. Existing editing methods often design a representation manipulation procedure by curating an edit direction in the text embedding or score space. However, such a procedure faces a key challenge: overestimating the edit strength harms visual consistency while underestimating it fails the editing task. Notably, each source image may require a different editing strength, and it is costly to search for an appropriate strength via trial-and-error. To address this challenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play framework for principled representation manipulation in diffusion-based image editing. At inference time, we decompose the source input in the latent (text embedding or diffusion score) space as a sparse linear combination of the representations of the collected visual concepts. This allows us to accurately estimate the presence of concepts in each image, which informs the edit. Based on the editing task (replace/add/remove), we perform a customized concept transplant process to impose the corresponding editing direction. To sufficiently model the concept space, we curate a conceptual representation dataset, CoLan-150K, which contains diverse descriptions and scenarios of visual terms and phrases for the latent dictionary. Experiments on multiple diffusion-based image editing baselines show that methods equipped with CoLan achieve state-of-the-art performance in editing effectiveness and consistency preservation.",
            "score": 10,
            "issue_id": 3117,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "1c289ffc8ceda51e",
            "authors": [
                "Jinqi Luo",
                "Tianjiao Ding",
                "Kwan Ho Ryan Chan",
                "Hancheng Min",
                "Chris Callison-Burch",
                "René Vidal"
            ],
            "affiliations": [
                "University of Pennsylvania"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02828.jpg",
            "data": {
                "categories": [
                    "#diffusion",
                    "#dataset",
                    "#cv",
                    "#inference"
                ],
                "emoji": "✂️",
                "ru": {
                    "title": "Точное редактирование изображений с помощью концептуального скальпеля",
                    "desc": "Статья представляет новый подход к редактированию изображений с помощью диффузионных моделей, называемый Concept Lancet (CoLan). CoLan решает проблему определения оптимальной силы редактирования для каждого изображения, разлагая входное изображение в латентном пространстве как линейную комбинацию визуальных концептов. Этот метод позволяет точно оценить присутствие концептов в изображении и выполнить соответствующее редактирование. Авторы также создали датасет CoLan-150K с разнообразными описаниями визуальных терминов для латентного словаря."
                },
                "en": {
                    "title": "Precision Editing with Concept Lancet",
                    "desc": "This paper introduces Concept Lancet (CoLan), a novel framework for improving image editing using diffusion models. It addresses the challenge of determining the right strength of edits needed for different images, which can vary significantly. CoLan utilizes a sparse linear combination of visual concept representations to accurately assess and manipulate the presence of these concepts in images. The framework is supported by a comprehensive dataset, CoLan-150K, which enhances the editing process by providing diverse visual descriptions and scenarios."
                },
                "zh": {
                    "title": "精准编辑，概念移植！",
                    "desc": "扩散模型在图像编辑任务中被广泛应用。现有的编辑方法通常通过在文本嵌入或评分空间中设计编辑方向来操控表示。然而，这种方法面临一个关键挑战：过高的编辑强度会损害视觉一致性，而过低的编辑强度则无法完成编辑任务。为了解决这个问题，我们提出了Concept Lancet（CoLan），这是一个零-shot的即插即用框架，能够在扩散基础的图像编辑中进行原则性的表示操控。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05288",
            "title": "LiveVQA: Live Visual Knowledge Seeking",
            "url": "https://huggingface.co/papers/2504.05288",
            "abstract": "We introduce LiveVQA, an automatically collected dataset of latest visual knowledge from the Internet with synthesized VQA problems. LiveVQA consists of 3,602 single- and multi-hop visual questions from 6 news websites across 14 news categories, featuring high-quality image-text coherence and authentic information. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and Qwen-2.5-VL family) demonstrates that stronger models perform better overall, with advanced visual reasoning capabilities proving crucial for complex multi-hop questions. Despite excellent performance on textual problems, models with tools like search engines still show significant gaps when addressing visual questions requiring latest visual knowledge, highlighting important areas for future research.",
            "score": 6,
            "issue_id": 3117,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "8302679426ac3ec4",
            "authors": [
                "Mingyang Fu",
                "Yuyang Peng",
                "Benlin Liu",
                "Yao Wan",
                "Dongping Chen"
            ],
            "affiliations": [
                "Huazhong University of Science and Technology",
                "University of Washington"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05288.jpg",
            "data": {
                "categories": [
                    "#cv",
                    "#reasoning",
                    "#survey",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "LiveVQA: Новый рубеж в визуальном вопросно-ответном анализе",
                    "desc": "LiveVQA - это автоматически собранный датасет с последними визуальными знаниями из интернета и синтезированными задачами визуальных вопросов и ответов (VQA). Он содержит 3602 одношаговых и многошаговых визуальных вопроса с 6 новостных сайтов по 14 категориям новостей, отличаясь высоким качеством согласованности изображений и текста. Оценка 15 мультимодальных языковых моделей (MLLM) показала, что более мощные модели работают лучше в целом, а продвинутые возможности визуального рассуждения критически важны для сложных многошаговых вопросов. Несмотря на отличные результаты в текстовых задачах, модели с инструментами вроде поисковых систем все еще демонстрируют значительные пробелы при ответах на визуальные вопросы, требующие актуальных визуальных знаний."
                },
                "en": {
                    "title": "Empowering Visual Question Answering with LiveVQA",
                    "desc": "LiveVQA is a new dataset designed to enhance visual question answering (VQA) by providing up-to-date visual knowledge sourced from the Internet. It includes 3,602 questions that require reasoning over images and text, covering various news topics. Our tests on 15 advanced machine learning language models (MLLMs) show that models with better visual reasoning skills excel at answering complex questions. However, even the best models struggle with visual questions that need the latest information, indicating a need for further research in this area."
                },
                "zh": {
                    "title": "最新视觉知识的问答挑战",
                    "desc": "我们介绍了LiveVQA，这是一个自动收集的最新视觉知识数据集，包含合成的视觉问答（VQA）问题。LiveVQA包含来自6个新闻网站的3,602个单跳和多跳视觉问题，涵盖14个新闻类别，具有高质量的图像-文本一致性和真实信息。我们的评估显示，15个大型语言模型（如GPT-4o、Gemma-3和Qwen-2.5-VL系列）中，性能更强的模型在整体表现上更好，尤其在复杂的多跳问题上，先进的视觉推理能力至关重要。尽管在文本问题上表现出色，但使用搜索引擎等工具的模型在处理需要最新视觉知识的视觉问题时仍存在显著差距，这突显了未来研究的重要领域。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.04823",
            "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
            "url": "https://huggingface.co/papers/2504.04823",
            "abstract": "Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this study, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, and QwQ-32B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes will be open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.",
            "score": 6,
            "issue_id": 3117,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "92bd3deed21195f2",
            "authors": [
                "Ruikang Liu",
                "Yuxuan Sun",
                "Manyi Zhang",
                "Haoli Bai",
                "Xianzhi Yu",
                "Tiezheng Yu",
                "Chun Yuan",
                "Lu Hou"
            ],
            "affiliations": [
                "Huawei Noahs Ark Lab",
                "Shenzhen International Graduate School, Tsinghua University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.04823.jpg",
            "data": {
                "categories": [
                    "#open_source",
                    "#optimization",
                    "#inference",
                    "#reasoning",
                    "#math",
                    "#training",
                    "#benchmark"
                ],
                "emoji": "🧠",
                "ru": {
                    "title": "Квантование моделей рассуждений: баланс между эффективностью и точностью",
                    "desc": "Это исследование посвящено изучению влияния квантования на языковые модели, специализирующиеся на рассуждениях. Авторы провели систематический анализ квантованных моделей рассуждений, оценивая различные семейства моделей с параметрами от 1,5B до 70B. Исследование охватывает квантование весов, KV-кэша и активаций с использованием современных алгоритмов при различных битовых ширинах. Результаты показывают, что хотя безлосстное квантование возможно при W8A8 или W4A16, более низкие битовые ширины значительно снижают точность."
                },
                "en": {
                    "title": "Optimizing Reasoning Models with Quantization",
                    "desc": "This paper investigates the effects of quantization on reasoning language models, which are known for their complex task performance but high inference costs. The authors systematically evaluate various quantization techniques on models like DeepSeek-R1-Distilled Qwen and LLaMA, focusing on different parameter sizes and quantization methods. They find that while lossless quantization is possible with certain configurations, lower bit-widths can lead to significant accuracy drops. Additionally, the study highlights that model size, origin, and task difficulty are crucial factors influencing performance, and suggests that adjusting model sizes or reasoning steps can improve outcomes."
                },
                "zh": {
                    "title": "量化推理模型的系统研究",
                    "desc": "最近，推理语言模型在复杂任务中表现出色，但其链式推理过程增加了推理开销。虽然量化技术已被广泛应用于降低大型语言模型的推理成本，但其对推理模型的影响仍未得到充分研究。我们首次系统性地研究了量化推理模型，评估了多个开源模型，并在不同的位宽下进行权重、KV缓存和激活量化的实验。研究发现，尽管可以实现无损量化，但较低的位宽会显著影响准确性，同时模型大小、来源和任务难度是性能的关键因素。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05299",
            "title": "SmolVLM: Redefining small and efficient multimodal models",
            "url": "https://huggingface.co/papers/2504.05299",
            "abstract": "Large Vision-Language Models (VLMs) deliver exceptional performance but require significant computational resources, limiting their deployment on mobile and edge devices. Smaller VLMs typically mirror design choices of larger models, such as extensive image tokenization, leading to inefficient GPU memory usage and constrained practicality for on-device applications.   We introduce SmolVLM, a series of compact multimodal models specifically engineered for resource-efficient inference. We systematically explore architectural configurations, tokenization strategies, and data curation optimized for low computational overhead. Through this, we identify key design choices that yield substantial performance gains on image and video tasks with minimal memory footprints.   Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during inference and outperforms the 300-times larger Idefics-80B model, despite an 18-month development gap. Our largest model, at 2.2B parameters, rivals state-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend beyond static images, demonstrating robust video comprehension capabilities.   Our results emphasize that strategic architectural optimizations, aggressive yet efficient tokenization, and carefully curated training data significantly enhance multimodal performance, facilitating practical, energy-efficient deployments at significantly smaller scales.",
            "score": 3,
            "issue_id": 3120,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "325b8841a555743d",
            "authors": [
                "Andrés Marafioti",
                "Orr Zohar",
                "Miquel Farré",
                "Merve Noyan",
                "Elie Bakouch",
                "Pedro Cuenca",
                "Cyril Zakka",
                "Loubna Ben Allal",
                "Anton Lozhkov",
                "Nouamane Tazi",
                "Vaibhav Srivastav",
                "Joshua Lochner",
                "Hugo Larcher",
                "Mathieu Morlon",
                "Lewis Tunstall",
                "Leandro von Werra",
                "Thomas Wolf"
            ],
            "affiliations": [
                "Hugging Face",
                "Stanford University"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05299.jpg",
            "data": {
                "categories": [
                    "#data",
                    "#multimodal",
                    "#video",
                    "#optimization",
                    "#low_resource",
                    "#architecture",
                    "#inference",
                    "#small_models"
                ],
                "emoji": "🤏",
                "ru": {
                    "title": "Маленькие модели - большие возможности: SmolVLM revolutionizes эффективность VLM",
                    "desc": "SmolVLM - это серия компактных мультимодальных моделей, разработанных для эффективного использования ресурсов при выводе. Исследователи систематически изучили архитектурные конфигурации, стратегии токенизации и подготовку данных, оптимизированные для низких вычислительных затрат. Самая маленькая модель, SmolVLM-256M, использует менее 1 ГБ видеопамяти при выводе и превосходит в 300 раз большую модель Idefics-80B. Результаты показывают, что стратегические архитектурные оптимизации и тщательно подобранные данные для обучения значительно улучшают мультимодальную производительность при меньших масштабах."
                },
                "en": {
                    "title": "SmolVLM: Compact Models for Efficient Vision-Language Tasks",
                    "desc": "This paper presents SmolVLM, a series of compact vision-language models designed to operate efficiently on mobile and edge devices. Unlike larger models that require extensive computational resources, SmolVLM employs optimized architectural configurations and tokenization strategies to minimize GPU memory usage. The smallest model, SmolVLM-256M, achieves superior performance on image and video tasks while using less than 1GB of GPU memory, outperforming much larger models. The findings highlight the importance of strategic design choices in enhancing multimodal capabilities while ensuring practical deployment in resource-constrained environments."
                },
                "zh": {
                    "title": "SmolVLM：高效的多模态模型",
                    "desc": "大型视觉语言模型（VLMs）表现优异，但需要大量计算资源，限制了它们在移动和边缘设备上的应用。较小的VLM通常模仿大型模型的设计选择，导致GPU内存使用效率低下。我们提出了SmolVLM，这是一系列专为资源高效推理而设计的紧凑型多模态模型。我们的研究表明，通过优化架构配置、标记策略和数据整理，可以在保持较小内存占用的同时显著提升图像和视频任务的性能。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.05304",
            "title": "Gaussian Mixture Flow Matching Models",
            "url": "https://huggingface.co/papers/2504.05304",
            "abstract": "Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an L_2 denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256times256.",
            "score": 2,
            "issue_id": 3115,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "b0223808c61a3545",
            "authors": [
                "Hansheng Chen",
                "Kai Zhang",
                "Hao Tan",
                "Zexiang Xu",
                "Fujun Luan",
                "Leonidas Guibas",
                "Gordon Wetzstein",
                "Sai Bi"
            ],
            "affiliations": [
                "Adobe Research, CA 95110, USA",
                "Hillbot",
                "Stanford University, CA 94305, USA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.05304.jpg",
            "data": {
                "categories": [
                    "#training",
                    "#cv",
                    "#diffusion",
                    "#inference"
                ],
                "emoji": "🌊",
                "ru": {
                    "title": "GMFlow: мощная генерация изображений с гауссовыми смесями",
                    "desc": "Статья представляет новую модель Gaussian mixture flow matching (GMFlow) для генерации изображений. GMFlow предсказывает параметры динамической гауссовой смеси для захвата мультимодального распределения скорости потока, что можно обучить с помощью потери KL-дивергенции. Авторы разработали специальные решатели GM-SDE/ODE для точного сэмплирования за небольшое число шагов. Также предложена новая схема вероятностного управления, улучшающая качество генерации изображений и решающая проблему пересыщенности цветов при классификационно-свободном управлении."
                },
                "en": {
                    "title": "GMFlow: Enhancing Image Generation with Dynamic Gaussian Mixtures",
                    "desc": "This paper introduces a new model called Gaussian Mixture Flow Matching (GMFlow) that improves upon traditional diffusion models and flow matching models. Instead of just predicting a single Gaussian mean, GMFlow predicts parameters for a dynamic Gaussian mixture, allowing it to better capture complex distributions in the data. The model addresses issues like discretization error and color saturation in generated images by using a novel probabilistic guidance scheme. Experimental results show that GMFlow achieves higher image generation quality with fewer sampling steps compared to existing methods."
                },
                "zh": {
                    "title": "高斯混合流匹配：提升图像生成质量的新方法",
                    "desc": "扩散模型通过高斯分布来近似去噪分布并预测其均值，而流匹配模型则将高斯均值重新参数化为流速。然而，它们在少步采样时表现不佳，主要是由于离散化误差，并且在无分类器引导下容易产生过饱和的颜色。为了解决这些问题，我们提出了一种新颖的高斯混合流匹配（GMFlow）模型：GMFlow预测动态高斯混合参数，以捕捉多模态流速分布，并通过KL散度损失进行学习。实验表明，GMFlow在生成质量上始终优于流匹配基线，在ImageNet 256x256上仅用6个采样步骤就达到了0.942的精度。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.04715",
            "title": "Are You Getting What You Pay For? Auditing Model Substitution in LLM\n  APIs",
            "url": "https://huggingface.co/papers/2504.04715",
            "abstract": "The proliferation of Large Language Models (LLMs) accessed via black-box APIs introduces a significant trust challenge: users pay for services based on advertised model capabilities (e.g., size, performance), but providers may covertly substitute the specified model with a cheaper, lower-quality alternative to reduce operational costs. This lack of transparency undermines fairness, erodes trust, and complicates reliable benchmarking. Detecting such substitutions is difficult due to the black-box nature, typically limiting interaction to input-output queries. This paper formalizes the problem of model substitution detection in LLM APIs. We systematically evaluate existing verification techniques, including output-based statistical tests, benchmark evaluations, and log probability analysis, under various realistic attack scenarios like model quantization, randomized substitution, and benchmark evasion. Our findings reveal the limitations of methods relying solely on text outputs, especially against subtle or adaptive attacks. While log probability analysis offers stronger guarantees when available, its accessibility is often limited. We conclude by discussing the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as a pathway towards provable model integrity, highlighting the trade-offs between security, performance, and provider adoption. Code is available at https://github.com/sunblaze-ucb/llm-api-audit",
            "score": 2,
            "issue_id": 3117,
            "pub_date": "2025-04-07",
            "pub_date_card": {
                "ru": "7 апреля",
                "en": "April 7",
                "zh": "4月7日"
            },
            "hash": "28e9dfa4b4a0421a",
            "authors": [
                "Will Cai",
                "Tianneng Shi",
                "Xuandong Zhao",
                "Dawn Song"
            ],
            "affiliations": [
                "University of California, Berkeley"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.04715.jpg",
            "data": {
                "categories": [
                    "#security",
                    "#inference",
                    "#benchmark",
                    "#ethics"
                ],
                "emoji": "🕵️",
                "ru": {
                    "title": "Защита от подмены: обеспечение честности в API языковых моделей",
                    "desc": "Статья рассматривает проблему доверия к API больших языковых моделей (LLM), когда провайдеры могут тайно подменять заявленные модели на более дешевые аналоги. Авторы формализуют задачу обнаружения таких подмен и оценивают эффективность существующих методов верификации. Исследование показывает ограниченность методов, основанных только на анализе текстовых выходов, особенно против адаптивных атак. В качестве потенциального решения предлагается использование доверенных сред исполнения (TEE) для обеспечения целостности модели."
                },
                "en": {
                    "title": "Ensuring Trust in Large Language Models: Detecting Substitutions in Black-Box APIs",
                    "desc": "This paper addresses the issue of trust in Large Language Models (LLMs) accessed through APIs, where users may unknowingly receive lower-quality models instead of the advertised ones. It formalizes the challenge of detecting these model substitutions, which is complicated by the black-box nature of LLMs that limits user interactions to simple input-output queries. The authors evaluate various existing verification techniques, revealing their limitations, particularly against sophisticated attacks that can evade detection. They propose hardware-based solutions like Trusted Execution Environments (TEEs) as a potential way to ensure model integrity, while also considering the trade-offs involved."
                },
                "zh": {
                    "title": "确保大型语言模型的透明性与信任",
                    "desc": "本文探讨了大型语言模型（LLMs）在黑箱API中使用所带来的信任挑战。用户支付服务费用时，依赖于模型的能力（如规模和性能），但提供者可能会偷偷用更便宜、质量更低的替代模型来降低成本。这种缺乏透明度的问题影响了公平性和信任度，并使得可靠的基准测试变得复杂。我们系统评估了现有的验证技术，并提出了基于硬件的解决方案，以提高模型的完整性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03964",
            "title": "Clinical ModernBERT: An efficient and long context encoder for\n  biomedical text",
            "url": "https://huggingface.co/papers/2504.03964",
            "abstract": "We introduce Clinical ModernBERT, a transformer based encoder pretrained on large scale biomedical literature, clinical notes, and medical ontologies, incorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with their textual descriptions. Building on ModernBERT the current state of the art natural language text encoder featuring architectural upgrades such as rotary positional embeddings (RoPE), Flash Attention, and extended context length up to 8,192 tokens our model adapts these innovations specifically for biomedical and clinical domains. Clinical ModernBERT excels at producing semantically rich representations tailored for long context tasks. We validate this both by analyzing its pretrained weights and through empirical evaluation on a comprehensive suite of clinical NLP benchmarks.",
            "score": 1,
            "issue_id": 3119,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "6602bd6699f6f653",
            "authors": [
                "Simon A. Lee",
                "Anthony Wu",
                "Jeffrey N. Chiang"
            ],
            "affiliations": [
                "Department of Computational Medicine & Neurosurgery UCLA",
                "Department of Computational Medicine UCLA"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03964.jpg",
            "data": {
                "categories": [
                    "#science",
                    "#long_context",
                    "#benchmark",
                    "#architecture",
                    "#healthcare",
                    "#dataset"
                ],
                "emoji": "🏥",
                "ru": {
                    "title": "Прорыв в обработке медицинских текстов: Clinical ModernBERT",
                    "desc": "Эта статья представляет Clinical ModernBERT - трансформерную модель, предобученную на биомедицинской литературе, клинических заметках и медицинских онтологиях. Модель основана на архитектуре ModernBERT и включает улучшения, такие как ротационные позиционные эмбеддинги и расширенный контекст до 8192 токенов. Clinical ModernBERT специализируется на создании семантически богатых представлений для задач с длинным контекстом в медицинской сфере. Эффективность модели подтверждена анализом предобученных весов и оценкой на наборе клинических NLP-бенчмарков."
                },
                "en": {
                    "title": "Empowering Clinical NLP with Advanced Transformer Technology",
                    "desc": "Clinical ModernBERT is a specialized transformer model designed for the biomedical and clinical fields. It is pretrained on a vast array of data, including biomedical literature and clinical notes, to enhance its understanding of medical language. The model incorporates advanced features like rotary positional embeddings and Flash Attention, allowing it to handle longer text inputs effectively. Its performance is validated through rigorous testing on various clinical natural language processing benchmarks, demonstrating its ability to generate meaningful representations for complex medical tasks."
                },
                "zh": {
                    "title": "生物医学领域的强大文本编码器",
                    "desc": "我们介绍了Clinical ModernBERT，这是一种基于变换器的编码器，经过大规模生物医学文献、临床笔记和医学本体的预训练。该模型结合了PubMed摘要、MIMIC IV临床数据和医学代码及其文本描述，采用了现代BERT的架构升级，如旋转位置嵌入（RoPE）和闪存注意力（Flash Attention）。Clinical ModernBERT在处理长上下文任务时，能够生成语义丰富的表示。我们通过分析其预训练权重和在临床自然语言处理基准上的实证评估来验证其有效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03193",
            "title": "Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language\n  Models for Domain-Generalized Semantic Segmentation",
            "url": "https://huggingface.co/papers/2504.03193",
            "abstract": "Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained traction in Domain Generalized Semantic Segmentation (DGSS) due to their strong generalization capabilities. However, existing DGSS methods often rely exclusively on either VFMs or VLMs, overlooking their complementary strengths. VFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g., CLIP) provide robust text alignment but struggle with coarse granularity. Despite their complementary strengths, effectively integrating VFMs and VLMs with attention mechanisms is challenging, as the increased patch tokens complicate long-sequence modeling. To address this, we propose MFuser, a novel Mamba-based fusion framework that efficiently combines the strengths of VFMs and VLMs while maintaining linear scalability in sequence length. MFuser consists of two key components: MVFuser, which acts as a co-adapter to jointly fine-tune the two models by capturing both sequential and spatial dynamics; and MTEnhancer, a hybrid attention-Mamba module that refines text embeddings by incorporating image priors. Our approach achieves precise feature locality and strong text alignment without incurring significant computational overhead. Extensive experiments demonstrate that MFuser significantly outperforms state-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and 71.87 mIoU on real-to-real benchmarks. The code is available at https://github.com/devinxzhang/MFuser.",
            "score": 1,
            "issue_id": 3120,
            "pub_date": "2025-04-04",
            "pub_date_card": {
                "ru": "4 апреля",
                "en": "April 4",
                "zh": "4月4日"
            },
            "hash": "6eca2c0b1acce1f3",
            "authors": [
                "Xin Zhang",
                "Robby T. Tan"
            ],
            "affiliations": [
                "ASUS Intelligent Cloud Services",
                "National University of Singapore"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03193.jpg",
            "data": {
                "categories": [
                    "#optimization",
                    "#multimodal",
                    "#architecture",
                    "#cv",
                    "#benchmark",
                    "#synthetic"
                ],
                "emoji": "🔍",
                "ru": {
                    "title": "MFuser: Эффективное слияние VFM и VLM для улучшенной семантической сегментации",
                    "desc": "Эта статья представляет новый подход к обобщенной семантической сегментации доменов (DGSS), называемый MFuser. Он объединяет сильные стороны моделей визуального основания (VFM) и визуально-языковых моделей (VLM), используя архитектуру на основе Mamba. MFuser включает в себя MVFuser для совместной доводки моделей и MTEnhancer для улучшения текстовых эмбеддингов с учетом визуальных признаков. Экспериментальные результаты показывают значительное превосходство MFuser над современными методами DGSS на различных бенчмарках."
                },
                "en": {
                    "title": "Harnessing the Power of Vision Models for Better Segmentation",
                    "desc": "This paper introduces MFuser, a new framework that combines Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) for Domain Generalized Semantic Segmentation (DGSS). VFMs are good at capturing detailed visual features, while VLMs excel in aligning text with images, but they have limitations when used separately. MFuser uses a co-adapter and a hybrid attention mechanism to effectively merge these models, allowing for better feature extraction and text alignment without heavy computational costs. The results show that MFuser outperforms existing DGSS methods, achieving high accuracy on various benchmarks."
                },
                "zh": {
                    "title": "融合视觉与语言，提升语义分割能力",
                    "desc": "视觉基础模型（VFM）和视觉语言模型（VLM）在领域泛化语义分割（DGSS）中因其强大的泛化能力而受到关注。现有的DGSS方法通常只依赖于VFM或VLM，忽视了它们的互补优势。我们提出了MFuser，一个新颖的融合框架，能够高效结合VFM和VLM的优点，同时保持序列长度的线性可扩展性。通过联合微调和混合注意力机制，MFuser在特征局部性和文本对齐方面表现出色，显著超越了现有的DGSS方法。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02812",
            "title": "BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose\n  Estimation",
            "url": "https://huggingface.co/papers/2504.02812",
            "abstract": "We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the sixth in a series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world scenarios. First, we introduced new model-free tasks, where no 3D object models are available and methods need to onboard objects just from provided reference videos. Second, we defined a new, more practical 6D object detection task where identities of objects visible in a test image are not provided as input. Third, we introduced new BOP-H3 datasets recorded with high-resolution sensors and AR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D models and onboarding videos to support both model-based and model-free tasks. Participants competed on seven challenge tracks, each defined by a task, object onboarding setup, and dataset group. Notably, the best 2024 method for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only 4% behind the best 2023 method for seen objects (GPose2023) although being significantly slower (24.9 vs 2.7s per image). A more practical 2024 method for this task is Co-op which takes only 0.8s per image and is 25X faster and 13% more accurate than GenFlow. Methods have a similar ranking on 6D detection as on 6D localization but higher run time. On model-based 2D detection of unseen objects, the best 2024 method (MUSE) achieves 21% relative improvement compared to the best 2023 method (CNOS). However, the 2D detection accuracy for unseen objects is still noticealy (-53%) behind the accuracy for seen objects (GDet2023). The online evaluation system stays open and is available at http://bop.felk.cvut.cz/",
            "score": 1,
            "issue_id": 3118,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "692278d7307c7950",
            "authors": [
                "Van Nguyen Nguyen",
                "Stephen Tyree",
                "Andrew Guo",
                "Mederic Fourmy",
                "Anas Gouda",
                "Taeyeop Lee",
                "Sungphill Moon",
                "Hyeontae Son",
                "Lukas Ranftl",
                "Jonathan Tremblay",
                "Eric Brachmann",
                "Bertram Drost",
                "Vincent Lepetit",
                "Carsten Rother",
                "Stan Birchfield",
                "Jiri Matas",
                "Yann Labbe",
                "Martin Sundermeyer",
                "Tomas Hodan"
            ],
            "affiliations": [
                "CTU Prague",
                "ENPC",
                "Google",
                "Heidelberg University",
                "KAIST",
                "MVTec",
                "Meta",
                "NAVER LABS",
                "NVIDIA",
                "Niantic",
                "TU Dortmund",
                "TU Munich",
                "University of Toronto"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02812.jpg",
            "data": {
                "categories": [
                    "#dataset",
                    "#cv",
                    "#benchmark"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "Прорыв в компьютерном зрении: новые горизонты оценки 6D позы объектов",
                    "desc": "Статья представляет результаты соревнования BOP Challenge 2024 по оценке 6D позы объектов. Введены новые задачи без использования 3D моделей и более практичные сценарии обнаружения объектов. Представлены новые наборы данных BOP-H3, записанные с помощью высокоточных сенсоров и AR/VR гарнитур. Лучшие методы 2024 года показали значительное улучшение точности по сравнению с методами 2023 года для различных задач, включая локализацию и обнаружение объектов."
                },
                "en": {
                    "title": "Advancing 6D Object Pose Estimation in Real-World Scenarios",
                    "desc": "The BOP Challenge 2024 focuses on advancing 6D object pose estimation by transitioning from controlled lab environments to real-world applications. This year, new model-free tasks were introduced, requiring methods to learn from reference videos without 3D models. The challenge also featured a practical 6D object detection task where object identities were not provided, alongside the release of the BOP-H3 datasets that simulate real-world conditions. Notably, the top-performing methods demonstrated significant improvements in accuracy and speed compared to previous years, highlighting the ongoing evolution in this field of machine learning."
                },
                "zh": {
                    "title": "BOP挑战赛：从实验室到真实世界的6D物体姿态估计",
                    "desc": "本文介绍了2024年BOP挑战赛的评估方法、数据集和结果，这是一个旨在捕捉6D物体姿态估计最新技术的公开竞赛。2024年的目标是将BOP从实验室环境转向真实世界场景，推出了新的无模型任务，要求方法仅通过参考视频进行物体识别。我们还定义了一个更实用的6D物体检测任务，测试图像中物体的身份不再作为输入提供。此外，BOP-H3数据集使用高分辨率传感器和AR/VR头显录制，支持模型基础和无模型任务。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.03770",
            "title": "JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language\n  Model",
            "url": "https://huggingface.co/papers/2504.03770",
            "abstract": "Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety mechanisms in models, leading to the generation of inappropriate or unsafe content. Detecting such attacks is critical to ensuring the responsible deployment of MLLMs. Existing jailbreak detection methods face three primary challenges: (1) Many rely on model hidden states or gradients, limiting their applicability to white-box models, where the internal workings of the model are accessible; (2) They involve high computational overhead from uncertainty-based analysis, which limits real-time detection, and (3) They require fully labeled harmful datasets, which are often scarce in real-world settings. To address these issues, we introduce a test-time adaptive framework called JAILDAM. Our method leverages a memory-based approach guided by policy-driven unsafe knowledge representations, eliminating the need for explicit exposure to harmful data. By dynamically updating unsafe knowledge during test-time, our framework improves generalization to unseen jailbreak strategies while maintaining efficiency. Experiments on multiple VLM jailbreak benchmarks demonstrate that JAILDAM delivers state-of-the-art performance in harmful content detection, improving both accuracy and speed.",
            "score": 1,
            "issue_id": 3117,
            "pub_date": "2025-04-03",
            "pub_date_card": {
                "ru": "3 апреля",
                "en": "April 3",
                "zh": "4月3日"
            },
            "hash": "01c2f4c752f3e71d",
            "authors": [
                "Yi Nian",
                "Shenzhe Zhu",
                "Yuehan Qin",
                "Li Li",
                "Ziyi Wang",
                "Chaowei Xiao",
                "Yue Zhao"
            ],
            "affiliations": [
                "University of Maryland",
                "University of Southern California",
                "University of Toronto",
                "University of Wisconsin-Madison"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.03770.jpg",
            "data": {
                "categories": [
                    "#alignment",
                    "#multimodal",
                    "#security",
                    "#dataset",
                    "#benchmark"
                ],
                "emoji": "🛡️",
                "ru": {
                    "title": "Адаптивная защита MLLM от атак 'jailbreak' без доступа к вредоносным данным",
                    "desc": "В статье представлен метод JAILDAM для обнаружения атак типа 'jailbreak' на мультимодальные большие языковые модели (MLLM). JAILDAM использует подход на основе памяти, управляемый представлениями небезопасных знаний, что устраняет необходимость в явном воздействии на вредоносные данные. Метод динамически обновляет небезопасные знания во время тестирования, улучшая обобщение на новые стратегии атак при сохранении эффективности. Эксперименты показывают, что JAILDAM обеспечивает современную производительность в обнаружении вредоносного контента, улучшая как точность, так и скорость."
                },
                "en": {
                    "title": "JAILDAM: Enhancing Safety in MLLMs Against Jailbreak Attacks",
                    "desc": "This paper discusses the challenges of detecting jailbreak attacks in multimodal large language models (MLLMs), which can generate harmful content. Jailbreak attacks manipulate models to bypass safety features, making detection crucial for responsible use. The authors present JAILDAM, a novel framework that adapts during testing to identify these attacks without needing extensive harmful datasets. By using a memory-based approach, JAILDAM enhances detection efficiency and accuracy against various jailbreak strategies."
                },
                "zh": {
                    "title": "提升多模态模型安全性的关键",
                    "desc": "多模态大型语言模型（MLLMs）在视觉语言任务中表现出色，但也存在生成有害内容的重大风险，尤其是通过越狱攻击。越狱攻击是指故意操控以绕过模型的安全机制，导致生成不当或不安全的内容。检测此类攻击对于确保MLLMs的负责任部署至关重要。我们提出了一种名为JAILDAM的测试时自适应框架，通过动态更新不安全知识来提高对未见越狱策略的泛化能力，同时保持高效性。"
                }
            }
        },
        {
            "id": "https://huggingface.co/papers/2504.02882",
            "title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for\n  Tool-Augmented Large Language Models",
            "url": "https://huggingface.co/papers/2504.02882",
            "abstract": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in real-world applications, but face challenges in handling incomplete queries and out-of-scope requests. While existing approaches rely mainly on Supervised Fine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method that enhances TA-LLM's dialogue capabilities through Direct Preference Optimization. We model TA-LLM interactions as a Markov Decision Process with 5 distinct dialogue states and categorize user queries into 3 types based on their state transition trajectories. We automatically construct paired trajectory datasets of correct and incorrect dialogue flows and introduce a specialized objective loss for dialogue control. Our comprehensive evaluation demonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in information gathering, 91% in tool call rejection) with substantial improvements over baseline (44% and 9.6% respectively) while maintaining core functionality. Our approach opens new possibilities for developing TA-LLMs that can handle diverse real-world scenarios without requiring additional expert demonstrations or human labeling.",
            "score": 1,
            "issue_id": 3120,
            "pub_date": "2025-04-02",
            "pub_date_card": {
                "ru": "2 апреля",
                "en": "April 2",
                "zh": "4月2日"
            },
            "hash": "c42de57890092432",
            "authors": [
                "Sunghee Jung",
                "Donghun Lee",
                "Shinbok Lee",
                "Gaeun Seo",
                "Daniel Lee",
                "Byeongil Ko",
                "Junrae Cho",
                "Kihyun Kim",
                "Eunggyun Kim",
                "Myeongcheol Shin"
            ],
            "affiliations": [
                "Kakao Corp. Seongnam-si, Gyeonggi-do, South Korea"
            ],
            "pdf_title_img": "assets/pdf/title_img/2504.02882.jpg",
            "data": {
                "categories": [
                    "#rlhf",
                    "#optimization",
                    "#training",
                    "#alignment"
                ],
                "emoji": "🤖",
                "ru": {
                    "title": "DiaTool-DPO: Умный диалог без экспертных примеров",
                    "desc": "DiaTool-DPO - это новый метод, улучшающий диалоговые возможности инструментально-расширенных больших языковых моделей (TA-LLM) с помощью прямой оптимизации предпочтений. Авторы моделируют взаимодействие TA-LLM как марковский процесс принятия решений с 5 состояниями диалога и 3 типами запросов пользователей. Метод автоматически создает наборы данных правильных и неправильных траекторий диалога и вводит специальную функцию потерь для управления диалогом. Оценка показывает, что DiaTool-DPO приближается к производительности GPT-4, значительно превосходя базовый уровень, при сохранении основной функциональности."
                },
                "en": {
                    "title": "Enhancing Dialogue with DiaTool-DPO for TA-LLMs",
                    "desc": "This paper introduces DiaTool-DPO, a new method to improve Tool-Augmented Large Language Models (TA-LLMs) in handling incomplete and out-of-scope queries. It treats TA-LLM interactions as a Markov Decision Process, identifying five dialogue states and categorizing user queries into three types based on their transitions. The method involves creating paired datasets of correct and incorrect dialogue flows and using a specialized loss function for better dialogue control. The results show that DiaTool-DPO significantly enhances performance, approaching that of GPT-4o, while reducing the need for expert input and human labeling."
                },
                "zh": {
                    "title": "提升对话能力的新方法：DiaTool-DPO",
                    "desc": "本文提出了一种新方法DiaTool-DPO，旨在提升工具增强大型语言模型（TA-LLM）的对话能力。我们将TA-LLM的交互建模为马尔可夫决策过程，并根据对话状态的转移轨迹将用户查询分为三类。通过自动构建正确和错误对话流的配对轨迹数据集，并引入专门的目标损失函数，我们的评估显示DiaTool-DPO在信息获取和工具调用拒绝方面的表现接近GPT-4o。该方法为开发能够处理多样化现实场景的TA-LLM开辟了新可能，无需额外的专家演示或人工标注。"
                }
            }
        }
    ],
    "link_prev": "2025-04-07.html",
    "link_next": "2025-04-09.html",
    "link_month": "2025-04.html",
    "short_date_prev": {
        "ru": "07.04",
        "en": "04/07",
        "zh": "4月7日"
    },
    "short_date_next": {
        "ru": "09.04",
        "en": "04/09",
        "zh": "4月9日"
    },
    "categories": {
        "#dataset": 7,
        "#data": 2,
        "#benchmark": 8,
        "#agents": 0,
        "#cv": 6,
        "#rl": 0,
        "#rlhf": 1,
        "#rag": 0,
        "#plp": 0,
        "#inference": 5,
        "#3d": 0,
        "#audio": 0,
        "#video": 2,
        "#multimodal": 4,
        "#math": 2,
        "#multilingual": 0,
        "#architecture": 3,
        "#healthcare": 1,
        "#training": 5,
        "#robotics": 0,
        "#agi": 0,
        "#games": 1,
        "#interpretability": 1,
        "#reasoning": 3,
        "#transfer_learning": 0,
        "#graphs": 0,
        "#ethics": 1,
        "#security": 2,
        "#optimization": 5,
        "#survey": 1,
        "#diffusion": 2,
        "#alignment": 2,
        "#story_generation": 1,
        "#hallucinations": 0,
        "#long_context": 2,
        "#synthetic": 1,
        "#machine_translation": 0,
        "#leakage": 0,
        "#open_source": 1,
        "#small_models": 2,
        "#science": 1,
        "#low_resource": 1
    },
    "zh": {
        "text": "这篇文章介绍了一个新的多语言问题解决基准，称为Multi-SWE-bench。它涵盖了Java、TypeScript、JavaScript、Go、Rust、C和C++等语言。该基准包含1,632个高质量实例，由68位专家注释。基于这个基准，作者评估了一系列先进的模型，并提供了详细的分析。此外，文章还宣布成立了一个开源社区Multi-SWE-RL，旨在为问题解决任务构建大规模强化学习训练数据集。",
        "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving",
        "pinyin": "这篇文章介绍了一个新的多语言问题解决基准，称为Multi-SWE-bench。它涵盖了Java、TypeScript、JavaScript、Go、Rust、C和C++等语言。该基准包含1,632个高质量实例，由68位专家注释。基于这个基准，作者评估了一系列先进的模型，并提供了详细的分析。此外，文章还宣布成立了一个开源社区Multi-SWE-RL，旨在为问题解决任务构建大规模强化学习训练数据集。\n\nzhè piān wén zhāng jiè shào le yī gè xīn de duō yǔ yán wèn tí jiě jué jī zhǔn, chēng wéi Multi-SWE-bench. tā hán gǎi le Java, TypeScript, JavaScript, Go, Rust, C hé C++ děng yǔ yán. gǎi jī zhǔn bāo hán 1,632 gè gāo zhì liàng shí lì, yǒu 68 wèi zhuān jiā zhù shì. jī yú zhè gè jī zhǔn, zuò zhě píng gū le yī xì liè xiān jìn de mó xíng, bìng tí gōng le xiáng xì de fēn xī. cǐ wài, wén zhāng hái xuān bù chéng lì le yī gè kāi yuán shè qū Multi-SWE-RL, zhǐ yú wèi wèn tí jiě jué rèn wù gòu jiàn dà guī mó qiáng huà xué xùn liàn shù jù jí.",
        "vocab": "[\n    {\"word\": \"涵盖\", \"pinyin\": \"hángài\", \"trans\": \"cover\"},\n    {\"word\": \"基准\", \"pinyin\": \"jīzhǔn\", \"trans\": \"benchmark\"},\n    {\"word\": \"注释\", \"pinyin\": \"zhùshì\", \"trans\": \"annotate\"},\n    {\"word\": \"评估\", \"pinyin\": \"pínggū\", \"trans\": \"evaluate\"},\n    {\"word\": \"先进\", \"pinyin\": \"xiānjìn\", \"trans\": \"advanced\"},\n    {\"word\": \"详细\", \"pinyin\": \"xiángxì\", \"trans\": \"detailed\"},\n    {\"word\": \"分析\", \"pinyin\": \"fēnxī\", \"trans\": \"analysis\"},\n    {\"word\": \"宣布\", \"pinyin\": \"xuānbù\", \"trans\": \"announce\"},\n    {\"word\": \"成立\", \"pinyin\": \"chénglì\", \"trans\": \"establish\"},\n    {\"word\": \"开源\", \"pinyin\": \"kāiyuán\", \"trans\": \"open-source\"},\n    {\"word\": \"社区\", \"pinyin\": \"shèqū\", \"trans\": \"community\"},\n    {\"word\": \"旨在\", \"pinyin\": \"zhǐzài\", \"trans\": \"aim to\"},\n    {\"word\": \"构建\", \"pinyin\": \"gòujiàn\", \"trans\": \"build\"},\n    {\"word\": \"任务\", \"pinyin\": \"rènwù\", \"trans\": \"task\"},\n    {\"word\": \"强化学习\", \"pinyin\": \"qiáng huà xuéxí\", \"trans\": \"reinforcement learning\"},\n    {\"word\": \"训练\", \"pinyin\": \"xùnliàn\", \"trans\": \"training\"},\n    {\"word\": \"数据集\", \"pinyin\": \"shùjùjí\", \"trans\": \"dataset\"}\n]",
        "trans": "This article introduces a new multilingual problem-solving benchmark called Multi-SWE-bench. It covers languages such as Java, TypeScript, JavaScript, Go, Rust, C, and C++. The benchmark contains 1,632 high-quality instances annotated by 68 experts. Based on this benchmark, the authors evaluated a series of advanced models and provided detailed analyses. Additionally, the article announces the establishment of an open-source community, Multi-SWE-RL, aimed at building large-scale reinforcement learning training datasets for problem-solving tasks.",
        "update_ts": "2025-04-07 09:12"
    }
}