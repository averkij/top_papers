
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 14 papers. October 2.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }        
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: #e73838;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">2 октября</span> | <span id="title-articles-count">14 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-10-01.html">⬅️ <span id="prev-date">01.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-10-03.html">➡️ <span id="next-date">03.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-10.html">📈 <span id='top-month-label'>Топ за месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'};
        let feedDateNext = {'ru': '03.10', 'en': '10/03', 'zh': '10月3日'};
        let feedDatePrev = {'ru': '01.10', 'en': '10/01', 'zh': '10月1日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Топ за месяц', 'en': 'Top by Month', 'zh': '月度热门论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2409.19951', 'title': 'Law of the Weakest Link: Cross Capabilities of Large Language Models', 'url': 'https://huggingface.co/papers/2409.19951', 'abstract': 'The development and evaluation of Large Language Models (LLMs) have largely focused on individual capabilities. However, this overlooks the intersection of multiple abilities across different types of expertise that are often required for real-world tasks, which we term cross capabilities. To systematically explore this concept, we first define seven core individual capabilities and then pair them to form seven common cross capabilities, each supported by a manually constructed taxonomy. Building on these definitions, we introduce CrossEval, a benchmark comprising 1,400 human-annotated prompts, with 100 prompts for each individual and cross capability. To ensure reliable evaluation, we involve expert annotators to assess 4,200 model responses, gathering 8,400 human ratings with detailed explanations to serve as reference examples. Our findings reveal that, in both static evaluations and attempts to enhance specific abilities, current LLMs consistently exhibit the "Law of the Weakest Link," where cross-capability performance is significantly constrained by the weakest component. Specifically, across 58 cross-capability scores from 17 models, 38 scores are lower than all individual capabilities, while 20 fall between strong and weak, but closer to the weaker ability. These results highlight the under-performance of LLMs in cross-capability tasks, making the identification and improvement of the weakest capabilities a critical priority for future research to optimize performance in complex, multi-dimensional scenarios.', 'score': 53, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': '8ea79f1e9cb3662c', 'data': {'categories': ['#benchmark', '#multimodal', '#interpretability'], 'emoji': '🔗', 'ru': {'title': 'Кросс-возможности LLM: слабое звено определяет успех', 'desc': "Статья представляет новый подход к оценке больших языковых моделей (LLM), фокусируясь на пересечении нескольких способностей, названном 'кросс-возможностями'. Авторы разработали бенчмарк CrossEval, включающий 1400 аннотированных промптов для оценки семи индивидуальных и семи кросс-возможностей. Исследование выявило 'закон слабейшего звена', согласно которому производительность в задачах с кросс-возможностями ограничена наиболее слабой компонентой. Результаты подчеркивают необходимость улучшения слабейших возможностей LLM для оптимизации их работы в сложных многомерных сценариях."}, 'en': {'title': 'Unlocking the Power of Cross Capabilities in LLMs', 'desc': 'This paper discusses the limitations of Large Language Models (LLMs) when performing tasks that require multiple skills, which the authors call cross capabilities. They define seven individual capabilities and create pairs to form seven cross capabilities, supported by a detailed taxonomy. The authors introduce CrossEval, a benchmark with 1,400 prompts to evaluate these capabilities, using expert annotators to assess model responses. Their findings indicate that LLMs often perform poorly in cross-capability tasks, revealing that the weakest skill significantly limits overall performance, suggesting a need for future research to strengthen these weak areas.'}, 'zh': {'title': '提升交叉能力，优化模型表现', 'desc': '这篇论文探讨了大型语言模型（LLMs）在多种能力交叉应用中的表现。研究者定义了七种核心能力，并将其配对形成七种常见的交叉能力。通过构建CrossEval基准，评估了1400个人工标注的提示，发现当前的LLMs在交叉能力任务中表现不佳，常常受到最弱能力的限制。研究结果强调了识别和提升最弱能力的重要性，以优化模型在复杂任务中的表现。'}}}, {'id': 'https://huggingface.co/papers/2410.00531', 'title': 'TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices', 'url': 'https://huggingface.co/papers/2410.00531', 'abstract': "Large model inference is shifting from cloud to edge due to concerns about the privacy of user interaction data. However, edge devices often struggle with limited computing power, memory, and bandwidth, requiring collaboration across multiple devices to run and speed up LLM inference. Pipeline parallelism, the mainstream solution, is inefficient for single-user scenarios, while tensor parallelism struggles with frequent communications. In this paper, we argue that tensor parallelism can be more effective than pipeline on low-resource devices, and present a compute- and memory-efficient tensor parallel inference system, named TPI-LLM, to serve 70B-scale models. TPI-LLM keeps sensitive raw data local in the users' devices and introduces a sliding window memory scheduler to dynamically manage layer weights during inference, with disk I/O latency overlapped with the computation and communication. This allows larger models to run smoothly on memory-limited devices. We analyze the communication bottleneck and find that link latency, not bandwidth, emerges as the main issue, so a star-based allreduce algorithm is implemented. Through extensive experiments on both emulated and real testbeds, TPI-LLM demonstrated over 80% less time-to-first-token and token latency compared to Accelerate, and over 90% compared to Transformers and Galaxy, while cutting the peak memory footprint of Llama 2-70B by 90%, requiring only 3.1 GB of memory for 70B-scale models.", 'score': 28, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': '993cc3dc03d1791f', 'data': {'categories': ['#inference', '#edge_computing', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Большие языковые модели на малых устройствах: эффективность и конфиденциальность', 'desc': 'В статье представлена система TPI-LLM для эффективного выполнения крупных языковых моделей на устройствах с ограниченными ресурсами. Она использует тензорный параллелизм и динамическое управление памятью для запуска 70-миллиардных моделей на edge-устройствах. TPI-LLM значительно снижает задержку и потребление памяти по сравнению с существующими решениями. Система обеспечивает конфиденциальность данных пользователя, сохраняя их локально на устройстве.'}, 'en': {'title': 'Efficient Tensor Parallelism for Edge Inference', 'desc': 'This paper discusses the shift of large model inference from cloud to edge devices due to privacy concerns. It highlights the challenges faced by edge devices, such as limited computing power and memory, and critiques existing methods like pipeline and tensor parallelism. The authors propose TPI-LLM, a tensor parallel inference system that efficiently manages memory and computation, allowing large models to run on low-resource devices. Their experiments show that TPI-LLM significantly reduces latency and memory usage compared to other systems, making it a viable solution for deploying large language models on edge devices.'}, 'zh': {'title': '边缘设备上的高效大模型推理', 'desc': '本论文讨论了大模型推理从云端转向边缘设备的原因，主要是为了保护用户隐私。由于边缘设备的计算能力、内存和带宽有限，多个设备之间的协作变得必要。我们提出了一种名为TPI-LLM的张量并行推理系统，能够在资源有限的设备上高效运行70B规模的模型。通过动态管理层权重和优化通信方式，TPI-LLM显著减少了推理时间和内存占用，提升了大模型在边缘设备上的运行效率。'}}}, {'id': 'https://huggingface.co/papers/2409.17912', 'title': 'Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect', 'url': 'https://huggingface.co/papers/2409.17912', 'abstract': 'We introduce Atlas-Chat, the first-ever collection of large language models specifically developed for dialectal Arabic. Focusing on Moroccan Arabic, also known as Darija, we construct our instruction dataset by consolidating existing Darija language resources, creating novel datasets both manually and synthetically, and translating English instructions with stringent quality control. Atlas-Chat-9B and 2B models, fine-tuned on the dataset, exhibit superior ability in following Darija instructions and performing standard NLP tasks. Notably, our models outperform both state-of-the-art and Arabic-specialized LLMs like LLaMa, Jais, and AceGPT, e.g., achieving a 13% performance boost over a larger 13B model on DarijaMMLU, in our newly introduced evaluation suite for Darija covering both discriminative and generative tasks. Furthermore, we perform an experimental analysis of various fine-tuning strategies and base model choices to determine optimal configurations. All our resources are publicly accessible, and we believe our work offers comprehensive design methodologies of instruction-tuning for low-resource language variants, which are often neglected in favor of data-rich languages by contemporary LLMs.', 'score': 20, 'issue_id': 1, 'pub_date': '2024-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '99e46689e9b93364', 'data': {'categories': ['#dataset', '#data', '#benchmark', '#multilingual', '#training'], 'emoji': '🗣️', 'ru': {'title': 'Прорыв в обработке диалектного арабского языка с помощью специализированных языковых моделей', 'desc': 'Исследователи представили Atlas-Chat - первую коллекцию больших языковых моделей, специально разработанных для диалектного арабского языка, в частности для марокканского диалекта Дарижа. Они создали набор данных для обучения, объединив существующие ресурсы Дарижа, создав новые наборы данных вручную и синтетически, а также переведя английские инструкции с строгим контролем качества. Модели Atlas-Chat-9B и 2B, дообученные на этом наборе данных, демонстрируют превосходную способность следовать инструкциям на Дарижа и выполнять стандартные задачи обработки естественного языка. Исследователи также провели экспериментальный анализ различных стратегий дообучения и выбора базовых моделей для определения оптимальных конфигураций.'}, 'en': {'title': 'Empowering Dialectal Arabic with Atlas-Chat!', 'desc': 'Atlas-Chat is a groundbreaking collection of large language models tailored for dialectal Arabic, specifically Moroccan Arabic or Darija. The models, Atlas-Chat-9B and 2B, were fine-tuned using a carefully constructed dataset that includes both existing resources and newly created data, ensuring high-quality instruction following. These models demonstrate exceptional performance in natural language processing tasks, surpassing other advanced models like LLaMa and Jais by a significant margin. The research also explores various fine-tuning strategies, providing valuable insights for developing models for low-resource languages, which are often overlooked in the field of machine learning.'}, 'zh': {'title': 'Atlas-Chat：为摩洛哥阿拉伯语量身定制的语言模型', 'desc': '我们介绍了Atlas-Chat，这是首个专门为方言阿拉伯语开发的大型语言模型集合。我们重点关注摩洛哥阿拉伯语（Darija），通过整合现有的Darija语言资源，手动和合成创建新数据集，并严格控制翻译质量来构建指令数据集。经过微调的Atlas-Chat-9B和2B模型在遵循Darija指令和执行标准自然语言处理任务方面表现出色，超越了现有的阿拉伯语专用大型语言模型。我们的研究为低资源语言变体的指令调优提供了全面的设计方法论，填补了当代大型语言模型在这方面的空白。'}}}, {'id': 'https://huggingface.co/papers/2409.19603', 'title': 'One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos', 'url': 'https://huggingface.co/papers/2409.19603', 'abstract': "We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designed <TRK> token, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.", 'score': 17, 'issue_id': 1, 'pub_date': '2024-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '911a506d3af5683e', 'data': {'categories': ['#video', '#multimodal', '#benchmark', '#agents'], 'emoji': '🎥', 'ru': {'title': 'VideoLISA: Разумная сегментация видео по языковым инструкциям', 'desc': 'VideoLISA - это мультимодальная большая языковая модель для видео, предназначенная для сегментации объектов на основе языковых инструкций. Она объединяет возможности рассуждения больших языковых моделей с моделью Segment Anything для генерации согласованных во времени масок сегментации. VideoLISA использует стратегию разреженно-плотной выборки для баланса между временным контекстом и пространственными деталями. Модель также применяет подход One-Token-Seg-All со специальным токеном <TRK> для сегментации и отслеживания объектов в нескольких кадрах.'}, 'en': {'title': 'VideoLISA: Revolutionizing Video Segmentation with Language Instructions', 'desc': 'VideoLISA is a multimodal large language model specifically designed for video segmentation tasks that require reasoning based on language instructions. It combines the reasoning abilities of large language models with the Segment Anything Model to create consistent segmentation masks over time. Unlike traditional image-based methods, VideoLISA effectively handles the complexities of video data by using a Sparse Dense Sampling strategy, which optimizes both temporal and spatial information. The model also introduces a One-Token-Seg-All approach, allowing it to track and segment objects across multiple frames, demonstrating its effectiveness in complex video object segmentation tasks.'}, 'zh': {'title': '视频分割的新突破：VideoLISA', 'desc': 'VideoLISA是一种基于视频的多模态大语言模型，旨在解决视频中的语言指令推理分割问题。它结合了大语言模型的推理能力和世界知识，并通过Segment Anything Model增强，生成基于语言指令的时间一致性分割掩码。与现有的基于图像的方法相比，VideoLISA通过引入稀疏密集采样策略，平衡了时间上下文和空间细节，从而有效应对视频任务中的时间动态理解。该模型还提出了一种One-Token-Seg-All方法，利用特定设计的<TRK>标记，实现跨多个帧的对象分割和跟踪。'}}}, {'id': 'https://huggingface.co/papers/2410.00890', 'title': 'Flex3D: Feed-Forward 3D Generation With Flexible Reconstruction Model And Input View Curation', 'url': 'https://huggingface.co/papers/2410.00890', 'abstract': 'Generating high-quality 3D content from text, single images, or sparse view images remains a challenging task with broad applications.Existing methods typically employ multi-view diffusion models to synthesize multi-view images, followed by a feed-forward process for 3D reconstruction. However, these approaches are often constrained by a small and fixed number of input views, limiting their ability to capture diverse viewpoints and, even worse, leading to suboptimal generation results if the synthesized views are of poor quality. To address these limitations, we propose Flex3D, a novel two-stage framework capable of leveraging an arbitrary number of high-quality input views. The first stage consists of a candidate view generation and curation pipeline. We employ a fine-tuned multi-view image diffusion model and a video diffusion model to generate a pool of candidate views, enabling a rich representation of the target 3D object. Subsequently, a view selection pipeline filters these views based on quality and consistency, ensuring that only the high-quality and reliable views are used for reconstruction. In the second stage, the curated views are fed into a Flexible Reconstruction Model (FlexRM), built upon a transformer architecture that can effectively process an arbitrary number of inputs. FlemRM directly outputs 3D Gaussian points leveraging a tri-plane representation, enabling efficient and detailed 3D generation. Through extensive exploration of design and training strategies, we optimize FlexRM to achieve superior performance in both reconstruction and generation tasks. Our results demonstrate that Flex3D achieves state-of-the-art performance, with a user study winning rate of over 92% in 3D generation tasks when compared to several of the latest feed-forward 3D generative models.', 'score': 17, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': '8eb332defa865614', 'data': {'categories': ['#3d', '#diffusion', '#architecture', '#training'], 'emoji': '🎨', 'ru': {'title': 'Гибкая генерация 3D-контента с произвольным числом ракурсов', 'desc': 'Flex3D - это новая двухэтапная система для создания 3D-контента из текста или изображений. На первом этапе генерируется и отбирается набор высококачественных видов объекта с помощью диффузионных моделей. На втором этапе отобранные виды обрабатываются трансформерной моделью FlexRM, которая напрямую генерирует 3D-представление в виде гауссовых точек. Система может работать с произвольным количеством входных видов, что позволяет получать более детальные и качественные 3D-модели.'}, 'en': {'title': 'Flex3D: Revolutionizing 3D Content Generation with Flexible View Inputs', 'desc': 'The paper introduces Flex3D, a two-stage framework designed to generate high-quality 3D content from various input sources like text and images. It overcomes limitations of existing methods by allowing an arbitrary number of high-quality input views, enhancing the diversity and quality of the generated 3D representations. The first stage involves generating and curating candidate views using advanced diffusion models, while the second stage employs a Flexible Reconstruction Model (FlexRM) based on transformer architecture for efficient 3D reconstruction. Flex3D demonstrates superior performance in 3D generation tasks, achieving a winning rate of over 92% in user studies compared to other models.'}, 'zh': {'title': 'Flex3D：灵活高效的3D内容生成', 'desc': '本文提出了一种名为Flex3D的新框架，用于从文本、单幅图像或稀疏视图图像生成高质量的3D内容。该框架分为两个阶段：第一阶段生成候选视图并进行筛选，确保使用高质量和一致性的视图进行重建。第二阶段使用基于变换器架构的灵活重建模型（FlexRM），能够处理任意数量的输入，并直接输出3D高斯点。实验结果表明，Flex3D在3D生成任务中表现优异，用户研究的胜率超过92%。'}}}, {'id': 'https://huggingface.co/papers/2409.19946', 'title': 'Illustrious: an Open Advanced Illustration Model', 'url': 'https://huggingface.co/papers/2409.19946', 'abstract': 'In this work, we share the insights for achieving state-of-the-art quality in our text-to-image anime image generative model, called Illustrious. To achieve high resolution, dynamic color range images, and high restoration ability, we focus on three critical approaches for model improvement. First, we delve into the significance of the batch size and dropout control, which enables faster learning of controllable token based concept activations. Second, we increase the training resolution of images, affecting the accurate depiction of character anatomy in much higher resolution, extending its generation capability over 20MP with proper methods. Finally, we propose the refined multi-level captions, covering all tags and various natural language captions as a critical factor for model development. Through extensive analysis and experiments, Illustrious demonstrates state-of-the-art performance in terms of animation style, outperforming widely-used models in illustration domains, propelling easier customization and personalization with nature of open source. We plan to publicly release updated Illustrious model series sequentially as well as sustainable plans for improvements.', 'score': 13, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': 'ac67234852b7f9e1', 'data': {'categories': ['#cv', '#multimodal', '#training', '#synthetic'], 'emoji': '🎨', 'ru': {'title': 'Illustrious: прорыв в генерации аниме-изображений с помощью ИИ', 'desc': 'Статья описывает подходы к улучшению качества генеративной модели текст-в-изображение для аниме-иллюстраций под названием Illustrious. Авторы фокусируются на трех ключевых аспектах: оптимизации размера батча и контроле дропаута, увеличении разрешения обучающих изображений и использовании многоуровневых подписей. Модель демонстрирует передовые результаты в генерации аниме-стиля, превосходя широко используемые модели в области иллюстраций. Illustrious планируется выпустить в открытый доступ с дальнейшими улучшениями.'}, 'en': {'title': 'Illustrious: Elevating Anime Image Generation to New Heights!', 'desc': 'This paper presents the Illustrious model, a text-to-image generative model specifically designed for creating high-quality anime images. The authors emphasize three key strategies for enhancing model performance: optimizing batch size and dropout for improved learning, increasing training image resolution for better character anatomy representation, and utilizing refined multi-level captions for comprehensive training data. The results show that Illustrious achieves superior performance in animation style compared to existing models, allowing for greater customization and personalization. The authors also plan to release updates and improvements to the model in an open-source format.'}, 'zh': {'title': 'Illustrious：动漫生成模型的新高度', 'desc': '本文介绍了一种名为Illustrious的文本到图像动漫生成模型，旨在实现最先进的图像质量。我们通过优化批量大小和丢弃率控制，加快了可控标记概念激活的学习速度。其次，通过提高图像训练分辨率，模型能够更准确地描绘角色解剖结构，生成超过20MP的高分辨率图像。最后，我们提出了精细的多层次标题，涵盖所有标签和各种自然语言标题，作为模型发展的关键因素。'}}}, {'id': 'https://huggingface.co/papers/2410.00337', 'title': 'SyntheOcc: Synthesize Geometric-Controlled Street View Images through 3D Semantic MPIs', 'url': 'https://huggingface.co/papers/2410.00337', 'abstract': 'The advancement of autonomous driving is increasingly reliant on high-quality annotated datasets, especially in the task of 3D occupancy prediction, where the occupancy labels require dense 3D annotation with significant human effort. In this paper, we propose SyntheOcc, which denotes a diffusion model that Synthesize photorealistic and geometric-controlled images by conditioning Occupancy labels in driving scenarios. This yields an unlimited amount of diverse, annotated, and controllable datasets for applications like training perception models and simulation. SyntheOcc addresses the critical challenge of how to efficiently encode 3D geometric information as conditional input to a 2D diffusion model. Our approach innovatively incorporates 3D semantic multi-plane images (MPIs) to provide comprehensive and spatially aligned 3D scene descriptions for conditioning. As a result, SyntheOcc can generate photorealistic multi-view images and videos that faithfully align with the given geometric labels (semantics in 3D voxel space). Extensive qualitative and quantitative evaluations of SyntheOcc on the nuScenes dataset prove its effectiveness in generating controllable occupancy datasets that serve as an effective data augmentation to perception models.', 'score': 10, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': 'a20c1db238f8e7e2', 'data': {'categories': ['#dataset', '#3d', '#synthetic'], 'emoji': '🚗', 'ru': {'title': 'Синтез реалистичных изображений для беспилотных автомобилей с помощью 3D меток', 'desc': 'SyntheOcc - это модель диффузии, которая синтезирует фотореалистичные изображения для автономного вождения, используя метки занятости в 3D пространстве. Модель решает проблему эффективного кодирования 3D геометрической информации в качестве условного входа для 2D модели диффузии. SyntheOcc использует семантические многоплоскостные изображения (MPI) для предоставления полного описания 3D сцены. Эксперименты на наборе данных nuScenes показывают эффективность SyntheOcc в генерации контролируемых наборов данных для улучшения моделей восприятия.'}, 'en': {'title': 'SyntheOcc: Revolutionizing 3D Occupancy Data Generation for Autonomous Driving', 'desc': 'This paper introduces SyntheOcc, a diffusion model designed to create photorealistic images conditioned on occupancy labels for autonomous driving scenarios. It addresses the challenge of encoding 3D geometric information into a 2D model by using 3D semantic multi-plane images (MPIs) for better spatial alignment. The generated images and videos are not only photorealistic but also accurately reflect the underlying 3D occupancy data. Evaluations on the nuScenes dataset demonstrate that SyntheOcc effectively produces diverse and controllable datasets, enhancing data augmentation for perception models.'}, 'zh': {'title': 'SyntheOcc：生成可控的3D占用数据集', 'desc': '本论文提出了一种名为SyntheOcc的扩散模型，用于合成高质量的3D占用预测数据集。该模型通过条件化占用标签，生成逼真的图像和几何控制图像，从而提供无限多样的标注数据集。SyntheOcc创新性地结合了3D语义多平面图像（MPI），为2D扩散模型提供了全面且空间对齐的3D场景描述。通过在nuScenes数据集上的广泛评估，证明了SyntheOcc在生成可控占用数据集方面的有效性，能够有效增强感知模型的训练。'}}}, {'id': 'https://huggingface.co/papers/2410.00086', 'title': 'ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer', 'url': 'https://huggingface.co/papers/2410.00086', 'abstract': 'Diffusion models have emerged as a powerful generative technology and have been found to be applicable in various scenarios. Most existing foundational diffusion models are primarily designed for text-guided visual generation and do not support multi-modal conditions, which are essential for many visual editing tasks. This limitation prevents these foundational diffusion models from serving as a unified model in the field of visual generation, like GPT-4 in the natural language processing field. In this work, we propose ACE, an All-round Creator and Editor, which achieves comparable performance compared to those expert models in a wide range of visual generation tasks. To achieve this goal, we first introduce a unified condition format termed Long-context Condition Unit (LCU), and propose a novel Transformer-based diffusion model that uses LCU as input, aiming for joint training across various generation and editing tasks. Furthermore, we propose an efficient data collection approach to address the issue of the absence of available training data. It involves acquiring pairwise images with synthesis-based or clustering-based pipelines and supplying these pairs with accurate textual instructions by leveraging a fine-tuned multi-modal large language model. To comprehensively evaluate the performance of our model, we establish a benchmark of manually annotated pairs data across a variety of visual generation tasks. The extensive experimental results demonstrate the superiority of our model in visual generation fields. Thanks to the all-in-one capabilities of our model, we can easily build a multi-modal chat system that responds to any interactive request for image creation using a single model to serve as the backend, avoiding the cumbersome pipeline typically employed in visual agents. Code and models will be available on the project page: https://ali-vilab.github.io/ace-page/.', 'score': 10, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': '21af441f6b636cb3', 'data': {'categories': ['#diffusion', '#multimodal', '#cv', '#benchmark', '#dataset'], 'emoji': '🎨', 'ru': {'title': 'ACE: универсальный инструмент для создания и редактирования изображений', 'desc': 'Исследователи представили ACE - универсальную модель для генерации и редактирования изображений. Модель использует унифицированный формат условий Long-context Condition Unit (LCU) и основана на архитектуре Transformer. Для обучения был разработан эффективный подход сбора данных с использованием мультимодальной языковой модели. ACE показывает высокую производительность в широком спектре задач визуальной генерации, что позволяет создать мультимодальную чат-систему на основе единой модели.'}, 'en': {'title': 'ACE: Unifying Visual Generation and Editing with Multi-Modal Diffusion Models', 'desc': "This paper introduces ACE, a novel diffusion model designed for multi-modal visual generation and editing tasks. Unlike existing models that focus mainly on text-guided image generation, ACE utilizes a Long-context Condition Unit (LCU) to unify various input conditions for improved performance. The authors also present an innovative data collection method to generate training pairs of images and textual instructions, enhancing the model's training process. Experimental results show that ACE outperforms existing models, making it a versatile tool for creating a multi-modal chat system for image generation."}, 'zh': {'title': 'ACE：全能的视觉生成与编辑模型', 'desc': '扩散模型作为一种强大的生成技术，已在多个场景中得到应用。现有的基础扩散模型主要用于文本引导的视觉生成，缺乏对多模态条件的支持，这限制了其在视觉编辑任务中的应用。为了解决这个问题，我们提出了ACE，一个全能的创作与编辑模型，能够在多种视觉生成任务中与专家模型相媲美。我们引入了统一的条件格式Long-context Condition Unit（LCU），并提出了一种基于Transformer的扩散模型，旨在实现多任务的联合训练。'}}}, {'id': 'https://huggingface.co/papers/2410.00418', 'title': 'Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration', 'url': 'https://huggingface.co/papers/2410.00418', 'abstract': 'Photo-realistic image restoration algorithms are typically evaluated by distortion measures (e.g., PSNR, SSIM) and by perceptual quality measures (e.g., FID, NIQE), where the desire is to attain the lowest possible distortion without compromising on perceptual quality. To achieve this goal, current methods typically attempt to sample from the posterior distribution, or to optimize a weighted sum of a distortion loss (e.g., MSE) and a perceptual quality loss (e.g., GAN). Unlike previous works, this paper is concerned specifically with the optimal estimator that minimizes the MSE under a constraint of perfect perceptual index, namely where the distribution of the reconstructed images is equal to that of the ground-truth ones. A recent theoretical result shows that such an estimator can be constructed by optimally transporting the posterior mean prediction (MMSE estimate) to the distribution of the ground-truth images. Inspired by this result, we introduce Posterior-Mean Rectified Flow (PMRF), a simple yet highly effective algorithm that approximates this optimal estimator. In particular, PMRF first predicts the posterior mean, and then transports the result to a high-quality image using a rectified flow model that approximates the desired optimal transport map. We investigate the theoretical utility of PMRF and demonstrate that it consistently outperforms previous methods on a variety of image restoration tasks.', 'score': 9, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': 'cd355be6fba7c501', 'data': {'categories': ['#cv', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Оптимальное восстановление изображений с помощью выпрямленных потоков', 'desc': 'Эта статья представляет новый алгоритм для восстановления изображений под названием Posterior-Mean Rectified Flow (PMRF). PMRF сначала предсказывает среднее значение апостериорного распределения, а затем использует модель выпрямленного потока для переноса результата в высококачественное изображение. Этот метод основан на теоретическом результате, показывающем, что оптимальная оценка может быть получена путем оптимального транспорта среднего значения апостериорного распределения в распределение оригинальных изображений. PMRF превосходит существующие методы в различных задачах восстановления изображений.'}, 'en': {'title': 'Achieving Perfect Perception in Image Restoration with PMRF', 'desc': 'This paper presents a new approach to photo-realistic image restoration that focuses on minimizing mean squared error (MSE) while ensuring perceptual quality. The authors introduce a method called Posterior-Mean Rectified Flow (PMRF), which first predicts the posterior mean of the image and then optimally transports this prediction to match the distribution of high-quality images. This approach is based on a theoretical framework that allows for the construction of an optimal estimator under the constraint of perfect perceptual quality. The results show that PMRF outperforms existing algorithms across various image restoration tasks, demonstrating its effectiveness in achieving both low distortion and high perceptual quality.'}, 'zh': {'title': '最优图像恢复：后验均值校正流', 'desc': '本文提出了一种新的图像恢复算法，称为后验均值校正流（PMRF），旨在在保持感知质量的同时最小化均方误差（MSE）。与传统方法不同，PMRF通过优化后验均值预测，并将其传输到真实图像的分布上，从而实现最佳估计。研究表明，PMRF在多种图像恢复任务中表现优于现有方法，具有较高的有效性。该算法的理论基础是通过最优传输理论构建的，确保恢复图像的分布与真实图像一致。'}}}, {'id': 'https://huggingface.co/papers/2409.20018', 'title': 'Visual Context Window Extension: A New Perspective for Long Video Understanding', 'url': 'https://huggingface.co/papers/2409.20018', 'abstract': 'Large Multimodal Models (LMMs) have demonstrated impressive performance in short video understanding tasks but face great challenges when applied to long video understanding. In contrast, Large Language Models (LLMs) exhibit outstanding capabilities in modeling long texts. Existing work attempts to address this issue by introducing long video-text pairs during training. However, these approaches require substantial computational and data resources. In this paper, we tackle the challenge of long video understanding from the perspective of context windows, aiming to apply LMMs to long video tasks without retraining on long video datasets. We first conduct an in-depth analysis of why pretrained LMMs struggle to understand lengthy video content, identifying that discrepancies between visual and language modalities lead to different context windows for visual and language tokens, making it difficult to directly extend the visual tokens to match the language context window. Based on this, we propose to adapt LMMs for long video understanding tasks by extending the visual context window, eliminating the need for retraining on large scalelong video datasets. To further mitigate the significant memory consumption caused by long sequences, we introduce a progressive pooling inference strategy that selectively adjusts the spatial resolution of frame embeddings, reducing the number of visual tokens while retaining important spatial information. Across multiple long video understanding benchmarks, our method consistently improves the performance as the number of video frames increases. On the MLVU benchmark, our method outperforms GPT-4o, even though our model size is only 7B. Additionally, in the 256-frame setting, our method reduces memory usage by approximately 45% compared to the baseline, without introducing any performance loss.', 'score': 8, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': '81b41650dd87a792', 'data': {'categories': ['#multimodal', '#video', '#inference', '#training'], 'emoji': '🎥', 'ru': {'title': 'Эффективное понимание длинных видео без переобучения', 'desc': 'Статья представляет новый подход к пониманию длинных видео с использованием мультимодальных моделей (LMM). Авторы предлагают расширить контекстное окно для визуальных токенов, чтобы улучшить обработку длинных видео без переобучения на больших наборах данных. Они также вводят стратегию прогрессивного пулинга для снижения потребления памяти при сохранении важной пространственной информации. Метод показывает улучшение производительности на нескольких бенчмарках для длинных видео, превосходя GPT-4o на MLVU, несмотря на меньший размер модели.'}, 'en': {'title': 'Enhancing Long Video Understanding with Efficient Context Adaptation', 'desc': 'This paper addresses the challenges faced by Large Multimodal Models (LMMs) in understanding long videos, which differ from their success in short video tasks. The authors analyze the issues arising from mismatched context windows between visual and language tokens, which hinder effective processing of lengthy video content. They propose a solution that extends the visual context window without the need for retraining on extensive long video datasets. Additionally, a progressive pooling inference strategy is introduced to reduce memory consumption while maintaining essential spatial information, leading to improved performance on long video understanding benchmarks.'}, 'zh': {'title': '提升长视频理解的多模态模型新策略', 'desc': '本文探讨了如何提高大型多模态模型（LMMs）在长视频理解任务中的表现。我们发现，视觉和语言模态之间的差异导致了不同的上下文窗口，使得直接扩展视觉标记以匹配语言上下文窗口变得困难。为了解决这个问题，我们提出了一种扩展视觉上下文窗口的方法，避免了对大型长视频数据集的重新训练。此外，我们还引入了一种渐进池化推理策略，以减少长序列带来的内存消耗，同时保留重要的空间信息。'}}}, {'id': 'https://huggingface.co/papers/2409.20563', 'title': 'DressRecon: Freeform 4D Human Reconstruction from Monocular Video', 'url': 'https://huggingface.co/papers/2409.20563', 'abstract': 'We present a method to reconstruct time-consistent human body models from monocular videos, focusing on extremely loose clothing or handheld object interactions. Prior work in human reconstruction is either limited to tight clothing with no object interactions, or requires calibrated multi-view captures or personalized template scans which are costly to collect at scale. Our key insight for high-quality yet flexible reconstruction is the careful combination of generic human priors about articulated body shape (learned from large-scale training data) with video-specific articulated "bag-of-bones" deformation (fit to a single video via test-time optimization). We accomplish this by learning a neural implicit model that disentangles body versus clothing deformations as separate motion model layers. To capture subtle geometry of clothing, we leverage image-based priors such as human body pose, surface normals, and optical flow during optimization. The resulting neural fields can be extracted into time-consistent meshes, or further optimized as explicit 3D Gaussians for high-fidelity interactive rendering. On datasets with highly challenging clothing deformations and object interactions, DressRecon yields higher-fidelity 3D reconstructions than prior art. Project page: https://jefftan969.github.io/dressrecon/', 'score': 7, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': 'aa9e4dffe4e6ad09', 'data': {'categories': ['#3d', '#cv'], 'emoji': '👕', 'ru': {'title': 'Реконструкция 3D-моделей людей в свободной одежде из видео', 'desc': "Представлен метод реконструкции согласованных во времени моделей человеческого тела из монокулярных видео, фокусирующийся на очень свободной одежде и взаимодействии с предметами. Ключевая идея заключается в комбинации общих человеческих приоров о форме тела с видеоспецифичной деформацией 'мешка костей'. Метод использует нейронную имплицитную модель, разделяющую деформации тела и одежды как отдельные слои модели движения. Для захвата тонкой геометрии одежды используются изображения-приоры, такие как поза тела, нормали поверхности и оптический поток."}, 'en': {'title': 'Revolutionizing Human Body Reconstruction from Single Videos', 'desc': 'This paper introduces a novel method for reconstructing human body models from single videos, particularly when subjects wear loose clothing or interact with objects. Unlike previous approaches that require multiple camera views or specific templates, this method combines learned human shape priors with video-specific deformations through test-time optimization. The authors utilize a neural implicit model that separates body and clothing movements, enhancing the accuracy of the reconstruction. By incorporating image-based priors like pose and optical flow, the method achieves high-quality, time-consistent 3D models, outperforming existing techniques in challenging scenarios.'}, 'zh': {'title': '从单目视频重建高保真人体模型', 'desc': '本文提出了一种从单目视频中重建时间一致的人体模型的方法，特别关注松散衣物或手持物体的交互。以往的人体重建工作通常局限于紧身衣物或不涉及物体交互，或者需要昂贵的多视角捕捉或个性化模板扫描。我们的方法结合了从大规模训练数据中学习的通用人体形状先验与视频特定的“骨骼袋”变形，通过测试时优化来实现高质量且灵活的重建。最终，我们的模型能够提取出时间一致的网格，或进一步优化为高保真度的3D高斯体，以实现交互式渲染。'}}}, {'id': 'https://huggingface.co/papers/2410.00231', 'title': 'Helpful DoggyBot: Open-World Object Fetching using Legged Robots and Vision-Language Models', 'url': 'https://huggingface.co/papers/2410.00231', 'abstract': "Learning-based methods have achieved strong performance for quadrupedal locomotion. However, several challenges prevent quadrupeds from learning helpful indoor skills that require interaction with environments and humans: lack of end-effectors for manipulation, limited semantic understanding using only simulation data, and low traversability and reachability in indoor environments. We present a system for quadrupedal mobile manipulation in indoor environments. It uses a front-mounted gripper for object manipulation, a low-level controller trained in simulation using egocentric depth for agile skills like climbing and whole-body tilting, and pre-trained vision-language models (VLMs) with a third-person fisheye and an egocentric RGB camera for semantic understanding and command generation. We evaluate our system in two unseen environments without any real-world data collection or training. Our system can zero-shot generalize to these environments and complete tasks, like following user's commands to fetch a randomly placed stuff toy after climbing over a queen-sized bed, with a 60% success rate. Project website: https://helpful-doggybot.github.io/", 'score': 6, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': '341c4a5116bec7b6', 'data': {'categories': ['#agents', '#robotics', '#cv', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Умный робопёс: манипуляция и навигация в помещениях без дополнительного обучения', 'desc': 'Статья представляет систему для квадропедальной мобильной манипуляции в помещениях. Система использует фронтальный захват для манипуляции объектами и контроллер нижнего уровня, обученный в симуляции с использованием эгоцентрической глубины для ловких навыков. Для семантического понимания и генерации команд применяются предобученные визуально-языковые модели с камерами. Система демонстрирует способность к обобщению в незнакомой среде без дополнительного обучения на реальных данных.'}, 'en': {'title': 'Empowering Quadrupeds for Indoor Mobile Manipulation', 'desc': "This paper presents a novel system for quadrupedal robots to perform mobile manipulation tasks in indoor settings. The system integrates a front-mounted gripper for handling objects and employs a low-level controller trained in simulation to enable agile movements like climbing. It also utilizes pre-trained vision-language models to enhance semantic understanding and command execution. The results demonstrate the robot's ability to generalize to new environments and successfully complete tasks with a 60% success rate, showcasing its effectiveness in real-world scenarios without prior training data."}, 'zh': {'title': '四足机器人：室内操控的新突破', 'desc': '本论文提出了一种四足机器人在室内环境中进行移动操控的系统。该系统使用前置抓手进行物体操控，并通过模拟训练的低级控制器实现灵活的技能，如攀爬和全身倾斜。我们还结合了预训练的视觉-语言模型，以增强语义理解和指令生成能力。实验表明，该系统能够在未见过的环境中零-shot泛化，成功完成任务，成功率达到60%。'}}}, {'id': 'https://huggingface.co/papers/2410.00545', 'title': 'What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study', 'url': 'https://huggingface.co/papers/2410.00545', 'abstract': 'Gender bias in machine translation (MT) is recognized as an issue that can harm people and society. And yet, advancements in the field rarely involve people, the final MT users, or inform how they might be impacted by biased technologies. Current evaluations are often restricted to automatic methods, which offer an opaque estimate of what the downstream impact of gender disparities might be. We conduct an extensive human-centered study to examine if and to what extent bias in MT brings harms with tangible costs, such as quality of service gaps across women and men. To this aim, we collect behavioral data from 90 participants, who post-edited MT outputs to ensure correct gender translation. Across multiple datasets, languages, and types of users, our study shows that feminine post-editing demands significantly more technical and temporal effort, also corresponding to higher financial costs. Existing bias measurements, however, fail to reflect the found disparities. Our findings advocate for human-centered approaches that can inform the societal impact of bias.', 'score': 5, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': '17621aa133820cd6', 'data': {'categories': ['#multilingual', '#ethics', '#dataset', '#data'], 'emoji': '🚺', 'ru': {'title': 'Гендерное неравенство в машинном переводе: реальные последствия для пользователей', 'desc': 'Это исследование посвящено изучению гендерного смещения в машинном переводе и его влияния на пользователей. Авторы провели масштабное исследование с участием 90 человек, которые редактировали результаты машинного перевода для обеспечения корректного гендерного перевода. Результаты показали, что редактирование женского рода требует значительно больше технических и временных усилий, что также соответствует более высоким финансовым затратам. Исследование подчеркивает важность человеко-ориентированных подходов для оценки социального воздействия смещения в машинном обучении.'}, 'en': {'title': 'Unveiling Gender Bias: The Hidden Costs of Machine Translation', 'desc': 'This paper addresses the issue of gender bias in machine translation (MT) and its potential negative effects on users. The authors conducted a human-centered study involving 90 participants to analyze the impact of biased MT outputs on service quality for different genders. Their findings reveal that translating feminine terms requires more effort and incurs higher costs compared to masculine terms. The study highlights the inadequacy of current bias measurement methods and calls for approaches that consider the real-world implications of bias in MT.'}, 'zh': {'title': '关注机器翻译中的性别偏见', 'desc': '这篇论文探讨了机器翻译中的性别偏见问题，指出这种偏见可能对个人和社会造成伤害。研究表明，现有的评估方法主要依赖自动化手段，无法准确反映性别差异对用户的实际影响。通过对90名参与者的行为数据进行分析，发现女性的后编辑工作需要更多的技术和时间投入，导致更高的经济成本。论文呼吁采用以人为本的方法，以更好地理解偏见对社会的影响。'}}}, {'id': 'https://huggingface.co/papers/2409.18313', 'title': 'Embodied-RAG: General non-parametric Embodied Memory for Retrieval and Generation', 'url': 'https://huggingface.co/papers/2409.18313', 'abstract': "There is no limit to how much a robot might explore and learn, but all of that knowledge needs to be searchable and actionable. Within language research, retrieval augmented generation (RAG) has become the workhouse of large-scale non-parametric knowledge, however existing techniques do not directly transfer to the embodied domain, which is multimodal, data is highly correlated, and perception requires abstraction.   To address these challenges, we introduce Embodied-RAG, a framework that enhances the foundational model of an embodied agent with a non-parametric memory system capable of autonomously constructing hierarchical knowledge for both navigation and language generation. Embodied-RAG handles a full range of spatial and semantic resolutions across diverse environments and query types, whether for a specific object or a holistic description of ambiance. At its core, Embodied-RAG's memory is structured as a semantic forest, storing language descriptions at varying levels of detail. This hierarchical organization allows the system to efficiently generate context-sensitive outputs across different robotic platforms. We demonstrate that Embodied-RAG effectively bridges RAG to the robotics domain, successfully handling over 200 explanation and navigation queries across 19 environments, highlighting its promise for general-purpose non-parametric system for embodied agents.", 'score': 3, 'issue_id': 1, 'pub_date': '2024-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '37095809c304fb89', 'data': {'categories': ['#agents', '#rag', '#multimodal', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Иерархическая память для интеллектуальных роботов', 'desc': 'Статья представляет Embodied-RAG - фреймворк для воплощённых агентов, который улучшает базовую модель с помощью непараметрической системы памяти. Эта система способна автономно создавать иерархические знания для навигации и генерации языка. Память Embodied-RAG структурирована как семантический лес, хранящий языковые описания на разных уровнях детализации. Фреймворк успешно обрабатывает более 200 запросов по объяснению и навигации в 19 различных средах.'}, 'en': {'title': 'Empowering Robots with Hierarchical Knowledge for Smart Navigation and Language Generation', 'desc': 'The paper introduces Embodied-RAG, a new framework that combines retrieval augmented generation (RAG) with embodied agents, which are robots that interact with the physical world. This framework allows robots to build a non-parametric memory system that organizes knowledge hierarchically, enabling them to navigate and generate language based on their environment. By structuring memory as a semantic forest, Embodied-RAG can efficiently respond to various queries about objects and surroundings. The results show that this approach successfully integrates RAG techniques into robotics, demonstrating its effectiveness across multiple environments and tasks.'}, 'zh': {'title': 'Embodied-RAG：机器人知识检索与生成的新框架', 'desc': '本文介绍了一种名为Embodied-RAG的框架，旨在解决机器人在多模态环境中知识检索和生成的问题。该框架通过非参数记忆系统增强了机器人基础模型，能够自主构建层次化知识，以支持导航和语言生成。Embodied-RAG的记忆结构为语义森林，能够存储不同细节层次的语言描述，从而高效生成上下文相关的输出。实验表明，Embodied-RAG成功处理了超过200个解释和导航查询，展示了其在机器人领域的广泛应用潜力。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (3)', '#agi', '#alignment', '#architecture (1)', '#audio', '#benchmark (4)', '#cv (5)', '#data (2)', '#dataset (4)', '#diffusion (2)', '#edge_computing (1)', '#ethics (1)', '#games', '#graphs', '#hallucinations', '#inference (2)', '#interpretability (1)', '#long_context', '#math', '#medicine', '#multilingual (2)', '#multimodal (7)', '#optimization (2)', '#plp', '#rag (1)', '#reasoning', '#rl', '#rlhf', '#robotics (2)', '#security', '#story_generation', '#survey', '#synthetic (2)', '#training (4)', '#transfer_learning', '#translation', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-10-02 09:00',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-02 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-02 09:00')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    